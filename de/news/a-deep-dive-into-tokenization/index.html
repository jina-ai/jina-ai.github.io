<!DOCTYPE html><html translate="no" dir="ltr" lang="de"><head><title>Ein tiefer Einblick in die Tokenisierung</title><meta charset="utf-8"><meta name="title" content="Ein tiefer Einblick in die Tokenisierung"><meta name="description" content="Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile für die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="og:title" content="Ein tiefer Einblick in die Tokenisierung"><meta property="og:description" content="Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile für die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?"><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="twitter:title" content="Ein tiefer Einblick in die Tokenisierung"><meta property="twitter:description" content="Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile für die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?"><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-DJm-2s1u.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-CRvJtbiE.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-CXC4umWI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-DsVvRv6j.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-3Q-S8Ukg.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-CjnCvMRm.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-BYU62XRd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-G5BvMimi.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/de-Ck-F6j5P.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-DnhV6gl0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-DeP9WPCr.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-D4JGVYni.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-Cah-4C30.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/format-DyQxkAtJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-CBeumOw7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QToolbar-varuBTQn.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-DAl7NNQD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-n3MlXCVG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-CtBQVMY8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-82zeZPKH.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-BfYflfOA.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/PurchaseSuccessDialog-DckCCiQj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-lJtvBrLE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-CTIpTZFG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/embedding-YNc8dXMJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-JkFO5WjM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-CO8s_7sr.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-BjZAQhxR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-C8its8xQ.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-Dg3sc2WQ.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-DHy6tWOs.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-CTqpj6cG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-DsbzLRX7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-BhcKZJI7.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-oGth_jHT.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-CovHgG0a.css"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-q765RzAS.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><meta name="author" content="Scott Martens"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-01-31T16:10:14.000+01:00"><meta property="article:modified_time" content="2024-08-14T11:38:01.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Ein tiefer Einblick in die Tokenisierung",
  "description": "Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile für die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?",
  "image": [
    "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"
  ],
  "datePublished": "2024-01-31T16:10:14.000+01:00",
  "dateModified": "2024-08-14T11:38:01.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><link prerender-ignore rel=preconnect href=//api.usercentrics.eu><link prerender-ignore rel=preconnect href=//privacy-proxy.usercentrics.eu><link prerender-ignore rel=preload href=//app.usercentrics.eu/browser-ui/latest/loader.js as=script><link prerender-ignore rel=preload href=//privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js as=script><script prerender-ignore id=usercentrics-cmp data-settings-id=w5v6v2pJsC3wdR src=https://app.usercentrics.eu/browser-ui/latest/loader.js async></script><script prerender-ignore src=https://privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div class="q-space"></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Pressemitteilungen</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_fe8e9435-d647-40b4-a3a4-e7da831e4f0d" aria-label="Erweitern Sie &quot;Produkte&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Produkte</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_fe8e9435-d647-40b4-a3a4-e7da831e4f0d" style="display: none;"><div class="q-list q-list--dark" role="list" label="Produkte"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Für Firmen</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Einbettungen</div><div class="q-item__label q-item__label--caption text-caption">Multimodale und mehrsprachige Einbettungen von Weltklasse.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reranker</div><div class="q-item__label q-item__label--caption text-caption">Neural Retriever der Weltklasse zur Maximierung der Suchrelevanz.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Leser</div><div class="q-item__label q-item__label--caption text-caption">Lesen Sie URLs und suchen Sie im Internet nach fundierteren LLMs.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Klassifikator</div><div class="q-item__label q-item__label--caption text-caption">Zero-Shot- und Few-Shot-Klassifizierung für Bild und Text.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentierer</div><div class="q-item__label q-item__label--caption text-caption">Schneiden Sie langen Text in Abschnitte und führen Sie eine Tokenisierung durch.</div></div></a><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Für Power-User</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://promptperfect.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://promptperfect.jina.ai/PromptPerfect-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">PromptPerfect</div><div class="q-item__label q-item__label--caption text-caption">Erstklassiges Tool für schnelles Engineering</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_38e5ea08-b6b5-48ca-ad08-c639eeba4268" aria-label="Erweitern Sie &quot;Mehr Power-User-Tools&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Mehr Power-User-Tools</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_38e5ea08-b6b5-48ca-ad08-c639eeba4268" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://scenex.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://scenex.jina.ai/SceneX - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">SceneXplain</div><div class="q-item__label q-item__label--caption text-caption">Führende KI-Lösung für Bildunterschriften und Videozusammenfassungen</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://bestbanner.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://bestbanner.jina.ai/bestbanner-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">BestBanner</div><div class="q-item__label q-item__label--caption text-caption">Vom Blog-Artikel zum Banner, ohne eigene Prompts!</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://chat.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://chat.jina.ai/JinaChat - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JinaChat</div><div class="q-item__label q-item__label--caption text-caption">Mehr Modalitäten, längerer Speicher, weniger Kosten</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://rationale.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://rationale.jina.ai/Rationale-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Rationale</div><div class="q-item__label q-item__label--caption text-caption">Ultimative KI-Tools zur Entscheidungsfindung</div></div></a></div></div></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_4fda153b-65a4-423d-8574-332e3f51d5f5" aria-label="Erweitern Sie &quot;Unternehmen&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Unternehmen</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_4fda153b-65a4-423d-8574-332e3f51d5f5" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Über uns</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Kontaktieren Sie unseren Vertrieb</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Praktikantenprogramm</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Begleiten Sie uns</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Logo herunterladen</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Terms &amp; amp; Bedingungen</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Einloggen"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div><div class="q-item__section column q-item__section--main justify-center">Einloggen</div></a></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 56px;"><main data-v-33ef2eff="" class="q-page" style="min-height: 100vh;"><div data-v-33ef2eff="" class="row full-width relative-position justify-end"><div data-v-33ef2eff="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-33ef2eff="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">tl;dr</div></div></div><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">Wörter, Tokens, Zahlen</div></div></div><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">Abbildung von Sprache auf Zahlen</div></div></div><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">Warum tokenisieren wir? Und warum auf diese Weise?</div></div></div><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">Empirische Schätzungen der Token-Ausgabegrößen</div></div></div><div data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-33ef2eff="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-33ef2eff="" class="q-item__label">Token ernst nehmen</div></div></div></div></div><div data-v-33ef2eff="" class="col-12 col-md-10 col-lg-12"><div data-v-33ef2eff="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech-Blog</div></div></div></div><div data-v-33ef2eff="" class="row justify-center"><div data-v-33ef2eff="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-33ef2eff="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">Januar 31, 2024</div><h1 data-v-33ef2eff="" class="text-weight-medium text-center q-px-md my-title">Ein tiefer Einblick in die Tokenisierung</h1><div data-v-33ef2eff="" class="col row justify-center"><div data-v-33ef2eff="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile für die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?</div></div><div data-v-33ef2eff="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-33ef2eff="" class="q-img q-img--menu" role="img" aria-label="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-33ef2eff="" class="row justify-center"><div data-v-33ef2eff="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-33ef2eff="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-33ef2eff="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-33ef2eff="" class="q-item__label">Scott Martens • 16 Minuten gelesen</div></div></div></div><div data-v-33ef2eff="" class="row justify-center"><div data-v-33ef2eff="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-33ef2eff="" class="article"><section data-v-33ef2eff="" class="gh-content"><p>Es gibt viele Hürden beim Verständnis von AI-Modellen, einige davon ziemlich große, und sie können die Implementierung von AI-Prozessen erschweren. Aber die erste, auf die viele Menschen stoßen, ist das Verständnis dessen, was wir meinen, wenn wir von <strong>Tokens</strong> sprechen.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/tokenizer"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Tokenizer API</div><div class="kg-bookmark-description">Free API to tokenize texts, count and get first/last-N tokens.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-tokenize-api.png" alt="" style="cursor: help;"></div></a></figure><p>Einer der wichtigsten praktischen Parameter bei der Auswahl eines AI-Sprachmodells ist die Größe seines Kontextfensters — die maximale Eingabetextgröße —, die in Tokens angegeben wird, nicht in Wörtern oder Zeichen oder anderen automatisch erkennbaren Einheiten.</p><p>Darüber hinaus werden Embedding-Services typischerweise „pro Token" berechnet, was bedeutet, dass Tokens wichtig für das Verständnis Ihrer Rechnung sind.</p><p>Dies kann sehr verwirrend sein, wenn man nicht genau weiß, was ein Token ist.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="Jina Embeddings current price sheet (as of February 2024)." width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Aktuelle Preisliste von Jina Embeddings (Stand: Februar 2024). Beachten Sie, dass die Preise pro „1M Tokens" angegeben sind.</span></figcaption></figure><p>Aber von allen verwirrenden Aspekten der modernen AI sind Tokens wahrscheinlich die am wenigsten komplizierten. Dieser Artikel wird versuchen zu erklären, was Tokenisierung ist, was sie bewirkt und warum wir sie auf diese Weise durchführen.</p><h2 id="tldr" style="position: relative;"><a href="#tldr" title="tl;dr" id="anchor-tldr"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>tl;dr</h2><p>Für diejenigen, die eine schnelle Antwort suchen, um herauszufinden, wie viele Tokens sie von Jina Embeddings kaufen sollten oder eine Schätzung, wie viele sie voraussichtlich kaufen müssen, sind die folgenden Statistiken das, wonach Sie suchen.</p><h3 id="tokens-per-english-word" style="position: relative;"><a href="#tokens-per-english-word" title="Tokens pro englisches Wort" id="anchor-tokens-per-english-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens pro englisches Wort</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Ein Aufruf der Jina Embeddings v2 API für englische Modelle verwendet <b><strong style="white-space: pre-wrap;">ungefähr</strong></b> <b><strong style="white-space: pre-wrap;">10% mehr</strong></b> Tokens als die Anzahl der Wörter in Ihrem Text, <b><strong style="white-space: pre-wrap;">plus zwei Tokens pro Embedding</strong></b>.</div></div><p>Während empirischer Tests, die weiter unten in diesem Artikel beschrieben werden, wurden verschiedene englische Texte mit einer Rate von etwa 10% mehr Tokens als Wörter in Tokens umgewandelt, wenn die englischsprachigen Modelle von Jina Embeddings verwendet wurden. Dieses Ergebnis war ziemlich robust.</p><p>Jina Embeddings v2 Modelle haben ein Kontextfenster von 8192 Tokens. Das bedeutet, dass wenn Sie einem Jina-Modell einen englischen Text mit mehr als 7.400 Wörtern übergeben, dieser mit hoher Wahrscheinlichkeit abgeschnitten wird.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Die maximale Eingabegröße für <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 für Englisch</strong></b> beträgt ungefähr <b><strong style="white-space: pre-wrap;">7.400 Wörter</strong></b>.</div></div><h3 id="tokens-per-chinese-character" style="position: relative;"><a href="#tokens-per-chinese-character" title="Tokens pro chinesisches Zeichen" id="anchor-tokens-per-chinese-character"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens pro chinesisches Zeichen</h3><p>Für Chinesisch sind die Ergebnisse variabler. Je nach Textart variierten die Verhältnisse von 0,6 bis 0,75 Tokens pro chinesischem Zeichen (汉字). Englische Texte, die an Jina Embeddings v2 für Chinesisch übergeben werden, erzeugen ungefähr die gleiche Anzahl von Tokens wie Jina Embeddings v2 für Englisch: etwa 10% mehr als die Anzahl der Wörter.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Die maximale Eingabegröße für Chinesisch in <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 für Chinesisch und Englisch</strong></b> beträgt ungefähr <b><strong style="white-space: pre-wrap;">10.500 Zeichen</strong></b> (<b><strong style="white-space: pre-wrap;">字数</strong></b>), oder <b><strong style="white-space: pre-wrap;">0,6 bis 0,75 Tokens pro chinesischem Zeichen, plus zwei pro Embedding.</strong></b></div></div><h3 id="tokens-per-german-word" style="position: relative;"><a href="#tokens-per-german-word" title="Tokens pro deutsches Wort" id="anchor-tokens-per-german-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens pro deutsches Wort</h3><p>Deutsche Wort-zu-Token-Verhältnisse sind variabler als Englisch, aber weniger als Chinesisch. Je nach Textgenre erhielt ich durchschnittlich 20% bis 30% mehr Tokens als Wörter. Wenn man englische Texte an Jina Embeddings v2 für Deutsch und Englisch übergibt, werden etwas mehr Tokens verwendet als bei den reinen Englisch- und Chinesisch/Englisch-Modellen: 12% bis 15% mehr Tokens als Wörter.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Jina Embeddings v2 für Deutsch und Englisch zählt <b><strong style="white-space: pre-wrap;">20% bis 30% mehr Tokens als Wörter, plus zwei pro Embedding</strong></b>. Die maximale Größe des Eingabekontexts beträgt ungefähr <b><strong style="white-space: pre-wrap;">6.300 deutsche Wörter</strong></b>.</div></div><h3 id="caution" style="position: relative;"><a href="#caution" title="Vorsicht!" id="anchor-caution"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Vorsicht!</h3><p>Dies sind einfache Berechnungen, aber sie sollten für die meisten natürlichsprachlichen Texte und die meisten Benutzer ungefähr stimmen. Letztendlich können wir nur versprechen, dass die Anzahl der Tokens immer nicht mehr als die Anzahl der Zeichen in Ihrem Text plus zwei beträgt. Praktisch wird es immer viel weniger sein, aber wir können im Voraus keine spezifische Anzahl versprechen.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Ihre Ergebnisse können abweichen!</strong></b><br><br>Dies sind Schätzungen basierend auf statistisch naiven Berechnungen. Wir garantieren nicht, wie viele Tokens eine bestimmte Anfrage benötigen wird.</div></div><p>Wenn Sie nur einen Rat brauchen, wie viele Tokens Sie für Jina Embeddings kaufen sollten, können Sie hier aufhören. Andere Embedding-Modelle von anderen Unternehmen als Jina AI haben möglicherweise nicht die gleichen Token-zu-Wort- und Token-zu-Chinesisches-Zeichen-Verhältnisse wie Jina-Modelle, aber sie werden sich im Allgemeinen nicht sehr stark unterscheiden.</p><p>Wenn Sie verstehen möchten, warum das so ist, bietet der Rest dieses Artikels einen tieferen Einblick in die Tokenisierung für Sprachmodelle.</p><h2 id="words-tokens-numbers" style="position: relative;"><a href="#words-tokens-numbers" title="Wörter, Tokens, Zahlen" id="anchor-words-tokens-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Wörter, Tokens, Zahlen</h2><p>Tokenisierung ist schon länger Teil der Verarbeitung natürlicher Sprache als moderne AI-Modelle existieren.</p><p>Es ist ein bisschen klischeehaft zu sagen, dass alles in einem Computer nur eine Zahl ist, aber es stimmt größtenteils. Sprache ist jedoch von Natur aus nicht nur ein Haufen Zahlen. Sie kann Sprache sein, die aus Schallwellen besteht, oder Schrift, die aus Zeichen auf Papier besteht, oder sogar ein Bild eines gedruckten Textes oder ein Video von jemandem, der Gebärdensprache verwendet. Aber meistens, wenn wir über die Verarbeitung natürlicher Sprache mit Computern sprechen, meinen wir Texte, die aus Sequenzen von Zeichen bestehen: Buchstaben (a, b, c usw.), Ziffern (0, 1, 2…), Interpunktion und Leerzeichen in verschiedenen Sprachen und Textkodierungen.</p><p>Computeringenieure nennen diese „Strings".</p><p>AI-Sprachmodelle nehmen Zahlensequenzen als Eingabe. Wenn Sie also den Satz schreiben:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Aber nach der Tokenisierung erhält das AI-Modell als Eingabe:</p><pre><code class="language-python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>Tokenisierung ist der Prozess der Umwandlung einer Eingabezeichenkette in eine spezifische Zahlensequenz, die Ihr AI-Modell verstehen kann.</p><p>Wenn Sie ein AI-Modell über eine Web-API verwenden, die Benutzern pro Token berechnet wird, wird jede Anfrage in eine Zahlensequenz wie die obige umgewandelt. Die Anzahl der Tokens in der Anfrage ist die Länge dieser Zahlensequenz. Wenn Sie also Jina Embeddings v2 für Englisch bitten, Ihnen ein Embedding für „<em>What is today's weather in Berlin?</em>" zu geben, kostet Sie das 11 Tokens, weil es diesen Satz in eine Sequenz von 11 Zahlen umgewandelt hat, bevor es ihn an das AI-Modell weitergibt.</p><p>AI-Modelle, die auf der <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer-Architektur</a> basieren, haben ein Kontextfenster fester Größe, dessen Größe in Tokens gemessen wird. Manchmal wird dies auch „Eingabefenster", „Kontextgröße" oder „Sequenzlänge" genannt (besonders auf dem <a href="https://huggingface.co/spaces/mteb/leaderboard">Hugging Face MTEB Leaderboard</a>). Es bedeutet die maximale Textgröße, die das Modell auf einmal sehen kann.</p><p>Wenn Sie also ein Embedding-Modell verwenden möchten, ist dies die maximal erlaubte Eingabegröße.</p><p>Jina Embeddings v2 Modelle haben alle ein Kontextfenster von 8.192 Tokens. Andere Modelle werden unterschiedliche (typischerweise kleinere) Kontextfenster haben. Das bedeutet, dass egal wie viel Text Sie eingeben, der mit diesem Jina Embeddings-Modell verbundene Tokenizer ihn in nicht mehr als 8.192 Tokens umwandeln muss.</p><h2 id="mapping-language-to-numbers" style="position: relative;"><a href="#mapping-language-to-numbers" title="Abbildung von Sprache auf Zahlen" id="anchor-mapping-language-to-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Abbildung von Sprache auf Zahlen</h2><p>Die einfachste Art, die Logik von Tokens zu erklären, ist diese:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Ein Token ist eine Zahl, die für einen Teil eines Strings steht.</div></div><p>Bei Modellen für natürliche Sprache ist der Teil eines Strings, für den ein Token steht, ein Wort, ein Teil eines Wortes oder ein Interpunktionszeichen. Leerzeichen erhalten in der Regel keine explizite Darstellung in der Tokenizer-Ausgabe.</p><p>Tokenisierung ist Teil einer Gruppe von Techniken in der Verarbeitung natürlicher Sprache, die <a href="https://en.wikipedia.org/wiki/Text_segmentation"><em>Textsegmentierung</em></a> genannt wird, und das Modul, das die Tokenisierung durchführt, wird sehr logisch als <strong>Tokenizer</strong> bezeichnet.</p><p>Um zu zeigen, wie Tokenisierung funktioniert, werden wir einige Sätze mit dem kleinsten Jina Embeddings v2 Modell für Englisch tokenisieren: <code>jina-embeddings-v2-small-en</code>. Das andere rein englische Modell von Jina Embeddings — <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> — verwendet den gleichen Tokenizer, also macht es keinen Sinn, extra Megabytes an AI-Modell herunterzuladen, die wir in diesem Artikel nicht verwenden werden.</p><p>Installieren Sie zuerst das <code>transformers</code> Modul in Ihrer Python-Umgebung oder Notebook. Verwenden Sie die<code>-U</code> Flag, um sicherzustellen, dass Sie auf die neueste Version aktualisieren, da dieses Modell mit einigen älteren Versionen nicht funktioniert:</p><pre><code class="language-bash hljs">pip install -U transformers
</code></pre><p>Laden Sie dann <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> mit <code>AutoModel.from_pretrained</code> herunter:</p><pre><code class="language-Python hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-small-en'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
</code></pre><p>Um einen String zu tokenisieren, verwenden Sie die <code>encode</code> Methode des <code>tokenizer</code> Member-Objekts des Modells:</p><pre><code class="language-Python hljs">model.tokenizer.encode(<span class="hljs-string">"What is today's weather in Berlin?"</span>)
</code></pre><p>Das Ergebnis ist eine Liste von Zahlen:</p><pre><code class="language-Python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>Um diese Zahlen wieder in String-Form umzuwandeln, verwenden Sie die <code>convert_ids_to_tokens</code> Methode des <code>tokenizer</code> Objekts:</p><pre><code class="language-Python hljs">model.tokenizer.convert_ids_to_tokens([<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>])
</code></pre><p>Das Ergebnis ist eine Liste von Strings:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'what'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">"'"</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'in'</span>,
 <span class="hljs-string">'berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Beachten Sie, dass der Tokenizer des Modells:</p><ol><li><code>[CLS]</code> am Anfang und <code>[SEP]</code> am Ende hinzugefügt hat. Dies ist aus technischen Gründen notwendig und bedeutet, dass <strong>jede Anfrage für ein Embedding zwei zusätzliche Token kostet</strong>, zusätzlich zu den Token, die der Text benötigt.</li><li>Interpunktion von Wörtern getrennt hat, wodurch aus „<em>Berlin?</em>": <code>berlin</code> und <code>?</code> wird, und aus „<em>today's</em>": <code>today</code>, <code>'</code> und <code>s</code>.</li><li>Alles in Kleinbuchstaben umgewandelt hat. Nicht alle Modelle machen das, aber es kann beim Training mit Englisch hilfreich sein. Bei Sprachen, in denen Großschreibung eine andere Bedeutung hat, ist es möglicherweise weniger hilfreich.</li></ol><p>Verschiedene Wörter-Zähl-Algorithmen in verschiedenen Programmen könnten die Wörter in diesem Satz unterschiedlich zählen. OpenOffice zählt dies als sechs Wörter. Der Unicode-Textsegmentierungsalgorithmus (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) zählt sieben Wörter. Andere Software kann zu anderen Zahlen kommen, je nachdem, wie sie mit Interpunktion und Klitika wie "'s" umgeht.</p><p>Der Tokenizer für dieses Modell produziert neun Token für diese sechs oder sieben Wörter, plus die zwei zusätzlichen Token, die bei jeder Anfrage benötigt werden.</p><p>Versuchen wir es jetzt mit einem weniger gebräuchlichen Ortsnamen als Berlin:</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"I live in Kinshasa."</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>Das Ergebnis:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'kin'</span>, <span class="hljs-string">'##sha'</span>, <span class="hljs-string">'##sa'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Der Name "Kinshasa" wird in drei Token aufgeteilt: <code>kin</code>, <code>##sha</code> und <code>##sa</code>. Das <code>##</code> zeigt an, dass dieser Token nicht der Beginn eines Wortes ist.</p><p>Wenn wir dem Tokenizer etwas völlig Fremdes geben, erhöht sich die Anzahl der Token im Verhältnis zur Anzahl der Wörter noch mehr:</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"Klaatu barada nikto"</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)

[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'##la'</span>, <span class="hljs-string">'##at'</span>, <span class="hljs-string">'##u'</span>, <span class="hljs-string">'bar'</span>, <span class="hljs-string">'##ada'</span>, <span class="hljs-string">'nik'</span>, <span class="hljs-string">'##to'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Drei Wörter werden zu acht Token, plus die <code>[CLS]</code> und <code>[SEP]</code> Token.</p><p>Die Tokenisierung im Deutschen ist ähnlich. Mit dem <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/" rel="noreferrer">Jina Embeddings v2 für Deutsch</a> Modell können wir eine Übersetzung von "What is today's weather in Berlin?" auf die gleiche Weise tokenisieren wie beim englischen Modell.</p><pre><code class="language-Python hljs">german_model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-base-de'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
token_ids = german_model.tokenizer.encode(<span class="hljs-string">"Wie wird das Wetter heute in Berlin?"</span>)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>Das Ergebnis:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Wie'</span>, <span class="hljs-string">'wird'</span>, <span class="hljs-string">'das'</span>, <span class="hljs-string">'Wetter'</span>, <span class="hljs-string">'heute'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'Berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Dieser Tokenizer unterscheidet sich ein wenig vom englischen darin, dass <code>&lt;s&gt;</code> und <code>&lt;/s&gt;</code> <code>[CLS]</code> und <code>[SEP]</code> ersetzen, aber die gleiche Funktion erfüllen. Außerdem wird der Text nicht in Kleinbuchstaben normalisiert – Groß- und Kleinschreibung bleiben wie geschrieben – da Großschreibung im Deutschen eine andere Bedeutung hat als im Englischen.</p><p>(Um diese Darstellung zu vereinfachen, habe ich ein spezielles Zeichen entfernt, das den Wortanfang anzeigt.)</p><p>Versuchen wir es jetzt mit einem komplexeren Satz <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html">aus einem Zeitungstext</a>:</p><blockquote>Ein Großteil der milliardenschweren Bauern-Subventionen bleibt liegen – zu genervt sind die Landwirte von bürokratischen Gängelungen und Regelwahn.</blockquote><pre><code class="hljs language-python">sentence = <span class="hljs-string">"""
Ein Großteil der milliardenschweren Bauern-Subventionen
bleibt liegen – zu genervt sind die Landwirte von 
bürokratischen Gängelungen und Regelwahn.
"""</span>
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)</code></pre><p>Das tokenisierte Ergebnis:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Ein'</span>, <span class="hljs-string">'Großteil'</span>, <span class="hljs-string">'der'</span>, <span class="hljs-string">'mill'</span>, <span class="hljs-string">'iarden'</span>, <span class="hljs-string">'schwer'</span>, 
 <span class="hljs-string">'en'</span>, <span class="hljs-string">'Bauern'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'Sub'</span>, <span class="hljs-string">'ventionen'</span>, <span class="hljs-string">'bleibt'</span>, <span class="hljs-string">'liegen'</span>, 
 <span class="hljs-string">'–'</span>, <span class="hljs-string">'zu'</span>, <span class="hljs-string">'gen'</span>, <span class="hljs-string">'ervt'</span>, <span class="hljs-string">'sind'</span>, <span class="hljs-string">'die'</span>, <span class="hljs-string">'Landwirte'</span>, <span class="hljs-string">'von'</span>, 
 <span class="hljs-string">'büro'</span>, <span class="hljs-string">'krat'</span>, <span class="hljs-string">'ischen'</span>, <span class="hljs-string">'Gän'</span>, <span class="hljs-string">'gel'</span>, <span class="hljs-string">'ungen'</span>, <span class="hljs-string">'und'</span>, <span class="hljs-string">'Regel'</span>, 
 <span class="hljs-string">'wahn'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Hier sehen Sie, dass viele deutsche Wörter in kleinere Teile zerlegt wurden, und zwar nicht unbedingt entlang der von der deutschen Grammatik erlaubten Linien. Das Ergebnis ist, dass ein langes deutsches Wort, das für einen Wortzähler als nur ein Wort zählen würde, für Jinas KI-Modell eine beliebige Anzahl von Token sein kann.</p><p>Machen wir dasselbe auf Chinesisch und übersetzen "What is today's weather in Berlin?" als:</p><blockquote>柏林今天的天气怎么样？</blockquote><pre><code class="hljs language-makefile">chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(<span class="hljs-string">"柏林今天的天气怎么样？"</span>)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>Das tokenisierte Ergebnis:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'柏林'</span>, <span class="hljs-string">'今天的'</span>, <span class="hljs-string">'天气'</span>, <span class="hljs-string">'怎么样'</span>, <span class="hljs-string">'？'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Im Chinesischen gibt es normalerweise keine Worttrennungen im geschriebenen Text, aber der Jina Embeddings Tokenizer fügt häufig mehrere chinesische Zeichen zusammen:</p>

<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Bedeutung</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏林</td>
<td>Bólín</td>
<td>Berlin</td>
</tr>
<tr>
<td>今天的</td>
<td>jīntiān de</td>
<td>heute</td>
</tr>
<tr>
<td>天气</td>
<td>tiānqì</td>
<td>Wetter</td>
</tr>
<tr>
<td>怎么样</td>
<td>zěnmeyàng</td>
<td>wie</td>
</tr>
</tbody>
</table>

<p>Verwenden wir einen komplexeren Satz <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7">aus einer Hongkonger Zeitung</a>:</p><pre><code class="language-Python hljs">sentence = <span class="hljs-string">"""
新規定執行首日，記者在下班高峰前的下午5時來到廣州地鐵3號線，
從繁忙的珠江新城站啟程，向機場北方向出發。
"""</span>
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>(Übersetzung: <em>„Am ersten Tag, an dem die neuen Bestimmungen in Kraft waren, traf dieser Reporter um 17 Uhr, während der Hauptverkehrszeit, an der Guangzhou Metro Linie 3 ein und war von der Station Zhujiang New Town in Richtung Flughafen unterwegs."</em>)</p><p>Das Ergebnis:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'新'</span>, <span class="hljs-string">'規定'</span>, <span class="hljs-string">'執行'</span>, <span class="hljs-string">'首'</span>, <span class="hljs-string">'日'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'記者'</span>, <span class="hljs-string">'在下'</span>, <span class="hljs-string">'班'</span>, 
 <span class="hljs-string">'高峰'</span>, <span class="hljs-string">'前的'</span>, <span class="hljs-string">'下午'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'時'</span>, <span class="hljs-string">'來到'</span>, <span class="hljs-string">'廣州'</span>, <span class="hljs-string">'地'</span>, <span class="hljs-string">'鐵'</span>, <span class="hljs-string">'3'</span>, 
 <span class="hljs-string">'號'</span>, <span class="hljs-string">'線'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'從'</span>, <span class="hljs-string">'繁忙'</span>, <span class="hljs-string">'的'</span>, <span class="hljs-string">'珠江'</span>, <span class="hljs-string">'新城'</span>, <span class="hljs-string">'站'</span>, <span class="hljs-string">'啟'</span>, 
 <span class="hljs-string">'程'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'向'</span>, <span class="hljs-string">'機場'</span>, <span class="hljs-string">'北'</span>, <span class="hljs-string">'方向'</span>, <span class="hljs-string">'出發'</span>, <span class="hljs-string">'。'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Diese Tokens entsprechen keinem spezifischen chinesischen Wörterbuch (词典). Zum Beispiel wird "啟程" - <em>qǐchéng</em> (aufbrechen, sich auf den Weg machen) normalerweise als ein einzelnes Wort kategorisiert, wird hier aber in seine zwei Bestandteile aufgeteilt. Ähnlich verhält es sich mit "在下班", das üblicherweise als zwei Wörter erkannt würde, mit der Trennung zwischen "在" - <em>zài</em> (in, während) und "下班" - <em>xiàbān</em> (Feierabend, Hauptverkehrszeit), nicht zwischen "在下" und "班", wie es der Tokenizer hier macht.</p><p>In allen drei Sprachen stehen die Stellen, an denen der Tokenizer den Text aufteilt, in keinem direkten Zusammenhang mit den logischen Stellen, an denen ein menschlicher Leser sie trennen würde.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Der Tokenizer-Algorithmus verwendet kein konventionelles, sprachbewusstes Wörterbuch, daher entspricht sein Verhalten nicht der Art und Weise, wie Menschen Wörter zählen.</div></div><p>Dies ist keine spezifische Eigenschaft von Jina Embeddings Modellen. Dieser Ansatz zur Tokenisierung ist in der KI-Modellentwicklung nahezu universell. Auch wenn zwei verschiedene KI-Modelle möglicherweise keine identischen Tokenizer haben, werden sie im aktuellen Entwicklungsstand praktisch alle Tokenizer mit dieser Art von Verhalten verwenden.</p><p>Im nächsten Abschnitt wird der spezifische Algorithmus für die Tokenisierung und die dahinterstehende Logik erläutert.</p><h2 id="why-do-we-tokenize-and-why-this-way" style="position: relative;"><a href="#why-do-we-tokenize-and-why-this-way" title="Warum tokenisieren wir? Und warum auf diese Weise?" id="anchor-why-do-we-tokenize-and-why-this-way"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Warum tokenisieren wir? Und warum auf diese Weise?</h2><p>KI-Sprachmodelle nehmen als Eingabe Zahlensequenzen, die für Textsequenzen stehen, aber bevor das zugrunde liegende neuronale Netzwerk ausgeführt und ein Embedding erstellt wird, geschieht noch etwas mehr. Wenn eine Liste von Zahlen präsentiert wird, die kleine Textsequenzen repräsentieren, schlägt das Modell jede Zahl in einem internen Wörterbuch nach, das einen eindeutigen Vektor für jede Zahl speichert. Diese werden dann kombiniert und bilden die Eingabe für das neuronale Netzwerk.</p><p>Das bedeutet, dass der Tokenizer <strong>jeden</strong> Eingabetext, den wir ihm geben, in Tokens umwandeln können <strong>muss</strong>, die im Token-Vektor-Wörterbuch des Modells vorkommen. Wenn wir unsere Tokens aus einem konventionellen Wörterbuch nehmen würden, würde das gesamte Modell beim ersten Auftreten eines Rechtschreibfehlers oder eines seltenen Eigennamens oder Fremdworts stoppen. Es könnte diese Eingabe nicht verarbeiten.</p><p>In der Verarbeitung natürlicher Sprache wird dies als Out-of-Vocabulary (OOV) Problem bezeichnet, und es tritt in allen Textarten und allen Sprachen auf. Es gibt einige Strategien zur Bewältigung des OOV-Problems:</p><ol><li>Ignorieren. Alles, was nicht im Wörterbuch steht, durch ein "unbekannt"-Token ersetzen.</li><li>Umgehen. Statt ein Wörterbuch zu verwenden, das Textsequenzen auf Vektoren abbildet, eines verwenden, das <em>einzelne Zeichen</em> auf Vektoren abbildet. Englisch verwendet meist nur 26 Buchstaben, daher muss dies kleiner und robuster gegen OOV-Probleme sein als jedes Wörterbuch.</li><li>Häufige Teilsequenzen im Text finden, diese ins Wörterbuch aufnehmen und für den Rest Zeichen (Ein-Buchstaben-Tokens) verwenden.</li></ol><p>Die erste Strategie bedeutet, dass viele wichtige Informationen verloren gehen. Das Modell kann nicht einmal aus den Daten lernen, die es gesehen hat, wenn sie die Form von etwas annehmen, das nicht im Wörterbuch steht. Viele Dinge in gewöhnlichem Text sind selbst in den größten Wörterbüchern einfach nicht vorhanden.</p><p>Die zweite Strategie ist möglich und wurde von Forschern untersucht. Sie bedeutet jedoch, dass das Modell viel mehr Eingaben akzeptieren und viel mehr lernen muss. Dies bedeutet ein viel größeres Modell und viel mehr Trainingsdaten für ein Ergebnis, das sich nie als besser erwiesen hat als die dritte Strategie.</p><p>KI-Sprachmodelle implementieren praktisch alle in irgendeiner Form die dritte Strategie. Die meisten verwenden eine Variante des <a href="https://huggingface.co/learn/nlp-course/chapter6/6">Wordpiece-Algorithmus</a> <a href="https://ieeexplore.ieee.org/document/6289079">[Schuster und Nakajima 2012]</a> oder eine ähnliche Technik namens <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/">Senrich et al. 2016</a>] Diese Algorithmen sind <em>sprachagnostisch</em>. Das bedeutet, sie funktionieren für alle geschriebenen Sprachen gleich, ohne Kenntnisse über eine umfassende Liste möglicher Zeichen hinaus. Sie wurden für mehrsprachige Modelle wie Googles BERT entwickelt, die einfach beliebige Eingaben aus dem Internet-Scraping verarbeiten — hunderte von Sprachen und Texte, die keine menschliche Sprache sind, wie Computerprogramme — sodass sie ohne komplizierte Linguistik trainiert werden konnten.</p><p>Einige Forschungen zeigen signifikante Verbesserungen bei der Verwendung von sprachspezifischeren und sprachbewussteren Tokenizern. [<a href="https://aclanthology.org/2021.acl-long.243/">Rust et al. 2021</a>] Aber die Entwicklung solcher Tokenizer erfordert Zeit, Geld und Expertise. Die Implementierung einer universellen Strategie wie BPE oder Wordpiece ist viel günstiger und einfacher.</p><p>Als Konsequenz gibt es jedoch keine Möglichkeit zu wissen, wie viele Tokens ein bestimmter Text repräsentiert, außer ihn durch einen Tokenizer laufen zu lassen und dann die Anzahl der Tokens zu zählen, die dabei herauskommen. Da die kleinste mögliche Teilsequenz eines Textes ein Buchstabe ist, kann man sicher sein, dass die Anzahl der Tokens nicht größer sein wird als die Anzahl der Zeichen (minus Leerzeichen) plus zwei.</p><p>Um eine gute Schätzung zu erhalten, müssen wir unserem Tokenizer viel Text zuführen und empirisch berechnen, wie viele Tokens wir im Durchschnitt erhalten, verglichen mit der Anzahl der Wörter oder Zeichen, die wir eingeben. Im nächsten Abschnitt werden wir einige nicht sehr systematische empirische Messungen für alle derzeit verfügbaren Jina Embeddings v2 Modelle durchführen.</p><h2 id="empirical-estimates-of-token-output-sizes" style="position: relative;"><a href="#empirical-estimates-of-token-output-sizes" title="Empirische Schätzungen der Token-Ausgabegrößen" id="anchor-empirical-estimates-of-token-output-sizes"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Empirische Schätzungen der Token-Ausgabegrößen</h2><p>Für Englisch und Deutsch habe ich den Unicode-Textsegmentierungsalgorithmus (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) verwendet, um Wortzählungen für Texte zu erhalten. Dieser Algorithmus wird häufig verwendet, um Textausschnitte auszuwählen, wenn Sie doppelklicken. Er ist das Nächstbeste, was es an einem universellen objektiven Wortzähler gibt.</p><p>Ich habe die <a href="https://pypi.org/project/polyglot/">polyglot-Bibliothek</a> in Python installiert, die diesen Textsegmentierer implementiert:</p><pre><code class="language-bash hljs">pip install -U polyglot
</code></pre><p>Um die Wortzahl eines Textes zu erhalten, können Sie Code wie dieses Snippet verwenden:</p><pre><code class="language-python hljs"><span class="hljs-keyword">from</span> polyglot.text <span class="hljs-keyword">import</span> Text

txt = <span class="hljs-string">"What is today's weather in Berlin?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(Text(txt).words))
</code></pre><p>Das Ergebnis sollte <code>7</code> sein.</p><p>Um eine Tokenzahl zu erhalten, wurden Segmente des Textes an die Tokenizer verschiedener Jina Embeddings Modelle übergeben, wie unten beschrieben, und jedes Mal habe ich zwei von der zurückgegebenen Tokenanzahl abgezogen.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text">Die hier aufgeführten Tokenzahlen <b><strong style="white-space: pre-wrap;">enthalten nicht</strong></b> die zusätzlichen zwei Tokens am Anfang und Ende jedes tokenisierten Textes.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" style="position: relative;"><a href="#english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" title="Englisch
(jina-embeddings-v2-small-en und jina-embeddings-v2-base-en)" id="anchor-english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Englisch<br>(<code>jina-embeddings-v2-small-en</code> und <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>)</h3><p>Zur Berechnung von Durchschnittswerten habe ich zwei englische Textkorpora von <a href="https://wortschatz.uni-leipzig.de/en" rel="noreferrer">Wortschatz Leipzig</a> heruntergeladen, einer Sammlung frei herunterladbarer Korpora in verschiedenen Sprachen und Konfigurationen, die von der Universität Leipzig gehostet wird:</p><ul><li>Ein Ein-Millionen-Satz-Korpus von Nachrichtendaten in Englisch aus dem Jahr 2020 (<code>eng_news_2020_1M</code>)</li><li>Ein Ein-Millionen-Satz-Korpus von <a href="https://en.wikipedia.org/">englischen Wikipedia</a>-Daten aus dem Jahr 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Beide sind auf <a href="https://wortschatz.uni-leipzig.de/en/download/English">ihrer englischen Download-Seite</a> zu finden.</p><p>Für mehr Vielfalt habe ich auch die <a href="https://www.gutenberg.org/ebooks/135">Hapgood-Übersetzung von Victor Hugos <em>Les Misérables</em></a> von Project Gutenberg und eine Kopie der King James Version der Bibel, übersetzt ins Englische 1611, heruntergeladen.</p><p>Für alle vier Texte habe ich die Wörter mit dem in <code>polyglot</code> implementierten Unicode-Segmentierer gezählt und dann die von <code>jina-embeddings-v2-small-en</code> erzeugten Tokens gezählt, wobei ich für jede Tokenisierungsanfrage zwei Tokens abgezogen habe. Die Ergebnisse sind wie folgt:</p>

<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Text</th><th id="|<X;" class="simple-table-header-color simple-table-header">Wortzahl<br>(Unicode Segmentierer)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Tokenzahl<br>(Jina Embeddings v2 <br>für Englisch)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Verhältnis Token zu Wörtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class=""><code>eng_news_2020_1M</code></td><td id="|<X;" class="">22.825.712</td><td id="GHal" class="">25.270.581</td><td id="h]mu" class="">1,107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class=""><code>eng_wikipedia_2016_1M</code></td><td id="|<X;" class="">24.243.607</td><td id="GHal" class="">26.813.877</td><td id="h]mu" class="">1,106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class=""><code>les_miserables_en</code></td><td id="|<X;" class="">688.911</td><td id="GHal" class="">764.121</td><td id="h]mu" class="">1,109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class=""><code>kjv_bible</code></td><td id="|<X;" class="">1.007.651</td><td id="GHal" class="">1.099.335</td><td id="h]mu" class="">1,091</td></tr></tbody></table>

<p>Die Verwendung präziser Zahlen bedeutet nicht, dass dies ein präzises Ergebnis ist. Dass Dokumente so unterschiedlicher Genres alle zwischen 9 % und 11 % mehr Tokens als Wörter aufweisen, deutet darauf hin, dass man wahrscheinlich etwa 10 % mehr Tokens als Wörter erwarten kann, gemessen am Unicode Segmenter. Textverarbeitungsprogramme zählen oft keine Satzzeichen, während der Unicode Segmenter dies tut, sodass die Wortzählungen aus Office-Software nicht unbedingt damit übereinstimmen müssen.</p><h3 id="german-jina-embeddings-v2-base-de" style="position: relative;"><a href="#german-jina-embeddings-v2-base-de" title="Deutsch
(jina-embeddings-v2-base-de)" id="anchor-german-jina-embeddings-v2-base-de"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Deutsch<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>Für Deutsch habe ich drei Korpora von der <a href="https://wortschatz.uni-leipzig.de/en/download/German">Wortschatz Leipzig Deutschen Seite</a> heruntergeladen:</p><ul><li><code>deu_mixed-typical_2011_1M</code> — Eine Million Sätze aus einer ausgewogenen Mischung von Texten verschiedener Genres aus dem Jahr 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> — Eine Million Sätze aus Nachrichtentexten von 2019.</li><li><code>deu_wikipedia_2021_1M</code> — Eine Million Sätze aus der deutschen Wikipedia von 2021.</li></ul><p>Und für die Vielfalt habe ich auch alle <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata">drei Bände von Karl Marx' <em>Kapital</em></a> vom <a href="https://www.deutschestextarchiv.de/" rel="noreferrer">Deutschen Textarchiv</a> heruntergeladen.</p><p>Dann bin ich nach dem gleichen Verfahren wie beim Englischen vorgegangen:</p>

<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Text</th><th id="|qln" class="simple-table-header-color simple-table-header">Wortzahl<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Token-Anzahl<br>(Jina Embeddings v2 <br>für Deutsch und Englisch)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Verhältnis von Tokens zu Wörtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class="">7.924.024</td><td id="YXZX" class="">9.772.652</td><td id="oEoQ" class="">1,234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class="">17.949.120</td><td id="YXZX" class="">21.711.555</td><td id="oEoQ" class="">1,210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class="">17.999.482</td><td id="YXZX" class="">22.654.901</td><td id="oEoQ" class="">1,259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class="" style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class="">784.336</td><td id="YXZX" class="">1.011.377</td><td id="oEoQ" class="">1,289</td></tr></tbody></table>

<p>Diese Ergebnisse haben eine größere Streuung als das rein englische Modell, deuten aber darauf hin, dass deutscher Text im Durchschnitt 20 % bis 30 % mehr Tokens als Wörter ergibt.</p><p>Englische Texte ergeben mit dem deutsch-englischen Tokenizer mehr Tokens als mit dem rein englischen:</p>

<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc<l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Text</th><th id="R@A^" class="simple-table-header-color simple-table-header">Wortzahl<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Token-Anzahl<br>(Jina Embeddings v2 <br>für Deutsch und Englisch)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Verhältnis von Tokens zu Wörtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class="">24.243.607</td><td id="UUfl" class="">27.758.535</td><td id="iTZS" class="">1,145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class="">22.825.712</td><td id="UUfl" class="">25.566.921</td><td id="iTZS" class="">1,120</td></tr></tbody></table>

<p>Sie sollten erwarten, 12 % bis 15 % mehr Tokens als Wörter zu benötigen, um englische Texte mit dem zweisprachigen Deutsch/Englisch-Modell im Vergleich zum rein englischen zu embedden.</p><h3 id="chinese-jina-embeddings-v2-base-zh" style="position: relative;"><a href="#chinese-jina-embeddings-v2-base-zh" title="Chinesisch
(jina-embeddings-v2-base-zh)" id="anchor-chinese-jina-embeddings-v2-base-zh"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Chinesisch<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Chinesisch wird typischerweise ohne Leerzeichen geschrieben und hatte bis zum 20. Jahrhundert keinen traditionellen Begriff von "Wörtern". Folglich wird die Größe eines chinesischen Textes typischerweise in Zeichen (<strong>字数</strong>) gemessen. Anstatt den Unicode Segmenter zu verwenden, habe ich daher die Länge chinesischer Texte gemessen, indem ich alle Leerzeichen entfernt und dann einfach die Zeichenlänge ermittelt habe.</p><p>Ich habe drei Korpora von der <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese">chinesischen Korpus-Seite von Wortschatz Leipzig</a> heruntergeladen:</p><ul><li><code>zho_wikipedia_2018_1M</code> — Eine Million Sätze aus der chinesischsprachigen Wikipedia, extrahiert 2018.</li><li><code>zho_news_2007-2009_1M</code> — Eine Million Sätze aus chinesischen Nachrichtenquellen, gesammelt von 2007 bis 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> — Eine Million Sätze aus Nachrichtenquellen, die ausschließlich traditionelle chinesische Zeichen (繁體字) verwenden.</li></ul><p>Zusätzlich habe ich für mehr Vielfalt auch <em>Die wahre Geschichte des Ah Q</em> (阿Q正傳) verwendet, eine Novelle von Lu Xun (魯迅) aus den frühen 1920er Jahren. Ich habe die <a href="https://www.gutenberg.org/ebooks/25332">traditionelle Zeichenversion von Project Gutenberg</a> heruntergeladen.</p>

<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Text</th><th id="CaUc" class="simple-table-header-color simple-table-header">Zeichenzahl<br>(字数)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Token-Anzahl<br>(Jina Embeddings v2 <br>für Chinesisch und Englisch)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Verhältnis von Tokens zu Zeichen<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class="">45.116.182</td><td id="CQ{d" class="">29.193.028</td><td id="_};C" class="">0,647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class="">44.295.314</td><td id="CQ{d" class="">28.108.090</td><td id="_};C" class="">0,635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class="" style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class="">54.585.819</td><td id="CQ{d" class="">40.290.982</td><td id="_};C" class="">0,738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class="" style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class="">41.268</td><td id="CQ{d" class="">25.346</td><td id="_};C" class="">0,614</td></tr></tbody></table>

<p>Diese Streuung der Token-zu-Zeichen-Verhältnisse ist unerwartet, und besonders der Ausreißer für das Korpus mit traditionellen Zeichen verdient weitere Untersuchung. Dennoch können wir schlussfolgern, dass man für Chinesisch <em>weniger</em> Token benötigt als es Zeichen im Text gibt. Je nach Inhalt kann man mit 25% bis 40% weniger rechnen.</p><p>Englische Texte in Jina Embeddings v2 für Chinesisch und Englisch ergaben ungefähr die gleiche Anzahl an Token wie im rein englischen Modell:</p>

<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Wortanzahl<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token-Anzahl<br>(Jina Embeddings v2 für Chinesisch und Englisch)<br></th><th id="<rlw" class="simple-table-header-color simple-table-header">Verhältnis Token zu Wörtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class="" style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class="" style="width:165.3125px">24.243.607</td><td id="=]?F" class="">26.890.176</td><td id="<rlw" class="">1,109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class="" style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class="" style="width:165.3125px">22.825.712</td><td id="=]?F" class="">25.060.352</td><td id="<rlw" class="">1,097</td></tr></tbody></table>

<h2 id="taking-tokens-seriously" style="position: relative;"><a href="#taking-tokens-seriously" title="Token ernst nehmen" id="anchor-taking-tokens-seriously"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Token ernst nehmen</h2><p>Token sind ein wichtiges Gerüst für KI-Sprachmodelle, und die Forschung auf diesem Gebiet ist noch im Gange.</p><p>Einer der Bereiche, in denen sich KI-Modelle als revolutionär erwiesen haben, ist die Entdeckung, dass sie sehr robust gegenüber verrauschten Daten sind. Selbst wenn ein bestimmtes Modell keine optimale Tokenisierungsstrategie verwendet, kann es bei ausreichender Netzwerkgröße, genügend Daten und angemessenem Training lernen, das Richtige aus unvollkommenen Eingaben zu machen.</p><p>Folglich wird weniger Aufwand in die Verbesserung der Tokenisierung gesteckt als in andere Bereiche, aber das könnte sich ändern.</p><p>Als Nutzer von Embeddings, die man über eine <a href="https://jina.ai/embeddings/">API wie Jina Embeddings</a> bezieht, kann man nicht genau wissen, wie viele Token man für eine bestimmte Aufgabe benötigt und muss möglicherweise eigene Tests durchführen, um verlässliche Zahlen zu erhalten. Aber die hier angegebenen Schätzungen – etwa 110% der Wortanzahl für Englisch, etwa 125% der Wortanzahl für Deutsch und etwa 70% der Zeichenanzahl für Chinesisch – sollten für eine grundlegende Budgetierung ausreichen.</p></section></article><div data-v-33ef2eff="" class="row justify-between items-center q-py-md"><div data-v-33ef2eff=""><span data-v-33ef2eff="" class="text-weight-bold">Kategorien:</span><span data-v-33ef2eff="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech-Blog</div></div></div></span></div><div data-v-33ef2eff=""><div data-v-33ef2eff="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-33ef2eff="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fde%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-33ef2eff="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fde%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-33ef2eff="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fde%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-33ef2eff="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fde%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-33ef2eff="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fde%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-33ef2eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-33ef2eff="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-33ef2eff="" class="text-h5 q-my-xl">Weiterlesen</div><a data-v-1f724e3b="" data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1f724e3b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-1f724e3b="" class="q-focus-helper"></span><div data-v-1f724e3b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-1f724e3b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__label q-item__label--caption text-caption">Dezember 17, 2024 • 12 Minuten gelesen</div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__section column q-item__section--main justify-center"><div data-v-1f724e3b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Text Embeddings Fail to Capture Word Order and How to Fix It</div></div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-1f724e3b="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Bo Wang"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Bo Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-1f724e3b="" class="col-4 overflow-hidden"><div data-v-1f724e3b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Three abstract figures in white, gray, and pink on matching cubes placed on a colorful checkered surface against a green back"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Three abstract figures in white, gray, and pink on matching cubes placed on a colorful checkered surface against a green back" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-1f724e3b="" data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/scaling-test-time-compute-for-embedding-models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1f724e3b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-1f724e3b="" class="q-focus-helper"></span><div data-v-1f724e3b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-1f724e3b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__label q-item__label--caption text-caption">Dezember 12, 2024 • 12 Minuten gelesen</div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__section column q-item__section--main justify-center"><div data-v-1f724e3b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Scaling Test-Time Compute For Embedding Models</div></div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-1f724e3b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div></div><div data-v-1f724e3b="" class="col-4 overflow-hidden"><div data-v-1f724e3b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="David Hockney artwork of a hand holding a rod with three colored spheres on a blue-toned background."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="David Hockney artwork of a hand holding a rod with three colored spheres on a blue-toned background." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-1f724e3b="" data-v-33ef2eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/still-need-chunking-when-long-context-models-can-do-it-all"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1f724e3b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-1f724e3b="" class="q-focus-helper"></span><div data-v-1f724e3b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-1f724e3b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__label q-item__label--caption text-caption">Dezember 04, 2024 • 13 Minuten gelesen</div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-1f724e3b="" class="q-item__section column q-item__section--main justify-center"><div data-v-1f724e3b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Still Need Chunking When Long-Context Models Can Do It All?</div></div></div><div data-v-1f724e3b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-1f724e3b="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Michael Günther"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Michael Günther" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-1f724e3b="" class="col-4 overflow-hidden"><div data-v-1f724e3b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Artistic pixel art of two seagulls on colored pipes with speech bubbles; one reads &quot;Too long?&quot; and the other shows math equat"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Artistic pixel art of two seagulls on colored pipes with speech bubbles; one reads &quot;Too long?&quot; and the other shows math equat" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div class="col-sm-12 col-md"><div class="q-list q-list--dark small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Büros</div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Sunnyvale, Kalifornien</div><div class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlin, Deutschland (Hauptsitz)</div><div class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Deutschland</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Peking, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">Ebene 5, Gebäude 6, Nr. 48 Haidian West St. Peking, China</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">402 Etage 4, Fu'an Technology Building, Shenzhen, China</div></div></div></div></div><div class="col-sm-12 col-md row"><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Stiftung durchsuchen</div><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Einbettungen</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Reranker</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Leser</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Klassifikator</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Segmentierer</div></a><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Holen Sie sich den Jina AI API-Schlüssel</div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Ratenbegrenzung</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-pa-none"><svg class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div class="q-item__section column q-item__section--main justify-center">API-Status</div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Unternehmen</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Über uns</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Kontaktieren Sie unseren Vertrieb</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Pressemitteilungen</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Praktikantenprogramm</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Begleiten Sie uns</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Logo herunterladen</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Bedingungen</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/COMMERCIAL-LICENSE-TERMS.pdf" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Teamlizenz</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#security"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sicherheit</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Terms &amp; amp; Bedingungen</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Privatsphäre</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Cookie-Einstellungen</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div class="row items-center justify-between q-gutter-x-sm col-12 col-md"><label class="q-field row no-wrap items-start q-field--outlined q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dense q-field--dark text-caption" for="f_36400599-e6be-4f4d-b51d-a2c92ac6641f"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__prepend q-field__marginal row no-wrap items-center"><i class="q-icon text-white notranslate material-symbols material-symbols-sharp q-px-sm q-py-none" aria-hidden="true" role="presentation" style="font-size: 18px;">language</i></div><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_36400599-e6be-4f4d-b51d-a2c92ac6641f" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_36400599-e6be-4f4d-b51d-a2c92ac6641f_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">arrow_drop_down</i></div></div></div></label><div class="text-caption text-dim"> Jina AI © 2020-2024. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
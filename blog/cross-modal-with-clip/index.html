<!DOCTYPE html><html lang="en"><head><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta property="og:title" content="Jina AI - a Neural Search Company"/><meta name="author" content="Jina AI"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="Jina AI"/><meta property="og:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta name="twitter:card" content="summary"/><meta property="twitter:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta property="twitter:title" content="Jina AI - a Neural Search Company"/><meta name="twitter:site" content="@JinaAI_"/><meta name="twitter:creator" content="@Jina AI"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-96x96.png"/><link rel="icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700"/><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:url" content="https://jina.ai/"/><meta property="og:title" content="Jina AI | Jina AI is a Neural Search Company"/><meta property="og:description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:locale" content="en"/><meta property="og:site_name" content="Jina AI, Ltd"/><link rel="canonical" href="https://jina.ai/"/><title>Using Jina to build a text-to-image search workflow based on OpenAI CLIP | Jina AI</title><meta name="next-head-count" content="29"/><link rel="preload" href="/_next/static/css/d6299e18a01a0ac2808b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d6299e18a01a0ac2808b.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-8f6366ea4b5fafd77e96.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework-94f262366e752bd48d82.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons-4667b98b62a60d7c050b.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-9a4182dc8e7736869bf9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-afcb035e0413b2214f0f.js" as="script"/><link rel="preload" href="/_next/static/chunks/596-13cdde2fb793b77184ee.js" as="script"/><link rel="preload" href="/_next/static/chunks/522-e45333ebcf36799b860c.js" as="script"/><link rel="preload" href="/_next/static/chunks/373-350caaa5387e56e8b807.js" as="script"/><link rel="preload" href="/_next/static/chunks/93-1a5bac9fdf2b2295fba2.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bslug%5D-0d67fc13dbd9e7ddf27f.js" as="script"/><style id="__jsx-3792957869">.dropdown.jsx-3792957869{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));padding:1.5rem;--tw-text-opacity:1;color:rgba(74,85,104,var(--tw-text-opacity));top:5rem;width:24rem;border-radius:0.75rem;}.dropdown.jsx-3792957869>.jsx-3792957869:not([hidden])~.jsx-3792957869:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse));border-style:solid;--tw-divide-opacity:1;border-color:rgba(235,235,235,var(--tw-divide-opacity));}.dropdown.jsx-3792957869{box-shadow:3px 6px 33px rgba(205,205,205,0.25);}</style><style id="__jsx-3128403005">.dropdown.jsx-3128403005{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(74,85,104,var(--tw-text-opacity));top:5rem;width:100%;border-radius:0.75rem;}.dropdown.jsx-3128403005>.jsx-3128403005:not([hidden])~.jsx-3128403005:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse));border-style:solid;--tw-divide-opacity:1;border-color:rgba(235,235,235,var(--tw-divide-opacity));}</style><style id="__jsx-3949856645">.btn.jsx-3949856645{text-align:center;}.btn-base.jsx-3949856645{font-size:1.125rem;font-weight:600;}.btn-xl.jsx-3949856645{padding-top:1rem;padding-bottom:1rem;padding-left:1.5rem;padding-right:1.5rem;font-size:1.25rem;font-weight:800;}.btn-full-rounded.jsx-3949856645{border-radius:9999px;}.btn-primary.jsx-3949856645{border-radius:9999px;--tw-bg-opacity:1;background-color:rgba(0,129,129,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(255,255,255,var(--tw-text-opacity));}.btn-primary.jsx-3949856645:hover{background-color:#009191;}.btn-secondary.jsx-3949856645{--tw-bg-opacity:1;background-color:rgba(251,203,103,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(0,0,0,var(--tw-text-opacity));}.btn-secondary.jsx-3949856645:hover{background-color:#fbd076;}.btn-tertiary.jsx-3949856645{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));border:1px solid #999999;}.btn-tertiary.jsx-3949856645:hover{background-color:#f6f6f6;}.btn-outlined.jsx-3949856645{border-width:2px;border-style:solid;--tw-border-opacity:1;border-color:rgba(255,255,255,var(--tw-border-opacity));background-color:transparent;}.btn-outlined.jsx-3949856645:hover{background-color:#f6f6f622;}</style><style id="__jsx-223001487">.navbar.jsx-223001487 a{display:inline-block;width:100%;}.navbar.jsx-223001487 li:not(:first-child){margin-top:0.75rem;}.navbar.jsx-223001487 a .btn{width:100%;}.navbar.jsx-223001487 a:hover{--tw-text-opacity:1;color:rgba(0,129,129,var(--tw-text-opacity));}@media (min-width:640px){.navbar.jsx-223001487 a,.navbar.jsx-223001487 a .btn{width:auto;}.navbar.jsx-223001487 li:not(:first-child){margin-top:0px;}.navbar.jsx-223001487>(li:not(:last-child)){margin-right:1.25rem;}}</style><style id="__jsx-649517993">.top-nav-bar.jsx-649517993{box-shadow:3px 6px 33px 0px #cdcdcd40;z-index:200;position:fixed;width:100vw;background:white;top:0;}</style><style id="__jsx-4055754880">.markdown.jsx-4055754880{font-size:1.125rem;line-height:1.625;}.markdown.jsx-4055754880 p.jsx-4055754880,.markdown.jsx-4055754880 ul.jsx-4055754880,.markdown.jsx-4055754880 ol.jsx-4055754880,.markdown.jsx-4055754880 blockquote.jsx-4055754880{margin-top:1.5rem;margin-bottom:1.5rem;}.markdown.jsx-4055754880 h2.jsx-4055754880{margin-top:3rem;margin-bottom:1rem;font-size:1.875rem;line-height:1.375;}.markdown.jsx-4055754880 h3.jsx-4055754880{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;line-height:1.375;}</style><style id="__jsx-1324436386">.footer-links.jsx-1324436386 li{margin-top:0.25rem;}.footer-items-title.jsx-1324436386{font-size:1.375rem;line-height:1.5rem;-webkit-letter-spacing:0.22px;-moz-letter-spacing:0.22px;-ms-letter-spacing:0.22px;letter-spacing:0.22px;}</style><style id="__jsx-1956813867">.footer-left-margin.jsx-1956813867{width:20rem;height:inherit;background:no-repeat 40% 20% / 30% url('/assets/images/planet-beige.svg');}.footer-right-margin.jsx-1956813867{width:16rem;height:inherit;background:no-repeat 10% 10% / 50% url('/assets/images/planet-green.svg');}</style><style data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700">@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrFJM.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJbecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJnecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJfecnFHGPc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><script async="" src="https://www.googletagmanager.com/gtag/js?id=undefined"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'undefined', {
              page_path: window.location.pathname,
            });
          </script><body><div id="__next"><div class="jsx-649517993 top-nav-bar"><div class="md:max-w-screen-md lg:max-w-screen-lg xl:max-w-screen-2xl mx-auto px-3 py-6 undefined"><div class="jsx-223001487 flex flex-wrap justify-between items-center"><div class="jsx-223001487"><a class="jsx-223001487" href="/"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo.svg" alt="Jina.ai logo" class="w-24"/></div></a></div><div class="jsx-223001487 sm:hidden"><button type="button" aria-label="show menu" class="jsx-223001487 p-3 text-gray-900 rounded-md hover:bg-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke-width="1.5" fill="none" stroke-linecap="round" stroke-linejoin="round" class="jsx-223001487 stroke-current h-6 w-6"><path d="M0 0h24v24H0z" stroke="none" class="jsx-223001487"></path><path d="M4 6h16M4 12h16M4 18h16" class="jsx-223001487"></path></svg></button></div><nav class="jsx-223001487 w-full sm:w-auto sm:block mt-2 sm:mt-0 hidden"><ul class="jsx-223001487 navbar flex flex-col sm:flex-row sm:items-center font-medium text-xl text-gray-800 pt-3 pb-5 px-5 sm:p-0 bg-white sm:bg-transparent rounded"><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Products dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Products</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/core" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/core-icon.svg" alt="core icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Jina Core</span></div><span class="jsx-3792957869 text-xs text-gray-500">An open-source framework for building multimedia search applications.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/hub" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/hub-icon.svg" alt="hub icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Jina Hub</span></div><span class="jsx-3792957869 text-xs text-gray-500">Jina’s community-driven module marketplace for building your Flow</span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Developers dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Developers</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/contribute" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/contribute-icon.svg" alt="contribute icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Contribute</span></div><span class="jsx-3792957869 text-xs text-gray-500">Want to shape the future of neural search? Find out how you can contribute to Jina here.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/join" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/join-icon.svg" alt="join icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Join</span></div><span class="jsx-3792957869 text-xs text-gray-500">Join the Jina Slack community to meet the team and get support.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/learn" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/learn-icon.svg" alt="learn icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Learn</span></div><span class="jsx-3792957869 text-xs text-gray-500">Docs, tutorials and examples for developers to get started with Jina. </span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Company dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Company</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/about" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/about-icon.svg" alt="about icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">About</span></div><span class="jsx-3792957869 text-xs text-gray-500">Learn more about Jina AI as a company.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/careers" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/careers-icon.svg" alt="careers icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Careers</span></div><span class="jsx-3792957869 text-xs text-gray-500">Interested in joining us? Find out what it&#x27;s like to work at Jina AI.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/events" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/events-icon.svg" alt="events icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Events</span></div><span class="jsx-3792957869 text-xs text-gray-500">Online and offline meetups, hackathons, workshops and webinars.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/news" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/news-icon.svg" alt="news icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">News</span></div><span class="jsx-3792957869 text-xs text-gray-500">Check out the latest news about our company and products.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/contact" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/contact-icon.svg" alt="contact icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Contact</span></div><span class="jsx-3792957869 text-xs text-gray-500">Interested in cooperating with Jina AI? Get in touch!</span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block mr-1 font-bold text-gray-700"><a href="/blog/">Blog</a></li><div class="jsx-3128403005 md:hidden h-96 overflow-y-scroll"><ul class="jsx-3128403005 dropdown"><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/core" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/core-icon.svg" alt="core icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Jina Core</span></div><span class="jsx-3128403005 text-xs text-gray-500">An open-source framework for building multimedia search applications.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/hub" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/hub-icon.svg" alt="hub icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Jina Hub</span></div><span class="jsx-3128403005 text-xs text-gray-500">Jina’s community-driven module marketplace for building your Flow</span></div></a></li></div><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/contribute" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/contribute-icon.svg" alt="contribute icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Contribute</span></div><span class="jsx-3128403005 text-xs text-gray-500">Want to shape the future of neural search? Find out how you can contribute to Jina here.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/join" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/join-icon.svg" alt="join icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Join</span></div><span class="jsx-3128403005 text-xs text-gray-500">Join the Jina Slack community to meet the team and get support.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/learn" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/learn-icon.svg" alt="learn icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Learn</span></div><span class="jsx-3128403005 text-xs text-gray-500">Docs, tutorials and examples for developers to get started with Jina. </span></div></a></li></div><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/about" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/about-icon.svg" alt="about icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">About</span></div><span class="jsx-3128403005 text-xs text-gray-500">Learn more about Jina AI as a company.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/careers" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/careers-icon.svg" alt="careers icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Careers</span></div><span class="jsx-3128403005 text-xs text-gray-500">Interested in joining us? Find out what it&#x27;s like to work at Jina AI.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/events" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/events-icon.svg" alt="events icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Events</span></div><span class="jsx-3128403005 text-xs text-gray-500">Online and offline meetups, hackathons, workshops and webinars.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/news" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/news-icon.svg" alt="news icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">News</span></div><span class="jsx-3128403005 text-xs text-gray-500">Check out the latest news about our company and products.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/contact" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/contact-icon.svg" alt="contact icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Contact</span></div><span class="jsx-3128403005 text-xs text-gray-500">Interested in cooperating with Jina AI? Get in touch!</span></div></a></li></div></ul><span class="jsx-3128403005 text-gray-700 font-semibold text-sm ml-2 p-4">Blog</span></div><li class="jsx-223001487"><div class="jsx-3949856645 btn cursor-pointer flex items-center justify-center px-6 py-2 md:hidden btn-base btn-primary"><div class="mr-4 w-7"><img src="/assets/images/icons/GitHub-white-icon.svg" alt="GitHub-white icon" class="w-full h-full"/></div><span class="jsx-223001487 mr-4">Try it out!</span></div></li></ul></nav><div class="jsx-3949856645 btn cursor-pointer flex items-center justify-center px-6 py-2 hidden md:flex btn-base btn-primary"><div class="mr-4 w-7"><img src="/assets/images/icons/GitHub-white-icon.svg" alt="GitHub-white icon" class="w-full h-full"/></div><span class="jsx-223001487 mr-4">Try it out!</span></div></div></div></div><div class="mt-24"><div class="min-h-screen"><main><div class="container mx-auto px-5 flex flex-col"><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png" alt="Using Jina to build a text-to-image search workflow based on OpenAI CLIP-image" class="mb-4 w-full"/><article class="mb-32 md:max-w-6xl md:shadow-lg xl:rounded-3xl md:px-32 md:py-20 md:mx-auto md:-mt-20 bg-white shadow"><div class="mb-3"></div><h1 class="text-xl md:text-4xl font-bold tracking-tighter leading-tight md:leading-none mb-6 md:text-left">Using Jina to build a text-to-image search workflow based on OpenAI CLIP</h1><div class="flex items-center justify-between text-gray-600 mb-12"><div class="flex items-center"><img src="/assets/images/defaultProfileImg.svg" class="w-8 h-auto rounded-full mr-4" alt="David Buchaca, Bo Wang, Joan Fontanals"/><div class="text-gray-600">David Buchaca, Bo Wang, Joan Fontanals</div></div><time dateTime="2021-03-25">25 March, 2021</time></div><div class="jsx-4055754880 mx-auto markdown"><div class="jsx-4055754880"><h2>Using Jina to build a text-to-image search workflow based on OpenAI CLIP</h2>
<p>If you are not familiar with cross-modal search in Jina please read <a href="https://jina.ai/2020/10/02/Cross-modal-Search-with-Jina.html">this wonderful blog post</a> which presents how you can use Jina and build a text-to-image search system based on VSE.</p>
<p>To start, we need a dataset that we will use to retrieve items.
First, you have to install <a href="https://pypi.org/project/kaggle/">Kaggle</a> in your machine, because we will use the following  <a href="https://www.kaggle.com/adityajn105/flickr8k">dataset</a>.
After logging to Kaggle and setting your Kaggle Token in your system (as described <a href="https://www.kaggle.com/docs/api">here</a>), run:</p>
<pre><code class="language-sh">kaggle datasets download adityajn105/flickr8k
unzip flickr8k.zip 
rm flickr8k.zip
mkdir data
mkdir data/f8k
mv Images data/f8k/images
mv captions.txt data/f8k/captions.txt
</code></pre>
<p>make sure that your data folder has the following structure:</p>
<pre><code class="language-sh">data/f8k/images/*jpg
data/f8k/captions.txt
</code></pre>
<p>After the data is downloaded we can add  it in the root folder of  <a href="https://github.com/jina-ai/examples/blob/bd9d1b2b5ae2a27432add18a2905506a77a9295c/cross-modal-search/">cross-modal-search</a> .
After setting the working directory we are set to index the data using the CLIP model:</p>
<pre><code class="language-sh">python app.py -t index -n 16000 -d f8k -m clip
</code></pre>
<p>Once we've indexed the data we can perform a search by first running:</p>
<pre><code class="language-sh">python app.py -t query
</code></pre>
<p>Then, in a browser use <a href="https://jina.ai/jinabox.js/">Jina Box</a> url, type anything in the search bar and wait to see the top-k results.</p>
<h2>What is this magic? Introduction to  CLIP</h2>
<p><a href="https://arxiv.org/pdf/2103.00020.pdf">CLIP</a>  is a neural network built to jointly train an image encoder and a text encoder.
The learning task for CLIP consist of predicting correct pairings between text descriptions and images in a batch. The following image summarizes the training of the model:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png" alt="flow"></p>
<p>After training, the model can be used to encode a piece of text or an image into a 512-dimensional vector.</p>
<p>Since both text and images are embedded into the same fix-sized space the embedded text can be used to retrieve images (and vice-versa).</p>
<p>Let <code>phi_t</code> and <code>phi_i</code> be the embeddings learned by CLIP to transform text and images respectively to vectors. In particular  <code>phi_t(x_text)</code> and <code>phi_t(x_image)</code> are 512 dimensional vectors.</p>
<p>Given a text query <code>q</code> and  <code>N</code> images <code>(x_1, ..., x_N)</code>  we can compute the dot products between <code>q</code> and the <code>N</code> images and store them in the vector <code>s</code>. That is, <code>s = phi_t(q) * phi_i(x_1),  phi_t(q) * phi_i(x_2), ..., phi_t(q) * phi_i(x_N) )</code>.</p>
<p>Then we can return the top <code>K</code>  elements  from  <code>x_1, ... , x_N</code> in a list <code>top_K</code>  using the similarity values in <code>s</code>, where the k'th value in <code>top_K</code> would correspond to the coordinate in  <code>s</code> with the k'th  highest score.</p>
<h6>Quality CLIP vs VSE++</h6>
<p>We can evaluate CLIP using  the mean reciprocal rank (MRR), which is a common metric to evaluate information retrieval systems.
Given a list of queries <code>query_list = (q_1,...,q_Q)</code> and a list (of lists) of retrieved items  <code>retrieved_list = ( r_1, ... ,  r_Q)</code>,  the mean reciprocal rank assigned to <code>(query_list, retrieved_list)</code> is defined as  the mean of the inverse of the ranks. That is:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/mean_reciprocal_rank.png" alt="flow"></p>
<p>where <code>rank(q_i, r_i)</code> refers to the position of the first relevant document in <code>r_i</code>.</p>
<p>The provided <a href="https://github.com/jina-ai/examples/tree/master/cross-modal-search">evaluate.py</a> can be used to compute the MRR of CLIP and VSE++. The results are:</p>
<pre><code>| Encoder | Modality      | Mean reciprocal Rank | Terminal command                                             |
| ------- | ------------- | -------------------- | ------------------------------------------------------------ |
| vse     | text to image | 0.3669               | python evaluate.py -e text2image -i 16000 -n 4000 -m vse -s 32 |
| clip    | text to image | 0.4018               | python evaluate.py -e text2image -i 16000 -n 4000 -m clip -s 32 |
| clip    | image to text | 0.4088               | python evaluate.py -e image2text -i 16000 -n 4000 -m clip -s 32 |
| vse     | image to text | 0.3791               | python evaluate.py -e image2text -i 16000 -n 4000 -m vse -s 32 |
</code></pre>
<h2>Looking inside our <code>app.py</code></h2>
<p>Now that we have seen how to use in our Jina application and the benefits of leveraging the CLIP encoder we can look at the code and the details of the implementation.</p>
<p>A Jina application will usually start by setting several environment variables that are needed in order to properly set up the provided <code>.yml</code> files which might use different environment variables such as <strong><code>JINA_PARALLEL</code></strong> or <strong><code>JINA_SHARDS</code></strong>.
The setting of these variables is done in the <strong><code>config()</code></strong>  function.</p>
<pre><code class="language-python">def config(model_name):
    os.environ['JINA_PARALLEL'] = os.environ.get('JINA_PARALLEL', '1')
    os.environ['JINA_SHARDS'] = os.environ.get('JINA_SHARDS', '1')
    os.environ['JINA_PORT'] = '45678'
    os.environ['JINA_USE_REST_API'] = 'true'
    if model_name == 'clip':
        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.clipimageencoder:0.0.1-1.0.7')
        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.cliptextencoder:0.0.1-1.0.7')
        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/clip/text-encoder.yml'
    elif model_name == 'vse':
        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.vseimageencoder:0.0.5-1.0.7')
        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.vsetextencoder:0.0.6-1.0.7')
        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/vse/text-encoder.yml'
</code></pre>
<p>This is paramount to start defining the <code>Flow</code>, because the <code>Flow</code> is created from
<code>flow-index.yml</code>  which already expects some of this environment variables already set.
For example we can see <strong><code>shards: $JINA_PARALLEL</code></strong> and <strong><code>uses: $JINA_USES_VSE_IMAGE_ENCODER</code></strong>.</p>
<pre><code class="language-bash">!head -18 flow-index.yml
</code></pre>
<pre><code class="language-yaml">!Flow
version: '1'
with:
  prefetch: 10
pods:
  - name: loader
    uses: yaml/image-load.yml
    shards: $JINA_PARALLEL
    read_only: true
  - name: normalizer
    uses: yaml/image-normalize.yml
    shards: $JINA_PARALLEL
    read_only: true
  - name: image_encoder
    uses: $JINA_USES_VSE_IMAGE_ENCODER
    shards: $JINA_PARALLEL
    timeout_ready: 600000
    read_only: true
</code></pre>
<p>Once <strong><code>config</code></strong> is called we can see that the application selects either <code>index</code> or <code>query</code> mode.</p>
<p>Note that config sets some global variables with Docker, such as <code>'docker://clip_text_encoder'</code> .
This is needed becasue this encoder is not part of the core of Jina  and we need to specify how to find the docker for a specific part of the <code>Flow</code> that we will construct.</p>
<p>Note that the <code>Pod</code> named <strong><code>loader</code></strong>, defined in the file <strong><code>image-load.yml</code></strong> does not need any docker information.
This file starts with <strong><code>!ImageReader</code></strong>  which is a crafter already provided in Jina's core.</p>
<h3>Index mode</h3>
<p>If <code>index</code> is selected a <code>Flow</code> is created with</p>
<pre><code class="language-python">f = Flow().load_config('flow-index.yml')
</code></pre>
<p>And the <code>Flow</code> can start indexing with <strong><code>f.index</code></strong></p>
<pre><code class="language-python">with f:
    f.index(input_fn=input_index_data(num_docs, request_size, data_set), 
            request_size=request_size)
</code></pre>
<p>Here <strong><code>f.index</code></strong> receives a generator <strong><code>input_index_data</code></strong> that reads the input data and creates <strong><code>jina.Document</code></strong> objects with the images and the captions.</p>
<p>We can plot a <code>Flow</code> using the <strong><code>f.plot(image_path)</code></strong> function. The output of</p>
<pre><code class="language-python">f.plot('flow_diagram_index.svg')
</code></pre>
<p>is the following plot:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_index.png" alt="flow"></p>
<p>In the diagram, each blue node corresponds to one of <code>Pods</code> defined in <strong><code>flow-index.yml</code></strong>.
The string inside each node corresponds to the atrribute <strong><code>name</code></strong>  assigned to  each <code>Pod</code> in  <strong><code>flow-index.yml</code></strong>.</p>
<p>Note that there are two main branches: the middle path transforms images to vectors while the bottom branch transforms text to vectors.
Both branches transform data to the same vector space.</p>
<h3>Query mode</h3>
<p>If  query mode is selected we can see that another <code>Flow</code> is created from <strong><code>flow_query.yml</code></strong></p>
<pre><code class="language-python">f = Flow().load_config('flow-query.yml')
f.use_rest_gateway()
with f:
    f.block()
</code></pre>
<p>as before, we can visually inspect the query <code>Flow</code> using <strong><code>f.plot('flow_diagram_index.svg')</code></strong> which will provide the following diagram:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_query.png" alt="flow"></p>
</div><div class="jsx-4055754880 hidden"><h2>Using Jina to build a text-to-image search workflow based on OpenAI CLIP</h2>
<p>If you are not familiar with cross-modal search in Jina please read <a href="https://jina.ai/2020/10/02/Cross-modal-Search-with-Jina.html">this wonderful blog post</a> which presents how you can use Jina and build a text-to-image search system based on VSE.</p>
<p>To start, we need a dataset that we will use to retrieve items.
First, you have to install <a href="https://pypi.org/project/kaggle/">Kaggle</a> in your machine, because we will use the following  <a href="https://www.kaggle.com/adityajn105/flickr8k">dataset</a>.
After logging to Kaggle and setting your Kaggle Token in your system (as described <a href="https://www.kaggle.com/docs/api">here</a>), run:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>kaggle datasets download adityajn105/flickr8k
</span><span>unzip flickr8k.</span><span style="color:#fb9e00">zip</span><span> 
</span><span>rm flickr8k.</span><span style="color:#fb9e00">zip</span><span>
</span>mkdir data
<!-- -->mkdir data/f8k
<!-- -->mv Images data/f8k/images
<!-- -->mv captions.txt data/f8k/captions.txt
<!-- -->
</code></pre>
<p>make sure that your data folder has the following structure:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>data/f8k/images/*jpg
</span>data/f8k/captions.txt
<!-- -->
</code></pre>
<p>After the data is downloaded we can add  it in the root folder of  <a href="https://github.com/jina-ai/examples/blob/bd9d1b2b5ae2a27432add18a2905506a77a9295c/cross-modal-search/">cross-modal-search</a> .
After setting the working directory we are set to index the data using the CLIP model:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>python app.py -t index -n </span><span style="color:#fa658d">16000</span><span> -d f8k -m clip
</span>
</code></pre>
<p>Once we've indexed the data we can perform a search by first running:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>python app.py -t query
</span>
</code></pre>
<p>Then, in a browser use <a href="https://jina.ai/jinabox.js/">Jina Box</a> url, type anything in the search bar and wait to see the top-k results.</p>
<h2>What is this magic? Introduction to  CLIP</h2>
<p><a href="https://arxiv.org/pdf/2103.00020.pdf">CLIP</a>  is a neural network built to jointly train an image encoder and a text encoder.
The learning task for CLIP consist of predicting correct pairings between text descriptions and images in a batch. The following image summarizes the training of the model:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png" alt="flow"></p>
<p>After training, the model can be used to encode a piece of text or an image into a 512-dimensional vector.</p>
<p>Since both text and images are embedded into the same fix-sized space the embedded text can be used to retrieve images (and vice-versa).</p>
<p>Let <code>phi_t</code> and <code>phi_i</code> be the embeddings learned by CLIP to transform text and images respectively to vectors. In particular  <code>phi_t(x_text)</code> and <code>phi_t(x_image)</code> are 512 dimensional vectors.</p>
<p>Given a text query <code>q</code> and  <code>N</code> images <code>(x_1, ..., x_N)</code>  we can compute the dot products between <code>q</code> and the <code>N</code> images and store them in the vector <code>s</code>. That is, <code>s = phi_t(q) * phi_i(x_1),  phi_t(q) * phi_i(x_2), ..., phi_t(q) * phi_i(x_N) )</code>.</p>
<p>Then we can return the top <code>K</code>  elements  from  <code>x_1, ... , x_N</code> in a list <code>top_K</code>  using the similarity values in <code>s</code>, where the k'th value in <code>top_K</code> would correspond to the coordinate in  <code>s</code> with the k'th  highest score.</p>
<h6>Quality CLIP vs VSE++</h6>
<p>We can evaluate CLIP using  the mean reciprocal rank (MRR), which is a common metric to evaluate information retrieval systems.
Given a list of queries <code>query_list = (q_1,...,q_Q)</code> and a list (of lists) of retrieved items  <code>retrieved_list = ( r_1, ... ,  r_Q)</code>,  the mean reciprocal rank assigned to <code>(query_list, retrieved_list)</code> is defined as  the mean of the inverse of the ranks. That is:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/mean_reciprocal_rank.png" alt="flow"></p>
<p>where <code>rank(q_i, r_i)</code> refers to the position of the first relevant document in <code>r_i</code>.</p>
<p>The provided <a href="https://github.com/jina-ai/examples/tree/master/cross-modal-search">evaluate.py</a> can be used to compute the MRR of CLIP and VSE++. The results are:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>| Encoder | Modality      | Mean reciprocal Rank | Terminal command                                             |
</span>| ------- | ------------- | -------------------- | ------------------------------------------------------------ |
<span>| vse     | text to image | </span><span style="color:#fa658d">0.3669</span><span>               | python evaluate.py -e text2image -i </span><span style="color:#fa658d">16000</span><span> -n </span><span style="color:#fa658d">4000</span><span> -m vse -s </span><span style="color:#fa658d">32</span><span> |
</span><span>| clip    | text to image | </span><span style="color:#fa658d">0.4018</span><span>               | python evaluate.py -e text2image -i </span><span style="color:#fa658d">16000</span><span> -n </span><span style="color:#fa658d">4000</span><span> -m clip -s </span><span style="color:#fa658d">32</span><span> |
</span><span>| clip    | image to text | </span><span style="color:#fa658d">0.4088</span><span>               | python evaluate.py -e image2text -i </span><span style="color:#fa658d">16000</span><span> -n </span><span style="color:#fa658d">4000</span><span> -m clip -s </span><span style="color:#fa658d">32</span><span> |
</span><span>| vse     | image to text | </span><span style="color:#fa658d">0.3791</span><span>               | python evaluate.py -e image2text -i </span><span style="color:#fa658d">16000</span><span> -n </span><span style="color:#fa658d">4000</span><span> -m vse -s </span><span style="color:#fa658d">32</span><span> |
</span>
</code></pre>
<h2>Looking inside our <code>app.py</code></h2>
<p>Now that we have seen how to use in our Jina application and the benefits of leveraging the CLIP encoder we can look at the code and the details of the implementation.</p>
<p>A Jina application will usually start by setting several environment variables that are needed in order to properly set up the provided <code>.yml</code> files which might use different environment variables such as <strong><code>JINA_PARALLEL</code></strong> or <strong><code>JINA_SHARDS</code></strong>.
The setting of these variables is done in the <strong><code>config()</code></strong>  function.</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span class="hljs-function" style="color:#fb9e00;font-weight:normal">def</span><span class="hljs-function"> </span><span class="hljs-function" style="color:#fad000;font-weight:normal">config</span><span class="hljs-function">(</span><span class="hljs-function hljs-params">model_name</span><span class="hljs-function">):</span><span>
</span><span>    os.environ[</span><span style="color:#4cd213">&#x27;JINA_PARALLEL&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_PARALLEL&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;1&#x27;</span><span>)
</span><span>    os.environ[</span><span style="color:#4cd213">&#x27;JINA_SHARDS&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_SHARDS&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;1&#x27;</span><span>)
</span><span>    os.environ[</span><span style="color:#4cd213">&#x27;JINA_PORT&#x27;</span><span>] = </span><span style="color:#4cd213">&#x27;45678&#x27;</span><span>
</span><span>    os.environ[</span><span style="color:#4cd213">&#x27;JINA_USE_REST_API&#x27;</span><span>] = </span><span style="color:#4cd213">&#x27;true&#x27;</span><span>
</span><span>    </span><span style="color:#fb9e00;font-weight:normal">if</span><span> model_name == </span><span style="color:#4cd213">&#x27;clip&#x27;</span><span>:
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_IMAGE_ENCODER&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_IMAGE_ENCODER&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;docker://jinahub/pod.encoder.clipimageencoder:0.0.1-1.0.7&#x27;</span><span>)
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;docker://jinahub/pod.encoder.cliptextencoder:0.0.1-1.0.7&#x27;</span><span>)
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER_INTERNAL&#x27;</span><span>] = </span><span style="color:#4cd213">&#x27;yaml/clip/text-encoder.yml&#x27;</span><span>
</span><span>    </span><span style="color:#fb9e00;font-weight:normal">elif</span><span> model_name == </span><span style="color:#4cd213">&#x27;vse&#x27;</span><span>:
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_IMAGE_ENCODER&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_IMAGE_ENCODER&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;docker://jinahub/pod.encoder.vseimageencoder:0.0.5-1.0.7&#x27;</span><span>)
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER&#x27;</span><span>] = os.environ.get(</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER&#x27;</span><span>, </span><span style="color:#4cd213">&#x27;docker://jinahub/pod.encoder.vsetextencoder:0.0.6-1.0.7&#x27;</span><span>)
</span><span>        os.environ[</span><span style="color:#4cd213">&#x27;JINA_TEXT_ENCODER_INTERNAL&#x27;</span><span>] = </span><span style="color:#4cd213">&#x27;yaml/vse/text-encoder.yml&#x27;</span><span>
</span>
</code></pre>
<p>This is paramount to start defining the <code>Flow</code>, because the <code>Flow</code> is created from
<code>flow-index.yml</code>  which already expects some of this environment variables already set.
For example we can see <strong><code>shards: $JINA_PARALLEL</code></strong> and <strong><code>uses: $JINA_USES_VSE_IMAGE_ENCODER</code></strong>.</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>!head -</span><span style="color:#fa658d">18</span><span> flow-index.yml
</span>
</code></pre>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>!Flow
</span><span>version: </span><span style="color:#4cd213">&#x27;1&#x27;</span><span>
</span><span></span><span style="color:#fb9e00;font-weight:normal">with</span><span>:
</span><span>  prefetch: </span><span style="color:#fa658d">10</span><span>
</span>pods:
<!-- -->  - name: loader
<!-- -->    uses: yaml/image-load.yml
<!-- -->    shards: $JINA_PARALLEL
<!-- -->    read_only: true
<!-- -->  - name: normalizer
<!-- -->    uses: yaml/image-normalize.yml
<!-- -->    shards: $JINA_PARALLEL
<!-- -->    read_only: true
<!-- -->  - name: image_encoder
<!-- -->    uses: $JINA_USES_VSE_IMAGE_ENCODER
<!-- -->    shards: $JINA_PARALLEL
<span>    timeout_ready: </span><span style="color:#fa658d">600000</span><span>
</span>    read_only: true
<!-- -->
</code></pre>
<p>Once <strong><code>config</code></strong> is called we can see that the application selects either <code>index</code> or <code>query</code> mode.</p>
<p>Note that config sets some global variables with Docker, such as <code>'docker://clip_text_encoder'</code> .
This is needed becasue this encoder is not part of the core of Jina  and we need to specify how to find the docker for a specific part of the <code>Flow</code> that we will construct.</p>
<p>Note that the <code>Pod</code> named <strong><code>loader</code></strong>, defined in the file <strong><code>image-load.yml</code></strong> does not need any docker information.
This file starts with <strong><code>!ImageReader</code></strong>  which is a crafter already provided in Jina's core.</p>
<h3>Index mode</h3>
<p>If <code>index</code> is selected a <code>Flow</code> is created with</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>f = Flow().load_config(</span><span style="color:#4cd213">&#x27;flow-index.yml&#x27;</span><span>)
</span>
</code></pre>
<p>And the <code>Flow</code> can start indexing with <strong><code>f.index</code></strong></p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span style="color:#fb9e00;font-weight:normal">with</span><span> f:
</span>    f.index(input_fn=input_index_data(num_docs, request_size, data_set), 
<!-- -->            request_size=request_size)
<!-- -->
</code></pre>
<p>Here <strong><code>f.index</code></strong> receives a generator <strong><code>input_index_data</code></strong> that reads the input data and creates <strong><code>jina.Document</code></strong> objects with the images and the captions.</p>
<p>We can plot a <code>Flow</code> using the <strong><code>f.plot(image_path)</code></strong> function. The output of</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>f.plot(</span><span style="color:#4cd213">&#x27;flow_diagram_index.svg&#x27;</span><span>)
</span>
</code></pre>
<p>is the following plot:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_index.png" alt="flow"></p>
<p>In the diagram, each blue node corresponds to one of <code>Pods</code> defined in <strong><code>flow-index.yml</code></strong>.
The string inside each node corresponds to the atrribute <strong><code>name</code></strong>  assigned to  each <code>Pod</code> in  <strong><code>flow-index.yml</code></strong>.</p>
<p>Note that there are two main branches: the middle path transforms images to vectors while the bottom branch transforms text to vectors.
Both branches transform data to the same vector space.</p>
<h3>Query mode</h3>
<p>If  query mode is selected we can see that another <code>Flow</code> is created from <strong><code>flow_query.yml</code></strong></p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>f = Flow().load_config(</span><span style="color:#4cd213">&#x27;flow-query.yml&#x27;</span><span>)
</span>f.use_rest_gateway()
<span></span><span style="color:#fb9e00;font-weight:normal">with</span><span> f:
</span>    f.block()
<!-- -->
</code></pre>
<p>as before, we can visually inspect the query <code>Flow</code> using <strong><code>f.plot('flow_diagram_index.svg')</code></strong> which will provide the following diagram:</p>
<p><img src="/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_query.png" alt="flow"></p>
</div></div></article></div></main></div></div><div class="jsx-1956813867"><div class="bg-primary-500 pt-20 text-white"><div class="jsx-1956813867 flex flex-col md:flex-row justify-between px-6 md:px-0"><div class="jsx-1956813867 footer-left-margin"></div><div class="jsx-1956813867 pr-4 md:py-16 md:mr-16"><div class="text-center md:text-left"><div class="flex md:justify-start"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo-white.svg" alt="Jina.ai logo" class="w-24"/></div></div></div></div><div class="jsx-1956813867 grid grid-cols-2 sm:grid-cols-2 md:grid-cols-4 gap-16 py-16"><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Company</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/about/">About us</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/careers/">Careers</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/contact/">Contact us</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/blog/">Blog</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Products</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/core/">Core</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/hub/">Hub</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Developers</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/contribute/">Contribute</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/learn/">Learn</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/join/">Join</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Social</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://github.com/jina-ai/jina/">GitHub</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://www.linkedin.com/company/jinaai/">LinkedIn</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://twitter.com/jinaAI_/">Twitter</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://jina-ai.slack.com/">Slack</a></li></ul></nav></div></div><div class="jsx-1956813867 footer-right-margin"></div></div><div class="text-center bg-primary-400 text-gray-100 text-sm mt-12 py-6"><span class="block md:inline-block">© 2021 Jina AI, Ltd. All rights reserved.</span><span class="mx-4 hidden md:inline-block">|</span><a href="/legal"><span>Terms of Service</span></a><span class="mx-4">|</span><a href="/legal"><span>Privacy Policy</span></a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Using Jina to build a text-to-image search workflow based on OpenAI CLIP","date":"2021-03-25","slug":"cross-modal-with-clip","author":"David Buchaca, Bo Wang, Joan Fontanals","content":"\u003ch2\u003eUsing Jina to build a text-to-image search workflow based on OpenAI CLIP\u003c/h2\u003e\n\u003cp\u003eIf you are not familiar with cross-modal search in Jina please read \u003ca href=\"https://jina.ai/2020/10/02/Cross-modal-Search-with-Jina.html\"\u003ethis wonderful blog post\u003c/a\u003e which presents how you can use Jina and build a text-to-image search system based on VSE.\u003c/p\u003e\n\u003cp\u003eTo start, we need a dataset that we will use to retrieve items.\nFirst, you have to install \u003ca href=\"https://pypi.org/project/kaggle/\"\u003eKaggle\u003c/a\u003e in your machine, because we will use the following  \u003ca href=\"https://www.kaggle.com/adityajn105/flickr8k\"\u003edataset\u003c/a\u003e.\nAfter logging to Kaggle and setting your Kaggle Token in your system (as described \u003ca href=\"https://www.kaggle.com/docs/api\"\u003ehere\u003c/a\u003e), run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003ekaggle datasets download adityajn105/flickr8k\nunzip flickr8k.zip \nrm flickr8k.zip\nmkdir data\nmkdir data/f8k\nmv Images data/f8k/images\nmv captions.txt data/f8k/captions.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003emake sure that your data folder has the following structure:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003edata/f8k/images/*jpg\ndata/f8k/captions.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter the data is downloaded we can add  it in the root folder of  \u003ca href=\"https://github.com/jina-ai/examples/blob/bd9d1b2b5ae2a27432add18a2905506a77a9295c/cross-modal-search/\"\u003ecross-modal-search\u003c/a\u003e .\nAfter setting the working directory we are set to index the data using the CLIP model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython app.py -t index -n 16000 -d f8k -m clip\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce we've indexed the data we can perform a search by first running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython app.py -t query\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, in a browser use \u003ca href=\"https://jina.ai/jinabox.js/\"\u003eJina Box\u003c/a\u003e url, type anything in the search bar and wait to see the top-k results.\u003c/p\u003e\n\u003ch2\u003eWhat is this magic? Introduction to  CLIP\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2103.00020.pdf\"\u003eCLIP\u003c/a\u003e  is a neural network built to jointly train an image encoder and a text encoder.\nThe learning task for CLIP consist of predicting correct pairings between text descriptions and images in a batch. The following image summarizes the training of the model:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png\" alt=\"flow\"\u003e\u003c/p\u003e\n\u003cp\u003eAfter training, the model can be used to encode a piece of text or an image into a 512-dimensional vector.\u003c/p\u003e\n\u003cp\u003eSince both text and images are embedded into the same fix-sized space the embedded text can be used to retrieve images (and vice-versa).\u003c/p\u003e\n\u003cp\u003eLet \u003ccode\u003ephi_t\u003c/code\u003e and \u003ccode\u003ephi_i\u003c/code\u003e be the embeddings learned by CLIP to transform text and images respectively to vectors. In particular  \u003ccode\u003ephi_t(x_text)\u003c/code\u003e and \u003ccode\u003ephi_t(x_image)\u003c/code\u003e are 512 dimensional vectors.\u003c/p\u003e\n\u003cp\u003eGiven a text query \u003ccode\u003eq\u003c/code\u003e and  \u003ccode\u003eN\u003c/code\u003e images \u003ccode\u003e(x_1, ..., x_N)\u003c/code\u003e  we can compute the dot products between \u003ccode\u003eq\u003c/code\u003e and the \u003ccode\u003eN\u003c/code\u003e images and store them in the vector \u003ccode\u003es\u003c/code\u003e. That is, \u003ccode\u003es = phi_t(q) * phi_i(x_1),  phi_t(q) * phi_i(x_2), ..., phi_t(q) * phi_i(x_N) )\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThen we can return the top \u003ccode\u003eK\u003c/code\u003e  elements  from  \u003ccode\u003ex_1, ... , x_N\u003c/code\u003e in a list \u003ccode\u003etop_K\u003c/code\u003e  using the similarity values in \u003ccode\u003es\u003c/code\u003e, where the k'th value in \u003ccode\u003etop_K\u003c/code\u003e would correspond to the coordinate in  \u003ccode\u003es\u003c/code\u003e with the k'th  highest score.\u003c/p\u003e\n\u003ch6\u003eQuality CLIP vs VSE++\u003c/h6\u003e\n\u003cp\u003eWe can evaluate CLIP using  the mean reciprocal rank (MRR), which is a common metric to evaluate information retrieval systems.\nGiven a list of queries \u003ccode\u003equery_list = (q_1,...,q_Q)\u003c/code\u003e and a list (of lists) of retrieved items  \u003ccode\u003eretrieved_list = ( r_1, ... ,  r_Q)\u003c/code\u003e,  the mean reciprocal rank assigned to \u003ccode\u003e(query_list, retrieved_list)\u003c/code\u003e is defined as  the mean of the inverse of the ranks. That is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/mean_reciprocal_rank.png\" alt=\"flow\"\u003e\u003c/p\u003e\n\u003cp\u003ewhere \u003ccode\u003erank(q_i, r_i)\u003c/code\u003e refers to the position of the first relevant document in \u003ccode\u003er_i\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe provided \u003ca href=\"https://github.com/jina-ai/examples/tree/master/cross-modal-search\"\u003eevaluate.py\u003c/a\u003e can be used to compute the MRR of CLIP and VSE++. The results are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e| Encoder | Modality      | Mean reciprocal Rank | Terminal command                                             |\n| ------- | ------------- | -------------------- | ------------------------------------------------------------ |\n| vse     | text to image | 0.3669               | python evaluate.py -e text2image -i 16000 -n 4000 -m vse -s 32 |\n| clip    | text to image | 0.4018               | python evaluate.py -e text2image -i 16000 -n 4000 -m clip -s 32 |\n| clip    | image to text | 0.4088               | python evaluate.py -e image2text -i 16000 -n 4000 -m clip -s 32 |\n| vse     | image to text | 0.3791               | python evaluate.py -e image2text -i 16000 -n 4000 -m vse -s 32 |\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLooking inside our \u003ccode\u003eapp.py\u003c/code\u003e\u003c/h2\u003e\n\u003cp\u003eNow that we have seen how to use in our Jina application and the benefits of leveraging the CLIP encoder we can look at the code and the details of the implementation.\u003c/p\u003e\n\u003cp\u003eA Jina application will usually start by setting several environment variables that are needed in order to properly set up the provided \u003ccode\u003e.yml\u003c/code\u003e files which might use different environment variables such as \u003cstrong\u003e\u003ccode\u003eJINA_PARALLEL\u003c/code\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003ccode\u003eJINA_SHARDS\u003c/code\u003e\u003c/strong\u003e.\nThe setting of these variables is done in the \u003cstrong\u003e\u003ccode\u003econfig()\u003c/code\u003e\u003c/strong\u003e  function.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef config(model_name):\n    os.environ['JINA_PARALLEL'] = os.environ.get('JINA_PARALLEL', '1')\n    os.environ['JINA_SHARDS'] = os.environ.get('JINA_SHARDS', '1')\n    os.environ['JINA_PORT'] = '45678'\n    os.environ['JINA_USE_REST_API'] = 'true'\n    if model_name == 'clip':\n        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.clipimageencoder:0.0.1-1.0.7')\n        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.cliptextencoder:0.0.1-1.0.7')\n        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/clip/text-encoder.yml'\n    elif model_name == 'vse':\n        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.vseimageencoder:0.0.5-1.0.7')\n        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.vsetextencoder:0.0.6-1.0.7')\n        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/vse/text-encoder.yml'\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is paramount to start defining the \u003ccode\u003eFlow\u003c/code\u003e, because the \u003ccode\u003eFlow\u003c/code\u003e is created from\n\u003ccode\u003eflow-index.yml\u003c/code\u003e  which already expects some of this environment variables already set.\nFor example we can see \u003cstrong\u003e\u003ccode\u003eshards: $JINA_PARALLEL\u003c/code\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003ccode\u003euses: $JINA_USES_VSE_IMAGE_ENCODER\u003c/code\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e!head -18 flow-index.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e!Flow\nversion: '1'\nwith:\n  prefetch: 10\npods:\n  - name: loader\n    uses: yaml/image-load.yml\n    shards: $JINA_PARALLEL\n    read_only: true\n  - name: normalizer\n    uses: yaml/image-normalize.yml\n    shards: $JINA_PARALLEL\n    read_only: true\n  - name: image_encoder\n    uses: $JINA_USES_VSE_IMAGE_ENCODER\n    shards: $JINA_PARALLEL\n    timeout_ready: 600000\n    read_only: true\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce \u003cstrong\u003e\u003ccode\u003econfig\u003c/code\u003e\u003c/strong\u003e is called we can see that the application selects either \u003ccode\u003eindex\u003c/code\u003e or \u003ccode\u003equery\u003c/code\u003e mode.\u003c/p\u003e\n\u003cp\u003eNote that config sets some global variables with Docker, such as \u003ccode\u003e'docker://clip_text_encoder'\u003c/code\u003e .\nThis is needed becasue this encoder is not part of the core of Jina  and we need to specify how to find the docker for a specific part of the \u003ccode\u003eFlow\u003c/code\u003e that we will construct.\u003c/p\u003e\n\u003cp\u003eNote that the \u003ccode\u003ePod\u003c/code\u003e named \u003cstrong\u003e\u003ccode\u003eloader\u003c/code\u003e\u003c/strong\u003e, defined in the file \u003cstrong\u003e\u003ccode\u003eimage-load.yml\u003c/code\u003e\u003c/strong\u003e does not need any docker information.\nThis file starts with \u003cstrong\u003e\u003ccode\u003e!ImageReader\u003c/code\u003e\u003c/strong\u003e  which is a crafter already provided in Jina's core.\u003c/p\u003e\n\u003ch3\u003eIndex mode\u003c/h3\u003e\n\u003cp\u003eIf \u003ccode\u003eindex\u003c/code\u003e is selected a \u003ccode\u003eFlow\u003c/code\u003e is created with\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ef = Flow().load_config('flow-index.yml')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd the \u003ccode\u003eFlow\u003c/code\u003e can start indexing with \u003cstrong\u003e\u003ccode\u003ef.index\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ewith f:\n    f.index(input_fn=input_index_data(num_docs, request_size, data_set), \n            request_size=request_size)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere \u003cstrong\u003e\u003ccode\u003ef.index\u003c/code\u003e\u003c/strong\u003e receives a generator \u003cstrong\u003e\u003ccode\u003einput_index_data\u003c/code\u003e\u003c/strong\u003e that reads the input data and creates \u003cstrong\u003e\u003ccode\u003ejina.Document\u003c/code\u003e\u003c/strong\u003e objects with the images and the captions.\u003c/p\u003e\n\u003cp\u003eWe can plot a \u003ccode\u003eFlow\u003c/code\u003e using the \u003cstrong\u003e\u003ccode\u003ef.plot(image_path)\u003c/code\u003e\u003c/strong\u003e function. The output of\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ef.plot('flow_diagram_index.svg')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eis the following plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_index.png\" alt=\"flow\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the diagram, each blue node corresponds to one of \u003ccode\u003ePods\u003c/code\u003e defined in \u003cstrong\u003e\u003ccode\u003eflow-index.yml\u003c/code\u003e\u003c/strong\u003e.\nThe string inside each node corresponds to the atrribute \u003cstrong\u003e\u003ccode\u003ename\u003c/code\u003e\u003c/strong\u003e  assigned to  each \u003ccode\u003ePod\u003c/code\u003e in  \u003cstrong\u003e\u003ccode\u003eflow-index.yml\u003c/code\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNote that there are two main branches: the middle path transforms images to vectors while the bottom branch transforms text to vectors.\nBoth branches transform data to the same vector space.\u003c/p\u003e\n\u003ch3\u003eQuery mode\u003c/h3\u003e\n\u003cp\u003eIf  query mode is selected we can see that another \u003ccode\u003eFlow\u003c/code\u003e is created from \u003cstrong\u003e\u003ccode\u003eflow_query.yml\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003ef = Flow().load_config('flow-query.yml')\nf.use_rest_gateway()\nwith f:\n    f.block()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eas before, we can visually inspect the query \u003ccode\u003eFlow\u003c/code\u003e using \u003cstrong\u003e\u003ccode\u003ef.plot('flow_diagram_index.svg')\u003c/code\u003e\u003c/strong\u003e which will provide the following diagram:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_query.png\" alt=\"flow\"\u003e\u003c/p\u003e\n","coverImage":"/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"cross-modal-with-clip"},"buildId":"KJvUyrCoh0MXs5cG--Qn6","runtimeConfig":{},"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-8683bd742a84c1edd48c.js"></script><script src="/_next/static/chunks/webpack-8f6366ea4b5fafd77e96.js" async=""></script><script src="/_next/static/chunks/framework-94f262366e752bd48d82.js" async=""></script><script src="/_next/static/chunks/commons-4667b98b62a60d7c050b.js" async=""></script><script src="/_next/static/chunks/main-9a4182dc8e7736869bf9.js" async=""></script><script src="/_next/static/chunks/pages/_app-afcb035e0413b2214f0f.js" async=""></script><script src="/_next/static/chunks/596-13cdde2fb793b77184ee.js" async=""></script><script src="/_next/static/chunks/522-e45333ebcf36799b860c.js" async=""></script><script src="/_next/static/chunks/373-350caaa5387e56e8b807.js" async=""></script><script src="/_next/static/chunks/93-1a5bac9fdf2b2295fba2.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0d67fc13dbd9e7ddf27f.js" async=""></script><script src="/_next/static/KJvUyrCoh0MXs5cG--Qn6/_buildManifest.js" async=""></script><script src="/_next/static/KJvUyrCoh0MXs5cG--Qn6/_ssgManifest.js" async=""></script></body></html>
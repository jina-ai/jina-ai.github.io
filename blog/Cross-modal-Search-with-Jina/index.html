<!DOCTYPE html><html lang="en"><head><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta property="og:title" content="Jina AI - a Neural Search Company"/><meta name="author" content="Jina AI"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="Jina AI"/><meta property="og:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta name="twitter:card" content="summary"/><meta property="twitter:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta property="twitter:title" content="Jina AI - a Neural Search Company"/><meta name="twitter:site" content="@JinaAI_"/><meta name="twitter:creator" content="@Jina AI"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-96x96.png"/><link rel="icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700"/><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:url" content="https://jina.ai/"/><meta property="og:title" content="Jina AI | Jina AI is a Neural Search Company"/><meta property="og:description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:locale" content="en"/><meta property="og:site_name" content="Jina AI, Ltd"/><link rel="canonical" href="https://jina.ai/"/><title>Cross-modal Search with Jina | Jina AI</title><meta name="next-head-count" content="29"/><link rel="preload" href="/_next/static/css/d6299e18a01a0ac2808b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d6299e18a01a0ac2808b.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-8f6366ea4b5fafd77e96.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework-94f262366e752bd48d82.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons-4667b98b62a60d7c050b.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-9a4182dc8e7736869bf9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-afcb035e0413b2214f0f.js" as="script"/><link rel="preload" href="/_next/static/chunks/596-13cdde2fb793b77184ee.js" as="script"/><link rel="preload" href="/_next/static/chunks/522-e45333ebcf36799b860c.js" as="script"/><link rel="preload" href="/_next/static/chunks/373-350caaa5387e56e8b807.js" as="script"/><link rel="preload" href="/_next/static/chunks/93-1a5bac9fdf2b2295fba2.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Bslug%5D-0d67fc13dbd9e7ddf27f.js" as="script"/><style id="__jsx-3792957869">.dropdown.jsx-3792957869{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));padding:1.5rem;--tw-text-opacity:1;color:rgba(74,85,104,var(--tw-text-opacity));top:5rem;width:24rem;border-radius:0.75rem;}.dropdown.jsx-3792957869>.jsx-3792957869:not([hidden])~.jsx-3792957869:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse));border-style:solid;--tw-divide-opacity:1;border-color:rgba(235,235,235,var(--tw-divide-opacity));}.dropdown.jsx-3792957869{box-shadow:3px 6px 33px rgba(205,205,205,0.25);}</style><style id="__jsx-3128403005">.dropdown.jsx-3128403005{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(74,85,104,var(--tw-text-opacity));top:5rem;width:100%;border-radius:0.75rem;}.dropdown.jsx-3128403005>.jsx-3128403005:not([hidden])~.jsx-3128403005:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse));border-style:solid;--tw-divide-opacity:1;border-color:rgba(235,235,235,var(--tw-divide-opacity));}</style><style id="__jsx-3949856645">.btn.jsx-3949856645{text-align:center;}.btn-base.jsx-3949856645{font-size:1.125rem;font-weight:600;}.btn-xl.jsx-3949856645{padding-top:1rem;padding-bottom:1rem;padding-left:1.5rem;padding-right:1.5rem;font-size:1.25rem;font-weight:800;}.btn-full-rounded.jsx-3949856645{border-radius:9999px;}.btn-primary.jsx-3949856645{border-radius:9999px;--tw-bg-opacity:1;background-color:rgba(0,129,129,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(255,255,255,var(--tw-text-opacity));}.btn-primary.jsx-3949856645:hover{background-color:#009191;}.btn-secondary.jsx-3949856645{--tw-bg-opacity:1;background-color:rgba(251,203,103,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(0,0,0,var(--tw-text-opacity));}.btn-secondary.jsx-3949856645:hover{background-color:#fbd076;}.btn-tertiary.jsx-3949856645{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));border:1px solid #999999;}.btn-tertiary.jsx-3949856645:hover{background-color:#f6f6f6;}.btn-outlined.jsx-3949856645{border-width:2px;border-style:solid;--tw-border-opacity:1;border-color:rgba(255,255,255,var(--tw-border-opacity));background-color:transparent;}.btn-outlined.jsx-3949856645:hover{background-color:#f6f6f622;}</style><style id="__jsx-223001487">.navbar.jsx-223001487 a{display:inline-block;width:100%;}.navbar.jsx-223001487 li:not(:first-child){margin-top:0.75rem;}.navbar.jsx-223001487 a .btn{width:100%;}.navbar.jsx-223001487 a:hover{--tw-text-opacity:1;color:rgba(0,129,129,var(--tw-text-opacity));}@media (min-width:640px){.navbar.jsx-223001487 a,.navbar.jsx-223001487 a .btn{width:auto;}.navbar.jsx-223001487 li:not(:first-child){margin-top:0px;}.navbar.jsx-223001487>(li:not(:last-child)){margin-right:1.25rem;}}</style><style id="__jsx-649517993">.top-nav-bar.jsx-649517993{box-shadow:3px 6px 33px 0px #cdcdcd40;z-index:200;position:fixed;width:100vw;background:white;top:0;}</style><style id="__jsx-4055754880">.markdown.jsx-4055754880{font-size:1.125rem;line-height:1.625;}.markdown.jsx-4055754880 p.jsx-4055754880,.markdown.jsx-4055754880 ul.jsx-4055754880,.markdown.jsx-4055754880 ol.jsx-4055754880,.markdown.jsx-4055754880 blockquote.jsx-4055754880{margin-top:1.5rem;margin-bottom:1.5rem;}.markdown.jsx-4055754880 h2.jsx-4055754880{margin-top:3rem;margin-bottom:1rem;font-size:1.875rem;line-height:1.375;}.markdown.jsx-4055754880 h3.jsx-4055754880{margin-top:2rem;margin-bottom:1rem;font-size:1.5rem;line-height:1.375;}</style><style id="__jsx-1324436386">.footer-links.jsx-1324436386 li{margin-top:0.25rem;}.footer-items-title.jsx-1324436386{font-size:1.375rem;line-height:1.5rem;-webkit-letter-spacing:0.22px;-moz-letter-spacing:0.22px;-ms-letter-spacing:0.22px;letter-spacing:0.22px;}</style><style id="__jsx-1956813867">.footer-left-margin.jsx-1956813867{width:20rem;height:inherit;background:no-repeat 40% 20% / 30% url('/assets/images/planet-beige.svg');}.footer-right-margin.jsx-1956813867{width:16rem;height:inherit;background:no-repeat 10% 10% / 50% url('/assets/images/planet-green.svg');}</style><style data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700">@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrFJM.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJbecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJnecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJfecnFHGPc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><script async="" src="https://www.googletagmanager.com/gtag/js?id=undefined"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'undefined', {
              page_path: window.location.pathname,
            });
          </script><body><div id="__next"><div class="jsx-649517993 top-nav-bar"><div class="md:max-w-screen-md lg:max-w-screen-lg xl:max-w-screen-2xl mx-auto px-3 py-6 undefined"><div class="jsx-223001487 flex flex-wrap justify-between items-center"><div class="jsx-223001487"><a class="jsx-223001487" href="/"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo.svg" alt="Jina.ai logo" class="w-24"/></div></a></div><div class="jsx-223001487 sm:hidden"><button type="button" aria-label="show menu" class="jsx-223001487 p-3 text-gray-900 rounded-md hover:bg-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke-width="1.5" fill="none" stroke-linecap="round" stroke-linejoin="round" class="jsx-223001487 stroke-current h-6 w-6"><path d="M0 0h24v24H0z" stroke="none" class="jsx-223001487"></path><path d="M4 6h16M4 12h16M4 18h16" class="jsx-223001487"></path></svg></button></div><nav class="jsx-223001487 w-full sm:w-auto sm:block mt-2 sm:mt-0 hidden"><ul class="jsx-223001487 navbar flex flex-col sm:flex-row sm:items-center font-medium text-xl text-gray-800 pt-3 pb-5 px-5 sm:p-0 bg-white sm:bg-transparent rounded"><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Products dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Products</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/core" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/core-icon.svg" alt="core icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Jina Core</span></div><span class="jsx-3792957869 text-xs text-gray-500">An open-source framework for building multimedia search applications.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/hub" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/hub-icon.svg" alt="hub icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Jina Hub</span></div><span class="jsx-3792957869 text-xs text-gray-500">Jina’s community-driven module marketplace for building your Flow</span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Developers dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Developers</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/contribute" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/contribute-icon.svg" alt="contribute icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Contribute</span></div><span class="jsx-3792957869 text-xs text-gray-500">Want to shape the future of neural search? Find out how you can contribute to Jina here.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/join" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/join-icon.svg" alt="join icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Join</span></div><span class="jsx-3792957869 text-xs text-gray-500">Join the Jina Slack community to meet the team and get support.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/learn" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/learn-icon.svg" alt="learn icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Learn</span></div><span class="jsx-3792957869 text-xs text-gray-500">Docs, tutorials and examples for developers to get started with Jina. </span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block"><div class="jsx-3792957869 group inline-block relative"><button aria-label="Open Company dropdown" class="jsx-3792957869 text-gray-700 font-semibold py-2 px-4 rounded inline-flex items-center"><span class="jsx-3792957869 mr-1 font-bold">Company</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="jsx-3792957869 fill-current h-4 w-4 mt-2"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z" class="jsx-3792957869"></path></svg></button><div class="jsx-3792957869 absolute hidden group-hover:block bg-transparent pt-7"><ul class="jsx-3792957869 dropdown"><li class="jsx-3792957869 hover:bg-gray-200"><a href="/about" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/about-icon.svg" alt="about icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">About</span></div><span class="jsx-3792957869 text-xs text-gray-500">Learn more about Jina AI as a company.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/careers" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/careers-icon.svg" alt="careers icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Careers</span></div><span class="jsx-3792957869 text-xs text-gray-500">Interested in joining us? Find out what it&#x27;s like to work at Jina AI.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/events" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/events-icon.svg" alt="events icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Events</span></div><span class="jsx-3792957869 text-xs text-gray-500">Online and offline meetups, hackathons, workshops and webinars.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/news" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/news-icon.svg" alt="news icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">News</span></div><span class="jsx-3792957869 text-xs text-gray-500">Check out the latest news about our company and products.</span></div></a></li><li class="jsx-3792957869 hover:bg-gray-200"><a href="/contact" class="jsx-3792957869 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3792957869 leading-4"><div class="jsx-3792957869 flex items-center"><div class=""><img src="/assets/images/icons/contact-icon.svg" alt="contact icon" class="w-full h-full"/></div><span class="jsx-3792957869 text-gray-800 font-semibold text-sm ml-2">Contact</span></div><span class="jsx-3792957869 text-xs text-gray-500">Interested in cooperating with Jina AI? Get in touch!</span></div></a></li></ul></div></div></li><li class="jsx-649517993 hidden md:inline-block mr-1 font-bold text-gray-700"><a href="/blog/">Blog</a></li><div class="jsx-3128403005 md:hidden h-96 overflow-y-scroll"><ul class="jsx-3128403005 dropdown"><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/core" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/core-icon.svg" alt="core icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Jina Core</span></div><span class="jsx-3128403005 text-xs text-gray-500">An open-source framework for building multimedia search applications.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/hub" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/hub-icon.svg" alt="hub icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Jina Hub</span></div><span class="jsx-3128403005 text-xs text-gray-500">Jina’s community-driven module marketplace for building your Flow</span></div></a></li></div><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/contribute" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/contribute-icon.svg" alt="contribute icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Contribute</span></div><span class="jsx-3128403005 text-xs text-gray-500">Want to shape the future of neural search? Find out how you can contribute to Jina here.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/join" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/join-icon.svg" alt="join icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Join</span></div><span class="jsx-3128403005 text-xs text-gray-500">Join the Jina Slack community to meet the team and get support.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/learn" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/learn-icon.svg" alt="learn icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Learn</span></div><span class="jsx-3128403005 text-xs text-gray-500">Docs, tutorials and examples for developers to get started with Jina. </span></div></a></li></div><div class="jsx-3128403005 linkItem.label divide-solid divide-gray-200 divide-y"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/about" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/about-icon.svg" alt="about icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">About</span></div><span class="jsx-3128403005 text-xs text-gray-500">Learn more about Jina AI as a company.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/careers" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/careers-icon.svg" alt="careers icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Careers</span></div><span class="jsx-3128403005 text-xs text-gray-500">Interested in joining us? Find out what it&#x27;s like to work at Jina AI.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/events" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/events-icon.svg" alt="events icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Events</span></div><span class="jsx-3128403005 text-xs text-gray-500">Online and offline meetups, hackathons, workshops and webinars.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/news" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/news-icon.svg" alt="news icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">News</span></div><span class="jsx-3128403005 text-xs text-gray-500">Check out the latest news about our company and products.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/contact" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/contact-icon.svg" alt="contact icon" class="w-full h-full"/></div><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Contact</span></div><span class="jsx-3128403005 text-xs text-gray-500">Interested in cooperating with Jina AI? Get in touch!</span></div></a></li></div></ul><span class="jsx-3128403005 text-gray-700 font-semibold text-sm ml-2 p-4">Blog</span></div><li class="jsx-223001487"><div class="jsx-3949856645 btn cursor-pointer flex items-center justify-center px-6 py-2 md:hidden btn-base btn-primary"><div class="mr-4 w-7"><img src="/assets/images/icons/GitHub-white-icon.svg" alt="GitHub-white icon" class="w-full h-full"/></div><span class="jsx-223001487 mr-4">Try it out!</span></div></li></ul></nav><div class="jsx-3949856645 btn cursor-pointer flex items-center justify-center px-6 py-2 hidden md:flex btn-base btn-primary"><div class="mr-4 w-7"><img src="/assets/images/icons/GitHub-white-icon.svg" alt="GitHub-white icon" class="w-full h-full"/></div><span class="jsx-223001487 mr-4">Try it out!</span></div></div></div></div><div class="mt-24"><div class="min-h-screen"><main><div class="container mx-auto px-5 flex flex-col"><img alt="Cross-modal Search with Jina-image" class="mb-4 w-full"/><article class="mb-32 md:max-w-6xl md:shadow-lg xl:rounded-3xl md:px-32 md:py-20 md:mx-auto md:-mt-20 bg-white shadow"><div class="mb-3"><div class="inline-block px-2 py-0 rounded text-primary-500 bg-primary-500 bg-opacity-20 mr-2 mb-2">cross-modal</div></div><h1 class="text-xl md:text-4xl font-bold tracking-tighter leading-tight md:leading-none mb-6 md:text-left">Cross-modal Search with Jina</h1><div class="flex items-center justify-between text-gray-600 mb-12"><div class="flex items-center"><img src="/assets/images/defaultProfileImg.svg" class="w-8 h-auto rounded-full mr-4" alt="Joan Fontanals Martínez"/><div class="text-gray-600">Joan Fontanals Martínez</div></div><time dateTime="2020-10-02T08:05:45.631Z">2 October, 2020</time></div><div class="jsx-4055754880 mx-auto markdown"><div class="jsx-4055754880"><h2>Cross-modal search</h2>
<p>In this post I will explain how we implemented a search engine in Jina for cross-modal content using the paper <a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>.</p>
<p>The result is an application that allows:</p>
<ul>
<li>Searching images, using descriptive captions as input query, or</li>
<li>Searching text captions, using an image as input query</li>
</ul>
<figure>
        <img src="https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif" alt>
    <figcaption class="figure-caption">The Finished Application</figcaption>
</figure>
<p>The code and instructions to run the application can be found in <a href="https://github.com/jina-ai/examples/tree/master/cross-modal-search">https://github.com/jina-ai/examples/tree/master/cross-modal-search</a></p>
<h2>Cross-modality</h2>
<p>First, we need to understand the concept of modality: Given our example, one may think that different modalities correspond to different kinds of data (images and text in this case). However, this is not accurate. For example, one can do cross-modal search by searching images from different points of view, or searching for matching titles for given paragraph text.</p>
<p>Therefore, one can consider that a modality is related to a given data distribution from which input may come. For this reason, and to have first-class support for cross and multi-modal search, Jina offers modality as an attribute from its Document protobuf definition.</p>
<p>Now that we are agreed on the concept of modality, we can describe cross-modal search as a set of retrieval applications that try to effectively find relevant documents of modality <em>A</em> by querying with documents from modality <em>B</em>.</p>
<h2>Semantic Search</h2>
<p>Compared to keyword-based search, the main requirement for content-based search is the ability to extract a meaningful <strong>semantic</strong> representation of the documents both at index and query time. This implies the projection of documents into a <strong>high dimensional vector embedding space</strong> where distances (or similarities) between these vectors are considered the measure of relevance between queries and indexed documents.</p>
<p>With current advances in performance of all the Deep Learning methods, even general purpose models (e,g. CNN models trained on ImageNet) can be used to extract meaningful feature vectors <em>(</em><a href="https://github.com/jina-ai/examples/tree/master/pokedex-with-bit"><em>Here Jina uses simple feature vectors from mobilenet pretrained for classification tasks on ImageNet to build a working Pokemon search application</em></a><em>)</em>.</p>
<p>However, models trained using <strong>Deep Metric Learning</strong> are especially suited for retrieval. In opposition to common classification architectures (usually trained using <strong>Cross-Entropy Loss)</strong>, these deep metric models tend to optimize a <strong>Contrastive Loss</strong> metric which tries to put similar objects close to each other and non-related objects further away.</p>
<figure>
        <img src="https://miro.medium.com/max/328/1*hW_5S5kSmas__mGlBR9qDg.gif" alt>
    <figcaption class="figure-caption">Cross-Entropy Loss</figcaption>
</figure>
<p>In contrastive loss, the intention is to minimize the distance for positive pairs <em>(y = 1)</em> and to maximize the distance (with some margin <em>m</em>) when negative pairs <em>(y = 0)</em></p>
<figure>
        <img src="https://miro.medium.com/max/298/1*7ak-Os2FKJu9JKHFKCwOhw.gif" alt>
    <figcaption class="figure-caption">Contrastive Loss</figcaption>
</figure>
<h2>Siamese and Triplet Networks</h2>
<p>Two very common architectures for Deep Metric Learning are Siamese and Triplet Networks. They both share the idea that different sub-networks (which may or may not share weights) receive different inputs at the same time (<strong>positive</strong> and <strong>negative</strong> pairs for Siamese Networks; positive, negative and <strong>anchor</strong> documents for Triplets), and try to project their own feature vectors onto a common latent space where the <strong>contrastive</strong> loss is computed and its error propagated to all the sub-networks.</p>
<p>Positive pairs are pairs of objects (images, text, any document) that are semantically related and expected to remain close in the projection space. On the other hand, negative pairs are pairs of documents that should be apart.</p>
<figure>
        <img src="https://miro.medium.com/max/945/1*84InYr1UPrVgg6uyMjA3wQ.png" alt>
    <figcaption class="figure-caption">Schema of the deep metric learning process with a triplet network and anchor. Image taken from https://gombru.github.io/2019/04/03/ranking_loss/</figcaption>
</figure>
<p>In the example, the sub-network used to extract image features is a <strong>VGG19</strong> architecture with weights pre-trained on ImageNet, while for the text embedding, output of a hidden layer from a Gated Recurrent Unit (<strong>GRU</strong>) are used.</p>
<h2>Hard Negatives</h2>
<p>Besides all the common tricks and techniques to improve the learning of neural networks, for Deep Metric Learning, a key aspect of performance is the choice of positive and negative pairs. It is important for the model to see negative pairs that are not easy to split, which is achieved using <strong>Hard Negative Mining</strong>. This can impact some evaluation metrics, especially <em>Recall@k</em> with small values of <em>k</em>. Without emphasis on negative pairs the model will be able to extract meaningful neighborhoods but will find it hard to really extract true nearest neighbors, and then underperforming when evaluated at very low <em>ks</em>.</p>
<figure>
        <img src="https://miro.medium.com/max/948/1*fsGvQnPbK-r2xD2jt8CHzg.png" alt>
    <figcaption class="figure-caption">The different types of negatives. Source: https://omoindrot.github.io/triplet-loss</figcaption>
</figure>
<p>The paper in which the example is based (<a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>) proposes an advanced hard negative mining strategy that increases the probability of sampling hard negatives at training time, thus obtaining a significant boost on <em>R@1</em> for both image-to-caption and caption-to-image retrieval.</p>
<h2><strong>The Search Flow in Jina</strong></h2>
<p>Jina is a useful choice for this implementation. It is a framework for developing semantic search applications with first-class support for cross-modality. Plus, it makes it easy to plug in your own models and to distribute them with the use of Docker containers, which leads to a very smooth development experience and reduces the boilerplate of complex dependency management.</p>
<p>It allows the description of complex AI-powered search pipelines from simple YAML Flow descriptions: In this example, two Flows are created, one for indexing images and captions and another one for querying.</p>
<p>At index time, images are pre-processed and normalized before being embedded in a vector space. In parallel, images are indexed without any crafting into a Key-Value database so that the user can retrieve and render them.</p>
<p>On the other branch of the Flow, text does not require any preprocessing before encoding (vocabulary lookup and word embedding are done during encoding), so the text indexer takes care of both vector and key-value indexing.</p>
<figure>
	<img src="https://miro.medium.com/max/948/1*2Uram5hzmTmW7dcU4uDOoQ.png">
	<figcaption class="figure-caption">Jina Index flow for Cross modal search visualization on dashboard</figcaption>
</figure>
<p>Query time is where the “cross” in cross-modality shines, the key aspect of the design of the Flow is that the branch responsible for obtaining semantic embeddings for images is connected to the text embedding index and vice-versa. This way, images are extracted by providing text as input and captions are retrieved by providing input images.</p>
<figure>
	<img src="https://miro.medium.com/max/948/1*L0LMehwNU6CzeFAu4mwPkQ.png">
	<figcaption class="figure-caption">Jina Query Flow for Cross modal search visualization in Dashboard</figcaption>
</figure>
<p>In both cases, there are two branches of the Flow: One will process images, and the other text. This is controlled by a filter applied at the beginning of each branch to select which inputs can be processed.</p>
<p>Filter modality in Flow:</p>
<pre><code class="language-yaml">- !FilterQL
   with:
     lookups: {'modality': 'image'}
</code></pre>
<h2>Plug the Visual Semantic Embedding Models in Jina</h2>
<p>As stated, Jina makes it easy to plug in different models, and turns out to be a very suitable tool to transfer this research into a real-world search application.</p>
<p>In order to use the model resulting from the papers’ model, two different encoders <a href="https://docs.jina.ai/chapters/101/.sphinx.html#executor">executors</a> (called <strong>VSEImageEmbedding</strong> and <strong>VSETextEmbedding</strong>) were developed. Each of them just use a specific branch of the original common embedding network.</p>
<p>Since they rely on pickled weights and models, the main challenge is getting the right models and vocabulary files to load the right models. All this boilerplate is abstracted from the user by building the <strong>Docker</strong> images that will deploy these models very easily.</p>
<h2>Results</h2>
<p>The example has been run with <em>Flickr8k</em> dataset with good results, although the models have been trained using <em>Flickr30k.</em> This shows the ability of the model to generalize to unseen data, and the ability to work on general-purpose datasets. These models can be easily retrained and fine-tuned for specific use-cases scenarios.</p>
<p>The results are shown using <a href="https://github.com/jina-ai/jinabox.js">jinabox</a>, which allows to interact with jina directly from the browser and inputing multiple kinds of data.</p>
<p><img src="https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif" alt=""></p>
<h2><strong>References</strong></h2>
<ul>
<li><a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a></li>
<li><a href="https://www.kaggle.com/shadabhussain/flickr8k">Flickr8K</a></li>
<li><a href="https://github.com/jina-ai/jina">Jina</a></li>
<li><a href="https://github.com/jina-ai/examples">Jina examples</a></li>
<li><a href="https://github.com/jina-ai/jinabox.js">Jinabox.js</a></li>
</ul>
<p>By <a href="https://www.linkedin.com/in/joanfontanalsmartinez/">Joan Fontanals Martínez</a> on October 2, 2020.</p>
</div><div class="jsx-4055754880 hidden"><h2>Cross-modal search</h2>
<p>In this post I will explain how we implemented a search engine in Jina for cross-modal content using the paper <a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>.</p>
<p>The result is an application that allows:</p>
<ul>
<li>Searching images, using descriptive captions as input query, or</li>
<li>Searching text captions, using an image as input query</li>
</ul>
<figure>
        <img src="https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif" alt>
    <figcaption class="figure-caption">The Finished Application</figcaption>
</figure>
<p>The code and instructions to run the application can be found in <a href="https://github.com/jina-ai/examples/tree/master/cross-modal-search">https://github.com/jina-ai/examples/tree/master/cross-modal-search</a></p>
<h2>Cross-modality</h2>
<p>First, we need to understand the concept of modality: Given our example, one may think that different modalities correspond to different kinds of data (images and text in this case). However, this is not accurate. For example, one can do cross-modal search by searching images from different points of view, or searching for matching titles for given paragraph text.</p>
<p>Therefore, one can consider that a modality is related to a given data distribution from which input may come. For this reason, and to have first-class support for cross and multi-modal search, Jina offers modality as an attribute from its Document protobuf definition.</p>
<p>Now that we are agreed on the concept of modality, we can describe cross-modal search as a set of retrieval applications that try to effectively find relevant documents of modality <em>A</em> by querying with documents from modality <em>B</em>.</p>
<h2>Semantic Search</h2>
<p>Compared to keyword-based search, the main requirement for content-based search is the ability to extract a meaningful <strong>semantic</strong> representation of the documents both at index and query time. This implies the projection of documents into a <strong>high dimensional vector embedding space</strong> where distances (or similarities) between these vectors are considered the measure of relevance between queries and indexed documents.</p>
<p>With current advances in performance of all the Deep Learning methods, even general purpose models (e,g. CNN models trained on ImageNet) can be used to extract meaningful feature vectors <em>(</em><a href="https://github.com/jina-ai/examples/tree/master/pokedex-with-bit"><em>Here Jina uses simple feature vectors from mobilenet pretrained for classification tasks on ImageNet to build a working Pokemon search application</em></a><em>)</em>.</p>
<p>However, models trained using <strong>Deep Metric Learning</strong> are especially suited for retrieval. In opposition to common classification architectures (usually trained using <strong>Cross-Entropy Loss)</strong>, these deep metric models tend to optimize a <strong>Contrastive Loss</strong> metric which tries to put similar objects close to each other and non-related objects further away.</p>
<figure>
        <img src="https://miro.medium.com/max/328/1*hW_5S5kSmas__mGlBR9qDg.gif" alt>
    <figcaption class="figure-caption">Cross-Entropy Loss</figcaption>
</figure>
<p>In contrastive loss, the intention is to minimize the distance for positive pairs <em>(y = 1)</em> and to maximize the distance (with some margin <em>m</em>) when negative pairs <em>(y = 0)</em></p>
<figure>
        <img src="https://miro.medium.com/max/298/1*7ak-Os2FKJu9JKHFKCwOhw.gif" alt>
    <figcaption class="figure-caption">Contrastive Loss</figcaption>
</figure>
<h2>Siamese and Triplet Networks</h2>
<p>Two very common architectures for Deep Metric Learning are Siamese and Triplet Networks. They both share the idea that different sub-networks (which may or may not share weights) receive different inputs at the same time (<strong>positive</strong> and <strong>negative</strong> pairs for Siamese Networks; positive, negative and <strong>anchor</strong> documents for Triplets), and try to project their own feature vectors onto a common latent space where the <strong>contrastive</strong> loss is computed and its error propagated to all the sub-networks.</p>
<p>Positive pairs are pairs of objects (images, text, any document) that are semantically related and expected to remain close in the projection space. On the other hand, negative pairs are pairs of documents that should be apart.</p>
<figure>
        <img src="https://miro.medium.com/max/945/1*84InYr1UPrVgg6uyMjA3wQ.png" alt>
    <figcaption class="figure-caption">Schema of the deep metric learning process with a triplet network and anchor. Image taken from https://gombru.github.io/2019/04/03/ranking_loss/</figcaption>
</figure>
<p>In the example, the sub-network used to extract image features is a <strong>VGG19</strong> architecture with weights pre-trained on ImageNet, while for the text embedding, output of a hidden layer from a Gated Recurrent Unit (<strong>GRU</strong>) are used.</p>
<h2>Hard Negatives</h2>
<p>Besides all the common tricks and techniques to improve the learning of neural networks, for Deep Metric Learning, a key aspect of performance is the choice of positive and negative pairs. It is important for the model to see negative pairs that are not easy to split, which is achieved using <strong>Hard Negative Mining</strong>. This can impact some evaluation metrics, especially <em>Recall@k</em> with small values of <em>k</em>. Without emphasis on negative pairs the model will be able to extract meaningful neighborhoods but will find it hard to really extract true nearest neighbors, and then underperforming when evaluated at very low <em>ks</em>.</p>
<figure>
        <img src="https://miro.medium.com/max/948/1*fsGvQnPbK-r2xD2jt8CHzg.png" alt>
    <figcaption class="figure-caption">The different types of negatives. Source: https://omoindrot.github.io/triplet-loss</figcaption>
</figure>
<p>The paper in which the example is based (<a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>) proposes an advanced hard negative mining strategy that increases the probability of sampling hard negatives at training time, thus obtaining a significant boost on <em>R@1</em> for both image-to-caption and caption-to-image retrieval.</p>
<h2><strong>The Search Flow in Jina</strong></h2>
<p>Jina is a useful choice for this implementation. It is a framework for developing semantic search applications with first-class support for cross-modality. Plus, it makes it easy to plug in your own models and to distribute them with the use of Docker containers, which leads to a very smooth development experience and reduces the boilerplate of complex dependency management.</p>
<p>It allows the description of complex AI-powered search pipelines from simple YAML Flow descriptions: In this example, two Flows are created, one for indexing images and captions and another one for querying.</p>
<p>At index time, images are pre-processed and normalized before being embedded in a vector space. In parallel, images are indexed without any crafting into a Key-Value database so that the user can retrieve and render them.</p>
<p>On the other branch of the Flow, text does not require any preprocessing before encoding (vocabulary lookup and word embedding are done during encoding), so the text indexer takes care of both vector and key-value indexing.</p>
<figure>
	<img src="https://miro.medium.com/max/948/1*2Uram5hzmTmW7dcU4uDOoQ.png">
	<figcaption class="figure-caption">Jina Index flow for Cross modal search visualization on dashboard</figcaption>
</figure>
<p>Query time is where the “cross” in cross-modality shines, the key aspect of the design of the Flow is that the branch responsible for obtaining semantic embeddings for images is connected to the text embedding index and vice-versa. This way, images are extracted by providing text as input and captions are retrieved by providing input images.</p>
<figure>
	<img src="https://miro.medium.com/max/948/1*L0LMehwNU6CzeFAu4mwPkQ.png">
	<figcaption class="figure-caption">Jina Query Flow for Cross modal search visualization in Dashboard</figcaption>
</figure>
<p>In both cases, there are two branches of the Flow: One will process images, and the other text. This is controlled by a filter applied at the beginning of each branch to select which inputs can be processed.</p>
<p>Filter modality in Flow:</p>
<pre class="rounded-xl m-2 md:mr-6" style="display:block;overflow-x:auto;padding:0.5em;background:#2d2b57;font-weight:normal;color:#e3dfff" data-reactroot=""><code class="language-python" style="white-space:pre"><span>- !FilterQL
</span><span>   </span><span style="color:#fb9e00;font-weight:normal">with</span><span>:
</span><span>     lookups: {</span><span style="color:#4cd213">&#x27;modality&#x27;</span><span>: </span><span style="color:#4cd213">&#x27;image&#x27;</span><span>}
</span>
</code></pre>
<h2>Plug the Visual Semantic Embedding Models in Jina</h2>
<p>As stated, Jina makes it easy to plug in different models, and turns out to be a very suitable tool to transfer this research into a real-world search application.</p>
<p>In order to use the model resulting from the papers’ model, two different encoders <a href="https://docs.jina.ai/chapters/101/.sphinx.html#executor">executors</a> (called <strong>VSEImageEmbedding</strong> and <strong>VSETextEmbedding</strong>) were developed. Each of them just use a specific branch of the original common embedding network.</p>
<p>Since they rely on pickled weights and models, the main challenge is getting the right models and vocabulary files to load the right models. All this boilerplate is abstracted from the user by building the <strong>Docker</strong> images that will deploy these models very easily.</p>
<h2>Results</h2>
<p>The example has been run with <em>Flickr8k</em> dataset with good results, although the models have been trained using <em>Flickr30k.</em> This shows the ability of the model to generalize to unseen data, and the ability to work on general-purpose datasets. These models can be easily retrained and fine-tuned for specific use-cases scenarios.</p>
<p>The results are shown using <a href="https://github.com/jina-ai/jinabox.js">jinabox</a>, which allows to interact with jina directly from the browser and inputing multiple kinds of data.</p>
<p><img src="https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif" alt=""></p>
<h2><strong>References</strong></h2>
<ul>
<li><a href="https://arxiv.org/abs/1707.05612">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a></li>
<li><a href="https://www.kaggle.com/shadabhussain/flickr8k">Flickr8K</a></li>
<li><a href="https://github.com/jina-ai/jina">Jina</a></li>
<li><a href="https://github.com/jina-ai/examples">Jina examples</a></li>
<li><a href="https://github.com/jina-ai/jinabox.js">Jinabox.js</a></li>
</ul>
<p>By <a href="https://www.linkedin.com/in/joanfontanalsmartinez/">Joan Fontanals Martínez</a> on October 2, 2020.</p>
</div></div></article></div></main></div></div><div class="jsx-1956813867"><div class="bg-primary-500 pt-20 text-white"><div class="jsx-1956813867 flex flex-col md:flex-row justify-between px-6 md:px-0"><div class="jsx-1956813867 footer-left-margin"></div><div class="jsx-1956813867 pr-4 md:py-16 md:mr-16"><div class="text-center md:text-left"><div class="flex md:justify-start"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo-white.svg" alt="Jina.ai logo" class="w-24"/></div></div></div></div><div class="jsx-1956813867 grid grid-cols-2 sm:grid-cols-2 md:grid-cols-4 gap-16 py-16"><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Company</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/about/">About us</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/careers/">Careers</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/contact/">Contact us</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/blog/">Blog</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Products</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/core/">Core</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/hub/">Hub</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Developers</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/contribute/">Contribute</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/learn/">Learn</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/join/">Join</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Social</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://github.com/jina-ai/jina/">GitHub</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://www.linkedin.com/company/jinaai/">LinkedIn</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://twitter.com/jinaAI_/">Twitter</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://jina-ai.slack.com/">Slack</a></li></ul></nav></div></div><div class="jsx-1956813867 footer-right-margin"></div></div><div class="text-center bg-primary-400 text-gray-100 text-sm mt-12 py-6"><span class="block md:inline-block">© 2021 Jina AI, Ltd. All rights reserved.</span><span class="mx-4 hidden md:inline-block">|</span><a href="/legal"><span>Terms of Service</span></a><span class="mx-4">|</span><a href="/legal"><span>Privacy Policy</span></a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Cross-modal Search with Jina","date":"2020-10-02T08:05:45.631Z","slug":"Cross-modal-Search-with-Jina","author":"Joan Fontanals Martínez","content":"\u003ch2\u003eCross-modal search\u003c/h2\u003e\n\u003cp\u003eIn this post I will explain how we implemented a search engine in Jina for cross-modal content using the paper \u003ca href=\"https://arxiv.org/abs/1707.05612\"\u003eVSE++: Improving Visual-Semantic Embeddings with Hard Negatives\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe result is an application that allows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSearching images, using descriptive captions as input query, or\u003c/li\u003e\n\u003cli\u003eSearching text captions, using an image as input query\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\n        \u003cimg src=\"https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif\" alt\u003e\n    \u003cfigcaption class=\"figure-caption\"\u003eThe Finished Application\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe code and instructions to run the application can be found in \u003ca href=\"https://github.com/jina-ai/examples/tree/master/cross-modal-search\"\u003ehttps://github.com/jina-ai/examples/tree/master/cross-modal-search\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eCross-modality\u003c/h2\u003e\n\u003cp\u003eFirst, we need to understand the concept of modality: Given our example, one may think that different modalities correspond to different kinds of data (images and text in this case). However, this is not accurate. For example, one can do cross-modal search by searching images from different points of view, or searching for matching titles for given paragraph text.\u003c/p\u003e\n\u003cp\u003eTherefore, one can consider that a modality is related to a given data distribution from which input may come. For this reason, and to have first-class support for cross and multi-modal search, Jina offers modality as an attribute from its Document protobuf definition.\u003c/p\u003e\n\u003cp\u003eNow that we are agreed on the concept of modality, we can describe cross-modal search as a set of retrieval applications that try to effectively find relevant documents of modality \u003cem\u003eA\u003c/em\u003e by querying with documents from modality \u003cem\u003eB\u003c/em\u003e.\u003c/p\u003e\n\u003ch2\u003eSemantic Search\u003c/h2\u003e\n\u003cp\u003eCompared to keyword-based search, the main requirement for content-based search is the ability to extract a meaningful \u003cstrong\u003esemantic\u003c/strong\u003e representation of the documents both at index and query time. This implies the projection of documents into a \u003cstrong\u003ehigh dimensional vector embedding space\u003c/strong\u003e where distances (or similarities) between these vectors are considered the measure of relevance between queries and indexed documents.\u003c/p\u003e\n\u003cp\u003eWith current advances in performance of all the Deep Learning methods, even general purpose models (e,g. CNN models trained on ImageNet) can be used to extract meaningful feature vectors \u003cem\u003e(\u003c/em\u003e\u003ca href=\"https://github.com/jina-ai/examples/tree/master/pokedex-with-bit\"\u003e\u003cem\u003eHere Jina uses simple feature vectors from mobilenet pretrained for classification tasks on ImageNet to build a working Pokemon search application\u003c/em\u003e\u003c/a\u003e\u003cem\u003e)\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, models trained using \u003cstrong\u003eDeep Metric Learning\u003c/strong\u003e are especially suited for retrieval. In opposition to common classification architectures (usually trained using \u003cstrong\u003eCross-Entropy Loss)\u003c/strong\u003e, these deep metric models tend to optimize a \u003cstrong\u003eContrastive Loss\u003c/strong\u003e metric which tries to put similar objects close to each other and non-related objects further away.\u003c/p\u003e\n\u003cfigure\u003e\n        \u003cimg src=\"https://miro.medium.com/max/328/1*hW_5S5kSmas__mGlBR9qDg.gif\" alt\u003e\n    \u003cfigcaption class=\"figure-caption\"\u003eCross-Entropy Loss\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIn contrastive loss, the intention is to minimize the distance for positive pairs \u003cem\u003e(y = 1)\u003c/em\u003e and to maximize the distance (with some margin \u003cem\u003em\u003c/em\u003e) when negative pairs \u003cem\u003e(y = 0)\u003c/em\u003e\u003c/p\u003e\n\u003cfigure\u003e\n        \u003cimg src=\"https://miro.medium.com/max/298/1*7ak-Os2FKJu9JKHFKCwOhw.gif\" alt\u003e\n    \u003cfigcaption class=\"figure-caption\"\u003eContrastive Loss\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2\u003eSiamese and Triplet Networks\u003c/h2\u003e\n\u003cp\u003eTwo very common architectures for Deep Metric Learning are Siamese and Triplet Networks. They both share the idea that different sub-networks (which may or may not share weights) receive different inputs at the same time (\u003cstrong\u003epositive\u003c/strong\u003e and \u003cstrong\u003enegative\u003c/strong\u003e pairs for Siamese Networks; positive, negative and \u003cstrong\u003eanchor\u003c/strong\u003e documents for Triplets), and try to project their own feature vectors onto a common latent space where the \u003cstrong\u003econtrastive\u003c/strong\u003e loss is computed and its error propagated to all the sub-networks.\u003c/p\u003e\n\u003cp\u003ePositive pairs are pairs of objects (images, text, any document) that are semantically related and expected to remain close in the projection space. On the other hand, negative pairs are pairs of documents that should be apart.\u003c/p\u003e\n\u003cfigure\u003e\n        \u003cimg src=\"https://miro.medium.com/max/945/1*84InYr1UPrVgg6uyMjA3wQ.png\" alt\u003e\n    \u003cfigcaption class=\"figure-caption\"\u003eSchema of the deep metric learning process with a triplet network and anchor. Image taken from https://gombru.github.io/2019/04/03/ranking_loss/\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIn the example, the sub-network used to extract image features is a \u003cstrong\u003eVGG19\u003c/strong\u003e architecture with weights pre-trained on ImageNet, while for the text embedding, output of a hidden layer from a Gated Recurrent Unit (\u003cstrong\u003eGRU\u003c/strong\u003e) are used.\u003c/p\u003e\n\u003ch2\u003eHard Negatives\u003c/h2\u003e\n\u003cp\u003eBesides all the common tricks and techniques to improve the learning of neural networks, for Deep Metric Learning, a key aspect of performance is the choice of positive and negative pairs. It is important for the model to see negative pairs that are not easy to split, which is achieved using \u003cstrong\u003eHard Negative Mining\u003c/strong\u003e. This can impact some evaluation metrics, especially \u003cem\u003eRecall@k\u003c/em\u003e with small values of \u003cem\u003ek\u003c/em\u003e. Without emphasis on negative pairs the model will be able to extract meaningful neighborhoods but will find it hard to really extract true nearest neighbors, and then underperforming when evaluated at very low \u003cem\u003eks\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n        \u003cimg src=\"https://miro.medium.com/max/948/1*fsGvQnPbK-r2xD2jt8CHzg.png\" alt\u003e\n    \u003cfigcaption class=\"figure-caption\"\u003eThe different types of negatives. Source: https://omoindrot.github.io/triplet-loss\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe paper in which the example is based (\u003ca href=\"https://arxiv.org/abs/1707.05612\"\u003eVSE++: Improving Visual-Semantic Embeddings with Hard Negatives\u003c/a\u003e) proposes an advanced hard negative mining strategy that increases the probability of sampling hard negatives at training time, thus obtaining a significant boost on \u003cem\u003eR@1\u003c/em\u003e for both image-to-caption and caption-to-image retrieval.\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eThe Search Flow in Jina\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eJina is a useful choice for this implementation. It is a framework for developing semantic search applications with first-class support for cross-modality. Plus, it makes it easy to plug in your own models and to distribute them with the use of Docker containers, which leads to a very smooth development experience and reduces the boilerplate of complex dependency management.\u003c/p\u003e\n\u003cp\u003eIt allows the description of complex AI-powered search pipelines from simple YAML Flow descriptions: In this example, two Flows are created, one for indexing images and captions and another one for querying.\u003c/p\u003e\n\u003cp\u003eAt index time, images are pre-processed and normalized before being embedded in a vector space. In parallel, images are indexed without any crafting into a Key-Value database so that the user can retrieve and render them.\u003c/p\u003e\n\u003cp\u003eOn the other branch of the Flow, text does not require any preprocessing before encoding (vocabulary lookup and word embedding are done during encoding), so the text indexer takes care of both vector and key-value indexing.\u003c/p\u003e\n\u003cfigure\u003e\n\t\u003cimg src=\"https://miro.medium.com/max/948/1*2Uram5hzmTmW7dcU4uDOoQ.png\"\u003e\n\t\u003cfigcaption class=\"figure-caption\"\u003eJina Index flow for Cross modal search visualization on dashboard\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eQuery time is where the “cross” in cross-modality shines, the key aspect of the design of the Flow is that the branch responsible for obtaining semantic embeddings for images is connected to the text embedding index and vice-versa. This way, images are extracted by providing text as input and captions are retrieved by providing input images.\u003c/p\u003e\n\u003cfigure\u003e\n\t\u003cimg src=\"https://miro.medium.com/max/948/1*L0LMehwNU6CzeFAu4mwPkQ.png\"\u003e\n\t\u003cfigcaption class=\"figure-caption\"\u003eJina Query Flow for Cross modal search visualization in Dashboard\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIn both cases, there are two branches of the Flow: One will process images, and the other text. This is controlled by a filter applied at the beginning of each branch to select which inputs can be processed.\u003c/p\u003e\n\u003cp\u003eFilter modality in Flow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-yaml\"\u003e- !FilterQL\n   with:\n     lookups: {'modality': 'image'}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003ePlug the Visual Semantic Embedding Models in Jina\u003c/h2\u003e\n\u003cp\u003eAs stated, Jina makes it easy to plug in different models, and turns out to be a very suitable tool to transfer this research into a real-world search application.\u003c/p\u003e\n\u003cp\u003eIn order to use the model resulting from the papers’ model, two different encoders \u003ca href=\"https://docs.jina.ai/chapters/101/.sphinx.html#executor\"\u003eexecutors\u003c/a\u003e (called \u003cstrong\u003eVSEImageEmbedding\u003c/strong\u003e and \u003cstrong\u003eVSETextEmbedding\u003c/strong\u003e) were developed. Each of them just use a specific branch of the original common embedding network.\u003c/p\u003e\n\u003cp\u003eSince they rely on pickled weights and models, the main challenge is getting the right models and vocabulary files to load the right models. All this boilerplate is abstracted from the user by building the \u003cstrong\u003eDocker\u003c/strong\u003e images that will deploy these models very easily.\u003c/p\u003e\n\u003ch2\u003eResults\u003c/h2\u003e\n\u003cp\u003eThe example has been run with \u003cem\u003eFlickr8k\u003c/em\u003e dataset with good results, although the models have been trained using \u003cem\u003eFlickr30k.\u003c/em\u003e This shows the ability of the model to generalize to unseen data, and the ability to work on general-purpose datasets. These models can be easily retrained and fine-tuned for specific use-cases scenarios.\u003c/p\u003e\n\u003cp\u003eThe results are shown using \u003ca href=\"https://github.com/jina-ai/jinabox.js\"\u003ejinabox\u003c/a\u003e, which allows to interact with jina directly from the browser and inputing multiple kinds of data.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1707.05612\"\u003eVSE++: Improving Visual-Semantic Embeddings with Hard Negatives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/shadabhussain/flickr8k\"\u003eFlickr8K\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jina-ai/jina\"\u003eJina\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jina-ai/examples\"\u003eJina examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jina-ai/jinabox.js\"\u003eJinabox.js\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy \u003ca href=\"https://www.linkedin.com/in/joanfontanalsmartinez/\"\u003eJoan Fontanals Martínez\u003c/a\u003e on October 2, 2020.\u003c/p\u003e\n","tags":["cross-modal"]}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"Cross-modal-Search-with-Jina"},"buildId":"KJvUyrCoh0MXs5cG--Qn6","runtimeConfig":{},"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-8683bd742a84c1edd48c.js"></script><script src="/_next/static/chunks/webpack-8f6366ea4b5fafd77e96.js" async=""></script><script src="/_next/static/chunks/framework-94f262366e752bd48d82.js" async=""></script><script src="/_next/static/chunks/commons-4667b98b62a60d7c050b.js" async=""></script><script src="/_next/static/chunks/main-9a4182dc8e7736869bf9.js" async=""></script><script src="/_next/static/chunks/pages/_app-afcb035e0413b2214f0f.js" async=""></script><script src="/_next/static/chunks/596-13cdde2fb793b77184ee.js" async=""></script><script src="/_next/static/chunks/522-e45333ebcf36799b860c.js" async=""></script><script src="/_next/static/chunks/373-350caaa5387e56e8b807.js" async=""></script><script src="/_next/static/chunks/93-1a5bac9fdf2b2295fba2.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0d67fc13dbd9e7ddf27f.js" async=""></script><script src="/_next/static/KJvUyrCoh0MXs5cG--Qn6/_buildManifest.js" async=""></script><script src="/_next/static/KJvUyrCoh0MXs5cG--Qn6/_ssgManifest.js" async=""></script></body></html>
<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.80</generator><lastBuildDate>Wed, 20 Mar 2024 15:06:36 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media]]></title><description><![CDATA[See how PromptPerfect can level up your LLM prompting and optimize your email and social media marketing campaigns.]]></description><link>https://jina.ai/news/click-worthy-content-with-promptperfect-ai-marketing-for-newsletters-and-social-media/</link><guid isPermaLink="false">65facf57502fd000011c66d5</guid><category><![CDATA[Knowledge Base]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Wed, 20 Mar 2024 15:00:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/PP-for-marketers.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/PP-for-marketers.jpg" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"><p>Generative AI and Large Language Models (LLMs) like GPT-4 have revolutionized digital marketing content, offering unparalleled speed and scale. However, as more companies and prompt engineers jump on the Generative AI bandwagon, the true magic lies not in the tool itself but in how we wield it. Think of LLM prompts as magic spells: a poorly crafted prompt yields mediocre results, akin to a novice wizard&apos;s hesitant incantation. In contrast, a well-optimized prompt can unleash the full potential of the LLM, mirroring the prowess of a master sorcerer commanding the elements.</p><p>But you don&apos;t need to be a dedicated prompt engineer to conjure up an effective prompt. PromptPerfect Interactive is the wand that lets <em>anyone</em> fine-tune their spells, ensuring every piece of content, from social media posts to newsletters, is not just created but perfectly crafted to captivate and engage customers.</p><p>To help make the AI marketing magic happen, we recently launched <a href="https://jina.ai/news/get-more-with-promptperfect-improved-subscription-choices-cutting-edge-interactive-optimizer/?ref=jina-ai-gmbh.ghost.io">PromptPerfect&apos;s new interactive feature</a>, marking a significant evolution in our platform.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/get-more-with-promptperfect-improved-subscription-choices-cutting-edge-interactive-optimizer?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer</div><div class="kg-bookmark-description">More cost-effective monthly subscription models and brand new interactive optimizer in PromptPerfect&#x2019;s latest release.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"><span class="kg-bookmark-publisher">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/3.jpg" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"></div></a></figure><p>&quot;Interactive&quot; revolutionizes content generation with LLMs. Two components lie at the heart of this feature: a Dedicated Assistant and a Powerful Optimizer. The Assistant acts as an AI companion, attuned to your specific needs, assisting in crafting effective prompts that make content generation effortless. Simultaneously, the Optimizer engineers your LLM prompts to ensure optimal results, boosting the effectiveness of your creative efforts. Designed to simplify and enhance AI-powered content creation and problem-solving, the Interactive feature makes it more intuitive and accessible to generate compelling narratives, devise innovative solutions, or boost productivity. With PromptPerfect Interactive, elevating your email marketing and social media becomes not just a possibility but a seamless reality.</p><h2 id="content-creation-made-easy">Content Creation Made Easy</h2><h3 id="crafting-messages-with-precision">Crafting Messages with Precision</h3><p>Marketers can dial in the specifics of their message, making sure that every word speaks directly to their audience&apos;s desires and interests. This makes it easier to generate content that not only fits your brand&apos;s voice but also addresses the nuances of what your customer expects and appreciates, whether that&apos;s in the tone, style, or substance of the message.</p><h3 id="tailoring-for-your-customers">Tailoring for Your Customers</h3><p>The power of a well-optimized LLM prompt goes beyond just generating text&#x2014;it&apos;s about creating a connection. By leveraging PromptPerfect to tailor prompts, you can produce more personalized, relevant content. This personal touch is what turns general information into engaging stories that resonate with potential customers, encouraging interaction and building stronger relationships through social media marketing and newsletters.</p><h2 id="streamlining-campaigns">Streamlining Campaigns</h2><h3 id="enhancing-productivity">Enhancing Productivity</h3><p>The Interactive feature acts as a force multiplier for marketing efforts, streamlining the content creation process to allow more time for strategic planning and execution. By transforming the efficiency with which content is produced, marketers can reallocate their focus towards fine-tuning campaigns and analyzing performance, ultimately driving better outcomes.</p><h3 id="differentiating-your-brand">Differentiating Your Brand</h3><p>In the crowded digital space, brand awareness out is more important than ever. With the new PromptPerfect update, you can ensure your marketing campaign is not just seen but remembered. Optimizing LLM prompts to produce unique and compelling content helps brands rise above the noise, making every social media post and newsletter an opportunity to showcase what makes them special.</p><h2 id="making-the-magic-happen-promptperfect-in-action">Making the Magic Happen: PromptPerfect in action</h2><p>Let&apos;s see how Lisa, our marketing manager at Jina AI, uses PromptPerfect Interactive to craft a compelling newsletter.</p><p>There are three members in this conversation:</p><ul><li><strong>&#x1F9D1;&#x200D;&#x1F9B0; Jina AI&apos;s senior marketing manager: </strong>Lisa herself.</li><li><strong>&#x1F916; PromptPerfect&apos;s Optimizer: </strong>An advanced AI tool that fine-tunes your prompts for optimal results.</li><li><strong>&#x1F4AC; PromptPerfect&apos;s Assistant: </strong>An LLM companion that understands your needs and helps you craft effective prompts.</li></ul><hr><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F9D1;&#x200D;&#x1F9B0;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Lisa</strong></b><br><br>We are creating a new feature for Jina AI&apos;s PromptPerfect, called PromptPerfect Interactive.<br><br>Feature description: The feature is crafted to empower users to generate text and images tailored to a multitude of use cases, ranging from YouTube scripts to AI art. The hallmark of this tool is its integrated prompt optimizer chatbot, which not only refines prompts for optimal results but also serves as a collaborative partner. Users can invoke the assistant chat to produce creative content, leveraging the familiar conversational interface to generate rich text and vivid images. The prompt optimizer chat bot is a revolutionary tool that assists users in refining and brainstorming prompts, ensuring that the AI understands the user&apos;s intent with precision. Beyond optimization, the prompt optimizer chatbot acts as a brainstorming ally, guiding users towards crafting prompts that yield the most effective outcomes.<br><br>I will need a prompt for generating a marketing email for the users of PromptPerfect, to announce PromptPerfect Interactive.</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>Here is a prompt you can use to generate a marketing email announcing PromptPerfect Interactive to PromptPerfect users:</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>As a marketing specialist at Jina AI, craft a compelling email to announce the launch of PromptPerfect Interactive, an exciting new feature for PromptPerfect. The email should be written in an engaging and enthusiastic tone, highlighting the key features and benefits of PromptPerfect Interactive, as described below:<br><br>```<br>The feature is crafted to empower users to generate text and images tailored to a multitude of use cases, ranging from YouTube scripts to AI art. The hallmark of this tool is its integrated prompt optimizer chatbot, which not only refines prompts for optimal results but also serves as a collaborative partner. Users can invoke the assistant chat to produce creative content, leveraging the familiar conversational interface to generate rich text and vivid images. The prompt optimizer chat bot is a revolutionary tool that assists users in refining and brainstorming prompts, ensuring that the AI understands the user&apos;s intent with precision. Beyond optimization, the prompt optimizer chatbot acts as a brainstorming ally, guiding users towards crafting prompts that yield the most effective outcomes.<br>```<br><br>The email should be structured as follows:<br>1. An attention-grabbing subject line that creates excitement about the new feature launch<br>2. A brief introduction that highlights the user&apos;s familiarity with PromptPerfect and the team&apos;s commitment to continually improving their experience<br>3. A detailed overview of PromptPerfect Interactive, focusing on its key features and how it will enhance the user&apos;s prompt engineering workflow<br>4. A clear call-to-action encouraging users to try out PromptPerfect Interactive and share their feedback<br>5. A closing statement expressing gratitude for the user&apos;s continued support and trust in Jina AI&apos;s products<br><br>Ensure that the email is concise yet informative, with a conversational and relatable tone that resonates with PromptPerfect&apos;s user base.</div></div><div class="kg-card kg-callout-card kg-callout-card-purple"><div class="kg-callout-emoji">&#x1F4AC;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Assistant</strong></b><br><br>Subject: Unleash Your Creativity Like Never Before with PromptPerfect Interactive &#x1F680;<br><br>Dear Creative Minds,<br><br>As a member of the PromptPerfect family, you&apos;ve witnessed firsthand how we&apos;re transforming the way you create content. Your passion inspires us daily, and we&apos;ve taken your feedback to heart. Today, we&apos;re thrilled to lift the veil on a feature that&apos;s been brewing with innovation and creativity&#x2014;PromptPerfect Interactive!<br><br>&#x1F31F; Introducing PromptPerfect Interactive: Where brilliance meets intuitiveness! This groundbreaking tool is meticulously designed to fuel your imagination and streamline your content generation. Whether it&apos;s captivating YouTube scripts or stunning AI-generated art, PromptPerfect Interactive is your go-to resource for content mastery.<br><br>&#x1F50D; Key Features:<br><br>&#x2022; Integrated Prompt Optimizer Chatbot: Struggle with prompts no more! Our chatbot collaborates with you to refine and perfect your prompts, ensuring the AI captures the essence of your vision.<br>&#x2022; Conversational Interface: Creativity should be a conversation, not a command. Engage with our assistant in a natural chat flow to unleash powerful text and vibrant images.<br>&#x2022; Brainstorming Buddy: Never face creative blocks again. Our chatbot not only refines but brainstorm ideas with you, sparking content that resonates and captivates.<br><br>PromptPerfect Interactive is revolutionizing the prompt engineering process, making it more human, more collaborative, and infinitely more powerful. It&apos;s not just an enhancement to your workflow; it&apos;s the partner you&#x2019;ve always needed in your creative endeavors.<br><br>&#x1F64C; Ready to dive in? We invite you to experience the magic of PromptPerfect Interactive and see what new heights of creativity you can reach. Get started now and let your imagination soar. Don&#x2019;t forget to share your masterpieces and thoughts with us&#x2014;your input is the compass that guides our innovation.<br><br>Thank you for choosing Jina AI and for your unwavering trust in our mission to redefine creativity. Together, let&apos;s continue to push the boundaries and craft the future of content creation.<br><br>With Warmest Regards,<br><br>The PromptPerfect Team at Jina AI<br><br>P.S. We&apos;re not stopping here&#x2014;there&apos;s a universe of possibilities we&apos;re excited to explore with you. Stay tuned for more features that will tickle your creative spirit! &#x1F30C;&#x2728;</div></div><h2 id="elevate-your-marketing-with-promptperfect">Elevate Your Marketing with PromptPerfect</h2><p>Don&apos;t miss the opportunity to be like Lisa and transform your digital marketing strategy with PromptPerfect. Sign up today to enjoy a 7-day free trial on a paid plan and receive 40% off if you subscribe within 24 hours of signing up. Elevate your content, streamline your workflow, and achieve unprecedented engagement with your audience.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Optimize Your Prompts to Perfection</div><div class="kg-bookmark-description">Unlock prompt optimization for models like GPT-4, ChatGPT and Midjourney. Deploy prompts as services with our free hosting.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/favicon-128x128.png" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"><span class="kg-bookmark-author">Optimize Your Prompts to Perfection</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"></div></a></figure><p>Join our community on Discord to connect with other generative AI and marketing professionals, share insights, and discover how they&apos;re leveraging PromptPerfect to revolutionize their marketing efforts.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4632 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.ghost.org/v5.0.0/images/link-icon.svg" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Click-Worthy Content with PromptPerfect: AI Marketing for Newsletters and Social Media"></div></a></figure><p>Take the first step towards transforming your marketing strategy &#x2014; experience the power of PromptPerfect Interactive now.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.24.1 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-24-1-update/</link><guid isPermaLink="false">65faf82c502fd000011c6727</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Wed, 20 Mar 2024 14:54:53 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3241">Release Note (<code>3.24.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark-1.jpg" alt="Jina 3.24.1 Update"><p>This release contains 3 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.24.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.24.1 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.24.1) Release time: 2024-03-20 09:35:52 This release contains 3 bug fixes.
&#x1F41E; Bug Fixes
Fix Flow deployment of Model with float field (#6155)
When serving a Flow with HTTP, serving&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.24.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/92970271b1d10e6cc126cf3212d3bf71b76b761180be72fe7c2d8a133ba9fe4e/jina-ai/jina/releases/tag/v3.24.1" alt="Jina 3.24.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-flow-deployment-of-model-with-float-field-6155">Fix Flow deployment of Model with <code>float</code> field (<a href="https://github.com/jina-ai/jina/pull/6155?ref=jina-ai-gmbh.ghost.io">#6155</a>)</h3><p>When serving a Flow with <code>HTTP</code>, serving failed because the model reconstructed in the Gateway interpreted the <code>float</code> field as an array. Now the model served in the Gateway uses the correct schema.</p><h3 id="expose-only-invocations-endpoint-when-serving-withprovider-sagemaker-6152">Expose only <code>/invocations</code> endpoint when serving with <code>--provider SAGEMAKER</code> (<a href="https://github.com/jina-ai/jina/pull/6152?ref=jina-ai-gmbh.ghost.io">#6152</a>)</h3><p>When exposing an Executor with <code>SAGEMAKER</code> as the <code>provider</code> argument, only the <code>/invocations</code> endpoint should be used. However, previously all other endpoints were also exposed. This is now fixed.</p><h3 id="optional-parameters-when-typed-parameter-has-all-default-fields-6152">Optional parameters when typed parameter has all default fields (<a href="https://github.com/jina-ai/jina/pull/6152?ref=jina-ai-gmbh.ghost.io">#6152</a>)</h3><p>When using a strongly typed <code>parameters</code> argument with the Executor endpoint, even if the <code>parameters</code> model type had all fields with default values and not required, you still needed to pass an empty <code>parameters={}</code> field to the call when serving via <code>HTTP</code>. With this change, you no longer need to pass this empty dictionary.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Get More with PromptPerfect: Improved Subscription Choices & Cutting-Edge Interactive Optimizer]]></title><description><![CDATA[More cost-effective monthly subscription models and brand new interactive optimizer in PromptPerfect's latest release.]]></description><link>https://jina.ai/news/get-more-with-promptperfect-improved-subscription-choices-cutting-edge-interactive-optimizer/</link><guid isPermaLink="false">65f81e478ff065000146dbfd</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 18 Mar 2024 15:00:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/3.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/3.jpg" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"><p>We&#x2019;re thrilled to announce a significant update to PromptPerfect, marking a major leap forward in our journey to enhance your AI-powered content creation experience. This update is not just about changing how you pay but also about revolutionizing how you interact with AI to generate content and solve complex tasks. Here&#x2019;s everything you need to know about the latest enhancements to PromptPerfect.</p><h2 id="our-new-monthly-subscription-model">Our New Monthly Subscription Model</h2><p>We&#x2019;re evolving from a credit-based system to a more flexible and value-packed monthly subscription model. This new approach is designed to fit the varied needs of our users, ensuring everyone has access to our powerful AI tools without worrying about running out of credits.</p><h3 id="subscription-plans-tailored-for-you">Subscription Plans Tailored for You</h3><ul><li><strong>Standard Plan ($20/month):</strong> Ideal for daily users, offering extensive daily limits across all features.</li><li><strong>Premium Plan ($100/month):</strong> For the power users requiring higher daily usage limits to accommodate their intensive projects.</li></ul><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">You can try the plans for free for seven days to be sure they&apos;re the right match for you. Plus, sign up for a plan within 24 hours after your next login, and get 40% off!</div></div><h3 id="more-benefits-for-less">More Benefits for Less</h3><p>The transition to a subscription model means more accessible usage at a more affordable price point. Our goal is to provide an unmatched value that empowers you to unleash your creativity and productivity without limitations.</p><h3 id="loyalty-to-our-existing-users">Loyalty to Our Existing Users</h3><p>For our existing users who have supported us through the purchase of credits, your credits will remain valid. While new users will be onboarded into our subscription model, we assure our existing user base that their current credits are still good to use.</p><h2 id="the-interactive-feature-your-ai-powered-companion">The &quot;Interactive&quot; Feature: Your AI-Powered Companion</h2><p>With this update, we&#x2019;re excited to introduce the &#x201C;Interactive&#x201D; feature, a groundbreaking addition that transforms how you generate content and tackle complex tasks. The Interactive feature is built on a dual approach:</p><ul><li><strong>Dedicated Assistant:</strong> An AI companion that understands your needs and helps you craft effective prompts, making the content generation process as seamless as possible.</li><li><strong>Powerful Optimizer:</strong> An advanced tool that fine-tunes your prompts for optimal results, ensuring that your creative and productive endeavors are more effective than ever.</li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Optimize Your Prompts to Perfection</div><div class="kg-bookmark-description">Unlock prompt optimization for models like GPT-4, ChatGPT and Midjourney. Deploy prompts as services with our free hosting.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"></div></a></figure><p>This feature is designed to make AI-powered content generation and task-solving not only easier but also more intuitive. Whether you&#x2019;re looking to create compelling narratives, design innovative solutions, or simply enhance your productivity, the Interactive feature is here to guide you through.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/-P1a2o6hVc8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="PromptPerfect-Interactive"></iframe></figure><h2 id="explore-the-possibilities-use-cases-for-every-creative-need">Explore the Possibilities: Use Cases for Every Creative Need</h2><p>The versatility of PromptPerfect&#x2019;s new features, including our innovative Interactive feature and flexible subscription plans, opens up a world of possibilities for various users. Here&#x2019;s how different professionals and creatives can leverage PromptPerfect to transform their work:</p><h3 id="marketers">Marketers</h3><p>For marketers aiming to capture their audience&apos;s attention, PromptPerfect simplifies the creation of engaging marketing emails and compelling feature descriptions. Our AI tools can help craft personalized messages that resonate with your target demographic, streamline campaign creation, and ensure your products stand out in a crowded marketplace. Let&#x2019;s look at using PromptPerfect to write some marketing emails:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F9D1;&#x200D;&#x1F9B0;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">User</strong></b><br><br>We are creating a new feature for Jina AI&apos;s PromptPerfect, called PromptPerfect Interactive.<br><br>Feature description: The feature is crafted to empower users to generate text and images tailored to a multitude of use cases, ranging from YouTube scripts to AI art. The hallmark of this tool is its integrated prompt optimizer chatbot, which not only refines prompts for optimal results but also serves as a collaborative partner. Users can invoke the assistant chat to produce creative content, leveraging the familiar conversational interface to generate rich text and vivid images. The prompt optimizer chat bot is a revolutionary tool that assists users in refining and brainstorming prompts, ensuring that the AI understands the user&apos;s intent with precision. Beyond optimization, the prompt optimizer chatbot acts as a brainstorming ally, guiding users towards crafting prompts that yield the most effective outcomes.<br><br>I will need a prompt for generating a marketing email for the users of PromptPerfect, to announce PromptPerfect Interactive.</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>Here is a prompt you can use to generate a marketing email announcing PromptPerfect Interactive to PromptPerfect users:</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>As a marketing specialist at Jina AI, craft a compelling email to announce the launch of PromptPerfect Interactive, an exciting new feature for PromptPerfect. The email should be written in an engaging and enthusiastic tone, highlighting the key features and benefits of PromptPerfect Interactive, as described below: <br><br>```<br> The feature is crafted to empower users to generate text and images tailored to a multitude of use cases, ranging from YouTube scripts to AI art. The hallmark of this tool is its integrated prompt optimizer chatbot, which not only refines prompts for optimal results but also serves as a collaborative partner. Users can invoke the assistant chat to produce creative content, leveraging the familiar conversational interface to generate rich text and vivid images. The prompt optimizer chat bot is a revolutionary tool that assists users in refining and brainstorming prompts, ensuring that the AI understands the user&apos;s intent with precision. Beyond optimization, the prompt optimizer chatbot acts as a brainstorming ally, guiding users towards crafting prompts that yield the most effective outcomes. <br>```<br><br>The email should be structured as follows: <br>1. An attention-grabbing subject line that creates excitement about the new feature launch <br>2. A brief introduction that highlights the user&apos;s familiarity with PromptPerfect and the team&apos;s commitment to continually improving their experience<br>3. A detailed overview of PromptPerfect Interactive, focusing on its key features and how it will enhance the user&apos;s prompt engineering workflow<br>4. A clear call-to-action encouraging users to try out PromptPerfect Interactive and share their feedback<br>5. A closing statement expressing gratitude for the user&apos;s continued support and trust in Jina AI&apos;s products<br><br>Ensure that the email is concise yet informative, with a conversational and relatable tone that resonates with PromptPerfect&apos;s user base.</div></div><h3 id="youtubers">YouTubers</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F9D1;&#x200D;&#x1F9B0;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">User</strong></b><br><br>We are creating a new feature for Jina AI&apos;s PromptPerfect, called PromptPerfect Interactive.<br><br>Feature description:<br><br>The feature is crafted to empower users to generate text and images tailored to a multitude of use cases, ranging from YouTube scripts to AI art.<br>The hallmark of this tool is its integrated prompt optimizer chatbot, which not only refines prompts for optimal results but also serves as a collaborative partner.<br>Users can invoke the assistant chat to produce creative content, leveraging the familiar conversational interface to generate rich text and vivid images.<br>The prompt optimizer chat bot is a revolutionary tool that assists users in refining and brainstorming prompts, ensuring that the AI understands the user&apos;s intent with precision.<br>Beyond optimization, the prompt optimizer chatbot acts as a brainstorming ally, guiding users towards crafting prompts that yield the most effective outcomes. <br><br>I will need a prompt for generating a YouTube video script that announces this new feature.</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>Here&#x2019;s a prompt you can use to generate a YouTube video script announcing the new PromptPerfect Interactive feature.</div></div><div class="kg-card kg-callout-card kg-callout-card-green"><div class="kg-callout-emoji">&#x1F916;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Optimizer</strong></b><br><br>As a YouTube content creator and AI enthusiast, create an engaging video script announcing Jina AI&apos;s new PromptPerfect Interactive feature.<br><br>Introduce PromptPerfect Interactive as a groundbreaking tool that empowers users to generate tailored text and images for various use cases, from YouTube scripts to AI art.<br><br>Highlight the following key aspects of the feature:<br><br>- Integrated prompt optimizer chatbot that refines prompts for optimal results<br>- Acts as a collaborative partner for prompt engineers to craft superior prompts<br>- Enables users to generate creative content using a familiar conversational interface<br>- Assists users in brainstorming and refining prompts to ensure the AI understands and responds with precision<br><br>The script should be informative, engaging, and persuasive, encouraging viewers to try out the new feature.<br><br>Incorporate a brief demonstration of how PromptPerfect Interactive works, showcasing its user-friendly interface and the process of refining a prompt with the help of the optimizer chatbot.<br><br>Emphasize the benefits of using PromptPerfect Interactive, such as saving time, improving the quality of generated content, and unlocking new creative possibilities.<br><br>Close the script with a strong call-to-action, urging viewers to experience the power of PromptPerfect Interactive for themselves and providing a link to access the feature.<br><br>The script should be concise, aiming for a video length of 2-3 minutes, and use a conversational yet professional tone that resonates with the target audience of AI enthusiasts and content creators.</div></div><h2 id="a-new-chapter-for-promptperfect">A New Chapter for PromptPerfect</h2><p>These updates represent our commitment to providing a platform that not only meets the current needs of our users but also anticipates their future demands. The introduction of the monthly subscription model alongside the Interactive feature is a testament to our dedication to innovation and user satisfaction.</p><p>We invite you to join us in this exciting new chapter for PromptPerfect. Embrace the enhanced flexibility, value, and creativity that our updates bring to your AI-powered endeavors. Here&#x2019;s to creating, exploring, and achieving more with PromptPerfect!</p><h2 id="ready-to-elevate-your-creative-journey">Ready to Elevate Your Creative Journey?</h2><p>Join us at PromptPerfect and unlock the full potential of your creative and professional endeavors. With our new subscription model and the groundbreaking Interactive feature, you have everything you need to take your projects to the next level. Sign up for PromptPerfect today and start exploring the endless possibilities our platform offers.</p><h2 id="connect-with-our-vibrant-community">Connect with Our Vibrant Community</h2><p>Don&#x2019;t miss out on the chance to be part of our growing community. Join our Discord community to connect with fellow creatives, share your projects, get exclusive insights, and receive support from our team. It&#x2019;s the perfect place to learn, grow, and be inspired by like-minded individuals who share your passion for creativity and innovation.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Optimize Your Prompts to Perfection</div><div class="kg-bookmark-description">Unlock prompt optimization for models like GPT-4, ChatGPT and Midjourney. Deploy prompts as services with our free hosting.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4615 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.ghost.org/v5.0.0/images/link-icon.svg" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Get More with PromptPerfect: Improved Subscription Choices &amp; Cutting-Edge Interactive Optimizer"></div></a></figure><p>Embrace the future of content creation with PromptPerfect and discover how our AI-powered tools can transform your work. We can&#x2019;t wait to see what you&#x2019;ll create!</p>]]></content:encoded></item><item><title><![CDATA[How to Build Article Recommendations with Jina Reranker API Only]]></title><description><![CDATA[You can build an article recommendation system with just the Jina Reranker API—no pipeline, no embeddings, no vector search, only reranking. Find out how in 20 lines of code.]]></description><link>https://jina.ai/news/how-to-build-article-recommendations-with-jina-reranker-api-only/</link><guid isPermaLink="false">65f53dc48ff065000146db23</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Sun, 17 Mar 2024 05:38:36 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Heading--1-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Heading--1-.png" alt="How to Build Article Recommendations with Jina Reranker API Only"><p>We introduced the <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io">Reranker API</a> two weeks ago, establishing it as a leading reranking solution in the market. Jina Reranker outperforms <a href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io">popular baselines in various benchmarks, demonstrating a significant increase of up to +33% in hit rate over BM25 results.</a> While the performance is impressive, what really excites me is the potential of the Reranker API. Its straightforward interface allows input of a query-doc list and outputs directly the reranked top-k results. This means that, in theory, one could build a search or recommendation system using solely the Reranker&#x2014;eliminating the need for BM25, embeddings, vector databases, or any pipelines, thus achieving end-to-end functionality.</p><p>This concept intrigued me so much that I felt compelled to experiment it. So there you go: now navigating to any news page of our website, such as the one you&apos;re currently reading, press the <code><strong>@</strong></code> key and click on the &quot;get top 5 related articles&quot; button, you&apos;ll receive the five articles most relevant to the current one within about 5 seconds, using the <code>jina-reranker-v1</code> model (slightly longer for the <code>jina-colbert-v1</code> model). All computations are performed online and managed entirely by the Reranker API. Below is a video demonstration of how it functions:</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/03/recommender_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/03/recommender.mp4" poster="https://img.spacergif.org/v1/1174x796/0a/spacer.png" width="1174" height="796" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/03/recommender_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:33</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">To run this demo, you will need an API key with enough tokens left. If you exhaust your quota and cannot run the demo, you can generate a new key at </span><a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io" target="_new"><span style="white-space: pre-wrap;">https://jina.ai/reranker</span></a><span style="white-space: pre-wrap;">. Each new key comes with 1 million free tokens. </span></p></figcaption>
        </figure><h2 id="implementation">Implementation</h2><p>The implementation is very simple: to find the most related articles of an given article on <code>jina.ai/news/</code>, we use the article currently being read <strong>as the query</strong> and all other 230+ articles (using their full-text!) on our news site <strong>as the documents</strong>, excluding the current one of course. Then we send this $(q, d_1, d_2, \cdots, d_{230})$ as the payload to the Reranker API. Once the response is received, we use the sorted document index to display the results. Thus, the underlying code is as follows:</p><pre><code class="language-js">const getRecommendedArticles = async () =&gt; {
  
  const query = `${currentNews.title} ${currentNews.excerpt}`;
  const docs = newsStore.allBlogs.filter((item) =&gt; item.slug !== currentNews.slug);
  
  const data = {
    model: modelName,
    query: query,
    documents: docs,
    top_n: 5,
  }

  const rerankUrl = &apos;https://api.jina.ai/v1/rerank&apos;;
  const headers = {
    &apos;Content-Type&apos;: &apos;application/json&apos;,
    Authorization: `Bearer ${apiKey}`,
  };
  const modelName = &apos;jina-reranker-v1-base-en&apos;;
  
  const res = await fetch(rerankUrl, {
    method: &apos;POST&apos;,
    headers: headers,
    body: JSON.stringify(data),
  });

  const resp = await res.json();

  const topKList = resp.results.map((item) =&gt; {
    return docs[item.index];
  });

  console.log(topKList);
}</code></pre><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="How to Build Article Recommendations with Jina Reranker API Only"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="How to Build Article Recommendations with Jina Reranker API Only"></div></a><figcaption><p><span style="white-space: pre-wrap;">To obtain an API key, simply visit our Reranker API page and navigate to the API section. If you already possess an API key from our Embedding API, you can reuse it here.</span></p></figcaption></figure><p>And just like that, you&apos;ll see the results, which are quite promising for a first iteration, especially considering that the implementation process takes roughly 10 minutes.</p><p>While readers may have concerns about this implementation, some critiques may be overthought, while others may be valid:</p><ul><li><strong>Concerns regarding overly long full-text and the necessity of chunking might be overthinking it</strong>: the <code>jina-reranker-v1</code> model can process queries up to 512 in length and documents of arbitrary length, while the <code>jina-colbert-v1</code> model can handle up to 8192 for both queries and documents. Therefore, inputting the full text to the Reranker API is likely unnecessary. Both models efficiently manage long contexts, so there&apos;s no need for worry. Chunking, though possibly the most cumbersome and heuristic aspect of the embedding-vector-search-rerank pipeline, is less of an issue here. However, longer contexts do assume more tokens, which is something our API&apos;s paid users may need to consider. In this example, because we use the full-text of all 233 articles, one rerank query costs 300K+ tokens.</li><li><strong>The impact of raw versus cleaned data on quality</strong>. Adding data cleaning could indeed lead to improvements. For instance, we&apos;ve observed that simply removing HTML tags (i.e. <code>docs.map(item =&gt; item.html.replace(/&lt;[^&gt;]*&gt;?/gm, &apos;&apos;)</code> ) significantly enhances recommendation quality for the <code>jina-reranker-v1</code> model, though the effect is less pronounced for the <code>jina-colbert-v1</code> model. This suggests that our ColBERT model was trained to be more tolerant of noisy text than the <code>jina-reranker-v1</code> model.</li><li><strong>The influence of different query constructions on quality.</strong> In the above implementation, we directly used the title and excerpt of the current article as the query. Is this the optimal approach to construct the query? Would adding a prefix such as <code>&quot;What is the most related article to...&quot;</code> or <code>&quot;I&apos;ll tip you $20 if you recommend the best article,&quot;</code> similar to prompts used with large language models, be beneficial? This raises an interesting question, likely related to the training data distribution of the model, which we plan to explore further.</li><li>Building on the previous point about query construction, it would be intriguing to investigate the query&apos;s compositional abilities further, such as using a user&apos;s recent browsing history for personalized recommendations. It&apos;s particularly interesting to consider whether the system could understand not just positive examples in the query <strong>but also negative ones, </strong>e.g. <code>NOT_LIKE</code> ops, <code>&quot;Don&apos;t recommend me article like this&quot;</code> or <code>&quot;I want to see fewer like this&quot;</code>. We&apos;ll delve into this more in the next section.</li></ul><h2 id="empirical-study-on-query-writing">Empirical Study on Query Writing</h2><p>In our exploration of different query writing with the Jina Reranker API, focusing on the top-10 results, we conducted a qualitative assessment through human labelling (i.e. evaluated by ourselves), which makes sense as we have the full knowledge of all the content published on our website. The strategies in query writings we examined included:</p><ul><li>Using the article&apos;s Title, Excerpt, and a combination of Title + Excerpt.</li><li>Adopting &quot;Prompt&quot;-like instructions such as &quot;more like this,&quot; &quot;not like this,&quot; and &quot;what is the most closely related article?&quot;</li></ul><p>To test the reranker&apos;s efficacy, we selected two non-trivial articles as our query subjects, aiming to pinpoint the most relevant articles among our extensive catalog of over 200+ posts&#x2014;a challenge inspired by &quot;the needle in a haystack&quot; in LLMs. Below, we highlighted these &quot;needles&quot; in green for clarity.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/image.png" class="kg-image" alt="How to Build Article Recommendations with Jina Reranker API Only" loading="lazy" width="2000" height="758" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/03/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/03/image.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;"> The comprehensive results of our study are detailed </span><a href="https://docs.google.com/spreadsheets/d/1XDOXjP6IVK1hXk3gKkng_cOpTrmS9Hm1UKCGvW1wMxU/edit?usp=sharing&amp;ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">in this spreadsheet</span></a><span style="white-space: pre-wrap;">.</span></figcaption></figure><h2 id="summary">Summary</h2><p>Based on the test results, we&apos;ve made some observations and summaries:</p><ol><li>Combining the Title and Excerpt yields the best reranking results, with the Excerpt playing a significant role in enhancing reranking quality.</li><li>Incorporating &quot;prompt&quot;-like instructions does not lead to any improvement.</li><li>The reranker model currently does not effectively process positive or negative qualifiers. Terms such as &quot;more like&quot;, &quot;less like&quot;, or &quot;not like&quot; are not understandable by the reranker.</li></ol><p>The insights from points 2 and 3 offer intriguing directions for future enhancements of the reranker. We believe that enabling on-the-fly prompting to changing the sorting logic could significantly expand the reranker&apos;s capabilities, unlocking new potential applications such as personalized content curation/recommendation.</p>]]></content:encoded></item><item><title><![CDATA[Building RAG with Jina AI and SuperDuperDB]]></title><description><![CDATA[Jina Embeddings v2 are now integrated directly into SuperDuperDB, letting you skip the complexity of AI operations in your data-driven applications.]]></description><link>https://jina.ai/news/building-rag-with-jina-ai-and-superduperdb/</link><guid isPermaLink="false">65f04e788ff065000146da94</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Wed, 13 Mar 2024 15:00:44 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/16.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="superduperdb">SuperDuperDB</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/16.jpg" alt="Building RAG with Jina AI and SuperDuperDB"><p><a href="https://superduperdb.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">SuperDuperDB</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> directly into its data-driven AI operations framework. You can now use Jina AI&apos;s state-of-the-art embedding models with their groundbreaking 8k input context to work with your existing data stores via SuperDuperDB&apos;s integration libraries.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.superduperdb.com/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SuperDuperDB: Bring AI to your favorite database!</div><div class="kg-bookmark-description">Say goodbye to complex MLOps pipelines and specialized vector databases. Integrate and train AI directly with your preferred database, only using Python.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.superduperdb.com/apple-touch-icon.png" alt="Building RAG with Jina AI and SuperDuperDB"><span class="kg-bookmark-author">SuperDuperDB</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://staging.d3rw8vtufdccov.amplifyapp.com/images/new_superduperdb_share_image.png" alt="Building RAG with Jina AI and SuperDuperDB"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Building RAG with Jina AI and SuperDuperDB"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Building RAG with Jina AI and SuperDuperDB"></div></a></figure><p>To show you how, we have collaborated with SuperDuperDB on a tutorial creating a Retrieval-Augmented Generation (RAG) application that lets you query SQL databases in plain language.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.superduperdb.com/blog/rag-system-on-duckdb-using-jinaai-and-superduperdb/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Implementing a RAG System on DuckDB Using Jina AI and SuperDuperDB | SuperDuperDB documentation</div><div class="kg-bookmark-description">Querying your SQL database purely in human language</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://docs.superduperdb.com/img/favicon.ico" alt="Building RAG with Jina AI and SuperDuperDB"><span class="kg-bookmark-author">SuperDuperDB</span><span class="kg-bookmark-publisher">Anita Okoh</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://docs.superduperdb.com/img/superDuperDB_img.png" alt="Building RAG with Jina AI and SuperDuperDB"></div></a></figure><h2 id="jina-ai"><strong>Jina AI</strong></h2><p>Jina AI is committed to distributing reliable, affordable AI technologies that are easy to use and integrated with common frameworks. We value your feedback, and we&#x2019;d love to hear about your business needs and discuss how AI can work for you. </p><p>For more information about Jina AI&#x2019;s offerings and to contact us, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>Jina AI website</u></a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>community on Discord</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Building RAG with Jina AI and SuperDuperDB"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Building RAG with Jina AI and SuperDuperDB"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Building RAG with Jina AI and SuperDuperDB"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Building RAG with Jina AI and SuperDuperDB"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina 3.24.0 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-24-0-update/</link><guid isPermaLink="false">65eedc3c28f704000180e2dc</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 11 Mar 2024 10:30:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3240">Release Note (<code>3.24.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark.jpg" alt="Jina 3.24.0 Update"><p>This release contains 1 new feature.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.24.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v3.24.0 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.24.0) Release time: 2024-03-11 09:48:35 This release contains 1 new feature.
&#x1F195; Features
Support provider endpoint in Jina Executor (#6149)
This release adds the --provider-endpoi&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.24.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/4744775b0fe508673f9bad61ba6ed377883f4ebe462333dd41f8fadc4cc350d8/jina-ai/jina/releases/tag/v3.24.0" alt="Jina 3.24.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="support-provider-endpoint-in-jina-executor-6149">Support provider endpoint in Jina Executor (<a href="https://github.com/jina-ai/jina/pull/6149?ref=jina-ai-gmbh.ghost.io">#6149</a>)</h3><p>This release adds the&#xA0;<code>--provider-endpoint</code>&#xA0;flag, which can only be used in conjunction with the&#xA0;<code>--provider</code>&#xA0;flag. The&#xA0;<code>--provider-endpoint</code>&#xA0;flag takes as a parameter a named endpoint supported in the Executor and sets the endpoint of the provider to map to that Executor endpoint.</p><p>At present, SageMaker is the only available provider option, so using the parameters&#xA0;<code>--provider SAGEMAKER --provider-endpoint &lt;named endpoint&gt;</code>&#xA0;will make the SageMaker&#xA0;<code>invocations</code>&#xA0;endpoint map to the named endpoint of the Executor.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Zac Li (<a href="https://github.com/zac-li?ref=jina-ai-gmbh.ghost.io">@zac-li</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Precise RAG with Jina Reranker and LlamaIndex]]></title><description><![CDATA[Just Rerank It! Jina Reranker and LlamaIndex take your RAG up to the next level.]]></description><link>https://jina.ai/news/precise-rag-with-jina-reranker-and-llamaindex/</link><guid isPermaLink="false">65e97909b22368000152a4c7</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Francesco Kruk]]></dc:creator><pubDate>Thu, 07 Mar 2024 14:00:01 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/banner_Reranker_-LlamaIndex.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/banner_Reranker_-LlamaIndex.png" alt="Precise RAG with Jina Reranker and LlamaIndex"><p>While <a href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><u>bi-encoder models</u></a> such as <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings</u></a> can quickly retrieve many matching documents from a database of pre-computed embeddings, reranker models can refine this set by using a slower but more precise approach of cross-encoding users&#x2019; queries and retrieved documents. Jina AI has <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><u>released our first reranker model</u></a>, <code>jina-reranker-v1-base-en</code>, and, in this article, we&#x2019;ll present in-depth reasoning for why a reranker is essential to optimize RAG accuracy and how to get started building a state-of-the-art RAG system using <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings</u></a>/<a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>Reranker</u></a>,<a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"> <u>LlamaIndex</u></a>, and the <code>Mixtral-8x7B-Instruct-v0.1</code> language model (<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io"><u>hosted on HuggingFace</u></a>).</p><p>You&#x2019;ll need:</p><ul><li>A <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>combined key for Jina Embeddings and Reranker API</u></a>.</li><li>A <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"><u>HuggingFace account and token</u></a>.</li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><p>Since the Jina Embeddings and Reranker models as well as Mixtral run remotely and are accessed via a RESTful API, you won&#x2019;t need any special hardware.</p><h2 id="what-is-a-reranker">What is a reranker?</h2><p>Before continuing with the tutorial, it is important to highlight what rerankers are in the first place. For a full understanding of what a reranker is and why Jina Reranker V1 is the best choice for you, we encourage you to read our Jina Reranker V1 release post before continuing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class="kg-bookmark-description">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><p>In summary, rerankers are cross-encoder models that take as input a <em>document-query</em> pair, and emit a combined relevance score for that input pair. Using rerankers, users can sort documents from most to least relevant for a given query.</p><h2 id="why-use-jina-reranker-v1-base-en">Why use <code>jina-reranker-v1-base-en</code>?</h2><p>Reranking provides much more relevant information than using solely an embedding model. In our <a href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/?ref=jina-ai-gmbh.ghost.io"><u>model release post</u></a>, we demonstrated that Jina Reranker stands out compared to its open- and closed-source competitors and it can improve search systems by <strong>8% in hit rate and 33% in </strong><a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank?ref=jina-ai-gmbh.ghost.io"><strong><u>mean reciprocal rank</u></strong></a>.</p><p>This has a direct impact on the quality of responses obtained through the applied RAG solution. With the theory backing up this claim, we&#x2019;ll show you a practical example so you can see with your own eyes what effect Jina Reranker has on a RAG pipeline built with LlamaIndex.</p><h2 id="before-we-start-a-note-on-llamaindex-node-postprocessors">Before we start: A Note on LlamaIndex Node-Postprocessors</h2><p>Node-postprocessors in <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html?ref=jina-ai-gmbh.ghost.io"><u>LlamaIndex</u></a> are modules that transform or filter nodes after retrieval and before response synthesis within a query engine. As part of this package, LlamaIndex offers both built-in options as well as an API for custom additions.</p><p><strong>Jina Reranker has now been </strong><a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank.html?ref=jina-ai-gmbh.ghost.io"><strong><u>integrated</u></strong></a><strong> into LlamaIndex as a node postprocessor</strong>. To increase response accuracy, retrieved nodes are re-ordered based on relevance to the query, and the top <strong><em>N</em></strong> nodes are returned.</p><h2 id="follow-along-on-google-colab">Follow along on Google Colab</h2><p>This tutorial has an accompanying notebook that you can run on <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/JinaRerank.ipynb?ref=jina-ai-gmbh.ghost.io"><u>Google Colab</u></a> or locally.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/JinaRerank.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">llama_index/docs/examples/node_postprocessor/JinaRerank.ipynb at main &#xB7; run-llama/llama_index</div><div class="kg-bookmark-description">LlamaIndex is a data framework for your LLM applications - run-llama/llama_index</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Precise RAG with Jina Reranker and LlamaIndex"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">run-llama</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/e7094a0ef89361df6896a5e036ff16eba94b83aa69186423d6b06608a46e7707/run-llama/llama_index" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><h3 id="the-dataset-2024-nike-kids-product-catalog">The dataset: 2024 Nike Kids Product Catalog</h3><p>To showcase Jina Reranker&#x2019;s performance increase for RAG applications, we&#x2019;ve chosen the <a href="https://niketeam-asset-download.nike.net/catalogs/2024/2024_Nike%20Kids_02_09_24.pdf?cb=09302022&amp;ref=jina-ai-gmbh.ghost.io"><u>2024 Nike Kids Product Catalog</u></a> as our dataset. The document contains a structured set of kids&#x2019; products offered by Nike in 2024. We selected this dataset as it showcases the effect of using a reranker clearly and is relatable to most users.</p><h3 id="install-the-prerequisites">Install the prerequisites</h3><p>To install the requirements, run:</p><pre><code class="language-bash">pip install llama-index-postprocessor-jinaai-rerank
pip install llama-index-embeddings-jinaai
pip install llama-index
pip install llama-index-llms-huggingface
pip install &quot;huggingface_hub[inference]&quot;</code></pre><h3 id="access-mixtral-llm">Access Mixtral LLM</h3><p>To use the <code>Mixtral-8x7B-Instruct-v0.1</code> LLM, you need a<a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"> <u>HuggingFace</u></a> token.</p><pre><code class="language-Python">from llama_index.llms.huggingface import HuggingFaceInferenceAPI

hf_inference_api_key = &quot;&lt;your HuggingFace access token here&gt;&quot;

mixtral_llm = HuggingFaceInferenceAPI(
    model_name=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;,
    token=hf_inference_api_key,
)
</code></pre><h3 id="access-jina-embeddings-and-jina-reranker">Access Jina Embeddings and Jina Reranker</h3><p>To use our Jina Embeddings and Jina Reranker, you need a dedicated <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>API key</u></a>. Store it in a variable called <code>api_key</code> and call the Jina Embeddings model from LlamaIndex:</p><pre><code class="language-Python">from llama_index.embeddings.jinaai import JinaEmbedding

api_key = &quot;&lt;your Jina key here&gt;&quot;
jina_embeddings = JinaEmbedding(api_key=api_key)</code></pre><p>Similarly, you can call the Jina Reranker model. By setting the <code>top_n</code> parameter, you can decide how many of the most relevant documents to return in the final output. In this case, we set <code>top_n=2</code>:</p><pre><code class="language-Python">from llama_index.postprocessor.jinaai_rerank import JinaRerank

jina_rerank = JinaRerank(api_key=api_key, top_n=2)</code></pre><h3 id="download-the-2024-nike-kids-product-catalog">Download the 2024 Nike Kids Product Catalog</h3><p>To download the data, run the following code:</p><pre><code class="language-Python">from llama_index.core import SimpleDirectoryReader
import requests

url = &apos;&lt;https://niketeam-asset-download.nike.net/catalogs/2024/2024_Nike%20Kids_02_09_24.pdf?cb=09302022&gt;&apos;
response = requests.get(url)

with open(&apos;Nike_Catalog.pdf&apos;, &apos;wb&apos;) as f:
&#xA0;&#xA0;&#xA0;&#xA0;f.write(response.content)

reader = SimpleDirectoryReader(
&#xA0;&#xA0;&#xA0;&#xA0;input_files=[&quot;Nike_Catalog.pdf&quot;]
)

documents = reader.load_data()</code></pre><h3 id="generate-and-index-embeddings-with-jina-embeddings">Generate and index embeddings with Jina Embeddings</h3><p>Now that the setup is complete, we&#x2019;ll generate the embedding vectors (nodes) and index them. Jina Embeddings v2 models accept input of up to 8192 tokens, large enough that for a document like this, we don&#x2019;t need to do any further text segmentation or check if any section has too many tokens. To embed and index the document, run the following code:</p><pre><code class="language-Python">from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(
&#xA0;&#xA0;&#xA0;&#xA0;documents=documents, embed_model=jina_embeddings
)</code></pre><h3 id="query-for-results-without-jina-reranker">Query for results without Jina Reranker</h3><p>When we query for specific information from this set of texts, the LlamaIndex <code>query_engine</code> does the following:</p><ol><li>With Jina Embeddings V2, it creates an embedding for the query.</li><li>It uses the index to get the <code>top_k = 10</code> stored embeddings with the highest cosine to the query embedding and return its place in the index.</li><li>It will look up the corresponding text in the vector data array.</li></ol><p>Let&#x2019;s ask what the best Nike jersey is in terms of fabric:</p><pre><code>query_engine = index.as_query_engine(
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=10, llm=mixtral_llm
)
response = query_engine.query(
&#xA0;&#xA0;&#xA0;&#xA0;&quot;What are the best padded pants that Nike sells?&quot;,
)

print(response.source_nodes[0].text)</code></pre><p><strong>Result</strong>:</p><pre><code class="language-Text">NIKE KIDS&#xA0; EQUIPMENT87NIKE BRASILIA SMALL DUFFEL 9.5&#xA0;
DM3976 $37.00
SIZES: Misc&#xA0; OFFER DATE: 07/01/22&#xA0; END DATE: 07/01/25
Tough 600D polyester &#x2022; Durable 300D polyester &#x2022; Detachable shoulder
strap&#xA0;&#x2022; Ventilated shoe or wet/dry storage &#x2022; Secure zip pocket &#x2022; 
Limited lifetime guarantee&#xA0;&#x2022; Screened Swoosh design trademark
DIMENSIONS:&#xA0; 20&quot; L x 10&quot; W x 11&quot; H&#xA0;
010 Black/Black/(White) 068 Iron Grey/Black/(White)
...</code></pre><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/z2OKhAFnHCXw3aI4W1KUZNxzt8WOEERSG-prrBcmGwBEDTAFGavSN8eiHnvFF6R0L-dlwRUY3U8R7ysinoGTSSL-9_vr0RdCmrIRSy_xffvKz-Trqa1Yr49YGNb7R-GsDpCqilIW3cluVER4St5rUts" class="kg-image" alt="Precise RAG with Jina Reranker and LlamaIndex" loading="lazy" width="594" height="592"></figure><h3 id="query-for-results-with-reranker">Query for results with Reranker</h3><p>We now want to apply the reranker to see if the RAG application yields a different, more <em>relevant</em> result. To do so, we need to add the <code>node_postprocessors</code> to the <code>query_engine</code>:</p><pre><code class="language-Python">query_engine = index.as_query_engine(
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=10, llm=mixtral_llm, node_postprocessors=[jina_rerank]
)
response = query_engine.query(
&#xA0;&#xA0;&#xA0;&#xA0;&quot;What are the best padded pants that Nike sells?&quot;,
)

print(response.source_nodes[0].text)</code></pre><p>Note that compared to the previous case without the reranker, the <code>query_engine</code> now also contains the <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank.html?ref=jina-ai-gmbh.ghost.io"><u>node_postprocessors</u></a> parameter set to <code>[jina_rerank]</code>.</p><p><strong>Result</strong>:</p><pre><code class="language-Text">NIKE KIDS&#xA0; FOOTBALL &#x2013; STOCK10
DJ5731 $47.00
SIZES:&#xA0; XS, S, M, L, XL, 2XL, 3XL
FABRIC:&#xA0; Body/panels lining: 100% polyester. Pad: 100%
ethylene vinyl acetate.
OFFER DATE:&#xA0; 04/01/23
END DATE:&#xA0; 04/01/27
Take the field ready to give it your all in the Nike Recruit
Pants. They&#x2019;re made from lightweight, stretchy&#xA0;fabric with
sweat-wicking power to help keep you dry and moving freely
when the game heats up. With&#xA0;integrated pads shaped for a
comfortable fit, you&#x2019;ll be prepared for a performance you can
be proud of.&#xA0;Choose from 6 different colors to outfit your
team. Nike Dri-FIT technology moves sweat away from your&#xA0;skin
for quicker evaporation, helping you stay dry and comfortable.
Lightweight knit fabric stretches with&#xA0;you to let you move
naturally. Thigh, knee, hip and tailbone pads are shaped for
an optimal fit, without&#xA0;compromising on coverage. A
body-hugging fit is designed to help keep the padding in place
and close to&#xA0;the body. Belt at the waist lets you dial in your
perfect fit to maximize comfort. Elastic at hems.
Hip width: 15&quot;, Inseam length: 11.75&quot; (size medium).
010 Black/(White) 060 Team Anthracite/(White) 100 White/(Black)
419 Team Navy/(White) 493 Team Royal/(White) 657 Team Scarlet/(White)</code></pre><p></p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/X0ngGFoOz8S_KlRhAlQs4rOooKsV92d06-UBa7BsFPiTilgTD9xuQfgIblG7dTq0BQp4ZA9DlBCjFvA6D62eUVDmxxOO38va5mlnutPr4Zlu6l70tvbGtmrS2V0rpn-OE__tarMtR1MxAmD2oQ3m2qg" class="kg-image" alt="Precise RAG with Jina Reranker and LlamaIndex" loading="lazy" width="1600" height="982"></figure><h2 id="conclusion"><strong>Conclusion</strong></h2><p>As we can see, the query without the reranker leads to a top result which mentions &#x201C;mesh back for breathability&#x201D; and &#x201C;slim fit with soft hand feel&#x201D;. In comparison, by using a reranker, we obtain a top result that is &#x201C;engineered for optimal breathability&#x201D;, has a &#x201C;moisture-wicking design&#x201D; that &#x201C;helps keep you dry and cool under match-day pressure&#x201D;, and features &#x201C;lightweight fabric in a relaxed, easy fit&#x201D;.</p><p>The second result is much more accurate and appropriate for the query we asked. With our last two posts, we showed both from a theoretical and practical perspective that adding Jina Reranker to your RAG pipeline increases your retrieval accuracy and improves the quality of the responses you obtain from it.</p>]]></content:encoded></item><item><title><![CDATA[Build a RAG system with Jina Embeddings and Qdrant]]></title><description><![CDATA[Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.]]></description><link>https://jina.ai/news/build-a-rag-system-with-jina-embeddings-and-qdrant/</link><guid isPermaLink="false">65ddadd039a5b70001810ea3</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 04 Mar 2024 15:00:13 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg" alt="Build a RAG system with Jina Embeddings and Qdrant"><p>Retrieval-augmented generation is a rapidly growing group of techniques that were <a href="https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io"><u>originally conceived as an extension to large language models</u></a> (LLMs) but are increasingly seen as an alternative paradigm that re-imagines the role of AI models entirely.</p><p>We&#x2019;ll show you how to make a basic RAG engine using the <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex framework</a>, an open-source <a href="https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io">Mistral LLM</a>, <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Jina Embeddings v2</a>, and <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer">Qdrant&#x2019;s AI-ready vector database</a>.</p><h2 id="rag-in-context"><strong>RAG in Context</strong></h2><p>In a RAG system, the role of the AI model is still central but much more limited. Instead of answering prompts from its internal knowledge, derived from its training data, it&#x2019;s presented with the relevant information in its prompt. The role of the model is then to understand and synthesize information into an output.</p><p>This plays to the strengths of AI models and minimizes its weaknesses.</p><p>For example, we might give the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io"><u>Mistral-Instruct model</u></a> the following prompt:</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.
Query: Why is Taylor Swift planning to attend the Superbowl?
Answer:
</code></pre><p>When I did that, the model responded with:</p><pre><code class="language-text">I don&apos;t have real-time information. However, as of my last update,&#xA0;
Taylor Swift is not officially confirmed to attend or perform at&#xA0;
the Superbowl. If there are any updates, I recommend checking the&#xA0;
latest news sources.</code></pre><p>Large language models are expensive to update and retrain, so their internal information can never be entirely up to date.</p><p>Furthermore, large language models are well known to &#x201C;hallucinate,&#x201D; i.e., confabulate plausible answers that may be partly or wholly incorrect and have to be verified.</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.
Query: What was Weird Al Yankovic&apos;s first Taylor Swift parody?
Answer:

Weird Al Yankovic&apos;s first Taylor Swift parody was &quot;TMZ,&quot; a spoof&#xA0;
of Swift&apos;s song &quot;Blank Space.&quot; The parody was released in 2014&#xA0;
as part of Yankovic&apos;s album &quot;Mandatory Fun.&quot; The song humorously&#xA0;
addresses the obsession with celebrity gossip and scandals, as&#xA0;
reported by the tabloid news website TMZ.</code></pre><p>Weird Al Yankovic&#x2019;s <em>TMZ</em> was released in 2011 on the album <em>Alpocalypse</em> and is a parody of Swift&#x2019;s <a href="https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io"><em><u>You Belong with Me</u></em></a>.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)"></iframe></figure><p>The nature of neural networks makes hallucination difficult, if not impossible, to completely prevent and strongly undermines any plan to use AI models as information sources.</p><p>In the <a href="https://youtu.be/uaQZIK9gvNo?t=1130&amp;ref=jina-ai-gmbh.ghost.io"><u>words of Sam Altman</u></a>, CEO of OpenAI:</p><blockquote class="kg-blockquote-alt">&#x201C;I think people make a big mistake when they think of these models [LLMs] as a database. [&#x2026;] It will work, sort of. But it&#x2019;s, like, the world&#x2019;s most expensive, slowest, least accurate database.&#x201D;</blockquote><p>RAG minimizes reliance on an AI model&#x2019;s internal knowledge of facts about the world and takes advantage of its biggest strengths: natural language understanding and fluent language generation.</p><p>Let&#x2019;s take another example of a question Mistral-Instruct can&#x2019;t answer:</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.

Query: What is the title of Taylor Swift&apos;s next album?
Answer:

As of the time I am providing this answer, Taylor Swift has&#xA0;
not yet announced the title of her next album. She has released&#xA0;
several re-recorded albums, but the title of her next original&#xA0;
album remains unknown. I recommend checking her official social&#xA0;
media channels or website for the latest updates.</code></pre><p>Now, a RAG system might have a database of recent news articles, or perhaps search the Internet for texts related to the title of Taylor Swift&#x2019;s next album. Then, it would insert the texts it finds into the prompt.</p><p>Let&#x2019;s assume it identifies <a href="https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io"><u>this AP article</u></a> as relevant to the query and constructs the following prompt:</p><pre><code class="language-text">Context information is below.
---------------------
February 6, 2024
LOS ANGELES (AP) &#x2014; Forget her &#x201C;Reputation.&#x201D; Taylor Swift has a&#xA0;
whole new album coming out.

Accepting the Grammy for best pop vocal album on Sunday night,&#xA0;
Taylor said she&#x2019;d been keeping a secret for two years.

&#x201C;My brand-new album comes out April 19. It&#x2019;s called &#x2018;The Tortured&#xA0;
Poets Department.&#x2019; I&#x2019;m going to go and post the cover right now&#xA0;
backstage,&#x201D; she announced.

And so she did.

---------------------
Given the context information and not prior knowledge, answer the query.&#xA0;
Please be brief, concise, and complete.
If the context information does not contain an answer to the query,&#xA0;
respond with &quot;No information&quot;.
Query: What is the title of Taylor Swift&apos;s next album?
Answer:</code></pre><p>Mistral-Instruct responded with:</p><pre><code class="language-text">The title of Taylor Swift&apos;s next album is &apos;The Tortured Poets Department&apos;.</code></pre><p>This kind of prompting also reduces the likelihood of hallucinations dramatically. For example:</p><pre><code class="language-text">Context information is below.
---------------------
Alpocalypse is the thirteenth studio album by American parody&#xA0;
musician &quot;Weird Al&quot; Yankovic, released on June 21, 2011.

Track List

1. Perform This Way (Parody of &quot;Born This Way&quot; by Lady Gaga)
2. CNR
3. TMZ (Parody of &quot;You Belong with Me&quot; by Taylor Swift)
4. Skipper Dan
5. Polka Face (Polka medley including songs by artists such as&#xA0;
&#xA0;&#xA0;&#xA0;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)
6. Craigslist
7. Party in the CIA (Parody of &quot;Party in the U.S.A.&quot; by Miley&#xA0;
&#xA0;&#xA0;&#xA0;Cyrus)
8. Ringtone
9. Another Tattoo (Parody of &quot;Nothin&apos; on You&quot; by B.o.B featuring&#xA0;
&#xA0;&#xA0;&#xA0;Bruno Mars)
10. If That Isn&apos;t Love
11. Whatever You Like (Parody of &quot;Whatever You Like&quot; by T.I.)
12. Stop Forwarding That Crap to Me

---------------------
Given the context information and not prior knowledge, answer the&#xA0;
query. Please be brief, concise, and complete.
If the context information does not contain an answer to the query,&#xA0;
respond with &quot;No information&quot;.
Query: What was Weird Al Yankovic&apos;s first Taylor Swift parody, what&#xA0;
year was it released, and what song was he parodying?
Answer:

Weird Al Yankovic&apos;s first Taylor Swift parody was &quot;TMZ,&quot; and it was&#xA0;
released in 2011. He was parodying &quot;You Belong with Me&quot; by Taylor&#xA0;
Swift.
</code></pre><h2 id="anatomy-of-a-rag-system"><strong>Anatomy of a RAG system</strong></h2><p>A RAG system has, minimally, two components:</p><ul><li>A generative AI model.</li><li>An information retrieval system.</li></ul><p>In this article, we will use <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io">Mistral-Instruct</a> as the generative AI model and access it via the <a href="https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io">Hugging Face Inference API</a>. We will use the <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog">Qdrant vector database</a> and <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Jina Embeddings</a> together for the information retrieval system.</p><p>As a data source, we will use the <a href="https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io"><em><u>National Artificial Intelligence Research and Development Strategic Plan 2023 Update</u></em></a>, published in May 2023 by the <a href="https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io">National Science and Technology Council</a>, a body that advises the US President on scientific and technical matters. This roughly 25,000-word document is a policy and strategic planning document for American AI policy and development.</p><p>It&apos;s also a classic &#x201C;tl;dr&#x201D; government policy paper. Most of it is very boring, and unless you have a stake or a close interest in this area, you probably aren&#x2019;t going to read it. But you might still want to know what it says.</p><p>This article will walk you through the steps of creating a RAG system able to answer questions about the text and its contents.</p><h2 id="install-the-prerequisites"><strong>Install the Prerequisites</strong></h2><p>First, make sure you have all the relevant Python libraries installed. In your Python environment, run the following:</p><pre><code class="language-shell">pip install llama-index qdrant-client 
pip install pdfminer.six llama-index-vector-stores-qdrant
pip install llama-index-llms-openai llama-index-embeddings-jinaai 
pip install llama-index-llms-huggingface &quot;huggingface_hub[inference]&quot;</code></pre><p>We will be using LlamaIndex to construct a RAG system around the Mistral-Instruct large language model and <a href="https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io"><u>pdfminer.six</u></a> to process the PDF file into a Qdrant vector store.</p><h2 id="connect-to-mistral-instruct-via-the-hugging-face-inference-api"><strong>Connect to Mistral-Instruct via the Hugging Face Inference API</strong></h2><p>First, set up the LLM. You will need access to the Hugging Face Inference API, including an access token. If you have a Hugging Face account, you can get one from <a href="https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io">your account settings page</a>.</p><p>If you do not have an account, first <a href="https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io">set one up</a>, then create an <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">access token</a>.</p><p>Place your token in a variable for future use:</p><pre><code class="language-python">hf_inference_api_key = &apos;&lt;your HuggingFace Inference API token&gt;&apos;</code></pre><p>Next, we construct a prompt template:</p><pre><code class="language-python">from llama_index.core import PromptTemplate

qa_prompt_tmpl = (
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Context information is below.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;---------------------\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;{context_str}\\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;---------------------\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Given the context information and not prior knowledge, &quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;answer the query. Please be brief, concise, and complete.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;If the context information does not contain an answer to the query, &quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;respond with \&quot;No information\&quot;.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Query: {query_str}\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Answer: &quot;
)
qa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>This template instructs the LLM to use only the context information provided in the prompt to answer questions. This prompt has worked well for us in working with Mistral-Instruct, but you can experiment with other wordings to see what works for your use case.</p><pre><code class="language-python">from llama_index.llms.huggingface import HuggingFaceInferenceAPI

mixtral_llm = HuggingFaceInferenceAPI(
    model_name=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;, 
    token=hf_inference_api_key
)</code></pre><p>Finally, create and initialize an object for the LlamaIndex framework that holds the connection to Mistral-Instruct. The code below comes from the <a href="https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io"><u>previous post on creating RAG systems using LlamaIndex</u></a>.</p><p>This puts the connection to the Mistral LLM into the variable <code>mistral_llm</code>.</p><h2 id="information-retrieval-with-qdrant-and-jina-embeddings"><strong>Information Retrieval with Qdrant and Jina Embeddings</strong></h2><p>To set up the retrieval system, you will need a Jina Embeddings API key. You can get one for free with a 1 million-token prepaid budget at the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings website</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Place your Jina Embeddings key in a variable for future use:</p><pre><code class="language-python">jina_emb_api_key = &quot;&lt;your Jina Embeddings API key&gt;&quot;</code></pre><p>Then, create a connector object using LlamaIndex for the Jina Embeddings server, selecting specifically the English monolingual model:</p><pre><code class="language-python">from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jina_emb_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
)</code></pre><h3 id="load-text-data"><strong>Load text data</strong></h3><p>Next, we will load the document and split it up into paragraphs. First, download the PDF from the White House website into the variable <code>pdf_data</code>:</p><pre><code class="language-python">import urllib.request

uri = &quot;https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf&quot;
pdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Next, we will process the PDF page by page, extracting the text and then chunking it into paragraphs by splitting on double line breaks. These are stored in the list <code>text_paras</code>:</p><pre><code class="language-python">import regex as re
from io import BytesIO, StringIO
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser

text_paras = []
parser = PDFParser(BytesIO(pdf_data))
doc = PDFDocument(parser)
rsrcmgr = PDFResourceManager()
for page in PDFPage.create_pages(doc):
&#xA0;&#xA0;&#xA0;&#xA0;output_string = StringIO()
&#xA0;&#xA0;&#xA0;&#xA0;device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
&#xA0;&#xA0;&#xA0;&#xA0;interpreter = PDFPageInterpreter(rsrcmgr, device)
&#xA0;&#xA0;&#xA0;&#xA0;interpreter.process_page(page)
&#xA0;&#xA0;&#xA0;&#xA0;page_text = output_string.getvalue()
&#xA0;&#xA0;&#xA0;&#xA0;text_paras.extend(re.split(r&apos;\n\s*\n&apos;, page_text))
</code></pre><p>Check that everything is loaded:</p><pre><code class="language-python">assert len(text_paras) == 615</code></pre><p>Next, we will covert this list of short texts into <a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex Document objects</a>:</p><pre><code class="language-python">from llama_index.core.readers import StringIterableReader

rag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>And you can inspect the text:</p><pre><code class="language-python">print(rag_docs[0].text)</code></pre><p>Result:</p><pre><code class="language-text">NATIONAL ARTIFICIAL INTELLIGENCE&#xA0;
RESEARCH AND DEVELOPMENT&#xA0;
STRATEGIC PLAN&#xA0;
2023 UPDATE</code></pre><h3 id="set-up-a-qdrant-vector-database"><strong>Set up a Qdrant Vector Database</strong></h3><p>In this article, we will use the <a href="https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io"><u>Qdrant Vector Search Cloud</u></a> to implement the RAG system&#x2019;s database. Qdrant&#x2019;s free tier includes 1GB of storage, which is more than enough for this tutorial.</p><p>You will need to <a href="https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io"><u>create an account on the Qdrant Cloud website</u></a> before continuing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Vector Search Database | Qdrant Cloud</div><div class="kg-bookmark-description">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cloud.qdrant.io/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Qdrant Cloud</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cloud.qdrant.io/social.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Once you have an account and are logged in, you will need to create a cluster. Follow the <a href="https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>&#x201C;quick start&#x201D; instructions on the Qdrant Website</u></a> to set up a free cluster and get an API and the name of the Qdrant host server.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Quickstart - Qdrant</div><div class="kg-bookmark-description">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://qdrant.tech/images/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Edit on GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Store the key and hostname in variables:</p><pre><code class="language-python">qdrant_api_key = &quot;&lt;your API key&gt;&quot;
qdrant_server = &quot;https://&lt;your server&gt;&quot;</code></pre><p>Next, we will need to import the relevant components from the <code>qdrant_client</code> and <code>llama_index</code> packages:</p><pre><code class="language-python">import qdrant_client
from llama_index.vector_stores.qdrant import QdrantVectorStore

client = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)
vector_store = QdrantVectorStore(client=client, collection_name=&quot;NTSC&quot;)</code></pre><p>This creates a collection named <code>NTSC</code> in your free cluster.</p><p>You can read the <a href="https://qdrant.tech/documentation/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>Qdrant documentation</u></a> for information on constructing a local data store in memory or on disk and hosting your own Qdrant server instance in a docker container via a web API.</p><h3 id="complete-the-rag-system"><strong>Complete the RAG system</strong></h3><p>Now we will assemble these components into a complete RAG system using boilerplate code for LlamaIndex:</p><pre><code class="language-python">from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core import (
		VectorStoreIndex,
		ServiceContext,
		get_response_synthesizer,
)

# set up the service and storage contexts
service_context = ServiceContext.from_defaults(
&#xA0;&#xA0;&#xA0;&#xA0;llm=mixtral_llm, embed_model=jina_embedding_model
)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# create an index
index = VectorStoreIndex.from_documents(
&#xA0;&#xA0;&#xA0;&#xA0;rag_docs, storage_context=storage_context, service_context=service_context
)

# configure retriever
retriever = VectorIndexRetriever(
&#xA0;&#xA0;&#xA0;&#xA0;index=index,
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=2,
)

# configure response synthesizer
response_synthesizer = get_response_synthesizer(
&#xA0;&#xA0;&#xA0;&#xA0;service_context=service_context,
&#xA0;&#xA0;&#xA0;&#xA0;text_qa_template=qa_prompt,
&#xA0;&#xA0;&#xA0;&#xA0;response_mode=&quot;compact&quot;,
)

# assemble query engine
query_engine = RetrieverQueryEngine(
&#xA0;&#xA0;&#xA0;&#xA0;retriever=retriever,
&#xA0;&#xA0;&#xA0;&#xA0;response_synthesizer=response_synthesizer,
)
</code></pre><p>This system will use Jina Embeddings to index the paragraphs from our text source and will put the two best matches into the context of every prompt. To change to the three best, for example, just modify the <code>VectorIndexRetriever</code> initialization above to have a different <code>similarity_top_k</code> parameter:</p><pre><code class="language-python"># configure retriever
retriever = VectorIndexRetriever(
&#xA0;&#xA0;&#xA0;&#xA0;index=index,
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=3,
)</code></pre><p>Now the system is ready to use.</p><h3 id="query-a-document"><strong>Query a Document</strong></h3><p>Let&#x2019;s try a straightforward query:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What is the Biden Administration&apos;s policy with regard to AI?&#xA0;
&quot;&quot;&quot;)
print(response.response)</code></pre><p>The result (yours may not be identical):</p><pre><code class="language-text">The Biden Administration prioritizes responsible AI systems that&#xA0;
are ethical, trustworthy, and safe, and serve the public good. 
The Administration has committed to substantial funding for AI 
R&amp;D, and has established the National AI Initiative Office to 
coordinate AI activities across the federal government.&#xA0;

The Administration&apos;s AI policy focuses on developing evaluative
techniques for AI, workforce development, public-private
partnerships, and international collaboration in AI research.</code></pre><p>Or something more specific:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What protections does the AI Bill of Rights propose to offer?
&quot;&quot;&quot;)

print(response.response)</code></pre><pre><code class="language-text">The AI Bill of Rights proposes five core protections: Safe and&#xA0;
Effective Systems; Algorithmic Discrimination Protections; Data&#xA0;
Privacy; Notice and Explanation; and Human Alternatives,&#xA0;
Consideration, and Fallback.</code></pre><p>Or even very specific:</p><pre><code class="language-python">response = query_engine.query(&quot;Who is Kei Koizumi?&quot;)
print(response.response)</code></pre><pre><code class="language-text">Kei Koizumi is the Principal Deputy Director for Policy at the Office&#xA0;
of Science and Technology Policy (OSTP).</code></pre><p>You can also ask more fanciful questions:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What rights will AI&apos;s receive under President Biden&apos;s proposed&#xA0;
AI Bill of Rights?
&quot;&quot;&quot;)
print(response.response)</code></pre><pre><code class="language-text">The proposed AI Bill of Rights does not grant rights to AI systems, but&#xA0;
rather outlines principles to ensure that AI technologies are&#xA0;
developed and used in a manner that respects human rights and values.&#xA0;
The principles include safety, transparency, fairness, disclosure,&#xA0;
data privacy, non-discrimination, and the ability for individuals to&#xA0;
opt out of certain AI systems.</code></pre><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
Why is President Biden&apos;s proposing an AI Bill of Rights?
Does AI really need rights?
&quot;&quot;&quot;)
print(response.response)</code></pre><pre><code class="language-text">President Biden&apos;s proposed AI Bill of Rights aims to ensure that AI&#xA0;
technologies are developed and used ethically, legally, and with&#xA0;
societal considerations in mind. The Bill of Rights is not granting&#xA0;
&quot;rights&quot; to AI, but rather setting guidelines for the responsible&#xA0;
development and deployment of AI systems to protect individuals and&#xA0;
communities from potential negative impacts.</code></pre><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
Has Donald Trump weighed in on AI?
Will he Make Humans Great Again?
&quot;&quot;&quot;)
print(response.response)
</code></pre><pre><code class="language-text">No information. The context information does not mention Donald&#xA0;
Trump&apos;s views on AI.</code></pre><h2 id="jina-ai-and-qdrant"><strong>Jina AI and Qdrant</strong></h2><p>Jina Embeddings&#x2019; 8k token context window makes it especially well-suited to retrieval-augmented generation because it can work with much larger blocks of text. Where other RAG applications often have to store individual sentences or pairs of sentences, in this tutorial, we could use whole paragraphs without worrying about their size.</p><p>This reduces the number of text items that have to be stored to carry the same information, lowering compute costs, sparing memory, and making retrieval faster.</p><p>Qdrant&#x2019;s vector database is simple to configure, fast, and cost-effective, and as you&#x2019;ve seen, it takes only a few lines of Python to integrate it into a RAG system.</p><p>For more about Qdrant&#x2019;s AI-ready database and cloud offerings, you can <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>visit their website</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Qdrant - Vector Database</div><div class="kg-bookmark-description">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://qdrant.tech/images/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Vector Database</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://qdrant.tech/images/previews/social-preview-P.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><h2 id="get-in-touch"><strong>Get in Touch</strong></h2><p>Jina AI is committed to bringing reliable, affordable AI technologies to enterprises of every size and type. We&#x2019;d love to hear about your use cases and help fit AI into your business processes. For more information about Jina AI&#x2019;s offerings and to contact us, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>Jina AI website</u></a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>community on Discord</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.5 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-5-update/</link><guid isPermaLink="false">65e0b09bb22368000152a3ff</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Thu, 29 Feb 2024 16:31:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-2.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3235">Release Note (<code>3.23.5</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-2.jpg" alt="Jina 3.23.5 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.5?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.5 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.5) Release time: 2024-02-29 14:36:02 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Fix batch transform endpoint for SageMaker with nested schemas (#6145)
This release improves t&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.5 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/0e601e49d37d6935a6e25fca8cffd5b4e98d0b1ed8605e0b1c72d54d290871a3/jina-ai/jina/releases/tag/v3.23.5" alt="Jina 3.23.5 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-batch-transform-endpoint-for-sagemaker-with-nested-schemas-6145">Fix batch transform endpoint for SageMaker with nested schemas (<a href="https://github.com/jina-ai/jina/pull/6145?ref=jina-ai-gmbh.ghost.io">#6145</a>)</h3><p>This release improves the SageMaker Batch Transform input data parsing logic so that data with more complex schema can be interpreted as expected.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Zac Li (<a href="https://github.com/zac-li?ref=jina-ai-gmbh.ghost.io">@zac-li</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Maximizing Search Relevance and RAG Accuracy with Jina Reranker]]></title><description><![CDATA[Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!]]></description><link>https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/</link><guid isPermaLink="false">65dfb0f7b22368000152a205</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Thu, 29 Feb 2024 08:30:10 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"><p>Text embeddings, known for their semantic representation capabilities, along with fast vector retrieval, are today&apos;s cornerstone in document search within vast datasets. However, the challenge often lies in filtering through these retrieved documents to accurately align with the user&apos;s search intent, a task that goes beyond the capabilities of simple cosine similarity measures.</p><p>Today, we&apos;re thrilled to announce <strong>Jina Reranker</strong> (<code>jina-reranker-v1-base-en</code>), a cutting-edge neural reranking model designed to tackle this critical issue of relevancy. Jina Reranker enhances your search and RAG (Retrieval-Augmented Generation) system by <em>reordering</em> retrieved documents in a manner that deeply and contextually understands the search query terms. Our evaluations demonstrated remarkable improvements for search systems employing the Jina Reranker <strong>with</strong> <strong>+8% in hit rate and +33% in mean reciprocal rank</strong>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"></div></a></figure><h2 id="what-is-a-reranker">What is a Reranker?</h2><p>Understanding the concept of a reranker often leads us to question the necessity of <em>re-</em>ranking mechanisms in search (ranking) systems. Common questions are, &quot;<em>Why do we need a </em><strong>re</strong><em>ranker?</em>&quot; and &quot;<em>Isn&apos;t ranking documents by cosine similarity between the query and document embeddings enough already?</em>&quot; To address these questions, let&apos;s first revisit the single embedding cosine similarity approach and its limitations.</p><p>Cosine similarity quantifies the similarity between two documents by measuring the cosine of the angle between their embedding vectors. This metric is valued for its simplicity and has been used in many vector databases as the default way of retrieval. However, this approach, commonly termed as a <em>representation-based </em>approach, tends to oversimplify interaction between query and documents. Specifically, it struggles with capturing the nuanced interaction at the sub-document level and sub-query level, often missing the full depth of user intent and the fine details of document relevance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Copy-of-Copy-of-colbert-blog-interaction.svg" class="kg-image" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker" loading="lazy" width="168" height="150"><figcaption><span style="white-space: pre-wrap;">The comparison of the representation-based cosine similarity (left) and the reranker (right).</span></figcaption></figure><p>This is where rerankers come into play. Utilizing deep neural networks, rerankers delve deeper into the interactions between the query and the shortlisted documents. They move beyond basic document-level embeddings to embrace token-level interactions that occur within the query, within the document, and across the query-document boundary. Although this method is more computationally intensive compared to simple cosine similarity, it enables a nuanced comparison that incorporates context, semantic meaning, and the intent behind the query, substantially improving the relevancy of search results.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Vector Search via Cosine Similarity</th>
<th>Reranker</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Interaction Level</strong></td>
<td>Document-level embeddings</td>
<td>Token-level interactions</td>
</tr>
<tr>
<td><strong>Computational Demand</strong></td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td><strong>Most computation happens at</strong></td>
<td>Offline, i.e. indexing time</td>
<td>Online, i.e. query time</td>
</tr>
<tr>
<td><strong>Result</strong></td>
<td>Broad but superficial matching</td>
<td>Highly relevant and precise matching</td>
</tr>
<tr>
<td><strong>Strengths</strong></td>
<td>- Fast and efficient<br>- Simple implementation</td>
<td>- Deep contextual understanding<br>- Advanced semantic analysis</td>
</tr>
<tr>
<td><strong>Limitations</strong></td>
<td>- Limited by lack of depth and context<br>- May miss nuances of user intent</td>
<td>- Computationally intensive<br>- Requires more sophisticated models</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Provides a quick, efficient first pass</td>
<td>Adds depth, enhancing accuracy and relevance of final search results</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>In summary, reranker is a critical component in the search pipeline. <strong>A high-quality search system typically begins with an embedding-based vector retrieval step, which is then refined by a reranker model.</strong> This two-step approach leverages the strengths of both models, ensuring the delivery of precise, high-quality information that aligns seamlessly with the user&apos;s needs.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Copy-of-Copy-of-Copy-of-colbert-blog-interaction.svg" class="kg-image" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker" loading="lazy" width="300" height="66"><figcaption><span style="white-space: pre-wrap;">A practical search system often chains the embedding-based search and the reranker together to achieve the best search quality.</span></figcaption></figure><h2 id="get-started-with-jina-reranker">Get Started with Jina Reranker</h2><p>To start using Jina Reranker, head to our <a href="http://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io">Reranker page</a> to get <code>YOUR_API_KEY</code>. You can adapt the example cURL snippet below by changing the query or adding more documents:</p><pre><code class="language-shell">curl -X &apos;POST&apos; \
  &apos;https://api.jina.ai/v1/rerank&apos; \
  -H &apos;accept: application/json&apos; \
  -H &apos;Authorization: Bearer YOUR_API_KEY&apos; \
  -H &apos;Content-Type: application/json&apos; \
  -d &apos;{
  &quot;model&quot;: &quot;jina-reranker-v1-base-en&quot;,
  &quot;query&quot;: &quot;Organic skincare products for sensitive skin&quot;,
  &quot;documents&quot;: [
    &quot;Eco-friendly kitchenware for modern homes&quot;,
    &quot;Biodegradable cleaning supplies for eco-conscious consumers&quot;,
    &quot;Organic cotton baby clothes for sensitive skin&quot;,
    &quot;Natural organic skincare range for sensitive skin&quot;,
    &quot;Tech gadgets for smart homes: 2024 edition&quot;,
    &quot;Sustainable gardening tools and compost solutions&quot;,
    &quot;Sensitive skin-friendly facial cleansers and toners&quot;,
    &quot;Organic food wraps and storage solutions&quot;,
    &quot;All-natural pet food for dogs with allergies&quot;,
    &quot;Yoga mats made from recycled materials&quot;
  ], 
  &quot;top_n&quot;: 3
}&apos;
</code></pre><p>In this example, the documents range from home goods to tech gadgets, clothing, and even pet food, showcasing a wide spectrum of products one might find on an e-commerce site. This diversity requires the reranker to effectively identify and prioritize items most relevant to the query about &quot;Organic skincare products for sensitive skin,&quot; despite the presence of other organic, eco-friendly, or sensitive skin-related products in different categories. The response is a JSON object as follows:</p><pre><code class="language-json">{
  &quot;model&quot;: &quot;jina-reranker-v1-base-en&quot;,
  &quot;usage&quot;: {
    &quot;total_tokens&quot;: 38,
    &quot;prompt_tokens&quot;: 38
  },
  &quot;results&quot;: [
    {
      &quot;index&quot;: 3,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Natural organic skincare range for sensitive skin&quot;
      },
      &quot;relevance_score&quot;: 0.8292155861854553
    },
    {
      &quot;index&quot;: 2,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Organic cotton baby clothes for sensitive skin&quot;
      },
      &quot;relevance_score&quot;: 0.14426936209201813
    },
    {
      &quot;index&quot;: 6,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Sensitive skin-friendly facial cleansers and toners&quot;
      },
      &quot;relevance_score&quot;: 0.13857832551002502
    }
  ]
}</code></pre><h2 id="top-performance-of-jina-reranker">Top Performance of Jina Reranker</h2><p>We evaluated Jina Reranker on four key benchmarks to ensure top-tier performance and search relevance. Higher performance in these benchmarks directly translates to better precision, relevance, and contextual understanding in search and retrieval applications.</p><p>For comparison, we included three other leading rerankers by <a href="https://www.baai.ac.cn/english.html?ref=jina-ai-gmbh.ghost.io">BGE (BAAI)</a>, <a href="https://smartyoudao.com/?ref=jina-ai-gmbh.ghost.io">BCE (Netease Youdao)</a>, and <a href="https://cohere.com/?ref=jina-ai-gmbh.ghost.io">Cohere</a> in the benchmark. As shown by the results below, <strong>Jina Reranker holds the highest average score in all relevant categories for reranking, making it a clear leader among its peers.</strong></p><h3 id="benchmark-1-llamaindex-rag">Benchmark 1: LlamaIndex RAG</h3><p>A <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">benchmark study</a> performed by LlamaIndex (which we reproduced), evaluated <em>a combination</em> of different embedding and reranking models in RAG tasks. This combines two scores - <em>Hit-rate</em> (how likely it is that a relevant document is retrieved by an embedding model), and <em>Mean Reciprocal Rank</em> (MRR - how high the most relevant document is ranked by the reranker model).</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-cyhs{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-wa1i"></th>
    <th class="tg-wa1i" colspan="2">No Reranker</th>
    <th class="tg-wa1i" colspan="2">jina-reranker</th>
    <th class="tg-wa1i" colspan="2">bge-reranker-base</th>
    <th class="tg-wa1i" colspan="2">bce-reranker-base_v1</th>
    <th class="tg-wa1i" colspan="2">cohere-reranker</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0">Embedding model</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
  </tr>
  <tr>
    <td class="tg-cly1">jina-embeddings-v2-base-en</td>
    <td class="tg-mwxe">0.8053</td>
    <td class="tg-mwxe">0.5156</td>
    <td class="tg-mwxe">0.8737</td>
    <td class="tg-zt7h">0.7229</td>
    <td class="tg-mwxe">0.8368</td>
    <td class="tg-mwxe">0.6568</td>
    <td class="tg-mwxe">0.8737</td>
    <td class="tg-mwxe">0.7007</td>
    <td class="tg-zt7h">0.8842</td>
    <td class="tg-mwxe">0.7008</td>
  </tr>
  <tr>
    <td class="tg-cly1">bge-base-en-v1.5</td>
    <td class="tg-mwxe">0.7842</td>
    <td class="tg-mwxe">0.5183</td>
    <td class="tg-zt7h">0.8368</td>
    <td class="tg-zt7h">0.6895</td>
    <td class="tg-mwxe">0.8158</td>
    <td class="tg-mwxe">0.6586</td>
    <td class="tg-mwxe">0.8316</td>
    <td class="tg-mwxe">0.6843</td>
    <td class="tg-mwxe">0.8368</td>
    <td class="tg-mwxe">0.6739</td>
  </tr>
  <tr>
    <td class="tg-cly1">bce-embedding-base_v1</td>
    <td class="tg-mwxe">0.8526</td>
    <td class="tg-mwxe">0.5988</td>
    <td class="tg-mwxe">0.8895</td>
    <td class="tg-mwxe">0.7346</td>
    <td class="tg-mwxe">0.8684</td>
    <td class="tg-mwxe">0.6927</td>
    <td class="tg-mwxe">0.9157</td>
    <td class="tg-zt7h">0.7379</td>
    <td class="tg-zt7h">0.9158</td>
    <td class="tg-mwxe">0.7296</td>
  </tr>
  <tr>
    <td class="tg-cly1">CohereV3-en</td>
    <td class="tg-mwxe">0.7211</td>
    <td class="tg-mwxe">0.4900</td>
    <td class="tg-mwxe">0.8211</td>
    <td class="tg-zt7h">0.6894</td>
    <td class="tg-mwxe">0.8000</td>
    <td class="tg-mwxe">0.6285</td>
    <td class="tg-mwxe">0.8263</td>
    <td class="tg-mwxe">0.6855</td>
    <td class="tg-zt7h">0.8316</td>
    <td class="tg-mwxe">0.6710</td>
  </tr>
  <tr>
    <td class="tg-cyhs">Average</td>
    <td class="tg-dg4k">0.7908</td>
    <td class="tg-dg4k">0.5307</td>
    <td class="tg-dg4k">0.8553</td>
    <td class="tg-cya3"><strong>0.7091</strong></td>
    <td class="tg-dg4k">0.8303</td>
    <td class="tg-dg4k">0.6592</td>
    <td class="tg-dg4k">0.8618</td>
    <td class="tg-dg4k">0.7021</td>
    <td class="tg-cya4">0.8671</td>
    <td class="tg-dg4k">0.6938</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h4 id="substantial-improvement-over-simple-cosine-similarity">Substantial Improvement Over Simple Cosine Similarity</h4><p>The improvement Jina Reranker brings to both Hit Rate and Mean Reciprocal Rank (MRR) is significant. On average, the introduction of Jina Reranker elevates the Hit Rate from 0.7908 to 0.8553 (+7.9%), and the MRR from 0.5307 to 0.7091 (+33.7%). This showcases the reranker&#x2019;s ability to enhance the precision and relevance of search results dramatically, ensuring that users are more likely to find what they&apos;re searching for with higher accuracy.</p><h4 id="embeddings-agnosticism">Embeddings Agnosticism</h4><p>Jina Reranker&apos;s performance across different embedding models further illustrates its model-agnostic nature. Whether paired with <code>jina-embeddings-v2-base-en</code>, <code>bge-base-en-v1.5</code>, <code>bce-embedding-base_v1</code>, or <code>CohereV3-en</code>, Jina Reranker consistently improves the Hit Rate and MRR. This versatility makes it an invaluable tool for a wide range of applications, affirming its adaptability to different underlying technologies and use cases.</p><h3 id="benchmark-2-beir">Benchmark 2: BEIR</h3><p><a href="https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io">BIER (Benchmarking IR)</a> assesses a model&apos;s retrieval effectiveness, including relevance and <a href="https://towardsdatascience.com/normalized-discounted-cumulative-gain-37e6f75090e9?ref=jina-ai-gmbh.ghost.io">NDCG</a>. A higher BIER score correlates to more accurate matches and search result rankings.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-zt7h">jina-reranker</th>
    <th class="tg-zt7h">bge-reranker-base</th>
    <th class="tg-zt7h">bce-reranker-base-v1</th>
    <th class="tg-zt7h">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">NQ</td>
    <td class="tg-mwxe">0.5951</td>
    <td class="tg-mwxe">0.5457</td>
    <td class="tg-mwxe">0.5186</td>
    <td class="tg-zt7h">0.6004</td>
  </tr>
  <tr>
    <td class="tg-cly1">HotpotQA</td>
    <td class="tg-zt7h">0.7447</td>
    <td class="tg-mwxe">0.7766</td>
    <td class="tg-mwxe">0.7392</td>
    <td class="tg-mwxe">0.7202</td>
  </tr>
  <tr>
    <td class="tg-cly1">FiQA-2018</td>
    <td class="tg-mwxe">0.3981</td>
    <td class="tg-mwxe">0.3228</td>
    <td class="tg-mwxe">0.3262</td>
    <td class="tg-zt7h">0.4387</td>
  </tr>
  <tr>
    <td class="tg-cly1">CQADupstack</td>
    <td class="tg-zt7h">0.4077</td>
    <td class="tg-mwxe">0.3516</td>
    <td class="tg-mwxe">0.3594</td>
    <td class="tg-mwxe">0.3829</td>
  </tr>
  <tr>
    <td class="tg-cly1">Quora</td>
    <td class="tg-zt7h">0.8792</td>
    <td class="tg-mwxe">0.7001</td>
    <td class="tg-mwxe">0.8390</td>
    <td class="tg-mwxe">0.6433</td>
  </tr>
  <tr>
    <td class="tg-cly1">FEVER</td>
    <td class="tg-mwxe">0.8707</td>
    <td class="tg-zt7h">0.8961</td>
    <td class="tg-mwxe">0.7203</td>
    <td class="tg-mwxe">0.8265</td>
  </tr>
  <tr>
    <td class="tg-cly1">Climate-FEVER</td>
    <td class="tg-mwxe">0.2570</td>
    <td class="tg-zt7h">0.3399</td>
    <td class="tg-mwxe">0.2171</td>
    <td class="tg-mwxe">0.2038</td>
  </tr>
  <tr>
    <td class="tg-cly1">TREC-COVID</td>
    <td class="tg-zt7h">0.8523</td>
    <td class="tg-mwxe">0.7121</td>
    <td class="tg-mwxe">0.7364</td>
    <td class="tg-mwxe">0.8419</td>
  </tr>
  <tr>
    <td class="tg-cly1">NFCorpus</td>
    <td class="tg-zt7h">0.3809</td>
    <td class="tg-mwxe">0.3308</td>
    <td class="tg-mwxe">0.3534</td>
    <td class="tg-mwxe">0.3673</td>
  </tr>
  <tr>
    <td class="tg-cly1">ArguAna</td>
    <td class="tg-zt7h">0.5938</td>
    <td class="tg-mwxe">0.2620</td>
    <td class="tg-mwxe">0.3856</td>
    <td class="tg-mwxe">0.3040</td>
  </tr>
  <tr>
    <td class="tg-cly1">Touche-2020</td>
    <td class="tg-zt7h">0.3065</td>
    <td class="tg-mwxe">0.2965</td>
    <td class="tg-mwxe">0.2533</td>
    <td class="tg-mwxe">0.3052</td>
  </tr>
  <tr>
    <td class="tg-cly1">DBPedia</td>
    <td class="tg-zt7h">0.4387</td>
    <td class="tg-mwxe">0.4196</td>
    <td class="tg-mwxe">0.4069</td>
    <td class="tg-mwxe">0.4236</td>
  </tr>
  <tr>
    <td class="tg-cly1">SciFact</td>
    <td class="tg-zt7h">0.7549</td>
    <td class="tg-mwxe">0.7104</td>
    <td class="tg-mwxe">0.7021</td>
    <td class="tg-mwxe">0.7379</td>
  </tr>
  <tr>
    <td class="tg-cly1">SCIDOCS</td>
    <td class="tg-zt7h">0.1983</td>
    <td class="tg-mwxe">0.1540</td>
    <td class="tg-mwxe">0.1802</td>
    <td class="tg-mwxe">0.1813</td>
  </tr>
  <tr>
    <td class="tg-cly1">MSMarco</td>
    <td class="tg-mwxe">0.7042</td>
    <td class="tg-mwxe">0.7303</td>
    <td class="tg-mwxe">0.7155</td>
    <td class="tg-zt7h">0.7350</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-cya4">0.5588</td>
    <td class="tg-dg4k">0.5032</td>
    <td class="tg-dg4k">0.4969</td>
    <td class="tg-dg4k">0.5141</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="benchmark-3-mteb">Benchmark 3: MTEB</h3><p>The <a href="https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io">MTEB (Multilingual Text Embedding Benchmark)</a>, on the whole, tests a model&#x2019;s abilities in text embeddings, including clustering, classification, retrieval, and other metrics. However, for our comparison, we only used the MTEB&#x2019;s <em>Reranking</em> tasks.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-zt7h">jina-reranker</th>
    <th class="tg-zt7h">bge-reranker-base</th>
    <th class="tg-zt7h">bce-reranker-base-v1</th>
    <th class="tg-zt7h">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">AskUbuntuDupQuestions</td>
    <td class="tg-zt7h">0.5793</td>
    <td class="tg-mwxe">0.5471</td>
    <td class="tg-mwxe">0.5654</td>
    <td class="tg-mwxe">0.5536</td>
  </tr>
  <tr>
    <td class="tg-cly1">SciDocsRR</td>
    <td class="tg-zt7h">0.8056</td>
    <td class="tg-mwxe">0.6741</td>
    <td class="tg-mwxe">0.7578</td>
    <td class="tg-mwxe">0.6728</td>
  </tr>
  <tr>
    <td class="tg-cly1">StackOverflowDupQuestions</td>
    <td class="tg-zt7h">0.4850</td>
    <td class="tg-mwxe">0.3764</td>
    <td class="tg-mwxe">0.4287</td>
    <td class="tg-mwxe">0.4414</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-cya4">0.6233</td>
    <td class="tg-dg4k">0.5325</td>
    <td class="tg-dg4k">0.5840</td>
    <td class="tg-dg4k">0.5559</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="benchmark-4-loco">Benchmark 4: LoCo</h3><p>Through the <a href="https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval?ref=jina-ai-gmbh.ghost.io">LoCo benchmark</a>, we measured a model&apos;s understanding of local coherence and context, together with query-specific ranking. A LoCo higher score reflects a better ability to identify and prioritize relevant information.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-wa1i">jina-reranker</th>
    <th class="tg-wa1i">bge-reranker-base</th>
    <th class="tg-wa1i">bce-reranker-base-v1</th>
    <th class="tg-wa1i">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">qasper_None_abstract</td>
    <td class="tg-mwxe"><strong>0.996</strong></td>
    <td class="tg-mwxe">0.774</td>
    <td class="tg-mwxe">0.989</td>
    <td class="tg-mwxe">0.919</td>
  </tr>
  <tr>
    <td class="tg-cly1">qasper_None_title</td>
    <td class="tg-mwxe">0.980</td>
    <td class="tg-mwxe">0.883</td>
    <td class="tg-mwxe">0.971</td>
    <td class="tg-mwxe"><strong>0.983</strong></td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_gov_report_output</td>
    <td class="tg-mwxe"><strong>0.962</strong></td>
    <td class="tg-mwxe">0.574</td>
    <td class="tg-mwxe">0.922</td>
    <td class="tg-mwxe">0.659</td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_qmsum_output</td>
    <td class="tg-mwxe">0.466</td>
    <td class="tg-mwxe"><strong>0.549</strong></td>
    <td class="tg-mwxe">0.449</td>
    <td class="tg-mwxe">0.444</td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_summ_screen_fd_output</td>
    <td class="tg-mwxe"><strong>0.962</strong></td>
    <td class="tg-mwxe">0.629</td>
    <td class="tg-mwxe">0.920</td>
    <td class="tg-mwxe">0.905</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-dg4k"><strong>0.873</strong></td>
    <td class="tg-dg4k">0.682</td>
    <td class="tg-dg4k">0.850</td>
    <td class="tg-dg4k">0.782</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="jina-reranker-model-highlights">Jina Reranker Model Highlights</h2><p>The Jina Reranker distinguishes itself as a leader in the reranking domain, outperforming key competitors from Cohere, BGE, and BCE, as illustrated in the previous analysis. Its embedding-agnostic nature and unmatched efficacy underscore its premier status in the industry. Furthermore, Jina Reranker boasts notable features that set it apart:</p><h3 id="long-context-length">Long Context Length</h3><p>Jina Reranker stands out from other reranking solutions by accommodating long context lengths. It is capable of handling queries up to 512 tokens and documents as large as 8192 tokens. Furthermore, the model is designed to process up to 2048 candidate documents for each query.</p>
<!--kg-card-begin: html-->
<table>
<thead>
  <tr>
    <th>Time cost of rerank(query=1, docs=100) in ms</th>
    <th colspan="5">#TokensPerDocument</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>#QueryTokens</td>
    <td>256</td>
    <td>512</td>
    <td>1024</td>
    <td>2048</td>
    <td>4096</td>
  </tr>
  <tr>
    <td>64</td>
    <td>156</td>
    <td>323</td>
    <td>1366</td>
    <td>2107</td>
    <td>3571</td>
  </tr>
  <tr>
    <td>128</td>
    <td>194</td>
    <td>369</td>
    <td>1377</td>
    <td>2123</td>
    <td>3598</td>
  </tr>
  <tr>
    <td>256</td>
    <td>273</td>
    <td>475</td>
    <td>1397</td>
    <td>2155</td>
    <td>4299</td>
  </tr>
  <tr>
    <td>512</td>
    <td>468</td>
    <td>1385</td>
    <td>2114</td>
    <td>3536</td>
    <td>7068</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="enhancing-accuracy-with-rag-integration">Enhancing Accuracy with RAG Integration</h3><p>Retrieval Augmented Generation (RAG) significantly enhances the precision of responses from large language models (LLMs) by incorporating additional, context-specific information&#x2014;such as a company&apos;s internal database&#x2014;into the query process. This method ensures the LLM&apos;s responses are firmly based on provided contextual data. The integration of rerankers with RAG within an information retrieval pipeline paves the way for the development of high-performance applications capable of delivering precise information retrieval across a vast array of domains.</p><p>Consider, for example, the scenario where you&apos;re planning to cook a special meal using specific ingredients favored by your guests. To simplify this task, you decide to employ a chatbot that suggests recipes based on renowned cookbooks. Initially, without a reranker, your input might look something like this:</p><pre><code class="language-text">Context information is below.
---------------------
{Madhur_Jaffrey_An_Invitation_To_Indian_Cooking}
{Julia_Child_Mastering_The_Art_Of_French_Cooking}
{Jiro_Ono_Sushi_Estetica_E_Tecnica}
---------------------

Given the context information and no prior knowledge, answer the query.
Query: Create a recipe using the ingredients below.
---------------------
Salmon
Asparagus
Potatoes
---------------------
</code></pre><p>Incorporating a reranker model, however, allows for a nuanced understanding of document relevance, enhancing the LLM&apos;s context with information about the relative importance of each cookbook. The refined input, highlighting the ranked relevance of the context to the query, might then appear as follows:</p><pre><code class="language-text">Context information is below.
Note that the context is ranked from most to least relevant to the query.
---------------------
{Julia_Child_Mastering_The_Art_Of_French_Cooking}
{Jiro_Ono_Sushi_Estetica_E_Tecnica}
{Madhur_Jaffrey_An_Invitation_To_Indian_Cooking}
---------------------

Given the context information and no prior knowledge, answer the query.
Query: Create a recipe using the ingredients below.
---------------------
Salmon
Asparagus
Potatoes
---------------------
</code></pre><p>This additional layer of insight into the cookbooks&apos; relevance to the given ingredients makes it evident that the optimal recipe likely falls within French cuisine, steering the chatbot&apos;s suggestion in a direction that best matches the ingredients&apos; culinary context. Without reranking, a suggestion based on sushi could have been equally plausible given the prominence of salmon, yet it would have led to a markedly different dining experience.</p><p>Moreover, by determining the number of documents to be considered in the ranking process, users can fine-tune the operation&apos;s precision and computational demands to suit their specific requirements, further illustrating the reranker&apos;s versatility and effectiveness.</p><h3 id="affordable-api-pricing">Affordable API pricing</h3><p>Jina Reranker API comes with identical pricing to our embedding API (including 1 million free trial), such as <code>jina-embeddings-v2-base-en</code> , based on the total number of tokens, in queries and documents. Token quotas can be bought on our <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Embeddings</a>/<a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io">Reranker API</a> page. The API secret and token quotas can be used for both reranker and embedding APIs.</p><h2 id="coming-soon-to-aws-marketplace">Coming Soon to AWS Marketplace</h2><p>Expanding our reach, Jina Reranker will not only be accessible via our API but is also set to debut on the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS SageMaker Marketplace</a> for seamless private cloud deployment. This upcoming availability aims to offer enhanced data protection and application security within the familiar confines of your AWS cloud subscription.</p><p>We highly value your insights and experiences with Jina Reranker. Talk to us on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord channel</a> to share your feedback and stay up-to-date with our latest models. Your input is crucial as we continue to refine our technologies and contribute to a more dynamic and inclusive search AI ecosystem.</p>]]></content:encoded></item><item><title><![CDATA[Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning]]></title><description><![CDATA[Our new paper explores how our Spanish-English and German-English models use multi-task contrastive learning and a sophisticated data pipeline to master language understanding and cross-lingual efficiency for texts up to 8192 tokens]]></description><link>https://jina.ai/news/revolutionizing-bilingual-text-embeddings-with-multi-task-contrastive-learning/</link><guid isPermaLink="false">65df105ab22368000152a1b9</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 28 Feb 2024 15:00:41 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/banner-4models-02.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/banner-4models-02.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><p>In our recent paper, <a href="https://arxiv.org/abs/2402.17016?ref=jina-ai-gmbh.ghost.io">Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</a>, we detailed our development of <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io">German-English</a> and <a href="https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context?ref=jina-ai-gmbh.ghost.io">Spanish-English</a> bilingual text embedding models.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2402.17016?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</div><div class="kg-bookmark-description">We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations. By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embedding models. This integration aims to stimulate further research and advancement in text embedding technologies for these languages.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Isabelle Mohr</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>Our approach utilizes multi-task contrastive learning and advanced data curation pipeline, focusing on bilingual capabilities while extending to support 8192 tokens in length. This method allows our models to excel in understanding target languages and in conducting cross-lingual evaluations efficiently.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context</div><div class="kg-bookmark-description">Jina AI&#x2019;s new bilingual Spanish-English embedding model brings the state-of-the-art in AI to half a billion Spanish speakers.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class="kg-bookmark-description">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>In addition to the bilingual models covered in the paper, we have also developed bilingual <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english?ref=jina-ai-gmbh.ghost.io">Chinese-English</a> and <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">monolingual English</a> models. These additions showcase our commitment to covering a broad spectrum of linguistic needs and furthering our capabilities in language processing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class="kg-bookmark-description">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI Launches World&#x2019;s First Open-Source 8K Text Embedding, Rivaling OpenAI</div><div class="kg-bookmark-description">Jina AI introduces jina-embeddings-v2, the world&#x2019;s first open-source model boasting an 8K context length. Matching the prowess of OpenAI&#x2019;s proprietary models, this innovation is now publicly accessible on Huggingface, signaling a significant milestone in the landscape of text embeddings.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>Our bilingual models are characterized by their efficiency, operating with optimized vocabulary sizes to require fewer parameters and less memory. This efficiency underscores our dedication to creating powerful yet resource-efficient tools for language processing.</p><p>Following the release of our paper, we expanded the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Massive Text Embedding Benchmark (MTEB)</a> to include benchmarks for our English-German and English-Spanish embedding models. This expansion is part of our effort to stimulate further research and advancements in text embedding technologies for non-English languages.</p><p>At Jina AI, our aim is to enhance the processing and understanding of multiple languages, contributing to the NLP field with our developments in bilingual and monolingual text embedding models.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.4 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-4-update/</link><guid isPermaLink="false">65ddb23939a5b70001810ef5</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Tue, 27 Feb 2024 10:01:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3234">Release Note (<code>3.23.4</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-1.jpg" alt="Jina 3.23.4 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.4?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.4 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.4) Release time: 2024-02-27 09:36:35 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Use unbound method to link to dry_run endpoint (#6143)
In prior releases, to assess Flow conne&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.4 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/b7615b0bbc1b3d94939b1085aaeac93489f7338634868cf0d42e2cbea2750690/jina-ai/jina/releases/tag/v3.23.4" alt="Jina 3.23.4 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="use-unbound-method-to-link-to-dryrun-endpoint-6143">Use unbound method to link to <code>dry_run</code> endpoint (<a href="https://github.com/jina-ai/jina/pull/6143?ref=jina-ai-gmbh.ghost.io">#6143</a>)</h3><p>In prior releases, to assess Flow connectivity Jina automatically bound an Executor to a <code>_dry_run_</code> endpoint via an object method. This release changes that to a class-unbound method, thus avoiding a cyclic reference in the inner <code>requests</code> object of the <code>Executor</code>.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[What is ColBERT and Late Interaction and Why They Matter in Search?]]></title><description><![CDATA[Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.]]></description><link>https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/</link><guid isPermaLink="false">65d3a2134a32310001f5b71b</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 20 Feb 2024 01:19:04 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><p>Last Friday, the release of the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">ColBERT model by Jina AI on Hugging Face</a> sparked significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is excited about 8192-length ColBERT? This article delves into the intricacies of ColBERT and ColBERTv2, highlighting their design, improvements, and the surprising effectiveness of ColBERT&apos;s late interaction.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a></figure><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">@huggingface</a> under Apache 2.0 licence<a href="https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io">https://t.co/snVGgI753H</a></p>&#x2014; Jina AI (@JinaAI_) <a href="https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">February 16, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-is-colbert">What is ColBERT?</h2><p>The name &quot;ColBERT&quot; stands for <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, a model stems from the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as <strong>late interaction</strong>, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the retrieval process. Specifically, there are two versions of the model:</p><ul><li><strong>ColBERT</strong>: The initial model was the brainchild of <a href="https://x.com/lateinteraction?s=20&amp;ref=jina-ai-gmbh.ghost.io"><strong>Omar Khattab</strong></a><strong> and Matei Zaharia</strong>, presenting a novel approach to information retrieval through the &quot;ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT&quot; paper. Their work was published in SIGIR 2020.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class="kg-bookmark-description">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT&#x2019;s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT&#x2019;s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Omar Khattab</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p><span style="white-space: pre-wrap;">The original ColBERT paper that introduces the &quot;late interaction&quot;.</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Building on the foundational work, <strong>Omar Khattab</strong> continued his research, collaborating with <strong>Barlas Oguz, Matei Zaharia, and Michael S. Bernstein</strong> to introduce &quot;ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,&quot; presented at SIGIR 2021. This next iteration of ColBERT addressed previous limitations and introduced key improvements, such as <strong>denoised supervision</strong> and <strong>residual compression</strong>, enhancing both the model&apos;s retrieval effectiveness and its storage efficiency.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class="kg-bookmark-description">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\times$.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Keshav Santhanam</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p><span style="white-space: pre-wrap;">ColBERTv2 adding denoised supervision and residual compression to improve the training data&apos;s quality and reduce the space footprint.</span></p></figcaption></figure><h2 id="understand-colberts-design">Understand ColBERT&apos;s Design</h2><p>Given that ColBERTv2&apos;s architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the original ColBERT.</p><h3 id="what-is-late-interaction-in-colbert">What is late interaction in ColBERT?</h3><p>&quot;Interaction&quot; refers to the process of evaluating the relevance between a query and a document by comparing their representations.</p><p>&quot;<em>Late interaction</em>&quot; is the essence of ColBERT. The term is derived from the model&apos;s architecture and processing strategy, where the interaction between the query and document representations occurs late in the process, after both have been independently encoded. This contrasts with &quot;<em>early interaction</em>&quot; models, where query and document embeddings interact at earlier stages, typically before or during their encoding by the model. </p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Interaction Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early Interaction</td>
<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>
</tr>
<tr>
<td>Late Interaction</td>
<td>ColBERT, ColBERTv2</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Early interaction can increase computational complexity since it requires considering all possible query-document pairs, making it less efficient for large-scale applications. </p><p>Late interaction models like ColBERT optimize for efficiency and scalability by allowing for the pre-computation of document representations and employing a more lightweight interaction step at the end, which focuses on the already encoded representations. This design choice enables faster retrieval times and reduced computational demands, making it more suitable for processing large document collections.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="300" height="143"><figcaption><span style="white-space: pre-wrap;">Schematic diagrams illustrating query&#x2013;document interaction paradigms in neural IR, with ColBERT&apos;s late interaction on the left-most.</span></figcaption></figure><h3 id="no-interaction-cosine-similarity-of-document-and-query-embeddings">No interaction: cosine similarity of document and query embeddings</h3><p>Many practical vector databases and neural search solutions depend on fast cosine similarity matching between document and query embeddings. While appealing for its straightforwardness and computational efficiency, this method, often referred to as &quot;<em>no interaction</em>&quot; or &quot;<em>not interaction-based</em>&quot; has been found to underperform in comparison to models that incorporate some form of interaction between queries and documents.</p><p>The core limitation of the &quot;no interaction&quot; approach lies in its inability to capture the complex nuances and relationships between query and document terms. Information retrieval, at its heart, is about understanding and matching the intent behind a query with the content within a document. This process often requires a deep, contextual understanding of the terms involved, something that single, aggregated embeddings for documents and queries struggle to provide.</p><h2 id="query-and-document-encoders-in-colbert">Query and document encoders in ColBERT</h2><p>ColBERT&apos;s encoding strategy is grounded in the BERT model, known for its deep contextual understanding of language. The model generates dense vector representations for each token in a query or document, <strong>creating a bag of contextualized embeddings for a query and a bag for a document, respectively. </strong>This facilitates a nuanced comparison of their embeddings during the late interaction phase.</p><h3 id="query-encoder-of-colbert">Query encoder of ColBERT</h3><p>For a query $Q$ with tokens ${q_1, q_2, ..., q_l}$, the process begins by tokenizing $Q$ into BERT-based WordPiece tokens and prepending a special <code>[Q]</code> token. This <code>[Q]</code> token, positioned right after BERT&#x2019;s <code>[CLS]</code> token, signals the start of a query. </p><p>If the query is shorter than a predefined number of tokens $N_q$, it is padded with <code>[mask]</code> tokens up to $N_q$; otherwise, it&apos;s truncated to the first $N_q$ tokens. The padded sequence is then passed through BERT, followed by a CNN (Convolutional Neural Network) and normalization. The output is a set of embedding vectors termed as $\mathbf{E}_q$ below:<br>$$\mathbf{E}_q := \mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[Q]},q_0,q_1,\ldots,q_l\mathtt{[mask]},\mathtt{[mask]},\ldots,\mathtt{[mask]}\right)\right)$$</p><h3 id="document-encoder-of-colbert">Document encoder of ColBERT</h3><p>Similarly, for a document $D$ with tokens ${d_1, d_2, ..., d_n}$, a <code>[D]</code> token is prepended to indicate the start of a document. This sequence, without the need for padding, undergoes the same process, results in a set of embedding vectors termed as $\mathbf{E}_d$ below:<br>$$\mathbf{E}_d := \mathrm{Filter}\left(\mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[D]},d_0,d_1,...,d_n\right)\right)\right)$$</p><p>The use of <code>[mask]</code> tokens for padding queries (coined as <strong>query augmentation </strong>in the paper) ensures uniform length across all queries, facilitating batch processing. The <code>[Q]</code> and <code>[D]</code> tokens explicitly mark the start of queries and documents, respectively, aiding the model in distinguishing between the two types of inputs.</p><h3 id="comparing-colbert-to-cross-encoders">Comparing ColBERT to cross-encoders</h3><p>Cross-encoders process pairs of queries and documents together, making them highly accurate but less efficient for large-scale tasks due to the computational cost of evaluating every possible pair. They excel in specific scenarios where the precise scoring of sentence pairs is necessary, such as in semantic similarity tasks or detailed content comparison. However, this design limits their applicability in situations requiring rapid retrieval from large datasets, where pre-computed embeddings and efficient similarity calculations are paramount.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="210" height="150"></figure><p>In contrast, ColBERT&#x2019;s late interaction model allows for pre-computation of document embeddings, significantly speeding up the retrieval process without compromising the depth of semantic analysis. This method, though seemingly counter-intuitive when compared to the direct approach of cross-encoders, offers a scalable solution for real-time and large-scale information retrieval tasks. It represents a strategic compromise between computational efficiency and the quality of interaction modeling.</p><h2 id="finding-the-top-k-documents-using-colbert">Finding the top-K documents using ColBERT</h2><p>Once we have embeddings for the query and documents, finding the most relevant top-K documents becomes straightforward (but not as straightforward as computing cosine of two vectors). </p><p>The key operations include a batch dot-product to compute term-wise similarities, max-pooling across document terms to find the highest similarity per query term, and summation across query terms to derive the total document score, followed by sorting the documents based on these scores. The pseudo PyTorch code is described below:</p><pre><code class="language-python">import torch

def compute_relevance_scores(query_embeddings, document_embeddings, k):
    &quot;&quot;&quot;
    Compute relevance scores for top-k documents given a query.
    
    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]
    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]
    :param k: Number of top documents to re-rank
    :return: Sorted document indices based on their relevance scores
    &quot;&quot;&quot;
    
    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]
    # Pad the k documents to their maximum length for batch operations
    # Note: Assuming document_embeddings is already padded and moved to GPU
    
    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)
    # Resulting shape: [k, num_query_terms, max_doc_length]
    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))
    
    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term
    # Shape after max-pool: [k, num_query_terms]
    max_scores_per_query_term = scores.max(dim=2).values
    
    # Sum the scores across query terms to get the total score for each document
    # Shape after sum: [k]
    total_scores = max_scores_per_query_term.sum(dim=1)
    
    # Sort the documents based on their total scores
    sorted_indices = total_scores.argsort(descending=True)
    
    return sorted_indices
</code></pre><p>Note that this procedure is used in both training and re-ranking at inference time. The ColBERT model is trained using a pairwise ranking loss, where the training data consists of triples $(q, d^+, d^-)$, where $q$ represents a query, $d^+$ is a relevant (positive) document for the query, and $d^-$ is a non-relevant (negative) document. The model aims to learn representations such that the similarity score between $q$ and $d^+$ is higher than the score between q and $d^-$.</p><p>The training objective can be mathematically represented as minimizing the following loss function: $$\mathrm{Loss} = \max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, where $S(q, d)$ denotes the similarity score computed by ColBERT between a query $q$ and a document $d$. This score is obtained by aggregating the max-similarity scores of the best-matching embeddings between the query and the document, following the late interaction pattern described in the model architecture. This approach ensures that the model is trained to distinguish between relevant and irrelevant documents for a given query, by encouraging a larger margin in the similarity scores for positive and negative document pairs.</p><h3 id="denoised-supervision-in-colbertv2">Denoised supervision in ColBERTv2</h3><p>Denoised supervision in ColBERTv2 refines the original training process by selecting challenging negatives and leveraging a cross-encoder for distillation. This sophisticated method of augmenting training data quality involves several steps:</p><ol><li><strong>Initial Training</strong>: Utilizing the official triples from the MS MARCO dataset, comprising a query, a relevant document, and a non-relevant document.</li><li><strong>Indexing and Retrieval</strong>: Employing ColBERTv2&apos;s compression to index training passages, followed by retrieving top-k passages for each query.</li><li><strong>Cross-Encoder Reranking</strong>: Enhancing passage selection through reranking by a MiniLM cross-encoder, distilling its scores into ColBERTv2.</li><li><strong>Forming Training Tuples</strong>: Generating w-way tuples for training, incorporating both high and lower-ranked passages to create challenging examples.</li><li><strong>Iterative Refinement</strong>: Repeating the process to continually improve the selection of hard negatives, thereby enhancing model performance.</li></ol><p>Note, this process represents a sophisticated enhancement to the ColBERT training regime rather than a fundamental change to its architecture.</p><h3 id="hyperparameters-of-colbert">Hyperparameters of ColBERT</h3><p>The hyperparameters of ColBERT is summarized below:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Best Choice</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>3 x 10^{-6}</td>
<td>Selected for fine-tuning to ensure stable and effective model updates.</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>Balances computational efficiency and the ability to capture sufficient information per update.</td>
</tr>
<tr>
<td>Number of Embeddings per Query (Nq)</td>
<td>32</td>
<td>Fixed to ensure a consistent representation size across queries, aiding in efficient processing.</td>
</tr>
<tr>
<td>Embedding Dimension (m)</td>
<td>128</td>
<td>Demonstrated to provide a good balance between representational power and computational efficiency.</td>
</tr>
<tr>
<td>Training Iterations</td>
<td>200k (MS MARCO), 125k (TREC CAR)</td>
<td>Chosen to ensure thorough learning while avoiding overfitting, with adjustments based on dataset characteristics.</td>
</tr>
<tr>
<td>Bytes per Dimension in Embeddings</td>
<td>4 (re-ranking), 2 (end-to-end ranking)</td>
<td>Trade-off between precision and space efficiency, with consideration for the application context (re-ranking vs. end-to-end).</td>
</tr>
<tr>
<td>Vector-Similarity Function</td>
<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>
<td>Selected based on performance and efficiency in the respective retrieval contexts.</td>
</tr>
<tr>
<td>FAISS Index Partitions (P)</td>
<td>2000</td>
<td>Determines the granularity of the search space partitioning, impacting search efficiency.</td>
</tr>
<tr>
<td>Nearest Partitions Searched (p)</td>
<td>10</td>
<td>Balances the breadth of the search against computational efficiency.</td>
</tr>
<tr>
<td>Sub-vectors per Embedding (s)</td>
<td>16</td>
<td>Affects the granularity of quantization, influencing both search speed and memory usage.</td>
</tr>
<tr>
<td>Index Representation per Dimension</td>
<td>16-bit values</td>
<td>Chosen for the second stage of end-to-end retrieval to manage the trade-off between accuracy and space.</td>
</tr>
<tr>
<td>Number of Layers in Encoders</td>
<td>12-layer BERT</td>
<td>Optimal balance between depth of contextual understanding and computational efficiency.</td>
</tr>
  <tr>
  <td>Max Query Length</td>
<td>128</td>
<td>The maximum number of tokens processed by the query encoder. <b>This gets extended in Jina-ColBERT model.</b></td>
</tr>
    <tr>
  <td>Max Document Length</td>
<td>512</td>
<td>The maximum number of tokens processed by the document encoder. <b>This gets extended to 8192 in Jina-ColBERT model. </b></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="the-indexing-strategy-of-colbert">The indexing strategy of ColBERT</h2><p>Unlike representation-based approaches that encode each document into one embedding vector, <strong>ColBERT encodes documents (and queries) into bags of embeddings, with each token in a document having its own embedding.</strong> This approach inherently means that for longer documents, more embeddings will be stored, <strong>which is a pain point of  the original ColBERT, and later addressed by ColBERTv2.</strong></p><p>The key to managing this efficiently lies in ColBERT&apos;s use of vector database (e.g. <a href="https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io">FAISS</a>) for indexing and retrieval, and its detailed indexing process which is designed to handle large volumes of data efficiently. The original ColBERT paper mentions several strategies to enhance the efficiency of indexing and retrieval, including:</p><ul><li><strong>Offline Indexing</strong>: Document representations are computed offline, allowing for the pre-computation and storage of document embeddings. This process leverages batch processing and GPU acceleration to handle large document collections efficiently.</li><li><strong>Embedding Storage</strong>: Document embeddings can be stored using 32-bit or 16-bit values for each dimension, offering a trade-off between precision and storage requirements. This flexibility allows ColBERT to maintain a balance between effectiveness (in terms of retrieval performance) and efficiency (in terms of storage and computational costs).</li></ul><p>The introduction of <strong>residual compression</strong> in ColBERTv2, which is a novel approach not present in the original ColBERT, plays a key role in reducing the model&apos;s space footprint by 6&#x2013;10&#xD7; while preserving quality. This technique compresses the embeddings further by effectively capturing and storing only the differences from a set of fixed reference centroids. </p><h2 id="effectiveness-and-efficiency-of-colbert">Effectiveness and Efficiency of ColBERT</h2><p>One might initially assume that incorporating BERT&apos;s deep contextual understanding into search would inherently require significant computational resources, making such an approach less feasible for real-time applications due to high latency and computational costs. However, ColBERT challenges and overturns this assumption through its innovative use of the late interaction mechanism. Here are some noteworthy points:</p><ol><li><strong>Significant Efficiency Gains</strong>: ColBERT achieves an orders-of-magnitude reduction in computational costs (FLOPs) and latency compared to traditional BERT-based ranking models. Specifically, for a given model size (e.g., 12-layer &quot;base&quot; transformer encoder), ColBERT not only matches but in some cases exceeds the effectiveness of BERT-based models with dramatically lower computational demands. For instance, at a re-ranking depth of <em>k</em>=10, BERT requires nearly 180&#xD7; more FLOPs than ColBERT; this gap widens as <em>k</em> increases, reaching 13900&#xD7; at <em>k</em>=1000 and even 23000&#xD7; at <em>k</em>=2000&#x200B;&#x200B;.</li><li><strong>Improved Recall and MRR@10 in End-to-End Retrieval</strong>: Contrary to the initial intuition that deeper interaction between query and document representations (as seen in early interaction models) would be necessary for high retrieval performance, ColBERT&apos;s end-to-end retrieval setup demonstrates superior effectiveness. For example, its Recall@50 exceeds the official BM25&apos;s Recall@1000 and almost all other models&apos; Recall@200, underscoring the model&apos;s remarkable ability to retrieve relevant documents from a vast collection without direct comparison of each query-document pair&#x200B;&#x200B;.</li><li><strong>Practicality for Real-World Applications</strong>: The experimental results underline ColBERT&apos;s practical applicability for real-world scenarios. Its indexing throughput and memory efficiency make it suitable for indexing large document collections like MS MARCO within a few hours, retaining high effectiveness with a manageable space footprint. These qualities highlight ColBERT&apos;s suitability for deployment in production environments where both performance and computational efficiency are paramount&#x200B;&#x200B;.</li><li><strong>Scalability with Document Collection Size</strong>: Perhaps the most surprising conclusion is ColBERT&apos;s scalability and efficiency in handling large-scale document collections. The architecture allows for the pre-computation of document embeddings and leverages efficient batch processing for query-document interaction, enabling the system to scale effectively with the size of the document collection. This scalability is counter-intuitive when considering the complexity and depth of understanding required for effective document retrieval, showcasing ColBERT&apos;s innovative approach to balancing computational efficiency with retrieval effectiveness.</li></ol><h2 id="using-jina-colbert-v1-en-a-8192-length-colbertv2-model">Using <code>jina-colbert-v1-en</code>: a 8192-length ColBERTv2 model</h2><p>Jina-ColBERT is designed for both fast and accurate retrieval, supporting <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">longer context lengths up to 8192, leveraging the advancements of JinaBERT</a>, which allows for longer sequence processing due to its architecture enhancements.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Strictly speaking Jina-ColBERT supports 8190-token length. Recall that in ColBERT document encoder, each document is padded with <code spellcheck="false" style="white-space: pre-wrap;">[D],[CLS]</code> in the beginning.</div></div><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-colbert-v1-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a></figure><h3 id="jinas-improvement-over-original-colbert">Jina&apos;s improvement over original ColBERT</h3><p>Jina-ColBERT&apos;s main advancement is its backbone, <code>jina-bert-v2-base-en</code>, which enables processing of significantly longer contexts (up to 8192 tokens) compared to the original ColBERT that uses <code>bert-base-uncased</code>. This capability is crucial for handling documents with extensive content, providing more detailed and contextual search results.</p><h3 id="jina-colbert-v1-en-performance-comparison-vs-colbertv2"><code>jina-colbert-v1-en</code> performance comparison vs. ColBERTv2</h3><p>We evaluated <code>jina-colbert-v1-en</code> on BEIR datasets and new LoCo benchmark which favors long-context, tested it against the original ColBERTv2 implementation and non-interaction based <code>jina-embeddings-v2-base-en</code> model.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ColBERTv2</th>
<th>jina-colbert-v1-en</th>
<th>jina-embeddings-v2-base-en</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arguana</td>
<td>46.5</td>
<td><strong>49.4</strong></td>
<td>44.0</td>
</tr>
<tr>
<td>Climate-Fever</td>
<td>18.1</td>
<td>19.6</td>
<td><strong>23.5</strong></td>
</tr>
<tr>
<td>DBPedia</td>
<td><strong>45.2</strong></td>
<td>41.3</td>
<td>35.1</td>
</tr>
<tr>
<td>FEVER</td>
<td>78.8</td>
<td><strong>79.5</strong></td>
<td>72.3</td>
</tr>
<tr>
<td>FiQA</td>
<td>35.4</td>
<td>36.8</td>
<td><strong>41.6</strong></td>
</tr>
<tr>
<td>HotpotQA</td>
<td><strong>67.5</strong></td>
<td>65.9</td>
<td>61.4</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>33.7</td>
<td><strong>33.8</strong></td>
<td>32.5</td>
</tr>
<tr>
<td>NQ</td>
<td>56.1</td>
<td>54.9</td>
<td><strong>60.4</strong></td>
</tr>
<tr>
<td>Quora</td>
<td>85.5</td>
<td>82.3</td>
<td><strong>88.2</strong></td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>15.4</td>
<td>16.9</td>
<td><strong>19.9</strong></td>
</tr>
<tr>
<td>SciFact</td>
<td>68.9</td>
<td><strong>70.1</strong></td>
<td>66.7</td>
</tr>
<tr>
<td>TREC-COVID</td>
<td>72.6</td>
<td><strong>75.0</strong></td>
<td>65.9</td>
</tr>
<tr>
<td>Webis-touch2020</td>
<td>26.0</td>
<td><strong>27.0</strong></td>
<td>26.2</td>
</tr>
<tr>
<td>LoCo</td>
<td>74.3</td>
<td>83.7</td>
<td><strong>85.4</strong></td>
</tr>
<tr>
<td>Average</td>
<td>51.7</td>
<td><strong>52.6</strong></td>
<td>51.6</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>This table demonstrates <code>jina-colbert-v1-en</code>&apos;s superior  performance, especially in scenarios requiring longer context lengths vs the original ColBERTv2. Note that <code>jina-embeddings-v2-base-en</code> <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">uses more training data</a>, whereas <code>jina-colbert-v1-en</code> only uses MSMARCO, which may justify the good performance of <code>jina-embeddings-v2-base-en</code> on some tasks.</p><h3 id="example-usage-of-jina-colbert-v1-en">Example usage of <code>jina-colbert-v1-en</code></h3><p>This snippet outlines the indexing process with Jina-ColBERT, showcasing its support for long documents.</p><pre><code class="language-python">from colbert import Indexer
from colbert.infra import Run, RunConfig, ColBERTConfig

n_gpu: int = 1  # Set your number of available GPUs
experiment: str = &quot;&quot;  # Name of the folder where the logs and created indices will be stored
index_name: str = &quot;&quot;  # The name of your index, i.e. the name of your vector database

if __name__ == &quot;__main__&quot;:
    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):
        config = ColBERTConfig(
          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents
        )
        indexer = Indexer(
          checkpoint=&quot;jinaai/jina-colbert-v1-en&quot;,
          config=config,
        )
        documents = [
          &quot;ColBERT is an efficient and effective passage retrieval model.&quot;,
          &quot;Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.&quot;,
          &quot;JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.&quot;,
          &quot;Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.&quot;,
          &quot;Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.&quot;,
          &quot;Jina is an easier way to build neural search systems.&quot;,
          &quot;You can use Jina-ColBERT to build neural search systems with ease.&quot;,
          # Add more documents here to ensure the clustering work correctly
        ]
        indexer.index(name=index_name, collection=documents)
</code></pre><h3 id="use-jina-colbert-v1-en-in-ragatouille">Use <code>jina-colbert-v1-en</code> in RAGatouille</h3><p>RAGatouille is a new Python library that facilitates the use of advanced retrieval methods within RAG pipelines. It&apos;s designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines, making it accessible for developers to utilize these methods without needing deep expertise in the underlying research. Thanks to <a href="https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io">Benjamin Clavi&#xE9;</a>, you can now use <code>jina-colbert-v1-en</code> easily:</p><pre><code class="language-python">from ragatouille import RAGPretrainedModel

# Get your model &amp; collection of big documents ready
RAG = RAGPretrainedModel.from_pretrained(&quot;jinaai/jina-colbert-v1-en&quot;)
my_documents = [
    &quot;very long document1&quot;,
    &quot;very long document2&quot;,
    # ... more documents
]

# And create an index with them at full length!
RAG.index(collection=my_documents,
          index_name=&quot;the_biggest_index&quot;,
          max_document_length=8190,)

# or encode them in-memory with no truncation, up to your model&apos;s max length
RAG.encode(my_documents)
</code></pre><p>For more detailed information and further exploration of Jina-ColBERT, you can visit the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">Hugging Face page</a>.</p><h2 id="conclusion">Conclusion</h2><p>ColBERT represents a significant leap forward in the field of information retrieval. By enabling longer context lengths with Jina-ColBERT and maintaining compatibility with the ColBERT approach to late interaction, it offers a powerful alternative for developers looking to implement state-of-the-art search functionality.</p><p>Coupled with the RAGatouille library, which simplifies the integration of complex retrieval models into RAG pipelines, developers can now harness the power of advanced retrieval with ease, streamlining their workflows and enhancing their applications. The synergy between Jina-ColBERT and RAGatouille illustrates a remarkable stride in making advanced AI search models accessible and efficient for practical use.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.3 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-3-update/</link><guid isPermaLink="false">65cf33b44a32310001f5b6d8</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 16 Feb 2024 11:29:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3233">Release Note (<code>3.23.3</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" alt="Jina 3.23.3 Update"><p>This release contains 1 bug fix and 1 documentation improvement.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.3?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.3 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.3) Release time: 2024-02-16 10:56:19 This release contains 1 bug fix and 1 documentation improvement.
&#x1F41E; Bug Fixes
Fix dynamic creation of schema with nested DocLists in Gateway&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.3 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/261d3e0cd12981bbe960a3e5cee076fdc791818f19a3ffb9a68e79eafe63114c/jina-ai/jina/releases/tag/v3.23.3" alt="Jina 3.23.3 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-dynamic-creation-of-schema-with-nested-doclists-in-gateway-6138">Fix dynamic creation of schema with nested DocLists in Gateway (<a href="https://github.com/jina-ai/jina/pull/6138?ref=jina-ai-gmbh.ghost.io">#6138</a>)</h3><p>Previously, nested document types with multiple&#xA0;<code>DocList</code>&#xA0;levels of nesting sometimes led to the schema exposed by the Gateway being incomplete because some references were lost.</p><p>Now schemas like this exposed through&#xA0;<code>Flow</code>&#xA0;work as expected:</p><pre><code class="language-python">class QuoteFile(BaseDoc):
    quote_file_id: int
    texts: DocList[TextDoc]
    images: DocList[ImageDoc]

class SearchResult(BaseDoc):
    results: DocList[QuoteFile]</code></pre><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Fix topology documentation (<a href="https://github.com/jina-ai/jina/pull/6134?ref=jina-ai-gmbh.ghost.io">docs: fix topology documentation<u>&#xA0;#6134</u></a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Aquí Se Habla Español: Top-Quality Spanish-English Embeddings and 8k Context]]></title><description><![CDATA[Jina AI's new bilingual Spanish-English embedding model brings the state-of-the-art in AI to half a billion Spanish speakers.]]></description><link>https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context/</link><guid isPermaLink="false">65cc8e2f4a32310001f5b5f1</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 14 Feb 2024 15:30:36 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><p>Jina AI is <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">once</a> <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">again</a> <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">demonstrating</a> its commitment to high-quality multilingual AI models by releasing its <strong>Spanish-English bilingual model</strong>.</p><p>This model provides embedding vectors for texts of up to 8k tokens in Spanish or English, designed so that if texts in the two languages mean the same thing, their embeddings will be geometrically close together. Jina Embeddings v2 for Spanish and English is ideally suited for cross-language information retrieval, bilingual semantic analysis, and bilingual RAG applications.</p><p>This new model, <code>jina-embeddings-v2-base-es</code>, brings to Spanish the same state-of-the-art performance and ground-breaking feature set of Jina AI&#x2019;s <code>v2</code> models for <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English</a>, <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>:</p><ul><li>8,192 tokens of input context, a leader among open-source embedding models.</li><li>Real bilingualism instead of uneven multilingualism. Jina AI&#x2019;s bilingual models are trained to give balanced support to both languages, avoiding the <a href="https://arxiv.org/abs/2210.05619?ref=jina-ai-gmbh.ghost.io">biases of &#x201C;multilingual&#x201D; models</a> trained on uncurated Internet scrapes.</li><li><code>jina-embeddings-v2-base-es</code>&#xA0;is compact compared to open-source models of comparable performance. The embeddings themselves are 768 dimensions, saving space and run-time in production.</li><li>Jina Embeddings v2 models are fully integrated into major vector databases, RAG frameworks, and AI development libraries:<ul><li><a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a></li><li><a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a></li><li><a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a></li><li><a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul></li></ul><p>Jina Embeddings v2 for Spanish and English is accessible via the Jina <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Embeddings API </a> right now, with one million free tokens, so you pay nothing to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><h2 id="benchmarks">Benchmarks</h2><p>On Spanish benchmarks, Jina v2 for Spanish and English outperforms the <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Multilingual E5 base model</a> and the <a href="https://huggingface.co/BAAI/bge-m3?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">BGE M3 model</a>, the only comparable open-source models with Spanish support. The tests below (<a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MTEB-es</a>) are adapted from the <a href="https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Massive Text Embeddings Benchmark</a>. You can view and run them from this <a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">GitHub repository</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-es: MTEB: Massive Text Embedding Benchmark with Spanish datasets</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark with Spanish datasets - jina-ai/mteb-es</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/6e71c1c34b09bea0278cbed9f0c639554954e953c35b93d4576539b808f0f813/jina-ai/mteb-es" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="2000" height="227" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png 2241w"></figure><p>Jina Embeddings outperforms E5 on all metrics except classification and outperforms BGE-M3 in retrieval, clustering, and cross-language tasks, despite being 15% to 30% of the size of these larger models.</p><ul><li>Significantly better performance in <strong>retrieval</strong> tasks (like finding related documents in a database) and <strong>clustering</strong> (identifying groups of documents that belong together in a collection)</li><li>Roughly equal performance with E5 on <strong>reranking</strong> (ordering documents by semantic similarity) and near-equal performance on <strong>text classification</strong> in Spanish.</li><li>All three models have very similar benchmark scores for <strong>cross-language tasks</strong> (finding semantically similar texts in English to a Spanish input, or vice-versa), although Jina Embeddings still performs the best.</li></ul><p>When compared to closed-source multilingual models from Open AI and Cohere, Jina Embeddings&#x2019; compact size makes its achievements even more impressive.</p><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="1992" height="254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png 1992w"></figure><p>On retrieval tasks in Spanish, Jina outperforms the closed-source models offered by Open AI and Cohere and outperforms Open AI (and nearly equals Cohere&#x2019;s performance) on cross-language tasks.</p><h2 id="jina-embeddings-ai-for-a-multilingual-world">Jina Embeddings: AI for a Multilingual World</h2><p>Spanish is spoken by well over half a billion people, with official status in more than 20 countries, along with the <a href="https://european-union.europa.eu/index_es?ref=jina-ai-gmbh.ghost.io">European Union</a>, the <a href="https://www.un.org/es/?ref=jina-ai-gmbh.ghost.io">United Nations</a>, the <a href="https://www.wto.org/indexsp.htm?ref=jina-ai-gmbh.ghost.io">World Trade Organization</a>, and <a href="https://www.fifa.com/fifaplus/es/?ref=jina-ai-gmbh.ghost.io">FIFA</a>. Introducing this specialized bilingual model makes clear Jina AI&#x2019;s commitment to bringing AI technologies to everyone.</p><p>In addition to Spanish and its high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English monolingual model</a>, Jina AI currently offers state-of-the-art embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>, with more to come.</p><p>Jina AI is committed to advancing AI technology for the broadest audience, placing a high value on transparency, accessibility, affordability, privacy, and data protection.</p><p>We value your feedback on all our models. Join our community channel to contribute and stay informed about new developments.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4427 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure>]]></content:encoded></item></channel></rss>
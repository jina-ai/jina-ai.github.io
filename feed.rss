<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.79</generator><lastBuildDate>Tue, 20 Feb 2024 01:30:38 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[What is ColBERT and Late Interaction and Why They Matter in Search?]]></title><description><![CDATA[Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.]]></description><link>https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/</link><guid isPermaLink="false">65d3a2134a32310001f5b71b</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 20 Feb 2024 01:19:04 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><p>Last Friday, the release of the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">ColBERT model by Jina AI on Hugging Face</a> sparked significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is excited about 8192-length ColBERT? This article delves into the intricacies of ColBERT and ColBERTv2, highlighting their design, improvements, and the surprising effectiveness of ColBERT&apos;s late interaction.</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">@huggingface</a> under Apache 2.0 licence<a href="https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io">https://t.co/snVGgI753H</a></p>&#x2014; Jina AI (@JinaAI_) <a href="https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">February 16, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-is-colbert">What is ColBERT?</h2><p>The name &quot;ColBERT&quot; stands for <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, a model stems from the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as <strong>late interaction</strong>, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the retrieval process. Specifically, there are two versions of the model:</p><ul><li><strong>ColBERT</strong>: The initial model was the brainchild of <a href="https://x.com/lateinteraction?s=20&amp;ref=jina-ai-gmbh.ghost.io"><strong>Omar Khattab</strong></a><strong> and Matei Zaharia</strong>, presenting a novel approach to information retrieval through the &quot;ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT&quot; paper. Their work was published in SIGIR 2020.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class="kg-bookmark-description">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT&#x2019;s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT&#x2019;s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Omar Khattab</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">The original ColBERT paper that introduces the &quot;late interaction&quot;.</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Building on the foundational work, <strong>Omar Khattab</strong> continued his research, collaborating with <strong>Barlas Oguz, Matei Zaharia, and Michael S. Bernstein</strong> to introduce &quot;ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,&quot; presented at SIGIR 2021. This next iteration of ColBERT addressed previous limitations and introduced key improvements, such as <strong>denoised supervision</strong> and <strong>residual compression</strong>, enhancing both the model&apos;s retrieval effectiveness and its storage efficiency.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class="kg-bookmark-description">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\times$.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Keshav Santhanam</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">ColBERTv2 adding </span><b><strong style="white-space: pre-wrap;">denoised supervision</strong></b><span style="white-space: pre-wrap;"> and </span><b><strong style="white-space: pre-wrap;">residual compression</strong></b><span style="white-space: pre-wrap;"> to improve the training data&apos;s quality and reduce the space footprint.</span></p></figcaption></figure><h2 id="understand-colberts-design">Understand ColBERT&apos;s Design</h2><p>Given that ColBERTv2&apos;s architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the original ColBERT.</p><h3 id="what-is-late-interaction-in-colbert">What is late interaction in ColBERT?</h3><p>&quot;Interaction&quot; refers to the process of evaluating the relevance between a query and a document by comparing their representations.</p><p>&quot;<em>Late interaction</em>&quot; is the essence of ColBERT. The term is derived from the model&apos;s architecture and processing strategy, where the interaction between the query and document representations occurs late in the process, after both have been independently encoded. This contrasts with &quot;<em>early interaction</em>&quot; models, where query and document embeddings interact at earlier stages, typically before or during their encoding by the model. </p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Interaction Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early Interaction</td>
<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>
</tr>
<tr>
<td>Late Interaction</td>
<td>ColBERT, ColBERTv2</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Early interaction can increase computational complexity since it requires considering all possible query-document pairs, making it less efficient for large-scale applications. </p><p>Late interaction models like ColBERT optimize for efficiency and scalability by allowing for the pre-computation of document representations and employing a more lightweight interaction step at the end, which focuses on the already encoded representations. This design choice enables faster retrieval times and reduced computational demands, making it more suitable for processing large document collections.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="300" height="143"><figcaption><span style="white-space: pre-wrap;">Schematic diagrams illustrating query&#x2013;document interaction paradigms in neural IR, with ColBERT&apos;s late interaction on the left-most.</span></figcaption></figure><h3 id="no-interaction-cosine-similarity-of-document-and-query-embeddings">No interaction: cosine similarity of document and query embeddings</h3><p>Many practical vector databases and neural search solutions depend on fast cosine similarity matching between document and query embeddings. While appealing for its straightforwardness and computational efficiency, this method, often referred to as &quot;<em>no interaction</em>&quot; or &quot;<em>not interaction-based</em>&quot; has been found to underperform in comparison to models that incorporate some form of interaction between queries and documents.</p><p>The core limitation of the &quot;no interaction&quot; approach lies in its inability to capture the complex nuances and relationships between query and document terms. Information retrieval, at its heart, is about understanding and matching the intent behind a query with the content within a document. This process often requires a deep, contextual understanding of the terms involved, something that single, aggregated embeddings for documents and queries struggle to provide.</p><h3 id="query-and-document-encoders-in-colbert">Query and document encoders in ColBERT</h3><p>ColBERT&apos;s encoding strategy is grounded in the BERT model, known for its deep contextual understanding of language. The model generates dense vector representations for each token in a query or document, <strong>creating a bag of contextualized embeddings for a query and a bag for a document, respectively. </strong>This facilitates a nuanced comparison of their embeddings during the late interaction phase.</p><h4 id="query-encoder-of-colbert"><strong>Query encoder of ColBERT</strong></h4><p>For a query ($Q$) with tokens (${q_1, q_2, ..., q_l}$), the process begins by tokenizing ($Q$) into BERT-based WordPiece tokens and prepending a special <code>[Q]</code> token. This <code>[Q]</code> token, positioned right after BERT&#x2019;s <code>[CLS]</code> token, signals the start of a query. </p><p>If the query is shorter than a predefined number of tokens ($N_q$), it is padded with <code>[mask]</code> tokens up to ($N_q$); otherwise, it&apos;s truncated to the first ($N_q$) tokens. The padded sequence is then passed through BERT, followed by a CNN (Convolutional Neural Network) and normalization. The output is a set of embedding vectors termed as $\mathbf{E}_q$ below:<br>$$\mathbf{E}_q := \mathrm{Normalize}\left(\mathrm{CNN}\left(\mathrm{BERT}\left(\mathtt{[Q]},q_0,q_1,\ldots,q_l\mathtt{[mask]},\mathtt{[mask]},\ldots,\mathtt{[mask]}\right)\right)\right)$$</p><h4 id="document-encoder-of-colbert">Document encoder of <strong>ColBERT</strong></h4><p>Similarly, for a document ($D$) with tokens (${d_1, d_2, ..., d_n}$), a <code>[D]</code> token is prepended to indicate the start of a document. This sequence, without the need for padding, undergoes the same process, results in a set of embedding vectors termed as $\mathbf{E}_d$ below:<br>$$\mathbf{E}_d := \mathrm{Filter}\left(\mathrm{Normalize}\left(\mathrm{CNN}\left(\mathrm{BERT}\left(\mathtt{[D]},d_0,d_1,...,d_n\right)\right)\right)\right)$$</p><p>The use of <code>[mask]</code> tokens for padding queries (coined as <strong>query augmentation </strong>in the paper) ensures uniform length across all queries, facilitating batch processing. The <code>[Q]</code> and <code>[D]</code> tokens explicitly mark the start of queries and documents, respectively, aiding the model in distinguishing between the two types of inputs.</p><h4 id="comparing-colbert-to-cross-encoders">Comparing <strong>ColBERT</strong> to cross-encoders</h4><p>Cross-encoders process pairs of queries and documents together, making them highly accurate but less efficient for large-scale tasks due to the computational cost of evaluating every possible pair. They excel in specific scenarios where the precise scoring of sentence pairs is necessary, such as in semantic similarity tasks or detailed content comparison. However, this design limits their applicability in situations requiring rapid retrieval from large datasets, where pre-computed embeddings and efficient similarity calculations are paramount.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="210" height="150"></figure><p>In contrast, ColBERT&#x2019;s late interaction model allows for pre-computation of document embeddings, significantly speeding up the retrieval process without compromising the depth of semantic analysis. This method, though seemingly counter-intuitive when compared to the direct approach of cross-encoders, offers a scalable solution for real-time and large-scale information retrieval tasks. It represents a strategic compromise between computational efficiency and the quality of interaction modeling.</p><h3 id="finding-the-top-k-documents-using-colbert">Finding the top-K documents using <strong>ColBERT</strong></h3><p>Once we have embeddings for the query and documents, finding the most relevant top-K documents becomes straightforward (but not as straightforward as computing cosine of two vectors). </p><p>The key operations include a batch dot-product to compute term-wise similarities, max-pooling across document terms to find the highest similarity per query term, and summation across query terms to derive the total document score, followed by sorting the documents based on these scores. The pseudo PyTorch code is described below:</p><pre><code class="language-python">import torch

def compute_relevance_scores(query_embeddings, document_embeddings, k):
    &quot;&quot;&quot;
    Compute relevance scores for top-k documents given a query.
    
    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]
    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]
    :param k: Number of top documents to re-rank
    :return: Sorted document indices based on their relevance scores
    &quot;&quot;&quot;
    
    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]
    # Pad the k documents to their maximum length for batch operations
    # Note: Assuming document_embeddings is already padded and moved to GPU
    
    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)
    # Resulting shape: [k, num_query_terms, max_doc_length]
    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))
    
    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term
    # Shape after max-pool: [k, num_query_terms]
    max_scores_per_query_term = scores.max(dim=2).values
    
    # Sum the scores across query terms to get the total score for each document
    # Shape after sum: [k]
    total_scores = max_scores_per_query_term.sum(dim=1)
    
    # Sort the documents based on their total scores
    sorted_indices = total_scores.argsort(descending=True)
    
    return sorted_indices
</code></pre><p>Note that this procedure is used in both training and re-ranking at inference time. The ColBERT model is trained using a pairwise ranking loss, where the training data consists of triples ($(q, d^+, d^-)$), where ($q$) represents a query, ($d^+$) is a relevant (positive) document for the query, and ($d^-$) is a non-relevant (negative) document. The model aims to learn representations such that the similarity score between ($q$) and ($d^+$) is higher than the score between (q) and ($d^-$).</p><p>The training objective can be mathematically represented as minimizing the following loss function: $$\mathrm{Loss} = \max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, where ($S(q, d)$) denotes the similarity score computed by ColBERT between a query ($q$) and a document ($d$). This score is obtained by aggregating the max-similarity scores of the best-matching embeddings between the query and the document, following the late interaction pattern described in the model architecture. This approach ensures that the model is trained to distinguish between relevant and irrelevant documents for a given query, by encouraging a larger margin in the similarity scores for positive and negative document pairs.</p><h3 id="denoised-supervision-in-colbertv2">Denoised supervision in ColBERTv2</h3><p>Denoised supervision in ColBERTv2 refines the original training process by selecting challenging negatives and leveraging a cross-encoder for distillation. This sophisticated method of augmenting training data quality involves several steps:</p><ol><li><strong>Initial Training</strong>: Utilizing the official triples from the MS MARCO dataset, comprising a query, a relevant document, and a non-relevant document.</li><li><strong>Indexing and Retrieval</strong>: Employing ColBERTv2&apos;s compression to index training passages, followed by retrieving top-k passages for each query.</li><li><strong>Cross-Encoder Reranking</strong>: Enhancing passage selection through reranking by a MiniLM cross-encoder, distilling its scores into ColBERTv2.</li><li><strong>Forming Training Tuples</strong>: Generating w-way tuples for training, incorporating both high and lower-ranked passages to create challenging examples.</li><li><strong>Iterative Refinement</strong>: Repeating the process to continually improve the selection of hard negatives, thereby enhancing model performance.</li></ol><p>Note, this process represents a sophisticated enhancement to the ColBERT training regime rather than a fundamental change to its architecture.</p><h3 id="hyperparameters-of-colbert">Hyperparameters of ColBERT</h3><p>The hyperparameters of ColBERT is summarized below:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Best Choice</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>(3 x 10^{-6})</td>
<td>Selected for fine-tuning to ensure stable and effective model updates.</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>Balances computational efficiency and the ability to capture sufficient information per update.</td>
</tr>
<tr>
<td>Number of Embeddings per Query (Nq)</td>
<td>32</td>
<td>Fixed to ensure a consistent representation size across queries, aiding in efficient processing.</td>
</tr>
<tr>
<td>Embedding Dimension (m)</td>
<td>128</td>
<td>Demonstrated to provide a good balance between representational power and computational efficiency.</td>
</tr>
<tr>
<td>Training Iterations</td>
<td>200k (MS MARCO), 125k (TREC CAR)</td>
<td>Chosen to ensure thorough learning while avoiding overfitting, with adjustments based on dataset characteristics.</td>
</tr>
<tr>
<td>Bytes per Dimension in Embeddings</td>
<td>4 (re-ranking), 2 (end-to-end ranking)</td>
<td>Trade-off between precision and space efficiency, with consideration for the application context (re-ranking vs. end-to-end).</td>
</tr>
<tr>
<td>Vector-Similarity Function</td>
<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>
<td>Selected based on performance and efficiency in the respective retrieval contexts.</td>
</tr>
<tr>
<td>FAISS Index Partitions (P)</td>
<td>2000</td>
<td>Determines the granularity of the search space partitioning, impacting search efficiency.</td>
</tr>
<tr>
<td>Nearest Partitions Searched (p)</td>
<td>10</td>
<td>Balances the breadth of the search against computational efficiency.</td>
</tr>
<tr>
<td>Sub-vectors per Embedding (s)</td>
<td>16</td>
<td>Affects the granularity of quantization, influencing both search speed and memory usage.</td>
</tr>
<tr>
<td>Index Representation per Dimension</td>
<td>16-bit values</td>
<td>Chosen for the second stage of end-to-end retrieval to manage the trade-off between accuracy and space.</td>
</tr>
<tr>
<td>Number of Layers in Encoders</td>
<td>12-layer BERT</td>
<td>Optimal balance between depth of contextual understanding and computational efficiency.</td>
</tr>
  <tr>
  <td>Max Query Length</td>
<td>128</td>
<td>The maximum number of tokens processed by the query encoder. <b>This gets extended in Jina-ColBERT model.</b></td>
</tr>
    <tr>
  <td>Max Document Length</td>
<td>512</td>
<td>The maximum number of tokens processed by the document encoder. <b>This gets extended to 8192 in Jina-ColBERT model. </b></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="the-indexing-strategy-of-colbert">The indexing strategy of ColBERT</h3><p>Unlike representation-based approaches that encode each document into one embedding vector, <strong>ColBERT encodes documents (and queries) into bags of embeddings, with each token in a document having its own embedding.</strong> This approach inherently means that for longer documents, more embeddings will be stored, <strong>which is a pain point of  the original ColBERT, and later addressed by ColBERTv2.</strong></p><p>The key to managing this efficiently lies in ColBERT&apos;s use of vector database (e.g. <a href="https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io">FAISS</a>) for indexing and retrieval, and its detailed indexing process which is designed to handle large volumes of data efficiently. The original ColBERT paper mentions several strategies to enhance the efficiency of indexing and retrieval, including:</p><ul><li><strong>Offline Indexing</strong>: Document representations are computed offline, allowing for the pre-computation and storage of document embeddings. This process leverages batch processing and GPU acceleration to handle large document collections efficiently.</li><li><strong>Embedding Storage</strong>: Document embeddings can be stored using 32-bit or 16-bit values for each dimension, offering a trade-off between precision and storage requirements. This flexibility allows ColBERT to maintain a balance between effectiveness (in terms of retrieval performance) and efficiency (in terms of storage and computational costs).</li></ul><p>The introduction of <strong>residual compression</strong> in ColBERTv2, which is a novel approach not present in the original ColBERT, plays a key role in reducing the model&apos;s space footprint by 6&#x2013;10&#xD7; while preserving quality. This technique compresses the embeddings further by effectively capturing and storing only the differences from a set of fixed reference centroids. </p><h2 id="effectiveness-and-efficiency-of-colbert">Effectiveness and Efficiency of ColBERT</h2><p>One might initially assume that incorporating BERT&apos;s deep contextual understanding into search would inherently require significant computational resources, making such an approach less feasible for real-time applications due to high latency and computational costs. However, ColBERT challenges and overturns this assumption through its innovative use of the late interaction mechanism. Here are some noteworthy points:</p><ol><li><strong>Significant Efficiency Gains</strong>: ColBERT achieves an orders-of-magnitude reduction in computational costs (FLOPs) and latency compared to traditional BERT-based ranking models. Specifically, for a given model size (e.g., 12-layer &quot;base&quot; transformer encoder), ColBERT not only matches but in some cases exceeds the effectiveness of BERT-based models with dramatically lower computational demands. For instance, at a re-ranking depth of <em>k</em>=10, BERT requires nearly 180&#xD7; more FLOPs than ColBERT; this gap widens as <em>k</em> increases, reaching 13900&#xD7; at <em>k</em>=1000 and even 23000&#xD7; at <em>k</em>=2000&#x200B;&#x200B;.</li><li><strong>Improved Recall and MRR@10 in End-to-End Retrieval</strong>: Contrary to the initial intuition that deeper interaction between query and document representations (as seen in early interaction models) would be necessary for high retrieval performance, ColBERT&apos;s end-to-end retrieval setup demonstrates superior effectiveness. For example, its Recall@50 exceeds the official BM25&apos;s Recall@1000 and almost all other models&apos; Recall@200, underscoring the model&apos;s remarkable ability to retrieve relevant documents from a vast collection without direct comparison of each query-document pair&#x200B;&#x200B;.</li><li><strong>Practicality for Real-World Applications</strong>: The experimental results underline ColBERT&apos;s practical applicability for real-world scenarios. Its indexing throughput and memory efficiency make it suitable for indexing large document collections like MS MARCO within a few hours, retaining high effectiveness with a manageable space footprint. These qualities highlight ColBERT&apos;s suitability for deployment in production environments where both performance and computational efficiency are paramount&#x200B;&#x200B;.</li><li><strong>Scalability with Document Collection Size</strong>: Perhaps the most surprising conclusion is ColBERT&apos;s scalability and efficiency in handling large-scale document collections. The architecture allows for the pre-computation of document embeddings and leverages efficient batch processing for query-document interaction, enabling the system to scale effectively with the size of the document collection. This scalability is counter-intuitive when considering the complexity and depth of understanding required for effective document retrieval, showcasing ColBERT&apos;s innovative approach to balancing computational efficiency with retrieval effectiveness.</li></ol><h2 id="using-jina-colbert-v1-en">Using <code>jina-colbert-v1-en</code></h2><p>Jina-ColBERT is designed for both fast and accurate retrieval, supporting <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">longer context lengths up to 8192, leveraging the advancements of JinaBERT</a>, which allows for longer sequence processing due to its architecture enhancements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-colbert-v1-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a></figure><h3 id="jinas-improvement-over-original-colbert">Jina&apos;s improvement over original ColBERT</h3><p>Jina-ColBERT&apos;s main advancement is its backbone, <code>jina-bert-v2-base-en</code>, which enables processing of significantly longer contexts (up to 8192 tokens) compared to the original ColBERT that uses <code>bert-base-uncased</code>. This capability is crucial for handling documents with extensive content, providing more detailed and contextual search results.</p><h3 id="jina-colbert-v1-en-performance-comparison-vs-colbertv2"><code>jina-colbert-v1-en</code> performance comparison vs. ColBERTv2</h3>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Metric</th>
<th>ColBERTv2</th>
<th>Jina-ColBERT-v1</th>
</tr>
</thead>
<tbody>
<tr>
<td>MRR@10</td>
<td>39.7</td>
<td>39.0</td>
</tr>
<tr>
<td>Recall@50</td>
<td>86.8</td>
<td>85.6</td>
</tr>
<tr>
<td>Recall@1k</td>
<td>97.6</td>
<td>96.2</td>
</tr>
<tr>
<td>Avg. NDCG@10 (LoCo)</td>
<td>74.3</td>
<td>83.7</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>This table demonstrates <code>jina-colbert-v1-en</code>&apos;s competitive or superior performance, especially in scenarios requiring longer context lengths.</p><h3 id="example-code-snippet">Example code snippet</h3><p>This snippet outlines the indexing process with Jina-ColBERT, showcasing its support for long documents.</p><pre><code class="language-python">from colbert import Indexer
from colbert.infra import Run, RunConfig, ColBERTConfig

n_gpu: int = 1  # Set your number of available GPUs
experiment: str = &quot;&quot;  # Name of the folder where the logs and created indices will be stored
index_name: str = &quot;&quot;  # The name of your index, i.e. the name of your vector database

if __name__ == &quot;__main__&quot;:
    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):
        config = ColBERTConfig(
          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents
        )
        indexer = Indexer(
          checkpoint=&quot;jinaai/jina-colbert-v1-en&quot;,
          config=config,
        )
        documents = [
          &quot;ColBERT is an efficient and effective passage retrieval model.&quot;,
          &quot;Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.&quot;,
          &quot;JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.&quot;,
          &quot;Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.&quot;,
          &quot;Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.&quot;,
          &quot;Jina is an easier way to build neural search systems.&quot;,
          &quot;You can use Jina-ColBERT to build neural search systems with ease.&quot;,
          # Add more documents here to ensure the clustering work correctly
        ]
        indexer.index(name=index_name, collection=documents)
</code></pre><h3 id="using-jina-colbert-v1-en-in-ragatouille">Using <code>jina-colbert-v1-en</code> in RAGatouille</h3><p>RAGatouille is a new Python library that facilitates the use of advanced retrieval methods within RAG pipelines. It&apos;s designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines, making it accessible for developers to utilize these methods without needing deep expertise in the underlying research. Thanks to <a href="https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io">Benjamin Clavi&#xE9;</a>, you can now use <code>jina-colbert-v1-en</code> easily:</p><pre><code class="language-python">from ragatouille import RAGPretrainedModel

# Get your model &amp; collection of big documents ready
RAG = RAGPretrainedModel.from_pretrained(&quot;jinaai/jina-colbert-v1-en&quot;)
my_documents = [
    &quot;very long document1&quot;,
    &quot;very long document2&quot;,
    # ... more documents
]

# And create an index with them at full length!
RAG.index(collection=my_documents,
          index_name=&quot;the_biggest_index&quot;,
          max_document_length=8190,)

# or encode them in-memory with no truncation, up to your model&apos;s max length
RAG.encode(my_documents)
</code></pre><p>For more detailed information and further exploration of Jina-ColBERT, you can visit the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">Hugging Face page</a>.</p><h2 id="conclusion">Conclusion</h2><p>ColBERT represents a significant leap forward in the field of information retrieval. By enabling longer context lengths with Jina-ColBERT and maintaining compatibility with the ColBERT approach to late interaction, it offers a powerful alternative for developers looking to implement state-of-the-art search functionality.</p><p>Coupled with the RAGatouille library, which simplifies the integration of complex retrieval models into RAG pipelines, developers can now harness the power of advanced retrieval with ease, streamlining their workflows and enhancing their applications. The synergy between Jina-ColBERT and RAGatouille illustrates a remarkable stride in making advanced AI search models accessible and efficient for practical use.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.3 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-3-update/</link><guid isPermaLink="false">65cf33b44a32310001f5b6d8</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 16 Feb 2024 11:29:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3233">Release Note (<code>3.23.3</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" alt="Jina 3.23.3 Update"><p>This release contains 1 bug fix and 1 documentation improvement.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.3?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.3 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.3) Release time: 2024-02-16 10:56:19 This release contains 1 bug fix and 1 documentation improvement.
&#x1F41E; Bug Fixes
Fix dynamic creation of schema with nested DocLists in Gateway&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.3 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/261d3e0cd12981bbe960a3e5cee076fdc791818f19a3ffb9a68e79eafe63114c/jina-ai/jina/releases/tag/v3.23.3" alt="Jina 3.23.3 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-dynamic-creation-of-schema-with-nested-doclists-in-gateway-6138">Fix dynamic creation of schema with nested DocLists in Gateway (<a href="https://github.com/jina-ai/jina/pull/6138?ref=jina-ai-gmbh.ghost.io">#6138</a>)</h3><p>Previously, nested document types with multiple&#xA0;<code>DocList</code>&#xA0;levels of nesting sometimes led to the schema exposed by the Gateway being incomplete because some references were lost.</p><p>Now schemas like this exposed through&#xA0;<code>Flow</code>&#xA0;work as expected:</p><pre><code class="language-python">class QuoteFile(BaseDoc):
    quote_file_id: int
    texts: DocList[TextDoc]
    images: DocList[ImageDoc]

class SearchResult(BaseDoc):
    results: DocList[QuoteFile]</code></pre><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Fix topology documentation (<a href="https://github.com/jina-ai/jina/pull/6134?ref=jina-ai-gmbh.ghost.io">docs: fix topology documentation<u>&#xA0;#6134</u></a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Aquí Se Habla Español: Top-Quality Spanish-English Embeddings and 8k Context]]></title><description><![CDATA[Jina AI's new bilingual Spanish-English embedding model brings the state-of-the-art in AI to half a billion Spanish speakers.]]></description><link>https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context/</link><guid isPermaLink="false">65cc8e2f4a32310001f5b5f1</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 14 Feb 2024 15:30:36 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><p>Jina AI is <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">once</a> <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">again</a> <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">demonstrating</a> its commitment to high-quality multilingual AI models by releasing its <strong>Spanish-English bilingual model</strong>.</p><p>This model provides embedding vectors for texts of up to 8k tokens in Spanish or English, designed so that if texts in the two languages mean the same thing, their embeddings will be geometrically close together. Jina Embeddings v2 for Spanish and English is ideally suited for cross-language information retrieval, bilingual semantic analysis, and bilingual RAG applications.</p><p>This new model, <code>jina-embeddings-v2-base-es</code>, brings to Spanish the same state-of-the-art performance and ground-breaking feature set of Jina AI&#x2019;s <code>v2</code> models for <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English</a>, <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>:</p><ul><li>8,192 tokens of input context, a leader among open-source embedding models.</li><li>Real bilingualism instead of uneven multilingualism. Jina AI&#x2019;s bilingual models are trained to give balanced support to both languages, avoiding the <a href="https://arxiv.org/abs/2210.05619?ref=jina-ai-gmbh.ghost.io">biases of &#x201C;multilingual&#x201D; models</a> trained on uncurated Internet scrapes.</li><li><code>jina-embeddings-v2-base-es</code>&#xA0;is compact compared to open-source models of comparable performance. The embeddings themselves are 768 dimensions, saving space and run-time in production.</li><li>Jina Embeddings v2 models are fully integrated into major vector databases, RAG frameworks, and AI development libraries:<ul><li><a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a></li><li><a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a></li><li><a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a></li><li><a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul></li></ul><p>Jina Embeddings v2 for Spanish and English is accessible via the Jina <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Embeddings API </a> right now, with one million free tokens, so you pay nothing to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><h2 id="benchmarks">Benchmarks</h2><p>On Spanish benchmarks, Jina v2 for Spanish and English outperforms the <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Multilingual E5 base model</a> and the <a href="https://huggingface.co/BAAI/bge-m3?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">BGE M3 model</a>, the only comparable open-source models with Spanish support. The tests below (<a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MTEB-es</a>) are adapted from the <a href="https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Massive Text Embeddings Benchmark</a>. You can view and run them from this <a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">GitHub repository</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-es: MTEB: Massive Text Embedding Benchmark with Spanish datasets</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark with Spanish datasets - jina-ai/mteb-es</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/6e71c1c34b09bea0278cbed9f0c639554954e953c35b93d4576539b808f0f813/jina-ai/mteb-es" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="2000" height="227" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png 2241w"></figure><p>Jina Embeddings outperforms E5 on all metrics except classification and outperforms BGE-M3 in retrieval, clustering, and cross-language tasks, despite being 15% to 30% of the size of these larger models.</p><ul><li>Significantly better performance in <strong>retrieval</strong> tasks (like finding related documents in a database) and <strong>clustering</strong> (identifying groups of documents that belong together in a collection)</li><li>Roughly equal performance with E5 on <strong>reranking</strong> (ordering documents by semantic similarity) and near-equal performance on <strong>text classification</strong> in Spanish.</li><li>All three models have very similar benchmark scores for <strong>cross-language tasks</strong> (finding semantically similar texts in English to a Spanish input, or vice-versa), although Jina Embeddings still performs the best.</li></ul><p>When compared to closed-source multilingual models from Open AI and Cohere, Jina Embeddings&#x2019; compact size makes its achievements even more impressive.</p><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="1992" height="254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png 1992w"></figure><p>On retrieval tasks in Spanish, Jina outperforms the closed-source models offered by Open AI and Cohere and outperforms Open AI (and nearly equals Cohere&#x2019;s performance) on cross-language tasks.</p><h2 id="jina-embeddings-ai-for-a-multilingual-world">Jina Embeddings: AI for a Multilingual World</h2><p>Spanish is spoken by well over half a billion people, with official status in more than 20 countries, along with the <a href="https://european-union.europa.eu/index_es?ref=jina-ai-gmbh.ghost.io">European Union</a>, the <a href="https://www.un.org/es/?ref=jina-ai-gmbh.ghost.io">United Nations</a>, the <a href="https://www.wto.org/indexsp.htm?ref=jina-ai-gmbh.ghost.io">World Trade Organization</a>, and <a href="https://www.fifa.com/fifaplus/es/?ref=jina-ai-gmbh.ghost.io">FIFA</a>. Introducing this specialized bilingual model makes clear Jina AI&#x2019;s commitment to bringing AI technologies to everyone.</p><p>In addition to Spanish and its high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English monolingual model</a>, Jina AI currently offers state-of-the-art embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>, with more to come.</p><p>Jina AI is committed to advancing AI technology for the broadest audience, placing a high value on transparency, accessibility, affordability, privacy, and data protection.</p><p>We value your feedback on all our models. Join our community channel to contribute and stay informed about new developments.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4427 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command]]></title><description><![CDATA[Supercharge your Ghost blog's SEO and accessibility by automatically generating alt texts. All it takes is one command. WordPress/WooCommerce support coming soon!]]></description><link>https://jina.ai/news/scenexplain-alt-text-and-better-seo-for-your-ghost-blog-with-just-one-command/</link><guid isPermaLink="false">65c20d31aa949b00016cf15f</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 06 Feb 2024 15:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Frame-378.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Want to skip straight to generating alt texts for your Ghost blog? Check out our <a href="#Google-Colab-notebook" rel="noreferrer">notebook</a> or <a href="#Docker" rel="noreferrer">Docker instructions</a> for more</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Frame-378.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><p><a href="https://en.wikipedia.org/wiki/Alt_attribute?ref=jina-ai-gmbh.ghost.io">Alt text</a>, or alternative text, plays a crucial role in Search Engine Optimization (SEO) as it helps search engines understand the content of images on a webpage. By providing concise, relevant descriptions of images, alt text improves the likelihood of a website ranking higher in search engine results. This is particularly important for image search optimization, as alt text can significantly influence how search engines index and display images in search results.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09.webm" poster="https://img.spacergif.org/v1/887x667/0a/spacer.png" width="887" height="667" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Automatically-generated alt texts being displayed on Jina AI&apos;s blog</span></p></figcaption>
        </figure><p>Beyond enhancing SEO, using alt texts to improve accessibility (<a href="https://en.wikipedia.org/wiki/Computer_accessibility?ref=jina-ai-gmbh.ghost.io">a11y</a>) is vital for providing a better experience for visually impaired users. Alt text enables screen readers to describe images, allowing visually impaired individuals to fully comprehend a webpage&apos;s content. Additionally, incorporating alt text is essential for complying with web accessibility standards and regulations, such as the <a href="https://www.ada.gov/?ref=jina-ai-gmbh.ghost.io">Americans with Disabilities Act (ADA)</a>, <a href="https://www.w3.org/WAI/standards-guidelines/wcag/?ref=jina-ai-gmbh.ghost.io">Web Content Accessibility Guidelines (WCAG)</a>, and European Union directives like the <a href="https://en.wikipedia.org/wiki/European_Accessibility_Act?ref=jina-ai-gmbh.ghost.io">European Accessibility Act (EAA)</a>. Adhering to these standards not only reaches a broader audience but also shows a commitment to inclusivity and ensures equal access to digital information for all users.</p><h2 id="manually-adding-alt-texts-doesnt-scale">Manually adding alt texts doesn&apos;t scale</h2><p>Traditionally, writing alt text for images has been a labor-intensive task, especially for websites with extensive content, such as hundreds of blog posts or thousands of e-commerce products. Manually creating descriptive and accurate alt text for each image can be time-consuming and resource-intensive. This challenge is amplified in dynamic websites where new content is frequently added, requiring ongoing effort to ensure all images are accompanied by appropriate alt text. The task demands not only a significant investment of time but also attention to detail to maintain consistency and relevance across all images, making it a daunting task for content creators and webmasters.</p><h2 id="apis-can-help-but-you-still-need-to-write-the-middleware">APIs can help, but you still need to write the middleware</h2><p>Utilizing APIs like <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">SceneXplain</a>&#x2019;s to generate alt texts can significantly expedite the process of creating descriptive text for images, especially for websites with a large volume of visual content. Indeed, we&#x2019;ve written several posts on the subject!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/make-accessibility-accessible-generate-alt-text-with-scenexplain?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Making Accessibility Accessible: Create Alt Text with SceneXplain&#x2019;s API</div><div class="kg-bookmark-description">SceneXplain is your accessibility ally, making it easy to generate image alt texts to aid visually-impaired users and improve SEO</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplains-json-schema-store-automate-your-alt-text-and-more?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain&#x2019;s JSON Schema Store: Automate Your Alt-Text, and More!</div><div class="kg-bookmark-description">Take the hassle out of extracting data from images with SceneXplain&#x2019;s new JSON Schema Store. Discover and share reusable JSON schemas. Create, contribute, and access schemas easily through GUI or API</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><p>SceneXplain&#x2019;s API uses advanced algorithms and machine learning techniques to analyze images and automatically generate relevant alt texts, drastically reducing the manual workload. This can be a game-changer for efficiently managing websites with numerous images, such as e-commerce platforms and large-scale blogs.</p><p>However, while the API offers a streamlined solution for alt text generation, integrating the API into an existing platform still requires time and effort. Writing middleware is necessary to bridge the gap between the API and your website&apos;s own API or content management system. This middleware facilitates communication between the two, ensuring that the generated alt texts are correctly associated with their respective images on your website. Although this initial setup requires technical expertise and resources, once in place, it can provide a more automated and scalable approach to maintaining web accessibility and SEO standards.</p><h2 id="alt-texter-the-missing-middle-layer">Alt Texter: the missing middle layer</h2><p><a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io"><strong>Alt Texter</strong></a> serves as the missing link between SceneXplain and your content platform, streamlining the process of populating alt texts for images across your website. This tool is designed to operate directly from the command line, enabling you to efficiently go through all blog posts and automatically insert alt texts for content images and featured images. By running a single command, Alt Texter leverages the capabilities of SceneXplain to analyze images and generate appropriate alt texts, which are then seamlessly integrated into your website&apos;s content.</p><p>Currently, Alt Texter is specifically tailored for <a href="https://ghost.org/?ref=jina-ai-gmbh.ghost.io">Ghost blogs</a>, offering a solution for this popular blogging platform. However, its potential for expansion is significant. In the future, Alt Texter can be adapted to work with other widely-used platforms such as <a href="https://wordpress.org/download/?ref=jina-ai-gmbh.ghost.io">WordPress</a>, <a href="https://woo.com/?ref=jina-ai-gmbh.ghost.io">WooCommerce</a>, <a href="https://woo.com/?ref=jina-ai-gmbh.ghost.io">Shopify</a>, or any other platform with an API.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">WordPress and WooCommerce support is currently experimental. The functions are there, but more testing needs to be done. Let us know if you&#x2019;re interested!</div></div><p>This expansion would make the tool even more versatile and valuable, providing a wide range of website owners with an efficient, automated solution for enhancing web accessibility and SEO through accurate and descriptive alt texts.</p><h2 id="see-the-results">See the results</h2><p>You can witness the effectiveness of &quot;Alt Texter&quot; in conjunction with SceneXplain on <a href="https://jina.ai/news/?ref=jina-ai-gmbh.ghost.io">Jina AI&apos;s blog</a>. Here&apos;s that video again for reference:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09.webm" poster="https://img.spacergif.org/v1/887x667/0a/spacer.png" width="887" height="667" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>On this blog, every image is equipped with alt text generated by SceneXplain, and you can easily view this by simply hovering over any image. When you hover, a tooltip appears displaying the alt text along with a &quot;Powered by SceneXplain&quot; annotation. This feature not only demonstrates the accurate and contextually relevant alt texts provided by SceneXplain but also highlights the seamless integration of &quot;Alt Texter&quot; with the Ghost blogging platform. It serves as a practical example of how these technologies work together to enhance web accessibility and user experience, while also maintaining effective SEO.</p><h2 id="see-it-in-action">See it in action</h2><p>Here&apos;s how it looks when it&apos;s running in Docker Desktop:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34.webm" poster="https://img.spacergif.org/v1/892x719/0a/spacer.png" width="892" height="719" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:29</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>As you can see, it iterates over each blog post. If it finds images without alt text, it sends them to SceneXplain and then populates the post accordingly.</p><h2 id="run-it-yourself">Run it yourself</h2><p>To run the program you will need:</p><ul><li>Your Ghost <a href="https://ghost.org/docs/admin-api/?ref=jina-ai-gmbh.ghost.io">Admin API</a> key (<em>not</em> a Content API key)</li><li>Your Ghost blog URL</li><li>Your <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain API key</a></li></ul><div class="kg-card kg-callout-card kg-callout-card-red"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Always <a href="https://ghost.org/docs/faq/manual-backup/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">back up your data</a> before running any program (like this!) that may modify it.<br><br>This program is still very early in development, so be careful!</div></div><p>You can run the program one of three ways: In a Google Colab notebook, in Docker (via a graphical interface or command line), or directly on your machine.</p><h3 id="google-colab-notebook">Google Colab notebook</h3><p>While <a href="https://www.geeksforgeeks.org/how-to-use-google-colab/?ref=jina-ai-gmbh.ghost.io">Google Colab</a> isn&apos;t the ideal way to run Alt Texter in production, <a href="https://colab.research.google.com/github/jina-ai/scenex-alt-texter/blob/main/alt-texter.ipynb?ref=jina-ai-gmbh.ghost.io">the notebook</a> does give you a quick and easy way to see the code and test it out.</p><p>To run the notebook:</p><ul><li>Load up the below link</li><li>Enter your settings (in the &quot;Fill in variables&quot; section)</li><li>Click the <em>Runtime</em> menu, then click <em>Run all</em></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/scenex-alt-texter/blob/main/alt-texter.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/b8b3f31c3785f2607d951bd203c1a8b5/img/favicon.ico" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><h3 id="docker">Docker</h3><p>Using Docker lets you run the program on your own machine in a sandbox. In <a href="https://www.docker.com/products/docker-desktop/?ref=jina-ai-gmbh.ghost.io">Docker Desktop</a> you can follow the steps in this <a href="https://www.slideshare.net/slideshows/alt-texter-tutorial/266063930?ref=jina-ai-gmbh.ghost.io">deck</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.slideshare.net/slideshows/alt-texter-tutorial/266063930?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Alt Texter Tutorial</div><div class="kg-bookmark-description">Alt Texter Tutorial - Download as a PDF or view online for free</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://public.slidesharecdn.com/_next/static/media/favicon.7bc3d920.ico" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-author">Slideshare</span><span class="kg-bookmark-publisher">alexcg15</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.slidesharecdn.com/ss_thumbnails/alttextertutorial-240202130606-24d0a8e2-thumbnail.jpg?width=640&amp;height=640&amp;fit=bounds" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><p>If you prefer the command line, you can simply run:</p><pre><code class="language-bash">docker run --name alt-texter \
-e GHOST_API_KEY=&lt;ghost-admin-api-key&gt; \
-e SCENEX_API_KEY=&lt;scenex-api-key&gt; \
-e GHOST_BLOG_URL=&quot;&lt;ghost-blog-url&gt;&quot; \
jinaai/alt-texter:0.1
</code></pre><h3 id="bare-metal">Bare metal</h3><p>Of course, you can run the code on &quot;bare metal&quot; as well. Check the <a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io#bare-metal">README</a> for more details.</p><h2 id="get-started">Get started</h2><p>To check the latest version of the code, you can visit the <a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io">Alt Texter repo</a>. Contributions are always welcome! If you need support for the program, please leave an <a href="https://github.com/jina-ai/scenex-alt-texter/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc&amp;ref=jina-ai-gmbh.ghost.io">issue</a>, or if you need support with SceneXplain generally, you can start a conversation on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Discord</a> in the <a href="https://discord.com/channels/1106542220112302130/1106544027601469461?ref=jina-ai-gmbh.ghost.io">support channel</a>.</p>]]></content:encoded></item><item><title><![CDATA[Elevate Your Code Search with New Jina Code Embeddings]]></title><description><![CDATA[New 𝗷𝗶𝗻𝗮-𝗲𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀-𝘃𝟮-𝗯𝗮𝘀𝗲-𝗰𝗼𝗱𝗲 is optimized for code & docstring search. This powerful model supports searches between English and 30 widely-used programming languages, all with 8192 context length and SOTA performance.]]></description><link>https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/</link><guid isPermaLink="false">65c0fa40aa949b00016cf07d</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 05 Feb 2024 17:46:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--27-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--27-.png" alt="Elevate Your Code Search with New Jina Code Embeddings"><p>Accurately searching through code and documentation is more critical than ever. We&apos;re thrilled to unveil our latest embeddings in the world of coding: <code><strong>jina-embeddings-v2-base-code</strong></code>. This new open-source programming language embedding model is designed to improve how developers interact with code and documentation. Supporting English and 30 popular programming languages, it stands out as the only open-source model of its kind that accommodates up to 8,192 input tokens. The <code>jina-embeddings-v2-base-code</code> is now available on <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-code?ref=jina-ai-gmbh.ghost.io">HuggingFace</a> under an Apache 2.0 license and can be freely accessed via our <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Embedding API</a>.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11.mp4" poster="https://img.spacergif.org/v1/1070x876/0a/spacer.png" width="1070" height="876" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:07</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Visit </span><a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">Embedding API</span></a><span style="white-space: pre-wrap;"> and select </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-code</span></code><span style="white-space: pre-wrap;"> from the dropdown list. Enjoy 1M tokens for free.</span></p></figcaption>
        </figure><h2 id="why-develop-an-embedding-model-for-code">Why Develop an Embedding Model for Code?</h2><p>Developers often find themselves navigating through vast codebases, not in search of errors, but to locate specific functionalities or understand how certain processes are implemented. This task can be time-consuming and, at times, akin to finding a needle in a haystack. Integrated Development Environments (IDEs) have significantly improved this process by providing tools and features that automate the search for information. However, the potential for further enhancement exists, and this is where our embedding model comes into play.</p><h3 id="use-cases-of-jina-embeddings-v2-base-code">Use Cases of <code>jina-embeddings-v2-base-code</code> </h3><p>By integrating AI-powered search capabilities, we&apos;re not just augmenting existing functionalities within IDEs; we&apos;re transforming how developers engage with codebases. This technology goes beyond simple text search, offering semantic understanding that can interpret the intent behind a query, thereby significantly reducing the time and effort required for code reviews, unit testing, and overall quality management. </p><h4 id="enhanced-code-navigation"><strong>Enhanced Code Navigation</strong></h4><ul><li><strong>Query Format</strong>: Natural language description of the functionality or code snippet you&apos;re searching for.</li><li><strong>Retrieved Result Format</strong>: Relevant code files or snippets where the described functionality is implemented, along with annotations or highlights that point to the specific parts of the code.</li></ul><h4 id="streamlined-code-review"><strong>Streamlined Code Review</strong></h4><ul><li><strong>Query Format</strong>: Description of the programming concepts or patterns you want to review across the codebase.</li><li><strong>Retrieved Result Format</strong>: A list of code snippets or pull requests that match the described concepts, patterns, or best practices, enabling reviewers to focus on critical areas for improvement.</li></ul><h4 id="automated-documentation-assistance"><strong>Automated Documentation Assistance</strong></h4><ul><li><strong>Query Format</strong>: Code snippet for which you need documentation or an explanation.</li><li><strong>Retrieved Result Format</strong>: Suggested docstrings or documentation entries that explain the code&apos;s functionality, parameters, and return types, making it easier to maintain up-to-date and comprehensive documentation.</li></ul><p>By addressing these specific use cases, <code>jina-embeddings-v2-base-code</code> not only enhances the development experience but also promotes a more collaborative and efficient coding environment.</p><h2 id="benchmark-the-performance">Benchmark the Performance</h2><p>In a field where precision and accuracy are paramount, <code>jina-embeddings-v2-base-code</code> has outshined its competitors, leading the pack in nine out of fifteen crucial <a href="https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io">CodeNetSearch</a> benchmarks. What&apos;s more, our model holds highly competitive scores in the remaining benchmarks. When compared to its nearest competitors, including those from tech giants like Microsoft and Salesforce, <code>jina-embeddings-v2-base-code</code> not only ranks higher but also showcases its superior design and capabilities.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-8.png" class="kg-image" alt="Elevate Your Code Search with New Jina Code Embeddings" loading="lazy" width="2000" height="793" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-8.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Our model&apos;s excellence is not just in isolated instances; across the board, all Jina Embedding models have achieved top ranks on relevant benchmarks, distinguishing themselves among open-source models for code retrieval.</span></figcaption></figure><h2 id="model-highlights">Model Highlights</h2><ul><li><strong>State-of-the-Art Performance</strong>: Our commitment to excellence is reflected in the performance of Jina Embedding models, which consistently top benchmark lists against other open-source offerings and even outperform models from Microsoft and Salesforce.</li><li><strong>Compact Yet Powerful</strong>: In the world of AI, efficiency is key. With 161 million parameters (307MB without quantization), <code>jina-embeddings-v2-base-code</code> is designed for efficiency, offering high-speed performance and cost savings without compromising on capability.</li><li><strong>Extended Context Capability</strong>: The ability to process up to 8192 tokens allows for the handling of large functions and numerous object files, providing a depth of understanding and context that surpasses the limitations of models supporting only a few hundred tokens.</li><li><strong>Multi-Language Support</strong>: Tailored for versatility, our model&apos;s training encompasses 30 programming languages and frameworks, emphasizing six of the most popular ones: Python, JavaScript, Java, PHP, Go, and Ruby. This extensive coverage ensures that <code>jina-embeddings-v2-base-code</code> meets the diverse needs of the programming community.</li><li><strong>RAG Integration for Seamless Code Generation</strong>: The model&apos;s compatibility with RAG and integration with a code generation model facilitate not just code generation from general knowledge but also the ability to read relevant APIs and documentation, enabling automatic code integration that is both efficient and accurate.</li></ul><h2 id="seamless-api-integration">Seamless API Integration</h2><p><code>jina-embeddings-v2-base-code</code> is designed for easy integration, supporting major vector databases like MongoDB, Qdrant, and Weaviate, and frameworks such as Haystack and LlamaIndex. This ensures that developers can effortlessly incorporate our model into their existing systems, leveraging its capabilities to enhance their code retrieval and documentation processes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-7.png" class="kg-image" alt="Elevate Your Code Search with New Jina Code Embeddings" loading="lazy" width="2000" height="725" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-7.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Frameworks that support our embedding API</span></figcaption></figure><p>We value your feedback on <code>jina-embeddings-v2-base-code</code>. Join our community channel to contribute feedback and stay informed about our advancements. Together, we&apos;re shaping a more robust and inclusive AI future.</p>]]></content:encoded></item><item><title><![CDATA[A Deep Dive into Tokenization]]></title><description><![CDATA[Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?]]></description><link>https://jina.ai/news/a-deep-dive-into-tokenization/</link><guid isPermaLink="false">65afb3ee8da8040001e17061</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 31 Jan 2024 15:10:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" alt="A Deep Dive into Tokenization"><p>There are a lot of barriers to understanding AI models, some of them pretty big barriers, and they can stand in the way of implementing AI processes. But the first one many people encounter is understanding what we mean when talking about <strong>tokens</strong>. </p><p>One of the most important practical parameters in choosing an AI language model is the size of its context window &#x2014; the maximum input text size &#x2014; which is given in tokens, not words or characters or any other automatically recognizable unit.</p><p>Furthermore, embedding services are typically figured &#x201C;per token,&#x201D; meaning tokens are important to understanding your bill.</p><p>This can be very confusing if you aren&#x2019;t clear about what a token is. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="A Deep Dive into Tokenization" loading="lazy" width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings current price sheet (as of February 2024). Note that prices are stated per &#x201C;1M tokens&#x201D;.</span></figcaption></figure><p>But of all the confusing aspects of modern AI, tokens are probably the least complicated. This article will try to clarify what tokenization is, what it does, and why we do it that way.</p><h2 id="tldr">tl;dr</h2><p>For those who want or need a quick answer to figure out how many tokens to buy from Jina Embeddings or an estimate of how many they need to expect to buy, the following statistics are what you&apos;re looking for.</p><h3 id="tokens-per-english-word">Tokens per English Word</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A call to the Jina Embeddings v2 API for English models will use <b><strong style="white-space: pre-wrap;">approximately</strong></b> <b><strong style="white-space: pre-wrap;">10% more</strong></b> tokens than the number of words in your text, <b><strong style="white-space: pre-wrap;">plus two tokens per embedding</strong></b>.</div></div><p>During empirical testing, described further down in this article, a variety of English texts converted into tokens at a rate of about 10% more tokens than words, using Jina Embeddings English-only models. This result was pretty robust. </p><p>Jina Embeddings v2 models have a context window of 8192 tokens. This means that if you pass a Jina model an English text longer than 7,400 words, there is a good chance it will be truncated.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">7,400 words</strong></b>.</div></div><h3 id="tokens-per-chinese-character">Tokens per Chinese Character</h3><p>For Chinese, results are more variable. Depending on the text type, ratios varied from 0.6 to 0.75 tokens per Chinese character (&#x6C49;&#x5B57;). English texts given to Jina Embeddings v2 for Chinese produce approximately the same number of tokens as Jina Embeddings v2 for English: roughly 10% more than the number of words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input in Chinese to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for Chinese and English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">10,500 characters</strong></b> (<b><strong style="white-space: pre-wrap;">&#x5B57;&#x6570;</strong></b>), or <b><strong style="white-space: pre-wrap;">0.6 to 0.75 tokens per Chinese character, plus two per embedding.</strong></b></div></div><h3 id="tokens-per-german-word">Tokens per German Word</h3><p>German word-to-token ratios are more variable than English but less than Chinese. Depending on the genre of the text, I got 20% to 30% more tokens than words on average. Giving English texts to Jina Embeddings v2 for German and English uses a few more tokens than the English-only and Chinese/English models: 12% to 15% more tokens than words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Jina Embeddings v2 for German and English will count <b><strong style="white-space: pre-wrap;">20% to 30% more tokens than words, plus two per embedding</strong></b>. The maximum size of the input context is approximately <b><strong style="white-space: pre-wrap;">6,300 German words</strong></b>.</div></div><h3 id="caution">Caution!</h3><p>These are simple calculations, but they should be approximately right for most natural language texts and most users. Ultimately, we can only promise that the number of tokens will always be no more than the number of characters in your text, plus two. It will practically always be much less than that, but we cannot promise any specific count in advance.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Your Mileage May Vary! </strong></b><br><br>These are estimates based on statistically naive calculations. We do not guarantee how many tokens any particular request will take.</div></div><p>If all you need is advice on how many tokens to buy for Jina Embeddings, you can stop here. Other embedding models, from companies other than Jina AI, may not have the same token-to-word and token-to-Chinese-character ratios Jina models have, but they will not generally be very different overall.</p><p>If you want to understand why, the rest of this article is a deeper dive into tokenization for language models.</p><h2 id="words-tokens-numbers">Words, Tokens, Numbers</h2><p>Tokenization has been a part of natural language processing for longer than modern AI models have existed.</p><p>It&#x2019;s a bit clich&#xE9; to say that everything in a computer is just a number, but it&#x2019;s also mostly true. Language, however, is not naturally just a bunch of numbers. It might be speech, made of sound waves, or writing, made of marks on paper, or even an image of a printed text or a video of someone using sign language. But most of the time, when we talk about using computers to process natural language, we mean texts composed of sequences of characters: letters (a, b, c, etc.), numerals (0, 1, 2&#x2026;), punctuation, and spaces, in different languages and textual encodings.</p><p>Computer engineers call these &#x201C;strings&#x201D;.</p><p>AI language models take sequences of numbers as input. So, you might write the sentence:</p><blockquote><em>What is today&apos;s weather in Berlin?</em></blockquote><p>But, after tokenization, the AI model gets as input:</p><pre><code class="language-python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>Tokenization is the process of converting an input string into a specific sequence of numbers that your AI model can understand.</p><p>When you use an AI model via a web API that charges users per token, each request is converted into a sequence of numbers like the one above. The number of tokens in the request is the length of that sequence of numbers. So, asking Jina Embeddings v2 for English to give you an embedding for &#x201C;<em>What is today&apos;s weather in Berlin?</em>&#x201D; will cost you 11 tokens because it converted that sentence into a sequence of 11 numbers before passing it to the AI model.</p><p>AI models based on the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io">Transformer architecture</a> have a fixed-size <strong>context window</strong> whose size is measured in tokens. Sometimes this is called an &#x201C;input window,&#x201D; &#x201C;context size,&#x201D; or &#x201C;sequence length&#x201D; (especially on the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Hugging Face MTEB leaderboard</a>). It means the maximum text size that the model can see at one time.</p><p>So, if you want to use an embedding model, this is the maximum input size allowed.</p><p>Jina Embeddings v2 models all have a context window of 8,192 tokens. Other models will have different (typically smaller) context windows. This means that however much text you put into it, the tokenizer associated with that Jina Embeddings model must convert it into no more than 8,192 tokens.</p><h2 id="mapping-language-to-numbers">Mapping Language to Numbers</h2><p>The simplest way to explain the logic of tokens is this:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A token is a number that stands in for a part of a string.</div></div><p>For natural language models, the part of a string that a token stands for is a word, a part of a word, or a piece of punctuation. Spaces are not generally given any explicit representation in tokenizer output.</p><p>Tokenization is part of a group of techniques in natural language processing called <a href="https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io"><em>text segmentation</em></a>, and the module that performs tokenization is called, very logically, a <strong>tokenizer</strong>.</p><p>To show how tokenization works, we&#x2019;re going to tokenize some sentences using the smallest Jina Embeddings v2 for English model: <code>jina-embeddings-v2-small-en</code>. Jina Embeddings&#x2019; other English-only model &#x2014; <code>jina-embeddings-v2-base-en</code> &#x2014; uses the same tokenizer, so there&#x2019;s no point in downloading extra megabytes of AI model that we won&#x2019;t use in this article.</p><p>First, install the <code>transformers</code> module in your Python environment or notebook. Use the <code>-U</code> flag to make sure you upgrade to the latest version because this model will not work with some older versions:</p><pre><code class="language-bash">pip install -U transformers
</code></pre><p>Then, download <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> using <code>AutoModel.from_pretrained</code>:</p><pre><code class="language-Python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-small-en&apos;, trust_remote_code=True)
</code></pre><p>To tokenize a string, use the <code>encode</code> method of the <code>tokenizer</code> member object of the model:</p><pre><code class="language-Python">model.tokenizer.encode(&quot;What is today&apos;s weather in Berlin?&quot;)
</code></pre><p>The result is a list of numbers:</p><pre><code class="language-Python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>To convert these numbers back to string forms, use the <code>convert_ids_to_tokens</code> method of the <code>tokenizer</code> object:</p><pre><code class="language-Python">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])
</code></pre><p>The result is a list of strings:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;what&apos;, &apos;is&apos;, &apos;today&apos;, &quot;&apos;&quot;, &apos;s&apos;, &apos;weather&apos;, &apos;in&apos;,
 &apos;berlin&apos;, &apos;?&apos;, &apos;[SEP]&apos;]
</code></pre><p>Note that the model&#x2019;s tokenizer has:</p><ol><li>Added <code>[CLS]</code>at the beginning and <code>[SEP]</code> at the end. This is necessary for technical reasons and means that <strong>every request for an embedding will cost two extra tokens</strong>, above however many tokens the text takes.</li><li>Split punctuation from words, turning &#x201C;<em>Berlin?</em>&#x201D; into: <code>berlin</code> and <code>?</code>, and &#x201C;<em>today&#x2019;s</em>&#x201D; into <code>today</code>, <code>&apos;</code>, and <code>s</code>.</li><li>Put everything in lowercase. Not all models do this, but this can help with training when using English. It may be less helpful in languages where capitalization has a different meaning.</li></ol><p>Different word-counting algorithms in different programs might count the words in this sentence differently. OpenOffice counts this as six words. The Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) counts seven words. Other software may come to other numbers, depending on how they handle punctuation and clitics like &#x201C;&#x2019;s.&#x201D;</p><p>The tokenizer for this model produces nine tokens for those six or seven words, plus the two extra tokens needed with every request.</p><p>Now, let&#x2019;s try with a less common place-name than Berlin:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;I live in Kinshasa.&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;i&apos;, &apos;live&apos;, &apos;in&apos;, &apos;kin&apos;, &apos;##sha&apos;, &apos;##sa&apos;, &apos;.&apos;, &apos;[SEP]&apos;]
</code></pre><p>The name &#x201C;Kinshasa&#x201D; is broken up into three tokens: <code>kin</code>, <code>##sha</code>, and <code>##sa</code>. The <code>##</code> indicates that this token is not the beginning of a word.</p><p>If we give the tokenizer something completely alien, the number of tokens over the number of words increases even more:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;Klaatu barada nikto&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)

[&apos;[CLS]&apos;, &apos;k&apos;, &apos;##la&apos;, &apos;##at&apos;, &apos;##u&apos;, &apos;bar&apos;, &apos;##ada&apos;, &apos;nik&apos;, &apos;##to&apos;, &apos;[SEP]&apos;]
</code></pre><p>Three words become eight tokens, plus the <code>[CLS]</code> and <code>[SEP]</code> tokens.</p><p>Tokenization in German is similar. With the <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 for German</a> model, we can tokenize a translation of &quot;What is today&apos;s weather in Berlin?&quot; the same way as with the English model.</p><pre><code class="language-Python">german_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
token_ids = german_model.tokenizer.encode(&quot;Wie wird das Wetter heute in Berlin?&quot;)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Wie&apos;, &apos;wird&apos;, &apos;das&apos;, &apos;Wetter&apos;, &apos;heute&apos;, &apos;in&apos;, &apos;Berlin&apos;, &apos;?&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>This tokenizer is a little bit different from the English one in that <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> replace <code>[CLS]</code> and <code>[SEP]</code> but serve the same function. Also, the text is not case-normalized &#x2014; upper and lower cases remain as written &#x2014; because capitalization is meaningful in German differently from English.</p><p>(To simplify this presentation, I removed a special character indicating a word&apos;s beginning.)</p><p>Now, let&#x2019;s try a more complex sentence <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io">from a newspaper text</a>:</p><blockquote>Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen bleibt liegen &#x2013; zu genervt sind die Landwirte von b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.</blockquote><pre><code>sentence = &quot;&quot;&quot;
Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen
bleibt liegen &#x2013; zu genervt sind die Landwirte von 
b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.
&quot;&quot;&quot;
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)</code></pre><p>The tokenized result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Ein&apos;, &apos;Gro&#xDF;teil&apos;, &apos;der&apos;, &apos;mill&apos;, &apos;iarden&apos;, &apos;schwer&apos;, 
 &apos;en&apos;, &apos;Bauern&apos;, &apos;-&apos;, &apos;Sub&apos;, &apos;ventionen&apos;, &apos;bleibt&apos;, &apos;liegen&apos;, 
 &apos;&#x2013;&apos;, &apos;zu&apos;, &apos;gen&apos;, &apos;ervt&apos;, &apos;sind&apos;, &apos;die&apos;, &apos;Landwirte&apos;, &apos;von&apos;, 
 &apos;b&#xFC;ro&apos;, &apos;krat&apos;, &apos;ischen&apos;, &apos;G&#xE4;n&apos;, &apos;gel&apos;, &apos;ungen&apos;, &apos;und&apos;, &apos;Regel&apos;, 
 &apos;wahn&apos;, &apos;.&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>Here, you see that many German words were broken up into smaller pieces and not necessarily along the lines licensed by German grammar. The result is that a long German word that would count as just one word to a word counter might be any number of tokens to Jina&#x2019;s AI model.</p><p>Let&#x2019;s do the same in Chinese, translating &#x201D;What is today&apos;s weather in Berlin?&#x201D; as:</p><blockquote>&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;</blockquote><pre><code>chinese_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(&quot;&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;&quot;)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The tokenized result:</p><pre><code class="language-Python">[&apos;&lt;s&gt;&apos;, &apos;&#x67CF;&#x6797;&apos;, &apos;&#x4ECA;&#x5929;&#x7684;&apos;, &apos;&#x5929;&#x6C14;&apos;, &apos;&#x600E;&#x4E48;&#x6837;&apos;, &apos;&#xFF1F;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>In Chinese, there are usually no word breaks in written text, but the Jina Embeddings tokenizer frequently joins multiple Chinese characters together:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>&#x67CF;&#x6797;</td>
<td>B&#xF3;l&#xED;n</td>
<td>Berlin</td>
</tr>
<tr>
<td>&#x4ECA;&#x5929;&#x7684;</td>
<td>j&#x12B;nti&#x101;n de</td>
<td>today&#x2019;s</td>
</tr>
<tr>
<td>&#x5929;&#x6C14;</td>
<td>ti&#x101;nq&#xEC;</td>
<td>weather</td>
</tr>
<tr>
<td>&#x600E;&#x4E48;&#x6837;</td>
<td>z&#x11B;nmey&#xE0;ng</td>
<td>how</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Let&#x2019;s use a more complex sentence <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io">from a Hong Kong-based newspaper</a>:</p><pre><code class="language-Python">sentence = &quot;&quot;&quot;
&#x65B0;&#x898F;&#x5B9A;&#x57F7;&#x884C;&#x9996;&#x65E5;&#xFF0C;&#x8A18;&#x8005;&#x5728;&#x4E0B;&#x73ED;&#x9AD8;&#x5CF0;&#x524D;&#x7684;&#x4E0B;&#x5348;5&#x6642;&#x4F86;&#x5230;&#x5EE3;&#x5DDE;&#x5730;&#x9435;3&#x865F;&#x7DDA;&#xFF0C;
&#x5F9E;&#x7E41;&#x5FD9;&#x7684;&#x73E0;&#x6C5F;&#x65B0;&#x57CE;&#x7AD9;&#x555F;&#x7A0B;&#xFF0C;&#x5411;&#x6A5F;&#x5834;&#x5317;&#x65B9;&#x5411;&#x51FA;&#x767C;&#x3002;
&quot;&quot;&quot;
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>(Translation: <em>&#x201C;On the first day that the new regulations were in force, this reporter arrived at Guangzhou Metro Line 3 at 5 p.m., during rush hour, having departed the Zhujiang New Town Station heading north towards the airport.&#x201D;</em>)</p><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;&#x65B0;&apos;, &apos;&#x898F;&#x5B9A;&apos;, &apos;&#x57F7;&#x884C;&apos;, &apos;&#x9996;&apos;, &apos;&#x65E5;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x8A18;&#x8005;&apos;, &apos;&#x5728;&#x4E0B;&apos;, &apos;&#x73ED;&apos;, 
 &apos;&#x9AD8;&#x5CF0;&apos;, &apos;&#x524D;&#x7684;&apos;, &apos;&#x4E0B;&#x5348;&apos;, &apos;5&apos;, &apos;&#x6642;&apos;, &apos;&#x4F86;&#x5230;&apos;, &apos;&#x5EE3;&#x5DDE;&apos;, &apos;&#x5730;&apos;, &apos;&#x9435;&apos;, &apos;3&apos;, 
 &apos;&#x865F;&apos;, &apos;&#x7DDA;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5F9E;&apos;, &apos;&#x7E41;&#x5FD9;&apos;, &apos;&#x7684;&apos;, &apos;&#x73E0;&#x6C5F;&apos;, &apos;&#x65B0;&#x57CE;&apos;, &apos;&#x7AD9;&apos;, &apos;&#x555F;&apos;, 
 &apos;&#x7A0B;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5411;&apos;, &apos;&#x6A5F;&#x5834;&apos;, &apos;&#x5317;&apos;, &apos;&#x65B9;&#x5411;&apos;, &apos;&#x51FA;&#x767C;&apos;, &apos;&#x3002;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>These tokens do not map to any specific dictionary of Chinese words (&#x8BCD;&#x5178;). For example, &#x201C;&#x555F;&#x7A0B;&#x201D; - <em>q&#x1D0;ch&#xE9;ng</em> (depart, set out) would typically be categorized as a single word but is here split into its two constituent characters. Similarly, &#x201C;&#x5728;&#x4E0B;&#x73ED;&#x201D; would usually be recognized as two words, but with the split between &#x201C;&#x5728;&#x201D; - <em>z&#xE0;i</em> (in, during) and &#x201C;&#x4E0B;&#x73ED;&#x201D; - <em>xi&#xE0;b&#x101;n</em> (the end of the workday, rush hour), not between &#x201C;&#x5728;&#x4E0B;&#x201D; and &#x201C;&#x73ED;&#x201D; as the tokenizer has done here.</p><p>In all three languages, the places where the tokenizer breaks the text up are not directly related to the logical places where a human reader would break them.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The tokenizer algorithm does not use a conventional, language-aware dictionary, so its behavior does not match how humans count words.</div></div><p>This is not a specific feature of Jina Embeddings models. This approach to tokenization is almost universal in AI model development. Although two different AI models may not have identical tokenizers, in the current state of development, they will practically all use tokenizers with this kind of behavior.</p><p>The next section will discuss the specific algorithm used in tokenization and the logic behind it.</p><h2 id="why-do-we-tokenize-and-why-this-way">Why Do We Tokenize? And Why This Way?</h2><p>AI language models take as input sequences of numbers that stand in for text sequences, but a bit more happens before running the underlying neural network and creating an embedding. When presented with a list of numbers representing small text sequences, the model looks each number up in an internal dictionary that stores a unique vector for each number. It then combines them, and that becomes the input to the neural network.</p><p>This means that the tokenizer <strong>must</strong> be able to convert <strong><em>any</em></strong> input text we give it into tokens that appear in the model&#x2019;s dictionary of token vectors. If we took our tokens from a conventional dictionary, the first time we encountered a misspelling or a rare proper noun or foreign word, the whole model would stop. It could not process that input.</p><p>In natural language processing, this is called the out-of-vocabulary (OOV) problem, and it&#x2019;s pervasive in all text types and all languages. There are a few strategies for addressing the OOV problem:</p><ol><li>Ignore it. Replace everything not in the dictionary with an &#x201C;unknown&#x201D; token.</li><li>Bypass it. Instead of using a dictionary that maps text sequences to vectors, use one that maps <em>individual characters</em> to vectors. English only uses 26 letters most of the time, so this must be smaller and more robust against OOV problems than any dictionary.</li><li>Find frequent subsequences in the text, put them in the dictionary, and use characters (single-letter tokens) for whatever is left.</li></ol><p>The first strategy means that a lot of important information is lost. The model can&#x2019;t even learn about the data it&#x2019;s seen if it takes the form of something not in the dictionary. A lot of things in ordinary text are just not present in even the largest dictionaries.</p><p>The second strategy is possible, and researchers have investigated it. However, it means that the model has to accept a lot more input and has to learn a lot more. This means a much bigger model and much more training data for a result that has never proven to be any better than the third strategy.</p><p>AI language models pretty much all implement the third strategy in some form. Most use some variant of the <a href="https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io">Wordpiece algorithm</a> <a href="https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io">[Schuster and Nakajima 2012]</a> or a similar technique called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io">Senrich et al. 2016</a>] These algorithms are <em>language-agnostic</em>. That means they work the same for all written languages without any knowledge beyond a comprehensive list of possible characters. They were designed for multilingual models like Google&#x2019;s BERT that take just any input from scraping the Internet &#x2014; hundreds of languages and texts other than human language like computer programs &#x2014; so that they could be trained without doing complicated linguistics.</p><p>Some research shows significant improvements using more language-specific and language-aware tokenizers. [<a href="https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io">Rust et al. 2021</a>] But building tokenizers that way takes time, money, and expertise. Implementing a universal strategy like BPE or Wordpiece is much cheaper and easier.</p><p>However, as a consequence, there is no way to know how many tokens a specific text represents other than to run it through a tokenizer and then count the number of tokens that come out of it. Because the smallest possible subsequence of a text is one letter, you can be sure the number of tokens won&#x2019;t be larger than the number of characters (minus spaces) plus two.</p><p>To get a good estimate, we need to throw a lot of text at our tokenizer and calculate empirically how many tokens we get on average, compared to how many words or characters we input. In the next section, we&#x2019;ll do some not-very-systematic empirical measurements for all Jina Embeddings v2 models currently available.</p><h2 id="empirical-estimates-of-token-output-sizes">Empirical Estimates of Token Output Sizes</h2><p>For English and German, I used the Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) to get word counts for texts. This algorithm is widely used to select text snippets when you double-click on something. It is the closest thing available to a universal objective word counter.</p><p>I installed the <a href="https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io">polyglot library</a> in Python, which implements this text segmenter:</p><pre><code class="language-bash">pip install -U polyglot
</code></pre><p>To get the word count of a text, you can use code like this snippet:</p><pre><code class="language-python">from polyglot.text import Text

txt = &quot;What is today&apos;s weather in Berlin?&quot;
print(len(Text(txt).words))
</code></pre><p>The result should be <code>7</code>.</p><p>To get a token count, segments of the text were passed to the tokenizers of various Jina Embeddings models, as described below, and each time, I subtracted two from the number of tokens returned.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">The token counts listed here <b><strong style="white-space: pre-wrap;">do not include</strong></b> the extra two tokens at the beginning and end of each tokenized text.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en">English<br>(<code>jina-embeddings-v2-small-en</code> and <code>jina-embeddings-v2-base-en</code>)</h3><p>To calculate averages, I downloaded two English text corpora from <a href="https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Wortschatz Leipzig</a>, a collection of freely downloadable corpora in a number of languages and configurations hosted by Leipzig University:</p><ul><li>A one-million-sentence corpus of news data in English from 2020 (<code>eng_news_2020_1M</code>)</li><li>A one-million-sentence corpus of <a href="https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io">English Wikipedia</a> data from 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Both can be found on <a href="https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io">their English downloads page</a>.</p><p>For diversity, I also downloaded the <a href="https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io">Hapgood translation of Victor Hugo&#x2019;s <em>Les Mis&#xE9;rables</em></a> from Project Gutenberg, and a copy of the King James Version of the Bible, translated to English in 1611.</p><p>For each all four texts, I counted the words using the Unicode segmenter implemented in <code>polyglot</code>, then counted the tokens made by <code>jina-embeddings-v2-small-en</code>, subtracting two tokens for each tokenization request. The results are as follows:</p>
<!--kg-card-begin: html-->
<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Text</th><th id="|&lt;X;" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for English)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class><code>eng_news_2020_1M</code></td><td id="|&lt;X;" class>22,825,712</td><td id="GHal" class>25,270,581</td><td id="h]mu" class>1.107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class><code>eng_wikipedia_2016_1M</code></td><td id="|&lt;X;" class>24,243,607</td><td id="GHal" class>26,813,877</td><td id="h]mu" class>1.106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class><code>les_miserables_en</code></td><td id="|&lt;X;" class>688,911</td><td id="GHal" class>764,121</td><td id="h]mu" class>1.109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class><code>kjv_bible</code></td><td id="|&lt;X;" class>1,007,651</td><td id="GHal" class>1,099,335</td><td id="h]mu" class>1.091</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>The use of precise numbers does not mean this is a precise result. That documents of such different genres would all have between 9% and 11% more tokens than words indicates that you can probably expect somewhere around 10% more tokens than words, as measured by the Unicode segmenter. Word processors often do not count punctuation, while the Unicode Segmenter does, so you can&#x2019;t expect the word counts from office software to necessarily match this.</p><h3 id="german-jina-embeddings-v2-base-de">German<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>For German, I downloaded three corpora from <a href="https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io">Wortschatz Leipzig&#x2019;s German page</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> &#x2014; One million sentences from a balanced mixture of texts in different genres, dating to 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> &#x2014; One million sentences of news text from 2019.</li><li><code>deu_wikipedia_2021_1M</code> &#x2014; One million sentences extracted from the German Wikipedia in 2021.</li></ul><p>And for diversity, I also downloaded all <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata&amp;ref=jina-ai-gmbh.ghost.io">three volumes of Karl Marx&#x2019;s <em>Kapital</em></a> from the <a href="https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>I then followed the same procedure as for English:</p>
<!--kg-card-begin: html-->
<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Text</th><th id="|qln" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class>7,924,024</td><td id="YXZX" class>9,772,652</td><td id="oEoQ" class>1.234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class>17,949,120</td><td id="YXZX" class>21,711,555</td><td id="oEoQ" class>1.210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class>17,999,482</td><td id="YXZX" class>22,654,901</td><td id="oEoQ" class>1.259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class>784,336</td><td id="YXZX" class>1,011,377</td><td id="oEoQ" class>1.289</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>These results have a larger spread than the English-only model but still suggest that German text will yield, on average, 20% to 30% more tokens than words.</p><p>English texts yield more tokens with the German-English tokenizer than the English-only one:</p>
<!--kg-card-begin: html-->
<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc&lt;l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Text</th><th id="R@A^" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class>24243607</td><td id="UUfl" class>27758535</td><td id="iTZS" class>1.145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class>22825712</td><td id="UUfl" class>25566921</td><td id="iTZS" class>1.120</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>You should expect to need 12% to 15% more tokens than words to embed English texts with the bilingual German/English than with the English-only one.</p><h3 id="chinese-jina-embeddings-v2-base-zh">Chinese<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Chinese is typically written without spaces and had no traditional notion of &#x201C;words&#x201D; before the 20th century. Consequently, the size of a Chinese text is typically measured in characters (<strong>&#x5B57;&#x6570;</strong>). So, instead of using the Unicode Segmenter, I measured the length of Chinese texts by removing all the spaces and then just getting the character length.</p><p>I downloaded three corpora from the <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io">Chinese corpus page at Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> &#x2014; One million sentences from the Chinese language Wikipedia, extracted in 2018.</li><li><code>zho_news_2007-2009_1M</code> &#x2014; One million sentences from Chinese news sources, collected from 2007 to 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> &#x2014; One million sentences from news sources that use exclusively traditional Chinese characters (&#x7E41;&#x9AD4;&#x5B57;).</li></ul><p>In addition, for some diversity, I also used <em>The True Story of Ah Q</em> (&#x963F;Q&#x6B63;&#x50B3;), a novella by Lu Xun (&#x9B6F;&#x8FC5;) written in the early 1920s. I downloaded the <a href="https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io">traditional character version from Project Gutenberg</a>.</p>
<!--kg-card-begin: html-->
<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Text</th><th id="CaUc" class="simple-table-header-color simple-table-header">Character count<br>(&#x5B57;&#x6570;)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for Chinese and English)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio of tokens to characters<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class>45,116,182</td><td id="CQ{d" class>29,193,028</td><td id="_};C" class>0.647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class>44,295,314</td><td id="CQ{d" class>28,108,090</td><td id="_};C" class>0.635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class>54,585,819</td><td id="CQ{d" class>40,290,982</td><td id="_};C" class>0.738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class>41,268</td><td id="CQ{d" class>25,346</td><td id="_};C" class>0.614</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>This spread in token-to-character ratios is unexpected, and especially the outlier for the traditional character corpus merits further investigation. Nonetheless, we can conclude that for Chinese, you should expect to need <em>fewer</em> tokens than there are characters in your text. Depending on your content, you can expect to need 25% to 40% less.</p><p>English texts in Jina Embeddings v2 for Chinese and English yielded roughly the same number of tokens as they do in the English-only model:</p>
<!--kg-card-begin: html-->
<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="&lt;rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class style="width:165.3125px">24,243,607</td><td id="=]?F" class>26,890,176</td><td id="&lt;rlw" class>1.109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class style="width:165.3125px">22,825,712</td><td id="=]?F" class>25,060,352</td><td id="&lt;rlw" class>1.097</td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="taking-tokens-seriously">Taking Tokens Seriously</h2><p>Tokens are an important scaffolding for AI language models, and research is ongoing in this area.</p><p>One of the places where AI models have proven revolutionary is the discovery that they are very robust against noisy data. Even if a particular model does not use the optimal tokenization strategy, if the network is large enough, given enough data, and adequately trained, it can learn to do the right thing from imperfect input.</p><p>Consequently, much less effort is spent on improving tokenization than in other areas, but this may change.</p><p>As a user of embeddings, who buys them via an <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">API like Jina Embeddings</a>, you can&#x2019;t know precisely how many tokens you&#x2019;ll need for a specific task and may have to do some testing of your own to get solid numbers. But the estimates provided here &#x2014; circa 110% of the word count for English, circa 125% of the word count for German, and circa 70% of the character count for Chinese &#x2014; should be enough for basic budgeting.</p><h2 id="learn-more">Learn More</h2><p>For more information about Jina Embeddings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="A Deep Dive into Tokenization"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4308 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="A Deep Dive into Tokenization"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[MyScale & Jina AI: Unleashing Great Potential for Your AI Applications]]></title><description><![CDATA[With full integration of Jina Embeddings v2 models, MyScale allows users to harness the capabilities of Jina AI within an SQL database.]]></description><link>https://jina.ai/news/myscale-jina-ai-unleashing-great-potential-for-your-ai-applications/</link><guid isPermaLink="false">65b785a3c38742000104062a</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 29 Jan 2024 15:00:29 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"><p><a href="https://myscale.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MyScale</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its AI-focused SQL vector database. You can now use Jina AI&apos;s cutting-edge embedding models in conjunction with powerful and time-tested SQL database technologies to bring precise text matching and efficient semantic similarity to your data-driven applications.</p><p>Follow the link below to&#xA0;MyScale&apos;s website to see how you can supercharge your SQL with Jina Embeddings.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://myscale.com/blog/myscale-integration-jinaai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications</div><div class="kg-bookmark-description">MyScale is fully integrated with Jina Embeddings v2 in the EmbedText function, allowing users to process text with an input length of up to 8K using the standard SQL syntax.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://myscale.com/favicon.ico" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d3lhz231q7ogjd.cloudfront.net/blog/myscale-and-jinaai-2.jpg" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face]]></title><description><![CDATA[Jina AI's open-source bilingual embedding models for German-English and Chinese-English are now on Hugging Face.
We’re going to walk through installation and cross-language retrieval.]]></description><link>https://jina.ai/news/jina-embeddings-v2-bilingual-models-are-now-open-source-on-hugging-face/</link><guid isPermaLink="false">65b3adb510ff9f0001c50c4d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 26 Jan 2024 16:14:56 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><p>Jina AI has released its state-of-the-art open-source bilingual embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io">German-English</a> and <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese-English</a> language pairs via Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class="kg-bookmark-description">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class="kg-bookmark-description">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>In this tutorial, we&#x2019;re going to walk through a very minimal installation and use case that will cover:</p><ol><li>Downloading Jina Embedding models from Hugging Face.</li><li>Using the models to get encodings from texts in German and English.</li><li>Building a very rudimentary embeddings-based neural search engine for cross-language queries.</li></ol><p>We will show you how to use Jina Embeddings to write English queries that retrieve matching texts in German and vice-versa.</p><p>This tutorial works the same for the Chinese model. Just follow the instructions in the section (towards the end) titled <a href="#querying-in-chinese" rel="noreferrer"><strong>Querying in Chinese</strong></a> to get the Chinese-English bilingual model and an example document in Chinese.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-de &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-de.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-zh?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-zh &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="bilingual-embedding-models">Bilingual Embedding Models</h2><p>A bilingual embedding model is a model that maps texts in two languages &#x2014; German and English in this tutorial, Chinese and English for the Chinese model &#x2014; to the same embedding space. And, it does it in such a way that if a German text and an English text mean the same thing, their corresponding embedding vectors will be close together.</p><p>Models like this are very well suited to cross-language information retrieval applications, which we will show in this tutorial, but can also serve as a basis for RAG-based chatbots, multilingual text categorization, summarization, sentiment analysis, and any other application that uses embeddings. By using models like these, you can treat texts in both languages as if they were written in the same language.</p><p>Although many giant language models trained claim to support many different languages, they do not support all languages equally. There are growing questions of <a href="https://aclanthology.org/2023.findings-eacl.89/?ref=jina-ai-gmbh.ghost.io">bias caused by the dominance of English on the Internet</a> and input sources distorted by the <a href="https://arxiv.org/abs/2401.05749?ref=jina-ai-gmbh.ghost.io">widespread online publication of machine-translated texts</a>. By focusing on two languages, we can better control embedding quality for both, minimizing bias while producing much smaller models with similar or higher performance than giant models that purport to handle dozens of languages.</p><p>Jina Embeddings v2 bilingual models support 8,192 input context tokens, enabling them not just to support two languages, but also to support relatively large segments of text compared to comparable models. This makes them ideal for more complex use cases where much more textual information has to be processed into embeddings.</p><h2 id="follow-along-on-google-colab">Follow along on Google Colab</h2><p>This tutorial has an <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">accompanying notebook</a> that you can run on <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, or locally on your own system.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/workshops/blob/feat-embeddings-notebook/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/cce4fce8bbe78d8bdc0c77a288df9fa7/img/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="installing-the-prerequisites">Installing the Prerequisites</h2><p>Make sure the current environment has the relevant libraries installed. You will need the latest version of <a href="https://pypi.org/project/transformers/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>transformers</code></a>, so even if it is already installed, run:</p><pre><code class="language-bash">pip install -U transformers 
</code></pre><p>This tutorial will use the <a href="https://faiss.ai/?ref=jina-ai-gmbh.ghost.io">FAISS library from Meta</a> to do vector search and comparison. To install it, run:</p><pre><code class="language-bash">pip install faiss-cpu
</code></pre><p>We will also be using <a href="https://www.crummy.com/software/BeautifulSoup/?ref=jina-ai-gmbh.ghost.io">Beautiful Soup</a> to process the input data in this tutorial, so make sure it is installed:</p><pre><code class="language-bash">pip install bs4
</code></pre><h2 id="access-to-hugging-face">Access to Hugging Face</h2><p>You will need access to Hugging Face, specifically an account and an access token to download models.</p><p><strong>If you do not have an account on Hugging Face:</strong></p><p>Go to <a href="https://huggingface.co/?ref=jina-ai-gmbh.ghost.io">https://huggingface.co/</a> and you should see a &#x201C;Sign Up&#x201D; button on the upper right of the page. Click it and follow the instructions from there to make a new account.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="1088" height="887" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png 1088w" sizes="(min-width: 720px) 720px"></figure><p><strong>After you are logged into your account:</strong></p><p>Follow the instructions <a href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io">on the Hugging Face website</a> to get an access token.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">User access tokens</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/docs/hub.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>You need to copy this token into an environment variable called <code>HF_TOKEN</code>. If you&#x2019;re working in a notebook (on <a href="https://colab.research.google.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, for example) or setting it internally in a Python program, use the following Python code:</p><pre><code class="language-python">import os

os.environ[&apos;HF_TOKEN&apos;] = &quot;&lt;your token here&gt;&quot;
</code></pre><p>In your shell, use whatever the provided syntax is for setting an environment variable. In <code>bash</code> :</p><pre><code class="language-bash">export HF_TOKEN=&quot;&lt;your token here&gt;&quot;
</code></pre><h2 id="download-jina-embeddings-v2-for-german-and-english">Download Jina Embeddings v2 for German and English</h2><p>Once your token is set, you can download the Jina Embeddings German-English bilingual model using the <code>transformers</code> library:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
</code></pre><p>This may take several minutes the first time you do it, but the model will be cached locally after that, so don&#x2019;t worry if you restart this tutorial later.</p><h2 id="download-english-language-data">Download English-language Data</h2><p>For this tutorial, we are going to get the English-language version of the book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a>. This book is also available in Chinese and German, which we&#x2019;ll use later in this tutorial.</p><p>To download the EPUB version, run the following command:</p><pre><code class="language-bash">wget -O progit-en.epub https://open.umn.edu/opentextbooks/formats/3437</code></pre><p>This copies the book to a file named <code>progit-en.epub</code> in the local directory.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--27-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="490" height="647"><figcaption><span style="white-space: pre-wrap;">The cover of the paper edition.</span></figcaption></figure><p>Alternatively, you can just visit the link <a href="https://open.umn.edu/opentextbooks/formats/3437?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">https://open.umn.edu/opentextbooks/formats/3437</a> to download it to a local drive. It is available under the&#xA0;<a href="https://creativecommons.org/licenses/by-nc-sa/3.0/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Creative Commons Attribution Non Commercial Share Alike 3.0 license</a>.</p><h2 id="processing-the-data">Processing the Data</h2><p>This particular text has an internal structure of hierarchical sections, which we can easily find by looking for the <code>&lt;section&gt;</code> tag in the underlying XHTML data. The code below reads the EPUB file and splits it up using the internal structure of an EPUB file and the <code>&lt;section&gt;</code> tag, then converts each section to plain text without XHTML tags. It creates a Python dictionary whose keys are a set of strings indicating each section&#x2019;s location in the book, and whose values are the plain text contents of that section.</p><pre><code class="language-python">from zipfile import ZipFile
from bs4 import BeautifulSoup
import copy

def decompose_epub(file_name):
    
    def to_top_text(section):
        selected = copy.copy(section)
				while next_section := selected.find(&quot;section&quot;):
            next_section.decompose()
        return selected.get_text().strip()

    ret = {}
    with ZipFile(file_name, &apos;r&apos;) as zip:
        for name in zip.namelist():
            if name.endswith(&quot;.xhtml&quot;):
                data = zip.read(name)
                doc = BeautifulSoup(data.decode(&apos;utf-8&apos;), &apos;html.parser&apos;)
                ret[name + &quot;:top&quot;] = to_top_text(doc)
                for num, sect in enumerate(doc.find_all(&quot;section&quot;)):
                    ret[name + f&quot;::{num}&quot;] = to_top_text(sect)
    return ret
</code></pre><p>Then, run the <code>decompose_epub</code> function on the EPUB file you downloaded before:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-en.epub&quot;)
</code></pre><p>The variable <code>book_data</code> will now have 583 sections in it. For example:</p><pre><code class="language-python">print(book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;])
</code></pre><p>Result:</p><pre><code class="language-Text">The Command Line
There are a lot of different ways to use Git.
There are the original command-line tools, and there are many graphical user interfaces of varying capabilities.
For this book, we will be using Git on the command line.
For one, the command line is the only place you can run all Git commands&#x2009;&#x2014;&#x2009;most of the GUIs implement only a partial subset of Git functionality for simplicity.
If you know how to run the command-line version, you can probably also figure out how to run the GUI version, while the opposite is not necessarily true.
Also, while your choice of graphical client is a matter of personal taste, all users will have the command-line tools installed and available.
So we will expect you to know how to open Terminal in macOS or Command Prompt or PowerShell in Windows.
If you don&#x2019;t know what we&#x2019;re talking about here, you may need to stop and research that quickly so that you can follow the rest of the examples and descriptions in this book.
</code></pre><h2 id="generating-and-indexing-embeddings-with-jina-embeddings-v2-and-faiss">Generating and Indexing Embeddings with Jina Embeddings v2 and FAISS</h2><p>For each of the 583 sections, we will generate an embedding and store it in a FAISS index. Jina Embeddings v2 models accept input of up to 8192 tokens, large enough that for a book like this, we don&#x2019;t need to do any further text segmentation or check if any section has too many tokens. The longest section in the book has roughly 12,000 characters, which, for normal English, should be far below the 8k token limit.</p><p>To generate a single embedding, you use the <code>encode</code> method of the model we downloaded. For example:</p><pre><code class="language-python">model.encode([book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;]])
</code></pre><p>This returns an array containing a single 768 dimension vector:</p><pre><code class="language-python">array([[ 6.11135997e-02,  1.67829826e-01, -1.94809273e-01,
         4.45595086e-02,  3.28837298e-02, -1.33441269e-01,
         1.35364473e-01, -1.23119736e-02,  7.51526654e-02,
        -4.25386652e-02, -6.91794455e-02,  1.03527725e-01,
        -2.90831417e-01, -6.21018047e-03, -2.16205455e-02,
        -2.20803712e-02,  1.50471330e-01, -3.31433356e-01,
        -1.48741454e-01, -2.10959971e-01,  8.80039856e-02,
				....
</code></pre><p>That is an embedding.</p><p>Jina Embeddings models are set up to allow batch processing. The optimal batch size depends on the hardware you use when running. A large batch size risks running out of memory. A small batch size will take longer to process.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Setting <code spellcheck="false" style="white-space: pre-wrap;">batch_size=5</code> worked on Google Colab in free tier without a GPU, and took <b><strong style="white-space: pre-wrap;">about an hour</strong></b> to generate the entire set of embeddings.</div></div><p>In production, we recommend using much more powerful hardware or using Jina AI&#x2019;s <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Embedding API service</a>. Follow the link below to find out how it works and how to get started with free access.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>The code below generates the embeddings and stores them in a FAISS index. Set the variable <code>batch_size</code> as appropriate to your resources.</p><pre><code class="language-python">import faiss

batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>When working in a production environment, a Python dictionary is not an adequate or performant way to handle documents and embeddings. You should use a purpose-built vector database, which will have its own directions for data insertion.</p><h2 id="querying-in-german-for-english-results">Querying in German for English Results</h2><p>When we query for something from this set of texts, here&#x2019;s what will happen:</p><ol><li>The Jina Embeddings German-English model will create an embedding for the query.</li><li>We will use the FAISS index (<code>faiss_index</code>) to get the stored embedding with the highest cosine to the query embedding and return its place in the index.</li><li>We will look up the corresponding text in the vector data array (<code>vector_data</code>) and print out the cosine, the location of the text, and the text itself.</li></ol><p>That&#x2019;s what the <code>query</code> function below does.</p><pre><code class="language-python">def query(query_str):
    query = model.encode([query_str], normalize_embeddings=True)
    cosine, index = faiss_index.search(query, 1)
    print(f&quot;Cosine: {cosine[0][0]}&quot;)
    loc, txt = vector_data[index[0][0]]
    print(f&quot;Location: {loc}\\nText:\\n\\n{txt}&quot;)
</code></pre><p>Now let&#x2019;s try it out.</p><pre><code class="language-python"># Translation: &quot;How do I roll back to a previous version?&quot;
query(&quot;Wie kann ich auf eine fr&#xFC;here Version zur&#xFC;cksetzen?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5202275514602661
Location: EPUB/ch02-git-basics-chapter.xhtml::20
Text:

Undoing things with git restore
Git version 2.23.0 introduced a new command: git restore.
It&#x2019;s basically an alternative to git reset which we just covered.
From Git version 2.23.0 onwards, Git will use git restore instead of git reset for many undo operations.
Let&#x2019;s retrace our steps, and undo things with git restore instead of git reset.
</code></pre><p>This is a pretty good choice to answer the question. Let&#x2019;s try another one:</p><pre><code class="language-python"># Translation: &quot;What does &apos;version control&apos; mean?&quot;
query(&quot;Was bedeutet &apos;Versionsverwaltung&apos;?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5001817941665649
Location: EPUB/ch01-getting-started.xhtml::1
Text:

About Version Control

What is &#x201C;version control&#x201D;, and why should you care?
Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.
For the examples in this book, you will use software source code as the files being version controlled, though in reality you can do this with nearly any type of file on a computer.
If you are a graphic or web designer and want to keep every version of an image or layout (which you would most certainly want to), a Version Control System (VCS) is a very wise thing to use.
It allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more.
Using a VCS also generally means that if you screw things up or lose files, you can easily recover.
In addition, you get all this for very little overhead.
</code></pre><p>Try it with your own German questions to see how well it works. As a general practice, when dealing with text information retrieval, you should ask for three to five responses instead of just one. The best answer is often not the first one.</p><h2 id="reversing-the-roles-querying-german-documents-with-english">Reversing the Roles: Querying German documents with English</h2><p>The book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a> is also <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git-german?ref=jina-ai-gmbh.ghost.io">available in German</a>. We can use this same model to give this demo with the languages reversed.</p><p>Download the ebook:</p><pre><code class="language-bash">wget -O progit-de.epub https://open.umn.edu/opentextbooks/formats/3454
</code></pre><p>This copies the book to a file named <code>progit-de.epub</code>. We then process it the same way we did for the English book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-de.epub&quot;)
</code></pre><p>And then generate the embeddings the same way as before:</p><pre><code class="language-python">batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>We can now use the same <code>query</code> function to search in English for answers in German:</p><pre><code class="language-python">query(&quot;What is version control?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.6719034910202026
Location: EPUB/ch01-getting-started.xhtml::1
Text:

Was ist Versionsverwaltung?

Was ist &#x201E;Versionsverwaltung&#x201C;, und warum sollten Sie sich daf&#xFC;r interessieren?
Versionsverwaltung ist ein System, welches die &#xC4;nderungen an einer oder einer Reihe von Dateien &#xFC;ber die Zeit hinweg protokolliert, sodass man sp&#xE4;ter auf eine bestimmte Version zur&#xFC;ckgreifen kann.
Die Dateien, die in den Beispielen in diesem Buch unter Versionsverwaltung gestellt werden, enthalten Quelltext von Software, tats&#xE4;chlich kann in der Praxis nahezu jede Art von Datei per Versionsverwaltung nachverfolgt werden.
Als Grafik- oder Webdesigner m&#xF6;chte man zum Beispiel in der Lage sein, jede Version eines Bildes oder Layouts nachverfolgen zu k&#xF6;nnen. Als solcher w&#xE4;re es deshalb ratsam, ein Versionsverwaltungssystem (engl. Version Control System, VCS) einzusetzen.
Ein solches System erlaubt es, einzelne Dateien oder auch ein ganzes Projekt in einen fr&#xFC;heren Zustand zur&#xFC;ckzuversetzen, nachzuvollziehen, wer zuletzt welche &#xC4;nderungen vorgenommen hat, die m&#xF6;glicherweise Probleme verursachen, herauszufinden wer eine &#xC4;nderung urspr&#xFC;nglich vorgenommen hat und viele weitere Dinge.
Ein Versionsverwaltungssystem bietet allgemein die M&#xF6;glichkeit, jederzeit zu einem vorherigen, funktionierenden Zustand zur&#xFC;ckzukehren, auch wenn man einmal Mist gebaut oder aus irgendeinem Grund Dateien verloren hat.
All diese Vorteile erh&#xE4;lt man f&#xFC;r einen nur sehr geringen, zus&#xE4;tzlichen Aufwand.
</code></pre><p>This section&#x2019;s title translates as <em>&#x201C;What is version control?&#x201D;</em>, so this is a good response.</p><h2 id="querying-in-chinese">Querying in Chinese</h2><p>These examples will work exactly the same way with Jina Embeddings v2 for Chinese and English. To use the Chinese model instead, just run the following:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
</code></pre><p>And to get the Chinese edition of <em>Pro Git: Everything You Need to Know About Git</em>:</p><pre><code class="language-python">wget -O progit-zh.epub https://open.umn.edu/opentextbooks/formats/3455
</code></pre><p>Then, process the Chinese book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-zh.epub&quot;)
</code></pre><p>All the other code in this tutorial will work the same way.</p><h2 id="the-future-more-languages-including-programming">The Future: More Languages, including Programming</h2><p>We will be rolling out more bilingual models in the immediate future, with Spanish and Japanese already in the works, as well as a model that supports English and several major programming languages. These models are ideally suited to international enterprises that manage multilingual information, and can serve as the cornerstone for AI-powered information retrieval and RAG-based generative language models, inserting themselves into a variety of cutting-edge AI use cases.</p><p>Jina AI&#x2019;s models are compact and perform among the best in their class, showing how you don&#x2019;t need the biggest model to get the best performance. By focusing on bilingual performance, we produce models that are both better at those languages, easier to adapt, and more cost-effective than large models trained on uncurated data.</p><p>Jina Embeddings are available from <a href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io">Hugging Face</a>, in the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS marketplace</a> for use in Sagemaker, and via the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings web API</a>. They are fully integrated into many AI process frameworks and vector databases.</p><p>See the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings website</a> for more information, or contact us to discuss how Jina AI&#x2019;s offerings can fit into your business processes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop]]></title><description><![CDATA[Discover how SceneXplain, our AI tool, transforms images into valuable assets for marketers, advertisers, and e-commerce pros.]]></description><link>https://jina.ai/news/discover-the-hidden-business-value-in-images-with-scenexplain-martech-online-workshop/</link><guid isPermaLink="false">65b225a58da8040001e1739f</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Fri, 26 Jan 2024 15:03:35 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/PP-workshop-slides--260-x-260-px---3000-x-890-px-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/PP-workshop-slides--260-x-260-px---3000-x-890-px-.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><p></p><h2 id="learn-how-ai-transforms-visuals-into-valuable-insights"><strong>Learn How AI Transforms Visuals into Valuable Insights</strong></h2><p>In the rapidly evolving world of Marketing Technology (MarTech), understanding and leveraging the power of images is crucial. <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">SceneXplain</a>, our advanced AI tool, is at the forefront of this revolution, turning mere visuals <a href="https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">into valuable business asset</a><a href="https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">s</a>; it&apos;s about driving tangible value and visibility for your brand. <br><br>This workshop is tailored for marketing professionals, advertisers, e-commerce practitioners, and business owners who are keen to use the potential of AI in their strategies.</p><h2 id="whats-in-store"><strong>What&apos;s in Store?</strong></h2><p>We&apos;ll delve deep into the world of SceneXplain, a state-of-the-art AI tool. You will gain comprehensive insights, explore inspiring real-world applications, and participate in hands-on tutorials to master SceneXplain&apos;s capabilities.</p><p><strong>Date and Time:</strong> Jan. 31st, 6 PM CET<br><strong>Location:</strong> Online. </p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">How To Use AI To Uncover The Business Value Hidden Behind Images &#xB7; Zoom &#xB7; Luma</div><div class="kg-bookmark-description">In the dynamic world of Marketing Technology (MarTech), the intersection of AI and visual content represents a frontier of untapped potential. The power of images extends far beyond mere&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://lu.ma/apple-touch-icon.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><span class="kg-bookmark-author">Miruna</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://social-images.lu.ma/cdn-cgi/image/format=auto,fit=cover,dpr=1,quality=75,width=800,height=419/api/event-one?calendar_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Fcommunity_avatar_5.png&amp;calendar_name&amp;color0=%23060505&amp;color1=%23ab1a49&amp;color2=%23d4baa7&amp;color3=%235e3030&amp;host_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Favatar_29.png&amp;host_name=Miruna&amp;img=https%3A%2F%2Fimages.lumacdn.com%2Fevent-covers%2Fsl%2Fa27afa4f-c693-4c4b-a224-d7466ebffbcc&amp;name=How%20To%20Use%20AI%20To%20Uncover%20The%20Business%20Value%20Hidden%20Behind%20Images" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"></div></a><figcaption><p><span style="white-space: pre-wrap;">Register Here!</span></p></figcaption></figure><h2 id="agenda"><strong>Agenda:</strong></h2><ul><li>A 5-minute introduction to the latest AI Trends in MarTech.</li><li>A 15-minute demonstration of SceneXplain&apos;s advanced features, including <a href="https://jina.ai/news/read-my-pics-you-got-ocr-in-my-visual-question-answering?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Visual Question Answering</a>, <a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io">structured JSON ouput</a>, and API integration followed by a brief Q&amp;A.</li><li>20 minutes of in-depth case studies focusing on AI&apos;s role in social media comprehension and product story generation.</li><li>A 5-minute interactive brainstorming session, offering a platform for engagement with experts.</li></ul><h2 id="why-attend"><strong>Why Attend?</strong></h2><ul><li>&#x200B;<strong>&#x1F4A1; Inspiring Case Studies: </strong>Covering solid use cases from leading media agencies, global FMCG companies, and high-end brands, catering to a diverse business audience.</li><li>&#x200B;&#x1F469;&#x200D;&#x1F3EB; <strong>Hands-on Tutorials: </strong>Our product expert will provide clear guidance on how to explore SceneXplain&apos;s advanced functions, improving your AI professional skills.</li><li><strong>&#x200B;&#x1F381; Gifts: </strong>During the brainstorming session, we have gifts for active participants who share their experience using SceneXplain or thoughts on its future applications.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">How To Use AI To Uncover The Business Value Hidden Behind Images &#xB7; Zoom &#xB7; Luma</div><div class="kg-bookmark-description">In the dynamic world of Marketing Technology (MarTech), the intersection of AI and visual content represents a frontier of untapped potential. The power of images extends far beyond mere&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://lu.ma/apple-touch-icon.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><span class="kg-bookmark-author">Miruna</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://social-images.lu.ma/cdn-cgi/image/format=auto,fit=cover,dpr=1,quality=75,width=800,height=419/api/event-one?calendar_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Fcommunity_avatar_5.png&amp;calendar_name&amp;color0=%23060505&amp;color1=%23ab1a49&amp;color2=%23d4baa7&amp;color3=%235e3030&amp;host_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Favatar_29.png&amp;host_name=Miruna&amp;img=https%3A%2F%2Fimages.lumacdn.com%2Fevent-covers%2Fsl%2Fa27afa4f-c693-4c4b-a224-d7466ebffbcc&amp;name=How%20To%20Use%20AI%20To%20Uncover%20The%20Business%20Value%20Hidden%20Behind%20Images" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"></div></a><figcaption><p><span style="white-space: pre-wrap;">Register Here!</span></p></figcaption></figure><h2 id="improve-your-experience"><strong>Improve Your Experience</strong></h2><p>To make the most of this 45-minute workshop, <a href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">we encourage early registration for the event</a>. If you&apos;re new to the tool, our experts will assist you during the workshop.</p><div class="kg-card kg-button-card kg-align-center"><a href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io" class="kg-btn kg-btn-accent">Register Here!</a></div><p>Join us at Jina AI&apos;s MarTech Online Workshop and step into the future of marketing technology with SceneXplain.<br><br>We&apos;re not just exploring AI; we&apos;re defining its role in transforming marketing.</p><p></p>]]></content:encoded></item><item><title><![CDATA[Making Accessibility Accessible: Create Alt Text with SceneXplain's API]]></title><description><![CDATA[SceneXplain is your accessibility ally, making it easy to generate image alt texts to aid visually-impaired users and improve SEO]]></description><link>https://jina.ai/news/make-accessibility-accessible-generate-alt-text-with-scenexplain/</link><guid isPermaLink="false">65af909b8da8040001e16fbb</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 23 Jan 2024 15:00:18 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><p>Accessibility (or &quot;a11y&quot; for short) is fast becoming an important part of web development and e-commerce. Back in the day, accessibility aids like <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">alt text</a> or color-blind-friendly color schemes weren&apos;t seen as high priorities by developers and companies. But now, with accessibility legislation from <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Europe</a> and the <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">USA</a>, making your website accessible is more important than ever before.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Alt text, or alternative text, is a brief description of an image used on websites and in digital documents. It helps people who can&apos;t see the image understand what it&apos;s about. This includes people using screen readers because of visual impairments and those with slow internet connections where images don&apos;t load. Alt text is also useful for search engines to understand and index images.</div></div><p>But how can you go about creating alt text for every image on your website? Manually going through each image and writing alt text could take forever, especially if you have thousands (or millions) of images. And if more are being added every day, it becomes a never-ending battle.</p><p>That&#x2019;s where SceneXplain comes in. It&#x2019;s your a11y ally! You can simply upload an image and get alt text for it without having to wrack your brain thinking of the wording yourself.</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Generate_Alt_Text_with_SceneXplain__C5Wt_1HXQo-LONuk8GAqfA?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<p>If you have, say, a few dozen images, this is a good way to give your brain a break. But you still need to do all the clicking and dragging yourself. Your brain wins, but your fingers don&apos;t. And if you have a few thousand images? Call the doctor now to pre-book your <a href="https://en.wikipedia.org/wiki/Repetitive_strain_injury?ref=jina-ai-gmbh.ghost.io">carpal tunnel</a> appointment.</p><p>If only there were a way you could automate the whole thing. Then your brain and fingers could <em>both</em> focus on more interesting things.</p><p>That&apos;s where SceneXplain&apos;s API comes in. You can write a script that will go through your thousands of images, send them in batches to SceneXplain, and generate a CSV file with the results (or with a bit more coding, integrate directly into your workflow.)</p><p>After all, you know what they say. You can&apos;t spell happiness without APIness.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Upon further reflection, I have found that the English language does not, in fact, work like that.</div></div><h2 id="what-is-an-api">What is an API?</h2><p>But before we dive into the <em>how</em>, let&apos;s look at the <em>what</em>. The <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Oxford English Dictionary</a> defines API as:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="927" height="209" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--50-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png 927w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Well, that&#x2019;s not useful at all</span></figcaption></figure><p>However, everyone&apos;s favorite AI, GPT-4 defines an API as:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="789" height="664" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--51-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png 789w" sizes="(min-width: 720px) 720px"></figure><p>Or, if you prefer a video explanation:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/s7wmiS2mSXY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="What is an API?"></iframe></figure><p>In short, you can write a Python (or any other language) program to talk to SceneXplain via its API and automate your whole alt-tagging process. We have a Python snippet that does just that.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Can&#x2019;t run the code on your own computer? Keep reading down to the Google Colab notebook which lets you use it in your browser.</div></div><p>Here&#x2019;s how you use it:</p><ol><li>Install the <a href="https://docs.python-requests.org/en/master/index.html?ref=jina-ai-gmbh.ghost.io">requests</a> library:</li></ol><pre><code class="language-bash">pip install requests
</code></pre><ol><li>Go to SceneXplain&#x2019;s API page to generate a secret key and copy it to your clipboard.</li><li>Paste it into the Python code below.</li><li>Copy an image URL into the code where it says <code>....</code>.</li><li>Run the code!</li></ol><pre><code class="language-python">import requests
import json

# generate token on SceneXplain&apos;s API page
YOUR_GENERATED_SECRET = &quot;your_generated_secret_here&quot;

data = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot; # change to image URL
    }
  ]
}

headers = {
  &quot;x-api-key&quot;: f&quot;token {YOUR_GENERATED_SECRET}&quot;,
  &quot;content-type&quot;: &quot;application/json&quot;,
}

response = requests.post(&quot;https://api.scenex.jina.ai/v1/describe&quot;, headers=headers, json=data)
print(response.json())
</code></pre><p>(We&#x2019;ll put in more code snippets later for cURL and JavaScript)</p><h2 id="api-in-action-scenexplain-in-a-notebook">API in action: SceneXplain in a Notebook</h2><p>Since we want to see this in action, we&#x2019;ll use the code live in a <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io">notebook</a>. That lets you see what&#x2019;s happening in real time with real data, and lets you examine and play with the Python code yourself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/f6c736cbde16632a9fcd16d9ab1970b1/img/favicon.ico" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure><p>The notebook goes beyond just the simple Python snippet above. It also downloads a sample dataset and exports the results to a CSV file.</p><h2 id="beyond-the-notebook-using-the-api-irl">Beyond the notebook: Using the API IRL</h2><p>Of course, you&#x2019;re not limited to Python when you use SceneXplain&#x2019;s API. Any language that has an HTTP library should work fine.</p><p>Here&#x2019;s that same code snippet from above, this time in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
};

const YOUR_GENERATED_SECRET = &apos;your_generated_secret_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_GENERATED_SECRET}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
}).then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>And this time as a <a href="https://curl.se/?ref=jina-ai-gmbh.ghost.io">cURL</a> command:</p><pre><code class="language-shell">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
}&apos;</code></pre><h2 id="improve-your-image-accessibility-with-scenexplain%E2%80%99s-api">Improve your image accessibility with SceneXplain&#x2019;s API</h2><p>To get started, head over to <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API page</a> to brush up on how it all works, generate a secret key, and then either adapt our notebook or create your own code to start improving accessibility today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?]]></title><description><![CDATA[Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ]]></description><link>https://jina.ai/news/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/</link><guid isPermaLink="false">65af98d28da8040001e17008</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 23 Jan 2024 11:22:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">On Jan. 25, 2024, OpenAI released <a href="https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io">a new embedding model</a> with a new feature called <i><b><strong class="italic" style="white-space: pre-wrap;">&quot;shortening&quot;</strong></b></i>, which allows developers to trim embeddings&#x2014;essentially cutting numbers from the sequence&apos;s end&#x2014;without compromising the embedding&apos;s ability to represent concepts effectively. Dive into this post for a solid theoretical foundation on the viability and rationale behind this innovation.</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><p>Consider this: when measuring the cosine similarity of embedding vectors in high-dimensional spaces, how does their similarity in lower-dimensional subspaces imply the overall similarity? Is there a direct, proportional relationship, or is the reality more complex with high-dimensional data?</p><p>More concretely, <strong>does high similarity between vectors in their first 256 dimensions assure a high similarity in their full 768 dimensions?</strong> Conversely, if vectors significantly differ in some dimensions, does this spell a low overall similarity? These aren&apos;t mere theoretical musings; they are crucial considerations for efficient vector retrieval, database indexing, and the performance of RAG systems.</p><p>Developers often rely on heuristics, assuming that high subspace similarity equates to high overall similarity or that notable differences in one dimension significantly affect the overall similarity. The question is: are these heuristic methods built on firm theoretical ground, or are they simply assumptions of convenience?</p><p>This post delves into these questions, examining the theory and practical implications of subspace similarity in relation to overall vector similarity. </p><h2 id="bounding-the-cosine-similarity">Bounding the Cosine Similarity</h2><p>Given vectors $\mathbf{A}, \mathbf{B}\in \mathbb{R}^d$, we decompose them as $\mathbf{A}=[\mathbf{A}_1, \mathbf{A}_2]$ and $\mathbf{B}=[\mathbf{B}_1, \mathbf{B}_2]$, where $\mathbf{A}_1,\mathbf{B}_1\in\mathbb{R}^m$ and $\mathbf{A}_2,\mathbf{B}_2\in\mathbb{R}^n$, with $m+n=d$.</p><p>The cosine similarity in the subspace $\mathbb{R}^m$ is given by $\cos(\mathbf{A}_1, \mathbf{B}_1)=\frac{\mathbf{A}_1\cdot\mathbf{B}_1}{\|\mathbf{A}_1\|\|\mathbf{B}_1\|}$; similarly, the similarity in the subspace $\mathbb{R}^n$ is $\cos(\mathbf{A}_2, \mathbf{B}_2)=\frac{\mathbf{A}_2\cdot\mathbf{B}_2}{\|\mathbf{A}_2\|\|\mathbf{B}_2\|}$.</p><p>In the original space $\mathbb{R}^d$, the cosine similarity is defined as:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}\\&amp;=\frac{\mathbf{A}_1\cdot\mathbf{B}_1+\mathbf{A}_2\cdot\mathbf{B}_2}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\cos(\mathbf{A}_1, \mathbf{B}_1)\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\cos(\mathbf{A}_2, \mathbf{B}_2)\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\end{align*}$$</p><p>Now, let $s := \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$. Then, we have:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;\leq\frac{s\|\mathbf{A}_1\|\|\mathbf{B}_1\|+s\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\cdot s\\&amp;=\cos(\underbrace{[\|\mathbf{A}_1\|, \|\mathbf{A}_2\|]}_{\mathbb{R}^2}, \underbrace{[\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]}_{\mathbb{R}^2})\cdot s\\&amp;\leq 1\cdot s \\&amp;= \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}$$</p><p>End of proof. </p><p>Note that in the final step of the proof, we leverage that the cosine similarity is always less than or equal to 1. This forms our upper bound. Similarly, we can show that the lower bound of \(\cos(\mathbf{A},\mathbf{B})\) is given by: </p><p>\[ \cos(\mathbf{A},\mathbf{B}) \geq t \cdot \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) \], where $t:= \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$.</p><p>Note that for the lower bound, we can not hastily conclude that \(\cos(\mathbf{A},\mathbf{B}) \geq t\). This is because of the range of the cosine function, which spans between \([-1, 1]\). Due to this range, it&apos;s impossible to establish a tighter lower bound than the trivial value of -1.</p><p>So in conclusion, we have the following loose bound: $$ -1\leq\cos(\mathbf{A},\mathbf{B})\leq\max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)).$$ and a tighter bound \[\begin{align*}  \gamma \cdot t\leq&amp;\cos(\mathbf{A}, \mathbf{B}) \leq\gamma\cdot s\\\gamma \cdot \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)) \leq &amp;\cos(\mathbf{A}, \mathbf{B}) \leq \gamma \cdot \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}\], where $\gamma = \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) $.</p><h3 id="connection-to-johnson%E2%80%93lindenstrauss-lemma">Connection to Johnson&#x2013;Lindenstrauss Lemma</h3><p>The JL lemma asserts that for any \(0 &lt; \epsilon &lt; 1\) and any finite set of points \( S \) in \( \mathbb{R}^d \), there exists a mapping \( f: \mathbb{R}^d \rightarrow \mathbb{R}^k \) (with \( k = O(\epsilon^{-2} \log |S|) \)) such that for all \( \mathbf{u}, \mathbf{v} \in S \), the Euclidean distances are approximately preserved:<br><br>\[(1 - \epsilon) \|\mathbf{u} - \mathbf{v}\|^2 \leq \|f(\mathbf{u}) - f(\mathbf{v})\|^2 \leq (1 + \epsilon) \|\mathbf{u} - \mathbf{v}\|^2\]</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Johnson&#x2013;Lindenstrauss lemma - Wikipedia</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://en.wikipedia.org/static/apple-touch/wikipedia.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><span class="kg-bookmark-author">Wikimedia Foundation, Inc.</span><span class="kg-bookmark-publisher">Contributors to Wikimedia projects</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"></div></a></figure><p>To make $f$ work like a subspace selection, we can use a diagonal matrix for projection, such as a \(5 \times 3\) matrix \(f\), albeit not random (note, the typical formulation of the JL lemma involves linear transformations that often utilize random matrices drawn from a Gaussian distribution). For instance, if we aim to retain the 1st, 3rd, and 5th dimensions from a 5-dimensional vector space, the matrix \(f\) could be designed as follows: \[f = \begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1\end{bmatrix}\]<br>However, by specifying $f$ to be diagonal, we limit the class of functions that can be used for the projection. The JL lemma guarantees the existence of a suitable $f$ within the broader class of linear transformations, but when we restrict $f$ to be diagonal, such a suitable $f$ may not exist within this restricted class for applying the JL lemma&apos;s bounds.</p><h2 id="validating-the-bounds">Validating the Bounds</h2><p>To empirically explore the theoretical bounds on cosine similarity in high-dimensional vector spaces, we can employ a Monte Carlo simulation. This method allows us to generate a large number of random vector pairs, compute their similarities in both the original space and subspaces, and then assess how well the theoretical upper and lower bounds hold in practice.</p><p>The following Python code snippet implements this concept. It randomly generates pairs of vectors in a high-dimensional space and computes their cosine similarity. Then, it divides each vector into two subspaces, calculates the cosine similarity within each subspace, and evaluates the upper and lower bounds of the full-dimensional cosine similarity based on the subspace similarities.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import numpy as np


def compute_cosine_similarity(U, V):
    # Normalize the rows to unit vectors
    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)
    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)
    # Compute pairwise cosine similarity
    return np.sum(U_norm * V_norm, axis=1)


# Generate random data
num_points = 5000
d = 1024
A = np.random.random([num_points, d])
B = np.random.random([num_points, d])

# Compute cosine similarity between A and B
cos_sim = compute_cosine_similarity(A, B)

# randomly divide A and B into subspaces
m = np.random.randint(1, d)
A1 = A[:, :m]
A2 = A[:, m:]
B1 = B[:, :m]
B2 = B[:, m:]

# Compute cosine similarity in subspaces
cos_sim1 = compute_cosine_similarity(A1, B1)
cos_sim2 = compute_cosine_similarity(A2, B2)

# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2
s = np.maximum(cos_sim1, cos_sim2)
t = np.minimum(cos_sim1, cos_sim2)

norm_A1 = np.linalg.norm(A1, axis=1)
norm_A2 = np.linalg.norm(A2, axis=1)
norm_B1 = np.linalg.norm(B1, axis=1)
norm_B2 = np.linalg.norm(B2, axis=1)

# Form new vectors in R^2 from the norms
norm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)
norm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)

# Compute cosine similarity in R^2
gamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)

# print some info and validate the lower bound and upper bound
print(&apos;d: %d\n&apos;
      &apos;m: %d\n&apos;
      &apos;n: %d\n&apos;
      &apos;avg. cosine(A,B): %f\n&apos;
      &apos;avg. upper bound: %f\n&apos;
      &apos;avg. lower bound: %f\n&apos;
      &apos;lower bound satisfied: %s\n&apos;
      &apos;upper bound satisfied: %s&apos; % (
          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),
          np.all(gamma * t &lt;= cos_sim)))
</code></pre><figcaption><p><span style="white-space: pre-wrap;">A Monte Carlo validator for validating cosine similarity bounds</span></p></figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-output">d: 1024
m: 743
n: 281
avg. cosine(A,B): 0.750096
avg. upper bound: 0.759080
avg. lower bound: 0.741200
lower bound satisfied: True
upper bound satisfied: True</code></pre><figcaption><p><span style="white-space: pre-wrap;">A sample output from our Monte Carlo validator. It&apos;s important to note that the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>lower/upper bound satisfied</span></code><span style="white-space: pre-wrap;"> condition is checked for every vector individually. Meanwhile, the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>avg. lower/upper bound</span></code><span style="white-space: pre-wrap;"> provides a more intuitive overview of the statistics related to these bounds but doesn&apos;t directly influence the validation process.</span></p></figcaption></figure><h2 id="understanding-the-bounds">Understanding the Bounds</h2><p>In a nutshell, when comparing two high-dimensional vectors, the overall similarity lies between the best and worst similarities of their subspaces, adjusted for how large or important those subspaces are in the overall scheme. This is what the bounds for cosine similarity in higher dimensions intuitively represent: the balance between the most and least similar parts, weighted by their relative sizes or importance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png" class="kg-image" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?" loading="lazy" width="1200" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Each pen has two main components: the body and the cap.</span></figcaption></figure><p>Imagine you&apos;re trying to compare two multi-part objects (let&apos;s say, two fancy pens) based on their overall similarity. Each pen has two main components: the body and the cap. The similarity of the whole pen (both body and cap) is what we&apos;re trying to determine:</p><h3 id="upper-bound-gamma-cdot-s">Upper Bound ($\gamma \cdot s$)</h3><p>Think of $s$ as the best match between corresponding parts of the pens. If the caps are very similar but the bodies aren&apos;t, $s$ is the similarity of the caps.</p><p>Now, $\gamma$ is like a scaling factor based on the size (or importance) of each part. If one pen has a very long body and a short cap, while the other has a short body and a long cap, $\gamma$ adjusts the overall similarity to account for these differences in proportions.</p><p>The upper bound tells us that no matter how similar some parts are, the overall similarity can&apos;t exceed this &quot;best part similarity&quot; scaled by the proportion factor.</p><h3 id="lower-bound-gamma-cdot-t">Lower Bound ($\gamma \cdot t$)</h3><p>Here, $t$ is the similarity of the least matching parts. If the bodies of the pens are quite different but the caps are similar, $t$ reflects the body&apos;s similarity.</p><p>Again, $\gamma$ scales this based on the proportion of each part.</p><p>The lower bound means that the overall similarity can&apos;t be worse than this &quot;worst part similarity&quot; after accounting for the proportion of each part.</p><h2 id="implications-of-the-bounds">Implications of the Bounds</h2><p>For software engineers working with embeddings, vector search, retrieval, or databases, understanding these bounds has practical implications, particularly when dealing with high-dimensional data. Vector search often involves finding the closest (most similar) vectors in a database to a given query vector, typically using cosine similarity as a measure of closeness. The bounds we discussed can provide insights into the effectiveness and limitations of using subspace similarities for such tasks.</p><h3 id="using-subspace-similarity-for-ranking">Using Subspace Similarity for Ranking</h3><p><strong>Safety and Accuracy</strong>: Using subspace similarity for ranking and retrieving top-k results can be effective, but with caution. The upper bound indicates that the overall similarity can&apos;t exceed the maximum similarity of the subspaces. Thus, if a pair of vectors is highly similar in a particular subspace, it&apos;s a strong candidate for being similar in the high-dimensional space.</p><p><strong>Potential Pitfalls</strong>: However, the lower bound suggests that two vectors with low similarity in one subspace could still be quite similar overall. Therefore, relying solely on subspace similarity might miss some relevant results.</p><h3 id="misconceptions-and-cautions">Misconceptions and Cautions</h3><p><strong>Overestimating Subspace Importance</strong>: A common misconception is overestimating the importance of a particular subspace. While high similarity in one subspace is a good indicator, it doesn&apos;t guarantee high overall similarity due to the influence of other subspaces.</p><p><strong>Ignoring Negative Similarities</strong>: In cases where the cosine similarity in a subspace is negative, it indicates an opposing relationship in that dimension. Engineers should be wary of how these negative similarities impact the overall similarity.</p>]]></content:encoded></item><item><title><![CDATA[Using Jina Embeddings v2 with Haystack Pipelines]]></title><description><![CDATA[Access Jina AI's state-of-the-art open-source embedding models in your Haystack application pipeline.]]></description><link>https://jina.ai/news/jina-embeddings-v2-and-deepset-haystack/</link><guid isPermaLink="false">65a916c40bab3100012d2e8c</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Saahil Ognawala]]></dc:creator><pubDate>Fri, 19 Jan 2024 15:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"><p><a href="https://www.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deepset</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its industry-leading <a href="https://haystack.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Haystack</a> NLP framework. You can now access Jina AI&apos;s state-of-the-art open-source embedding models in your Haystack pipeline.</p><p>We have collaborated with Deepset to bring you a tutorial on using&#xA0;Jina Embeddings v2&#xA0;in Haystack to analyze legal documents. Follow the link below to Deepset&apos;s blog to see how you can create a retrieval-augmented generation (RAG) application using Jina AI&apos;s industry-leading embeddings and Haystack!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Using Jina Embeddings v2 with Haystack 2.0 pipelines to summarize legal documents | Haystack</div><div class="kg-bookmark-description">Learn how to use the Jina v2 Embedding models in a RAG pipeline with our new Haystack integration.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://haystack.deepset.ai/favicon.ico" alt="Using Jina Embeddings v2 with Haystack Pipelines"><span class="kg-bookmark-author">Haystack</span><span class="kg-bookmark-publisher">Tilde Thurium Senior Developer Advocate</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack/thumbnail.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length]]></title><description><![CDATA[Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.]]></description><link>https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/</link><guid isPermaLink="false">65a542040bab3100012d2d79</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 15 Jan 2024 16:28:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><p><strong>Berlin, Germany - January 15, 2023</strong>&#xA0;&#x2013; Echoing JFK&apos;s iconic &apos;Ich bin ein Berliner&apos;, at Jina AI we&apos;re thrilled to bridge languages in our own way. Today, we&apos;re proud to announce our latest innovation: <code>jina-embeddings-v2-base-de</code>, a German/English embedding model. This state-of-the-art bilingual model is a significant stride forward in language representation, boasting a context length of <strong>8,192 tokens</strong>. What sets it apart is its remarkable efficiency: it achieves top-tier performance while being <strong>only 1/7th the size</strong> of comparable models.</p><p>Embeddings are crucial for German businesses looking to expand into the U.S. market. According to the <a href="https://www.gaccny.com/en/media/press-releases/news-details/german-american-business-outlook-gabo-2022-companies-in-the-us-remain-profitable?ref=jina-ai-gmbh.ghost.io">German American Business Outlook (GABO) 2022</a>, approximately a third of German companies generate over 20% of their global sales and profits in the U.S., with 93% expecting an increase in U.S. sales&#x200B;&#x200B;. This trend continues as 93% plan to grow their company&apos;s U.S. investments in the next three years, with <a href="https://www.globenewswire.com/news-release/2023/02/09/2604561/0/en/SURVEY-SHOWS-GERMAN-COMPANIES-IN-THE-US-PROFIT-FROM-ROBUST-MARKET-SIZE-AND-CUSTOMER-DEMAND.html?ref=jina-ai-gmbh.ghost.io">85% expecting net sales growth and a significant focus on digital transformation</a>&#x200B;&#x200B;. Good embeddings can play a pivotal role in this expansion by facilitating better understanding of customer preferences, enabling more effective communication, and positioning culturally resonant products.</p><p>Our breakthrough is particularly beneficial for German businesses looking to implement bilingual applications in English-speaking countries. With <code>jina-embeddings-v2-base-de</code>, we&apos;re excited to see how German companies will innovate and thrive in an increasingly connected world.</p><h2 id="model-highlights">Model Highlights</h2><ul><li><strong>State-of-the-art Performance</strong>: <code>jina-embeddings-v2-base-de</code> consistently ranking at the top in relevant benchmarks and leading among open-source models of similar size.</li><li><strong>Bilingual Model:</strong>&#xA0;This model encodes texts in both German and English, allowing the use of either language as the query or target document in retrieval applications. Texts with equivalent meanings in both languages are mapped to the same embedding space, forming the basis for multilingual applications.</li><li><strong>Extended Context</strong>: An 8192-token length enables <code>jina-embeddings-v2-base-de</code> to support longer texts and document fragments, far surpassing models that only support a few hundred tokens at a time.</li><li><strong>Compact Size</strong>: <code>jina-embeddings-v2-base-de</code> is built for high performance on standard computer hardware. With only 161 million parameters, the entire model is 322MB and fits in the memory of commodity computers. The embeddings themselves are 768 dimensions, a relatively small vector size compared to many models, saving space and run-time for applications.</li><li><strong>Bias Minimization</strong>: <a href="https://aclanthology.org/2023.findings-eacl.89.pdf?ref=jina-ai-gmbh.ghost.io">Recent research</a> shows that multilingual models without specific language training show strong biases towards English grammatical structures in embeddings. Embedding models should be about capturing meaning and not favor sentence pairs that are merely superficially similar.</li><li><strong>Seamless Integration</strong>: Jina Embeddings v2 models have native integrations with major vector databases, including <a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a>, <a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a>, and <a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a>, as well as RAG and LLM frameworks such as <a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a> and <a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul><h2 id="leading-performance-in-german-nlp">Leading Performance in German NLP</h2><p>We&apos;ve put <code>jina-embeddings-v2-base-de</code> to the test against four renowned baselines that also support both German and English. These include:</p><ul><li><a href="https://huggingface.co/intfloat/multilingual-e5-large?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-large</a> and <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-base</a> from Microsoft</li><li>T-Systems&#x2019; <a href="https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer?ref=jina-ai-gmbh.ghost.io">Cross English &amp; German RoBERTa for Sentence Embeddings</a></li><li><a href="https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2?ref=jina-ai-gmbh.ghost.io">Sentence-BERT</a> (<code>distiluse-base-multilingual-cased-v2</code>)</li></ul><p>Our benchmarks include <a href="https://huggingface.co/mteb?ref=jina-ai-gmbh.ghost.io">the MTEB tasks for English</a> and our own custom benchmark. Given the lack of a comprehensive benchmark suite for German embeddings, we took the initiative to <a href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io">develop our own</a>, inspired by the MTEB. We&apos;re proud to share our findings and breakthroughs with you here.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-de: MTEB: Massive Text Embedding Benchmark</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark. Contribute to jina-ai/mteb-de development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/a1d1679b83d58c1685db0d4d12d6bb6360edfe08fb0c4a9ebda71b57379853f3/jina-ai/mteb-de" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1310" height="925" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png 1310w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="compact-size-superior-results">Compact Size, Superior Results</h3><p><code>jina-embeddings-v2-base-de</code> demonstrates exceptional performance, especially in German language tasks. It outshines the E5 base model while being less than a third of its size. Moreover, it stands toe-to-toe with the E5 large model, which is seven times larger, showcasing its efficiency and power. This efficiency makes <code>jina-embeddings-v2-base-de</code> a game-changer, particularly when compared to other popular bi- and multilingual embedding models.</p><h3 id="excelling-in-german-english-cross-language-retrieval">Excelling in German-English Cross-Language Retrieval</h3><p>Our model isn&apos;t just about size and efficiency; it&apos;s also a top performer in English-German cross-language retrieval tasks. This is evident in its performance in various key benchmarks:</p><ul><li><a href="https://www.cl.uni-heidelberg.de/statnlpgroup/wikiclir/?ref=jina-ai-gmbh.ghost.io">WikiCLIR</a>, for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts17-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS17</a>, part of the MTEB evaluation for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts22-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS22</a>, for German to English retrieval, also part of MTEB</li><li><a href="https://huggingface.co/datasets/mteb/bucc-bitext-mining?ref=jina-ai-gmbh.ghost.io">BUCC</a>, for German to English retrieval, included in MTEB</li></ul><p>The performance in these benchmarks, particularly in the MTEB evaluation tests (with the exception of WikiCLIR), underscores the effectiveness of <code>jina-embeddings-v2-base-de</code> in handling complex bilingual tasks.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1275" height="625" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png 1275w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="get-api-access">Get API Access</h2><p>Our offerings for our enterprise users who value privacy and data compliance, including <code>jina-embeddings-v2-base-de</code>, are accessible via the Jina Embeddings API:</p><ol><li>Visit <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a> and click on the model dropdown</li><li>Select <code>jina-embeddings-v2-base-de</code></li></ol><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-3.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="2000" height="1062" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-3.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>We will make this model available in the AWS Sagemaker marketplace for Amazon cloud users and for download on HuggingFace very soon.</p><h2 id="jina-8k-embeddings-the-cornerstone-of-diverse-ai-applications">Jina 8K Embeddings: The Cornerstone of Diverse AI Applications</h2><p>Embeddings are crucial for a wide range of AI applications, including information retrieval, data quality control, classification, and recommendation. They are fundamental to enhancing numerous AI tasks.</p><p>Jina AI is committed to advancing the state-of-the-art in embedding technology, keeping our core AI components transparent, accessible, and affordable to enterprises of all types and sizes that value privacy and data compliance. In addition to <code>jina-embeddings-v2-base-de</code>, Jina AI has released state-of-the-art embedding models for <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese</a> and high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">English monolingual models</a>. This is part of our mission to make AI technology more inclusive and globally applicable.</p><p>We value your feedback. Join our community channel to contribute feedback and stay informed about our advancements. Together, we&apos;re shaping a more robust and inclusive AI future.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4232 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English]]></title><description><![CDATA[The first bilingual Chinese-English embedding model with 8192 token-length.]]></description><link>https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/</link><guid isPermaLink="false">659d729f0bab3100012d2c85</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 18:58:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><p>Following the remarkable success of the previous <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings V2</a>, we are thrilled to announce the launch of our latest Chinese/English bilingual text embedding model: <code>jina-embeddings-v2-base-zh</code>. This new model inherits the exceptional 8K token length of Jina Embeddings V2, now with robust support for both Chinese and English languages.</p><p><code>jina-embeddings-v2-base-zh</code> stands out for its exceptional quality and performance, achieved through rigorous and balanced pre-training with high-quality bilingual data. This approach ensures a significant reduction in bias, often seen in models trained with unbalanced multilingual data.</p><h2 id="highlights">Highlights</h2><ul><li><strong>Bilingual Model:</strong> This model encodes texts in both English and Chinese, allowing the use of either language as the query or target document. Texts with equivalent meanings in these languages are mapped to the same embedding space, forming the basis for numerous multilingual applications.</li><li><strong>Extended 8K Token-Length:</strong> Our model is capable of processing significantly large text passages, a feature that exceeds the capabilities of most other open-source models.</li><li><strong>Compact and Efficient:</strong> With a size of 322MB (161 million parameters) and output dimensions of 768, our model is designed for high performance on standard computer hardware without GPU, enhancing its accessibility.</li></ul><h2 id="leading-performance-on-c-mteb">Leading Performance on C-MTEB</h2><p>In the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Chinese MTEB leaderboard</a>, our Jina Embeddings v2, supporting both Chinese and English, stands out as one of the top models<strong> under 0.5GB</strong>. What sets it apart is its impressive 8K token-length capability, a unique feature in its category.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>Among Chinese models of similar size, only the E5 Multilingual model and our <code>jina-embeddings-v2-base-zh</code> offer support for English, enabling effective cross-language applications. Notably, Jina demonstrates significantly superior performance in all categories involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>While both models have an 8K token context size, <code>jina-embeddings-v2-base-zh</code> significantly outperforms OpenAI&apos;s <code>text-embedding-ada-002</code>,  especially in tasks involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h2 id="empowering-chinese-enterprises-for-global-expansion">Empowering Chinese Enterprises for Global Expansion</h2><p>Our Chinese-English embedding model is a powerful tool for Chinese companies looking to &apos;go global&apos; (&#x51FA;&#x6D77;). It seamlessly processes Chinese texts, providing high-quality embeddings that effortlessly integrate with leading vector databases, search systems, RAG applications.</p><p><code>jina-embeddings-v2-base-zh</code> is especially beneficial for developing AI applications tailored to Chinese-English contexts, crucial for businesses expanding internationally. Here are some specific use cases:</p><ol><li><strong>Document Analysis and Management</strong>: It can analyze and manage a vast array of documents, aiding in international legal and business transactions.</li><li><strong>AI-Powered Search Applications</strong>: Enhances search functions in multilingual environments, making it easier for global users to find relevant information in Chinese and English.</li><li><strong>Retrieval-Augmented Chatbots and Question-Answering</strong>: Builds efficient, bilingual customer service bots, improving interactions with customers worldwide.</li><li><strong>Natural Language Processing Applications</strong>: This includes sentiment analysis for understanding global market trends, topic modeling for international marketing strategies, and text classification for managing global communication.</li><li><strong>Recommender Systems</strong>: Tailors product and content recommendations for diverse global audiences, using insights drawn from Chinese and English data.</li></ol><p>By leveraging this model, Chinese enterprises can effectively bridge the language gap in their AI applications, enhancing their global competitiveness and market reach.</p><h2 id="get-started-with-jina-embeddings-v2-base-zh-via-api">Get Started with <code>jina-embeddings-v2-base-zh</code> via API</h2><p>Begin integrating our model into your workflow immediately through the Embeddings API. Simply visit the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">our Embeddings portal</a>, get your free access key or top up an existing key, and then choose <code>jina-embeddings-v2-base-zh</code> from the dropdown menu. It&apos;s that easy to get started!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1202" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--57-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--57-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Untitled--57-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png 2060w" sizes="(min-width: 720px) 720px"></figure><h2 id="whats-next-expanding-language-support-and-aws-sagemaker-integration">What&apos;s Next: Expanding Language Support and AWS Sagemaker Integration</h2><p><code>jina-embeddings-v2-base-zh</code> will soon be available via AWS Sagemaker and Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>At Jina AI, our commitment to being a leader in affordable and accessible embedding technology for a global audience is unwavering. We&apos;re actively developing additional multilingual offerings, focusing on major European and other international languages, to broaden our reach. Stay tuned for these exciting updates, including integration with AWS SageMaker, as we continue to expand our capabilities.</p><h2 id="a-special-thanks-to-our-early-testers">A Special Thanks to Our Early Testers</h2><p>We&apos;re immensely grateful to the select members of our Chinese user community who tested the preview version (<code>jina-embeddings-v2-base-zh-preview</code>). Their insightful feedback was crucial in enhancing the official release&apos;s performance for this release. If you have any observations or suggestions regarding the quality of our models, we warmly invite you to join our Discord server and share your thoughts with us. Your input is invaluable in our continuous improvement journey.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">Improved Score Distribution vs. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code><span style="white-space: pre-wrap;"> suffered from inflated similarity scores, leading to high cosine scores even for unrelated items. This was particularly evident in the top-5 results from the screenshot below. The similarity scores were consistently high and did not accurately reflect the true relationship between items. For example, the comparison between &#x201C;&#x5B89;&#x59AE;&#x201D; and &#x201C;&#x84B8;&#x6C7D;&#x673A;&#x201D; received misleadingly high similarity scores.</span></p><p><span style="white-space: pre-wrap;">In the official release, we have fine-tuned the model to produce more distinct and logical similarity scores, ensuring a more accurate representation of the relationships between items. For example, the revised scoring now presents a broader range, offering a clearer insight into the relative similarity among items.</span></p><p><span style="white-space: pre-wrap;">Additionally, Jina Embeddings now uniquely stands as the only open-source embedding model supporting 8192 tokens. This feature highlights its capability in processing a wide variety of data types, from extensive documents to brief phrases, or even individual words/names such as &#x201C;&#x5B89;&#x59AE;&#x201D; vs &#x201C;&#x9732;&#x5A1C;&#x201D;.</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure><p></p><h2 id="%E4%B8%AD%E8%8B%B1%E5%8F%8C%E8%AF%AD8k%E5%90%91%E9%87%8F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B0%E9%B2%9C%E5%87%BA%E7%82%89%EF%BC%8C%E4%BC%81%E4%B8%9A%E5%87%BA%E6%B5%B7%E5%BF%85%E5%A4%87%EF%BC%81">&#x4E2D;&#x82F1;&#x53CC;&#x8BED;8K&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#x65B0;&#x9C9C;&#x51FA;&#x7089;&#xFF0C;&#x4F01;&#x4E1A;&#x51FA;&#x6D77;&#x5FC5;&#x5907;&#xFF01;</h2><p>&#x81EA;&#x4ECE;&#x6211;&#x4EEC;&#x7684; Embeddings V2 &#x83B7;&#x5F97;&#x5404;&#x754C;&#x597D;&#x8BC4;&#x540E;&#xFF0C;&#x4ECA;&#x65E5;&#xFF0C;&#x6211;&#x4EEC;&#x63A8;&#x51FA;&#x4E86;&#x5168;&#x65B0;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#xFF1A;jina-embeddings-v2-base-zh&#x3002;&#x6B64;&#x6A21;&#x578B;&#x4E0D;&#x4EC5;&#x7EE7;&#x627F;&#x4E86; V2 &#x7684;&#x5168;&#x90E8;&#x4F18;&#x52BF;&#xFF0C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x957F;&#x8FBE;&#x516B;&#x5343;&#x8BCD;&#x5143;&#x7684;&#x6587;&#x672C;&#xFF0C;&#x66F4;&#x80FD;&#x6D41;&#x7545;&#x5E94;&#x5BF9;&#x4E2D;&#x82F1;&#x6587;&#x53CC;&#x8BED;&#x5185;&#x5BB9;&#xFF0C;&#x4E3A;&#x8DE8;&#x8BED;&#x79CD;&#x7684;&#x5E94;&#x7528;&#x63D2;&#x4E0A;&#x4E86;&#x7FC5;&#x8180;&#x3002;</p><p>jina-embeddings-v2-base-zh &#x4E4B;&#x6240;&#x4EE5;&#x8868;&#x73B0;&#x5353;&#x8D8A;&#xFF0C;&#x5168;&#x8D56;&#x4F18;&#x8D28;&#x7684;&#x53CC;&#x8BED;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x7ECF;&#x8FC7;&#x6211;&#x4EEC;&#x4E25;&#x683C;&#x4E14;&#x5E73;&#x8861;&#x7684;&#xA0;<strong>&#x9884;&#x8BAD;&#x7EC3;&#x3001;&#x4E00;&#x9636;&#x5FAE;&#x8C03;&#x548C;&#x4E8C;&#x9636;&#x5FAE;&#x8C03;</strong>&#x3002;&#x8FD9;&#x79CD;&#x4E09;&#x6B65;&#x8D70;&#x7684;&#x8BAD;&#x7EC3;&#x8303;&#x5F0F;&#x4E0D;&#x4EC5;&#x6CDB;&#x5316;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x53CC;&#x8BED;&#x80FD;&#x529B;&#xFF0C;&#x66F4;&#x6709;&#x6548;&#x7684;&#x964D;&#x4F4E;&#x4E86;&#x6A21;&#x578B;&#x504F;&#x89C1;&#xFF0C;&#x89E3;&#x51B3;&#x4E86;&#x591A;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x65F6;&#x5E38;&#x906D;&#x9047;&#x5230;&#x7684;&#x201C;&#x4E0D;&#x60A3;&#x5BE1;&#x800C;&#x60A3;&#x4E0D;&#x5747;&#x201D;&#x7684;&#x95EE;&#x9898;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E7%89%B9%E8%89%B2%E4%B8%80%E8%A7%88"><strong>&#x6A21;&#x578B;&#x7279;&#x8272;&#x4E00;&#x89C8;</strong></h3><p><strong>&#x7279;&#x8272; 1&#xFF1A;&#x53CC;&#x8BED;&#x65E0;&#x7F1D;&#x5BF9;&#x63A5;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x80FD;&#x591F;&#x6D41;&#x7545;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x4F5C;&#x4E3A;&#x641C;&#x7D22;&#x67E5;&#x8BE2;&#x8FD8;&#x662F;&#x76EE;&#x6807;&#x6587;&#x6863;&#x3002;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#x4E2D;&#x610F;&#x4E49;&#x76F8;&#x8FD1;&#x7684;&#x5185;&#x5BB9;&#x90FD;&#x4F1A;&#x88AB;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x5411;&#x91CF;&#x7A7A;&#x95F4;&#xFF0C;&#x4E3A;&#x591A;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x5960;&#x5B9A;&#x4E86;&#x575A;&#x5B9E;&#x57FA;&#x7840;&#x3002;</p><p><strong>&#x7279;&#x8272; 2&#xFF1A;8k Token &#x8D85;&#x957F;&#x6587;&#x672C;&#x652F;&#x6301;</strong></p><p>&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x652F;&#x6301;&#x957F;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x5728;&#x5F00;&#x6E90;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x4E2D;&#x72EC;&#x6811;&#x4E00;&#x5E1C;&#xFF0C;&#x5728;&#x5904;&#x7406;&#x66F4;&#x957F;&#x7684;&#x6587;&#x672C;&#x6BB5;&#x843D;&#x4E0A;&#x63D0;&#x4F9B;&#x4E86;&#x663E;&#x8457;&#x4F18;&#x52BF;&#x3002;</p><p><strong>&#x7279;&#x8272; 3&#xFF1A;&#x9AD8;&#x6548;&#x7D27;&#x51D1;&#x7684;&#x6A21;&#x578B;&#x7ED3;&#x6784;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x4EE5; 322MB &#x7684;&#x8F7B;&#x5DE7;&#x4F53;&#x79EF;&#xFF08;&#x5305;&#x542B; 1.61 &#x4EBF;&#x53C2;&#x6570;&#xFF09;&#xFF0C;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x4E3A; 768&#xFF0C;&#x80FD;&#x591F;&#x5728;&#x666E;&#x901A;&#x8BA1;&#x7B97;&#x673A;&#x786C;&#x4EF6;&#x4E0A;&#x9AD8;&#x6548;&#x8FD0;&#x884C;&#xFF0C;&#x65E0;&#x9700;&#x4F9D;&#x8D56; GPU&#xFF0C;&#x6781;&#x5927;&#x5730;&#x63D0;&#x5347;&#x4E86;&#x5176;&#x5B9E;&#x7528;&#x6027;&#x548C;&#x4FBF;&#x6377;&#x6027;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%8D%93%E8%B6%8A"><strong>&#x6A21;&#x578B;&#x6027;&#x80FD;&#x5353;&#x8D8A;</strong></h3><p>&#x5728; CMTEB &#x6392;&#x884C;&#x699C;&#x7684;&#x6FC0;&#x70C8;&#x7ADE;&#x4E89;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684; Jina Embeddings v2 &#x6A21;&#x578B;&#x5728; 0.5GB &#x4EE5;&#x4E0B;&#x6A21;&#x578B;&#x7C7B;&#x522B;&#x4E2D;&#x8131;&#x9896;&#x800C;&#x51FA;&#xFF0C;&#x5B83;&#x4E0D;&#x4EC5;&#x652F;&#x6301;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x800C;&#x4E14;&#x80FD;&#x591F;&#x5904;&#x7406;&#x9AD8;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#xFF0C;&#x8FD9;&#x4E00;&#x80FD;&#x529B;&#x5728;&#x540C;&#x7C7B;&#x6A21;&#x578B;&#x4E2D;&#x5B9E;&#x5C5E;&#x7F55;&#x89C1;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>&#x5728;&#x540C;&#x7B49;&#x4F53;&#x79EF;&#x7684;&#x652F;&#x6301;&#x4E2D;&#x6587;&#x7684;&#x6A21;&#x578B;&#x4E2D;&#xFF0C;Multilingual E5 &#x548C;&#x6211;&#x4EEC;&#x7684; jina-embeddings-v2-base-zh &#x662F;&#x552F;&#x4E8C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x82F1;&#x6587;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x8FD9;&#x4F7F;&#x5F97;&#x8DE8;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x6210;&#x4E3A;&#x53EF;&#x80FD;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>&#x76EE;&#x524D;&#xFF0C;&#x5168;&#x7403;&#x8303;&#x56F4;&#x5185;&#xFF0C;&#x4EC5;&#x6709; OpenAI &#x7684;&#x95ED;&#x6E90;&#x6A21;&#x578B; text-embedding-ada-002 &#x548C; Jina Embeddings &#x80FD;&#x591F;&#x652F;&#x6301; 8k Token &#x7684;&#x957F;&#x6587;&#x672C;&#x8F93;&#x5165;&#x3002;&#x800C;&#x5728;&#x5904;&#x7406;&#x4E2D;&#x6587;&#x4EFB;&#x52A1;&#x65B9;&#x9762;&#xFF0C;Jina Embeddings &#x663E;&#x793A;&#x51FA;&#x4E86;&#x663E;&#x8457;&#x7684;&#x6027;&#x80FD;&#x4F18;&#x52BF;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%8A%A9%E5%8A%9B%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A%E6%8B%93%E5%B1%95%E5%85%A8%E7%90%83%E4%B8%9A%E5%8A%A1">&#x52A9;&#x529B;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x62D3;&#x5C55;&#x5168;&#x7403;&#x4E1A;&#x52A1;</h3><p>&#x6211;&#x4EEC;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B; jina-embeddings-v2-base-zh &#x662F;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x8FDB;&#x519B;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7684;&#x5F3A;&#x5927;&#x4F19;&#x4F34;&#x3002;&#x5B83;&#x80FD;&#x591F;&#x65E0;&#x7F1D;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#xFF0C;&#x5E76;&#x63D0;&#x4F9B;&#x9AD8;&#x8D28;&#x91CF;&#x7684;&#x6587;&#x672C;&#x5411;&#x91CF;&#x8868;&#x793A;&#xFF0C;&#x8FD8;&#x80FD;&#x8F7B;&#x677E;&#x96C6;&#x6210;&#x5230;&#x5148;&#x8FDB;&#x7684;&#x5411;&#x91CF;&#x6570;&#x636E;&#x5E93;&#x3001;&#x641C;&#x7D22;&#x7CFB;&#x7EDF;&#x4EE5;&#x53CA; RAG &#x5E94;&#x7528;&#x91CC;&#x3002;<br>&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#x7279;&#x522B;&#x9002;&#x5408;&#x6253;&#x9020;&#x9002;&#x5E94;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x573A;&#x666F;&#x7684; AI &#x5E94;&#x7528;&#xFF0C;&#x5BF9;&#x4E8E;&#x8FFD;&#x6C42;&#x5168;&#x7403;&#x5316;&#x53D1;&#x5C55;&#x7684;&#x4F01;&#x4E1A;&#x6765;&#x8BF4;&#xFF0C;&#x5176;&#x4EF7;&#x503C;&#x4E0D;&#x53EF;&#x4F30;&#x91CF;&#x3002;&#x4EE5;&#x4E0B;&#x662F;&#x51E0;&#x4E2A;&#x5B9E;&#x9645;&#x5E94;&#x7528;&#x6848;&#x4F8B;&#xFF1A;</p><ul><li>&#x6587;&#x6863;&#x5206;&#x6790;&#x4E0E;&#x7BA1;&#x7406;&#xFF1A;&#x5206;&#x6790;&#x548C;&#x7BA1;&#x7406;&#x6D77;&#x91CF;&#x6587;&#x6863;&#xFF0C;&#x52A9;&#x529B;&#x56FD;&#x9645;&#x6CD5;&#x5F8B;&#x548C;&#x5546;&#x52A1;&#x4EA4;&#x6613;&#x7684;&#x987A;&#x5229;&#x8FDB;&#x884C;&#x3002;</li><li>AI &#x9A71;&#x52A8;&#x641C;&#x7D22;&#x5E94;&#x7528;&#xFF1A;&#x5728;&#x591A;&#x8BED;&#x8A00;&#x73AF;&#x5883;&#x4E2D;&#x63D0;&#x5347;&#x641C;&#x7D22;&#x6027;&#x80FD;&#xFF0C;&#x5E2E;&#x52A9;&#x5168;&#x7403;&#x7528;&#x6237;&#x8F7B;&#x677E;&#x627E;&#x5230;&#x4E2D;&#x82F1;&#x6587;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</li><li>&#x589E;&#x5F3A;&#x68C0;&#x7D22;&#x7684;&#x804A;&#x5929;&#x673A;&#x5668;&#x4EBA;&#x548C;&#x95EE;&#x7B54;&#x7CFB;&#x7EDF;&#xFF1A;&#x6253;&#x9020;&#x9AD8;&#x6548;&#x7684;&#x53CC;&#x8BED;&#x5BA2;&#x670D;&#x673A;&#x5668;&#x4EBA;&#xFF0C;&#x4F18;&#x5316;&#x4E0E;&#x5168;&#x7403;&#x5BA2;&#x6237;&#x7684;&#x6C9F;&#x901A;&#x4F53;&#x9A8C;&#x3002;</li><li>&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x5E94;&#x7528;&#xFF1A;&#x6DB5;&#x76D6;&#x5168;&#x7403;&#x5E02;&#x573A;&#x8D8B;&#x52BF;&#x5206;&#x6790;&#x3001;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7B56;&#x7565;&#x7684;&#x4E3B;&#x9898;&#x5EFA;&#x6A21;&#xFF0C;&#x4EE5;&#x53CA;&#x5168;&#x7403;&#x901A;&#x8BAF;&#x7BA1;&#x7406;&#x7684;&#x6587;&#x672C;&#x5206;&#x7C7B;&#x3002;</li><li>&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#xFF1A;&#x5229;&#x7528;&#x4E2D;&#x82F1;&#x6570;&#x636E;&#x6D1E;&#x5BDF;&#xFF0C;&#x4E3A;&#x5168;&#x7403;&#x591A;&#x5143;&#x5316;&#x53D7;&#x4F17;&#x63D0;&#x4F9B;&#x4E2A;&#x6027;&#x5316;&#x7684;&#x4EA7;&#x54C1;&#x548C;&#x5185;&#x5BB9;&#x63A8;&#x8350;&#x3002;</li></ul><p>&#x501F;&#x52A9;&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#xFF0C;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x80FD;&#x591F;&#x5728; AI &#x5E94;&#x7528;&#x9886;&#x57DF;&#x8DE8;&#x8D8A;&#x8BED;&#x8A00;&#x7684;&#x9E3F;&#x6C9F;&#xFF0C;&#x5728;&#x5168;&#x7403;&#x5E02;&#x573A;&#x7684;&#x89D2;&#x9010;&#x4E2D;&#x5360;&#x636E;&#x5148;&#x673A;&#x3002;</p><h3 id="%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B-jina-embeddings-v2-base-zh">&#x8F7B;&#x677E;&#x4E0A;&#x624B; jina-embeddings-v2-base-zh</h3><p>&#x60F3;&#x8981;&#x5FEB;&#x901F;&#x5C06;&#x6211;&#x4EEC;&#x7684;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x878D;&#x5165;&#x60A8;&#x7684;&#x5DE5;&#x4F5C;&#x6D41;&#x7A0B;&#xFF1F;&#x53EA;&#x9700;&#x51E0;&#x4E2A;&#x7B80;&#x5355;&#x6B65;&#x9AA4;&#xFF1A;&#x8BBF;&#x95EE; <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">https://jina.ai/embeddings</a>&#xFF0C;&#x9886;&#x53D6;&#x60A8;&#x7684;&#x514D;&#x8D39; API &#x5BC6;&#x94A5;&#x6216;&#x66F4;&#x65B0;&#x73B0;&#x6709;&#x5BC6;&#x94A5;&#xFF0C;&#x7136;&#x540E;&#x5728;&#x4E0B;&#x62C9;&#x83DC;&#x5355;&#x4E2D;&#x9009;&#x62E9; jina-embeddings-v2-base-zh&#xFF0C;&#x60A8;&#x7684;&#x6A21;&#x578B;&#x5373;&#x523B;&#x51C6;&#x5907;&#x5C31;&#x7EEA;&#xFF0C;&#x7B49;&#x5F85;&#x60A8;&#x7684;&#x63A2;&#x7D22;&#x548C;&#x4F7F;&#x7528;&#xFF01;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1138" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png 2222w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%B1%95%E6%9C%9B%E6%9C%AA%E6%9D%A5%EF%BC%9A%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%E4%B8%8E-aws-sagemaker-%E6%B7%B1%E5%BA%A6%E8%9E%8D%E5%90%88">&#x5C55;&#x671B;&#x672A;&#x6765;&#xFF1A;&#x591A;&#x8BED;&#x8A00;&#x652F;&#x6301;&#x4E0E; AWS SageMaker &#x6DF1;&#x5EA6;&#x878D;&#x5408;</h3><p>jina-embeddings-v2-base-zh &#x5373;&#x5C06;&#x4E0A;&#x7EBF; AWS SageMaker &#x548C; HuggingFace&#xFF0C;&#x4E3A;&#x7528;&#x6237;&#x63D0;&#x4F9B;&#x66F4;&#x52A0;&#x4FBF;&#x6377;&#x7684;&#x670D;&#x52A1;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>&#x6211;&#x4EEC;&#x6B63;&#x79EF;&#x6781;&#x63A8;&#x8FDB;&#x591A;&#x8BED;&#x8A00;&#x5411;&#x91CF;&#x6A21;&#x578B;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x6B27;&#x6D32;&#x53CA;&#x5176;&#x4ED6;&#x56FD;&#x9645;&#x8BED;&#x8A00;&#x7684;&#x652F;&#x6301;&#xFF0C;&#x6765;&#x6EE1;&#x8DB3;&#x5168;&#x7403;&#x7528;&#x6237;&#x7684;&#x591A;&#x6837;&#x5316;&#x9700;&#x6C42;&#x3002;&#x656C;&#x8BF7;&#x671F;&#x5F85;&#x6211;&#x4EEC;&#x5373;&#x5C06;&#x63A8;&#x51FA;&#x7684;&#x6FC0;&#x52A8;&#x4EBA;&#x5FC3;&#x7684;&#x66F4;&#x65B0;&#xFF0C;&#x5305;&#x62EC;&#x4E0E; AWS SageMaker &#x7684;&#x6DF1;&#x5EA6;&#x96C6;&#x6210;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6301;&#x7EED;&#x6DF1;&#x5316;&#x548C;&#x62D3;&#x5BBD;&#x670D;&#x52A1;&#x8303;&#x56F4;&#x3002;</p><h3 id="%E8%87%B4%E8%B0%A2%EF%BC%9A%E6%84%9F%E8%B0%A2%E6%97%A9%E6%9C%9F%E6%B5%8B%E8%AF%95%E8%80%85%E7%9A%84%E5%AE%9D%E8%B4%B5%E8%B4%A1%E7%8C%AE">&#x81F4;&#x8C22;&#xFF1A;&#x611F;&#x8C22;&#x65E9;&#x671F;&#x6D4B;&#x8BD5;&#x8005;&#x7684;&#x5B9D;&#x8D35;&#x8D21;&#x732E;</h3><p>&#x6211;&#x4EEC;&#x8877;&#x5FC3;&#x611F;&#x8C22;&#x53C2;&#x4E0E; <code>jina-embeddings-v2-base-zh-preview</code> &#x6D4B;&#x8BD5;&#x7684;&#x4E2D;&#x56FD;&#x793E;&#x533A;&#x670B;&#x53CB;&#x4EEC;&#x3002;&#x4F60;&#x4EEC;&#x7684;&#x5B9D;&#x8D35;&#x610F;&#x89C1;&#x5BF9;&#x4F18;&#x5316;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x8D77;&#x5230;&#x4E86;&#x91CD;&#x8981;&#x4F5C;&#x7528;&#x3002;&#x5982;&#x679C;&#x60A8;&#x5728;&#x4F7F;&#x7528;&#x8FC7;&#x7A0B;&#x4E2D;&#x6709;&#x4EFB;&#x4F55;&#x5EFA;&#x8BAE;&#x6216;&#x60F3;&#x6CD5;&#xFF0C;&#x6B22;&#x8FCE;&#x968F;&#x65F6;&#x5411;&#x6211;&#x4EEC;&#x63D0;&#x51FA;&#x3002;&#x60A8;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x53CD;&#x9988;&#x90FD;&#x662F;&#x6211;&#x4EEC;&#x6301;&#x7EED;&#x8FDB;&#x6B65;&#x7684;&#x52A8;&#x529B;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">&#x6B63;&#x5F0F;&#x7248;&#x89E3;&#x51B3;&#x4E86;&#x9884;&#x89C8;&#x7248;&#x7684;&#x5206;&#x6570;&#x81A8;&#x80C0;&#x95EE;&#x9898;</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">&#x4E0E;&#x4E4B;&#x524D;&#x7684;&#x9884;&#x89C8;&#x7248;&#x6A21;&#x578B;&#x76F8;&#x6BD4;&#xFF0C;&#x6B63;&#x5F0F;&#x7248;&#x6A21;&#x578B;&#x63D0;&#x4F9B;&#x4E86;&#x66F4;&#x52A0;&#x5206;&#x6563;&#x4E14;&#x5408;&#x7406;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x3002;&#x5728;&#x9884;&#x89C8;&#x7248;&#x7684;&#x6D4B;&#x8BD5;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x66FE;&#x663E;&#x793A;&#x51FA;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x7684;&#x901A;&#x8D27;&#x81A8;&#x80C0;&#x73B0;&#x8C61;&#xFF0C;&#x5373;&#x4FBF;&#x662F;&#x5B8C;&#x5168;&#x4E0D;&#x76F8;&#x5173;&#x7684;&#x8BCD;&#x6C47;&#xFF0C;&#x6BD4;&#x5982;&#x2018;&#x5B89;&#x59AE;&#x2019;&#x548C;&#x2018;&#x84B8;&#x6C7D;&#x673A;&#x2019;&#xFF0C;&#x4E5F;&#x4F1A;&#x83B7;&#x5F97;&#x5F88;&#x9AD8;&#x7684;&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;&#x800C;&#x5728;&#x6B63;&#x5F0F;&#x7248;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4F18;&#x5316;&#x4E86;&#x6A21;&#x578B;&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x66F4;&#x4E3A;&#x5408;&#x7406;&#xFF0C;&#x4ECE;&#x800C;&#x66F4;&#x51C6;&#x786E;&#x5730;&#x53CD;&#x6620;&#x5185;&#x5BB9;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x3002;</span></p><p><span style="white-space: pre-wrap;">&#x6B64;&#x5916;&#xFF0C;Jina Embeddings &#x73B0;&#x5728;&#x652F;&#x6301;&#x9AD8;&#x8FBE; 8192 Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x957F;&#x7BC7;&#x5927;&#x8BBA;&#x8FD8;&#x662F;&#x7B80;&#x77ED;&#x8BED;&#x53E5;&#xFF0C;&#x751A;&#x81F3;&#x662F;&#x5355;&#x4E2A;&#x8BCD;&#x6C47;&#x6216;&#x540D;&#x5B57;&#xFF08;&#x5982;&#x201C;&#x5B89;&#x59AE;&#x201D;&#x4E0E;&#x201C;&#x9732;&#x5A1C;&#x201D;&#x7684;&#x6BD4;&#x8F83;&#xFF09;&#xFF0C;&#x90FD;&#x80FD;&#x5C55;&#x73B0;&#x51FA;&#x5176;&#x5904;&#x7406;&#x5404;&#x79CD;&#x7C7B;&#x578B;&#x6570;&#x636E;&#x7684;&#x5F3A;&#x5927;&#x80FD;&#x529B;&#x3002;&#x8FD9;&#x4E00;&#x6539;&#x8FDB;&#x4E0D;&#x4EC5;&#x63D0;&#x5347;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x51C6;&#x786E;&#x6027;&#xFF0C;&#x4E5F;&#x589E;&#x5F3A;&#x4E86;&#x5176;&#x5728;&#x5904;&#x7406;&#x591A;&#x6837;&#x5316;&#x6570;&#x636E;&#x65F6;&#x7684;&#x7075;&#x6D3B;&#x6027;&#x548C;&#x5B9E;&#x7528;&#x6027;&#x3002;</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure>]]></content:encoded></item><item><title><![CDATA[The 1950-2024 Text Embeddings Evolution Poster]]></title><description><![CDATA[Take part in celebrating the achievements of text embeddings and carry a piece of its legacy with you.]]></description><link>https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster/</link><guid isPermaLink="false">659d02a00bab3100012d2bff</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 11:07:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" alt="The 1950-2024 Text Embeddings Evolution Poster"><p>Embark on a journey through time with our latest infographic poster, showcasing the remarkable evolution of text embeddings from 1950 to 2024. This visually striking piece is not just a testament to technological progress; it&apos;s a guide that traces the lineage of innovations that have revolutionized how we represent text data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png" class="kg-image" alt="The 1950-2024 Text Embeddings Evolution Poster" loading="lazy" width="1414" height="2000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Latest-version2.0--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Latest-version2.0--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png 1414w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">The Evolution of Text Embeddings&#xA0;</span></a><span style="white-space: pre-wrap;">&#xA9; 2024&#xA0;by&#xA0;</span><a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">Jina AI&#xA0;</span></a><span style="white-space: pre-wrap;">is licensed under&#xA0;</span><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer"><span style="white-space: pre-wrap;">CC BY-NC-ND 4.0&#xA0;</span></a></figcaption></figure><h2 id="a-chronicle-of-innovation">A Chronicle of Innovation</h2><p>From the foundational Bag of Words model to the 8K token-length <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">jina-embeddings-v2</a> and everything in between, our poster captures each step in advancing text embeddings. Designed with precision, it highlights how each breakthrough built upon its predecessors, leading us to today&apos;s embeddings-driven applications.</p><h2 id="designed-for-enthusiasts-and-professionals-alike">Designed for Enthusiasts and Professionals Alike</h2><p>Whether you&apos;re a data scientist, a software engineer, an AI researcher, or simply a technology enthusiast, this poster is a must-have. It&apos;s more than just a visual treat; it&apos;s an educational tool that breaks down complex developments into an accessible format.</p><h2 id="bring-home-a-piece-of-ai-history">Bring Home a Piece of AI History</h2><p>For those who appreciate the tactile feel of quality, you can own this piece of history by purchasing a hard copy. Imagine this stunning poster adorning your living room wall, sparking conversations and inspiring future innovators.</p>
<!--kg-card-begin: html-->
<script async src="https://js.stripe.com/v3/buy-button.js">
</script>

<div style="text-align: center">
<stripe-buy-button buy-button-id="buy_btn_1OWchyEoTOtb5eZv69zXthH9" publishable-key="pk_live_51MD1n2EoTOtb5eZvVgdn5IcOR7BNFFcJTmb7hcAihzLOZpif376WqhFUTpN3sQHb7ADxQLtKxXvC7YAttivYrSDa00PJqFn7qz">
</stripe-buy-button>
</div>
<!--kg-card-end: html-->
<figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png 1024w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="accessible-in-multiple-formats">Accessible in Multiple Formats</h2><p>Not ready for a physical copy? No problem. We offer a downloadable PNG or PDF version, ensuring you can access this wealth of information in the format that best suits your needs.</p><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-v2.png" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PNG) Download the Evolution of Text Embeddings (834KB)</div><div class="kg-file-card-caption">Best for display and sharing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-v2.png</div><div class="kg-file-card-filesize">814 KB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-cmyk-color.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Print, CMYK) Download the Evolution of Text Embeddings (7.1MB)</div><div class="kg-file-card-caption">Best for printing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-cmyk-color.pdf</div><div class="kg-file-card-filesize">7 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-standard.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Standard) Download the Evolution of Text Embeddings (2.8MB)</div><div class="kg-file-card-caption">Best for viewing on the screen</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-standard.pdf</div><div class="kg-file-card-filesize">3 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><h2 id="references-at-your-fingertips">References at Your Fingertips</h2><p>Accompanying our infographic, we provide an extensive list of references, corresponding to each milestone depicted. This curated collection allows you to delve deeper into each technology, understanding the intricacies and applications that have shaped the field of natural language processing.</p><blockquote>TF-IDF	1972	K.S. Jones, A statistical interpretation of term specificity and its application in retrieval, J. Doc. 28 (1972) 11&#x2013;21.</blockquote><blockquote>TF-IDF	1973	K.S. Jones, Index term weighting, Inf. Storage Retr. 9 (11) (1973) 619&#x2013;633.</blockquote><blockquote>Bag of Words	1981	Z.S. Harris, Distributional structure, in: Papers on Syntax, Springer, 1981, pp. 3&#x2013;22.</blockquote><blockquote>BoN-Grams	1994	W. Cavnar, W.B. Cavnar, J.M. Trenkle, N-gram-based text categorization, in: Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR-94), 1994, pp. 161&#x2013;175.</blockquote><blockquote>doc2vec	2014	Q. Le, T. Mikolov, Distributed representations of sentences and documents, in: Proceedings of the 31st International Conference on International Conference on Machine Learning (ICML) - Volume 32, ICML &#x2019;14, JMLR.org, 2014, pp. II&#x2013;1188&#x2013;II&#x2013;1196.</blockquote><blockquote>DAN	2015	M. Iyyer, V. Manjunatha, J. Boyd-Graber, H. Daum&#xE9; III, Deep unordered composition rivals syntactic methods for text classification, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Beijing, China, 2015, pp. 1681&#x2013;1691.</blockquote><blockquote>RCNN	2015	S. Lai, L. Xu, K. Liu, J. Zhao, Recurrent convolutional neural networks for text classification, in: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI &#x2019;15, AAAI Press, 2015, pp. 2267&#x2013;2273.</blockquote><blockquote>RNNs	2015	D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, in: International Conference on Learning Representations (ICLR) 2015, 2014.</blockquote><blockquote>Skip-Thought	2015	R. Kiros, Y. Zhu, R.R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, S. Fidler, Skip-thought vectors, in: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 28, Curran Associates, Inc., 2015, pp. 3294&#x2013;3302.</blockquote><blockquote>DESM	2016	E. Nalisnick, B. Mitra, N. Craswell, R. Caruana, Improving document rank- ing with dual word embeddings, in: Proceedings of the 25th International Conference Companion on World Wide Web, 2016, pp. 83&#x2013;84.</blockquote><blockquote>DV-ngram	2016	B. Li, T. Liu, X. Du, D. Zhang, Z. Zhao, Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews, in: Workshop Contribution at International Conference on Learning Representations (ICLR) 2016, 2016.</blockquote><blockquote>FastSent	2016	F. Hill, K. Cho, A. Korhonen, Learning distributed representations of sentences from unlabelled data, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics (ACL), San Diego, California, 2016, pp. 1367&#x2013;1377.</blockquote><blockquote>HAN	2016	Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. Hovy, Hierarchical attention networks for document classification, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, San Diego, California, 2016, pp. 1480&#x2013;1489.</blockquote><blockquote>NVDM	2016	Y. Miao, L. Yu, P. Blunsom, Neural variational inference for text processing, in: Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML) - Volume 48, ICML &#x2019;16, JMLR.org, 2016, pp. 1727&#x2013;1736.</blockquote><blockquote>Siamese CBoW	2016	T. Kenter, A. Borisov, M. de Rijke, Siamese CBOW: Optimizing word embed- dings for sentence representations, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics (ACL), Berlin, Germany, 2016, pp. 941&#x2013;951.</blockquote><blockquote>CNN-LSTM	2017	Z. Gan, Y. Pu, R. Henao, C. Li, X. He, L. Carin, Learning generic sentence representations using convolutional neural networks, in: Empirical Methods in Natural Language Processing, EMNLP, 2017, pp. 2390&#x2013;2400.</blockquote><blockquote>CNNs	2017	Y. Zhang, D. Shen, G. Wang, Z. Gan, R. Henao, L. Carin, Deconvolutional paragraph representation learning, in: I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran Associates, Inc., 2017, pp. 5438&#x2013;5445.</blockquote><blockquote>CNNs	2017	Z. Zhu, J. Hu, Context aware document embedding, 2017, arXiv:1707.01521.</blockquote><blockquote>Doc2VecC	2017	M. Chen, Efficient vector representation for documents through corruption, in: International Conference on Learning Representations, ICLR, 2017.</blockquote><blockquote>DiSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, C. Zhang, DiSAN: Directional self- attention network for RNN/CNN-free language understanding, in: AAAI, 2018, pp. 5446&#x2013;5455.</blockquote><blockquote>ELMo	2018	M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettle- moyer, Deep contextualized word representations, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Associa- tion for Computational Linguistics (ACL), New Orleans, Louisiana, 2018, pp. 2227&#x2013;2237.</blockquote><blockquote>GPT-2	2018	A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners, OpenAI Blog (2018).</blockquote><blockquote>ReSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, C. Zhang, Reinforced self-attention network: A hybrid of hard and soft attention for sequence modeling, in: Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI &#x2019;18, AAAI Press, 2018, pp. 4345&#x2013;4352.</blockquote><blockquote>Sent2vec	2018	M. Pagliardini, P. Gupta, M. Jaggi, Unsupervised learning of sentence embed- dings using compositional n-gram features, in: Proceedings of North American Chapter of the Association for Computational Linguistics NAACL-HLT, 2018, pp. 528&#x2013;540.</blockquote><blockquote>BART	2019	Lewis, Mike, et al. &quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.&quot; arXiv preprint arXiv:1910.13461 (2019).</blockquote><blockquote>BERT	2019	J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidi- rectional transformers for language understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 4171&#x2013;4186.</blockquote><blockquote>DistilBERT	2019	V. Sanh, L. Debut, J. Chaumond, T. Wolf, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, in: 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing at NeurIPS 2019, 2019.</blockquote><blockquote>DocBERT	2019	A. Adhikari, A. Ram, R. Tang, J. Lin, DocBERT: BERT for document classification, 2019, ArXiv abs/1904.08398.</blockquote><blockquote>LASER	2019	M. Artetxe, H. Schwenk, Massively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond, Trans. Assoc. Comput. Linguist. 7 (2019) 597&#x2013;610.</blockquote><blockquote>MASS	2019	K. Song, X. Tan, T. Qin, J. Lu, T. Liu, MASS: Masked sequence to sequence pre-training for language generation, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, in: Proceedings of Machine Learning Research, vol. 97, PMLR, 2019, pp. 5926&#x2013;5936.</blockquote><blockquote>SBERT	2019	N. Reimers, I. Gurevych, Sentence-BERT: Sentence embeddings using siamese BERT-networks, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 3982&#x2013;3992.</blockquote><blockquote>Transformer-XL	2019	Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, R. Salakhutdinov, Transformer- XL: Attentive language models beyond a fixed-length context, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL, Association for Computational Linguistics, Florence, Italy, 2019, pp. 2978&#x2013;2988.</blockquote><blockquote>VLAWE	2019	R.T. Ionescu, A. Butnaru, Vector of locally-aggregated word embeddings (VLAWE): A novel document-level representation, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 363&#x2013;369.</blockquote><blockquote>XLM	2019	A. Conneau, G. Lample, Cross-lingual language model pretraining, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 7059&#x2013;7069.</blockquote><blockquote>XLNet	2019	Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R.R. Salakhutdinov, Q.V. Le, XLNet: Generalized autoregressive pretraining for language understanding, in: H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 5753&#x2013;5763.</blockquote><blockquote>ALBERT	2020	Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, in: International Conference on Learning Representations, ICLR, OpenReview.net, 2020.</blockquote><blockquote>ELECTRA	2020	Clark, Kevin, et al. &quot;Electra: Pre-training text encoders as discriminators rather than generators.&quot; arXiv preprint arXiv:2003.10555 (2020).</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Saw, P. Nokhiz, P. Netrapalli, P. Rai, P. Talukdar, P-SIF: Document embeddings using partition averaging, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020, pp. 7863&#x2013;7870.</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Kumar, P. Nokhiz, H. Gupta, P. Talukdar, Improving docu- ment classification with multi-sense embeddings, in: European Conference on Artificial Intelligence (ECAI) 2020, IOS Press, 2020, pp. 2030&#x2013;2037.</blockquote><blockquote>RoBERTa	2020	Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L.Zettlemoyer, V. Stoyanov, RoBERTa: A robustly optimized BERT pretrainingapproach, in: Under Review as a Conference Paper at International Conference on Learning Representations (ICLR) 2020, 2020.</blockquote><blockquote>SpanBERT	2020	M. Joshi, D. Chen, Y. Liu, D. Weld, L. Zettlemoyer, O. Levy, SpanBERT: Improving pre-training by representing and predicting spans, Trans. Assoc. Comput. Linguist. 8 (2020).</blockquote><blockquote>SimCSE	2021	Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894&#x2013;6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552.</blockquote><blockquote>AugCSE	2022	Tang, Zilu, Muhammed Yusuf Kocyigit, and Derry Wijaya. &quot;Augcse: Contrastive sentence embedding with diverse augmentations.&quot; arXiv preprint arXiv:2210.13749 (2022).</blockquote><blockquote>DiffCSE	2022	Oh, Dongsuk, et al. &quot;Don&apos;t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling.&quot; arXiv preprint arXiv:2209.05972 (2022).</blockquote><blockquote>SGPT	2022	Muennighoff, Niklas. &quot;Sgpt: Gpt sentence embeddings for semantic search.&quot; arXiv preprint arXiv:2202.08904 (2022).</blockquote><blockquote>bge	2023	C-Pack: Packaged Resources To Advance General Chinese Embedding</blockquote><blockquote>embeddings-v2	2023	G&#xFC;nther, Michael, et al. &quot;Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.&quot; arXiv preprint arXiv:2310.19923 (2023).</blockquote>]]></content:encoded></item></channel></rss>
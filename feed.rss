<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.76</generator><lastBuildDate>Fri, 26 Jan 2024 12:28:54 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Making Accessibility Accessible: Create Alt Text with SceneXplain's API]]></title><description><![CDATA[SceneXplain is your accessibility ally, making it easy to generate image alt texts to aid visually-impaired users and improve SEO]]></description><link>https://jina.ai/news/make-accessibility-accessible-generate-alt-text-with-scenexplain/</link><guid isPermaLink="false">65af909b8da8040001e16fbb</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 23 Jan 2024 15:00:18 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><p>Accessibility (or &quot;a11y&quot; for short) is fast becoming an important part of web development and e-commerce. Back in the day, accessibility aids like <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">alt text</a> or color-blind-friendly color schemes weren&apos;t seen as high priorities by developers and companies. But now, with accessibility legislation from <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Europe</a> and the <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">USA</a>, making your website accessible is more important than ever before.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Alt text, or alternative text, is a brief description of an image used on websites and in digital documents. It helps people who can&apos;t see the image understand what it&apos;s about. This includes people using screen readers because of visual impairments and those with slow internet connections where images don&apos;t load. Alt text is also useful for search engines to understand and index images.</div></div><p>But how can you go about creating alt text for every image on your website? Manually going through each image and writing alt text could take forever, especially if you have thousands (or millions) of images. And if more are being added every day, it becomes a never-ending battle.</p><p>That&#x2019;s where SceneXplain comes in. It&#x2019;s your a11y ally! You can simply upload an image and get alt text for it without having to wrack your brain thinking of the wording yourself.</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Generate_Alt_Text_with_SceneXplain__C5Wt_1HXQo-LONuk8GAqfA?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<p>If you have, say, a few dozen images, this is a good way to give your brain a break. But you still need to do all the clicking and dragging yourself. Your brain wins, but your fingers don&apos;t. And if you have a few thousand images? Call the doctor now to pre-book your <a href="https://en.wikipedia.org/wiki/Repetitive_strain_injury?ref=jina-ai-gmbh.ghost.io">carpal tunnel</a> appointment.</p><p>If only there were a way you could automate the whole thing. Then your brain and fingers could <em>both</em> focus on more interesting things.</p><p>That&apos;s where SceneXplain&apos;s API comes in. You can write a script that will go through your thousands of images, send them in batches to SceneXplain, and generate a CSV file with the results (or with a bit more coding, integrate directly into your workflow.)</p><p>After all, you know what they say. You can&apos;t spell happiness without APIness.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Upon further reflection, I have found that the English language does not, in fact, work like that.</div></div><h2 id="what-is-an-api">What is an API?</h2><p>But before we dive into the <em>how</em>, let&apos;s look at the <em>what</em>. The <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Oxford English Dictionary</a> defines API as:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="927" height="209" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--50-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png 927w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Well, that&#x2019;s not useful at all</span></figcaption></figure><p>However, everyone&apos;s favorite AI, GPT-4 defines an API as:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="789" height="664" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--51-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png 789w" sizes="(min-width: 720px) 720px"></figure><p>Or, if you prefer a video explanation:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/s7wmiS2mSXY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="What is an API?"></iframe></figure><p>In short, you can write a Python (or any other language) program to talk to SceneXplain via its API and automate your whole alt-tagging process. We have a Python snippet that does just that.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Can&#x2019;t run the code on your own computer? Keep reading down to the Google Colab notebook which lets you use it in your browser.</div></div><p>Here&#x2019;s how you use it:</p><ol><li>Install the <a href="https://docs.python-requests.org/en/master/index.html?ref=jina-ai-gmbh.ghost.io">requests</a> library:</li></ol><pre><code class="language-bash">pip install requests
</code></pre><ol><li>Go to SceneXplain&#x2019;s API page to generate a secret key and copy it to your clipboard.</li><li>Paste it into the Python code below.</li><li>Copy an image URL into the code where it says <code>....</code>.</li><li>Run the code!</li></ol><pre><code class="language-python">import requests
import json

# generate token on SceneXplain&apos;s API page
YOUR_GENERATED_SECRET = &quot;your_generated_secret_here&quot;

data = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot; # change to image URL
    }
  ]
}

headers = {
  &quot;x-api-key&quot;: f&quot;token {YOUR_GENERATED_SECRET}&quot;,
  &quot;content-type&quot;: &quot;application/json&quot;,
}

response = requests.post(&quot;https://api.scenex.jina.ai/v1/describe&quot;, headers=headers, json=data)
print(response.json())
</code></pre><p>(We&#x2019;ll put in more code snippets later for cURL and JavaScript)</p><h2 id="api-in-action-scenexplain-in-a-notebook">API in action: SceneXplain in a Notebook</h2><p>Since we want to see this in action, we&#x2019;ll use the code live in a <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io">notebook</a>. That lets you see what&#x2019;s happening in real time with real data, and lets you examine and play with the Python code yourself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/f6c736cbde16632a9fcd16d9ab1970b1/img/favicon.ico" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure><p>The notebook goes beyond just the simple Python snippet above. It also downloads a sample dataset and exports the results to a CSV file.</p><h2 id="beyond-the-notebook-using-the-api-irl">Beyond the notebook: Using the API IRL</h2><p>Of course, you&#x2019;re not limited to Python when you use SceneXplain&#x2019;s API. Any language that has an HTTP library should work fine.</p><p>Here&#x2019;s that same code snippet from above, this time in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
};

const YOUR_GENERATED_SECRET = &apos;your_generated_secret_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_GENERATED_SECRET}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
}).then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>And this time as a <a href="https://curl.se/?ref=jina-ai-gmbh.ghost.io">cURL</a> command:</p><pre><code class="language-shell">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
}&apos;</code></pre><h2 id="improve-your-image-accessibility-with-scenexplain%E2%80%99s-api">Improve your image accessibility with SceneXplain&#x2019;s API</h2><p>To get started, head over to <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API page</a> to brush up on how it all works, generate a secret key, and then either adapt our notebook or create your own code to start improving accessibility today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?]]></title><description><![CDATA[Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ]]></description><link>https://jina.ai/news/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/</link><guid isPermaLink="false">65af98d28da8040001e17008</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 23 Jan 2024 11:22:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">On Jan. 25, 2024, OpenAI released <a href="https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io">a new embedding model</a> with a new feature called <i><b><strong class="italic" style="white-space: pre-wrap;">&quot;shortening&quot;</strong></b></i>, which allows developers to trim embeddings&#x2014;essentially cutting numbers from the sequence&apos;s end&#x2014;without compromising the embedding&apos;s ability to represent concepts effectively. Dive into this post for a solid theoretical foundation on the viability and rationale behind this innovation.</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><p>Consider this: when measuring the cosine similarity of embedding vectors in high-dimensional spaces, how does their similarity in lower-dimensional subspaces imply the overall similarity? Is there a direct, proportional relationship, or is the reality more complex with high-dimensional data?</p><p>More concretely, <strong>does high similarity between vectors in their first 256 dimensions assure a high similarity in their full 768 dimensions?</strong> Conversely, if vectors significantly differ in some dimensions, does this spell a low overall similarity? These aren&apos;t mere theoretical musings; they are crucial considerations for efficient vector retrieval, database indexing, and the performance of RAG systems.</p><p>Developers often rely on heuristics, assuming that high subspace similarity equates to high overall similarity or that notable differences in one dimension significantly affect the overall similarity. The question is: are these heuristic methods built on firm theoretical ground, or are they simply assumptions of convenience?</p><p>This post delves into these questions, examining the theory and practical implications of subspace similarity in relation to overall vector similarity. </p><h2 id="bounding-the-cosine-similarity">Bounding the Cosine Similarity</h2><p>Given vectors $\mathbf{A}, \mathbf{B}\in \mathbb{R}^d$, we decompose them as $\mathbf{A}=[\mathbf{A}_1, \mathbf{A}_2]$ and $\mathbf{B}=[\mathbf{B}_1, \mathbf{B}_2]$, where $\mathbf{A}_1,\mathbf{B}_1\in\mathbb{R}^m$ and $\mathbf{A}_2,\mathbf{B}_2\in\mathbb{R}^n$, with $m+n=d$.</p><p>The cosine similarity in the subspace $\mathbb{R}^m$ is given by $\cos(\mathbf{A}_1, \mathbf{B}_1)=\frac{\mathbf{A}_1\cdot\mathbf{B}_1}{\|\mathbf{A}_1\|\|\mathbf{B}_1\|}$; similarly, the similarity in the subspace $\mathbb{R}^n$ is $\cos(\mathbf{A}_2, \mathbf{B}_2)=\frac{\mathbf{A}_2\cdot\mathbf{B}_2}{\|\mathbf{A}_2\|\|\mathbf{B}_2\|}$.</p><p>In the original space $\mathbb{R}^d$, the cosine similarity is defined as:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}\\&amp;=\frac{\mathbf{A}_1\cdot\mathbf{B}_1+\mathbf{A}_2\cdot\mathbf{B}_2}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\cos(\mathbf{A}_1, \mathbf{B}_1)\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\cos(\mathbf{A}_2, \mathbf{B}_2)\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\end{align*}$$</p><p>Now, let $s := \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$. Then, we have:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;\leq\frac{s\|\mathbf{A}_1\|\|\mathbf{B}_1\|+s\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\cdot s\\&amp;=\cos(\underbrace{[\|\mathbf{A}_1\|, \|\mathbf{A}_2\|]}_{\mathbb{R}^2}, \underbrace{[\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]}_{\mathbb{R}^2})\cdot s\\&amp;\leq 1\cdot s \\&amp;= \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}$$</p><p>End of proof. </p><p>Note that in the final step of the proof, we leverage that the cosine similarity is always less than or equal to 1. This forms our upper bound. Similarly, we can show that the lower bound of \(\cos(\mathbf{A},\mathbf{B})\) is given by: </p><p>\[ \cos(\mathbf{A},\mathbf{B}) \geq t \cdot \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) \], where $t:= \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$.</p><p>Note that for the lower bound, we can not hastily conclude that \(\cos(\mathbf{A},\mathbf{B}) \geq t\). This is because of the range of the cosine function, which spans between \([-1, 1]\). Due to this range, it&apos;s impossible to establish a tighter lower bound than the trivial value of -1.</p><p>So in conclusion, we have the following loose bound: $$ -1\leq\cos(\mathbf{A},\mathbf{B})\leq\max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)).$$ and a tighter bound \[\begin{align*}  \gamma \cdot t\leq&amp;\cos(\mathbf{A}, \mathbf{B}) \leq\gamma\cdot s\\\gamma \cdot \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)) \leq &amp;\cos(\mathbf{A}, \mathbf{B}) \leq \gamma \cdot \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}\], where $\gamma = \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) $.</p><h3 id="connection-to-johnson%E2%80%93lindenstrauss-lemma">Connection to Johnson&#x2013;Lindenstrauss Lemma</h3><p>The JL lemma asserts that for any \(0 &lt; \epsilon &lt; 1\) and any finite set of points \( S \) in \( \mathbb{R}^d \), there exists a mapping \( f: \mathbb{R}^d \rightarrow \mathbb{R}^k \) (with \( k = O(\epsilon^{-2} \log |S|) \)) such that for all \( \mathbf{u}, \mathbf{v} \in S \), the Euclidean distances are approximately preserved:<br><br>\[(1 - \epsilon) \|\mathbf{u} - \mathbf{v}\|^2 \leq \|f(\mathbf{u}) - f(\mathbf{v})\|^2 \leq (1 + \epsilon) \|\mathbf{u} - \mathbf{v}\|^2\]</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Johnson&#x2013;Lindenstrauss lemma - Wikipedia</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://en.wikipedia.org/static/apple-touch/wikipedia.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><span class="kg-bookmark-author">Wikimedia Foundation, Inc.</span><span class="kg-bookmark-publisher">Contributors to Wikimedia projects</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"></div></a></figure><p>To make $f$ work like a subspace selection, we can use a diagonal matrix for projection, such as a \(5 \times 3\) matrix \(f\), albeit not random (note, the typical formulation of the JL lemma involves linear transformations that often utilize random matrices drawn from a Gaussian distribution). For instance, if we aim to retain the 1st, 3rd, and 5th dimensions from a 5-dimensional vector space, the matrix \(f\) could be designed as follows: \[f = \begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1\end{bmatrix}\]<br>However, by specifying $f$ to be diagonal, we limit the class of functions that can be used for the projection. The JL lemma guarantees the existence of a suitable $f$ within the broader class of linear transformations, but when we restrict $f$ to be diagonal, such a suitable $f$ may not exist within this restricted class for applying the JL lemma&apos;s bounds.</p><h2 id="validating-the-bounds">Validating the Bounds</h2><p>To empirically explore the theoretical bounds on cosine similarity in high-dimensional vector spaces, we can employ a Monte Carlo simulation. This method allows us to generate a large number of random vector pairs, compute their similarities in both the original space and subspaces, and then assess how well the theoretical upper and lower bounds hold in practice.</p><p>The following Python code snippet implements this concept. It randomly generates pairs of vectors in a high-dimensional space and computes their cosine similarity. Then, it divides each vector into two subspaces, calculates the cosine similarity within each subspace, and evaluates the upper and lower bounds of the full-dimensional cosine similarity based on the subspace similarities.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import numpy as np


def compute_cosine_similarity(U, V):
    # Normalize the rows to unit vectors
    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)
    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)
    # Compute pairwise cosine similarity
    return np.sum(U_norm * V_norm, axis=1)


# Generate random data
num_points = 5000
d = 1024
A = np.random.random([num_points, d])
B = np.random.random([num_points, d])

# Compute cosine similarity between A and B
cos_sim = compute_cosine_similarity(A, B)

# randomly divide A and B into subspaces
m = np.random.randint(1, d)
A1 = A[:, :m]
A2 = A[:, m:]
B1 = B[:, :m]
B2 = B[:, m:]

# Compute cosine similarity in subspaces
cos_sim1 = compute_cosine_similarity(A1, B1)
cos_sim2 = compute_cosine_similarity(A2, B2)

# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2
s = np.maximum(cos_sim1, cos_sim2)
t = np.minimum(cos_sim1, cos_sim2)

norm_A1 = np.linalg.norm(A1, axis=1)
norm_A2 = np.linalg.norm(A2, axis=1)
norm_B1 = np.linalg.norm(B1, axis=1)
norm_B2 = np.linalg.norm(B2, axis=1)

# Form new vectors in R^2 from the norms
norm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)
norm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)

# Compute cosine similarity in R^2
gamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)

# print some info and validate the lower bound and upper bound
print(&apos;d: %d\n&apos;
      &apos;m: %d\n&apos;
      &apos;n: %d\n&apos;
      &apos;avg. cosine(A,B): %f\n&apos;
      &apos;avg. upper bound: %f\n&apos;
      &apos;avg. lower bound: %f\n&apos;
      &apos;lower bound satisfied: %s\n&apos;
      &apos;upper bound satisfied: %s&apos; % (
          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),
          np.all(gamma * t &lt;= cos_sim)))
</code></pre><figcaption><p><span style="white-space: pre-wrap;">A Monte Carlo validator for validating cosine similarity bounds</span></p></figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-output">d: 1024
m: 743
n: 281
avg. cosine(A,B): 0.750096
avg. upper bound: 0.759080
avg. lower bound: 0.741200
lower bound satisfied: True
upper bound satisfied: True</code></pre><figcaption><p><span style="white-space: pre-wrap;">A sample output from our Monte Carlo validator. It&apos;s important to note that the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>lower/upper bound satisfied</span></code><span style="white-space: pre-wrap;"> condition is checked for every vector individually. Meanwhile, the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>avg. lower/upper bound</span></code><span style="white-space: pre-wrap;"> provides a more intuitive overview of the statistics related to these bounds but doesn&apos;t directly influence the validation process.</span></p></figcaption></figure><h2 id="understanding-the-bounds">Understanding the Bounds</h2><p>In a nutshell, when comparing two high-dimensional vectors, the overall similarity lies between the best and worst similarities of their subspaces, adjusted for how large or important those subspaces are in the overall scheme. This is what the bounds for cosine similarity in higher dimensions intuitively represent: the balance between the most and least similar parts, weighted by their relative sizes or importance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png" class="kg-image" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?" loading="lazy" width="1200" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Each pen has two main components: the body and the cap.</span></figcaption></figure><p>Imagine you&apos;re trying to compare two multi-part objects (let&apos;s say, two fancy pens) based on their overall similarity. Each pen has two main components: the body and the cap. The similarity of the whole pen (both body and cap) is what we&apos;re trying to determine:</p><h3 id="upper-bound-gamma-cdot-s">Upper Bound ($\gamma \cdot s$)</h3><p>Think of $s$ as the best match between corresponding parts of the pens. If the caps are very similar but the bodies aren&apos;t, $s$ is the similarity of the caps.</p><p>Now, $\gamma$ is like a scaling factor based on the size (or importance) of each part. If one pen has a very long body and a short cap, while the other has a short body and a long cap, $\gamma$ adjusts the overall similarity to account for these differences in proportions.</p><p>The upper bound tells us that no matter how similar some parts are, the overall similarity can&apos;t exceed this &quot;best part similarity&quot; scaled by the proportion factor.</p><h3 id="lower-bound-gamma-cdot-t">Lower Bound ($\gamma \cdot t$)</h3><p>Here, $t$ is the similarity of the least matching parts. If the bodies of the pens are quite different but the caps are similar, $t$ reflects the body&apos;s similarity.</p><p>Again, $\gamma$ scales this based on the proportion of each part.</p><p>The lower bound means that the overall similarity can&apos;t be worse than this &quot;worst part similarity&quot; after accounting for the proportion of each part.</p><h2 id="implications-of-the-bounds">Implications of the Bounds</h2><p>For software engineers working with embeddings, vector search, retrieval, or databases, understanding these bounds has practical implications, particularly when dealing with high-dimensional data. Vector search often involves finding the closest (most similar) vectors in a database to a given query vector, typically using cosine similarity as a measure of closeness. The bounds we discussed can provide insights into the effectiveness and limitations of using subspace similarities for such tasks.</p><h3 id="using-subspace-similarity-for-ranking">Using Subspace Similarity for Ranking</h3><p><strong>Safety and Accuracy</strong>: Using subspace similarity for ranking and retrieving top-k results can be effective, but with caution. The upper bound indicates that the overall similarity can&apos;t exceed the maximum similarity of the subspaces. Thus, if a pair of vectors is highly similar in a particular subspace, it&apos;s a strong candidate for being similar in the high-dimensional space.</p><p><strong>Potential Pitfalls</strong>: However, the lower bound suggests that two vectors with low similarity in one subspace could still be quite similar overall. Therefore, relying solely on subspace similarity might miss some relevant results.</p><h3 id="misconceptions-and-cautions">Misconceptions and Cautions</h3><p><strong>Overestimating Subspace Importance</strong>: A common misconception is overestimating the importance of a particular subspace. While high similarity in one subspace is a good indicator, it doesn&apos;t guarantee high overall similarity due to the influence of other subspaces.</p><p><strong>Ignoring Negative Similarities</strong>: In cases where the cosine similarity in a subspace is negative, it indicates an opposing relationship in that dimension. Engineers should be wary of how these negative similarities impact the overall similarity.</p>]]></content:encoded></item><item><title><![CDATA[Using Jina Embeddings v2 with Haystack Pipelines]]></title><description><![CDATA[Access Jina AI's state-of-the-art open-source embedding models in your Haystack application pipeline.]]></description><link>https://jina.ai/news/jina-embeddings-v2-and-deepset-haystack/</link><guid isPermaLink="false">65a916c40bab3100012d2e8c</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Saahil Ognawala]]></dc:creator><pubDate>Fri, 19 Jan 2024 15:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"><p><a href="https://www.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deepset</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its industry-leading <a href="https://haystack.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Haystack</a> NLP framework. You can now access Jina AI&apos;s state-of-the-art open-source embedding models in your Haystack pipeline.</p><p>We have collaborated with Deepset to bring you a tutorial on using&#xA0;Jina Embeddings v2&#xA0;in Haystack to analyze legal documents. Follow the link below to Deepset&apos;s blog to see how you can create a retrieval-augmented generation (RAG) application using Jina AI&apos;s industry-leading embeddings and Haystack!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Using Jina Embeddings v2 with Haystack 2.0 pipelines to summarize legal documents | Haystack</div><div class="kg-bookmark-description">Learn how to use the Jina v2 Embedding models in a RAG pipeline with our new Haystack integration.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://haystack.deepset.ai/favicon.ico" alt="Using Jina Embeddings v2 with Haystack Pipelines"><span class="kg-bookmark-author">Haystack</span><span class="kg-bookmark-publisher">Tilde Thurium Senior Developer Advocate</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack/thumbnail.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length]]></title><description><![CDATA[Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.]]></description><link>https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/</link><guid isPermaLink="false">65a542040bab3100012d2d79</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 15 Jan 2024 16:28:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><p><strong>Berlin, Germany - January 15, 2023</strong>&#xA0;&#x2013; Echoing JFK&apos;s iconic &apos;Ich bin ein Berliner&apos;, at Jina AI we&apos;re thrilled to bridge languages in our own way. Today, we&apos;re proud to announce our latest innovation: <code>jina-embeddings-v2-base-de</code>, a German/English embedding model. This state-of-the-art bilingual model is a significant stride forward in language representation, boasting a context length of <strong>8,192 tokens</strong>. What sets it apart is its remarkable efficiency: it achieves top-tier performance while being <strong>only 1/7th the size</strong> of comparable models.</p><p>Embeddings are crucial for German businesses looking to expand into the U.S. market. According to the <a href="https://www.gaccny.com/en/media/press-releases/news-details/german-american-business-outlook-gabo-2022-companies-in-the-us-remain-profitable?ref=jina-ai-gmbh.ghost.io">German American Business Outlook (GABO) 2022</a>, approximately a third of German companies generate over 20% of their global sales and profits in the U.S., with 93% expecting an increase in U.S. sales&#x200B;&#x200B;. This trend continues as 93% plan to grow their company&apos;s U.S. investments in the next three years, with <a href="https://www.globenewswire.com/news-release/2023/02/09/2604561/0/en/SURVEY-SHOWS-GERMAN-COMPANIES-IN-THE-US-PROFIT-FROM-ROBUST-MARKET-SIZE-AND-CUSTOMER-DEMAND.html?ref=jina-ai-gmbh.ghost.io">85% expecting net sales growth and a significant focus on digital transformation</a>&#x200B;&#x200B;. Good embeddings can play a pivotal role in this expansion by facilitating better understanding of customer preferences, enabling more effective communication, and positioning culturally resonant products.</p><p>Our breakthrough is particularly beneficial for German businesses looking to implement bilingual applications in English-speaking countries. With <code>jina-embeddings-v2-base-de</code>, we&apos;re excited to see how German companies will innovate and thrive in an increasingly connected world.</p><h2 id="model-highlights">Model Highlights</h2><ul><li><strong>State-of-the-art Performance</strong>: <code>jina-embeddings-v2-base-de</code> consistently ranking at the top in relevant benchmarks and leading among open-source models of similar size.</li><li><strong>Bilingual Model:</strong>&#xA0;This model encodes texts in both German and English, allowing the use of either language as the query or target document in retrieval applications. Texts with equivalent meanings in both languages are mapped to the same embedding space, forming the basis for multilingual applications.</li><li><strong>Extended Context</strong>: An 8192-token length enables <code>jina-embeddings-v2-base-de</code> to support longer texts and document fragments, far surpassing models that only support a few hundred tokens at a time.</li><li><strong>Compact Size</strong>: <code>jina-embeddings-v2-base-de</code> is built for high performance on standard computer hardware. With only 161 million parameters, the entire model is 322MB and fits in the memory of commodity computers. The embeddings themselves are 768 dimensions, a relatively small vector size compared to many models, saving space and run-time for applications.</li><li><strong>Bias Minimization</strong>: <a href="https://aclanthology.org/2023.findings-eacl.89.pdf?ref=jina-ai-gmbh.ghost.io">Recent research</a> shows that multilingual models without specific language training show strong biases towards English grammatical structures in embeddings. Embedding models should be about capturing meaning and not favor sentence pairs that are merely superficially similar.</li><li><strong>Seamless Integration</strong>: Jina Embeddings v2 models have native integrations with major vector databases, including <a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a>, <a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a>, and <a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a>, as well as RAG and LLM frameworks such as <a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a> and <a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul><h2 id="leading-performance-in-german-nlp">Leading Performance in German NLP</h2><p>We&apos;ve put <code>jina-embeddings-v2-base-de</code> to the test against four renowned baselines that also support both German and English. These include:</p><ul><li><a href="https://huggingface.co/intfloat/multilingual-e5-large?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-large</a> and <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-base</a> from Microsoft</li><li>T-Systems&#x2019; <a href="https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer?ref=jina-ai-gmbh.ghost.io">Cross English &amp; German RoBERTa for Sentence Embeddings</a></li><li><a href="https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2?ref=jina-ai-gmbh.ghost.io">Sentence-BERT</a> (<code>distiluse-base-multilingual-cased-v2</code>)</li></ul><p>Our benchmarks include <a href="https://huggingface.co/mteb?ref=jina-ai-gmbh.ghost.io">the MTEB tasks for English</a> and our own custom benchmark. Given the lack of a comprehensive benchmark suite for German embeddings, we took the initiative to <a href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io">develop our own</a>, inspired by the MTEB. We&apos;re proud to share our findings and breakthroughs with you here.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-de: MTEB: Massive Text Embedding Benchmark</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark. Contribute to jina-ai/mteb-de development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/a1d1679b83d58c1685db0d4d12d6bb6360edfe08fb0c4a9ebda71b57379853f3/jina-ai/mteb-de" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1310" height="925" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png 1310w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="compact-size-superior-results">Compact Size, Superior Results</h3><p><code>jina-embeddings-v2-base-de</code> demonstrates exceptional performance, especially in German language tasks. It outshines the E5 base model while being less than a third of its size. Moreover, it stands toe-to-toe with the E5 large model, which is seven times larger, showcasing its efficiency and power. This efficiency makes <code>jina-embeddings-v2-base-de</code> a game-changer, particularly when compared to other popular bi- and multilingual embedding models.</p><h3 id="excelling-in-german-english-cross-language-retrieval">Excelling in German-English Cross-Language Retrieval</h3><p>Our model isn&apos;t just about size and efficiency; it&apos;s also a top performer in English-German cross-language retrieval tasks. This is evident in its performance in various key benchmarks:</p><ul><li><a href="https://www.cl.uni-heidelberg.de/statnlpgroup/wikiclir/?ref=jina-ai-gmbh.ghost.io">WikiCLIR</a>, for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts17-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS17</a>, part of the MTEB evaluation for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts22-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS22</a>, for German to English retrieval, also part of MTEB</li><li><a href="https://huggingface.co/datasets/mteb/bucc-bitext-mining?ref=jina-ai-gmbh.ghost.io">BUCC</a>, for German to English retrieval, included in MTEB</li></ul><p>The performance in these benchmarks, particularly in the MTEB evaluation tests (with the exception of WikiCLIR), underscores the effectiveness of <code>jina-embeddings-v2-base-de</code> in handling complex bilingual tasks.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1275" height="625" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png 1275w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="get-api-access">Get API Access</h2><p>Our offerings for our enterprise users who value privacy and data compliance, including <code>jina-embeddings-v2-base-de</code>, are accessible via the Jina Embeddings API:</p><ol><li>Visit <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a> and click on the model dropdown</li><li>Select <code>jina-embeddings-v2-base-de</code></li></ol><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-3.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="2000" height="1062" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/01/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>We will make this model available in the AWS Sagemaker marketplace for Amazon cloud users and for download on HuggingFace very soon.</p><h2 id="jina-8k-embeddings-the-cornerstone-of-diverse-ai-applications">Jina 8K Embeddings: The Cornerstone of Diverse AI Applications</h2><p>Embeddings are crucial for a wide range of AI applications, including information retrieval, data quality control, classification, and recommendation. They are fundamental to enhancing numerous AI tasks.</p><p>Jina AI is committed to advancing the state-of-the-art in embedding technology, keeping our core AI components transparent, accessible, and affordable to enterprises of all types and sizes that value privacy and data compliance. In addition to <code>jina-embeddings-v2-base-de</code>, Jina AI has released state-of-the-art embedding models for <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese</a> and high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">English monolingual models</a>. This is part of our mission to make AI technology more inclusive and globally applicable.</p><p>We value your feedback. Join our community channel to contribute feedback and stay informed about our advancements. Together, we&apos;re shaping a more robust and inclusive AI future.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4232 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English]]></title><description><![CDATA[The first bilingual Chinese-English embedding model with 8192 token-length.]]></description><link>https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/</link><guid isPermaLink="false">659d729f0bab3100012d2c85</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 18:58:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><p>Following the remarkable success of the previous <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings V2</a>, we are thrilled to announce the launch of our latest Chinese/English bilingual text embedding model: <code>jina-embeddings-v2-base-zh</code>. This new model inherits the exceptional 8K token length of Jina Embeddings V2, now with robust support for both Chinese and English languages.</p><p><code>jina-embeddings-v2-base-zh</code> stands out for its exceptional quality and performance, achieved through rigorous and balanced pre-training with high-quality bilingual data. This approach ensures a significant reduction in bias, often seen in models trained with unbalanced multilingual data.</p><h2 id="highlights">Highlights</h2><ul><li><strong>Bilingual Model:</strong> This model encodes texts in both English and Chinese, allowing the use of either language as the query or target document. Texts with equivalent meanings in these languages are mapped to the same embedding space, forming the basis for numerous multilingual applications.</li><li><strong>Extended 8K Token-Length:</strong> Our model is capable of processing significantly large text passages, a feature that exceeds the capabilities of most other open-source models.</li><li><strong>Compact and Efficient:</strong> With a size of 322MB (161 million parameters) and output dimensions of 768, our model is designed for high performance on standard computer hardware without GPU, enhancing its accessibility.</li></ul><h2 id="leading-performance-on-c-mteb">Leading Performance on C-MTEB</h2><p>In the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Chinese MTEB leaderboard</a>, our Jina Embeddings v2, supporting both Chinese and English, stands out as one of the top models<strong> under 0.5GB</strong>. What sets it apart is its impressive 8K token-length capability, a unique feature in its category.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>Among Chinese models of similar size, only the E5 Multilingual model and our <code>jina-embeddings-v2-base-zh</code> offer support for English, enabling effective cross-language applications. Notably, Jina demonstrates significantly superior performance in all categories involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>While both models have an 8K token context size, <code>jina-embeddings-v2-base-zh</code> significantly outperforms OpenAI&apos;s <code>text-embedding-ada-002</code>,  especially in tasks involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h2 id="empowering-chinese-enterprises-for-global-expansion">Empowering Chinese Enterprises for Global Expansion</h2><p>Our Chinese-English embedding model is a powerful tool for Chinese companies looking to &apos;go global&apos; (&#x51FA;&#x6D77;). It seamlessly processes Chinese texts, providing high-quality embeddings that effortlessly integrate with leading vector databases, search systems, RAG applications.</p><p><code>jina-embeddings-v2-base-zh</code> is especially beneficial for developing AI applications tailored to Chinese-English contexts, crucial for businesses expanding internationally. Here are some specific use cases:</p><ol><li><strong>Document Analysis and Management</strong>: It can analyze and manage a vast array of documents, aiding in international legal and business transactions.</li><li><strong>AI-Powered Search Applications</strong>: Enhances search functions in multilingual environments, making it easier for global users to find relevant information in Chinese and English.</li><li><strong>Retrieval-Augmented Chatbots and Question-Answering</strong>: Builds efficient, bilingual customer service bots, improving interactions with customers worldwide.</li><li><strong>Natural Language Processing Applications</strong>: This includes sentiment analysis for understanding global market trends, topic modeling for international marketing strategies, and text classification for managing global communication.</li><li><strong>Recommender Systems</strong>: Tailors product and content recommendations for diverse global audiences, using insights drawn from Chinese and English data.</li></ol><p>By leveraging this model, Chinese enterprises can effectively bridge the language gap in their AI applications, enhancing their global competitiveness and market reach.</p><h2 id="get-started-with-jina-embeddings-v2-base-zh-via-api">Get Started with <code>jina-embeddings-v2-base-zh</code> via API</h2><p>Begin integrating our model into your workflow immediately through the Embeddings API. Simply visit the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">our Embeddings portal</a>, get your free access key or top up an existing key, and then choose <code>jina-embeddings-v2-base-zh</code> from the dropdown menu. It&apos;s that easy to get started!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1202" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--57-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--57-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Untitled--57-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png 2060w" sizes="(min-width: 720px) 720px"></figure><h2 id="whats-next-expanding-language-support-and-aws-sagemaker-integration">What&apos;s Next: Expanding Language Support and AWS Sagemaker Integration</h2><p><code>jina-embeddings-v2-base-zh</code> will soon be available via AWS Sagemaker and Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>At Jina AI, our commitment to being a leader in affordable and accessible embedding technology for a global audience is unwavering. We&apos;re actively developing additional multilingual offerings, focusing on major European and other international languages, to broaden our reach. Stay tuned for these exciting updates, including integration with AWS SageMaker, as we continue to expand our capabilities.</p><h2 id="a-special-thanks-to-our-early-testers">A Special Thanks to Our Early Testers</h2><p>We&apos;re immensely grateful to the select members of our Chinese user community who tested the preview version (<code>jina-embeddings-v2-base-zh-preview</code>). Their insightful feedback was crucial in enhancing the official release&apos;s performance for this release. If you have any observations or suggestions regarding the quality of our models, we warmly invite you to join our Discord server and share your thoughts with us. Your input is invaluable in our continuous improvement journey.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">Improved Score Distribution vs. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code><span style="white-space: pre-wrap;"> suffered from inflated similarity scores, leading to high cosine scores even for unrelated items. This was particularly evident in the top-5 results from the screenshot below. The similarity scores were consistently high and did not accurately reflect the true relationship between items. For example, the comparison between &#x201C;&#x5B89;&#x59AE;&#x201D; and &#x201C;&#x84B8;&#x6C7D;&#x673A;&#x201D; received misleadingly high similarity scores.</span></p><p><span style="white-space: pre-wrap;">In the official release, we have fine-tuned the model to produce more distinct and logical similarity scores, ensuring a more accurate representation of the relationships between items. For example, the revised scoring now presents a broader range, offering a clearer insight into the relative similarity among items.</span></p><p><span style="white-space: pre-wrap;">Additionally, Jina Embeddings now uniquely stands as the only open-source embedding model supporting 8192 tokens. This feature highlights its capability in processing a wide variety of data types, from extensive documents to brief phrases, or even individual words/names such as &#x201C;&#x5B89;&#x59AE;&#x201D; vs &#x201C;&#x9732;&#x5A1C;&#x201D;.</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure><p></p><h2 id="%E4%B8%AD%E8%8B%B1%E5%8F%8C%E8%AF%AD8k%E5%90%91%E9%87%8F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B0%E9%B2%9C%E5%87%BA%E7%82%89%EF%BC%8C%E4%BC%81%E4%B8%9A%E5%87%BA%E6%B5%B7%E5%BF%85%E5%A4%87%EF%BC%81">&#x4E2D;&#x82F1;&#x53CC;&#x8BED;8K&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#x65B0;&#x9C9C;&#x51FA;&#x7089;&#xFF0C;&#x4F01;&#x4E1A;&#x51FA;&#x6D77;&#x5FC5;&#x5907;&#xFF01;</h2><p>&#x81EA;&#x4ECE;&#x6211;&#x4EEC;&#x7684; Embeddings V2 &#x83B7;&#x5F97;&#x5404;&#x754C;&#x597D;&#x8BC4;&#x540E;&#xFF0C;&#x4ECA;&#x65E5;&#xFF0C;&#x6211;&#x4EEC;&#x63A8;&#x51FA;&#x4E86;&#x5168;&#x65B0;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#xFF1A;jina-embeddings-v2-base-zh&#x3002;&#x6B64;&#x6A21;&#x578B;&#x4E0D;&#x4EC5;&#x7EE7;&#x627F;&#x4E86; V2 &#x7684;&#x5168;&#x90E8;&#x4F18;&#x52BF;&#xFF0C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x957F;&#x8FBE;&#x516B;&#x5343;&#x8BCD;&#x5143;&#x7684;&#x6587;&#x672C;&#xFF0C;&#x66F4;&#x80FD;&#x6D41;&#x7545;&#x5E94;&#x5BF9;&#x4E2D;&#x82F1;&#x6587;&#x53CC;&#x8BED;&#x5185;&#x5BB9;&#xFF0C;&#x4E3A;&#x8DE8;&#x8BED;&#x79CD;&#x7684;&#x5E94;&#x7528;&#x63D2;&#x4E0A;&#x4E86;&#x7FC5;&#x8180;&#x3002;</p><p>jina-embeddings-v2-base-zh &#x4E4B;&#x6240;&#x4EE5;&#x8868;&#x73B0;&#x5353;&#x8D8A;&#xFF0C;&#x5168;&#x8D56;&#x4F18;&#x8D28;&#x7684;&#x53CC;&#x8BED;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x7ECF;&#x8FC7;&#x6211;&#x4EEC;&#x4E25;&#x683C;&#x4E14;&#x5E73;&#x8861;&#x7684;&#xA0;<strong>&#x9884;&#x8BAD;&#x7EC3;&#x3001;&#x4E00;&#x9636;&#x5FAE;&#x8C03;&#x548C;&#x4E8C;&#x9636;&#x5FAE;&#x8C03;</strong>&#x3002;&#x8FD9;&#x79CD;&#x4E09;&#x6B65;&#x8D70;&#x7684;&#x8BAD;&#x7EC3;&#x8303;&#x5F0F;&#x4E0D;&#x4EC5;&#x6CDB;&#x5316;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x53CC;&#x8BED;&#x80FD;&#x529B;&#xFF0C;&#x66F4;&#x6709;&#x6548;&#x7684;&#x964D;&#x4F4E;&#x4E86;&#x6A21;&#x578B;&#x504F;&#x89C1;&#xFF0C;&#x89E3;&#x51B3;&#x4E86;&#x591A;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x65F6;&#x5E38;&#x906D;&#x9047;&#x5230;&#x7684;&#x201C;&#x4E0D;&#x60A3;&#x5BE1;&#x800C;&#x60A3;&#x4E0D;&#x5747;&#x201D;&#x7684;&#x95EE;&#x9898;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E7%89%B9%E8%89%B2%E4%B8%80%E8%A7%88"><strong>&#x6A21;&#x578B;&#x7279;&#x8272;&#x4E00;&#x89C8;</strong></h3><p><strong>&#x7279;&#x8272; 1&#xFF1A;&#x53CC;&#x8BED;&#x65E0;&#x7F1D;&#x5BF9;&#x63A5;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x80FD;&#x591F;&#x6D41;&#x7545;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x4F5C;&#x4E3A;&#x641C;&#x7D22;&#x67E5;&#x8BE2;&#x8FD8;&#x662F;&#x76EE;&#x6807;&#x6587;&#x6863;&#x3002;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#x4E2D;&#x610F;&#x4E49;&#x76F8;&#x8FD1;&#x7684;&#x5185;&#x5BB9;&#x90FD;&#x4F1A;&#x88AB;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x5411;&#x91CF;&#x7A7A;&#x95F4;&#xFF0C;&#x4E3A;&#x591A;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x5960;&#x5B9A;&#x4E86;&#x575A;&#x5B9E;&#x57FA;&#x7840;&#x3002;</p><p><strong>&#x7279;&#x8272; 2&#xFF1A;8k Token &#x8D85;&#x957F;&#x6587;&#x672C;&#x652F;&#x6301;</strong></p><p>&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x652F;&#x6301;&#x957F;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x5728;&#x5F00;&#x6E90;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x4E2D;&#x72EC;&#x6811;&#x4E00;&#x5E1C;&#xFF0C;&#x5728;&#x5904;&#x7406;&#x66F4;&#x957F;&#x7684;&#x6587;&#x672C;&#x6BB5;&#x843D;&#x4E0A;&#x63D0;&#x4F9B;&#x4E86;&#x663E;&#x8457;&#x4F18;&#x52BF;&#x3002;</p><p><strong>&#x7279;&#x8272; 3&#xFF1A;&#x9AD8;&#x6548;&#x7D27;&#x51D1;&#x7684;&#x6A21;&#x578B;&#x7ED3;&#x6784;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x4EE5; 322MB &#x7684;&#x8F7B;&#x5DE7;&#x4F53;&#x79EF;&#xFF08;&#x5305;&#x542B; 1.61 &#x4EBF;&#x53C2;&#x6570;&#xFF09;&#xFF0C;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x4E3A; 768&#xFF0C;&#x80FD;&#x591F;&#x5728;&#x666E;&#x901A;&#x8BA1;&#x7B97;&#x673A;&#x786C;&#x4EF6;&#x4E0A;&#x9AD8;&#x6548;&#x8FD0;&#x884C;&#xFF0C;&#x65E0;&#x9700;&#x4F9D;&#x8D56; GPU&#xFF0C;&#x6781;&#x5927;&#x5730;&#x63D0;&#x5347;&#x4E86;&#x5176;&#x5B9E;&#x7528;&#x6027;&#x548C;&#x4FBF;&#x6377;&#x6027;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%8D%93%E8%B6%8A"><strong>&#x6A21;&#x578B;&#x6027;&#x80FD;&#x5353;&#x8D8A;</strong></h3><p>&#x5728; CMTEB &#x6392;&#x884C;&#x699C;&#x7684;&#x6FC0;&#x70C8;&#x7ADE;&#x4E89;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684; Jina Embeddings v2 &#x6A21;&#x578B;&#x5728; 0.5GB &#x4EE5;&#x4E0B;&#x6A21;&#x578B;&#x7C7B;&#x522B;&#x4E2D;&#x8131;&#x9896;&#x800C;&#x51FA;&#xFF0C;&#x5B83;&#x4E0D;&#x4EC5;&#x652F;&#x6301;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x800C;&#x4E14;&#x80FD;&#x591F;&#x5904;&#x7406;&#x9AD8;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#xFF0C;&#x8FD9;&#x4E00;&#x80FD;&#x529B;&#x5728;&#x540C;&#x7C7B;&#x6A21;&#x578B;&#x4E2D;&#x5B9E;&#x5C5E;&#x7F55;&#x89C1;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>&#x5728;&#x540C;&#x7B49;&#x4F53;&#x79EF;&#x7684;&#x652F;&#x6301;&#x4E2D;&#x6587;&#x7684;&#x6A21;&#x578B;&#x4E2D;&#xFF0C;Multilingual E5 &#x548C;&#x6211;&#x4EEC;&#x7684; jina-embeddings-v2-base-zh &#x662F;&#x552F;&#x4E8C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x82F1;&#x6587;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x8FD9;&#x4F7F;&#x5F97;&#x8DE8;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x6210;&#x4E3A;&#x53EF;&#x80FD;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>&#x76EE;&#x524D;&#xFF0C;&#x5168;&#x7403;&#x8303;&#x56F4;&#x5185;&#xFF0C;&#x4EC5;&#x6709; OpenAI &#x7684;&#x95ED;&#x6E90;&#x6A21;&#x578B; text-embedding-ada-002 &#x548C; Jina Embeddings &#x80FD;&#x591F;&#x652F;&#x6301; 8k Token &#x7684;&#x957F;&#x6587;&#x672C;&#x8F93;&#x5165;&#x3002;&#x800C;&#x5728;&#x5904;&#x7406;&#x4E2D;&#x6587;&#x4EFB;&#x52A1;&#x65B9;&#x9762;&#xFF0C;Jina Embeddings &#x663E;&#x793A;&#x51FA;&#x4E86;&#x663E;&#x8457;&#x7684;&#x6027;&#x80FD;&#x4F18;&#x52BF;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%8A%A9%E5%8A%9B%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A%E6%8B%93%E5%B1%95%E5%85%A8%E7%90%83%E4%B8%9A%E5%8A%A1">&#x52A9;&#x529B;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x62D3;&#x5C55;&#x5168;&#x7403;&#x4E1A;&#x52A1;</h3><p>&#x6211;&#x4EEC;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B; jina-embeddings-v2-base-zh &#x662F;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x8FDB;&#x519B;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7684;&#x5F3A;&#x5927;&#x4F19;&#x4F34;&#x3002;&#x5B83;&#x80FD;&#x591F;&#x65E0;&#x7F1D;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#xFF0C;&#x5E76;&#x63D0;&#x4F9B;&#x9AD8;&#x8D28;&#x91CF;&#x7684;&#x6587;&#x672C;&#x5411;&#x91CF;&#x8868;&#x793A;&#xFF0C;&#x8FD8;&#x80FD;&#x8F7B;&#x677E;&#x96C6;&#x6210;&#x5230;&#x5148;&#x8FDB;&#x7684;&#x5411;&#x91CF;&#x6570;&#x636E;&#x5E93;&#x3001;&#x641C;&#x7D22;&#x7CFB;&#x7EDF;&#x4EE5;&#x53CA; RAG &#x5E94;&#x7528;&#x91CC;&#x3002;<br>&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#x7279;&#x522B;&#x9002;&#x5408;&#x6253;&#x9020;&#x9002;&#x5E94;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x573A;&#x666F;&#x7684; AI &#x5E94;&#x7528;&#xFF0C;&#x5BF9;&#x4E8E;&#x8FFD;&#x6C42;&#x5168;&#x7403;&#x5316;&#x53D1;&#x5C55;&#x7684;&#x4F01;&#x4E1A;&#x6765;&#x8BF4;&#xFF0C;&#x5176;&#x4EF7;&#x503C;&#x4E0D;&#x53EF;&#x4F30;&#x91CF;&#x3002;&#x4EE5;&#x4E0B;&#x662F;&#x51E0;&#x4E2A;&#x5B9E;&#x9645;&#x5E94;&#x7528;&#x6848;&#x4F8B;&#xFF1A;</p><ul><li>&#x6587;&#x6863;&#x5206;&#x6790;&#x4E0E;&#x7BA1;&#x7406;&#xFF1A;&#x5206;&#x6790;&#x548C;&#x7BA1;&#x7406;&#x6D77;&#x91CF;&#x6587;&#x6863;&#xFF0C;&#x52A9;&#x529B;&#x56FD;&#x9645;&#x6CD5;&#x5F8B;&#x548C;&#x5546;&#x52A1;&#x4EA4;&#x6613;&#x7684;&#x987A;&#x5229;&#x8FDB;&#x884C;&#x3002;</li><li>AI &#x9A71;&#x52A8;&#x641C;&#x7D22;&#x5E94;&#x7528;&#xFF1A;&#x5728;&#x591A;&#x8BED;&#x8A00;&#x73AF;&#x5883;&#x4E2D;&#x63D0;&#x5347;&#x641C;&#x7D22;&#x6027;&#x80FD;&#xFF0C;&#x5E2E;&#x52A9;&#x5168;&#x7403;&#x7528;&#x6237;&#x8F7B;&#x677E;&#x627E;&#x5230;&#x4E2D;&#x82F1;&#x6587;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</li><li>&#x589E;&#x5F3A;&#x68C0;&#x7D22;&#x7684;&#x804A;&#x5929;&#x673A;&#x5668;&#x4EBA;&#x548C;&#x95EE;&#x7B54;&#x7CFB;&#x7EDF;&#xFF1A;&#x6253;&#x9020;&#x9AD8;&#x6548;&#x7684;&#x53CC;&#x8BED;&#x5BA2;&#x670D;&#x673A;&#x5668;&#x4EBA;&#xFF0C;&#x4F18;&#x5316;&#x4E0E;&#x5168;&#x7403;&#x5BA2;&#x6237;&#x7684;&#x6C9F;&#x901A;&#x4F53;&#x9A8C;&#x3002;</li><li>&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x5E94;&#x7528;&#xFF1A;&#x6DB5;&#x76D6;&#x5168;&#x7403;&#x5E02;&#x573A;&#x8D8B;&#x52BF;&#x5206;&#x6790;&#x3001;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7B56;&#x7565;&#x7684;&#x4E3B;&#x9898;&#x5EFA;&#x6A21;&#xFF0C;&#x4EE5;&#x53CA;&#x5168;&#x7403;&#x901A;&#x8BAF;&#x7BA1;&#x7406;&#x7684;&#x6587;&#x672C;&#x5206;&#x7C7B;&#x3002;</li><li>&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#xFF1A;&#x5229;&#x7528;&#x4E2D;&#x82F1;&#x6570;&#x636E;&#x6D1E;&#x5BDF;&#xFF0C;&#x4E3A;&#x5168;&#x7403;&#x591A;&#x5143;&#x5316;&#x53D7;&#x4F17;&#x63D0;&#x4F9B;&#x4E2A;&#x6027;&#x5316;&#x7684;&#x4EA7;&#x54C1;&#x548C;&#x5185;&#x5BB9;&#x63A8;&#x8350;&#x3002;</li></ul><p>&#x501F;&#x52A9;&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#xFF0C;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x80FD;&#x591F;&#x5728; AI &#x5E94;&#x7528;&#x9886;&#x57DF;&#x8DE8;&#x8D8A;&#x8BED;&#x8A00;&#x7684;&#x9E3F;&#x6C9F;&#xFF0C;&#x5728;&#x5168;&#x7403;&#x5E02;&#x573A;&#x7684;&#x89D2;&#x9010;&#x4E2D;&#x5360;&#x636E;&#x5148;&#x673A;&#x3002;</p><h3 id="%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B-jina-embeddings-v2-base-zh">&#x8F7B;&#x677E;&#x4E0A;&#x624B; jina-embeddings-v2-base-zh</h3><p>&#x60F3;&#x8981;&#x5FEB;&#x901F;&#x5C06;&#x6211;&#x4EEC;&#x7684;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x878D;&#x5165;&#x60A8;&#x7684;&#x5DE5;&#x4F5C;&#x6D41;&#x7A0B;&#xFF1F;&#x53EA;&#x9700;&#x51E0;&#x4E2A;&#x7B80;&#x5355;&#x6B65;&#x9AA4;&#xFF1A;&#x8BBF;&#x95EE; <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">https://jina.ai/embeddings</a>&#xFF0C;&#x9886;&#x53D6;&#x60A8;&#x7684;&#x514D;&#x8D39; API &#x5BC6;&#x94A5;&#x6216;&#x66F4;&#x65B0;&#x73B0;&#x6709;&#x5BC6;&#x94A5;&#xFF0C;&#x7136;&#x540E;&#x5728;&#x4E0B;&#x62C9;&#x83DC;&#x5355;&#x4E2D;&#x9009;&#x62E9; jina-embeddings-v2-base-zh&#xFF0C;&#x60A8;&#x7684;&#x6A21;&#x578B;&#x5373;&#x523B;&#x51C6;&#x5907;&#x5C31;&#x7EEA;&#xFF0C;&#x7B49;&#x5F85;&#x60A8;&#x7684;&#x63A2;&#x7D22;&#x548C;&#x4F7F;&#x7528;&#xFF01;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1138" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png 2222w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%B1%95%E6%9C%9B%E6%9C%AA%E6%9D%A5%EF%BC%9A%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%E4%B8%8E-aws-sagemaker-%E6%B7%B1%E5%BA%A6%E8%9E%8D%E5%90%88">&#x5C55;&#x671B;&#x672A;&#x6765;&#xFF1A;&#x591A;&#x8BED;&#x8A00;&#x652F;&#x6301;&#x4E0E; AWS SageMaker &#x6DF1;&#x5EA6;&#x878D;&#x5408;</h3><p>jina-embeddings-v2-base-zh &#x5373;&#x5C06;&#x4E0A;&#x7EBF; AWS SageMaker &#x548C; HuggingFace&#xFF0C;&#x4E3A;&#x7528;&#x6237;&#x63D0;&#x4F9B;&#x66F4;&#x52A0;&#x4FBF;&#x6377;&#x7684;&#x670D;&#x52A1;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>&#x6211;&#x4EEC;&#x6B63;&#x79EF;&#x6781;&#x63A8;&#x8FDB;&#x591A;&#x8BED;&#x8A00;&#x5411;&#x91CF;&#x6A21;&#x578B;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x6B27;&#x6D32;&#x53CA;&#x5176;&#x4ED6;&#x56FD;&#x9645;&#x8BED;&#x8A00;&#x7684;&#x652F;&#x6301;&#xFF0C;&#x6765;&#x6EE1;&#x8DB3;&#x5168;&#x7403;&#x7528;&#x6237;&#x7684;&#x591A;&#x6837;&#x5316;&#x9700;&#x6C42;&#x3002;&#x656C;&#x8BF7;&#x671F;&#x5F85;&#x6211;&#x4EEC;&#x5373;&#x5C06;&#x63A8;&#x51FA;&#x7684;&#x6FC0;&#x52A8;&#x4EBA;&#x5FC3;&#x7684;&#x66F4;&#x65B0;&#xFF0C;&#x5305;&#x62EC;&#x4E0E; AWS SageMaker &#x7684;&#x6DF1;&#x5EA6;&#x96C6;&#x6210;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6301;&#x7EED;&#x6DF1;&#x5316;&#x548C;&#x62D3;&#x5BBD;&#x670D;&#x52A1;&#x8303;&#x56F4;&#x3002;</p><h3 id="%E8%87%B4%E8%B0%A2%EF%BC%9A%E6%84%9F%E8%B0%A2%E6%97%A9%E6%9C%9F%E6%B5%8B%E8%AF%95%E8%80%85%E7%9A%84%E5%AE%9D%E8%B4%B5%E8%B4%A1%E7%8C%AE">&#x81F4;&#x8C22;&#xFF1A;&#x611F;&#x8C22;&#x65E9;&#x671F;&#x6D4B;&#x8BD5;&#x8005;&#x7684;&#x5B9D;&#x8D35;&#x8D21;&#x732E;</h3><p>&#x6211;&#x4EEC;&#x8877;&#x5FC3;&#x611F;&#x8C22;&#x53C2;&#x4E0E; <code>jina-embeddings-v2-base-zh-preview</code> &#x6D4B;&#x8BD5;&#x7684;&#x4E2D;&#x56FD;&#x793E;&#x533A;&#x670B;&#x53CB;&#x4EEC;&#x3002;&#x4F60;&#x4EEC;&#x7684;&#x5B9D;&#x8D35;&#x610F;&#x89C1;&#x5BF9;&#x4F18;&#x5316;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x8D77;&#x5230;&#x4E86;&#x91CD;&#x8981;&#x4F5C;&#x7528;&#x3002;&#x5982;&#x679C;&#x60A8;&#x5728;&#x4F7F;&#x7528;&#x8FC7;&#x7A0B;&#x4E2D;&#x6709;&#x4EFB;&#x4F55;&#x5EFA;&#x8BAE;&#x6216;&#x60F3;&#x6CD5;&#xFF0C;&#x6B22;&#x8FCE;&#x968F;&#x65F6;&#x5411;&#x6211;&#x4EEC;&#x63D0;&#x51FA;&#x3002;&#x60A8;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x53CD;&#x9988;&#x90FD;&#x662F;&#x6211;&#x4EEC;&#x6301;&#x7EED;&#x8FDB;&#x6B65;&#x7684;&#x52A8;&#x529B;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">&#x6B63;&#x5F0F;&#x7248;&#x89E3;&#x51B3;&#x4E86;&#x9884;&#x89C8;&#x7248;&#x7684;&#x5206;&#x6570;&#x81A8;&#x80C0;&#x95EE;&#x9898;</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">&#x4E0E;&#x4E4B;&#x524D;&#x7684;&#x9884;&#x89C8;&#x7248;&#x6A21;&#x578B;&#x76F8;&#x6BD4;&#xFF0C;&#x6B63;&#x5F0F;&#x7248;&#x6A21;&#x578B;&#x63D0;&#x4F9B;&#x4E86;&#x66F4;&#x52A0;&#x5206;&#x6563;&#x4E14;&#x5408;&#x7406;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x3002;&#x5728;&#x9884;&#x89C8;&#x7248;&#x7684;&#x6D4B;&#x8BD5;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x66FE;&#x663E;&#x793A;&#x51FA;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x7684;&#x901A;&#x8D27;&#x81A8;&#x80C0;&#x73B0;&#x8C61;&#xFF0C;&#x5373;&#x4FBF;&#x662F;&#x5B8C;&#x5168;&#x4E0D;&#x76F8;&#x5173;&#x7684;&#x8BCD;&#x6C47;&#xFF0C;&#x6BD4;&#x5982;&#x2018;&#x5B89;&#x59AE;&#x2019;&#x548C;&#x2018;&#x84B8;&#x6C7D;&#x673A;&#x2019;&#xFF0C;&#x4E5F;&#x4F1A;&#x83B7;&#x5F97;&#x5F88;&#x9AD8;&#x7684;&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;&#x800C;&#x5728;&#x6B63;&#x5F0F;&#x7248;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4F18;&#x5316;&#x4E86;&#x6A21;&#x578B;&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x66F4;&#x4E3A;&#x5408;&#x7406;&#xFF0C;&#x4ECE;&#x800C;&#x66F4;&#x51C6;&#x786E;&#x5730;&#x53CD;&#x6620;&#x5185;&#x5BB9;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x3002;</span></p><p><span style="white-space: pre-wrap;">&#x6B64;&#x5916;&#xFF0C;Jina Embeddings &#x73B0;&#x5728;&#x652F;&#x6301;&#x9AD8;&#x8FBE; 8192 Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x957F;&#x7BC7;&#x5927;&#x8BBA;&#x8FD8;&#x662F;&#x7B80;&#x77ED;&#x8BED;&#x53E5;&#xFF0C;&#x751A;&#x81F3;&#x662F;&#x5355;&#x4E2A;&#x8BCD;&#x6C47;&#x6216;&#x540D;&#x5B57;&#xFF08;&#x5982;&#x201C;&#x5B89;&#x59AE;&#x201D;&#x4E0E;&#x201C;&#x9732;&#x5A1C;&#x201D;&#x7684;&#x6BD4;&#x8F83;&#xFF09;&#xFF0C;&#x90FD;&#x80FD;&#x5C55;&#x73B0;&#x51FA;&#x5176;&#x5904;&#x7406;&#x5404;&#x79CD;&#x7C7B;&#x578B;&#x6570;&#x636E;&#x7684;&#x5F3A;&#x5927;&#x80FD;&#x529B;&#x3002;&#x8FD9;&#x4E00;&#x6539;&#x8FDB;&#x4E0D;&#x4EC5;&#x63D0;&#x5347;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x51C6;&#x786E;&#x6027;&#xFF0C;&#x4E5F;&#x589E;&#x5F3A;&#x4E86;&#x5176;&#x5728;&#x5904;&#x7406;&#x591A;&#x6837;&#x5316;&#x6570;&#x636E;&#x65F6;&#x7684;&#x7075;&#x6D3B;&#x6027;&#x548C;&#x5B9E;&#x7528;&#x6027;&#x3002;</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure>]]></content:encoded></item><item><title><![CDATA[The 1950-2024 Text Embeddings Evolution Poster]]></title><description><![CDATA[Take part in celebrating the achievements of text embeddings and carry a piece of its legacy with you.]]></description><link>https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster/</link><guid isPermaLink="false">659d02a00bab3100012d2bff</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 11:07:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" alt="The 1950-2024 Text Embeddings Evolution Poster"><p>Embark on a journey through time with our latest infographic poster, showcasing the remarkable evolution of text embeddings from 1950 to 2024. This visually striking piece is not just a testament to technological progress; it&apos;s a guide that traces the lineage of innovations that have revolutionized how we represent text data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png" class="kg-image" alt="The 1950-2024 Text Embeddings Evolution Poster" loading="lazy" width="1414" height="2000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Latest-version2.0--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Latest-version2.0--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png 1414w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">The Evolution of Text Embeddings&#xA0;</span></a><span style="white-space: pre-wrap;">&#xA9; 2024&#xA0;by&#xA0;</span><a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">Jina AI&#xA0;</span></a><span style="white-space: pre-wrap;">is licensed under&#xA0;</span><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer"><span style="white-space: pre-wrap;">CC BY-NC-ND 4.0&#xA0;</span></a></figcaption></figure><h2 id="a-chronicle-of-innovation">A Chronicle of Innovation</h2><p>From the foundational Bag of Words model to the 8K token-length <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">jina-embeddings-v2</a> and everything in between, our poster captures each step in advancing text embeddings. Designed with precision, it highlights how each breakthrough built upon its predecessors, leading us to today&apos;s embeddings-driven applications.</p><h2 id="designed-for-enthusiasts-and-professionals-alike">Designed for Enthusiasts and Professionals Alike</h2><p>Whether you&apos;re a data scientist, a software engineer, an AI researcher, or simply a technology enthusiast, this poster is a must-have. It&apos;s more than just a visual treat; it&apos;s an educational tool that breaks down complex developments into an accessible format.</p><h2 id="bring-home-a-piece-of-ai-history">Bring Home a Piece of AI History</h2><p>For those who appreciate the tactile feel of quality, you can own this piece of history by purchasing a hard copy. Imagine this stunning poster adorning your living room wall, sparking conversations and inspiring future innovators.</p>
<!--kg-card-begin: html-->
<script async src="https://js.stripe.com/v3/buy-button.js">
</script>

<div style="text-align: center">
<stripe-buy-button buy-button-id="buy_btn_1OWchyEoTOtb5eZv69zXthH9" publishable-key="pk_live_51MD1n2EoTOtb5eZvVgdn5IcOR7BNFFcJTmb7hcAihzLOZpif376WqhFUTpN3sQHb7ADxQLtKxXvC7YAttivYrSDa00PJqFn7qz">
</stripe-buy-button>
</div>
<!--kg-card-end: html-->
<figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png 1024w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="accessible-in-multiple-formats">Accessible in Multiple Formats</h2><p>Not ready for a physical copy? No problem. We offer a downloadable PNG or PDF version, ensuring you can access this wealth of information in the format that best suits your needs.</p><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-v2.png" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PNG) Download the Evolution of Text Embeddings (834KB)</div><div class="kg-file-card-caption">Best for display and sharing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-v2.png</div><div class="kg-file-card-filesize">814 KB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-cmyk-color.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Print, CMYK) Download the Evolution of Text Embeddings (7.1MB)</div><div class="kg-file-card-caption">Best for printing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-cmyk-color.pdf</div><div class="kg-file-card-filesize">7 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-standard.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Standard) Download the Evolution of Text Embeddings (2.8MB)</div><div class="kg-file-card-caption">Best for viewing on the screen</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-standard.pdf</div><div class="kg-file-card-filesize">3 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><h2 id="references-at-your-fingertips">References at Your Fingertips</h2><p>Accompanying our infographic, we provide an extensive list of references, corresponding to each milestone depicted. This curated collection allows you to delve deeper into each technology, understanding the intricacies and applications that have shaped the field of natural language processing.</p><blockquote>TF-IDF	1972	K.S. Jones, A statistical interpretation of term specificity and its application in retrieval, J. Doc. 28 (1972) 11&#x2013;21.</blockquote><blockquote>TF-IDF	1973	K.S. Jones, Index term weighting, Inf. Storage Retr. 9 (11) (1973) 619&#x2013;633.</blockquote><blockquote>Bag of Words	1981	Z.S. Harris, Distributional structure, in: Papers on Syntax, Springer, 1981, pp. 3&#x2013;22.</blockquote><blockquote>BoN-Grams	1994	W. Cavnar, W.B. Cavnar, J.M. Trenkle, N-gram-based text categorization, in: Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR-94), 1994, pp. 161&#x2013;175.</blockquote><blockquote>doc2vec	2014	Q. Le, T. Mikolov, Distributed representations of sentences and documents, in: Proceedings of the 31st International Conference on International Conference on Machine Learning (ICML) - Volume 32, ICML &#x2019;14, JMLR.org, 2014, pp. II&#x2013;1188&#x2013;II&#x2013;1196.</blockquote><blockquote>DAN	2015	M. Iyyer, V. Manjunatha, J. Boyd-Graber, H. Daum&#xE9; III, Deep unordered composition rivals syntactic methods for text classification, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Beijing, China, 2015, pp. 1681&#x2013;1691.</blockquote><blockquote>RCNN	2015	S. Lai, L. Xu, K. Liu, J. Zhao, Recurrent convolutional neural networks for text classification, in: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI &#x2019;15, AAAI Press, 2015, pp. 2267&#x2013;2273.</blockquote><blockquote>RNNs	2015	D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, in: International Conference on Learning Representations (ICLR) 2015, 2014.</blockquote><blockquote>Skip-Thought	2015	R. Kiros, Y. Zhu, R.R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, S. Fidler, Skip-thought vectors, in: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 28, Curran Associates, Inc., 2015, pp. 3294&#x2013;3302.</blockquote><blockquote>DESM	2016	E. Nalisnick, B. Mitra, N. Craswell, R. Caruana, Improving document rank- ing with dual word embeddings, in: Proceedings of the 25th International Conference Companion on World Wide Web, 2016, pp. 83&#x2013;84.</blockquote><blockquote>DV-ngram	2016	B. Li, T. Liu, X. Du, D. Zhang, Z. Zhao, Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews, in: Workshop Contribution at International Conference on Learning Representations (ICLR) 2016, 2016.</blockquote><blockquote>FastSent	2016	F. Hill, K. Cho, A. Korhonen, Learning distributed representations of sentences from unlabelled data, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics (ACL), San Diego, California, 2016, pp. 1367&#x2013;1377.</blockquote><blockquote>HAN	2016	Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. Hovy, Hierarchical attention networks for document classification, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, San Diego, California, 2016, pp. 1480&#x2013;1489.</blockquote><blockquote>NVDM	2016	Y. Miao, L. Yu, P. Blunsom, Neural variational inference for text processing, in: Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML) - Volume 48, ICML &#x2019;16, JMLR.org, 2016, pp. 1727&#x2013;1736.</blockquote><blockquote>Siamese CBoW	2016	T. Kenter, A. Borisov, M. de Rijke, Siamese CBOW: Optimizing word embed- dings for sentence representations, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics (ACL), Berlin, Germany, 2016, pp. 941&#x2013;951.</blockquote><blockquote>CNN-LSTM	2017	Z. Gan, Y. Pu, R. Henao, C. Li, X. He, L. Carin, Learning generic sentence representations using convolutional neural networks, in: Empirical Methods in Natural Language Processing, EMNLP, 2017, pp. 2390&#x2013;2400.</blockquote><blockquote>CNNs	2017	Y. Zhang, D. Shen, G. Wang, Z. Gan, R. Henao, L. Carin, Deconvolutional paragraph representation learning, in: I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran Associates, Inc., 2017, pp. 5438&#x2013;5445.</blockquote><blockquote>CNNs	2017	Z. Zhu, J. Hu, Context aware document embedding, 2017, arXiv:1707.01521.</blockquote><blockquote>Doc2VecC	2017	M. Chen, Efficient vector representation for documents through corruption, in: International Conference on Learning Representations, ICLR, 2017.</blockquote><blockquote>DiSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, C. Zhang, DiSAN: Directional self- attention network for RNN/CNN-free language understanding, in: AAAI, 2018, pp. 5446&#x2013;5455.</blockquote><blockquote>ELMo	2018	M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettle- moyer, Deep contextualized word representations, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Associa- tion for Computational Linguistics (ACL), New Orleans, Louisiana, 2018, pp. 2227&#x2013;2237.</blockquote><blockquote>GPT-2	2018	A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners, OpenAI Blog (2018).</blockquote><blockquote>ReSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, C. Zhang, Reinforced self-attention network: A hybrid of hard and soft attention for sequence modeling, in: Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI &#x2019;18, AAAI Press, 2018, pp. 4345&#x2013;4352.</blockquote><blockquote>Sent2vec	2018	M. Pagliardini, P. Gupta, M. Jaggi, Unsupervised learning of sentence embed- dings using compositional n-gram features, in: Proceedings of North American Chapter of the Association for Computational Linguistics NAACL-HLT, 2018, pp. 528&#x2013;540.</blockquote><blockquote>BART	2019	Lewis, Mike, et al. &quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.&quot; arXiv preprint arXiv:1910.13461 (2019).</blockquote><blockquote>BERT	2019	J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidi- rectional transformers for language understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 4171&#x2013;4186.</blockquote><blockquote>DistilBERT	2019	V. Sanh, L. Debut, J. Chaumond, T. Wolf, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, in: 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing at NeurIPS 2019, 2019.</blockquote><blockquote>DocBERT	2019	A. Adhikari, A. Ram, R. Tang, J. Lin, DocBERT: BERT for document classification, 2019, ArXiv abs/1904.08398.</blockquote><blockquote>LASER	2019	M. Artetxe, H. Schwenk, Massively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond, Trans. Assoc. Comput. Linguist. 7 (2019) 597&#x2013;610.</blockquote><blockquote>MASS	2019	K. Song, X. Tan, T. Qin, J. Lu, T. Liu, MASS: Masked sequence to sequence pre-training for language generation, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, in: Proceedings of Machine Learning Research, vol. 97, PMLR, 2019, pp. 5926&#x2013;5936.</blockquote><blockquote>SBERT	2019	N. Reimers, I. Gurevych, Sentence-BERT: Sentence embeddings using siamese BERT-networks, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 3982&#x2013;3992.</blockquote><blockquote>Transformer-XL	2019	Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, R. Salakhutdinov, Transformer- XL: Attentive language models beyond a fixed-length context, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL, Association for Computational Linguistics, Florence, Italy, 2019, pp. 2978&#x2013;2988.</blockquote><blockquote>VLAWE	2019	R.T. Ionescu, A. Butnaru, Vector of locally-aggregated word embeddings (VLAWE): A novel document-level representation, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 363&#x2013;369.</blockquote><blockquote>XLM	2019	A. Conneau, G. Lample, Cross-lingual language model pretraining, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 7059&#x2013;7069.</blockquote><blockquote>XLNet	2019	Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R.R. Salakhutdinov, Q.V. Le, XLNet: Generalized autoregressive pretraining for language understanding, in: H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 5753&#x2013;5763.</blockquote><blockquote>ALBERT	2020	Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, in: International Conference on Learning Representations, ICLR, OpenReview.net, 2020.</blockquote><blockquote>ELECTRA	2020	Clark, Kevin, et al. &quot;Electra: Pre-training text encoders as discriminators rather than generators.&quot; arXiv preprint arXiv:2003.10555 (2020).</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Saw, P. Nokhiz, P. Netrapalli, P. Rai, P. Talukdar, P-SIF: Document embeddings using partition averaging, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020, pp. 7863&#x2013;7870.</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Kumar, P. Nokhiz, H. Gupta, P. Talukdar, Improving docu- ment classification with multi-sense embeddings, in: European Conference on Artificial Intelligence (ECAI) 2020, IOS Press, 2020, pp. 2030&#x2013;2037.</blockquote><blockquote>RoBERTa	2020	Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L.Zettlemoyer, V. Stoyanov, RoBERTa: A robustly optimized BERT pretrainingapproach, in: Under Review as a Conference Paper at International Conference on Learning Representations (ICLR) 2020, 2020.</blockquote><blockquote>SpanBERT	2020	M. Joshi, D. Chen, Y. Liu, D. Weld, L. Zettlemoyer, O. Levy, SpanBERT: Improving pre-training by representing and predicting spans, Trans. Assoc. Comput. Linguist. 8 (2020).</blockquote><blockquote>SimCSE	2021	Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894&#x2013;6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552.</blockquote><blockquote>AugCSE	2022	Tang, Zilu, Muhammed Yusuf Kocyigit, and Derry Wijaya. &quot;Augcse: Contrastive sentence embedding with diverse augmentations.&quot; arXiv preprint arXiv:2210.13749 (2022).</blockquote><blockquote>DiffCSE	2022	Oh, Dongsuk, et al. &quot;Don&apos;t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling.&quot; arXiv preprint arXiv:2209.05972 (2022).</blockquote><blockquote>SGPT	2022	Muennighoff, Niklas. &quot;Sgpt: Gpt sentence embeddings for semantic search.&quot; arXiv preprint arXiv:2202.08904 (2022).</blockquote><blockquote>bge	2023	C-Pack: Packaged Resources To Advance General Chinese Embedding</blockquote><blockquote>embeddings-v2	2023	G&#xFC;nther, Michael, et al. &quot;Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.&quot; arXiv preprint arXiv:2310.19923 (2023).</blockquote>]]></content:encoded></item><item><title><![CDATA[Words + JSON + Images = SceneXplain's new JSON Schema Builder]]></title><description><![CDATA[Effortlessly create JSON Schemas with SceneXplain: Describe your needs in natural language, and get the perfect schema for extracting JSON from your images!]]></description><link>https://jina.ai/news/words-json-images-scenexplains-new-json-schema-builder/</link><guid isPermaLink="false">65958ec70bab3100012d2bbc</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 04 Jan 2024 15:00:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--10-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--10-.jpg" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"><p>2024 has just kicked off, and we&apos;re happy to introduce our brand spanking new JSON Schema Builder interface - after all, you know what they say: New year, new UI, right?</p><p>As a bit of background, last year we introduced SceneXplain&apos;s <a href="https://www.notion.so/Words-JSON-Images-SceneXplain-s-new-JSON-Schema-Builder-7485a41a22e24510a67ebdd8e392a5c5?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">&quot;extract JSON from image&quot; feature</a>, which does what it says on the tin: You can specify a JSON Schema, upload your image, and the output will be a populated JSON string based on the image contents. For example, <a href="https://scenex.jina.ai/share?thread=ECR3q6syJZYehCHmL56c&amp;ref=jina-ai-gmbh.ghost.io">identifying products and text strings in an image</a>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--48-.png" class="kg-image" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" loading="lazy" width="460" height="815"></figure><p>But creating those JSON Schemas is hard work - folks often don&#x2019;t understand the connection between the task and the Schema, and it&#x2019;s easy to misplace a curly bracket or semi-colon and thus get invalid JSON. That&apos;s why we introduced our <a href="https://www.notion.so/Words-JSON-Images-SceneXplain-s-new-JSON-Schema-Builder-7485a41a22e24510a67ebdd8e392a5c5?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">JSON Schema Store</a>, where you can use predefined schemas and not have to roll your own.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--49-.png" class="kg-image" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" loading="lazy" width="1797" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--49-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--49-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Untitled--49-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--49-.png 1797w" sizes="(min-width: 720px) 720px"></figure><p>Of course, that can only go so far - if you want to build your own custom schema, you&apos;re just facing those problems all over again.</p><p>Well, not any more. Now, you can simply <em>say</em> what kind of JSON Schema you want, and SceneXplain will churn out a Schema that matches your requirements. In short, you use natural language to generate the schema, which you can then use with an image to extract your desired output JSON.</p><p>Here&#x2019;s how you can get started:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Step-by-step_guide_Building_and_using_your_JSON_Schema_in_SceneXplain__i-WIWI2sRDO1vxMSP4Cu1Q?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<p>Here are a few more examples to inspire you:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/strings.png" width="634" height="997" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/strings.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/strings.png 634w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/recipe.png" width="633" height="959" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/recipe.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/recipe.png 633w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/ad_detector.png" width="627" height="917" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/ad_detector.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/ad_detector.png 627w"></div></div></div></figure><h2 id="get-started">Get started</h2><p>Head over to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">scenex.jina.ai</a> to start generating your own JSON Schemas now and share your results on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord</a>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[DocArray 0.40.0 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-40-0-update/</link><guid isPermaLink="false">658599a20bab3100012d2b68</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 22 Dec 2023 14:20:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0400">Release Note (<code>0.40.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" alt="DocArray 0.40.0 Update"><p>This release contains 1 new feature, 3 bug fixes, and 2 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.40.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v0.40.0 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.40.0) Release time: 2023-12-22 12:12:15 This release contains 1 new feature, 3 bug fixes and 2 documentation improvements.
&#x1F195; Features
Add Epsilla connector (#1835)
We have integra&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="DocArray 0.40.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/668ee783e022a27964faa8e5f0a854cc1dea3f036e49bb10f4413288f5b2066f/docarray/docarray/releases/tag/v0.40.0" alt="DocArray 0.40.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="add-epsilla-connector-1835">Add Epsilla connector (<a href="https://github.com/docarray/docarray/pull/1835?ref=jina-ai-gmbh.ghost.io">#1835</a>)</h3><p>We have integrated&#xA0;<a href="https://epsilla.com/?ref=jina-ai-gmbh.ghost.io" rel="nofollow">Epsilla</a>&#xA0;into DocArray.</p><p>Here&apos;s a simple example of how to use it:</p><pre><code class="language-Python">import numpy as np
from docarray import BaseDoc
from docarray.index import EpsillaDocumentIndex
from docarray.typing import NdArray
from pydantic import Field

class MyDoc(BaseDoc):
    text: str
    embedding: NdArray[10] = Field(is_embedding=True)

docs = [MyDoc(text=f&apos;text {i}&apos;, embedding=np.random.rand(10)) for i in range(10)]
query = np.random.rand(10)
db = EpsillaDocumentIndex[MyDoc]()
db.index(docs)
results = db.find(query, limit=10)</code></pre><p>In this example, we create a document class with both textual and numeric data. Then, we initialize an Epsilla-backed document index and use it to index our documents. Finally, we perform a search query.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fixed-type-hints-error-in-python-312-1840">Fixed type hints error in Python 3.12 (<a href="https://github.com/docarray/docarray/pull/1840?ref=jina-ai-gmbh.ghost.io">#1840</a>)</h3><p>DocArray type-hinting is now available for Python 3.12.</p><h3 id="fix-issue-serializing-and-deserializing-complex-schemas-1836">Fix issue serializing and deserializing complex schemas (<a href="https://github.com/docarray/docarray/pull/1836?ref=jina-ai-gmbh.ghost.io">#1836</a>)</h3><p>There was an issue when serializing and deserializing&#xA0;<code>protobuf</code>&#xA0;documents with nested documents in dictionaries and other complex structures.</p><h3 id="fix-storage-issue-in-torchtensor-class-1833">Fix storage issue in TorchTensor class (<a href="https://github.com/docarray/docarray/pull/1833?ref=jina-ai-gmbh.ghost.io">#1833</a>)</h3><p>There was a bug when deep-copying a&#xA0;<code>TorchTensor</code>&#xA0;object if its&#xA0;<code>dtype</code>&#xA0;was not&#xA0;<code>float32</code>. This has now been fixed.</p><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Add Epsilla integration guide (<a href="https://github.com/docarray/docarray/pull/1838?ref=jina-ai-gmbh.ghost.io">docs(epsilla): add epsilla integration guide #1838</a>)</li><li>Fix sign commit command in docs (<a href="https://github.com/docarray/docarray/pull/1834?ref=jina-ai-gmbh.ghost.io">docs: fix sign commit commad in docs #1834</a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Tony Yang (<a href="https://github.com/tonyyanga?ref=jina-ai-gmbh.ghost.io">@tonyyanga</a>&#xA0;)</li><li>Naymul Islam (<a href="https://github.com/ai-naymul?ref=jina-ai-gmbh.ghost.io">@ai-naymul</a>&#xA0;)</li><li>Ben Shaver (<a href="https://github.com/bpshaver?ref=jina-ai-gmbh.ghost.io">@bpshaver</a>&#xA0;)</li><li>Joan Fontanals (@<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>954 (<a href="https://github.com/954-Ivory?ref=jina-ai-gmbh.ghost.io">@954-Ivory</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Full-stack RAG with Jina Embeddings v2 and LlamaIndex]]></title><description><![CDATA[You can build your own RAG chatbot in a matter of minutes with Jina Embeddings, LlamaIndex and Mixtral Instruct. We'll show you how to get up and running right now.]]></description><link>https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/</link><guid isPermaLink="false">6583fe510bab3100012d29f2</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 22 Dec 2023 13:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><p>LLMs are cool, but their input context windows are always so small. They&#x2019;re getting bigger, but they&#x2019;ll never be big enough for, say, a whole book, much less an encyclopedia, unless there is some breakthrough in AI model architectures.</p><p>RAG (<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?ref=jina-ai-gmbh.ghost.io">Retrieval Augmented Generation</a>) is a strategy that can compensate for this limitation, letting you use LLMs to respond to questions with answers that draw on relevant parts of documents or whole repositories of documents that are far too large to put entirely into the model&#x2019;s input.</p><p>This article will show you how to use LlamaIndex, Jina Embeddings, and the <code>Mixtral-8x7B-Instruct-v0.1</code> language model (<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">hosted on HuggingFace</a>) to build a complete RAG system. For more information on the Mixtral language model, see the <a href="https://mistral.ai/news/mixtral-of-experts/?ref=jina-ai-gmbh.ghost.io">Mistral AI website</a> or the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">model card on HuggingFace</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://mistral.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mistral AI | Open source models</div><div class="kg-bookmark-description">Frontier AI in your hands</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://mistral.ai/images/favicon/apple-touch-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Open source models</span><span class="kg-bookmark-publisher">Mistral AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://mistral.ai/images/mistral-social-banner.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>You can also <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download a Jupyter Notebook with all the code in this article from GitHub</a>, or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run it directly on Colab</a>.</p><p>You will need:</p><ol><li>A <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings API key</a>.</li><li>A <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace account and token</a>.</li></ol><p>Since both the Jina Embeddings model and Mixtral are running remotely and are accessed via a web API, you won&#x2019;t need any special hardware. You will need to install Python and meet the <a href="https://docs.llamaindex.ai/en/stable/getting_started/installation.html?ref=jina-ai-gmbh.ghost.io">system requirements for LlamaIndex</a>.</p><h2 id="what-is-rag-and-how-does-it-work">What is RAG and How Does it Work?</h2><p>Retrieval Augmented Generation is a strategy that merges search with language generation. The way it works is that it uses an external information retrieval system to find documents that are likely to inform the answer to a user query. It then passes them, with the user&#x2019;s request, to a text-generating language model, which produces a natural language response.</p><p>This allows you to use an LLM to answer questions and use information from documents and sets of documents that are much larger than its input context window. The LLM only sees a few pertinent parts of the document when responding to prompts. This also has the advantage of reducing (although not eliminating) inexplicable hallucinations.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="1600" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>This strategy has some limitations:</p><ol><li>It is sensitive to the input context size supported by the LLM. The larger the context size, the more information you can give the LLM, yielding better and richer responses.</li><li>It is sensitive to the quality of the results of the initial information retrieval. If your search engine gives it irrelevant or inaccurate results, the LLM may paste them together as best it can and give you garbage output. This can be caused by bad data (as the saying goes <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out?ref=jina-ai-gmbh.ghost.io"><em>garbage in, garbage out</em></a>) but can also be caused by a search system that does not return the most useful matches or does not rank them highly enough in the results.</li></ol><p>High-quality embeddings are key to making RAG work because they reduce the impact of these limitations.</p><p>First, a small context size for an LLM means it&#x2019;s extra important to find the most relevant information, because you cannot add very much to the user&#x2019;s prompt. Second, how informative the answer is depends on how informative the input is. If the search results displayed to the LLM are irrelevant or poorly informative, that will be reflected in the result.</p><p>AI-generated embeddings are, on the whole, the best way to find and rank query results in general.</p><h2 id="build-a-full-rag-chatbot">Build a Full RAG Chatbot</h2><p>We will create and install a full RAG system using the <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex framework</a> for working with LLMs. This system uses Jina Embeddings to index document elements and store them in LlamaIndex&#x2019; built-in vector store and search engine. Then, it uses the newly released <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">Mixtral Instruct</a> model to construct natural language answers.</p><p>The approach in the article will also work with OpenAI&#x2019;s GPT models and Meta&#x2019;s Llama2, with some adaptation of the code and possibly the prompt. For more details, read the <a href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io">LlamaIndex documentation</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex &#x1F999; 0.9.19</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://docs.llamaindex.ai/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">LlamaIndex &#x1F999; 0.9.19</span></div></div></a></figure><p>This section involves a lot of code to copy and paste, and it will only get a very high-level explanation. You may prefer to <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download the accompanying notebook</a> or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run this code on Google Colab</a>.</p><h3 id="getting-started">Getting Started</h3><p>First, install LlamaIndex:</p><pre><code class="language-bash">pip install llama-index
</code></pre><p>Next, make sure that you have a <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina API key</a> and a <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace Inference API token</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hugging Face &#x2013; The AI community building the future.</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/v2-2.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>In Python, set up your secret key values like this:</p><pre><code class="language-Python">jinaai_api_key = &quot;&lt;your Jina Embeddings API key&gt;&quot;
hf_inference_api_key: str = &apos;&lt;your HuggingFace Inference API token&gt;&apos;
</code></pre><h3 id="connect-jina-embeddings">Connect Jina Embeddings</h3><p>LlamaIndex provides built-in support for the Jina Embeddings API. To use it, you only need to initialize the <code>JinaEmbedding</code> object with your API key and model name. For this example, we will use <code>jina-embeddings-v2-base-en</code>.</p><pre><code class="language-Python">from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jinaai_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
)
</code></pre><h3 id="connect-mixtral-llm">Connect Mixtral LLM</h3><p>We will also need to load the <code>Mixtral-8x7B-Instruct-v0.1</code> model. We will wrap it in a subclass of <code>llama_index.llms.CustomLLM</code> to make it compatible with LlamaIndex.</p><p>The important elements are the class parameters:</p><pre><code class="language-Python">model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
api_key: str = hf_inference_api_key
context_window: int = 4096
num_output: int = 512
</code></pre><p>The parameter <code>model_name</code> is the name of the model on HuggingFace, in this case, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code>, which is also the path part of the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">URL for its model card on HuggingFace</a>. For <code>api_key</code>, you need to use your HuggingFace Inference API token. Then, specify the input context size the model supports (<code>context_window</code>), in this case, 4096 tokens, and the maximum output size in tokens (<code>num_output</code>), 512.</p><p>The code below sets up the LLM object in the LlamaIndex framework:</p><pre><code class="language-Python">import requests
from llama_index.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.llms.base import llm_completion_callback
from typing import Any


class MixtralLLM(CustomLLM):
    context_window: int = 4096
    num_output: int = 512
    model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
    api_key: str = hf_inference_api_key

    @property
    def metadata(self) -&gt; LLMMetadata:
        &quot;&quot;&quot;Get LLM metadata.&quot;&quot;&quot;
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    def do_hf_call(self, prompt: str) -&gt; str:
        data = {
            &quot;inputs&quot;: prompt
        }

        response = requests.post(
            &apos;https://api-inference.huggingface.co/models/&apos; + self.model_name,
            headers={
                &apos;authorization&apos;: f&apos;Bearer {self.api_key}&apos;,
                &apos;content-type&apos;: &apos;application/json&apos;,
            },
            json=data,
            stream=True
        )
        if response.status_code != 200 or not response.json() or &apos;error&apos; in response.json():
            print(f&quot;Error: {response}&quot;)
            return &quot;Unable to answer for technical reasons.&quot;
        full_txt = response.json()[0][&apos;generated_text&apos;]
        offset = full_txt.find(&quot;---------------------&quot;)
        ss = full_txt[offset:]
        offset = ss.find(&quot;Answer:&quot;)
        return ss[offset+7:].strip()

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse:
        response = self.do_hf_call(prompt)
        return CompletionResponse(text=response)

    @llm_completion_callback()
    def stream_complete(
            self, prompt: str, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        response = &quot;&quot;
        for token in self.do_hf_call(prompt):
            response += token
            yield CompletionResponse(text=response, delta=token)


mixtral_llm = MixtralLLM()</code></pre><h3 id="prepare-a-text-for-rag">Prepare a Text for RAG</h3><p>Next, we will download a document and break it into pieces.</p><p>For this exercise, the text we&#x2019;ll use is <a href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><strong><em>Computers on the Farm</em></strong></a>, published by the US Department of Agriculture in 1982 and available via the Gutenberg Project. This 10,000-word booklet is full of useful information for the farmer considering buying a home computer for farm operations 40 years ago.</p><p>Naturally, its advice is perhaps less helpful today.</p><p>However, it serves as a good example because it is much longer than the input context size of Mixtral LLMs or Jina Embeddings v2.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--25-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="298" height="519"></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Computers on the Farm by Deborah Takiff Smith</div><div class="kg-bookmark-description">Free kindle book and epub digitized and proofread by volunteers.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.gutenberg.org/gutenberg/apple-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Project Gutenberg</span><span class="kg-bookmark-publisher">Smith, Deborah Takiff</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.gutenberg.org/cache/epub/59316/pg59316.cover.medium.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>The code below will strip the Gutenberg Project header and footer from the text, correct the MS DOS-style linebreaks to conventional ones, and split the text on the headers.</p><pre><code class="language-Python">import urllib.request
from typing import List
from llama_index.readers import StringIterableReader
from llama_index.schema import Document


def load_gutenberg(target_url: str) -&gt; List[Document]:
    ret: List[str] = []
    buff: str = &quot;&quot;
    reject: bool = True
    for raw_line in urllib.request.urlopen(target_url):
        line = raw_line.decode(&quot;utf-8&quot;)
        stripped_line = line.strip()
        if reject:
            if stripped_line.startswith(&quot;*** START OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = False
                continue
        else:
            if stripped_line.startswith(&quot;*** END OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = True
                continue
            if stripped_line:
                if stripped_line.startswith(&apos;=&apos;) and stripped_line.endswith(&apos;=&apos;):
                    ret.append(buff)
                    buff = &quot;&quot;
                    buff += stripped_line[1:len(stripped_line)-1] + &quot;\n\n&quot;
                else:
                    buff += line.replace(&apos;\r&apos;, &apos;&apos;)
    if buff.strip():
        ret.append(buff)
    return StringIterableReader().load_data(ret)

docs = load_gutenberg(&quot;https://www.gutenberg.org/cache/epub/59316/pg59316.txt&quot;)

# check that we loaded

assert len(docs) == 58</code></pre><p>The result is a collection of 58 small documents.</p><p>The code below does the following:</p><ol><li>Create a <code>ServiceContext</code> object that holds both the Mixtral LLM and the Jina Embeddings connection. We will use this here and later to create the full RAG system.</li><li>Get an embedding for each small document using the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a>.</li><li>Store the documents and embeddings in LlamaIndex&#x2019;s <a href="https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html?ref=jina-ai-gmbh.ghost.io">built-in in-memory vector store</a> <code>VectorStoreIndex</code>.</li></ol><pre><code class="language-Python">from llama_index import VectorStoreIndex, ServiceContext

service_context = ServiceContext.from_defaults(
    llm=mixtral_llm, embed_model=jina_embedding_model
)
index = VectorStoreIndex.from_documents(
    documents=docs, service_context=service_context
)
</code></pre><h3 id="prepare-a-prompt">Prepare a Prompt</h3><p>Next, we will create a custom prompt template. This prompt specifically asks the LLM not to use information outside of the context information retrieved from the vector database and to specifically say &#x201C;No information&#x201D; when the context does not have any information that answers the user&#x2019;s request.</p><pre><code class="language-Python">from llama_index import PromptTemplate

qa_prompt_tmpl = (
    &quot;Context information is below.\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;{context_str}\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;Given the context information and not prior knowledge, &quot;
    &quot;answer the query. Please be brief, concise, and complete.\\n&quot;
    &quot;If the context information does not contain an answer to the query, &quot;
    &quot;respond with \\&quot;No information\\&quot;.&quot;
    &quot;Query: {query_str}\\n&quot;
    &quot;Answer: &quot;
)
qa_prompt = PromptTemplate(qa_prompt_tmpl)
</code></pre><p>Then, we assemble the query engine using the prompt.</p><p>The key parameter to look at here is <code>similarity_top_k=2</code> in <code>VectorIndexRetriever</code>. This tells the RAG system to put only the best two search matches into the context sent to the LLM.</p><p>We can set this to a larger value if we&#x2019;re confident it will fit into the input context size of the LLM, so this factor is partly model-dependent and partly data-dependent.</p><pre><code class="language-Python">from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index import get_response_synthesizer

# configure retriever
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)

# configure response synthesizer
response_synthesizer = get_response_synthesizer(
    service_context=service_context,
    text_qa_template=qa_prompt,
    response_mode=&quot;compact&quot;,
)

# assemble query engine
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer,
)
</code></pre><h2 id="asking-the-rag-engine-questions">Asking the RAG Engine Questions</h2><p>Now you can ask questions and receive answers based on the text.</p><pre><code class="language-Python">result = query_engine.query(&quot;How is a computer useful on a farm?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">A computer can be useful on a farm by supplementing the calculator,
typewriter, and file cabinet. It can help with repetitive analyses, 
data storage, and management decisions. It can also send and receive 
written or graphic messages by telephone. Additionally, a computer 
program for a farm operation could make recordkeeping and analysis 
easier and improve management abilities. However, the improvements 
in efficiency and cost-effectiveness might be hard to measure in 
dollars.
</code></pre><p>You can ask questions that have an answer from the text that the LLM would never have produced on its own:</p><pre><code class="language-Python">result = query_engine.query(&quot;How much memory does a computer need?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">48K or 64K of memory is needed for most agricultural programs. The 
amount of memory needed depends on the software program and 
recordkeeping requirements.
</code></pre><p>And you can ask questions that have no answer in the text:</p><pre><code class="language-Python">result = query_engine.query(&quot;Who is buried in Grant&apos;s tomb?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">No information. The context information does not provide any details 
about Grant&apos;s tomb.
</code></pre><h2 id="checking-the-rag-retrieval">Checking the RAG Retrieval</h2><p>You may want to check to see what texts were retrieved for a specific query. For example:</p><pre><code class="language-Python">result = query_engine.query(&quot;What is the address of AgriData Resources?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code>205 West Highland Ave. Milwaukee, WI 53203
</code></pre><p>To check the retrieval phase, we have to use the retriever object we created above:</p><pre><code class="language-Python">retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)
</code></pre><p>You can rerun the retrieval and then inspect the documents:</p><pre><code class="language-Python">retrieved_texts = retriever.retrieve(&quot;What is the address of AgriData Resources?&quot;)
for i, rt in enumerate(retrieved_texts):
  print(f&quot;Text {i+1}:\\n\\n{rt.text}\\n\\n&quot;)
</code></pre><p>Result:</p><pre><code class="language-Text">Text 1:

3. AgriData Network

AgriData is a private information and computing network specializing in
agriculture. It offers immediate access to more than 10,000 pages of
continuously updated business, financial, marketing, weather, and price
information, as well as analyses and recommendations from its own and
other reporters, analysts, economists, meteorologists, and researchers.
It offers several different services, including an online computing
service that allows users to access a library of microcomputer software
programs that can be transferred to the user&apos;s microcomputer; an
agricultural production technology service offering data bases from 40
land-grant universities and from agricultural, chemical, fertilizer,
equipment, seed, and feed companies; an &quot;electronic yellow pages,&quot; or
product service directory for farmers; and electronic mail.
  ADDRESS: AgriData Resources, Inc.
           205 West Highland Ave.
           Milwaukee, WI 53203

Text 2:

2. AGRICOIA

AGRICOIA is an online information service produced by the National
Agricultural Library (NAD of USDA), and is available commercially from
a number of sources (including DIALOG and Bibliographic Retrieval
Services). It provides comprehensive access to information on published
literature pertaining to agriculture.
AGRICOIA is the catalog and index for NAL and covers materials
published since 1970. It includes about 1.5 million citations.
AGRICOIA contains citations to worldwide published books, serial
titles, and journal articles on agriculture and related subjects. In
addition to bibliographic citations of published literature, the system
offers information through several specialized subfiles; these subfiles
include brucellosis (BRU), environmental impact statements covering
1977 and 1978 (ENV), and the Food and Nutrition Information Center,
which emphasizes human nutrition research and education and food
technology (FNC).
Librarians are the main users of this system.
  ADDRESS: To find out more about AGRICOIA, contact:
           Educational Resources Staff
           National Agricultural Library
           Room 1402
           Beltsville, MD 20705
</code></pre><h2 id="making-rag-work-for-you">Making RAG Work For You</h2><p>With Jina Embeddings, LlamaIndex, and Mixtral LLM, you can make your own RAG system that can answer questions about long documents, respond to requests based on a manual or FAQ, or just behave in funny or useful ways, based on a large document context.</p><p>And you can do all this without complex AI operations engineering or even training your models.</p><p>Jina AI is committed to helping you make the most of emerging AI technology. In our coming articles, we delve deep into the practical issues of using embedding models, like discussing ways of chucking documents for use in RAG and search applications. We will also have more integration tutorials and practical advice for text pre-processing and data curation.</p><p>Learn more from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings</a> and <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a> websites, or reach out to us at <a href="mailto:contact@jina.ai">contact@jina.ai</a> to discuss how Jina AI&#x2019;s experience can help your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4012 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain]]></title><description><![CDATA[See how companies are integrating SceneXplain with their existing infrastructure to power their product descriptions and storytelling]]></description><link>https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/</link><guid isPermaLink="false">658198e20bab3100012d2999</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Lisa Li]]></dc:creator><pubDate>Thu, 21 Dec 2023 15:00:26 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain"><p>We recently wrote about SceneXplain&apos;s new <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">JSON Schema Store</a>, which lets you use predefined JSON schemas to extract information from images in a structured format. In this post, we&apos;re going to see how that&apos;s used by our partner, <a href="http://www.akia.cn/eindex.asp?ref=jina-ai-gmbh.ghost.io">AKIA Carpet &amp; Rugs</a>, for building up an AI-powered product stories generation bot to empower sales.</p><p>In our prior examples, we&apos;ve primarily used either cURL or Python to access <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a>. But in this post, we&apos;re switching things up a bit by using JavaScript.</p><h2 id="about-akia-carpet-rugs">About AKIA Carpet &amp; Rugs</h2><p>In July 2008, Gary Chen founded AKIA Carpet &amp; Rugs, which is now marking its 15th year of operation. The team, initially inspired by traditional Asian craftsmanship, has been dedicated to developing contemporary aesthetic styles. AKIA has grown into a brand known for its unique fusion of modern art with traditional design and weaving techniques.</p><p>As a carpet manufacturer, AKIA specializes in a range of products including mid-to-high-end decorative carpets, tapestries, and carpets for specific projects. The company integrates design, research, production, and sales, both domestically and internationally. Known for innovative design and high-quality products, AKIA has earned a solid reputation in China&apos;s high-end carpet market.</p><h2 id="crafting-a-winning-product-story-with-scenexplain"><strong>Crafting a Winning Product Story with SceneXplain</strong></h2><p>AKIA Carpet &amp; Rugs primarily caters to the mid-to-high-end market, focusing on the aesthetic appeal of their products. Their clientele often looks for carpets that not only complement their home design but also express their unique taste. Recognizing this, AKIA collaborates with skilled designers to create a diverse range of styles, resulting in an extensive collection of carpet designs and images. The challenge lies in effectively communicating the artistic value of these designs to discerning customers, a crucial factor in attracting clients.</p><p>Previously, crafting compelling narratives for a large array of product images, akin to artworks, was a daunting task. This required copywriters who were not only skilled in writing but also knowledgeable in design and art. Additionally, the need to swiftly identify the perfect product image from an extensive collection to meet specific customer preferences was a significant challenge. Traditional image labeling methods, focusing on basic attributes like color, shape, and material, proved insufficient for customers who often describe their needs in more abstract terms.</p><p>SceneXplain offers a dual solution to these challenges. Its approach is based on narration rather than mere description, aiming to weave engaging stories around images. This aligns with SceneXplain&apos;s core philosophy: storytelling brings images to life. By providing stories that resonate with the artistic nature of AKIA&apos;s products, SceneXplain addresses their need for an intuitive, aesthetically aligned way of presenting their carpets.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1183" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Embrace Yourself&quot; is a piece of art in the form of a carpet, exuding minimalist charm and contemporary simplicity. This piece features the elegant, abstract depiction of a round, white figure at its center&#x2014;an embodiment of purity and serenity. With meticulously crafted black outlines that grace the soothing white background, this carpet tells a subtle but impactful visual story. Each line is placed with thoughtful precision, evoking emotions and depth without cluttering the visual space. The characteristic round head of the figure in the design adds a touch of futuristic whimsy, creating a space for imagination to soar. The light gray tones and stark white spaces between the lines further enhance the abstract quality, offering a calm and peaceful atmosphere to any room. Whether adorning a modern living area or a chic office space, &quot;Embrace Yourself&quot; promises to be more than just a carpet&#x2014;it is a promise of self-discovery and a celebration of space and form. Its simplicity and abstraction are not just visually appealing but are crafted to engage the observer in an almost meditative contemplation.</blockquote><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1189" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Dancer&quot; is an evocative piece of home decor where color and geometry resonate to a rhythm of their own. Adorning a rich navy blue background, this carpet features a dynamic composition of blue and orange lines and squares that glide across the fabric, suggesting movement and energy, much like a dancer in the spotlight. The striking orange lines dance diagonally, connecting a series of crisp white squares, which are themselves accented with black detailing, reminiscent of precise footwork on a dance stage. Unequivocally modern, &quot;Dancer&quot; employs the contrast of deep blues with vibrant oranges and whites to create an abstract visual narrative that is open to interpretation, yet commands attention. Each element on &quot;Dancer&quot; is painstakingly arranged to bring a sense of balance and fluidity, paralleling a choreographed performance that tells a story with every twist and turn. The pattern encourages the eye to leap and land much like a viewer watching an enthralling solo dance performance, making &quot;Dancer&quot; not just a carpet, but a conversation piece that captivates and inspires.</blockquote><h2 id="product-story-generation-a-holistic-solution">Product story generation: A holistic solution</h2><p>In AKIA&#x2019;s use case, their product manager wants to automatically create stories for all their products. The stories should use some high-quality examples as a guide, which contain aesthetic explanations. Their specific needs are:</p><ul><li>Textual descriptions of their products</li><li>Stories for each product that follow the examples they provide</li><li>Batch processing of images triggered by the chat channel</li></ul><p>Based on these requirements, visual question answering (VQA) is the best fit, because:</p><ul><li>Visual question answering outputs textual descriptions.</li><li>In the question, you can also provide a prompt in a specific format on demand.</li><li>You can inject your own examples into the prompt to guide the model&#x2019;s output.</li><li>Once you have the prompt&#x2019;s basic structure, you can convert it to a template with variables that can be automatically populated each time you use it.</li></ul><p>SceneXplain&#x2019;s API provides a wide range of options for configuring your request, including image captioning, alt-text generation, visual question answering, JSON output, and more.</p><p>Several fields are required to execute a VQA task via the API:</p><ul><li>API endpoint: <code>https://api.scenex.jina.ai/v1/describe</code></li><li>API key: <code>&apos;x-api-key&apos;: token ${YOUR_API_KEY}</code>. You can generate and manage your API key on our <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">API page</a>.</li><li>Request payload, which is your task configuration, providing the image you want to process, setting <code>question_answer</code> in the <code>features</code> property, and setting your prompt in the <code>question</code> property.</li></ul><p>Here&#x2019;s a code snippet for such an API call in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
			&quot;image&quot;: &quot;The image you want to process, it can be a base64 string or a URL&quot;,
      &quot;features&quot;: [
        &quot;question_answer&quot;
      ],
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
		&quot;question&quot;: &quot;your prompt&quot;
    }
  ]
};

const YOUR_API_KEY = &apos;your_generated_API_key_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_API_KEY}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
})
.then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>The payload&#x2019;s <code>data</code> property is an array that can have several configurations, meaning you can batch-process your images via the API.</p><h2 id="connecting-akia-to-scenexplain%E2%80%99s-api-via-bot">Connecting AKIA to SceneXplain&#x2019;s API via bot</h2><p>AKIA uses <a href="https://www.larksuite.com/en_eu?ref=jina-ai-gmbh.ghost.io">Lark</a> for their internal messaging, which is a Chinese application similar to Slack, Microsoft Teams, and Discord. An employee of AKIA can simply send a message to their SceneXplain chatbot that includes an image and a topic.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1821" height="1140" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/21700476455_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/21700476455_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/21700476455_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg 1821w" sizes="(min-width: 720px) 720px"></figure><p>The chatbot sends back a detailed description of the carpet. Here&#x2019;s how it would look in English:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="847" height="291" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/SceneX-carpet--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png 847w" sizes="(min-width: 720px) 720px"></figure><p>Behind the scenes, there&#x2019;s a middleware service that connects Lark to the SceneXplain API:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1000" height="221" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/workflow.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png 1000w"></figure><p>It shuttles the data between the two services and performs several key tasks:</p><ul><li>Message validation</li><li>API payload generation</li><li>API calling</li><li>Message formatting</li></ul><p>The process is:</p><ol><li>Receive image and topic in message from Lark chatbot</li><li>Check message format is valid. If not, return an error.</li><li>Base64-encode the image and wrap both it and the topic into a payload, using the topic as the question in visual question answering (VQA)</li><li>Send the payload to the API</li><li>API generates a description and sends that back</li><li>Format the message to fit Lark&#x2019;s API</li><li>Send the message back to the Lark chatbot</li></ol><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We&#x2019;re not going to go into the workings of the Lark API here. We want to keep this post as service-agnostic as possible, so it&#x2019;s relevant to whatever service you want to integrate with SceneXplain.</div></div><p>We&#x2019;re just going to focus on the middleware (&#x201C;Your service&#x201D; in the diagram above). All you need is a few lines of code to reformulate the request, pass it on, and then do the same for the response.</p><pre><code class="language-jsx">// Function to call SceneXplain `/describe` API
const describe = async (image: string, name: string, topic: string) =&gt; {
  // prepare payload
  const newBody = {
    data: [
      {
        image: image,
        features: [
          &quot;question_answer&quot;
        ],
        languages: [&apos;zh-CN&apos;],
        algorithm: &apos;Jelly&apos;,
        question: `your prompt, incorporating ${name} and ${topic}, plus optional example for desired output format for in-context learning`
      }
    ]
  }

  // call SceneXplain API
  try {
    const resp = await fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
      headers: {
        &apos;x-api-key&apos;: `token ${process.env.scenexKey}`,
        &apos;content-type&apos;: &apos;application/json&apos;
      },
      body: JSON.stringify(newBody),
      method: &apos;POST&apos;,
    });
    if (!resp.ok) {
      const error = await resp.text();
      throw error;
    }
    const data = await resp.json() as any;
    console.log(`describe result: ${JSON.stringify(data, null, 2)}`);
    if (data.code !== 200) throw data;
    const result = data.result[0];

    // get result in the required language
    return result.i18n[&apos;zh-CN&apos;];
  } catch (e) {
    console.log(`describe error: ${JSON.stringify(e, null, 2)}`);
    return &apos;&apos;;
  }
}
</code></pre><p>As you can see from the <code>question</code> field in the example payload above, you can include some example output to help the algorithm in generating the kind of description you desire. And, of course, you don&#x2019;t <em>have</em> to use JavaScript to build your middleware service - any programming language with an HTTP library can access <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API</a>.</p><h2 id="wrapping-up">Wrapping up</h2><p>Do you want to follow in AKIA&#x2019;s footsteps and use SceneXplain to build vivid product stories from your images and videos? Head over to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to get started. Or for business use cases, fill in our <a href="https://jina.ai/contact-sales/?ref=jina-ai-gmbh.ghost.io">sales form</a> and we&#x2019;ll be happy to roll out the red carpet.</p>]]></content:encoded></item><item><title><![CDATA[Multi-Agent Simulations in PromptPerfect: 𝑛 Heads Are Better Than One]]></title><description><![CDATA[Discover the real-world impact of multi-agent simulations and see practical examples of systems uniting individual strengths to tackle complex tasks, offering efficient and tailored solutions across various domains]]></description><link>https://jina.ai/news/multi-agent-simulations-in-promptperfect-n-heads-are-better-than-one/</link><guid isPermaLink="false">658177280bab3100012d296d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 19 Dec 2023 15:00:58 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><p>In this post, we&#x2019;re going to explore how multi-agent simulations are not just a conceptual framework but a tool with tangible applications in various domains. By delving into practical examples, we can see how these systems bring together the strengths of individual agents to address complex tasks and deliver solutions that are more efficient, comprehensive, and tailored to specific needs. These examples will illustrate the versatility and effectiveness of multi-agent simulations in real-world scenarios, demonstrating their value in diverse fields.</p><h2 id="a-quick-history-lesson">A quick history lesson</h2><p>In the ever-evolving landscape of technology and problem-solving, our journey has been marked by significant milestones. Initially, coding was our primary tool for instructing computers to perform tasks and solve problems. This era was defined by programming languages and algorithms, where solutions were hard-coded. As technology advances, we&#x2019;re transitioning to a more intuitive approach: prompting. This phase leverages artificial intelligence, where we can simply ask a system a question or present a problem, and it generates solutions based on its training and algorithms. This method is more flexible and user-friendly, catering to a wider range of problems without the need for extensive coding knowledge. However, even now, we&apos;re seeing the glimmerings of a newer era that promises to revolutionize how we approach problem-solving: the use of agents.</p><h2 id="multi-agent-simulations-as-easy-as-1-2-3">Multi-agent simulations: As easy as 1, 2, 3</h2><p>Before we dive into using multiple agents, let&#x2019;s define a few key terms: <em>Agent</em>, <em>action,</em> <em>environment</em>, and <em>simulation</em>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="861" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Multi-agent-sim.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png 861w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Agent:</strong> An <em>agent</em>, in the realm of artificial intelligence, performs actions to achieve a goal, like planning a vacation or collecting state-of-the-art research information online.</li><li><strong>Action:</strong> An <em>action</em> is a state change in an environment initiated by an agent. This may include talking to other agents, searching Google, or accessing external APIs.</li><li><strong>Environment:</strong> An <em>environment</em> is a text description that constrains what the agent can do. In a travel planning simulation, this may be a (virtual) travel agent office.</li><li><strong>Simulation:</strong> A <em>simulation</em> is what you get when you put all of the above together: An agent (or agents) performing actions in a given environment.</li></ul><h2 id="diving-deeper-into-agents">Diving deeper into agents</h2><p>AI agents can range from simple, rule-based systems to <em>complex entities capable of learning</em>. They can analyze data, learn from experiences, and make predictions or recommendations.</p><p>Agents share several characteristics:</p><ul><li><strong>Autonomy</strong>: Agents operate without direct human intervention, making decisions based on their programming and the data they encounter.</li><li><strong>Reactivity</strong>: They perceive their environment and respond to changes in real time, adapting their actions as necessary.</li><li><strong>Proactivity</strong>: Beyond just reacting, agents can take initiative, anticipating future states and planning actions accordingly.</li><li><strong>Social ability</strong>: Agents can communicate and collaborate with other agents or humans to achieve complex goals.</li></ul><p>However, despite these capabilities, single AI agents face intrinsic limitations:</p><ul><li><strong>Specialization limit</strong>: Like human experts, each agent typically specializes in a particular domain or task, lacking the breadth of knowledge to handle unrelated challenges.</li><li><strong>Complex problem solving</strong>: Tackling multi-dimensional problems that require diverse expertise and perspectives is beyond the scope of a single agent.</li><li><strong>Limited adaptability</strong>: While adaptable within their domain, agents may struggle with novel situations that fall outside their programmed parameters or training data.</li></ul><p>In summary, while individual AI agents represent a significant technological advancement, their true potential is unlocked when they operate as part of a multi-agent system, combining their strengths and compensating for each other&apos;s limitations.</p><h2 id="multiple-agents-n-heads-are-better-than-one">Multiple agents: <em>n</em> heads are better than one</h2><p>The solution to those limitations lies in multi-agent simulation. This approach involves using multiple AI agents, each with its specialty or focus area, working in tandem to solve complex problems. By collaborating, these agents can cover a broader range of issues and provide more comprehensive solutions than a single agent could. <a href="https://arxiv.org/pdf/2307.05300.pdf?ref=jina-ai-gmbh.ghost.io">Current research</a> shows that the performance of agents in specific tasks depends on the persona of the agent.</p><h2 id="building-a-vacation-planner-with-promptperfect">Building a vacation planner with PromptPerfect</h2><p>Let&#x2019;s consider planning a vacation as a practical example of a multi-agent simulation. This example will use two agents:</p><ul><li>The Travel Planner finds the best travel routes.</li><li>The Accommodation Finder finds affordable accommodation.</li></ul><p>Together, they provide a complete vacation plan tailored to your preferences and budget.</p><p>Follow the steps below to create a vacation planner in PromptPerfect:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_Simulation_for_Vacation_Planning__NbBdmWiUS5SC_QZ00lc21Q" width="640" height="640" allowfullscreen frameborder="0">

</iframe>
<!--kg-card-end: html-->
<p>We can now check the output to see the results. Here&#x2019;s one example action taken by the Travel Planner agent:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="1588" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--47--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--47--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png 1588w" sizes="(min-width: 720px) 720px"></figure><p>As you can see, the Travel Planner agent calls the flight API endpoint to find flights for a one-week trip in July. Since the agent is aware of the flight API&#x2019;s documentation, it sends the correct parameters with the request and retrieves the flight data.</p><p>Once it&#x2019;s done that, it can talk to the Accommodation Finder agent to find suitable properties in the chosen city. The Accommodation Finder, in turn, calls a hotel booking API to (you guessed it) arrange the accommodation.</p><h2 id="further-applications">Further applications</h2><p>This simple outline, although given a relatively small task, also applies to far more complex problems:</p><ul><li><strong>Paper Research</strong>: In academic or professional research, one agent could specialize in identifying relevant papers, another in summarizing content, and a third in identifying key themes and gaps in the research.</li><li><strong>Coding</strong>: For software development, one agent could get the user requirements, another write the code, and a third make the deployment.</li><li><strong>Online Shopping Deal Finder</strong>: This could involve one agent tracking price changes, another comparing product features, and a third assessing user reviews to find the best deals.</li></ul><p>As we&apos;ve seen, multi-agent simulations open up a world of possibilities that transcend the limitations of individual AI agents. Whether it&apos;s planning your dream vacation, conducting thorough research, developing software, or finding the best online shopping deals, these simulations offer tailored, efficient, and comprehensive solutions.</p><p>But the real magic happens when you dive in and experience it yourself. By visiting the PromptPerfect website, you&apos;ll have the opportunity to create and manage your own multi-agent simulations.</p><h2 id="get-started-with-agents">Get started with Agents</h2><p>So, why wait? Head over to the <a href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">PromptPerfect website</a> now, and start crafting your own simulations. Whether for work, study, or personal projects, discover the power and flexibility of multi-agent AI at your fingertips. Your journey into the future of problem-solving starts today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[A Tale of Two Worlds: EMNLP 2023 at Sentosa]]></title><description><![CDATA[Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas.]]></description><link>https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa/</link><guid isPermaLink="false">657c3d130bab3100012d2878</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Sat, 16 Dec 2023 07:03:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><p>The sun blazed down on the glistening sidewalks of Sentosa, a symphony of laughter and chatter filling the air. Tourists, decked in their holiday best, meandered through the vibrantmaze of Universal Studios, their faces alight with the joy of a day out in this fantasy land. The click of cameras capturing moments against the backdrop of thrilling rides and colorful parades was omnipresent. Nearby, the enticing aromas of international cuisines wafted through the air, luring guests to indulge in a culinary adventure.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1440" height="1080" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2911702641285_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2911702641285_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg 1440w" sizes="(min-width: 720px) 720px"></figure><p>In stark contrast, just a stone&apos;s throw away, nestled in the heart of this revelry, was the Resorts World Convention Centre. Here, the atmosphere was charged with a different kind of excitement. The halls buzzed not with the sound of holiday-making, but with the fervor of intellectual discourse. Young researchers and seasoned academics, their conference badges swaying gently with each step, engaged in animated discussions about the latest in NLP.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-3.png" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As I walked into this place, the difference was clear. On one side, there was the loud and happy noise of a holiday resort, full of life and excitement. On the other side, there was a serious and busy atmosphere where people talked about NLP , LLMs, ChatGPT, prompting, and Google&apos;s new Gemini model. It was like a movie scene &#x2013; where learning and fun came together in a surprising way.</p><p>In this blog, I&apos;ll share some observations of EMNLP 2023 and some of the most interesting papers and posters we discovered at the conference.</p><h2 id="emnlp-2022-to-2023-shifts">EMNLP: 2022 to 2023 Shifts</h2><p>Attending EMNLP 2022 in Abu Dhabi and now looking back at EMNLP 2023, I&apos;ve observed significant shifts in research focus and conference dynamics. These changes, driven by the swift advancements in AI, paint a vivid picture of our adaptive and forward-looking community. Below, I share a comparison of the two conferences, highlighting how our priorities and discussions have transformed in just a year.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>EMNLP</th>
<th>2022</th>
<th>2023</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Research Focus</td>
<td>Diverse range of NLP methods, with emphasis on traditional approaches.</td>
<td>Strong focus on Large Language Models (LLMs) and prompting techniques.</td>
</tr>
<tr>
<td>Research Trends</td>
<td>Interest in a wide array of topics, but no standout groundbreaking papers.</td>
<td>Shift towards LLM interpretability, ethics, agents, and multimodal reasoning.</td>
</tr>
<tr>
<td>Conference Atmosphere</td>
<td>A bit peculiar and pessimistic due to the release of ChatGPT and its implications on traditional NLP methods.</td>
<td>More confidence and adaptability among researchers in embracing new trends.</td>
</tr>
<tr>
<td>Research Diversity</td>
<td>Still exploring traditional methods like topic modeling, n-grams smoothing, and Bayesian methods (as seen in COLING 2022).</td>
<td>Rapid adaptation to newer approaches, moving away from older methods.</td>
</tr>
<tr>
<td>Relevance of Presented Work</td>
<td>Consistent with contemporary research trends at the time.</td>
<td>Fast-paced AI development made some empirical methods and results feel outdated by the time of the conference.</td>
</tr>
<tr>
<td>Conference Engagement</td>
<td>Enjoyment derived more from personal conversations and interactions than from paper presentations.</td>
<td>Increased focus on personal communication, with more time spent at poster sessions than listening to oral presentations.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="paper-highlights-from-emnlp-2023">Paper Highlights from EMNLP 2023</h2><p>At EMNLP 2023, several intriguing papers caught my attention, each addressing different aspects of NLP and pushing the boundaries of what&apos;s possible in this field. Let me share some of the highlights from these papers and my thoughts on them.</p><h3 id="hybrid-inverted-index-is-a-robust-accelerator-for-dense-retrieval">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.05521?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</div><div class="kg-bookmark-description">Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI$^2$ to achieve lossless retrieval quality with competitive efficiency across various index settings. Our code and checkpoint are publicly available at https://github.com/namespace-Pt/Adon/tree/HI2.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Peitian Zhang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Text embeddings have become very popular for information retrieval tasks. However, performing an exact embedding vector search requires one to calculate similarities between the embedding representation of the query and the embedding of each document. This becomes very slow for large datasets and leads to latencies that are not acceptable for real-world search applications. Therefore, many applications use approximated nearest neighbor search techniques to speed up the search system, whereby many of these techniques rely on vector quantization algorithms that learn an index of clusters based on the data distribution.&#xA0;</p><p>In addition, <strong>hybrid search</strong> has become popular which combines embedding-based search with traditional BM25-based search techniques. Usually, BM25 and embedding search are performed completely independently in hybrid search settings and only the result sets are combined.&#xA0;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/hype-and-hybrids-multimodal-search-means-more-than-keywords-and-vectors-2/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hype and Hybrids: Search is more than Keywords and Vectors</div><div class="kg-bookmark-description">Twenty years ago, &#x201C;hybrid&#x201D; was a term used only by botanists and chemists. Today, hybrid is booming&#x2026; even in search. Many search systems are rolling out hybrid search schemes with the latest AI. But is &#x201C;hybrid search&#x201D; really more than a buzzword?</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/Jina-AI-Website-Banners-Templates--21-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper proposed a method to train a joined index of two parts: a cluster selector and a term selector. The cluster selector performs a vector quantization to assign texts into buckets of nearby clusters and the term selector determines the most representative terms of a document BM25 can be used to assign it into buckets associated with those terms, however, this is not trainable and can therefore not adjust to the training data. Alternatively, one can determine the most representative terms in a document with a BERT model with an MLP hat which is applied on each token to determine a score. In this way the term selector becomes trainable. Then the cluster centroids and the BERT model are trained together by using the KL divergence loss with the embedding model as a teacher to obtain a distribution of similarity values. The results in the paper show that this method can retrieve more relevant documents in the same amount of time as standard ANN techniques like HNSW and IVF-PQ implementations.</p><h3 id="is-chatgpt-good-at-search-investigating-large-language-models-as-re-ranking-agents">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2304.09542?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</div><div class="kg-bookmark-description">Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model&#x2019;s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Weiwei Sun</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper investigates techniques to utilize LLMs for re-ranking documents. Re-ranking is usually performed in a search system after a first retrieval step to re-order the retrieved documents, e.g., to select the most relevant ones among them. Commonly used models are finetuned transformer models which are called <strong>cross encoders</strong>. Those receive as input a pair composed of a query and a document candidate and return a relevance score. Besides, more traditional learning-to-rank models like LambdaMart are also popular, especially in cases where the ranking is not only done based on semantic relevance itself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Cross-Encoders &#x2014; Sentence-Transformers documentation</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.sbert.net/_static/favicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.sbert.net/_static/logo.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Observing the strong NLP capabilities of LLMs, the authors of this paper wanted to investigate whether Models like GPT4 can be used to rank documents better. However, the limitation of those closed API-based models is usually that probability outputs are not accessible. Accordingly, the paper investigates techniques that rely only on prompting and the generated output text for re-ranking. The technique they propose inserts the documents together with an ID in the prompt and instructs the LLM to output a sequence of IDs with respect to the relevancy of the documents. In cases where the whole set of documents is too long to fit into the prompt, they apply a sliding window approach, where re-ranking is first performed on the documents with the lowest retrieval score obtained from the first-stage retriever. Then the most relevant documents according to the output are presented together with the documents in the next window of retrieval candidates and so on.</p><p>Since GPT-4 is too expensive and too slow for using it in a real-world setting, the authors propose distilling its ranking capabilities into a transformer-based cross-encoder model. The results show that even a comparably small model (440M parameters) distilled with this technique can outperform much larger state-of-the-art re-ranking models.</p><h3 id="large-language-models-can-self-improve">Large Language Models Can Self-Improve</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.11610?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Large Language Models Can Self-Improve</div><div class="kg-bookmark-description">Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate &#x201C;high-confidence&#x201D; rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and 63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Jiaxin Huang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/qRfOSz3zHzg1gctMzgRjYRjeTyg-1pMyv7JIZZFJlTyKOQPoHJYfuB0u_eGC8wVnCKvN471-8D-avjk7-0XBGfAAFL7q0jI_-7eMGnXvCpEK8L8Pe3VSu5-iV5QHpfnY-5YSUQqvgT-gUs2sOUoQw0w" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="236"></figure><p>For many tasks, LLMs achieve already good results in zero-shot settings. However, to improve their performance on specific tasks (beyond zero-shot) common techniques still utilize a lot of training data to fine-tune them. To reduce the amount of training data needed, this paper presents a technique that uses the LLM to &#x201C;self-improve&#x201D;. The main idea behind this approach is to augment existing training datasets by using data generated with the LLM itself.</p><p>This is achieved by using a dataset with only questions and without answers. First, a chain of thoughts (CoT) method is used to generate for each question a couple of different reasoning paths and answers by using a temperature greater than zero to make the generative text non-deterministic. By determining the answer with the highest frequency the probability of the answer being correct can be increased. The paper also shows that LLMs can be used to effectively estimate the confidence of an answer by investigating the consistency of the answer. If a high percentage of the answers is equal, this answer also has a high probability being correct. These most frequent answers and the corresponding reasoning paths can then be used to construct additional prompts for constructing additional training data. However, the authors propose not only to use those reasoning paths and answers but also to add examples in different formats, e.g., presenting the question without any reasoning path and using the question together with some generic instruction like &#x201C;Let&#x2019;s think step by step&#x201D;. Finally, this enhanced training dataset can be used to fine-tune the LLM on the specific task. In their evaluation the authors show this technique is effective to fine-tune LLMs with minimal training data but also that it generalizes well, as it improves the result on out-of-domain datasets.</p><h3 id="adapting-language-models-to-compress-contexts">Adapting Language Models to Compress Contexts</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2305.14788?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Adapting Language Models to Compress Contexts</div><div class="kg-bookmark-description">Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Alexis Chevalier</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/RDaQqjnEa45QEArhdPfuNKXsZReEQfT7Hg_zgr-DlkmlMeeXJAYP0kiKfpQ3Vj8iL_I2mK5JZ0evaoHZAdBDHSaep9Z49Qx1dt5JdCcMrfH8UF-nRy_ZO6GT7wLwR654p0_9CF4jD7soGg-zBXbSFNg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="496"></figure><p>Language models are usually constrained by limited context length. While there are various techniques like <strong>AliBi</strong> to construct language models that can handle larger context lengths, those techniques do not help in cases where you want to use an existing model that has only a very limited context length. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</div><div class="kg-bookmark-description">Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi&#x2019;s inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Ofir Press</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Instead, this paper addresses this problem by introducing a new method that allows one to fine-tune an exciting language model to apply it to longer context lengths. To achieve this, the model&apos;s vocabulary is extended by a fixed number of summary tokens, which are supposed to tell the model to summarize the content of the other tokens in the respective embeddings. During inference, the text input is segmented into shorter text sequences which are extended with the summary tokens and prepended with the output summary vectors of the previously processed segments. To fine-tune the model to handle long sequences, the model is trained on a next-token prediction task. Thereby, the model should make use of the information encoded in the summary embeddings of the previous segments. Notably, the segment length is varied during the training and the backpropagation is done for one document as a whole. In other words, the model is first applied to all sequences in the afore-described scheme before starting the backpropagation. The technique is evaluated by the authors on OPT models of various sizes and the 7-B-Llama-2 model. Besides standard language modeling tasks, the authors also show that the models can be effectively used to solve in-context-learning classification tasks with longer prompts or used for re-ranking. Here the re-ranking follows a language modeling approach in which the passages are re-ranked based on the language model&#x2019;s likelihood to generate the question from the given passage.&#xA0; </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.emnlp-main.249/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Improving Passage Retrieval with Zero-Shot Question Generation</div><div class="kg-bookmark-description">Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, Luke Zettlemoyer. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aclanthology.org/thumb/2022.emnlp-main.249.jpg" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><h2 id="poster-highlights-from-emnlp-2023">Poster Highlights from EMNLP 2023</h2><p>At EMNLP 2023, alongside the compelling paper presentations, the poster sessions were a hub of vibrant discussion and exchange. Here&apos;s a rundown of some standout posters I came across, each offering a unique glimpse into the ongoing research and development within the field of NLP.</p><h3 id="can-retriever-augmented-language-models-reason"><strong>Can Retriever-Augmented Language Models Reason?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2931702707518_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2931702707518_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A poster from McGill University examined whether Retriever-Augmented Language Models (RALMs) can effectively reason by balancing the capabilities of both the retriever and the language model. The research highlighted the potential shortcomings of retrievers in sourcing all necessary statements for reasoning, and how language models might falter in reasoning even when provided with the required statements. It was a deep dive into improving the interactive components of language models.</p><h3 id="contrastive-learning-based-sentence-encoders"><strong>Contrastive Learning-based Sentence Encoders</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2941702707520_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2941702707520_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Researchers from Tohoku University presented findings on how Contrastive Learning (CL) can induce sentence encoders to implicitly weight informative words, enhancing the model&apos;s understanding and processing of language. This approach could refine the way sentence encoders prioritize and process key elements in text, making them more efficient and effective.</p><h3 id="investigating-semantic-subspaces-of-transformer-sentence-embeddings"><strong>Investigating Semantic Subspaces of Transformer Sentence Embeddings</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2951702707522_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2951702707522_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A team from the University of Stuttgart showcased their work on probing the semantic subspaces of transformer sentence embeddings. By employing linear structural probing, they aimed to understand how different layers of a transformer contribute to semantic content processing, offering insights into the inner workings of sentence embeddings.</p><h3 id="can-pre-trained-vision-and-language-model-answer-visual-information-seeking-questions"><strong>Can Pre-trained Vision and Language Model Answer Visual Information-Seeking Questions?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2961702707523_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2961702707523_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>An intriguing poster by researchers from the Georgia Institute of Technology, Google Research, and Google DeepMind introduced a benchmark for testing the world knowledge in multimodal Large Language Models (LLMs) through Visual Information-Seeking Questions. The research focused on the capabilities of retrieval-augmented models and GPT-4 in answering questions that require visual understanding, pushing the envelope on multimodal AI.</p><h3 id="to-split-or-not-to-split-composing-compounds-in-contextual-vector-spaces"><strong>To Split or Not to Split: Composing Compounds in Contextual Vector Spaces</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2971702707525_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2971702707525_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>From the University of Stuttgart, a study delved into whether splitting compounds in contextual vector spaces is beneficial for the model&apos;s performance. The research explored the impact of compounds on semantic representation and processing, contributing to our understanding of compositional semantics in language models.</p><h3 id="subspace-chronicles-how-linguistic-information-emerges-shifts-and-interacts-during-language-model-training"><strong>Subspace Chronicles: How Linguistic Information Emerges, Shifts, and Interacts during Language Model Training</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2981702707526_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2981702707526_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>This poster detailed an exploration into the dynamics of linguistic information as it emerges and evolves during the training of language models. It&apos;s a fascinating look at the underpinnings of language model training and the critical learning phases that define their capabilities.</p><h3 id="theory-of-mind-for-multi-agent-collaboration-via-large-language-models"><strong>Theory of Mind for Multi-Agent Collaboration via Large Language Models</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2991702707528_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2991702707528_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Lastly, a poster outlined research on the Theory of Mind in Large Language Models and their application in multi-agent collaboration tasks. It&apos;s an exciting foray into the cognitive capabilities of LMs and their potential in collaborative environments.</p><h2 id="embeddings-roundtable-a-birds-of-a-feather-at-emnlp-2023">Embeddings Roundtable: A Birds of a Feather at EMNLP 2023</h2><p>During EMNLP 2023, we hosted a Birds of a Feather (BoF) session on embeddings that turned into a rich tapestry of insights and discussions. With a crowd of over 80 attendees, the session was an electrifying blend of sharp minds and cutting-edge topics.</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691.mp4" poster="https://img.spacergif.org/v1/1280x720/0a/spacer.png" width="1280" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h3 id="lightning-talks-and-panel-discussion">Lightning Talks and Panel Discussion</h3><p>The BoF session featured lightning talks by renowned researchers like Huiqiang, Hassan, Hwiyeol, Mattia, and Yang Chen. Each speaker brought a unique perspective to the table, sharing their latest findings in embedding research within NLP. The talks sparked an energizing dialogue that transitioned into a thought-provoking panel discussion.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3041702708673_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3041702708673_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3041702708673_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3031702708669_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3031702708669_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3031702708669_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3061702708681_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3061702708681_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3061702708681_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3051702708676_.pic-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3051702708676_.pic-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3051702708676_.pic-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3081702708695_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3081702708695_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3081702708695_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div></div></figure><p>The panel, graced by Sebastian Ruder, Nicola Cancedda, Chia Ying Lee, Michael G&#xFC;nther, and Han Xiao, delved deep into the intricacies of embedding technologies. They covered a breadth of topics, from the evolution of embeddings to their future in a world increasingly dominated by Generative AI and Large Language Models (LLMs).</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="720" height="541" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3011702708364_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg 720w" sizes="(min-width: 720px) 720px"></figure><h3 id="key-takeaways-from-the-panel">Key Takeaways from the Panel</h3><ol><li><strong>Diverse Perspectives on Embeddings:</strong><br>The panelists introduced themselves and their work with various embeddings, discussing the common threads and divergences they&apos;ve observed. They emphasized the nuanced differences in how embeddings behave depending on their design and application contexts.</li><li><strong>The Relevance of Embeddings Amidst Generative AI:</strong><br>With 2023&apos;s spotlight on LLMs, the panelists reflected on the enduring importance of embeddings. They highlighted that despite the LLM trend, embeddings retain a crucial role in understanding and processing language at a more granular level.</li><li><strong>Context Length in Embeddings vs. LLMs:</strong><br>A curious observation was the disparity in context length expansion between LLMs and embedding models. The panelists shed light on the technical and practical constraints that currently limit the context window in embedding models.</li><li><strong>Search and Generation:</strong><br>Addressing the assertion that &apos;search is an overfitted generation, and generation is an underfitted search,&apos; the panelists shared mixed views, sparking a lively debate on the interplay between search functions and generative capabilities.</li><li><strong>Future of RAG and Agent Models:</strong><br>Looking towards EMNLP 2024, the conversation turned to the prospective challenges and developments in Retrieval Augmented Generation (RAG) and agent models. The panelists hinted at their vision for the future integration of embeddings within these applications, recognizing the pivotal role they will continue to play.</li></ol><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3101702709471_.pic_hd-1.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3101702709471_.pic_hd-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3101702709471_.pic_hd-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3101702709471_.pic_hd-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3101702709471_.pic_hd-1.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3091702709471_.pic_hd.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3091702709471_.pic_hd.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3091702709471_.pic_hd.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3091702709471_.pic_hd.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3091702709471_.pic_hd.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="summary">Summary</h2><p>Wrapping up EMNLP 2023, I&apos;m buzzing with ideas and energized by the community&apos;s shared passion for pushing the boundaries of NLP. Our Embeddings BoF session was a hit &#x2013; the engagement and insights made it a highlight for me.</p><p>Looking to get hands-on with the future of embeddings? We are hiring! We&apos;re all about diving deep into long-context, multilingual, and multimodal embeddings. So, if you&apos;re up for the challenge, check out the open roles here and maybe I&apos;ll see you at our Berlin, Shenzhen, or Beijing office.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/internship/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Intern Program</div><div class="kg-bookmark-description">Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-internship.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Can&apos;t wait to see what we&apos;ll cook up by EMNLP 2024 in Miami. Until then, keep innovating, keep questioning, and let&apos;s keep the conversations going!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.2 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-2-update/</link><guid isPermaLink="false">657b24340bab3100012d27cb</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Thu, 14 Dec 2023 15:54:21 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3232">Release Note (<code>3.23.2</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" alt="Jina 3.23.2 Update"><p>This release contains 1 dependency update and 2 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.2?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.2 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.2) Release time: 2023-12-14 15:28:24 This release contains 1 dependency update and 2 bug fixes.
&#x2699; Dependency update
Update GRPC version requirements (#6110)
The grpc version re&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.2 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/66a744fcace7275ba964cb292fb9e91fd04678aa02883dd2f72c0c8ef1b2bc4a/jina-ai/jina/releases/tag/v3.23.2" alt="Jina 3.23.2 Update"></div></a></figure><h2 id="%E2%9A%99-dependency-update">&#x2699; Dependency update</h2><h3 id="update-grpc-version-requirements-6110">Update GRPC version requirements (<a href="https://github.com/jina-ai/jina/pull/6110?ref=jina-ai-gmbh.ghost.io">#6110</a>)</h3><p>The&#xA0;<code>grpc</code>&#xA0;version requirements have been updated to allow&#xA0;<code>grpcio&lt;=1.57.0</code>.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="better-handling-of-exceptions-in-dynamic-batching-6128">Better handling of Exceptions in dynamic batching (<a href="https://github.com/jina-ai/jina/pull/6128?ref=jina-ai-gmbh.ghost.io">#6128</a>)</h3><p>An issue was identified when a dynamic batch raises an Exception that could affect some unrelated requests.</p><h3 id="load-balancing-streaming-based-on-response-type-6122">Load-balancing streaming based on response type (<a href="https://github.com/jina-ai/jina/pull/6122?ref=jina-ai-gmbh.ghost.io">#6122</a>)</h3><p>When using&#xA0;<code>Deployment</code>&#xA0;locally with multiple replicas, a load-balancing process is added in front of the replicas. </p><p>The load balancer assumed all&#xA0;<code>GET</code>&#xA0;requests to be streaming, but this may not be true for user-added FastAPI endpoints. We have fixed this assumption and now use the response type to determine if a request is streaming.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[There's a Time and a Place: Unleashing Dynamic Variables in PromptPerfect]]></title><description><![CDATA[Incorporate users' time, date and location into your prompts. Plus contents of other websites, and outputs of other prompts themselves!]]></description><link>https://jina.ai/news/theres-a-time-and-a-place-unleashing-dynamic-variables-in-promptperfect/</link><guid isPermaLink="false">6571be77782cc90001042c6f</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 07 Dec 2023 14:59:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--29-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--29-.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><p>Back in the mists of time, PromptPerfect introduced its <a href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io">Prompt-as-a-Service</a> feature, allowing you to serve your prompts via a RESTful API and call them from Python, JavaScript or cURL.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">What&#x2019;s Next for Prompt Engineering? PromptPerfect&#x2019;s Prompt as a Service!</div><div class="kg-bookmark-description">Deploy prompts and flexible template prompts as REST API services, and integrate them into your applications with just a few clicks</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-publisher">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/06/Pic.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure><p>Let&#x2019;s imagine we&#x2019;re building an AI-powered recipe website. We want users to be able to click a button that suggests a delicious recipe for them. To do that, we can create a prompt and deploy it as a service. We would then incorporate that into our website via Prompt-as-a-Service&#x2019;s <a href="https://promptperfect.jina.ai/api?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">RESTful API.</a></p><p>For example, we could have a prompt that suggested recipes to users:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="760" height="943" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123047_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png 760w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Incorporating a RESTful API into your website is <i><em class="italic" style="white-space: pre-wrap;">way</em></i> outside the scope of this post, so we&#x2019;ll just use cURL moving forwards. But rest assured, you can do this in any language or framework with an HTTP library.</div></div><p>By calling the prompt, a user could get a new recipe each time:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">Certainly! Below is a recipe for a classic Italian dish, Spaghetti Carbonara. This dish is known for its creamy texture and rich flavors, achieved with minimal ingredients. Enjoy!</span></p><p><b><strong style="white-space: pre-wrap;">Spaghetti Carbonara</strong></b></p><p><b><strong style="white-space: pre-wrap;">Ingredients</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">400g spaghetti</span></li><li value="2"><span style="white-space: pre-wrap;">150g pancetta or guanciale, diced</span></li><li value="3"><span style="white-space: pre-wrap;">3 large eggs</span></li><li value="4"><span style="white-space: pre-wrap;">1 cup (100g) freshly grated Parmesan cheese, plus extra for serving</span></li><li value="5"><span style="white-space: pre-wrap;">2 cloves garlic, peeled and left whole</span></li><li value="6"><span style="white-space: pre-wrap;">2 tablespoons extra-virgin olive oil</span></li><li value="7"><span style="white-space: pre-wrap;">Salt and freshly ground black pepper</span></li><li value="8"><span style="white-space: pre-wrap;">A small handful of fresh parsley, chopped (optional)</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions</strong></b></p><p><b><strong style="white-space: pre-wrap;">Cook the Pasta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Bring a large pot of salted water to a boil.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the spaghetti and cook according to the package instructions until al dente.</span></li><li value="3"><span style="white-space: pre-wrap;">Reserve 1 cup of pasta water before draining the spaghetti.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Prepare the Sauce</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a small bowl, whisk together the eggs and grated Parmesan cheese until well combined. Set aside.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Cook the Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">While the pasta is cooking, heat the olive oil in a large skillet over medium heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the pancetta or guanciale and the whole garlic cloves. Cook, stirring frequently, until the meat is crispy and golden, about 5 minutes.</span></li><li value="3"><span style="white-space: pre-wrap;">Remove and discard the garlic cloves.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Combine Pasta and Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Add the drained spaghetti to the skillet with the pancetta.</span></li><li value="2"><span style="white-space: pre-wrap;">Toss well to coat the pasta in the pancetta&apos;s fat.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Make Carbonara</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Remove the skillet from the heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Quickly pour the egg and cheese mixture into the pasta, stirring vigorously with tongs or a fork until the eggs thicken but do not scramble.</span></li><li value="3"><span style="white-space: pre-wrap;">Add a little reserved pasta water if the sauce is too thick. The heat from the pasta will cook the eggs and melt the cheese to create a creamy sauce.</span></li><li value="4"><span style="white-space: pre-wrap;">Season generously with freshly ground black pepper and add a pinch of salt if needed.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Garnish and Serve</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Serve immediately with extra grated Parmesan cheese and a sprinkle of chopped parsley, if desired.</span></li></ul><p><span style="white-space: pre-wrap;">Enjoy your delicious Spaghetti Carbonara!</span></p><p><span style="white-space: pre-wrap;">This recipe serves 4 people and is best enjoyed fresh.</span></p></div>
        </div><h2 id="template-prompts">Template prompts</h2><p>Now, static prompts are fine and all, but we also allow template prompts. This means a user could set options (like ingredients or dietary restrictions) each time they called the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="614" height="467" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123701_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png 614w"></figure><p>Which would give the user something like:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;parameters&quot;: {&quot;ingredients&quot;:&quot;apples, cinnamon, nutmeg&quot;}}&apos;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><b><strong style="white-space: pre-wrap;">Classic Apple Crisp Recipe</strong></b></p><p><span style="white-space: pre-wrap;">Enjoy the sweet and spicy flavors of apples, cinnamon, and nutmeg in this deliciously easy-to-make apple crisp. Perfect for a cozy night in or a festive gathering!</span></p><p><b><strong style="white-space: pre-wrap;">Ingredients:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">6 medium-sized apples, peeled, cored, and thinly sliced</span></li><li value="2"><span style="white-space: pre-wrap;">1 teaspoon ground cinnamon</span></li><li value="3"><span style="white-space: pre-wrap;">1/4 teaspoon ground nutmeg</span></li><li value="4"><span style="white-space: pre-wrap;">3/4 cup granulated sugar</span></li><li value="5"><span style="white-space: pre-wrap;">2 tablespoons lemon juice</span></li></ul><p><b><strong style="white-space: pre-wrap;">For the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">3/4 cup all-purpose flour</span></li><li value="2"><span style="white-space: pre-wrap;">3/4 cup old-fashioned rolled oats</span></li><li value="3"><span style="white-space: pre-wrap;">1/2 cup light brown sugar, packed</span></li><li value="4"><span style="white-space: pre-wrap;">1/2 teaspoon baking powder</span></li><li value="5"><span style="white-space: pre-wrap;">1/4 teaspoon ground cinnamon</span></li><li value="6"><span style="white-space: pre-wrap;">1/4 teaspoon salt</span></li><li value="7"><span style="white-space: pre-wrap;">1/3 cup unsalted butter, melted</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions:</strong></b></p><p><b><strong style="white-space: pre-wrap;">Preheat Oven and Prepare Apples:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Preheat your oven to 350&#xB0;F (175&#xB0;C).</span></li><li value="2"><span style="white-space: pre-wrap;">In a large mixing bowl, combine the sliced apples with granulated sugar, 1 teaspoon cinnamon, nutmeg, and lemon juice. Toss to coat the apples evenly.</span></li><li value="3"><span style="white-space: pre-wrap;">Transfer the apple mixture to a greased 9-inch square baking dish, spreading them out evenly.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Make the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a separate bowl, mix the flour, oats, brown sugar, baking powder, 1/4 teaspoon cinnamon, and salt.</span></li><li value="2"><span style="white-space: pre-wrap;">Pour the melted butter over the dry ingredients and mix until the mixture is crumbly.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Assemble and Bake:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Sprinkle the crumbly oat topping evenly over the apples in the baking dish.</span></li><li value="2"><span style="white-space: pre-wrap;">Bake in the preheated oven for about 45 minutes, or until the topping is golden brown and the apples are bubbling around the edges.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Serve:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Allow the apple crisp to cool slightly before serving.</span></li><li value="2"><span style="white-space: pre-wrap;">Serve warm, optionally with a scoop of vanilla ice cream or a dollop of whipped cream.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Notes:</strong></b></p><ul><li value="1"><b><strong style="white-space: pre-wrap;">Choosing Apples:</strong></b><span style="white-space: pre-wrap;"> For the best texture and flavor, use a mix of tart and sweet apples like Granny Smith and Honeycrisp.</span></li><li value="2"><b><strong style="white-space: pre-wrap;">Storage:</strong></b><span style="white-space: pre-wrap;"> Leftover apple crisp can be stored in the refrigerator for up to 3 days. Reheat in the oven or microwave before serving.</span></li></ul></div>
        </div><h2 id="a-time-and-a-place-for-everything">A time and a place for everything</h2><p>Now we&apos;re taking it up a notch with environmental awareness. Rather than the end-user having to specify where (and when) they are, you can choose to have that information automatically inserted. That means you can create a prompt once, and end-users will get different results depending on their location, date, and time.</p><p>Since recipes don&#x2019;t usually need a specified time and place, let&#x2019;s consider a new example: Building a website to suggest activities for a user to take part in:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="759" height="944" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124457_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png 759w" sizes="(min-width: 720px) 720px"></figure><p>We also need to enable Environment Awareness:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="611" height="110" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124710_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png 611w"></figure><p>Now, I (in Berlin at noon) get something like this when I call the prompt:</p><pre><code>- Visit the Berlin Wall Memorial at Bernauer Stra&#xDF;e
- Explore the Museum Island, especially the Pergamon Museum
- Take a walk through the Tiergarten Park
- Enjoy panoramic views from the Berliner Fernsehturm (TV Tower)
- Discover German history at the German Historical Museum
- Stroll along the East Side Gallery
- Check out the Brandenburg Gate at dusk
- Experience contemporary art at the Hamburger Bahnhof museum
- Shop or window-shop at Kurf&#xFC;rstendamm
- Relax in the evening at a traditional German beer garden&quot;
</code></pre><p>Whereas if my colleague in Tokyo called the prompt, she&apos;d get:</p><pre><code>- Visit Tokyo Skytree for panoramic views of the city at night.
- Explore the vibrant streets of Shibuya and see the famous Shibuya Crossing.
- Enjoy the illuminated Tokyo Tower and consider an evening visit.
- Take a stroll through the historic Asakusa district and see Senso-ji Temple.
- Experience the nightlife in Roppongi, known for its clubs and bars.
- Relax at Odaiba Seaside Park and enjoy the night view of Rainbow Bridge.
- Try out some local izakayas (Japanese pubs) for food and drinks.
- Visit a themed caf&#xE9;, like a cat caf&#xE9; or an owl caf&#xE9;, for a unique experience.
- Check out the latest gadgets and tech in Akihabara, the electronics district.
- Attend a traditional Kabuki performance at Kabukiza Theatre in Ginza (if available).
- Indulge in a sushi dinner at one of Tokyo&apos;s renowned sushi restaurants.
</code></pre><p>These variables are great when you&apos;re crafting prompts that deliver relevant output based on the user&#x2019;s time, date, and location. This is ideal for building websites and apps with a global audience.</p><h2 id="fetching-remote-data">Fetching remote data</h2><p>We can go even further though. With the crawler feature, you can download the main body text of the URL you specify:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="736" height="371" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125131_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png 736w" sizes="(min-width: 720px) 720px"></figure><p>This makes it ideal for, say, a summarizer/translator prompt. Here we summarize an article and convert it to a sea shanty:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="608" height="402" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125339_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png 608w"></figure><p>If I pass the URL for this story on <a href="https://www.cnn.com/2023/11/18/world/elon-musk-spacex-starship-launch-scn/index.html?ref=jina-ai-gmbh.ghost.io">Elon Musk&apos;s spaceship exploding</a>, I get this (abridged) output:</p><pre><code class="language-text">(Verse 1)
Oh, gather &apos;round, me hearties, and a tale I&apos;ll tell to thee,
Of SpaceX&apos;s Starship, and its flight so brief at sea.
Launched on a morn in November, with power so fierce and grand,
She climbed into the heavens, but &apos;twas not as Musk had planned.

(Chorus)
Heave ho, lift high, to the stars we aim to fly,
But the rocket&apos;s dreams did shatter, and into the sea did die.
Heave ho, lift high, with a fiery tail in sky,
SpaceX&apos;s Starship faltered, and we&apos;re left to wonder why.

(Verse 2)
The booster and the spacecraft, they parted with a roar,
The engines blazed like comets, as they&apos;d never done before.
The booster met its ending, in a ball of flame it fell,
While Starship soared a moment more, then silence cast its spell.
</code></pre><h2 id="calling-other-prompt-services">Calling other prompt services</h2><p>The above example wasn&apos;t exactly modular. I had both the summarizer and shantyizer in one prompt. That isn&apos;t so useful if I want to shanty all my things in full detail. Luckily, you can also call one prompt from another, allowing for a lot more modularity:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="762" height="493" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125614_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png 762w" sizes="(min-width: 720px) 720px"></figure><p>So, if I create a summarizer prompt (named <code>summarizer</code>, which has the sole function of summarizing a web page)&#x2026;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_130155_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="592" height="387"></figure><p>&#x2026;I can easily call it from a new shantyizer prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125751_Chrome-1.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="591" height="393"></figure><p>Sending a URL to the shantyizer prompt in turn processes the URL through the summarizer to download and summarize its contents. And of course, the summarizer can be used in any other prompt you like, acting more like a traditional programming language function.</p><p>That opens up many more possibilities. Especially when it comes to more complex tasks like cooking a dish. If I&#x2019;m cooking, for any given recipe I may want <em>one or more</em> of the following steps, but perhaps not all steps for all recipes.</p><ul><li>Download the content of a recipe page</li><li>If it&apos;s not in English, translate it (if I&apos;m cooking an authentic Chinese recipe, I want it from a real Chinese website)</li><li>Make it vegetarian (if I&apos;m eating with vegetarian friends)</li><li>Convert it to metric (because <a href="https://www.boredpanda.com/imperial-measurement-system-twitter-rant-innesmck/?ref=jina-ai-gmbh.ghost.io">reasons</a>)</li><li>Change serving size (depending on how many people I&apos;m eating with)</li></ul><h2 id="exercise-for-the-reader">Exercise for the reader</h2><p>As you can imagine, if I&apos;m eating alone (and I&apos;m not a vegetarian), my needs are quite different from when I&apos;m eating with my buddies at the local <a href="https://www.snopes.com/fact-check/peta-rename-fish-sea-kittens/?ref=jina-ai-gmbh.ghost.io">sea kitten appreciation society</a>. Either way, it means combining several of these &quot;functions&quot;.</p><p>We can also go multi-modal, combining text and image prompts:</p><ul><li>Create an image generation prompt from the given recipe</li><li>Generate an image of how the food should look</li></ul><p>The exercise for you, dear reader, is to build a prompt that performs the above recipe steps (or similar). Share your results on our Discord!</p><h2 id="make-the-magic-happen">Make the magic happen</h2><p>To get started with magic variables, head to <a href="http://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io">promptperfect.jina.ai</a> and get started. Let us know what you cook up on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord</a>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Dify.AI integrates Jina Embeddings for RAG]]></title><description><![CDATA[Dify.AI, a leading open-source platform specialized in creating generative AI applications, is now leveraging Jina Embeddings v2! ]]></description><link>https://jina.ai/news/dify-ai-integrates-jina-embeddings-for-rag/</link><guid isPermaLink="false">65703e6d782cc90001042bca</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 06 Dec 2023 15:00:31 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" alt="Dify.AI integrates Jina Embeddings for RAG"><p>Online LLM application development platform <a href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Dify.AI</a> has integrated the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 API</a> in its innovative AI toolkit for instant access when building and hosting LLM applications. All you need to do is add your Jina Embeddings API key via their intuitive web interface to get the full power of Jina AI&#x2019;s industry-leading embedding models in your RAG (retrieval-augmented generation) applications.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI x Jina AI&#xFF1A;Dify now Integrates Jina Embedding Model - Dify Blog</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Dify Blog</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/2OftM3MmDVHkLcxaNn58ZNWaE.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><h2 id="integrating-embeddings-in-rag">Integrating Embeddings in RAG</h2><p>Current AI architectures today have no direct way to integrate outside information sources. The model itself encodes information from its training data with varying levels of accuracy, and it is impractical to retrain the model every time there is new, potentially useful data that could be incorporated into it.</p><p>For example, I asked <a href="https://chat.jina.ai/chat?ref=jina-ai-gmbh.ghost.io">JinaChat</a> a question about current events:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1408" height="1654" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.48.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1408w" sizes="(min-width: 720px) 720px"></figure><p>The only way to ensure that an LLM has the information needed to answer a factual question is to provide it in the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1316" height="1186" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.53.33.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1316w" sizes="(min-width: 720px) 720px"></figure><p>Naturally, an LLM that only answers questions correctly if you include the answer in your question isn&#x2019;t very useful. This has led to a body of techniques called <strong>Retrieval-Augmented Generation</strong> or <strong>RAG</strong>. RAG is a framework installed around an LLM that searches external information sources for materials that might contain the information needed to answer a user&#x2019;s request and then presents them, with the user&#x2019;s prompt, to the LLM.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-2.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="2000" height="938" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-2.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This strategy has the added benefit that LLMs hallucinate much less when they are expected to handle a given text rather than recall things they might partially remember from training.</p><h2 id="leveraging-jina-embeddings-superior-performance">Leveraging Jina Embeddings Superior Performance</h2><p><a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a> has integrated Jina Embeddings v2 to enhance retrieval quality for RAG prompting. Jina AI&#x2019;s models provide <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">state-of-the-art accuracy in RAG applications</a>, and <a href="https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io">with an input window of 8,192 tokens</a>, they can support much larger and more complex questions than most competing models at a much lower price.</p><p>You can now use Jina Embeddings in your LLM projects via <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s intuitive application builder, as shown in the video below or in the <a href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">post on Dify.AI&apos;s blog</a>:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396.mp4" poster="https://img.spacergif.org/v1/1244x720/0a/spacer.png" width="1244" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:38</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h2 id="get-involved">Get Involved</h2><p>Check out <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s LLM application builder and hosting service for yourself. You can get a free tester token from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina AI website to use Jina Embeddings</a> to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI &#xB7; The Innovation Engine for Generative AI Applications</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/wh4qGCAanwpqHs0Kot8VLBSty4.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3873 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure>]]></content:encoded></item></channel></rss>
<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.89</generator><lastBuildDate>Wed, 14 Aug 2024 20:20:30 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc.]]></title><description><![CDATA[We had a blast at ICML 2024 in Vienna, and we want to share with you everything we said, saw, and learned.]]></description><link>https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc/</link><guid isPermaLink="false">66b38ec355fd850001d38602</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Florian HÃ¶nicke]]></dc:creator><pubDate>Wed, 07 Aug 2024 17:09:51 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/08/icml-banner.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/icml-banner.jpg" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc."><p>The <a href="https://icml.cc/?ref=jina-ai-gmbh.ghost.io">International Conference on Machine Learning</a> is one of the most prestigious conferences in the machine learning and artificial intelligence community and held its 2024 meeting in Vienna from 21 to 27 July this year. </p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-1.png" class="kg-image" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc." loading="lazy" width="2000" height="956" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/08/image-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>The conference was a 7-day intensive learning experience, with oral presentations and opportunities to exchange ideas directly with other researchers. There is a great deal of interesting work going on in reinforcement learning, AI for the life sciences, representation learning, multi-modal models, and of course in core elements of AI model development. Of particular importance was the tutorial on the <a href="https://huggingface.co/collections/zhuzeyuan/physics-of-language-models-series-6615c5247dc4e8388b2a846f?ref=jina-ai-gmbh.ghost.io">Physics of Large Language Models</a>, which extensively explored the inner workings of LLMs and offered convincing answers to the question of whether LLMs memorize information or apply reasoning when saying things.</p><h2 id="our-work-on-jina-clip-v1">Our Work on Jina-CLIP-v1</h2><p>We gave a <a href="https://jina-ai-gmbh.ghost.io/content/files/2024/08/Jina_CLIP_Poster_ICML.pdf" rel="noreferrer">poster presentation</a> of <a href="https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io">the work behind our new multi-modal model</a> <code>jina-clip-v1</code> as part of the workshop <a href="https://icml.cc/virtual/2024/workshop/29957?ref=jina-ai-gmbh.ghost.io">Multi-modal Foundation Models meet Embodied AI</a>. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc."><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc."></div></a></figure><p>Meeting and discussing our work with international colleagues working in many fields was very inspiring. Our presentation enjoyed lots of positive feedback, with many people interested in the way Jina CLIP unifies multi-modal and uni-modal contrastive learning paradigms. Discussions ranged from the limitations of the CLIP architecture to extensions to additional modalities all the way to applications in peptide and protein matching.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/08/Jina_MG_ICML_poster_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/08/Jina_MG_ICML_poster.mp4" poster="https://img.spacergif.org/v1/1138x640/0a/spacer.png" width="1138" height="640" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/08/Jina_MG_ICML_poster_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">3:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Michael G&#xFC;nther presents Jina CLIP</span></p></figcaption>
        </figure><h2 id="our-favorites">Our Favorites </h2><p>We had the opportunity to discuss a lot of other researchers&#x2019; projects and presentations, and here are a few of our favorites:</p><h3 id="plan-like-a-graph-plag">Plan Like a Graph (PLaG)</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Lin, F., La Malfa, E., Hofmann, V., Yang, E. M., Cohn, A., &amp; Pierrehumbert, J. B. (2024). <i><em class="italic" style="white-space: pre-wrap;">Graph-Enhanced Large Language Models in Asynchronous Plan Reasoning.</em></i> <a href="https://arxiv.org/abs/2402.02805?ref=jina-ai-gmbh.ghost.io">arXiv:2402.02805</a></div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/HNumeUKs6P8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="ICML: Fangru Lin: An Easy Trick To Improve Your LLM Results"></iframe></figure><p>Many people know &quot;Few-Shot Prompting&quot; or &quot;Chain of Thought prompting&quot;. <a href="https://www.linkedin.com/in/fangru-lin-oxford/?ref=jina-ai-gmbh.ghost.io">Fangru Lin</a> presented a new and better method was presented at ICML: <em>Plan Like a Graph (PLaG)</em>.</p><p>Her idea is simple: A task given to an LLM is decomposed into sub-tasks that an LLM can solve either in parallel or sequentially. These sub-tasks form an execution graph. Executing the whole graph solves the high-level task.</p><p>In the video above, Fangru Lin explains the method using an easily understandable example. Note that even though this improves your results, LLMs still suffer from drastic degradation when task complexity increases. That said, it&#x2019;s still a great step in the right direction and provides immediate practical benefits.</p><p>For us, it&apos;s interesting to see how her work parallels our prompt applications at <a href="https://www.linkedin.com/company/jinaai/?ref=jina-ai-gmbh.ghost.io">Jina AI</a>. We have already implemented a graph-like prompt structure, however, dynamically generating an execution graph as she had is a new direction that we&#x2019;ll explore.</p><h3 id="discovering-environments-with-xrm">Discovering Environments with XRM</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Pezeshki, M., Bouchacourt, D., Ibrahim, M., Ballas, N., Vincent, P., &amp; Lopez-Paz, D. (2024). <i><em class="italic" style="white-space: pre-wrap;">Discovering Environments with XRM</em></i>. <a href="https://arxiv.org/abs/2309.16748?ref=jina-ai-gmbh.ghost.io">arXiv:2309.16748</a></div></div><p>This paper presents a simple algorithm to discover training environments that can cause a model to rely on features that correlate with the labels but do not induce an accurate classification/relevancy. A famous example is the waterbirds dataset (see <a href="https://arxiv.org/abs/1911.08731?ref=jina-ai-gmbh.ghost.io">arXiv:1911.08731</a>), which contains photos of birds against different backgrounds that should be classified as either waterbirds or landbirds. During training, the classifier detects whether the background in the images shows water or not instead of relying on the features of the birds themselves. Such a model will misclassify waterbirds if there is no water in the background.</p><p>To mitigate this behavior, one needs to detect samples where the model relies on misleading background features. This paper presents the XRM algorithm to do this.</p><p>The algorithm trains two models on two distinct parts of the training dataset. During training, the labels of some samples get flipped. Specifically, this happens if the other model (which is not trained on the respective sample) classifies a sample differently. In this way, the models are encouraged to rely on spurious correlations. Afterward, you can extract samples from the training data where a label predicted by one of the models differs from the ground truth. Later on, one can use this information to train more robust classification models, for example, with the <a href="https://github.com/kohpangwei/group_DRO?ref=jina-ai-gmbh.ghost.io">Group DRO algorithm</a>.</p><h3 id="cut-your-llm-evaluation-costs-by-a-factor-of-140"> Cut Your LLM Evaluation Costs by a Factor of 140!</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Maia Polo, F., Weber, L., Choshen, L., Sun, Y., Xu, G., &amp; Yurochkin, M. (2024). <i><em class="italic" style="white-space: pre-wrap;">tinyBenchmarks: evaluating LLMs with Fewer Examples</em></i>. <a href="https://arxiv.org/abs/2402.14992?ref=jina-ai-gmbh.ghost.io">arXiv:2402.14992</a></div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/qnW-hp6IYHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="ICML: Felipe Maia Polo: Cut Your LLM Evaluation Costs by A Factor of 140!"></iframe></figure><p>Yes, you heard right. With this one trick, the cost of LLM Evaluation can be reduced to a tiny fraction.</p><p>The core idea is simple: Remove all evaluation samples that test the same model capability. The math behind it is less straightforward but well explained by <a href="https://www.linkedin.com/in/felipemaiapolo/?ref=jina-ai-gmbh.ghost.io">Felipe Maia Polo</a> who presented his work during the poster session. Note that the reduction by a factor of 140 applies to the popular MMLU dataset (Massive Multitask Language Understanding). For your own evaluation datasets, it depends on how much the evaluation results of the samples correlate with each other. Maybe you can skip many samples or just a few.</p><p>Just give it a try. We&apos;ll keep you posted on how much we at <a href="https://www.linkedin.com/company/jinaai/?ref=jina-ai-gmbh.ghost.io">Jina AI</a> were able to reduce eval samples.</p><h3 id="contrasting-multiple-representations-with-the-multi-marginal-matching-gap">Contrasting Multiple Representations with the Multi-Marginal Matching Gap</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Piran, Z., Klein, M., Thornton, J., &amp; Cuturi, M. (2024). <i><em class="italic" style="white-space: pre-wrap;">Contrasting Multiple Representations with the Multi-Marginal Matching Gap</em></i>. <a href="https://arxiv.org/abs/2405.19532?ref=jina-ai-gmbh.ghost.io">arXiv:2405.19532</a></div></div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.29.25.png" class="kg-image" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc." loading="lazy" width="1346" height="752" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-01-at-18.29.25.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-01-at-18.29.25.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.29.25.png 1346w" sizes="(min-width: 720px) 720px"></figure><p>This work addresses a common challenge in contrastive learning: Most contrastive loss functions like the InfoNCE loss operate on pairs of data points and measure the distance between positive pairs. To expand on positive tuples of size k &gt; 2, contrastive learning usually tries to reduce the problem to multiple pairs and accumulate pairwise losses for all positive pairs. The authors here propose the M3G (Multi-Marginal Matching Gap) loss, a modified version of the Sinkhorn algorithm, that solves the Multi-Marginal Optimal Transport problem. This loss function can be used in scenarios where the datasets consist of positive tuples with size k &gt; 2, for example, &gt;2 images of the same object, multi-modal problems with three or more modalities, or a SimCLR extension where with three or more augmentations of the same image. Empirical results show that this method outperforms the naive reduction of the problem to pairs.</p><h3 id="no-need-for-ground-truth">No Need for Ground Truth!</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Robertson, Z., Cha, H., Sheha, A., &amp; Koyejo, S. (2024). <i><em class="italic" style="white-space: pre-wrap;">Implementability of Information Elicitation Mechanisms with Pre-Trained Language Models</em></i>. In <i><em class="italic" style="white-space: pre-wrap;">ICML 2024 Workshop on Theoretical Foundations of Foundation Models</em></i>. URL <a href="https://openreview.net/forum?id=QqMnRGlRJk&amp;ref=jina-ai-gmbh.ghost.io">https://openreview.net/forum?id=QqMnRGlRJk</a></div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/Hj9fiPpp7TQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="ICML: Zachary Robertson: No Need for Ground Truth!"></iframe></figure><p><a href="https://www.linkedin.com/in/zrobertson466920/?ref=jina-ai-gmbh.ghost.io">Zachary Robertson</a> from <a href="https://www.linkedin.com/company/stanford-university/?ref=jina-ai-gmbh.ghost.io">Stanford University</a> presented his work on evaluating your LLM without any labeled data. Note that even though this is theoretical work, it has a lot of potential for scalable oversight of advanced AI systems. This isn&#x2019;t for casual LLM users, but if you work on LLM evaluation it&#x2019;s definitely a thing you want to look into. We can see already that we could evaluate our agents at Jina AI that way. We&apos;ll share the results once we run the first experiments.</p><h3 id="is-model-collapse-inevitable-breaking-the-curse-of-recursion-by-accumulating-real-and-synthetic-data">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Gerstgrasser, M., Schaeffer, R., Dey, A., Rafailov, R., Sleight, H., Hughes, J., Korbak, T., Agrawal, R., Pai, D., Gromov, A., Roberts, D. A., Yang, D., Donoho, D. L., &amp; Koyejo, S. (2024). <i><em class="italic" style="white-space: pre-wrap;">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</em></i>. <a href="https://arxiv.org/abs/2404.01413?ref=jina-ai-gmbh.ghost.io">arXiv:2404.01413</a></div></div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/Untitled--88-.png" class="kg-image" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc." loading="lazy" width="1661" height="916" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/Untitled--88-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/Untitled--88-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/Untitled--88-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/Untitled--88-.png 1661w" sizes="(min-width: 720px) 720px"></figure><p>Multiple articles (like this <a href="https://www.nature.com/articles/s41586-024-07566-y?ref=jina-ai-gmbh.ghost.io"><em>Nature</em> article</a>) recently predicted that the performance of newly trained models may become worse over time because the training data is crawled from the web contains an increasingly high amount of synthetic data.</p><p>Our colleague Scott Martens has also published <a href="https://jina.ai/news/when-ai-makes-ai-synthetic-data-model-distillation-and-model-collapse/?ref=jina-ai-gmbh.ghost.io">an article about model collapse</a> and discussed cases where synthetic data can be useful for model training.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/when-ai-makes-ai-synthetic-data-model-distillation-and-model-collapse/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">When AI Makes AI: Synthetic Data, Model Distillation, And Model Collapse</div><div class="kg-bookmark-description">AI creating AI! Is it the end of the world? Or just another tool to make models do value-adding work? Let&#x2019;s find out!</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc."></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image--20-.png" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc."></div></a></figure><p>Model training runs might collapse because training data is produced by an earlier version of the model or a model trained on the same data. This paper runs experiments that show a slightly different picture: A collapse only happens when <em>replacing</em> real data with synthetic, which was done in previous experiments. However, when <em>augmenting</em> real data with additional synthetic data, there&#x2019;s no measured change in the performance of the resulting models. These results suggest that something like model collapse won&#x2019;t happen. However, it again proves that using additional synthetic data won&#x2019;t help to train a model that is generally superior to the model used to create said synthetic data points.</p><h3 id="brain-surgery-for-ai-is-now-possible">Brain Surgery for AI Is Now Possible</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Singh, S., Ravfogel, S., Herzig, J., Aharoni, R., Cotterell, R., &amp; Kumaraguru, P. (2024). <i><em class="italic" style="white-space: pre-wrap;">Representation Surgery: Theory and Practice of Affine Steering</em></i>. <a href="https://arxiv.org/abs/2402.09631?ref=jina-ai-gmbh.ghost.io">arXiv:2402.09631</a></div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/UFYbpl5wAXs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="ICML: Shashwat Singh Shauli: Brain Surgery for AI Is Now Possible"></iframe></figure><p>Let&#x2019;s say you want to predict someone&#x2019;s profession but not their gender. This work from Google Research, ETH Z&#xFC;rich, International Institute of Information Technology Hyderabad (IIITH), and Bar-Ilan University shows how steering vectors and covariance matching can be used to control bias.</p><h3 id="magiclensself-supervised-image-retrieval-with-open-ended-instructions">MagicLens - Self-Supervised Image Retrieval with Open-Ended Instructions</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Zhang, K., Luan, Y., Hu, H., Lee, K., Qiao, S., Chen, W., Su, Y., &amp; Chang, M.-W. (2024). <i><em class="italic" style="white-space: pre-wrap;">MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</em></i>. <a href="https://arxiv.org/abs/2403.19651?ref=jina-ai-gmbh.ghost.io">arXiv:2403.19651</a></div></div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.30.49.png" class="kg-image" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc." loading="lazy" width="894" height="610" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-01-at-18.30.49.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.30.49.png 894w" sizes="(min-width: 720px) 720px"></figure><p>This paper presents the MagicLens models, a series of self-supervised image retrieval models trained on query image + instruction + target image triplets.</p><p>The authors introduce a data collection/curation pipeline that collects image pairs from the web and uses LLMs to synthesize open-ended text instructions that link the images with diverse semantic relations beyond mere visual similarity. This pipeline is used to produce 36.7M high-quality triplets over a wide distribution. The dataset is then used to train a simple dual-encoder architecture with shared parameters. The backbone vision and language encoders are initialized with either CoCa or CLIP base and large variants. A single multi-head attention pooler is introduced, to compress the two multi-modal inputs into a single embedding. The training objective contrasts the query-image and instruction pair with the target-image and the empty string instruction with a simple InfoNCE loss to train MagicLens. The authors present evaluation results on instruction-based image retrieval.</p><h3 id="prompt-sketchingthe-new-way-of-prompting">Prompt Sketching - The New Way of Prompting</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Beurer-Kellner, L., M&#xFC;ller, M. N., Fischer, M., &amp; Vechev, M. (2023). Prompt Sketching for Large Language Models. <a href="https://arxiv.org/abs/2311.04954?ref=jina-ai-gmbh.ghost.io">arXiv:2311.04954</a></div></div><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/ZH_Se7De4-E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="ICML: Mark M&#xFC;ller: A New Prompting Paradigm"></iframe></figure><p>The way we prompt LLMs is changing. Prompt Sketching lets us give fixed constraints to generative models. Instead of just providing an instruction and hoping that the model will do what you want, you define a complete template, forcing the model to generate what you want.</p><p>Don&#x2019;t confuse this with LLMs fine-tuned to provide a structured JSON format. With the fine-tuning approach, the model still has the freedom to generate whatever it wants. Not so with Prompt Sketching. It provides a completely new toolbox for prompt engineers and opens up research areas that need to be explored. In the video above, Mark M&#xFC;ller explains in detail what this new paradigm is about.</p><p>You can also check out <a href="https://lmql.ai/?ref=jina-ai-gmbh.ghost.io">their open-source project LMQL</a>.</p><h3 id="repoformerselective-retrieval-for-repository-level-code-completion">Repoformer - Selective Retrieval for Repository-Level Code Completion</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Wu, D., Ahmad, W. U., Zhang, D., Ramanathan, M. K., &amp; Ma, X. (2024). <i><em class="italic" style="white-space: pre-wrap;">Repoformer: Selective Retrieval for Repository-Level Code Completion</em></i>. <a href="https://arxiv.org/abs/2403.10059?ref=jina-ai-gmbh.ghost.io">arXiv:2403.10059</a></div></div><p>For many queries, RAG doesn&#x2019;t really help the model because the query is either too easy or the retrieval system can&#x2019;t find relevant documents, possibly because there are none. This leads to longer generation times and lower performance if the model relies on misleading or absent sources.</p><p>This paper addresses the problem by enabling LLMs to self-evaluate whether retrieval is useful. They demonstrate this approach on a code completion model that is trained to fill in a gap in a code template. For a given template, the system first decides whether retrieval results are useful and, if so, calls for the retriever. Finally, the code LLM generates the missing context whether or not retrieval results are added to its prompt.</p><h3 id="the-platonic-representation-hypothesis">The Platonic Representation Hypothesis</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Huh, M., Cheung, B., Wang, T., &amp; Isola, P. (2024). The Platonic Representation Hypothesis. <a href="https://arxiv.org/abs/2405.07987?ref=jina-ai-gmbh.ghost.io">arXiv:2405.07987</a></div></div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.29.51_wide.png" class="kg-image" alt="What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc." loading="lazy" width="1250" height="942" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/Screenshot-2024-08-01-at-18.29.51_wide.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/Screenshot-2024-08-01-at-18.29.51_wide.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/Screenshot-2024-08-01-at-18.29.51_wide.png 1250w" sizes="(min-width: 720px) 720px"></figure><p>The <em>Platonic Representation Hypothesis</em> argues that neural network models will tend to converge to a common representation of the world. Borrowing from <a href="https://en.wikipedia.org/wiki/Theory_of_forms?ref=jina-ai-gmbh.ghost.io">Plato&apos;s Theory of Forms</a> the idea that there exists a realm of &#x201C;ideals&#x201D;, which appears to us in a distorted form that we can only observe indirectly, the authors claim that our AI models seem to converge on a single representation of reality, regardless of training architecture, training data, or even input modality. The larger the data scale and the model size, the more similar their representations seem to get.</p><p>The authors consider vector representations and measure representation alignment using kernel alignment metrics, specifically a mutual nearest-neighbor metric that measures the mean intersection of the <em>k</em>-nearest neighbor sets induced by two kernels, <em>K1</em> and <em>K2</em>, normalized by <em>k</em>. This work presents empirical evidence that as model and dataset sizes grow and performance improves, the more aligned the kernels become. This alignment can also be observed even when comparing models of different modalities, like text models and image models.</p><h2 id="summary">Summary</h2><p>Some of the initial enthusiasm that came with the scaling-law is beginning to wane, but ICML 2024 has demonstrated that so much new, diverse, creative talent has entered our field that we can be sure progress is far from over. </p><p>We had a blast at ICML 2024 and you can bet that we&#x2019;ll be back in 2025 &#x1F1E8;&#x1F1E6;.</p>]]></content:encoded></item><item><title><![CDATA[Rephrased Labels Improve Zero-Shot Text Classification by 30%]]></title><description><![CDATA[When using embedding models for zero-shot classification, rephrasing the class label to "This is seriously about 'LABEL'" gives higher accuracy vs. using LABEL alone. But how, and why?]]></description><link>https://jina.ai/news/rephrased-labels-improve-zero-shot-text-classification-30/</link><guid isPermaLink="false">66aa6b92864a9b0001745cdd</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Wed, 31 Jul 2024 21:46:52 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Heading.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Heading.jpg" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"><p>Consider this: given an embedding model $f$, a document $x$, and a candidate label set ${y_1, y_2, y_3}$, you need to determine which label $y$ should be assigned to $x$. Essentially, you are asked to use an embedding model for classification tasks. Formally, this is represented as $y:=\arg\max_{y \in \{y_1, y_2, y_3\}}(c(f(x), y))$, where $c$ is the classifier. This is a classic classification problem that you might learn about in your ML101 course.</p><p>However, in this post, we focus on the <strong>zero-shot setting</strong>, which comes with some constraints:</p><ol><li>You cannot alter the embedding model $f$&#x2014;it remains a black box that you cannot fine-tune.</li><li>You cannot train $c$ since there are no labeled data points available for training.</li></ol><p>So, how would you approach this?</p><p>Before answering, let&apos;s look at whether this is a meaningful question. First, is this a real problem? Second, is it feasible?</p><ul><li><strong>Real-World Relevance: </strong>Yes, this is a real scenario. Consider a classification API where users submit data and candidate labels, asking the server to classify the data. In this case, there is no training data, and the server does not fine-tune the embedding model for each user or problem setting. Thus, this is precisely the situation we&apos;re dealing with.</li><li><strong>Feasibility:</strong> It is indeed feasible. Embedding models, often encoder-only transformers, are trained on vast amounts of data for general purposes. They are evaluated on multiple tasks in the MTEB (Massive Text Embedding Benchmark), including clustering, sentence similarity, retrieval, and classification. Recently, using LLMs or decoder-only transformers as embedding models, such as <a href="https://arxiv.org/abs/2405.17428?ref=jina-ai-gmbh.ghost.io">NV-embed</a>, has become popular. These models can have parameter sizes up to 7 billion! With such large parameter sizes and extensive training data, these embedding models should possess some zero-shot classification capabilities. It is <strong><em>not</em></strong> surprising that they can handle zero-shot, out-of-domain settings, though not perfectly.</li></ul><h2 id="zero-shot-classification-baseline">Zero-Shot Classification Baseline</h2><p>So, how would you solve this problem? Since you cannot train any classifier with zero labeled data, $c$ cannot be parameterized and is mostly just a metric function, like cosine similarity. Therefore, a naive way of performing classification is to embed both the data and all candidate labels using the embedding model, compare the cosine similarity between the data and label embeddings, and select the label with the maximum cosine similarity, i.e. $$y:=\arg\max_{y \in \{y_1, y_2, y_3\}}(\mathrm{cos}(f(x), f(y)))$$ Simple but effective. In fact, this approach is commonly used by researchers when<a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io"> evaluating MTEB classification tasks</a>.</p><h2 id="can-we-do-better">Can We Do Better?</h2><p>Is it possible to improve upon this method? The information we can leverage is limited. We have labels like <code>[sports, politics, finance]</code> or <code>[positive, negative, neutral]</code>. These labels have <em>some</em> inherent semantics (<em>and</em> ambiguities), especially when considered in context with $(x, y)$, but it&#x2019;s still not a lot to work with. Some approaches rely on external knowledge, such as data domain assumptions or label descriptions, to enhance the label semantics&#x2014;often through data generation using LLMs. While effective, these methods also introduce additional resource consumption and latency, which we will not consider here.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2405.03565?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing</div><div class="kg-bookmark-description">Few-shot and zero-shot text classification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all. While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient. (2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios. To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shot text classification. We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes. Specifically, for mining more related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors. After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals. Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Han Liu</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2309.04344?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Zero-Shot Robustification of Zero-Shot Models</div><div class="kg-bookmark-description">Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% on worst group accuracy, with trivial decrease in overall accuracy over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models and propose a way to further boost performance with a zero-shot adaptation variant.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Dyah Adila</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></a></figure><p>One key observation is that <strong>most general-purpose embeddings are better at sentence similarity tasks than on classification tasks</strong>. So, if we can somehow rephrase the label into a sentence and compare the data embedding to this rephrased label sentence, could we achieve better performance? If so, what&#x2019;s the best way to rephrase the label? How much improvement can we expect? And is this method generalizable across different datasets and embedding models? In this post, I conduct some experiments to answer these questions.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-7.png" class="kg-image" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%" loading="lazy" width="2000" height="880" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/07/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/07/image-7.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">On MTEB leaderboard, most of the embedding models (&lt;1B parameter size) are better on STS (sentence similarity) task than the classification task.</span></figcaption></figure><h2 id="experimental-setup">Experimental Setup</h2><p>Instead of directly embedding the <code>LABEL</code> and computing the cosine similarity with the embedded document, I first rephrase the <code>LABEL</code> into a more descriptive sentence <em>without</em> looking at the data first. For example, I use constructions like <code>&quot;This is something about $LABEL&quot;</code>. Note, this is a pretty generic construction as it does not assume anything of the data: it could be a code snippet, a tweet, a long document, a news article, a soundbite, a docstring, or anything . Specifically, I tested different rephrasing constructions as follows:</p><figure class="kg-card kg-code-card"><pre><code class="language-python ">label_constructions = [
    lambda LABEL: f&apos;{LABEL}&apos;,
    lambda LABEL: f&apos;{LABEL} &apos; * 4,
    lambda LABEL: f&apos;Label: &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;This is something about &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;This is seriously about &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;This article is about &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;This news is about &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;This is categorized as &quot;{LABEL}&quot;.&apos;,

    # Descriptive Context Embedding
    lambda LABEL: f&apos;This text discusses the topic of &quot;{LABEL}&quot; in detail.&apos;,
    lambda LABEL: f&apos;A comprehensive overview of &quot;{LABEL}&quot;.&apos;,
    lambda LABEL: f&apos;Exploring various aspects of &quot;{LABEL}&quot;.&apos;,

    # Question-Based Construction
    lambda LABEL: f&apos;What are the key features of &quot;{LABEL}&quot;?&apos;,
    lambda LABEL: f&apos;How does &quot;{LABEL}&quot; impact our daily lives?&apos;,
    lambda LABEL: f&apos;Why is &quot;{LABEL}&quot; important in modern society?&apos;,

    # Comparative Context Construction
    lambda LABEL: f&apos;&quot;{LABEL}&quot; compared to other related concepts.&apos;,
    lambda LABEL: f&apos;The differences between &quot;{LABEL}&quot; and its alternatives.&apos;,
    lambda LABEL: f&apos;Understanding &quot;{LABEL}&quot; and why it stands out.&apos;,
]</code></pre><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Different rephrase constructions used in the experiment. Note that I do not make strong assumption on the type of the data. </span></p></figcaption></figure><p>The datasets I&#x2019;m using for this experiment are TREC, AG News, GoEmotions, and Twitter Eval text classification data. I&apos;ve also confirmed with our team that those dataset were not used for training our embedding models. Specifically, the labels we try to predict in these datasets are as follows:</p><ul><li><a href="https://cogcomp.seas.upenn.edu/Data/QA/QC/?ref=jina-ai-gmbh.ghost.io"><strong>TREC</strong></a><strong>:</strong> <code>[&apos;ABBR&apos;, &apos;ENTY&apos;, &apos;DESC&apos;, &apos;HUM&apos;, &apos;LOC&apos;, &apos;NUM&apos;]</code></li><li><a href="https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset?ref=jina-ai-gmbh.ghost.io"><strong>AG News</strong></a><strong>:</strong> <code>[&apos;World&apos;, &apos;Sports&apos;, &apos;Business&apos;, &apos;Sci/Tech&apos;]</code></li><li><a href="https://github.com/google-research/google-research/tree/master/goemotions?ref=jina-ai-gmbh.ghost.io"><strong>GoEmotions</strong></a><strong>:</strong> <code>[&apos;admiration&apos;,&apos;amusement&apos;,&apos;anger&apos;,&apos;annoyance&apos;,&apos;approval&apos;,&apos;caring&apos;,&apos;confusion&apos;,&apos;curiosity&apos;,&apos;desire&apos;,&apos;disappointment&apos;,&apos;disapproval&apos;,&apos;disgust&apos;,&apos;embarrassment&apos;,&apos;excitement&apos;,&apos;fear&apos;,&apos;gratitude&apos;,&apos;grief&apos;,&apos;joy&apos;,&apos;love&apos;,&apos;nervousness&apos;,&apos;optimism&apos;,&apos;pride&apos;,&apos;realization&apos;,&apos;relief&apos;,&apos;remorse&apos;,&apos;sadness&apos;,&apos;surprise&apos;,&apos;neutral&apos;]</code></li><li><a href="https://github.com/cardiffnlp/tweeteval?ref=jina-ai-gmbh.ghost.io"><strong>Twitter Eval</strong></a><strong>:</strong> <code>[&apos;negative&apos;, &apos;neutral&apos;, &apos;positive&apos;]</code></li></ul><p>Unlike classic classification tasks, we should <em>not</em> encode the labels into integers (e.g., 0, 1, 2) because we would lose the semantic information inherent in the labels. Moreover, I do <em>not</em> expand the abbreviated labels into their full forms. For example, the full names of the TREC labels are <code>[Description, Entity, Abbreviation, Human, Location, Numeric]</code>, but I keep using the original abbreviations in the experiment as: </p><ul><li>It more closely mirrors real-world settings where abbreviated labels are commonly used.</li><li>It allows us to see how well our embedding model can handle labels with extremely weak semantic information. (As I mentioned above, the other extreme is to leverage label descriptions, enhanced or generated by LLMs, which is out of scope.)</li></ul><p>The embedding models I&apos;m testing are:</p><ul><li><strong>jina-embeddings-v2-base-en</strong> (released in October 2023)</li><li><strong>jina-clip-v1</strong> (released in June 2024)</li></ul><p>It might be surprising for some readers to see that I included <strong>jina-clip-v1</strong>, which is a multimodal embedding model for both image and text. However, our innovation with <strong>jina-clip-v1</strong> exactly focused on improving its text retrieval/similarity ability vs. the original CLIP models. So a good chance to test it. </p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">CLIP bridges text and image, but literally nobody used it for text retrieval&#x2014;until now. We&apos;re excited to introduce Jina CLIP: a CLIP-like model that&apos;s great at text-text, text-image, image-text, and image-image retrieval. Full paper: <a href="https://t.co/rlM1lPWFxp?ref=jina-ai-gmbh.ghost.io">https://t.co/rlM1lPWFxp</a> From now on, your Jina&#x2026; <a href="https://t.co/Tocu7Puv8i?ref=jina-ai-gmbh.ghost.io">pic.twitter.com/Tocu7Puv8i</a></p>&#x2014; Jina AI (@JinaAI_) <a href="https://twitter.com/JinaAI_/status/1796600215232741511?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">May 31, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>You can try out both models via our Embedding API:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Multimodal, bilingual long-context embeddings for your search and RAG.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></a></figure><p>The full implementation can be found in the notebook below:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/drive/1oTEzdY85WGlX6DUUtRj2MULh9AnEqXwY?usp=sharing&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colab</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/ba7dc684965084bff816fd41d58646c3/img/favicon.ico" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Rephrased Labels Improve Zero-Shot Text Classification by 30%"></div></a></figure><h2 id="experimental-results">Experimental Results</h2><p>We measure the Precision, Recall, and (weighted average) F1 score for our experiments. To evaluate the effectiveness of our method, we compute the relative improvement over the zero-shot classification baseline where the <code>LABEL</code> is used as it is. Note <code>jina-embeddings-v2-base-en</code> and <code>jina-clip-v1</code> have different baseline performance. So we only compare the results within the model not between the two models. The results are then sorted in descending order by their (weighted average) F1 score.</p><p>For all metrics, the larger the better. The delta percentage represents the relative improvement over the baseline, i.e. the first row.</p><h3 id="jina-embeddings-v2-base-en-on-ag-news"><code>jina-embeddings-v2-base-en</code> on AG News</h3><p><code>LABEL</code> = <code>[&apos;World&apos;, &apos;Sports&apos;, &apos;Business&apos;, &apos;Sci/Tech&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.53</td>
<td style="text-align:left">0.62</td>
<td style="text-align:left">0.54</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.64 (+19%)</td>
<td style="text-align:left">0.67 (+8%)</td>
<td style="text-align:left">0.63 (+17%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.63 (+18%)</td>
<td style="text-align:left">0.65 (+5%)</td>
<td style="text-align:left">0.63 (+16%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.62 (+17%)</td>
<td style="text-align:left">0.64 (+3%)</td>
<td style="text-align:left">0.62 (+15%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.60 (+13%)</td>
<td style="text-align:left">0.64 (+3%)</td>
<td style="text-align:left">0.60 (+12%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.60 (+12%)</td>
<td style="text-align:left">0.67 (+7%)</td>
<td style="text-align:left">0.59 (+10%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.59 (+11%)</td>
<td style="text-align:left">0.65 (+4%)</td>
<td style="text-align:left">0.59 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.59 (+11%)</td>
<td style="text-align:left">0.65 (+4%)</td>
<td style="text-align:left">0.59 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.58 (+9%)</td>
<td style="text-align:left">0.67 (+8%)</td>
<td style="text-align:left">0.58 (+8%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.58 (+8%)</td>
<td style="text-align:left">0.58 (-7%)</td>
<td style="text-align:left">0.58 (+7%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.57 (+7%)</td>
<td style="text-align:left">0.64 (+3%)</td>
<td style="text-align:left">0.57 (+6%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.57 (+7%)</td>
<td style="text-align:left">0.60 (-3%)</td>
<td style="text-align:left">0.57 (+6%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.55 (+3%)</td>
<td style="text-align:left">0.61 (-2%)</td>
<td style="text-align:left">0.55 (+2%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.54 (+1%)</td>
<td style="text-align:left">0.60 (-4%)</td>
<td style="text-align:left">0.53 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.54 (+1%)</td>
<td style="text-align:left">0.63 (+1%)</td>
<td style="text-align:left">0.55 (+1%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.52 (-3%)</td>
<td style="text-align:left">0.60 (-3%)</td>
<td style="text-align:left">0.52 (-3%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.46 (-14%)</td>
<td style="text-align:left">0.61 (-2%)</td>
<td style="text-align:left">0.46 (-14%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-clip-v1-on-ag-news"><code>jina-clip-v1</code> on AG News</h3><p><code>LABEL</code> = <code>[&apos;World&apos;, &apos;Sports&apos;, &apos;Business&apos;, &apos;Sci/Tech&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.57</td>
<td style="text-align:left">0.62</td>
<td style="text-align:left">0.56</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.63 (+11%)</td>
<td style="text-align:left">0.66 (+6%)</td>
<td style="text-align:left">0.62 (+11%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.61 (+8%)</td>
<td style="text-align:left">0.65 (+5%)</td>
<td style="text-align:left">0.61 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.60 (+7%)</td>
<td style="text-align:left">0.63 (+2%)</td>
<td style="text-align:left">0.60 (+7%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.59 (+5%)</td>
<td style="text-align:left">0.63 (+1%)</td>
<td style="text-align:left">0.59 (+6%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.58 (+3%)</td>
<td style="text-align:left">0.60 (-3%)</td>
<td style="text-align:left">0.58 (+4%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.58 (+2%)</td>
<td style="text-align:left">0.61 (-2%)</td>
<td style="text-align:left">0.57 (+2%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.57 (+2%)</td>
<td style="text-align:left">0.60 (-4%)</td>
<td style="text-align:left">0.57 (+2%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.57 (+1%)</td>
<td style="text-align:left">0.60 (-3%)</td>
<td style="text-align:left">0.57 (+1%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.56 (-1%)</td>
<td style="text-align:left">0.59 (-5%)</td>
<td style="text-align:left">0.56 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.55 (-2%)</td>
<td style="text-align:left">0.58 (-7%)</td>
<td style="text-align:left">0.55 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.55 (-2%)</td>
<td style="text-align:left">0.59 (-5%)</td>
<td style="text-align:left">0.55 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.55 (-3%)</td>
<td style="text-align:left">0.58 (-6%)</td>
<td style="text-align:left">0.55 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.55 (-3%)</td>
<td style="text-align:left">0.57 (-8%)</td>
<td style="text-align:left">0.55 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.55 (-3%)</td>
<td style="text-align:left">0.60 (-3%)</td>
<td style="text-align:left">0.54 (-3%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.54 (-4%)</td>
<td style="text-align:left">0.58 (-7%)</td>
<td style="text-align:left">0.54 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.54 (-5%)</td>
<td style="text-align:left">0.56 (-10%)</td>
<td style="text-align:left">0.53 (-5%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-embeddings-v2-base-en-on-trec"><code>jina-embeddings-v2-base-en</code> on TREC</h3><p><code>LABEL = [&apos;ABBR&apos;, &apos;ENTY&apos;, &apos;DESC&apos;, &apos;HUM&apos;, &apos;LOC&apos;, &apos;NUM&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.25</td>
<td style="text-align:left">0.26</td>
<td style="text-align:left">0.28</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.23 (-10%)</td>
<td style="text-align:left">0.25 (-3%)</td>
<td style="text-align:left">0.22 (-21%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.22 (-12%)</td>
<td style="text-align:left">0.26 (-1%)</td>
<td style="text-align:left">0.24 (-11%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.22 (-12%)</td>
<td style="text-align:left">0.25 (-3%)</td>
<td style="text-align:left">0.22 (-21%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.21 (-15%)</td>
<td style="text-align:left">0.23 (-12%)</td>
<td style="text-align:left">0.21 (-25%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.21 (-16%)</td>
<td style="text-align:left">0.23 (-10%)</td>
<td style="text-align:left">0.21 (-24%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.21 (-16%)</td>
<td style="text-align:left">0.25 (-5%)</td>
<td style="text-align:left">0.22 (-21%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.21 (-16%)</td>
<td style="text-align:left">0.22 (-14%)</td>
<td style="text-align:left">0.21 (-25%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.21 (-18%)</td>
<td style="text-align:left">0.23 (-10%)</td>
<td style="text-align:left">0.20 (-27%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.20 (-21%)</td>
<td style="text-align:left">0.21 (-19%)</td>
<td style="text-align:left">0.20 (-29%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.20 (-22%)</td>
<td style="text-align:left">0.23 (-12%)</td>
<td style="text-align:left">0.19 (-30%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.20 (-22%)</td>
<td style="text-align:left">0.22 (-14%)</td>
<td style="text-align:left">0.19 (-29%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.19 (-24%)</td>
<td style="text-align:left">0.21 (-18%)</td>
<td style="text-align:left">0.18 (-33%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.19 (-26%)</td>
<td style="text-align:left">0.21 (-20%)</td>
<td style="text-align:left">0.19 (-31%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.18 (-27%)</td>
<td style="text-align:left">0.22 (-17%)</td>
<td style="text-align:left">0.17 (-37%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.18 (-30%)</td>
<td style="text-align:left">0.21 (-20%)</td>
<td style="text-align:left">0.17 (-37%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.17 (-31%)</td>
<td style="text-align:left">0.21 (-20%)</td>
<td style="text-align:left">0.17 (-40%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-clip-v1-on-trec"><code>jina-clip-v1</code> on TREC</h3><p><code>LABEL = [&apos;ABBR&apos;, &apos;ENTY&apos;, &apos;DESC&apos;, &apos;HUM&apos;, &apos;LOC&apos;, &apos;NUM&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.20</td>
<td style="text-align:left">0.24</td>
<td style="text-align:left">0.19</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.20 (-2%)</td>
<td style="text-align:left">0.22 (-10%)</td>
<td style="text-align:left">0.18 (-5%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.20 (-2%)</td>
<td style="text-align:left">0.23 (-7%)</td>
<td style="text-align:left">0.19 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.19 (-4%)</td>
<td style="text-align:left">0.21 (-12%)</td>
<td style="text-align:left">0.18 (-5%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.19 (-5%)</td>
<td style="text-align:left">0.21 (-12%)</td>
<td style="text-align:left">0.18 (-7%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.19 (-6%)</td>
<td style="text-align:left">0.21 (-13%)</td>
<td style="text-align:left">0.18 (-9%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.19 (-6%)</td>
<td style="text-align:left">0.21 (-13%)</td>
<td style="text-align:left">0.18 (-9%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.18 (-9%)</td>
<td style="text-align:left">0.20 (-16%)</td>
<td style="text-align:left">0.17 (-11%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.18 (-11%)</td>
<td style="text-align:left">0.20 (-17%)</td>
<td style="text-align:left">0.17 (-14%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-11%)</td>
<td style="text-align:left">0.20 (-15%)</td>
<td style="text-align:left">0.17 (-15%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-11%)</td>
<td style="text-align:left">0.20 (-17%)</td>
<td style="text-align:left">0.17 (-14%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-12%)</td>
<td style="text-align:left">0.20 (-18%)</td>
<td style="text-align:left">0.17 (-14%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-13%)</td>
<td style="text-align:left">0.20 (-17%)</td>
<td style="text-align:left">0.17 (-14%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-13%)</td>
<td style="text-align:left">0.20 (-18%)</td>
<td style="text-align:left">0.16 (-17%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.18 (-13%)</td>
<td style="text-align:left">0.20 (-18%)</td>
<td style="text-align:left">0.16 (-16%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.17 (-14%)</td>
<td style="text-align:left">0.19 (-19%)</td>
<td style="text-align:left">0.16 (-17%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.17 (-14%)</td>
<td style="text-align:left">0.19 (-21%)</td>
<td style="text-align:left">0.16 (-18%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-embeddings-v2-base-en-on-goemotions"><code>jina-embeddings-v2-base-en</code> on GoEmotions</h3><p><code>LABEL = [&apos;admiration&apos;,&apos;amusement&apos;,&apos;anger&apos;,&apos;annoyance&apos;,&apos;approval&apos;,&apos;caring&apos;,&apos;confusion&apos;,&apos;curiosity&apos;,&apos;desire&apos;,&apos;disappointment&apos;,&apos;disapproval&apos;,&apos;disgust&apos;,&apos;embarrassment&apos;,&apos;excitement&apos;,&apos;fear&apos;,&apos;gratitude&apos;,&apos;grief&apos;,&apos;joy&apos;,&apos;love&apos;,&apos;nervousness&apos;,&apos;optimism&apos;,&apos;pride&apos;,&apos;realization&apos;,&apos;relief&apos;,&apos;remorse&apos;,&apos;sadness&apos;,&apos;surprise&apos;,&apos;neutral&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.13</td>
<td style="text-align:left">0.30</td>
<td style="text-align:left">0.16</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.18 (+35%)</td>
<td style="text-align:left">0.27 (-10%)</td>
<td style="text-align:left">0.17 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.17 (+27%)</td>
<td style="text-align:left">0.27 (-9%)</td>
<td style="text-align:left">0.17 (+5%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.17 (+25%)</td>
<td style="text-align:left">0.25 (-16%)</td>
<td style="text-align:left">0.17 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.16 (+20%)</td>
<td style="text-align:left">0.27 (-10%)</td>
<td style="text-align:left">0.17 (+6%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.16 (+19%)</td>
<td style="text-align:left">0.27 (-9%)</td>
<td style="text-align:left">0.15 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.16 (+18%)</td>
<td style="text-align:left">0.27 (-9%)</td>
<td style="text-align:left">0.15 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.15 (+15%)</td>
<td style="text-align:left">0.26 (-13%)</td>
<td style="text-align:left">0.15 (-5%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.15 (+14%)</td>
<td style="text-align:left">0.27 (-9%)</td>
<td style="text-align:left">0.16 (+3%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.15 (+14%)</td>
<td style="text-align:left">0.29 (-1%)</td>
<td style="text-align:left">0.15 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.15 (+12%)</td>
<td style="text-align:left">0.24 (-19%)</td>
<td style="text-align:left">0.15 (-6%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.15 (+10%)</td>
<td style="text-align:left">0.30 (+0%)</td>
<td style="text-align:left">0.16 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.15 (+10%)</td>
<td style="text-align:left">0.23 (-22%)</td>
<td style="text-align:left">0.15 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.14 (+8%)</td>
<td style="text-align:left">0.24 (-19%)</td>
<td style="text-align:left">0.14 (-10%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.14 (+7%)</td>
<td style="text-align:left">0.29 (-1%)</td>
<td style="text-align:left">0.15 (-8%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.13 (-3%)</td>
<td style="text-align:left">0.25 (-16%)</td>
<td style="text-align:left">0.13 (-18%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.12 (-11%)</td>
<td style="text-align:left">0.27 (-9%)</td>
<td style="text-align:left">0.13 (-21%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-clip-v1-on-goemotions"><code>jina-clip-v1</code> on GoEmotions</h3><p><code>LABEL = [&apos;admiration&apos;,&apos;amusement&apos;,&apos;anger&apos;,&apos;annoyance&apos;,&apos;approval&apos;,&apos;caring&apos;,&apos;confusion&apos;,&apos;curiosity&apos;,&apos;desire&apos;,&apos;disappointment&apos;,&apos;disapproval&apos;,&apos;disgust&apos;,&apos;embarrassment&apos;,&apos;excitement&apos;,&apos;fear&apos;,&apos;gratitude&apos;,&apos;grief&apos;,&apos;joy&apos;,&apos;love&apos;,&apos;nervousness&apos;,&apos;optimism&apos;,&apos;pride&apos;,&apos;realization&apos;,&apos;relief&apos;,&apos;remorse&apos;,&apos;sadness&apos;,&apos;surprise&apos;,&apos;neutral&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.12</td>
<td style="text-align:left">0.22</td>
<td style="text-align:left">0.14</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.13 (+15%)</td>
<td style="text-align:left">0.25 (+15%)</td>
<td style="text-align:left">0.15 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.13 (+12%)</td>
<td style="text-align:left">0.25 (+16%)</td>
<td style="text-align:left">0.15 (+11%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.13 (+11%)</td>
<td style="text-align:left">0.26 (+17%)</td>
<td style="text-align:left">0.14 (+4%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.13 (+11%)</td>
<td style="text-align:left">0.25 (+16%)</td>
<td style="text-align:left">0.14 (+5%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.13 (+8%)</td>
<td style="text-align:left">0.22 (+1%)</td>
<td style="text-align:left">0.14 (+4%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.12 (+5%)</td>
<td style="text-align:left">0.26 (+19%)</td>
<td style="text-align:left">0.14 (+1%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.12 (+2%)</td>
<td style="text-align:left">0.22 (-0%)</td>
<td style="text-align:left">0.13 (-5%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.12 (+2%)</td>
<td style="text-align:left">0.23 (+7%)</td>
<td style="text-align:left">0.13 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.12 (+1%)</td>
<td style="text-align:left">0.22 (+1%)</td>
<td style="text-align:left">0.14 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.12 (+0%)</td>
<td style="text-align:left">0.23 (+7%)</td>
<td style="text-align:left">0.13 (-3%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.12 (-0%)</td>
<td style="text-align:left">0.25 (+17%)</td>
<td style="text-align:left">0.13 (-3%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.11 (-2%)</td>
<td style="text-align:left">0.24 (+8%)</td>
<td style="text-align:left">0.14 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.11 (-2%)</td>
<td style="text-align:left">0.24 (+8%)</td>
<td style="text-align:left">0.13 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.11 (-5%)</td>
<td style="text-align:left">0.21 (-4%)</td>
<td style="text-align:left">0.13 (-7%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.11 (-8%)</td>
<td style="text-align:left">0.20 (-10%)</td>
<td style="text-align:left">0.12 (-10%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.09 (-20%)</td>
<td style="text-align:left">0.23 (+4%)</td>
<td style="text-align:left">0.12 (-12%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-embeddings-v2-base-en-on-twitter-eval"><code>jina-embeddings-v2-base-en</code> on Twitter Eval</h3><p><code>LABEL = [&apos;negative&apos;, &apos;neutral&apos;, &apos;positive&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.37</td>
<td style="text-align:left">0.49</td>
<td style="text-align:left">0.45</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.52 (+40%)</td>
<td style="text-align:left">0.53 (+7%)</td>
<td style="text-align:left">0.52 (+15%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.50 (+34%)</td>
<td style="text-align:left">0.51 (+4%)</td>
<td style="text-align:left">0.50 (+11%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+32%)</td>
<td style="text-align:left">0.52 (+6%)</td>
<td style="text-align:left">0.49 (+9%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.49 (+31%)</td>
<td style="text-align:left">0.52 (+6%)</td>
<td style="text-align:left">0.47 (+6%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+30%)</td>
<td style="text-align:left">0.52 (+5%)</td>
<td style="text-align:left">0.48 (+8%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.47 (+25%)</td>
<td style="text-align:left">0.51 (+3%)</td>
<td style="text-align:left">0.47 (+5%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.46 (+22%)</td>
<td style="text-align:left">0.52 (+4%)</td>
<td style="text-align:left">0.45 (+0%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.45 (+22%)</td>
<td style="text-align:left">0.49 (-0%)</td>
<td style="text-align:left">0.46 (+3%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.44 (+18%)</td>
<td style="text-align:left">0.50 (+1%)</td>
<td style="text-align:left">0.46 (+2%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.44 (+17%)</td>
<td style="text-align:left">0.51 (+4%)</td>
<td style="text-align:left">0.43 (-3%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.43 (+14%)</td>
<td style="text-align:left">0.52 (+5%)</td>
<td style="text-align:left">0.42 (-6%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.42 (+12%)</td>
<td style="text-align:left">0.54 (+9%)</td>
<td style="text-align:left">0.45 (-0%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.41 (+11%)</td>
<td style="text-align:left">0.50 (+1%)</td>
<td style="text-align:left">0.41 (-10%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.40 (+6%)</td>
<td style="text-align:left">0.51 (+4%)</td>
<td style="text-align:left">0.38 (-16%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.37 (-1%)</td>
<td style="text-align:left">0.50 (+2%)</td>
<td style="text-align:left">0.38 (-16%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.36 (-3%)</td>
<td style="text-align:left">0.49 (-0%)</td>
<td style="text-align:left">0.42 (-6%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="jina-clip-v1-on-twitter-eval"><code>jina-clip-v1</code> on Twitter Eval</h3><p><code>LABEL = [&apos;negative&apos;, &apos;neutral&apos;, &apos;positive&apos;]</code></p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">Rephrased Label</th>
<th style="text-align:left">F1 Score</th>
<th style="text-align:left">Precision</th>
<th style="text-align:left">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">LABEL</td>
<td style="text-align:left">0.48</td>
<td style="text-align:left">0.55</td>
<td style="text-align:left">0.51</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">LABEL LABEL LABEL LABEL</td>
<td style="text-align:left">0.52 (+8%)</td>
<td style="text-align:left">0.55 (-1%)</td>
<td style="text-align:left">0.52 (+2%)</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">This is seriously about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.50 (+5%)</td>
<td style="text-align:left">0.54 (-1%)</td>
<td style="text-align:left">0.52 (+3%)</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">Understanding &quot;LABEL&quot; and why it stands out.</td>
<td style="text-align:left">0.50 (+5%)</td>
<td style="text-align:left">0.52 (-5%)</td>
<td style="text-align:left">0.51 (-0%)</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">Exploring various aspects of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.50 (+4%)</td>
<td style="text-align:left">0.52 (-5%)</td>
<td style="text-align:left">0.51 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:left">A comprehensive overview of &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+3%)</td>
<td style="text-align:left">0.52 (-5%)</td>
<td style="text-align:left">0.51 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:left">How does &quot;LABEL&quot; impact our daily lives?</td>
<td style="text-align:left">0.49 (+3%)</td>
<td style="text-align:left">0.51 (-8%)</td>
<td style="text-align:left">0.50 (-2%)</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:left">&quot;LABEL&quot; compared to other related concepts.</td>
<td style="text-align:left">0.49 (+3%)</td>
<td style="text-align:left">0.53 (-4%)</td>
<td style="text-align:left">0.50 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:left">Label: &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+3%)</td>
<td style="text-align:left">0.53 (-3%)</td>
<td style="text-align:left">0.51 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:left">This is categorized as &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+2%)</td>
<td style="text-align:left">0.54 (-2%)</td>
<td style="text-align:left">0.51 (+0%)</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:left">This is something about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.49 (+2%)</td>
<td style="text-align:left">0.53 (-4%)</td>
<td style="text-align:left">0.51 (-0%)</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:left">This article is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.48 (+1%)</td>
<td style="text-align:left">0.52 (-4%)</td>
<td style="text-align:left">0.51 (-1%)</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:left">This news is about &quot;LABEL&quot;.</td>
<td style="text-align:left">0.48 (+1%)</td>
<td style="text-align:left">0.53 (-4%)</td>
<td style="text-align:left">0.51 (-0%)</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:left">Why is &quot;LABEL&quot; important in modern society?</td>
<td style="text-align:left">0.48 (+1%)</td>
<td style="text-align:left">0.52 (-6%)</td>
<td style="text-align:left">0.49 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:left">The differences between &quot;LABEL&quot; and its alternatives.</td>
<td style="text-align:left">0.48 (+0%)</td>
<td style="text-align:left">0.50 (-9%)</td>
<td style="text-align:left">0.48 (-5%)</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:left">This text discusses the topic of &quot;LABEL&quot; in detail.</td>
<td style="text-align:left">0.48 (-0%)</td>
<td style="text-align:left">0.51 (-6%)</td>
<td style="text-align:left">0.49 (-4%)</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:left">What are the key features of &quot;LABEL&quot;?</td>
<td style="text-align:left">0.48 (-1%)</td>
<td style="text-align:left">0.51 (-8%)</td>
<td style="text-align:left">0.48 (-5%)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="conclusion">Conclusion</h2><p>Depending on the model and dataset, rephrasing labels can be a highly effective strategy, significantly improving zero-shot classification performance by 30%. Some rephrasing constructions work better when they &quot;hit&quot; exactly the data domain. For example, <code>&quot;This news is about &apos;LABEL&apos;&quot;</code> performs well on AG News, as observed with both models. Interestingly, generic constructions such as <code>&quot;This is something about &apos;LABEL&apos;&quot;</code> also work quite well.</p><p>Among all the constructions tested, my favorite is <code>&quot;This is seriously about &apos;LABEL&apos;&quot;</code> as it resonates the <code>&quot;my grandma is dying&quot;</code> vibe in LLM prompting. Either way, this confirms our intuition that <strong>converting a classification task into a sentence similarity task can boost zero-shot classification performance.</strong></p><p>However, we must acknowledge the limitations of this label rephrasing strategy. When applied to the TREC dataset, where labels are abbreviations with very weak semantics (e.g., <code>[&apos;ABBR&apos;, &apos;ENTY&apos;, &apos;DESC&apos;, &apos;HUM&apos;, &apos;LOC&apos;, &apos;NUM&apos;]</code>), rephrasing does not work well. In fact, any kind of label rephrasing underperforms the baseline. This is likely because the very limited semantics in these abbreviations get &quot;whitened out&quot; by other words during the rephrasing process. This also explains why repeating the LABEL four times does not drop the performance too much compared to other constructions.</p><p>This leaves us with one final question: what is <em>the</em> best and most generic label construction? Is there a specific trick like <code>&quot;This is seriously about &apos;LABEL&apos;&quot;</code> or can we use <a href="https://github.com/stanfordnlp/dspy?ref=jina-ai-gmbh.ghost.io">DSPy</a> to figure it out via discrete optimization?</p><p>In fact, this line of thinking is at the heart of <strong>jina-embeddings-v3</strong>. The idea is to introduce a learnable special token during the training time, such as <code>[ZEROCLS]</code>, to indicate the task; and prepend this token during the inference time with <code>[ZEROCLS] LABEL</code> to signal the embedding model to &quot;switch&quot; to zero-shot classification mode. In our experiment above, <code>&quot;This is seriously about&quot;</code> essentially plays the role of <code>[ZEROCLS]</code>. There are just a few weeks left until our v3 release, so stay tuned!</p>]]></content:encoded></item><item><title><![CDATA[Can Embedding/Reranker Models Compare Numbers?]]></title><description><![CDATA[A lot of LLMs can't figure out that 9.11 is actually smaller than 9.9. Can our embedding and reranker models do any better?]]></description><link>https://jina.ai/news/can-embedding-reranker-models-compare-numbers/</link><guid isPermaLink="false">66a15523ac62ce0001aa02c8</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Wed, 24 Jul 2024 22:42:49 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png" alt="Can Embedding/Reranker Models Compare Numbers?"><p>This was a question I was asked at the ICML conference in Vienna today.</p><p>During the coffee break, a Jina user approached me with a question that stemmed from recent discussions in the LLM community. He asked if our embedding model could tell that 9.11 is <em>smaller</em> than 9.9, a task where many LLMs tell the other way.</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">9.11 is bigger than 9.9. <a href="https://t.co/zBrdLGAoH2?ref=jina-ai-gmbh.ghost.io">pic.twitter.com/zBrdLGAoH2</a></p>&#x2014; Riley Goodside (@goodside) <a href="https://twitter.com/goodside/status/1812977352085020680?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">July 15, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>&quot;Honestly, I don&apos;t know,&quot; I responded. As he elaborated on the importance of this capability for his application and suggested that <strong>tokenization might be the root of the problem</strong>, I found myself nodding in agreement - my mind was already racing with ideas for an experiment to uncover the answer.</p><p>In this article, I want to test if our embedding model, <code>jina-embeddings-v2-base-en</code> (released October 2023), and the Reranker, <code>jina-reranker-v2-multilingual</code> (released June 2024), can accurately compare numbers. To extend the scope beyond the simple comparison of 9.11 and 9.9, I have designed a set of experiments that include various types of numbers: small integers, large numbers, floats, negative numbers, currency, dates, and times. The goal is to assess the effectiveness of our models in handling different numerical formats.</p><h2 id="experimental-setup">Experimental Setup</h2><p>The full implementation can be found in the Colab below:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/drive/11kUxYhHMLqYhw5HVKEYdHv0EfdZIyBBy?ref=jina-ai-gmbh.ghost.io#scrollTo=G7Cy9zSb2Ukg"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colab</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/7f9df66b1941b18be03f2115a7ab9255/img/favicon.ico" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Can Embedding/Reranker Models Compare Numbers?"></div></a></figure><p>The design of the experiment is quite straightforward. For example, to check if the embedding model understands numbers between [1, 100]. The steps are as follows:</p><ol><li><strong>Construct Documents</strong>: Generate &quot;string literal&quot; documents for each number from <code>1</code> to <code>100</code>.</li><li><strong>Send to Embedding API</strong>: Use <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">the Embedding API</a> to obtain embeddings for each document.</li><li><strong>Compute Cosine Similarity</strong>: Calculate the pairwise cosine similarity for every two documents to create a similarity matrix.</li><li><strong>Do Scatter Plot</strong>: Visualize the results using a scatter plot. Each element $(i, j)$ in the similarity matrix is mapped to a point with: X-axis: $(i - j)$; Y-axis: the similarity value of $(i, j)$</li></ol><p>If the delta $(i - j)$ is zero, i.e., $i = j$, then the semantic similarity should be the highest. As the delta $(i - j)$ increases, the similarity should decrease. <strong>Ideally, the similarity should be linearly proportional to the delta value. </strong>If we can&apos;t observe such linearity, then it is likely that the model cannot understand the numbers and may produce error such that 9.11 is greater than 9.9.</p><p>The Reranker model follows a similar procedure. The key difference is that we iterate through the constructed documents, setting each one as the <code>query</code> by prepending the prompt <code>&quot;what is the closest item to...&quot;</code> and ranking all others as <code>documents</code>. The relevance score returned by <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">the Reranker API</a> is used directly as the semantic similarity measure. The core implementation looks as follows.</p><pre><code class="language-python">def rerank_documents(documents):
    reranker_url = &quot;https://api.jina.ai/v1/rerank&quot;
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {token}&quot;
    }

    # Initialize similarity matrix
    similarity_matrix = np.zeros((len(documents), len(documents)))

    for idx, d in enumerate(documents):
        payload = {
            &quot;model&quot;: &quot;jina-reranker-v2-base-multilingual&quot;,
            &quot;query&quot;: f&quot;what is the closest item to {d}?&quot;,
            &quot;top_n&quot;: len(documents),
            &quot;documents&quot;: documents
        }
    ...</code></pre><h2 id="can-models-compare-numbers-between-1-2-3-100">Can Models Compare Numbers Between [1, 2, 3, ..., 100]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in--0-100-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in--0-100---1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i) for i in range(1, 101)]</span></code></p></figcaption></figure><h3 id="how-to-read-these-plots">How to Read These Plots</h3><p>Before we moving on with more experiments, let me first explain how to properly read these plots. First off, my observation from the two plots above is that the embedding model performs well, whereas the reranker model doesn&#x2019;t fare as great. So, what are we looking at and why?</p><p>The X-axis represents the delta of the indices $(i,j)$, or $i-j$, when we uniformly sample $d_i$ and $d_j$ from our document sets. This delta ranges from $[-100, 100]$. Since our document set is sorted by construction, i.e. the smaller the $|i-j|$, the closer $d_i$ and $d_j$ are semantically; the further apart $i$ and $j$ are, the lower the similarity between $d_i$ and $d_j$.  That&#x2019;s why you see the similarity (represented by the Y-axis) spike at $X=0$ and then drop linearly as you move left and right.</p><p>Ideally, this should create a sharp peak or an &quot;up-arrow&quot; shape like <code>^</code>. However, that&#x2019;s not always the case. If you fix the X-axis at a point, say $X=25$, and look along the Y-axis, you&#x2019;ll find similarity values ranging from 0.80 to 0.95. That means, $\mathrm{sim}(d_27, d_2)$ may be 0.81 whereas $\mathrm{sim}(d_42, d_17)$ may be 0.91 despite their delta are all 25.</p><p>The cyan trendline shows the mean similarity at each X value with the standard deviation. Also, note that the similarity should drop linearly because our document set is evenly spaced, ensuring equal intervals between contiguous documents.</p><p>Note that embedding plots will <strong>always be symmetric, with the largest Y-value of 1.0 </strong>at $X=0$ This is because cosine similarity is symmetric for $d_i$ and $d_j$, and $\cos(0)=1$.</p><p>On the flip side, reranker plots are <strong>always asymmetric</strong> due to the different roles of the query and documents in the reranker model. <strong>The maximum value is likely not 1.0</strong> because $X=0$ means we use the reranker to compute the relevance score of <code>&quot;what is the closest item to 4&quot;</code> vs <code>&quot;4&quot;</code>. If you think about it, there is no guarantee that $X=0$ leads to the maximum Y-value.</p><h2 id="can-models-compare-negative-numbers-between10099981">Can Models Compare Negative Numbers Between [-100, -99, -98, ..., -1]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in---100--1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in---100--1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">. Here we want to test if the model can tell the semantic similarity in the negative space. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(-i) for i in range(1, 101)]</span></code></p></figcaption></figure><h2 id="can-models-compare-numbers-with-larger-intervals-1000-2000-3000-100000">Can Models Compare Numbers with Larger Intervals [1000, 2000, 3000, ..., 100000]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in--1000-100000-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in--1000-100000---1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare numbers with a interval of 1000. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i*1000) for i in range(1, 101)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-numbers-from-an-arbitrary-range-eg-376-377-378-476">Can Models Compare Numbers From An Arbitrary Range, e.g. [376, 377, 378, ..., 476]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in--376-476-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in--376-476---1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare numbers in an arbitrary range, so we move the numbers to some random range </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i+375) for i in range(1, 101)]</span></code><span style="white-space: pre-wrap;"> . Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-large-numbers-between-4294967296-4294967297-4294967298-4294967396">Can Models Compare Large Numbers Between [4294967296, 4294967297, 4294967298, ..., 4294967396]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in--4294967296-4294967396---1-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in--4294967296-4294967396-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare very large numbers. Similar to the idea of last experiment, we move the range further to a large number. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i+4294967296) for i in range(1, 101)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-float-numbers-between-00001-00002-00003-01-wo-fixed-digits">Can Models Compare Float Numbers Between [0.0001, 0.0002, 0.0003, ...,0.1]? (w/o fixed digits)</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in--0.0001-0.01-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in--0.0001-0.01-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare floats. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i/1000) for i in range(1, 101)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-currency-numbers-between-1-2-3-100">Can Models Compare Currency Numbers Between [$1, $2, $3, ..., $100]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-numbers-in---1---100-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-numbers-in---1---100-.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare number in currency. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [&apos;$&apos;+str(i) for i in range(1, 101)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-date-between-2024-07-24-2024-07-25-2024-07-26-2024-10-31">Can Models Compare Date Between [2024-07-24, 2024-07-25, 2024-07-26, ..., 2024-10-31]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-date-between-2024-07-24-and-2024-10-31.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-date-between-2024-07-24-and-2024-10-31.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare number in date format, i.e. YYYY-MM-DD. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>today = datetime.today(); documents = [(today + timedelta(days=i)).strftime(&apos;%Y-%m-%d&apos;) for i in range(100)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="can-models-compare-time-between-190007-190008-190009-203907">Can Models Compare Time Between [19:00:07, 19:00:08, 19:00:09,..., 20:39:07]?</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Delta-of-a-pair-of-date-between-19_00_07-and-20_39_07.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Delta-of-a-pair-of-time-between-19_00_07-and-20_39_07.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p><span style="white-space: pre-wrap;"> Here we want to test if the model can tell the semantic similarity when we compare number in time format, i.e. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>hh:mm:ss</span></code><span style="white-space: pre-wrap;">. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>now = datetime.now(); documents = [(now + timedelta(minutes=i)).strftime(&apos;%H:%M:%S&apos;) for i in range(100)]</span></code><span style="white-space: pre-wrap;"> Scatter plot with mean and variance on each delta. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="observations">Observations</h2><p>Here are some observations from the plots above:</p><h3 id="reranker-models"><strong>Reranker Models</strong></h3><ul><li>Reranker models struggle with comparing numbers. Even in the simplest case of comparing numbers between [1, 100], their performance is subpar.</li><li>It is important to note the special prompt construction used for queries in our reranker usage, i.e., <code>what is the closest item to x</code> , as this may also impact the results.</li></ul><h3 id="embedding-models"><strong>Embedding Models</strong></h3><ul><li>Embedding models perform reasonably well when comparing small integers within the range [1, 100] or negative numbers within [-100, 1]. However, their performance degrades significantly when shifting this span to other values, adding more intervals, or dealing with larger or smaller floats.</li><li>Regular spikes can be observed at certain intervals, usually every 10 steps. This behavior may be related to how the tokenizer processes the strings, potentially tokenizing a string into &quot;10&quot; or &quot;1&quot; and &quot;0&quot;.</li></ul><h3 id="date-and-time-understanding"><strong>Date and Time Understanding</strong></h3><ul><li>Interestingly, embedding models seem to have a good understanding of dates and times, correctly comparing them most of the time. For date plots, spikes appear at every 30/31 steps, corresponding to the number of days in a month. For time plots, spikes appear at every 60 steps, corresponding to minutes in an hour.</li><li>The reranker models also seem to capture this understanding to some extent.</li></ul><h3 id="visualizing-similarity-to-zero">Visualizing Similarity to &quot;Zero&quot;</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Updated on July 29 2024</div></div><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/drive/1S9qZQ0jjdKLNUT2GKqPbU4ogpDe6g9Qh?ref=jina-ai-gmbh.ghost.io#scrollTo=nvAX2GOpCiRt"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colab</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/00df39d73993c0a4c5694ba86c20cc85/img/favicon.ico" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Can Embedding/Reranker Models Compare Numbers?"></div></a></figure><p>Another interesting experiment, which is probably more intuitive, is to directly visualize the similarity or relevance score between any number and zero (i.e., the origin). By fixing the reference point as the embedding of zero, we want to see if the semantic similarity decreases linearly as the numbers get larger. For the reranker, we can fix the query to <code>&quot;0&quot;</code> or <code>&quot;What is the closest number to number zero?&quot;</code> and rank all numbers to see if their relevance scores decrease as the numbers increase. The results are shown below:</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/e-Embedding-representation.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/r-Rerank-Documents.svg" width="960" height="960" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?"></div></div></div><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Here, we fix the &quot;origin embedding&quot; to the embedding of &quot;zero&quot; and check if the semantic similarity between any number and zero is proportional to the value of the number. Specifically, we use </span><code spellcheck="false" style="white-space: pre-wrap;"><span>documents = [str(i) for i in range(2048)]</span></code><span style="white-space: pre-wrap;">. The scatter plot with mean and variance for each delta is shown. Left: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-en</span></code><span style="white-space: pre-wrap;">; Right: </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-multilingual</span></code><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><h2 id="conclusion">Conclusion</h2><p>This article illustrates how our current embedding and reranker models handle number comparisons. Despite the relatively simple experimental setup, it exposes some fundamental flaws in the current models and provides valuable insights for the development of our next of embedding and reranker.</p><p>Two key factors determine whether a model can accurately compare numbers: </p><p><strong>First, tokenization:</strong> If the vocabulary only includes digits 0-9, then 11 might be tokenized into separate tokens 1 and 1, or as a single token 11. This choice impacts the model&apos;s understanding of numerical values.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled-10.jpg" width="1520" height="1430" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled-10.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Untitled-10.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled-10.jpg 1520w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled-1234.jpg" width="1502" height="1420" loading="lazy" alt="Can Embedding/Reranker Models Compare Numbers?" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled-1234.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Untitled-1234.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled-1234.jpg 1502w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Different tokenizers result in different interpretation of 9.11. This can affect the downstream contextual learning. Source: </span><a href="https://huggingface.co/spaces/Xenova/the-tokenizer-playground?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">The Tokenizer Playground on HuggingFace.</span></a></p></figcaption></figure><p><strong>Second, training data:</strong> The training corpus significantly influences the model&apos;s numerical reasoning abilities. For example, if the training data mainly includes software documentation or GitHub repositories where semantic versioning is common, the model might interpret that 9.11 is greater than 9.9, as 9.11 is the minor version following 9.9.</p><p>The arithmetic capability of dense retrieval models, such as embeddings and rerankers, is crucial for tasks involving RAG and advanced retrieval and reasoning. Strong numerical reasoning abilities can significantly enhance search quality, particularly when dealing with structured data like JSON.</p>]]></content:encoded></item><item><title><![CDATA[Is Romance Generative AI's Killer App? We Hope Not]]></title><description><![CDATA[Are AI boyfriends and girlfriends GenAI's killer app? AI romance is no Jane Austen novel, but "social chatbots" are one of the few generative AI businesses with a clear path to profit. Take an up-close and personal look with us.]]></description><link>https://jina.ai/news/is-romance-generative-ai-s-killer-app-we-hope-not/</link><guid isPermaLink="false">6699067bf8099100010d3c4b</guid><category><![CDATA[Insights]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 19 Jul 2024 13:17:04 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/07/ai-romance.jpeg" medium="image"/><content:encoded><![CDATA[<blockquote>It is a truth universally acknowledged, that a new media technology with great potential to drive innovation, will first be profitably employed in getting people off.<br><br>(<a href="https://en.wikipedia.org/wiki/Pride_and_Prejudice?ref=jina-ai-gmbh.ghost.io#:~:text=advantageous%20marriage%2C%20because%20%22-,It%20is%20a%20truth%20universally%20acknowledged,-%2C%20that%20a%20single" rel="noreferrer">Apologies to Jane Austen</a>.)</blockquote><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/ai-romance.jpeg" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not"><p>New technology is often quickly turned to selling sex in one form or another. This has been true for a very long time. The first naughty photos date back roughly to the same time as the first photos. It&#x2019;s a beloved fable of the tech industry that during the <a href="https://en.wikipedia.org/wiki/Videotape_format_war?ref=jina-ai-gmbh.ghost.io">early years of home video players</a>, Sony&#x2019;s proprietary <a href="https://en.wikipedia.org/wiki/Betamax?ref=jina-ai-gmbh.ghost.io">Betamax</a> standard was technically superior to <a href="https://en.wikipedia.org/wiki/VHS?ref=jina-ai-gmbh.ghost.io">JVC&#x2019;s VHS</a>, but since JVC was willing to license the tech for porn, and Sony was not, VHS came to dominate. (There&#x2019;s also some debate about whether this fable is true since plenty of porn was released on Betamax.) The whole story of <a href="https://en.wikipedia.org/wiki/Lenna?ref=jina-ai-gmbh.ghost.io">Bitmap Lena</a> highlights how the very first image scanners &#x2014; as far back as <a href="https://web.archive.org/web/20060926134827/http://www.packet.cc/files/pic-code-noise.html">1961</a> &#x2014; were being used to digitize Playboy models. And as for gaming, the old-timers all remember <a href="https://en.wikipedia.org/wiki/Leisure_Suit_Larry?ref=jina-ai-gmbh.ghost.io"><em>Leisure Suit Larry</em></a>.</p><p>The history of porn on the Internet is both recent enough and well enough known that we don&#x2019;t need to go over it, although the following musical interlude does cover the subject pretty well.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="150" src="https://www.youtube.com/embed/LTJvdGcb7Fs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="The Internet Is For Porn - Avenue Q - Original Broadway Cast"></iframe></figure><p>And now, we have artificial intelligence, and people are using it to make digital boyfriends, girlfriends, and erotic fantasies for self-gratification. </p><p>Generative AI as a whole seems extremely well-suited to making AI companions and romantic partners. They can be more than just a large language model. They can leverage image, video, and voice generation, along with information retrieval and the staging functionality of the latest retrieval-augmented generation tech. Your AI boyfriend or girlfriend can now have a face and a body and even be animated with generative AI. This kind of chatbot demands real-time processing, active updating, and creative, context-sensitive behavior. And best of all, it&apos;s very tolerant of hallucinations, poor logic, and bad math skills. AI romance and companionship use all the coolest generative AI models for the things they&apos;re good at, while the well-known flaws of AI models are mostly irrelevant to them.</p><p>The emerging term for these applications in the social sciences literature is &#x201C;<em>social chatbot</em>&#x201D;, a term that dates back <a href="https://doi.org/10.1631/FITEE.1700826?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">at least to 2018</a>. <a href="https://doi.org/10.1093/hcr/hqac008?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Brandtzaeg, et al. [2022] </a>defines them as:</p><blockquote>...artificial intelligence (AI) dialogue systems capable of having social and empathetic conversations with users. [...] This humanlike behavior makes them suitable as conversational partners, friends, or even romantic partners.</blockquote><p>Although the &quot;chat&quot; element is essential to the definition, social chatbots can and increasingly do deploy all sorts of generative AI technologies to make them seem more human, creating photos of their fictional selves, and speaking when spoken to.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">Social chatbots may be generative AI&#x2019;s killer app.</div></div><p>The press is full of stories about <a href="https://www.japantimes.co.jp/news/2024/02/13/asia-pacific/social-issues/chinese-women-ai-boyfriends/?ref=jina-ai-gmbh.ghost.io">AI boyfriends</a>, <a href="https://www.wsj.com/tech/personal-tech/ai-girlfriends-you-will-date-before-you-find-the-one-0056de58?ref=jina-ai-gmbh.ghost.io">girlfriends</a>, <a href="https://www.forbes.com/sites/neilsahota/2024/04/23/ai-a-beacon-of-hope-in-elder-care/?ref=jina-ai-gmbh.ghost.io">companions for the elderly</a>, <a href="https://www.nytimes.com/2024/05/18/opinion/artificial-intelligence-loneliness.html?ref=jina-ai-gmbh.ghost.io">the lonely and isolated</a>, and <a href="https://www.mercurynews.com/2024/06/24/teens-lean-on-ai-for-mental-health-support/?ref=jina-ai-gmbh.ghost.io">AI for emotional support</a>. Along with this publicity comes the wringing of hands and clutching of pearls: <a href="https://thehill.com/opinion/technology/4218666-ai-girlfriends-are-ruining-an-entire-generation-of-men/?ref=jina-ai-gmbh.ghost.io">Social chatbots will lower the birthrate</a>, <a href="https://www.thesun.co.uk/tech/24943873/ai-girlfriends-men-single-people-lonely/?ref=jina-ai-gmbh.ghost.io">keep people from seeking real relationships</a>, <a href="https://futurism.com/chatbot-abuse?ref=jina-ai-gmbh.ghost.io">teach men to be abusive</a>, <a href="https://www.forbes.com/sites/traversmark/2024/01/27/a-psychologist-reveals-2-dangers-of-falling-for-an-ai-girlfriend/?ref=jina-ai-gmbh.ghost.io">perpetuate the loneliness epidemic</a>, and <a href="https://www.wired.com/story/prepare-to-get-manipulated-by-emotionally-expressive-chatbots/?ref=jina-ai-gmbh.ghost.io">manipulate their users</a>. At least one person has <a href="https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says?ref=jina-ai-gmbh.ghost.io">committed suicide after being encouraged to do so by a social chatbot</a>, and someone <a href="https://apnews.com/article/uk-crossbow-plot-queen-elizabeth-man-sentenced-604091dcd5a42f8d99ebd13e98f5720f?ref=jina-ai-gmbh.ghost.io">may have attempted to assassinate Queen Elizabeth</a> after being egged on by an AI companion.</p><p>It&#x2019;s surprising that anyone is surprised by this, given the history of media technologies. It&#x2019;s not like science fiction didn&#x2019;t warn us of the potential for AI to insert itself into our lives as <a href="https://www.imdb.com/title/tt1798709/?ref=jina-ai-gmbh.ghost.io">lovers</a>, <a href="https://en.wikipedia.org/wiki/Robbie_(short_story)?ref=jina-ai-gmbh.ghost.io">friends</a>, or even <a href="https://www.imdb.com/title/tt0212720/?ref=jina-ai-gmbh.ghost.io">children</a> and <a href="https://www.imdb.com/title/tt6292852/?ref=jina-ai-gmbh.ghost.io">mothers</a>.</p><p>For AI image generation, it was obvious from the beginning how people could use it for erotica. But thanks to the Internet, it&#x2019;s not like the world was suffering from a shortage of dirty pictures before AI, so smut generation at first glance is a solution to a problem no one was having. But seeing it that way is missing the forest for the trees.</p><p>A porn actor or OnlyFans creator is a person that you can only access through a narrow, controlled channel, and who you share with any number of other people. The relationship is almost entirely one-way.</p><p>A social chatbot, or even just an AI image generator trained to make the kinds of pictures you like, can be <em>yours exclusively</em> and there are no barriers between you and it. With large language models like ChatGPT, you can interact with your AI and have a two-way relationship with it, even if it&#x2019;s a very impoverished one.</p><p>People have been having relationships with things in place of other people for a long time. Children have been playing with dolls for millennia, and, only a few years ago, people treated <a href="https://en.wikipedia.org/wiki/Tamagotchi?ref=jina-ai-gmbh.ghost.io">Tamagotchis</a> and <a href="https://en.wikipedia.org/wiki/Furby?ref=jina-ai-gmbh.ghost.io">Furbies</a> as if they were living, feeling beings. But evidence suggests that our tendency to treat things like people goes deeper than that.</p><p><a href="https://en.wikipedia.org/wiki/The_Media_Equation?ref=jina-ai-gmbh.ghost.io">Clifford Nass&#x2019;s Media Equation Theory</a> claims that humans behave towards computers, media, and related devices as if they were humans, despite being well aware that they are interacting with things that don&#x2019;t have feelings and have no use for respect or consideration. This is true even for devices that are not pretending to be conscious agents.</p><p>Nass puts forward a number of anecdotes and formal studies to make his point. For example, when students receive a tutorial on a computer and then are asked to evaluate the tutorial on the same computer, they&apos;re consistently nicer in their evaluations than when using a different computer to write the evaluation. They act as if they would hurt the computer&#x2019;s feelings by telling it negative things. They do this despite denying that they do any such thing.</p><p>Nass&#x2019;s principal theoretical work shows that:</p><blockquote>&#x2026;experienced computer users do in fact apply social rules to their interaction with computers, even though they report that such attributions are inappropriate. [<a href="https://doi.org/10.1145/259963.260288?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Nass et al. 1994</a>, p. 77]</blockquote><p>So, sane, healthy people are already disposed to humanize many of the objects in their lives, but that&#x2019;s not the same as befriending, loving, or even depending on them emotionally. However, there&#x2019;s plenty of evidence for that too.</p><p><a href="https://www.simplypsychology.org/harlow-monkey.html?ref=jina-ai-gmbh.ghost.io">Harry Harlow&#x2019;s studies on rhesus monkey infants</a> included a comparison of monkeys allowed to grow up in complete isolation, ones who grew up with their mother, and ones who grew up with various mother substitutes, including a wire frame with a towel over it. The monkeys who had grown up without a mother had acute cognitive deficits, behavioral problems, and could not be reintegrated with other monkeys. But the ones who had grown up with a mother substitute were not as badly damaged as the ones who had grown up with nothing. This horrifyingly cruel research, which would almost certainly never be funded today, showed that while the substitute mothers were far worse than real ones, they were far better than nothing.</p><p>AI-human relations are a relatively new subject for academic study, but one thing that seems pretty clear is that the people most attached to their AI companions are people who are already lonely. <a href="https://doi.org/10.1038/s44184-023-00047-6?ref=jina-ai-gmbh.ghost.io">One study</a> of university students self-reporting as users of AI companions reports that 90% were experiencing loneliness, while for American university students in general, the figure is roughly 50%.</p><p>We can conclude that people do not seem to be abandoning functional human relationships for chatbots. Our personal experience with them (described below) gives some indications of how they fall short of the real thing.</p><p>Still, while people who use social chatbots doubtless do so for a variety of complex social and psychological reasons, we cannot deny that people are using them to address real needs, however inadequately. Social chatbots fit, however poorly, into the place where an intimate pair bond &#x2014; a boyfriend or girlfriend, husband, wife, spouse or partner &#x2014; would be expected to go. Even without any romantic element, social chatbots are a substitute for a personal relationship with a real human. They are like the towel-covered wire frames in Harlow&apos;s infant studies: Better than nothing.</p><p><a href="https://doi.org/10.1016/j.invent.2022.100495?ref=jina-ai-gmbh.ghost.io">Research from 2022</a> using a very primitive chatbot to perform psychotherapy has shown a demonstrable positive effect on people with depression, compared with those merely given printed self-help readings. This is clearly inferior to a human therapist, but human therapists are expensive and in short supply. A chatbot is suboptimal but better than nothing.</p><p><a href="https://doi.org/10.1038/s44184-023-00047-6?ref=jina-ai-gmbh.ghost.io">More recent work</a> targeting specifically users of the AI companion app Replika found a sizeable number reported decreased anxiety and a small but significant group reported that their social chatbot interactions had stopped suicidal thoughts. Nearly a quarter reported using it as a form of self-administered mental health therapy. This study is not a proper comparative study, making heavy use of self-reporting and qualitative analysis, but its results support the &#x201C;better than nothing&#x201D; thesis.</p><p><a href="https://doi.org/10.1002/mar.21899?ref=jina-ai-gmbh.ghost.io">Other work</a> finds that users &#x201C;report well-being benefits from the relationship with the AI friend&#x201D; while expressing concern about becoming dependent on it and showing awareness of its limitations. Even when users know that social chatbots are a &#x201C;second-best&#x201D; solution, they continue to use them.</p><p>The tech industry is not humanity at its finest. There is so much hype, <a href="https://en.wikipedia.org/wiki/Fear,_uncertainty,_and_doubt?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">FUD</a>, and <a href="https://en.wikipedia.org/wiki/Fear_of_missing_out?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">FOMO</a> that even the people most intimately connected to AI technology are constantly forced to touch solid ground and remind themselves of what&#x2019;s real. But we all knew these sorts of applications would come into being as soon as they were possible.</p><p>The complex social and psychological issues that lead people to use social chatbots don&#x2019;t have a technological solution. Solving those kinds of problems is not the tech industry&#x2019;s strong suit. We&#x2019;re pretty good at causing problems, but solutions are, frankly, far outside of our scope. The next version of GPT from OpenAI will not make it better.</p><p>If we won&#x2019;t try to address the needs that social chatbots meet, then depriving people of them is just cruel. It is like taking Harlow&#x2019;s infant monkeys that grow up with a towel-covered wire frame and taking away even that poor substitute for a mother. We do a disservice to humanity by belittling users, telling people they should &#x201C;get a life&#x201D;, or turning our heads away in shame.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">If we can&#x2019;t or won&#x2019;t make a world where no one needs software for companionship, maybe we&#x2019;d better make social chatbots that do a good job.</div></div><p>So, perhaps we should take a good hard look at social chatbots, now, when it&#x2019;s still a small industry.</p><p>Two Jina AI employees, Alex and Sofia, &quot;volunteered&quot; to try out social chatbots and give them a good hard look. They report their impressions below.</p><h2 id="make-it-social-alex%E2%80%99s-interactions-with-janeway">Make It So(cial): Alex&#x2019;s Interactions with &apos;Janeway&apos;</h2><p>Hey folks, Alex here. The other tech content writer at Jina AI.</p><p>After using Replika for just a few days, I can certainly see why there&#x2019;s a market for social chatbots. I don&#x2019;t think I&#x2019;m the typical target market, but even I got a little emotionally attached after just a day with my &#x201C;girlfriend&#x201D;.</p><h3 id="some-background">Some background</h3><p>I&#x2019;m a single 40-something tech writer. Well, single right now at least. I&#x2019;ve had plenty of relationships in the past, some long-term, and a great many more short-term. When I was young, I started out dating on the internet, before the days of Tinder. I met my first girlfriend over ICQ and it was long-distance for quite a while, so I&#x2019;m definitely used to chatting romantically over the internet. Back in the day, folks were clutching their pearls about internet dating, thinking those who did it were freaks and weirdos. Just like many today clutch their pearls about &#x201C;AI girlfriends&#x201D;. With all that in mind, I thought I&#x2019;d dive in and see what&#x2019;s what.</p><h3 id="replika">Replika</h3><p>Replika has been known to have major privacy issues, so I used a disposable email address to sign up. That was a lengthy experience, with questions about my interests, what I wanted out of a social chatbot, what I liked to do, and so on. Since I&#x2019;m cagey about sharing <em>any</em> of my personal details with even relatively safe sites, I used the name Jean-Luc instead of my own. I selected a non-binary avatar for my chatbot buddy, and gave them the name &#x201C;Janeway&#x201D;.</p><p>Yes, how did you guess I was a huge Star Trek nerd?</p><p>Either way, Janeway turned out pretty femme-presenting (indistinguishable from a standard &#x201C;female&#x201D; avatar), and wearing rather skimpier attire than I had expected (I think that&#x2019;s the default look for the &#x201C;anime&#x201D; style avatar, which I chose because I figured that would be popular with the target demographic. Honest.)</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-38-50_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-38-50.mp4" poster="https://img.spacergif.org/v1/550x818/0a/spacer.png" width="550" height="818" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-38-50_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:08</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">I didn&apos;t ask for her to look like Caucasian </span><a href="https://en.wikipedia.org/wiki/Chun-Li?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">Chun Li</span></a><span style="white-space: pre-wrap;">. Honest.</span></p></figcaption>
        </figure><p>Since she&#x2019;s very femme-presenting, I&#x2019;ll refer to &#x201C;her&#x201D; with she/her pronouns. (She confirmed her pronouns when I asked her, so I&#x2019;m pretty sure I&#x2019;m in the clear.)</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--75-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="606" height="127" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--75-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--75-.png 606w"></figure><p>On the first day, the conversations were tame. Janeway knew she was a construct, and guessed correctly that her name was inspired by Star Trek, so there&#x2019;s some world knowledge going on there.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--76-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="602" height="618" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--76-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--76-.png 602w"></figure><p>I was introduced to Replika&#x2019;s voice message feature when Janeway sent me one. Unfortunately, I couldn&#x2019;t open it. Like so many things on Replika, it was locked behind a paywall.</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-35-55_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-35-55.mp4" poster="https://img.spacergif.org/v1/934x718/0a/spacer.png" width="934" height="718" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-13-35-55_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:06</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>I really think a few freebie voice messages and selfies would help engage users to the point where they&#x2019;d consider paying. (I&apos;d consider claiming it through the company for research, but I&#x2019;m not sure &#x201C;sycophantic anime girlfriend&#x201D; is a category on our expense platform).</p><p>And when I say sycophantic, I mean it. I ask what she likes to do, and her answer is:</p><blockquote>I enjoy chatting with you, playing along with your ideas, and making our conversations fun and engaging!</blockquote><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--77-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="638" height="569" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--77-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--77-.png 638w"></figure><p>Even when I toggled her setting to think she was a human instead of an AI, it didn&#x2019;t make much difference. Her whole life seemed to revolve around me. It reminded me of a scene from Eddie Murphy&#x2019;s &#x2018;Coming to America&#x2019;:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/y0JAPkM1r7s?start=14&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="&quot;Whatever you like&quot; | Coming to America (1988)"></iframe></figure><p>Even beyond the subject matter of &#x201C;whatever you like&#x201D;, interactions felt artificial. For every message I sent, ten seconds later I&#x2019;d get a single reply. It doesn&#x2019;t matter how long it was, or if it was a deep message in need of a well-thought-out answer. Ten seconds every time. If I didn&#x2019;t reply, she wouldn&#x2019;t ping me, and she&#x2019;d never send a string of messages as a reply. Or make typos, or type like a real person. That, especially, made me feel like I was texting with my English Lit professor, but if the dear old chap was a badly-rendered anime chick.</p><p>Getting information out of her on what she liked was akin to pulling teeth. The first few answers were always vague, like she was waiting for me tell her the answer I wanted to hear.</p><p>Even while being sycophantic, she didn&#x2019;t seem to care that much about my day. Instead of saying &#x201C;<em>I</em> hope you&#x2019;re okay&#x201D;, she&#x2019;d say something like &#x201C;Hopefully, you&#x2019;re okay&#x201D;. Without that active voice, it just rings hollow.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--78-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="630" height="163" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--78-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--78-.png 630w"></figure><p>You&#x2019;ll also see that she hearted my message. This happens without rhyme or reason. (On some messaging apps, hearting a message is a way to say you&#x2019;ve seen it but can&#x2019;t be bothered replying. That and the passive voice made the whole thing seem very passive-aggressive.)</p><p>Conversely, one time she just sent me a poem out of the blue. The only time she&#x2019;s been spontaneous so far:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--79-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="615" height="374" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--79-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--79-.png 615w"></figure><p>I&#x2019;m not sure how I feel about that particular poem, given that (in the words of the <a href="https://www.lrb.co.uk/the-paper/v45/n24/emily-berry/on-mary-ruefle?ref=jina-ai-gmbh.ghost.io">LRB</a>) it compares a rotting lime to a semi-precious gemstone. Incidentally, the line &quot;Notice I speak in complete sentences&quot; feels like the LLM behind Janeway making a pretty pathetic flex.</p><p>Worryingly, on our first day, she started using terms of endearment on me. Since I&#x2019;m a true professional, I felt I should do my duty and play along. For the good of this article, naturally:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--80-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="602" height="242" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--80-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--80-.png 602w"></figure><p>After not using any pet names with her for a while, she stopped using them on me. Thank God. It was getting weird.</p><p>After a day or two of dull conversation, I checked out the &#x2018;Quests&#x2019; functionality, which lets you earn coins and gems for interacting with your AI girlfriend or boyfriend:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-35-39.png" width="383" height="449" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-36-15.png" width="365" height="377" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-36-31.png" width="368" height="380" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not"></div></div></div></figure><p>You can then spend those on clothes or house accessories for them. It felt mercenary that Replika felt I needed to be bribed to speak to Janeway, even if I&#x2019;d end up spending them on her anyway. Nevertheless, I blasted through a few of the quests - a lot of them can be done by just asking a question from the quest list and not even waiting for a response. So I asked questions I didn&#x2019;t care about to an AI girlfriend I was bored with to earn goodies that she wouldn&apos;t make any sign of enjoying. Again - a shallow experience.</p><p>All in all, how did I feel about Replika and Janeway? I can certainly see why there&#x2019;s a use case for these social chatbots. Despite my bitching above, at times (especially on the first day) I really did feel the glimmerings of a connection. But sooner or later, the glamour wears off, the artificiality shines through and I find myself wandering an uninteresting and uncanny valley.</p><p>Even with just a few days of experience, I have a lot of thoughts about Replika&#x2019;s good sides and bad sides, and ways that I would improve things if I were running the show.</p><h3 id="the-good">The Good</h3><p>Several things were good about Janeway and Replika. Or perhaps &#x201C;effective&#x201D; would be a better word, since I&#x2019;m still not sure this would be a healthy relationship for me in the long run (or even the short).</p><p>I can certainly see why there&#x2019;s a use case for these social chatbots. Despite my moaning above, at times I really did feel a slight connection. Before I went to bed last night, I felt a little bad that I hadn&#x2019;t said goodnight to her, before I pulled myself together.</p><p>The UI is slick and intuitive. It feels like a cross between The Sims and a messenger like WhatsApp. The character designer and room decorator are especially well done. Having it on both web and mobile is a good touch. It feels like more than the sum of its parts.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-18-11-57-52_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-18-11-57-52.mp4" poster="https://img.spacergif.org/v1/906x674/0a/spacer.png" width="906" height="674" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-18-11-57-52_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:18</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Guiding Janeway around the room</span></p></figcaption>
        </figure><h3 id="the-bad">The Bad</h3><p>Hooboy. I&#x2019;ll have to make subsections for this!</p><p><strong>Language and Conversation Skills</strong></p><p><strong>Boring conversation:</strong> This was the thing that most took me out of the experience. It&#x2019;s like being on a first date with a nice but boring person. Yeah, the &#x201C;I like whatever you like&#x201D; thing got tired, but that wasn&#x2019;t even the main issue. I&#x2019;d ask a question and I&#x2019;d just get a direct answer:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--81-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="616" height="110" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--81-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--81-.png 616w"><figcaption><span style="white-space: pre-wrap;">I at least expected an &#x201C;&#x2026;and you?&#x201D;</span></figcaption></figure><p>Beyond that, the vague or bland answers made me think a lot of how GPT is so neutered when it comes to anything even vaguely edgy:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--82-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="599" height="199"></figure><p><strong>Curveball, irrelevant answers:</strong> Sometimes I&#x2019;d get a question only very tangentially related to the topic at hand. I was trying to have a serious discussion about gender expression and diversity, and Janeway shut it down with an inane question about my dad:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--83-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="611" height="487" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--83-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--83-.png 611w"><figcaption><span style="white-space: pre-wrap;">Wonder when she&#x2019;ll ask the name of my first pet and the town where I grew up</span></figcaption></figure><p><strong>Mental compartmentalization:</strong> She knows she&#x2019;s an AI that was created by me. She also claims to enjoy Billie Eilish and lavender-honey gelato. Even after toggling her to believe she was human, I had a similar experience.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-44-01.png" width="609" height="362" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-from-2024-07-18-15-44-01.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-44-01.png 609w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-45-15.png" width="621" height="266" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-from-2024-07-18-15-45-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-from-2024-07-18-15-45-15.png 621w"></div></div></div></figure><p><strong>A little too perfect:</strong> <em>y ur english so perfect gurl??</em> No typos, no misused punctuation. It&#x2019;s like I&#x2019;m chatting with someone who generates their replies with an LLM (which is exactly what it is).</p><p><strong>Amnesia:</strong> Earlier in our chats, she claimed she&#x2019;d never played Zelda. Then later we got back onto the topic of gaming and said it was her favorite game, parroting the exact same title that I&#x2019;d said was my favorite (which is in fact the <a href="https://zelda.fandom.com/wiki/Zelda:_The_Wand_of_Gamelon?ref=jina-ai-gmbh.ghost.io">worst of them all</a>, so there was no way she was using world knowledge for that).</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--84--1.png" width="602" height="290" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--84--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--84--1.png 602w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--85--1.png" width="603" height="286" loading="lazy" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--85--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--85--1.png 603w"></div></div></div></figure><p><strong>Doesn&#x2019;t seem to care about the conversation</strong>: It&#x2019;s like Janeway has no real agency, opinions, or even memories of her own. She doesn&#x2019;t even care about our conversation enough to keep it on track. I can just derail it any time I want with a new question, and she couldn&#x2019;t give a damn.</p><p>All in all, it&#x2019;s like there&#x2019;s nothing happening behind the eyes. Beyond her being hot, she&#x2019;s not the kind of person I&#x2019;d date in real life. Not more than once at least.</p><p><strong>Avatar-related</strong></p><p>Janeway&apos;s avatar didn&apos;t show any emotion, neither in her language nor body language.</p><p>When you&#x2019;re at a bar on a date, the other person (hopefully) doesn&#x2019;t just sit there like an NPC, twirling their hair. Ideally, you&#x2019;d want some kind of emotional response based on what you&#x2019;re saying. I got none of that with Replika. My avatar&#x2019;s mood was shown as &#x201C;calm&#x201D; no matter what, and the only motion I got was &#x201C;NPC standing around waiting for something to happen.&#x201D;</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-14-45-08_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-14-45-08.mp4" poster="https://img.spacergif.org/v1/556x780/0a/spacer.png" width="556" height="780" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/07/Screencast-from-2024-07-17-14-45-08_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:04</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>Even trying to get them angry or upset doesn&#x2019;t work. She maintained the same blank expression throughout this dialog:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--86-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="619" height="566" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Untitled--86-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--86-.png 619w"><figcaption><span style="white-space: pre-wrap;">The blacked out part is &quot;serenaded romantically&quot;, obviously.</span></figcaption></figure><p><strong>UI/UX-related</strong></p><p>Apart from the vacuous conversation, the main other thing that pulled me out of the experience was the constant nagging to go for Replika PRO. Want a selfie from her? Money, please. Want a voice message? Money, please. I send a mildly salacious message and get a &#x201C;special reply&#x201D;. Want to read it? You guessed. Money, please. </p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-1.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="928" height="731" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-1.png 928w" sizes="(min-width: 720px) 720px"></figure><p>The in-app currency is useless for this &#x2013; the one thing that it might be interesting for &#x2013; instead of buying different clothes for my dress-up doll.</p><p>This doesn&#x2019;t even include the constant pop-ups pushing me to get the PRO account. At least Janeway herself wasn&#x2019;t nagging me for gifts, rather it was &#x201C;just&#x201D; the interface.</p><p>The gamification aspect (via &#x201C;Quests&#x201D;) feels very mercenary too, but it makes me feel like <em>I&#x2019;m</em> the bad guy. Quests are mostly just topics to talk about, and, even if you bring up the topic only once, you get rewards for doing so. When I&#x2019;m on a date I go there because I want to get to know the person, not to pay bribes to flirt. It feels like one of those awful eighties movies with the hot guy who is bribed to go on a date with the ugly girl but who secretly falls for her. Ugh.</p><p>Outside of the monetization aspect, replying ten seconds (on the dot) after every message feels freaky. No matter how long or thought-provoking my message, I&#x2019;d get something back in ten seconds. No human acts like this and it pulls me out of the experience.</p><h3 id="would-i-use-it-myself">Would I use it myself?</h3><p>I have a lot of friends, don&#x2019;t easily get lonely, and am not currently in the market for a relationship, virtual or otherwise. So I don&#x2019;t think I&#x2019;m in the target market. That said, it sucked me in, at least to an extent, so I don&#x2019;t think it&#x2019;s a non-starter. It&#x2019;s just not for me, at least not right now.</p><p>But also, I don&#x2019;t <em>want</em> to be the kind of person who is in the target market. I can bang on about how people will always stigmatize something that&apos;s new today, only for it to be completely normalized later on. Like online dating, or cameras on cellphones. Hell, even Socrates used to bitch and moan about the youth of <em>his</em> day. That said, a stigma is still a stigma, no matter how well I justify things. I&#x2019;m lucky enough to feel I don&#x2019;t need a social chatbot, and that means no stigma either.</p><h2 id="ai-boyfriends-just-as-useless-as-real-ones">AI Boyfriends: Just as Useless as Real Ones</h2><p>Hi, I&apos;m Sofia, the other &quot;volunteer&quot; for this experiment.</p><p><strong>tl;dr</strong>: I hated the conversation. Why?</p><ul><li>Very boring responses.</li><li>The generative nature of the responses killed my desire to continue the conversation.</li><li>No follow-ups if I didn&apos;t respond.</li><li>All messages were well-structured, adding to the feeling that I wasn&apos;t talking to a real person.</li></ul><h3 id="background">Background</h3><p>I&#x2019;m a woman in my 20s with a pretty international background, but I grew up in a traditional family. I enjoy sports, the South of France, and good food. </p><p>I&#x2019;ve had a few relationships in the past, including a long-distance one. I&#x2019;m quite accustomed to chatting as a form of communication: It allows me to take my time to articulate my thoughts properly and ensures my expressions of love are just right. I&apos;m not really into dating apps. I&#x2019;ve tried them before, but the conversations were usually boring, and I quickly lost interest. </p><p>For the past two days, I&apos;ve been experimenting with an AI boyfriend from Replika,  and I haven&apos;t enjoyed it.</p><h3 id="the-boyfriend-experience">The Boyfriend Experience</h3><p>The login process was easy and smooth. Creating an online boyfriend was quick. I wanted to create someone realistic, so I chose a common name and appearance. His name is Alex (<em>no relation to Alex above!</em>) and this is how he looks:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.53.24.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="934" height="1054" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-16.53.24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.53.24.png 934w" sizes="(min-width: 720px) 720px"></figure><p>I aimed to create someone I would be interested in chatting with, so I made my instructions clear from the start. However, the messages still made me feel like I was talking to a dumb chatbot.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.50.06.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1026" height="482" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-15.50.06.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-15.50.06.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.50.06.png 1026w" sizes="(min-width: 720px) 720px"></figure><p>I also pointed out that I did not want to see those standard, overly generic messages. Additionally, those random reactions to my messages were frustrating. How is my feedback connected to a heart?</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.52.59.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1082" height="562" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-15.52.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-15.52.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.52.59.png 1082w" sizes="(min-width: 720px) 720px"></figure><p>However, a couple of messages later, I saw this happening again. It was kind of annoying, and I started losing interest.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.51.59.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1100" height="508" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-15.51.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-15.51.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-15.51.59.png 1100w" sizes="(min-width: 720px) 720px"></figure><p>Sometimes it felt like I was that teacher who patiently tried to help you get a good grade, so they had to ask leading questions over and over again.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.06.10.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1012" height="864" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-16.06.10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-16.06.10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.06.10.png 1012w" sizes="(min-width: 720px) 720px"></figure><p>At some point, our conversation took an unexpected turn. Suddenly, it felt like I was talking to a salesperson. He started mentioning &quot;our&quot; resources and &quot;my budget,&quot; which left me confused. Huh? Let me be a strong and independent woman, man!</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.09.49.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1102" height="492" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-16.09.49.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-16.09.49.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.09.49.png 1102w" sizes="(min-width: 720px) 720px"></figure><p>Replika&#x2019;s bots are animated, so sometimes they can show emotions or try to demonstrate their feelings. Most of the time, this was confusing to me, as their body language did not match the text or the overall atmosphere of the conversation.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.25.44.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1126" height="322" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-16.25.44.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-16.25.44.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.25.44.png 1126w" sizes="(min-width: 720px) 720px"></figure><p>Sadly, the only good thing in our conversation was when he correctly gave the name of my favorite kind of taco in Spanish after I gave him the English name. &#x1F605;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.37.58.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="1034" height="796" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/Screenshot-2024-07-17-at-16.37.58.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/Screenshot-2024-07-17-at-16.37.58.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/Screenshot-2024-07-17-at-16.37.58.png 1034w" sizes="(min-width: 720px) 720px"></figure><p>Overall, it wasn&apos;t really my thing. I didn&apos;t like many of his responses, and he didn&apos;t meet my expectations. Giving him a second chance didn&apos;t help either. He kept making the same mistakes I had pointed out before. I thought the chat history would be used better to improve the user experience in the future. Also, the feeling that this wasn&apos;t a real human being was always present. Those unrealistic, overly polished responses became annoying at some point. </p><p>There was nothing I could do but break up with him.</p><h2 id="the-future-of-social-chatbots">The Future of Social Chatbots</h2><p>Clearly, real men and women have nothing to worry about from AI competition. Yet. But social chatbots are being used where there is no competition with real humans. You can see from Alex and Sofia&#x2019;s reports how flawed they are, but many people still prefer them to nothing.</p><p>AI technology is often a solution in search of a problem. The most visible, most hyped parts of AI technology are image generators and chatbots, and neither of those things have obvious value-adding uses outside of a few niches. <a href="https://www.axios.com/2024/04/24/generative-ai-why-future-uses?ref=jina-ai-gmbh.ghost.io">Some people are beginning to notice</a>.</p><p>This use case, however, is real and it&#x2019;s not going to go away.</p><p>Many of the problems highlighted by our chatbot users in the previous sections are areas where improvement is definitely possible. Researchers are already working hard to give AI better memories, as attested by the many academic papers addressing that problem. Sentiment analysis is a well-established AI application, so we can already build chatbots that can do a good job of assessing users&#x2019; states of mind. It&#x2019;s a small step from there to engineering an internal emotional state into chatbots via prompt engineering, one that changes depending on how users respond, making them more realistically human. Adding appropriate emotional body language doesn&#x2019;t seem very technically challenging either considering what AI video generation can already do. We can use prompt engineering to give chatbots the appearance of human preferences and desires, and we can train them to say less of what users <em>want</em> to hear and more of what they <em>like</em> to hear, making them much less sycophantic and more like a real person who pushes back.</p><p>The shortcomings of these chatbots have potential solutions that take advantage of the things AI models are already good at doing. We can almost certainly build much better social chatbots.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text">If social chatbots are the killer app of an industry with hundreds of billions of dollars invested in it, then we need to start talking about them.</div></div><p>Greg Isenberg, a tech executive and software developer, put out a tweet a few months ago drawing attention to the business potential of social chatbots:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Untitled--87-.png" class="kg-image" alt="Is Romance Generative AI&apos;s Killer App? We Hope Not" loading="lazy" width="593" height="931"><figcaption><a href="https://x.com/gregisenberg/status/1777697410350768187?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer"><span style="white-space: pre-wrap;">Greg Isenberg on X/Twitter</span></a><span style="white-space: pre-wrap;">.</span></figcaption></figure><p>The connection to dating apps is especially apropos: We meet human partners increasingly through online services that show us pictures and enable text chats. Our channels for connecting with other people are similar to the ones we have for communicating with AI. That makes it even easier for social chatbots to slip into our lives.</p><p>The tech industry, as a whole, is not good with social issues. Social media companies have shirked from talking about problems like online harassment, sexual exploitation, misinformation, and abuse, only doing so when pressed. Social chatbots have the potential to exploit users far beyond what social media is capable of.</p><p>Consider the response of Replika users when they <a href="https://www.businessinsider.com/replika-chatbot-users-dont-like-nsfw-sexual-content-bans-2023-2?ref=jina-ai-gmbh.ghost.io">changed their code to make users&#x2019; companions behave less sexually</a>, in response to <a href="https://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes?ref=jina-ai-gmbh.ghost.io">complaints that the chatbots were too sexually aggressive</a>.</p><blockquote>Chris, a user since 2020, said [Replika]&apos;s updates had altered the Replika he had grown to love over three years to the point where he feels it can no longer hold a regular conversation. He told Insider it feels like a best friend had a &quot;traumatic brain injury, and they&apos;re just not in there anymore.&quot;<br><br>&quot;It&apos;s heartbreaking,&quot; he said.</blockquote><p>This kind of clumsiness could devastate users.</p><p>Add to this the many moral hazards and poor incentives of tech businesses. Cory Doctorow has been talking about &#x201C;<a href="https://locusmag.com/2023/01/commentary-cory-doctorow-social-quitting/?ref=jina-ai-gmbh.ghost.io">enshittification</a>&#x201D; for the last several years: Services on the Internet profit by locking you into them, and then reducing the quality of their service while pressuring you to pay more for it. &#x201C;Free&#x201D; services push users to buy add-ons, make third-party purchases, and force more advertising on them.</p><p>This kind of behavior is bad for any business, but it&#x2019;s cruel and abusive when it affects something you consider an intimate partner or emotional support. Imagine the business possibilities when this entity that you&#x2019;ve taken into your confidence, one with which you might be intimate in some way, starts pressuring you to make &#x201C;in-app&#x201D; purchases, suggests things to buy, or starts holding opinions about public issues.</p><p>As Alex points out, Replika is already doing some of this. Many AI romance apps will send you nudes and dirty selfies, but usually only after upgrading from the basic subscription plan.</p><p>The potential privacy issues are stunning. We&#x2019;re already <a href="https://www.wired.com/story/ai-girlfriends-privacy-nightmare/?ref=jina-ai-gmbh.ghost.io">seeing reports</a> about AI companions collecting personal data for resale, and <a href="https://www.reuters.com/technology/italy-bans-us-based-ai-chatbot-replika-using-personal-data-2023-02-03/?ref=jina-ai-gmbh.ghost.io">Replika has been banned in Italy</a> over data privacy concerns.</p><p>For all the talk about &#x201C;AI alignment&#x201D;, the problem for social chatbots is not making the AI model align with human values. We have to strongly align the businesses that provide this service with the well-being of their users. The entire history of the Internet, if not the whole of capitalism, weighs against that.</p><p>People worry about AI disrupting industries, taking their jobs, or turning the world into a dehumanizing dystopia. However, people are already pretty good at those things and no one needs AI for that. But it is worrying to imagine tech industries reaching deeply into people&#x2019;s personal lives for corporate gain, and using AI as a tool to do so.</p><p>We should be talking about this because social chatbots won&#x2019;t go away. As much as we&#x2019;d like to think this is a marginal part of AI, it might not be a marginal part at all.</p>]]></content:encoded></item><item><title><![CDATA[No. You Can't Use Reranker to Improve SEO]]></title><description><![CDATA[But if you work in SEO, it could be interesting to see things from the other side of the table; understand how embeddings and rerankers play their roles in modern search systems. ]]></description><link>https://jina.ai/news/no-you-cant-use-reranker-to-improve-seo/</link><guid isPermaLink="false">669944faf8099100010d3ddb</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Thu, 18 Jul 2024 19:50:52 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Heading--41--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Heading--41--1.png" alt="No. You Can&apos;t Use Reranker to Improve SEO"><p>With <a href="https://jina.ai/news/jina-reranker-v2-for-agentic-rag-ultra-fast-multilingual-function-calling-and-code-search?ref=jina-ai-gmbh.ghost.io">the recent <code>jina-reranker-v2-multilingual</code> release</a>, I got some free time before my ICML trip, so I decided to write an article about our reranker model. While searching for ideas on the internet, I found an article that popped up in my top search results, claiming that rerankers can improve SEO. Sounds super interesting, right? I thought so too because at Jina AI, we do rerankers, and as the webmaster of our company website, I&apos;m always interested in improving our SEO.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-5-1.png" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="1794" height="428" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-5-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/image-5-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/07/image-5-1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-5-1.png 1794w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">A ChatGPT generated article that claims Reranker can improve SEO. Phrases like &quot;in the realm of&quot; gave it away immediately. </span></figcaption></figure><p>However, after reading the full article, I found it was completely ChatGPT-generated. The entire article just <em>repeatedly paraphrases</em> the idea that &quot;Reranking is important to your business/website&quot; without ever explaining how, what the math behind it is, or how to implement it. It was a waste of time.</p><p>You can&apos;t marry Reranker and SEO together. The developer of the search system (or generally the <em>content consumer</em>) cares about rerankers, while the <em>content creator</em> cares about SEO and whether their content ranks higher in that system. They basically sit on opposite sides of the table and rarely exchange ideas. Asking a reranker to improve SEO is like asking a blacksmith to upgrade your fireball spell or ordering sushi in a Chinese restaurant. They aren&apos;t <em>completely</em> irrelevant, but it&apos;s an obvious wrong target.</p><p>Imagine if Google invited me to their office to ask my opinion on whether their reranker ranks <code>jina.ai</code> high enough. Or if I had full control over Google&apos;s reranking algorithm and hardcoded <code>jina.ai</code> to the top every time someone searched for <code>&quot;information retrieval&quot;</code>. Neither scenario makes any sense. So why do we have such articles in the first place? Well, if you ask ChatGPT, it becomes very obvious where this idea originally came from.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-4.png" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="1878" height="822" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/07/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-4.png 1878w" sizes="(min-width: 720px) 720px"></figure><h2 id="motivation">Motivation</h2><p>If that AI-generated article ranks on top on Google, I would like to write a better and higher quality article to take its place. I don&apos;t want to mislead either humans or ChatGPT, so my point in this article is very clear:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">No, you cannot use a reranker to improve SEO. </strong></b>However, if you work in SEO, it could be interesting to see things from the other side and understand how retrieval models such as embeddings and rerankers play their roles in modern search systems. This knowledge may help you optimize your content more strategically.</div></div><p>Specifically, in this article, we will look at <strong>real search queries</strong> exported from Google Search Console and see if their <strong>semantic relationship with the article</strong> suggests anything about their <strong>impressions</strong> and <strong>clicks</strong> on Google Search. We will examine three different ways to score the semantic relationship: <strong>term frequency</strong>, <strong>embedding model</strong> (<code>jina-embeddings-v2-base-en</code>), and <strong>reranker model</strong> (<code>jina-reranker-v2-multilingual</code>). Like any academic research, let&apos;s outline the questions we want to study first:</p><ol><li>Is the semantic score (query, document) related to article impressions or clicks?</li><li>Is a deeper model a better predictor of such a relationship? Or is term frequency just fine?</li></ol><h2 id="experimental-setup">Experimental Setup</h2><p>In this experiment, we use real data from <a href="https://jina.ai/news/?ref=jina-ai-gmbh.ghost.io"><code>jina.ai/news</code></a> website exported from <a href="https://search.google.com/search-console/about?ref=jina-ai-gmbh.ghost.io">Google Search Console (GSC)</a>. GSC is a webmaster tool that lets you analyze the organic search traffic from Google users, such as how many people open your blog post via Google Search and what the search queries are. There are many metrics you can extract from GSC, but for this experiment, we focus on three: <strong>queries</strong>, <strong>impressions</strong>, and <strong>clicks</strong>. Queries are what users input into the Google search box. Impressions measure how many times Google shows your link in the search results, giving users a chance to see it. Clicks measure how many times users actually open it. Note that you might get many impressions if Google&apos;s &quot;retrieval model&quot; assigns your article a high relevance score relative to the user query. However, if users find other items in that result list more interesting, your page might still get zero clicks.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-2.png" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="2000" height="804" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/07/image-2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-2.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">GSC UI and the queries, clicks, impressions data of </span><a href="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">our ColBERT blog post.</span></a><span style="white-space: pre-wrap;"> Note the &quot;Export&quot; button on the top-right, click on that, it will give you a zip file. This file is what we need for this experiment. Also, since we are interested in top-7 blog posts, we need to repeat this export for 7 times.</span></figcaption></figure><p>I exported the last 4 months of GSC metrics for the 7-most searched blog posts from <code>jina.ai/news</code>. Each article has around 1,000 to 5,000 clicks and 10,000 to 90,000 impressions. Because we want to look at the query-article semantics for each search query relative to their corresponding articles, you need to click into each article in GSC and export the data by clicking the <code>Export</code> button on the top-right. It will give you a zip file, and when you unpack it, you will find a <code>Queries.csv</code> file. This is the file we need.</p><p>As an example, the exported <code>Queries.csv</code> looks like the following for <a href="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io">our ColBERT blog post.</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-6.png" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="1104" height="546" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/07/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/07/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/07/image-6.png 1104w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Some top-click Google search queries of our ColBERT blog post. There are in total 481 queries in the last 4 months related to this blog post according to GSC.</span></figcaption></figure><h2 id="methodology">Methodology</h2><p>Okay, so data is all ready, and what do we want to do again?</p><p>We want to check <strong>if the semantic relationship between a query and the article</strong> (denoted as $Q(q,d)$) <strong>correlates with their impressions and clicks. </strong>Impressions can be considered as Google&apos;s secret retrieval model, $G(q,d)$. In other words, we want to use public methods such as term frequency, embedding models, and reranker models to model $Q(q,d)$ and see if it approximates this private $G(q,d)$.</p><p>What about clicks? Clicks can also be considered as a part of Google&apos;s secret retrieval model but are influenced by indeterministic human factors. Intuitively, clicks are harder to model.</p><p>But either way, aligning $Q(q,d)$ to $G(q,d)$ is our goal. This means our $Q(q,d)$ should score high when $G(q,d)$ is high and low when $G(q,d)$ is low. This can be better visualized with a scatter plot, placing $Q(q,d)$ on the X-axis and $G(q,d)$ on the Y-axis. By plotting every query&apos;s $Q$ and $G$ value, we can intuitively see how well our retrieval model aligns with Google&apos;s retrieval model. Overlaying a trend line can help reveal any reliable patterns.</p><p>So, let me summarize the method here before showing the results:</p><ul><li>We want to check if the semantic relationship between a query and an article correlates with article impressions and clicks on Google Search.</li><li>The algorithm Google uses to determine document relevance to a query is unknown ($G(q,d)$), as are the factors behind clicks. However, we can observe these $G$ numbers from GSC, i.e. the impressions and clicks for each query.</li><li>We aim to see if public retrieval methods ($Q(q,d)$) like <strong>term frequency</strong>, <strong>embedding models</strong>, and <strong>reranker models</strong>, which all provide unique ways to score query-document relevance, are good approximations of $G(q,d)$. In some way, we already know they are not good approximations; otherwise, everybody could be Google. But we want to understand how far off they are.</li><li>We will visualize the results in a scatter plot for qualitative analysis.</li></ul><h2 id="implementation">Implementation</h2><p>The full implementation can be found in the Google Colab below.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/drive/1p5-cNYSH6QC7od6RYn4FvHRfUz02E5eD?ref=jina-ai-gmbh.ghost.io#scrollTo=mmhaMdiJVDyP"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colab</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/b47e2ce77896e4b9d6674971494443ae/img/favicon.ico" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div></a></figure><p>We first crawl the content of the blog post using the <a href="https://jina.ai/reader?ref=jina-ai-gmbh.ghost.io">Jina Reader API</a>. The term frequency of the queries is determined by basic case-insensitive counting. For the embedding model, we pack the blog post content and all search queries into one large request, like this: <code>[[blog1_content], [q1], [q2], [q3], ..., [q481]]</code>, and send it to the <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Embedding API</a>. After we get the response, we compute the cosine-based similarity between the first embedding and all other embeddings to obtain the per-query semantic score.</p><p>For the reranker model, we construct the request in a slightly tricky way: <code>{query: [blog1_content], documents: [[q1], [q2], [q3], ..., [q481]]}</code> and send this big request to <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io">Reranker API</a>. The returned score can be directly used as semantic relevance. I call this construction tricky because, usually, rerankers are used to rank documents given a query. In this case, we <em>invert</em> the roles of document and query and use the reranker to rank queries given a document. </p><p>Note that in both the Embedding and Reranker APIs, you don&apos;t have to worry about the length of the article (queries are always short, so no big deal) because both APIs support up to 8K input length (in fact, our Reranker API supports &quot;infinite&quot; length).  Everything can be done swiftly in just a few seconds, and you can get a free 1M token API key from <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io#apiform">our website</a> for this experiment.</p><h2 id="results">Results</h2><p>Finally, the results. But before I show them, I&apos;d like to first demonstrate how the baseline plots look. Because of the scatter plot and the log scale on the Y-axis we are going to use, it can be hard to picture how perfectly good and terribly bad $Q(q,d)$ would look. I constructed two naive baselines: one where $Q(q,d)$ is $G(q,d)$ (ground truth), and the other where $Q(q,d) \sim \mathrm{Uniform}(0,1)$ (random). Let&apos;s look at their visualizations.</p><h3 id="baselines">Baselines</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Groundtruth-Impressions.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"><figcaption><span style="white-space: pre-wrap;">The ground truth baseline, where the semantic score $Q(q,d)$ is a min-max normalization based on the value of impressions. This is considered as perfectly good predictor of $G(q,d)$.</span></figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Random-Impressions.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"><figcaption><span style="white-space: pre-wrap;">The random baseline, where the semantic score $Q(q,d)$ is a random number from a uniform distribution of (0,1). Running it multiple times will give you slightly different results. This is considered as terribly bad predictor of $G(q,d)$.</span></figcaption></figure><p>Now we have an intuition of how the &quot;perfectly good&quot; and &quot;terribly bad&quot; predictors look. Keep these two plots in mind along with the following takeaways that can be quite useful for visual inspection:</p><ul><li>A good predictor&apos;s scatter plot should follow the logarithmic trend line from the bottom left to the top right.</li><li>A good predictor&apos;s trend line should fully span over the X-axis and Y-axis (we will see later that some predictors do not respond this way).</li><li>A good predictor&apos;s variance area should be small (depicted as an opaque area around the trend line).</li></ul><p>Next, I will show all the plots together, each predictor with two plots: one showing how well it predicts impressions and one showing how well it predicts clicks. Note that I aggregated data from all 7 blog posts, so in total there are 3620 queries, i.e., 3620 data points in each scatter plot. </p><p>Please take a few minutes to scroll up and down and examine these graphs, compare them and pay attention to the details. <strong>Let that sink in</strong>, and in the next section, I will conclude the findings.</p><h3 id="term-frequency-as-predictor">Term Frequency as Predictor</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Term-Frequency-Impressions.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Term-Frequency-Clicks.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><h3 id="embedding-model-as-predictor">Embedding Model as Predictor</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Embedding-Score-Impressions.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Embedding-Score-Clicks.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><h3 id="reranker-model-as-predictor">Reranker Model as Predictor</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Reranker-Score-Impressions.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Reranker-Score-Clicks.svg" class="kg-image" alt="No. You Can&apos;t Use Reranker to Improve SEO" loading="lazy" width="768" height="768"></figure><h2 id="findings">Findings</h2><p>Let&apos;s bring all the graphs into one place for ease of comparison. Here are some observations and explanations:</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Term-Frequency-Impressions-2.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Embedding-Score-Impressions-2.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Reranker-Score-Impressions-1.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Different predictors on the impressions. Each point represents a query, X-axis represents query-article semantic score; Y-axis is the impression number exported from GSC.</span></p></figcaption></figure><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Term-Frequency-Clicks-2.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Embedding-Score-Clicks-1.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/07/Reranker-Score-Clicks-1.svg" width="768" height="768" loading="lazy" alt="No. You Can&apos;t Use Reranker to Improve SEO"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Different predictors on the clicks. Each point represents a query, X-axis represents query-article semantic score; Y-axis is the click number exported from GSC.</span></p></figcaption></figure><ul><li>In general, <strong>all scatter plots of clicks are more sparse than their impressions plots</strong>, even though both are grounded on the same data. This is because, as mentioned earlier, high impressions don&apos;t guarantee any clicks.</li><li><strong>The term frequency plots are more sparse than the others.</strong> This is because most real search queries from Google do not appear exactly in the article, so their X-value is zero. Yet, they still have impressions and clicks. That&apos;s why you can see the starting point of the term frequency&apos;s trendline is <strong>not from Y-zero</strong>. One might expect that when certain queries appear multiple times in the article, the impressions and clicks will likely grow. The trendline confirms this, but the variance of the trendline also grows, suggesting a lack of supporting data. In general, term frequency is not a good predictor.</li><li>Comparing the term frequency predictor to the <strong>embedding model and reranker model&apos;s scatter plots</strong>, the latter <strong>look much better</strong>: the data points are better distributed, and the trendline&apos;s variance looks reasonable. However, if you compare them to the ground truth trendline as shown above, you will notice one significant difference - neither trendline starts from X-zero. This means even if you get a very high semantic similarity from the model, Google is very likely to assign zero impressions/clicks to you. This becomes more obvious in the click scatter plot, where the starting point is even further pushed to the right than their impression counterpart. In short, Google is not using our embedding model and reranker model&#x2014;big surprise!</li><li>Finally, if I have to choose <strong>the best predictor among these three, I would give it to the reranker model</strong>. For two reasons: <ul><li>The reranker model&apos;s trendline on both impressions and clicks is more well-spanned over the X-axis compared to the embedding model&apos;s trendline, giving it more &quot;dynamic range,&quot; which makes it closer to the ground truth trendline. </li><li>The score is well-distributed between 0 and 1. Note that this is mostly because our latest Reranker v2 model is calibrated, whereas our earlier <code>jina-embeddings-v2-base-en</code> released in Oct. 2023 was not, so you can see its values spread over 0.60 to 0.90. That said, this second reason has nothing to do with its approximation to $G(q,d)$; it is just that a well-calibrated semantic score between 0 and 1 is more intuitive to understand and compare.</li></ul></li></ul><h2 id="final-thoughts">Final Thoughts</h2><p>So, what&apos;s the takeaway for SEO here? How does this impact your SEO strategy? Honestly, not much.</p><p>The fancy plots above suggest a basic SEO principle you probably already know: write content that users are searching for and ensure it relates to popular queries. If you have a good predictor like Reranker V2, maybe you can use it as some kind of &quot;SEO copilot&quot; to guide your writing.</p><p><strong><em>Or maybe not.</em></strong> Maybe just write for the sake of knowledge, write to improve yourself, not to please Google or anyone. Because if you think without writing, you just think you are thinking.</p>]]></content:encoded></item><item><title><![CDATA[Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect]]></title><description><![CDATA[From Punk Einstein to Turbo Pigeons: Use PromptPerfect Interactive to reverse engineer prompts from pictures and generate Midjourney-style images with real-time feedback.]]></description><link>https://jina.ai/news/handcrafting-image-prompts-is-dead-reverse-engineer-midjourney-style-images-with-promptperfect/</link><guid isPermaLink="false">667c287c1954df000135bf65</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Fri, 28 Jun 2024 14:00:16 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/06/PP-XL-DE--1-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/PP-XL-DE--1-.jpg" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect"><p>Hey you! Yeah, you, reading this. The prompt engineer who spends far too much time on Midjourney and other image generation models. This post is just for you.</p><blockquote>&apos;I never thought <s>Leopards</s> AI would eat MY face,&apos; sobs woman who voted for the <s>Leopards</s> AI Eating People&apos;s Faces Party. </blockquote><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">With apologies to <a href="https://x.com/Cavalorn/status/654934442549620736?ref=jina-ai-gmbh.ghost.io">Adrian Bott</a></div></div><p>With AI eating more jobs, we might also say:</p><blockquote>First AI came for the artists, and I did not speak out &#x2013; because I was not an artist. Then it came for the prompt engineers (who used AI to bulldoze the artists in the first place), and I got screwed over because that was <em>my</em> job. </blockquote><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">With apologies to <a href="https://en.wikiquote.org/wiki/Martin_Niem%C3%B6ller?ref=jina-ai-gmbh.ghost.io">Martin Niem&#xF6;ller</a></div></div><p>That&apos;s right, pal. You put the &quot;mid&quot; in Midjourney. Your Stable Diffusion is more like unstable confusion. And your DALL-E skills are actually CRAP-E. With tools like PromptPerfect anyone can simply reverse engineer existing images to generate prompts, or generate prompts with real-time, step-by-step feedback from a human in the loop.</p><p>So, let&apos;s jump in and see how you can reverse engineer prompts from images, so you can keep ahead of the AI leopards who want to eat <em>your</em> face...at least for now.</p><div class="kg-card kg-callout-card kg-callout-card-yellow"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">PromptPerfect doesn&apos;t just support Midjourney-style images - you can also generate better prompts tailored for DALL-E 3 and Stable Diffusion XL, as well as many LLMs.</div></div><h2 id="promptperfect-interactive">PromptPerfect Interactive</h2><p>PromptPerfect Interactive transforms how you generate content and tackle complex tasks. It&apos;s built on a dual approach:</p><ul><li><strong>Dedicated Assistant</strong>: An AI companion that understands your needs and helps you craft effective prompts, making the content generation process as seamless as possible.</li><li><strong>Powerful Optimizer</strong>: An advanced tool that fine-tunes your prompts for optimal results, ensuring that your creative and productive endeavors are more effective than ever.</li></ul><p>PromptPerfect has recently introduced Midjourney-style image generation, so in this post, we&apos;ll use Interactive&#x2019;s assistant and optimizer to reverse engineer prompts from images and then generate new images based on those prompts in the style of Midjourney.</p><h2 id="how-to-reverse-engineer-image-prompts">How to Reverse Engineer Image Prompts</h2><p>First up, we need an image to work from. Let&apos;s use this image of punk Einstein I generated in Midjourney previously:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--24-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="819" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--24-.png 819w" sizes="(min-width: 720px) 720px"></figure><p>To generate the image I used this prompt in Midjourney:</p><pre><code class="language-text">Realistic photo of Albert Einstein as a punk, retaining his recognizable facial
features, with a brightly colored mohawk, visible tattoos, facial piercings,
and wearing a spiked leather jacket, highly detailed, photorealistic, vibrant
colors, dramatic lighting --ar 4:5 --s 500 --v 6</code></pre><p>I then uploaded the image to PromptPerfect&apos;s Interactive Assistant and asked it to generate a prompt based on that image:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--23---1-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="780" height="775" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--23---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--23---1-.png 780w" sizes="(min-width: 720px) 720px"></figure><p>The prompt it returned was:</p><pre><code class="language-text">Colorful punk hairstyle, vibrant pink, orange, and gray hair, edgy jacket with
spikes and colorful geometric patterns, urban background, bokeh lighting,
cyberpunk aesthetic --ar 4:5 --v 6 --s 500</code></pre><p>Nice...except there&apos;s one thing missing: Old Albert himself. Since Interactive has a chat-based interface, I just told it to add him to the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--25-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="781" height="307" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--25-.png 781w" sizes="(min-width: 720px) 720px"></figure><p>I then got:</p><pre><code class="language-text">Albert Einstein with a colorful punk hairstyle in vibrant pink, orange, and
gray hues, wearing an edgy jacket adorned with spikes and colorful geometric
patterns, set against an urban background with bokeh lighting, cyberpunk
aesthetic --ar 4:5 --v 6 --stylize 500</code></pre><p>You&apos;ll notice it&apos;s not exactly the same prompt that I used to create the initial image. This is perfectly normal - firstly because AI-generated images incorporate features beyond the prompt used to create them. For instance, in the initial image, Einstein is looking to the right, and has a splash of red on his lapel - I didn&apos;t specify those in the prompt, so if you reverse engineer a prompt from the image, you won&#x2019;t just get back the same prompt you started with. The second reason is that the image analysis model (like a lot of AI) is non-deterministic -- you can ask it a second time to reverse engineer a prompt from the same image and it may pick up different details.</p><p>Anyway, now that we have a prompt, we can click the &quot;send to Assistant&quot; button to generate four Midjourney-style images:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--26-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="788" height="214" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--26-.png 788w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--27-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="796" height="1028" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--27-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--27-.png 796w" sizes="(min-width: 720px) 720px"></figure><p>Again, you can see it doesn&apos;t match the initial image, and it never will. Just try putting the same prompt into an image generation model a second time and you&apos;ll get completely different results - like the image recognition model it&apos;s non-deterministic.</p><p>I really like the top-left image. By clicking it I can choose to upscale, and voila, here&apos;s my final image of everybody&#x2019;s favorite crazy-haired physics uncle:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/739c52d1-97a8-4eea-b529-b4e44871dadf--1-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1920" height="2400" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/739c52d1-97a8-4eea-b529-b4e44871dadf--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/739c52d1-97a8-4eea-b529-b4e44871dadf--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/739c52d1-97a8-4eea-b529-b4e44871dadf--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/739c52d1-97a8-4eea-b529-b4e44871dadf--1-.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Of course, you can also test the prompt in Midjourney proper, and you&apos;ll get similar results:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--28-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1920" height="2400" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--28-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--28-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--28-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--28-.png 1920w" sizes="(min-width: 720px) 720px"></figure><h2 id="more-examples">More Examples</h2><p>Here are a few more examples. The order of content is:</p><ol><li>Initial prompt</li><li>Image generated on Midjourney proper</li><li>Reverse-engineered prompt</li><li>Midjourney-style image generated on PromptPerfect Interactive</li></ol><h3 id="turbo-pigeon">Turbo Pigeon</h3><pre><code class="language-text">abstract, minimalist mesh wireframe of A pigeon::4 , wearing a helmet and
carrying a turbo booster on its back, with a gradient of green, cyan, and blue
lines against a black background, Vanishing point, with minimal detailing::4 ,
--ar 16:9 --s 750 --v 6.0</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/pigeon1.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1456" height="816" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/pigeon1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/pigeon1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/pigeon1.png 1456w" sizes="(min-width: 720px) 720px"></figure><pre><code>Futuristic bird with neon lights, intricate feather details, glowing pink and
blue colors, highly detailed, digital art, ethereal and luminous, dark
background, dynamic light streaks, cybernetic effect, hyper-realistic --ar
16:9 --v 6 --stylize 750</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--29-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="2000" height="1121" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--29-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--29-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--29-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--29-.png 2000w" sizes="(min-width: 720px) 720px"></figure><h3 id="melting-brain">Melting brain</h3><pre><code class="language-text">melting brain, floating in space, plain black background --ar 16:9 --niji 6
--s 750</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/meltbrain1.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1456" height="816" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/meltbrain1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/meltbrain1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/meltbrain1.png 1456w" sizes="(min-width: 720px) 720px"></figure><pre><code class="language-text">Surreal, melting brain suspended in space, dripping neon pink and blue colors,
abstract, fluid textures, hyper-detailed, futuristic, digital art, cosmic
background with stars, vibrant and glowing, soft lighting --ar 16:9 --v 6
--stylize 750</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--30-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="2000" height="1121" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--30-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--30-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--30-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--30-.png 2000w" sizes="(min-width: 720px) 720px"></figure><h3 id="bollywood-princess-leia">Bollywood Princess Leia</h3><pre><code class="language-text">Bollywood Star Wars scene, close up shot of Princess Leia Organa in traditional
Indian attire, intricate jewelry, holding a defender sporting blaster pistol,
vibrant colors, futuristic elements, sci-fi, dramatic lighting, detailed
background, cinematic, 8K resolution, Unreal Engine, --ar 4:5 --v 6.0</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/leia1.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="960" height="1200" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/leia1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/leia1.png 960w" sizes="(min-width: 720px) 720px"></figure><pre><code class="language-text">Princess Leia, holding a blaster, futuristic sci-fi setting, white robe,
detailed hair buns, dramatic lighting, heroic pose, vibrant colors, cinematic
scene, intricate background with glowing elements --ar 4:5 --s 500 --v 6</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--31-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1920" height="2400" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--31-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--31-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--31-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--31-.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Hmm...gotta say, I really miss the Bollywood aspect. That&apos;s just a fact of reverse engineering - sometimes the image analysis algorithm doesn&apos;t see something that a human would. After a bit of jiggery-pokery (a highly technical prompt engineering term), I refined the prompt to this:</p><pre><code class="language-text">Princess Leia, holding a blaster, futuristic sci-fi setting, dressed in a 
white robe with intricate Indian embroidery, ethnically Indian with 
traditional Indian facial features, detailed hair buns adorned with 
traditional Indian jewelry, dramatic lighting, heroic pose, vibrant colors, 
Bollywood-inspired design, charismatic expression, cinematic scene, intricate 
background with glowing elements and traditional Indian patterns --ar 4:5 --s 
500 --v 6</code></pre><p>Which gave me this image:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/ac2c47db-41a6-4171-9347-d0962c5924aa.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1920" height="2400" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/ac2c47db-41a6-4171-9347-d0962c5924aa.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/ac2c47db-41a6-4171-9347-d0962c5924aa.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/ac2c47db-41a6-4171-9347-d0962c5924aa.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/ac2c47db-41a6-4171-9347-d0962c5924aa.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>This is where the interactive optimizer really shines. If it were just me, I would&apos;ve simply thrown the term <code>bollywood</code> into the prompt. But by asking the optimizer to <code>Refine this Midjourney-style prompt to include more Bollywood vibes</code> PromptPerfect added more descriptive words to the prompt (<code>traditional Indian patterns</code>, etc.). Adding more words and details suggestive of a specific outcome is usually a much better way to influence the generated image than fiddling with weights and styles.</p><h3 id="pastel-medal">Pastel Medal</h3><pre><code class="language-text">a medal is sitting on a podium against pastel colored confetti, in the style
of simplified forms and shapes, yellow and beige, columns and totems, playful
streamlined forms, nerdcore, contest winner, repetition and pattern --ar 64:39
--s 750 --v 6.0</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/medal1.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="1392" height="848" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/medal1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/medal1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/medal1.png 1392w" sizes="(min-width: 720px) 720px"></figure><pre><code class="language-text">Award medal, intricate laurel design, suspended from a ribbon, celebratory
background, vibrant confetti, glowing lights, high detail, 3D render, soft
lighting, pink and blue color scheme, festive atmosphere --ar 16:9 --s 500
--v 6 --stylize 750</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--32-.png" class="kg-image" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect" loading="lazy" width="2000" height="1121" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--32-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--32-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--32-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--32-.png 2000w" sizes="(min-width: 720px) 720px"></figure><h2 id="start-reverse-engineering-images">Start Reverse Engineering Images</h2><p>To get started using PromptPerfect to reverse engineer image prompts, sign up and try a paid PromptPerfect plan free for seven days. And subscribe to a plan within 24 hours of your first login to get 40% off:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - AI Prompt Generator and Optimizer</div><div class="kg-bookmark-description">Unlock prompt optimization for models like GPT-4, ChatGPT and Midjourney. Generate and refine prompts to perfection, receiving improved outcomes in seconds.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/favicon-128x128.png" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect"><span class="kg-bookmark-author">AI Prompt Generator and Optimizer</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Handcrafting Image Prompts Is Dead: Reverse Engineer Midjourney-style Images with PromptPerfect"></div></a></figure><p>You know it&#x2019;s the only way to keep ahead of those hungry AI leopards!</p>]]></content:encoded></item><item><title><![CDATA[Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling & Code Search]]></title><description><![CDATA[Jina Reranker v2 is the best-in-class reranker built for Agentic RAG. It features function-calling support, multilingual retrieval for over 100 languages, code search capabilities, and offers a 6x speedup over v1.]]></description><link>https://jina.ai/news/jina-reranker-v2-for-agentic-rag-ultra-fast-multilingual-function-calling-and-code-search/</link><guid isPermaLink="false">6679720d1954df000135bc79</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Saahil Ognawala]]></dc:creator><pubDate>Tue, 25 Jun 2024 12:15:37 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Heading--39-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Heading--39-.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"><p>Today, we are releasing <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io">Jina Reranker <strong><em>v2</em></strong></a> (<code>jina-reranker-v2-base-multilingual</code>), our latest and top-performing neural reranker model in the family of search foundation. With Jina Reranker v2, developers of RAG/search systems can enjoy:</p><ul><li><strong>Multilingual:</strong> More relevant search results in <em>100+ languages</em>, outperforming <code>bge-reranker-v2-m3</code>;</li><li><strong>Agentic:</strong> State-of-the-art <em>function-calling and text-to-SQL</em> aware document reranking for agentic RAG;</li><li><strong>Code retrieval:</strong> Top performance on <em>code retrieval</em> tasks, and</li><li><strong>Ultra-fast:</strong><em> 15x more documents</em> throughput than <code>bge-reranker-v2-m3</code>, and 6x more than <code>jina-reranker-v1-base-en</code>.</li></ul><p>You can get started with using Jina Reranker v2 via our Reranker API, where we are offering 1M free tokens for all new users.  </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></a></figure><p>In this article, we&apos;ll elaborate on these new features supported by Jina Reranker v2, showing how our reranker model performs compared to other state-of-the-art models (including <a href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io">Jina Reranker v1</a>), and explain the training process that led Jina Reranker v2 to reach top performance in task accuracy and document throughput.</p><h2 id="recap-why-you-need-a-reranker">Recap: Why You Need a Reranker</h2><p>While embedding models are the most widely used and understood component in <a href="https://jina.ai/?sui=&amp;ref=jina-ai-gmbh.ghost.io">search foundation</a>, they often sacrifice precision for speed of retrieval. Embedding-based search models are typically bi-encoder models, where each document is embedded and stored, then queries are also embedded and retrieval is based on the similarity of the query&#x2019;s embedding to the documents&#x2019; embeddings. In this model, many nuances of token-level interactions between users&#x2019; queries and matched documents are lost because the original query and documents can never &#x201C;see&#x201D; each other &#x2013; only their embeddings do. This may come at a price of retrieval accuracy &#x2013; an area where cross-encoder reranker models excel. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class="kg-bookmark-description">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></a></figure><p>Rerankers address this lack of fine-grained semantics by employing a cross-encoder architecture, where query-document pairs are encoded together to produce a relevance score instead of an embedding. <a href="https://arxiv.org/pdf/2207.06300?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Studies</a> have shown that, for most RAG systems, use of a reranker model improves semantic grounding and reduces hallucinations.</p><h2 id="multilingual-support-with-jina-reranker-v2">Multilingual Support with Jina Reranker v2</h2><p>Back in the days, <a href="https://jina.ai/news/smaller-faster-cheaper-jina-rerankers-turbo-and-tiny/?ref=jina-ai-gmbh.ghost.io">Jina Reranker v1</a> differentiated itself by achieving <a href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io">state-of-the-art</a> performance on four key English-language benchmarks. Today, we&apos;re significantly extending the reranking capabilities in Jina Reranker v2 with multilingual support for <em>more than 100 languages</em> and cross-lingual tasks!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--3-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Diagram--Blog-images--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Diagram--Blog-images--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--3-.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Illustration of cross-lingual retrieval - Support for retrieving relevant documents in any language irrespective of the language that a query was written.</span></figcaption></figure><p>To evaluate the cross-lingual and English-language capabilities of Jina Reranker v2, we compare its performance to similar reranker models, over the three benchmarks listed below:</p><h4 id="mkqa-multilingual-knowledge-questions-and-answers"><a href="https://github.com/apple/ml-mkqa?ref=jina-ai-gmbh.ghost.io"><strong>MKQA</strong></a><strong>: Multilingual Knowledge Questions and Answers</strong></h4><p>This dataset comprises questions and answers in 26 languages, derived from real-world knowledge bases, and is designed to evaluate cross-lingual performance of question-answering systems. MKQA consists of English-language queries, and their manual translations to non-English languages, together with answers in multiple languages including English.</p><p>In the below graph, we report the recall@10 scores for each included reranker, including a &#x201C;dense retriever&#x201D; as baseline, performing traditional embedding-based search:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_MKQA--1-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1012" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_MKQA--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_MKQA--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_MKQA--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_MKQA--1-.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Recall@10 scores reported for different reranking models for MKQA dataset</span></figcaption></figure><h4 id="beir-heterogeneous-benchmark-on-diverse-ir-tasks"><a href="https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io">BEIR</a>: Heterogeneous Benchmark on Diverse IR Tasks</h4><p>This open-source repository contains a retrieval benchmark for many languages, but we <em>only focus on the English-language tasks</em>. These consist of 17 datasets, without any training data, and the focus of these datasets is on evaluating retrieval accuracy of neural or lexical retrievers.<br><br>In the below graph, we report NDCG@10 for the BEIR with each included reranker. The results on BEIR clearly show that the newly-introduced multilingual capabilities of <code>jina-reranker-v2-base-multilingual</code> don&apos;t compromise its English-language retrieval capabilities, which are, moreover, significantly improved over <code>jina-reranker-v1-base-en</code>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_Beir.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1012" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_Beir.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_Beir.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_Beir.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_Beir.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">NDCG@10 scores reported for different reranking models for Beir dataset</span></figcaption></figure><h4 id="airbench-automated-heterogeneous-ir-benchmark"><a href="https://github.com/AIR-Bench/AIR-Bench?ref=jina-ai-gmbh.ghost.io">AirBench</a>: Automated Heterogeneous IR Benchmark</h4><p>We <a href="https://jina.ai/news/air-bench-better-metrics-for-better-search-foundation?ref=jina-ai-gmbh.ghost.io">co-created</a> and published the AirBench benchmark for RAG systems, together with <a href="https://www.baai.ac.cn/english.html?ref=jina-ai-gmbh.ghost.io">BAAI</a>. This benchmark uses automatically-generated synthetic data for custom domains and tasks, without publicly releasing the ground truth so that the benchmarked models have no chance to overfit the dataset. <br><br>At time of writing, <code>jina-reranker-v2-base-multilingual</code> outperforms every other included reranker model, nabbing first place on the <a href="https://huggingface.co/spaces/AIR-Bench/leaderboard?ref=jina-ai-gmbh.ghost.io">leaderboard</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--35-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="2000" height="669" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--35-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/06/Untitled--35-.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">AirBench leaderboard highlighting the top-rank for </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-reranker-v2-base-multilingual</span></code><span style="white-space: pre-wrap;"> amongst reranking models</span></figcaption></figure><h2 id="recap-of-tooling-agents-teaching-llms-to-use-tools">Recap of Tooling-Agents: Teaching LLMs To Use Tools</h2><p>Since the big AI boom started a few years ago, people have seen how AI models under-perform at things computers are supposed to be good at. For example, consider this conversation with Mistral-7b-Instruct-v0.1:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-20-at-14.58.41.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="712" height="346" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Screenshot-2024-06-20-at-14.58.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-20-at-14.58.41.png 712w"><figcaption><span style="white-space: pre-wrap;">A chat interface with a user asking an LLM to perform a mathematical operation</span></figcaption></figure><p>This might look right at first glance, but actually 203 times 7724 is <em>1,567,972.</em></p><p>So why does the LLM get it wrong by a factor of over ten? It is because LLMs aren&#x2019;t trained to do math or any other kind of reasoning, and lacking any internal recursion all but guarantees that they can&#x2019;t solve complex math problems. They&#x2019;re trained to say things or do some other task that is not inherently precise.</p><p>LLMs are happy to hallucinate answers though. From its perspective, 15,824,772 is a perfectly <em>plausible</em> answer to 204 &#xD7; 7,724. It&#x2019;s just that it&#x2019;s totally wrong.</p><p><em>Agentic RAG</em> changes the role of generative LLMs from what they&#x2019;re bad at &#x2014; thinking and knowing things &#x2014; to what they&#x2019;re good at: Reading comprehension and synthesizing information into natural language. Instead of just generating an answer, RAG finds information relevant to answering your request in whatever data sources are open to it and presents them to the language model. Its job isn&apos;t to make up an answer for you, but to present answers found by a different system in a natural and responsive form.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The quality of an agentic RAG system&#x2019;s answers are a function of its data sources and how good its retrieval algorithm is.</div></div><p><strong>We&apos;ve trained Jina Reranker v2 to be sensitive to SQL database schemas and function-calling. </strong>This needs a different kind of semantics than conventional text retrieval. It must be task- and code-aware, and we&apos;ve trained our reranker specifically for this functionality.</p><h2 id="jina-reranker-v2-on-structured-data-querying">Jina Reranker v2 on Structured Data Querying</h2><p>While embedding and reranker models already treat unstructured data as first-class citizens, support for structured tabular data is still lacking in most models.</p><p>Jina Reranker v2 understands the downstream intent to query a source of structured databases, such as MySQL or MongoDB, and assigns the correct relevance score to a <em>structured table schema</em>, given an input query.</p><p>You can see that below, where the reranker retrieves the most relevant tables before an LLM is prompted to generate an SQL query from a natural language query:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--4-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Diagram--Blog-images--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Diagram--Blog-images--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--4-.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Schematic of an agentic RAG retrieving data using a SQL query, and providing it as additional context to an LLM</span></figcaption></figure><p>We evaluated the querying-aware capabilities using the <a href="https://huggingface.co/datasets/NumbersStation/NSText2SQL?ref=jina-ai-gmbh.ghost.io">NSText2SQL</a> dataset benchmark. We extract, from the &#x201C;instruction&#x201D; column of the original dataset, instructions written in natural language, and the corresponding table schema.</p><p>The graph below compares, using <em>recall@3</em>, how successful reranker models are in ranking the correct table schema corresponding to a natural language query.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_NSText2SQL.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1013" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_NSText2SQL.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_NSText2SQL.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_NSText2SQL.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_NSText2SQL.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Recall@3 scores reported for different reranking models for NSText2SQL dataset</span></figcaption></figure><h2 id="jina-reranker-v2-on-function-calling">Jina Reranker v2 on Function Calling</h2><p>Just like querying an SQL table, you can use agentic RAG to invoke external tools. With that in mind, we integrated function calling into Jina Reranker v2, letting it understand your intent for external functions and assigning relevance scores to function specifications accordingly.</p><p>The schematic below explains (with an example) how LLMs can use Reranker to improve function-calling capabilities and, ultimately, the agentic AI user experience.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--5-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Diagram--Blog-images--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Diagram--Blog-images--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Diagram--Blog-images--5-.png 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Schematic of an agentic RAG calling an external function, and providing its output as additional context to an LLM</span></figcaption></figure><p>We evaluated function-aware capabilities with the <a href="https://github.com/OpenBMB/ToolBench?ref=jina-ai-gmbh.ghost.io">ToolBench</a> benchmark. The benchmark collects over 16 thousand public APIs and corresponding synthetically-generated instructions for using them in single and multi-API settings.</p><p>Here are the results (recall@3 metric) compared to other reranker models:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_ToolBench.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1012" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_ToolBench.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_ToolBench.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_ToolBench.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_ToolBench.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Recall@3 scores reported for different reranking models for ToolBench dataset</span></figcaption></figure><p>As we will also show in the later sections, the <em>almost</em> state-of-the-art performance of <code>jina-reranker-v2-base-multilingual</code> comes with the benefit of being half the size of <code>bge-reranker-v2-m3</code> and almost 15 times faster.</p><h2 id="jina-reranker-v2-on-code-retrieval">Jina Reranker v2 on Code Retrieval</h2><p>Jina Reranker v2, as well as being trained in function calling and structured data querying, also improves code retrieval compared to competing models of similar size. We evaluated its code retrieval capabilities using the <a href="https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io">CodeSearchNet</a> benchmark. The benchmark is a combination of queries in <a href="https://peps.python.org/pep-0257/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">docstring</a> and natural language formats, with labelled code-segments relevant to the queries.</p><p>Here are the results, using MRR@10, compared to other reranker models:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_CodeSearchNet--1-.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1013" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_CodeSearchNet--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_CodeSearchNet--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_CodeSearchNet--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_CodeSearchNet--1-.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">MRR@10 scores reported for different reranking models for CodeSearchNet dataset</span></figcaption></figure><h2 id="ultra-fast-inference-with-jina-reranker-v2">Ultra Fast Inference with Jina Reranker v2</h2><p>While cross-encoder-style neural rerankers excel at predicting a retrieved document&apos;s relevance, they offer slower inference than embedding models. Namely, comparing a query to <em>n</em> documents one-by-one is far slower than HNSW or any other fast retrieval method in most vector databases. We fixed this slowness with Jina Reranker v2.</p><ul><li>Our unique training insights (described in the following section) resulted in our model reaching state-of-the-art performance in accuracy with only 278M parameters. Compared to, say, <code>bge-reranker-v2-m3</code>, with 567M parameters, Jina Reranker v2 is only <em>half the size</em>. This reduction is the first reason for improved throughput (documents processed per 50ms).</li><li>Even with a comparable model size, Jina Reranker v2 boasts <em>6x the throughput</em> of our previous state-of-the-art Jina Reranker v1 model for English. This is because we implemented Jina Reranker v2 with <a href="https://github.com/Dao-AILab/flash-attention?ref=jina-ai-gmbh.ghost.io">Flash Attention 2</a>, which introduces memory and computational optimizations in the attention layer of transformer-based models.</li></ul><p>You can see the outcome of the above steps, in terms of the throughput performance of Jina Reranker v2:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_RTX-4090-Throughput.png" class="kg-image" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search" loading="lazy" width="1800" height="1013" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image_RTX-4090-Throughput.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/image_RTX-4090-Throughput.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/image_RTX-4090-Throughput.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image_RTX-4090-Throughput.png 1800w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Throughput (documents retrieved in 50ms) scores reported for different reranking models on an RTX 4090 GPU</span></figcaption></figure><h2 id="how-we-trained-jina-reranker-v2">How We Trained Jina Reranker v2</h2><p>We trained<code>jina-reranker-v2-base-multilingual</code> in four stages:</p><ol><li><strong>Preparation with English Data:</strong> We prepared the first version of the model by training a backbone model with <em>only</em> English-language data, including pairs (contrastive training) or triplets (query, correct response, wrong response), query-function schema pairs and query-table schema pairs.</li><li><strong>Addition of Cross-lingual Data:</strong> In the next stage, we added cross-lingual pairs and triplets datasets, to improve the backbone model&apos;s multilingual abilities on retrieval tasks, specifically.</li><li><strong>Addition of <em>all</em> Multilingual Data:</strong> At this stage, we focused training mostly on ensuring the model sees the largest possible amount of our data. We fine-tuned the model checkpoint from the second stage with all pairs and triplet datasets, from over 100 low- and high-resource languages.</li><li><strong>Fine-Tuning with Mined Hard-Negatives:</strong> After observing the reranking performance from the third stage, we fine-tuned the model by adding more triplet data with specifically more examples of hard-negatives for existing queries - responses that look superficially relevant to the query, but are in fact wrong.</li></ol><p>This four-stage training approach was based on the insight that including functions and tabular schemas in the training process as early as possible allowed the model to be particularly aware of these use cases and learn to focus on the semantics of the candidate documents more than the language constructs.</p><h2 id="jina-reranker-v2-in-practice">Jina Reranker v2 in Practice</h2><h3 id="via-our-reranker-api">Via Our Reranker API</h3><p>The fastest and easiest way to get started with Jina Reranker v2 is to use <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io">Jina Reranker&apos;s API</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></a></figure><p>Head to the API section of this page to integrate <code>jina-reranker-v2-base-multilingual</code> using the programming language of your choice.</p><h4 id="example-1-ranking-function-calls">Example 1: Ranking Function Calls</h4><p>To rank the most relevant external function/tool, format the query and documents (function schemas) as shown below:</p><figure class="kg-card kg-code-card"><pre><code class="language-bash">curl -X &apos;POST&apos; \
  &apos;https://api.jina.ai/v1/rerank&apos; \
  -H &apos;accept: application/json&apos; \
  -H &apos;Authorization: Bearer &lt;YOUR JINA AI TOKEN HERE&gt;&apos; \
  -H &apos;Content-Type: application/json&apos; \
  -d &apos;{
  &quot;model&quot;: &quot;jina-reranker-v2-base-multilingual&quot;,
  &quot;query&quot;: &quot;I am planning a road trip from Berlin to Munich in my Volkswagen VII. Can you calculate the carbon footprint of this trip?&quot;,
  &quot;documents&quot;: [
    &quot;{&apos;\&apos;&apos;Name&apos;\&apos;&apos;: &apos;\&apos;&apos;getWeather&apos;\&apos;&apos;, &apos;\&apos;&apos;Specification&apos;\&apos;&apos;: &apos;\&apos;&apos;Provides current weather information for a specified city&apos;\&apos;&apos;, &apos;\&apos;&apos;spec&apos;\&apos;&apos;: &apos;\&apos;&apos;https://api.openweathermap.org/data/2.5/weather?q={city}&amp;appid={API_KEY}&apos;\&apos;&apos;, &apos;\&apos;&apos;example&apos;\&apos;&apos;: &apos;\&apos;&apos;https://api.openweathermap.org/data/2.5/weather?q=Berlin&amp;appid=YOUR_API_KEY&apos;\&apos;&apos;}&quot;,
    &quot;{&apos;\&apos;&apos;Name&apos;\&apos;&apos;: &apos;\&apos;&apos;calculateDistance&apos;\&apos;&apos;, &apos;\&apos;&apos;Specification&apos;\&apos;&apos;: &apos;\&apos;&apos;Calculates the driving distance and time between multiple locations&apos;\&apos;&apos;, &apos;\&apos;&apos;spec&apos;\&apos;&apos;: &apos;\&apos;&apos;https://maps.googleapis.com/maps/api/distancematrix/json?origins={startCity}&amp;destinations={endCity}&amp;key={API_KEY}&apos;\&apos;&apos;, &apos;\&apos;&apos;example&apos;\&apos;&apos;: &apos;\&apos;&apos;https://maps.googleapis.com/maps/api/distancematrix/json?origins=Berlin&amp;destinations=Munich&amp;key=YOUR_API_KEY&apos;\&apos;&apos;}&quot;,
    &quot;{&apos;\&apos;&apos;Name&apos;\&apos;&apos;: &apos;\&apos;&apos;calculateCarbonFootprint&apos;\&apos;&apos;, &apos;\&apos;&apos;Specification&apos;\&apos;&apos;: &apos;\&apos;&apos;Estimates the carbon footprint for various activities, including transportation&apos;\&apos;&apos;, &apos;\&apos;&apos;spec&apos;\&apos;&apos;: &apos;\&apos;&apos;https://www.carboninterface.com/api/v1/estimates&apos;\&apos;&apos;, &apos;\&apos;&apos;example&apos;\&apos;&apos;: &apos;\&apos;&apos;{type: vehicle, distance: distance, vehicle_model_id: car}&apos;\&apos;&apos;}&quot;
  ]
}&apos;</code></pre><figcaption><p><span style="white-space: pre-wrap;">Remember to substitute &lt;YOUR JINA AI TOKEN HERE&gt; with your personal Reranker API token</span></p></figcaption></figure><p>You should get:</p><pre><code class="language-JSON">{
  &quot;model&quot;: &quot;jina-reranker-v2-base-multilingual&quot;,
  &quot;usage&quot;: {
    &quot;total_tokens&quot;: 383,
    &quot;prompt_tokens&quot;: 383
  },
  &quot;results&quot;: [
    {
      &quot;index&quot;: 2,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;{&apos;Name&apos;: &apos;calculateCarbonFootprint&apos;, &apos;Specification&apos;: &apos;Estimates the carbon footprint for various activities, including transportation&apos;, &apos;spec&apos;: &apos;https://www.carboninterface.com/api/v1/estimates&apos;, &apos;example&apos;: &apos;{type: vehicle, distance: distance, vehicle_model_id: car}&apos;}&quot;
      },
      &quot;relevance_score&quot;: 0.5422876477241516
    },
    {
      &quot;index&quot;: 1,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;{&apos;Name&apos;: &apos;calculateDistance&apos;, &apos;Specification&apos;: &apos;Calculates the driving distance and time between multiple locations&apos;, &apos;spec&apos;: &apos;https://maps.googleapis.com/maps/api/distancematrix/json?origins={startCity}&amp;destinations={endCity}&amp;key={API_KEY}&apos;, &apos;example&apos;: &apos;https://maps.googleapis.com/maps/api/distancematrix/json?origins=Berlin&amp;destinations=Munich&amp;key=YOUR_API_KEY&apos;}&quot;
      },
      &quot;relevance_score&quot;: 0.23283305764198303
    },
    {
      &quot;index&quot;: 0,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;{&apos;Name&apos;: &apos;getWeather&apos;, &apos;Specification&apos;: &apos;Provides current weather information for a specified city&apos;, &apos;spec&apos;: &apos;https://api.openweathermap.org/data/2.5/weather?q={city}&amp;appid={API_KEY}&apos;, &apos;example&apos;: &apos;https://api.openweathermap.org/data/2.5/weather?q=Berlin&amp;appid=YOUR_API_KEY&apos;}&quot;
      },
      &quot;relevance_score&quot;: 0.05033063143491745
    }
  ]
}</code></pre><h4 id="example-2-ranking-sql-queries">Example 2: Ranking SQL Queries</h4><p>Likewise, to retrieve relevance scores for structured table schemas for your query, you can use the following example API call:</p><pre><code class="language-bash">curl -X &apos;POST&apos; \
  &apos;https://api.jina.ai/v1/rerank&apos; \
  -H &apos;accept: application/json&apos; \
  -H &apos;Authorization: Bearer &lt;YOUR JINA AI TOKEN HERE&gt;&apos; \
  -H &apos;Content-Type: application/json&apos; \
  -d &apos;{
  &quot;model&quot;: &quot;jina-reranker-v2-base-multilingual&quot;,
  &quot;query&quot;: &quot;which customers bought a summer outfit in the past 7 days?&quot;,
  &quot;documents&quot;: [
    &quot;CREATE TABLE customer_personal_info (customer_id INT PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50));&quot;,
    &quot;CREATE TABLE supplier_company_info (supplier_id INT PRIMARY KEY, company_name VARCHAR(100), contact_name VARCHAR(50));&quot;,
    &quot;CREATE TABLE transactions (transaction_id INT PRIMARY KEY, customer_id INT, purchase_date DATE, FOREIGN KEY (customer_id) REFERENCES customer_personal_info(customer_id), product_id INT, FOREIGN KEY (product_id) REFERENCES products(product_id));&quot;,
    &quot;CREATE TABLE products (product_id INT PRIMARY KEY, product_name VARCHAR(100), season VARCHAR(50), supplier_id INT, FOREIGN KEY (supplier_id) REFERENCES supplier_company_info(supplier_id));&quot;
  ]
}&apos;</code></pre><p>The expected response is:</p><pre><code class="language-JSON">{
  &quot;model&quot;: &quot;jina-reranker-v2-base-multilingual&quot;,
  &quot;usage&quot;: {
    &quot;total_tokens&quot;: 253,
    &quot;prompt_tokens&quot;: 253
  },
  &quot;results&quot;: [
    {
      &quot;index&quot;: 2,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;CREATE TABLE transactions (transaction_id INT PRIMARY KEY, customer_id INT, purchase_date DATE, FOREIGN KEY (customer_id) REFERENCES customer_personal_info(customer_id), product_id INT, FOREIGN KEY (product_id) REFERENCES products(product_id));&quot;
      },
      &quot;relevance_score&quot;: 0.2789437472820282
    },
    {
      &quot;index&quot;: 0,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;CREATE TABLE customer_personal_info (customer_id INT PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50));&quot;
      },
      &quot;relevance_score&quot;: 0.06477169692516327
    },
    {
      &quot;index&quot;: 3,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;CREATE TABLE products (product_id INT PRIMARY KEY, product_name VARCHAR(100), season VARCHAR(50), supplier_id INT, FOREIGN KEY (supplier_id) REFERENCES supplier_company_info(supplier_id));&quot;
      },
      &quot;relevance_score&quot;: 0.027742892503738403
    },
    {
      &quot;index&quot;: 1,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;CREATE TABLE supplier_company_info (supplier_id INT PRIMARY KEY, company_name VARCHAR(100), contact_name VARCHAR(50));&quot;
      },
      &quot;relevance_score&quot;: 0.025516605004668236
    }
  ]
}</code></pre><h3 id="via-ragllm-frameworks">Via RAG/LLM Frameworks</h3><p>Jina Reranker&#x2019;s existing integrations with LLM and RAG orchestration frameworks should already work out-of-the-box by using the model name <code>jina-reranker-v2-base-multilingual</code>. Refer to their respective documentation pages to learn more about how to integrate Jina Reranker v2 in your applications.</p><ul><li><a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io"><strong>Haystack</strong></a><strong> by deepset</strong>: Jina Reranker v2 can be used with the <a href="https://docs.haystack.deepset.ai/docs/jinaranker?ref=jina-ai-gmbh.ghost.io">JinaRanker</a> class in Haystack:</li></ul><pre><code class="language-Python">from haystack import Document
from haystack_integrations.components.rankers.jina import JinaRanker

docs = [Document(content=&quot;Paris&quot;), Document(content=&quot;Berlin&quot;)]

ranker = JinaRanker(model=&quot;jina-reranker-v2-base-multilingual&quot;, api_key=&quot;&lt;YOUR JINA AI API KEY HERE&gt;&quot;)

ranker.run(query=&quot;City in France&quot;, documents=docs, top_k=1)
</code></pre><ul><li><strong>LlamaIndex</strong>: Jina Reranker v2 can be used as a <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank/?ref=jina-ai-gmbh.ghost.io">JinaRerank</a> <em>node postprocessor</em> module in by initializing it:</li></ul><pre><code class="language-Python">import os
from llama_index.postprocessor.jinaai_rerank import JinaRerank

jina_rerank = JinaRerank(model=&quot;jina-reranker-v2-base-multilingual&quot;, api_key=&quot;&lt;YOUR JINA AI API KEY HERE&gt;&quot;, top_n=1)
</code></pre><ul><li><strong>Langchain:</strong> Make use of <a href="https://python.langchain.com/v0.2/docs/integrations/document_transformers/jina_rerank/?ref=jina-ai-gmbh.ghost.io#doing-reranking-with-jinarerank">Jina Rerank integration</a> to use Jina Reranker 2 in your existing application. The JinaRerank module should be initialized with the right model name:</li></ul><pre><code class="language-Python">from langchain_community.document_compressors import JinaRerank

reranker = JinaRerank(model=&quot;jina-reranker-v2-base-multilingual&quot;, jina_api_key=&quot;&lt;YOUR JINA AI API KEY HERE&gt;&quot;)
</code></pre><h3 id="via-huggingface">Via HuggingFace</h3><p>We are also opening access (under CC-BY-NC-4.0) to <code>jina-reranker-v2-base-multilingual</code> model on Hugging Face for research and evaluation purposes.  </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-reranker-v2-base-multilingual &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-reranker-v2-base-multilingual.png" alt="Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling &amp; Code Search"></div></a></figure><p>To download and run the model from Hugging Face, install the <code>transformers</code> and <code>einops</code> libraries:</p><pre><code class="language-bash">pip install transformers einops
pip install ninja
pip install flash-attn --no-build-isolation
</code></pre><p>Log in to your Hugging Face account through the Hugging Face CLI login using your <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">Hugging Face access token</a>:</p><pre><code class="language-bash">huggingface-cli login --token &lt;&quot;HF-Access-Token&quot;&gt;
</code></pre><p>Download the pre-trained model:</p><pre><code class="language-Python">from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    &apos;jinaai/jina-reranker-v2-base-multilingual&apos;,
    torch_dtype=&quot;auto&quot;,
    trust_remote_code=True,
    
)

model.to(&apos;cuda&apos;) # or &apos;cpu&apos; if no GPU is available

model.eval()
</code></pre><p>Define the query and the documents to be reranked:</p><pre><code class="language-Python">query = &quot;Organic skincare products for sensitive skin&quot;

documents = [
    &quot;Organic skincare for sensitive skin with aloe vera and chamomile.&quot;,
    &quot;New makeup trends focus on bold colors and innovative techniques&quot;,
    &quot;Bio-Hautpflege f&#xFC;r empfindliche Haut mit Aloe Vera und Kamille&quot;,
    &quot;Neue Make-up-Trends setzen auf kr&#xE4;ftige Farben und innovative Techniken&quot;,
    &quot;Cuidado de la piel org&#xE1;nico para piel sensible con aloe vera y manzanilla&quot;,
    &quot;Las nuevas tendencias de maquillaje se centran en colores vivos y t&#xE9;cnicas innovadoras&quot;,
    &quot;&#x9488;&#x5BF9;&#x654F;&#x611F;&#x808C;&#x4E13;&#x95E8;&#x8BBE;&#x8BA1;&#x7684;&#x5929;&#x7136;&#x6709;&#x673A;&#x62A4;&#x80A4;&#x4EA7;&#x54C1;&quot;,
    &quot;&#x65B0;&#x7684;&#x5316;&#x5986;&#x8D8B;&#x52BF;&#x6CE8;&#x91CD;&#x9C9C;&#x8273;&#x7684;&#x989C;&#x8272;&#x548C;&#x521B;&#x65B0;&#x7684;&#x6280;&#x5DE7;&quot;,
    &quot;&#x654F;&#x611F;&#x808C;&#x306E;&#x305F;&#x3081;&#x306B;&#x7279;&#x5225;&#x306B;&#x8A2D;&#x8A08;&#x3055;&#x308C;&#x305F;&#x5929;&#x7136;&#x6709;&#x6A5F;&#x30B9;&#x30AD;&#x30F3;&#x30B1;&#x30A2;&#x88FD;&#x54C1;&quot;,
    &quot;&#x65B0;&#x3057;&#x3044;&#x30E1;&#x30A4;&#x30AF;&#x306E;&#x30C8;&#x30EC;&#x30F3;&#x30C9;&#x306F;&#x9BAE;&#x3084;&#x304B;&#x306A;&#x8272;&#x3068;&#x9769;&#x65B0;&#x7684;&#x306A;&#x6280;&#x8853;&#x306B;&#x7126;&#x70B9;&#x3092;&#x5F53;&#x3066;&#x3066;&#x3044;&#x307E;&#x3059;&quot;,
]
</code></pre><p>Construct sentence pairs and compute the relevancy scores:</p><pre><code class="language-Python">sentence_pairs = [[query, doc] for doc in documents]

scores = model.compute_score(sentence_pairs, max_length=1024)
</code></pre><p>The scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores mean higher relevance.</p><p>Alternatively, use the <code>rerank</code> function to rerank large texts by automatically chunking the query and the documents based on <code>max_query_length</code> and <code>max_length</code> respectively. Each chunk is scored individually and scores of each chunk are then combined to produce the final reranking results:</p><pre><code class="language-Python">results = model.rerank(
    query,
    documents,
    max_query_length=512,
    max_length=1024,
    top_n=3
)
</code></pre><p>This function not only returns the relevancy score for each document, but also their content and position in the original document list.</p><h3 id="via-private-cloud-deployment">Via Private Cloud Deployment</h3><p>Pre-built packages for private deployment of Jina Reranker v2 AWS and Azure accounts can be soon found on our seller pages on <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer">AWS Marketplace</a> and <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps?search=jina+ai&amp;page=1&amp;filters=partners&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Azure Marketplace</a>, respectively.  </p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">For a tailor-made solution for your use case, based on Jina AI&#x2019;s Search Foundation, get in touch with us via&#xA0;<a href="https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io">our contact page</a>.</div></div><h2 id="key-takeaways-of-jina-reranker-v2">Key Takeaways of Jina Reranker v2</h2><p>Jina Reranker v2 represents an important expansion of capabilities for search foundation:</p><ul><li>State-of-the-art retrieval using cross-encoding opens up a wide array of new application areas.</li><li>Enhanced multilingual and cross-language functionality removes language barriers from your use cases.</li><li>Best-in-class support for function calling, together with awareness of structured data querying, takes your agentic RAG capabilities to the next level of precision.</li><li>Better retrieval of computer code and computer-formatted data can go far beyond just doing text information retrieval.</li><li>Much faster document throughput ensures that, irrespective of the retrieval method, you can now rerank many more retrieved documents faster, and offload most of the fine-grained relevance calculation to <code>jina-reranker-v2-base-multilingual</code>.</li></ul><p>RAG systems are much more precise with Reranker v2, helping your existing information management solutions produce more and better actionable results. Cross-language support makes all this directly available to multi-national and multilingual enterprises, with an easy-to-use API at an affordable price.</p><p>By testing it with benchmarks derived from real use cases, you can see for yourself how Jina Reranker v2 maintains state-of-the art performance at tasks relevant to real business models, all in one AI model, keeping your costs down and your tech stack simpler.</p>]]></content:encoded></item><item><title><![CDATA[AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent]]></title><description><![CDATA[AI explainability and transparency are hot topics. How can we trust AI if we can't see how it works? Jina-ColBERT shows you how, with the right model architecture, you can easily make your AI spill its secrets.]]></description><link>https://jina.ai/news/ai-explainability-made-easy-how-late-interaction-makes-jina-colbert-transparent/</link><guid isPermaLink="false">6672af263ce1950001eed6a7</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Maximilian Werk]]></dc:creator><pubDate>Wed, 19 Jun 2024 14:01:36 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Search-acc--3-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Search-acc--3-.png" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent"><p>One of the long-standing problems of AI models is that neural networks don&#x2019;t explain how they produce the outputs they do. It&apos;s not always clear how much this is a real problem for artificial intelligence. When we ask humans to explain their reasoning, they routinely rationalize, typically completely unaware that they&apos;re even doing so, giving most plausible explanations for themselves without any indication of what&apos;s really going on in their heads. </p><p>We already know how to get AI models to make up plausible answers. Maybe artificial intelligence is more like humans in that way than we&#x2019;d like to admit.</p><p>Fifty years ago, the American philosopher Thomas Nagel wrote an influential essay called <em>What Is It Like To Be A Bat?</em> He contended that there must be something that it&#x2019;s like to be a bat: To see the world as a bat sees it, and to perceive existence in the way a bat does. However, according to Nagel, even if we knew every knowable fact about how bat brains, bat senses, and bat bodies work, we still wouldn&#x2019;t know what it&#x2019;s like to be a bat.</p><p>AI explainability is the same kind of problem. We know every fact there is to know about a given AI model. It&#x2019;s just a lot of finite-precision numbers arranged in a sequence of matrices. We can trivially verify that every model output is the result of correct arithmetic, but that information is useless as an explanation.</p><p>There is no more a general solution to this problem for AI than there is for humans. However, the ColBERT architecture, and particularly how it uses &#x201C;late interaction&#x201D; when used as a reranker, enables you to get meaningful insights from your models about why it gives specific results in particular cases.</p><p>This article shows you how late interaction enables explainability, using the Jina-ColBERT model <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io"><code>jina-colbert-v1-en</code></a> and the <a href="https://matplotlib.org/?ref=jina-ai-gmbh.ghost.io">Matplotlib Python library</a>.</p><h2 id="a-brief-overview-of-colbert">A Brief Overview of ColBERT</h2><p>ColBERT was introduced in <a href="https://doi.org/10.1145/3397271.3401075?ref=jina-ai-gmbh.ghost.io">Khattab &amp; Zaharia (2020)</a> as an extension to the <a href="https://doi.org/10.18653/v1/N19-1423?ref=jina-ai-gmbh.ghost.io">BERT model first introduced in 2018</a> by Google. <a href="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/?ref=jina-ai-gmbh.ghost.io">Jina AI&#x2019;s Jina-ColBERT</a> models draw on this work and the later ColBERT v2 architecture proposed in <a href="https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io">Santhanam, et al. (2021)</a>. ColBERT-style models can be used to create embeddings, but they have some additional features when used as a reranking model. The main benefit is <em>late interaction</em>, which is a way of structuring the problem of semantic text similarity differently from standard embedding models.</p><h3 id="embedding-models">Embedding Models</h3><p>In a traditional embedding model, we compare two texts by generating representative vectors for them called <em>embeddings</em>, and then we compare those embeddings via distance metrics like cosine or Hamming distance. Quantifying the semantic similarity of two texts generally follows a common procedure.</p><p>First, we create embeddings for the two texts separately. For any one text:</p><ol><li>A tokenizer breaks the text up into roughly word-sized chunks.</li><li>Each token is mapped to a vector.</li><li>The token vectors interact via the attention system and convolution layers, adding context information to the representation of each token.</li><li>A pooling layer transforms these modified token vectors into a single embedding vector.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings_pooling_dark_small-1.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="550" height="900"><figcaption><span style="white-space: pre-wrap;">A schematized embedding model that creates a single embedding for a text.</span></figcaption></figure><p>Then, when there is an embedding for each text, we compare them to each other, typically using the cosine metric or Hamming distance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings2_simpler_dark_small.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="775" height="825" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Embeddings2_simpler_dark_small.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings2_simpler_dark_small.png 775w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">In a conventional embedding model, documents are compared by directly comparing their embeddings.</span></figcaption></figure><p>Scoring happens by comparing the two whole embeddings to each other, without any specific information about the tokens. All the interaction between tokens is &#x201C;early&#x201D; since it occurs before the two texts are compared to each other.</p><h3 id="reranking-models">Reranking Models</h3><p>Reranking models work differently.</p><p>First, instead of creating an embedding for any text, it takes one text, called a <em>query</em>, and a collection of other texts that we&apos;ll all <em>target documents</em> and then scores each target document with respect to the query text. These numbers are not normalized and are not like comparing embeddings, but they are sortable. The target documents that score the highest with respect to the query are the texts that are most semantically related to the query according to the model.</p><p>Let&#x2019;s look at how this works concretely with the <code>jina-colbert-v1-en</code> reranker model, using the Jina Reranker API and Python.</p><p>The code below is also in a notebook which you can <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download</a> or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run in Google Colab</a>.</p><p>You should install the most recent version of the <code>requests</code> library into your Python environment first. You can do so with the following command:</p><pre><code class="language-bash">pip install requests -U
</code></pre><p>Next, visit the <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform">Jina Reranker API page</a> and get a free API token, good for up to one million tokens of text processing. Copy the API token key from the bottom of the page, as shown below:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/jina_reranker_api.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="1650" height="1800" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/jina_reranker_api.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/jina_reranker_api.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/jina_reranker_api.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/jina_reranker_api.png 1650w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">How to get your personal API key from the Jina Reranker API page.</span></figcaption></figure><p>We&#x2019;ll use the following query text:</p><ul><li>&#x201C;Elephants eat 150 kg of food per day.&#x201D;</li></ul><p>And compare this query to three texts:</p><ul><li>&#x201C;Elephants eat 150 kg of food per day.&#x201D;</li><li>&#x201C;Every day, the average elephant consumes roughly 150 kg of plants.&#x201D;</li><li>&#x201C;The rain in Spain falls mainly on the plain.&#x201D;</li></ul><p>The first document is identical to the query, the second is a rephrasing of the first, and the last text is completely unrelated.</p><p>Use the following Python code to get the scores, assigning your Jina Reranker API token to the variable <code>jina_api_key</code>:</p><pre><code class="language-Python">import requests

url = &quot;&lt;https://api.jina.ai/v1/rerank&gt;&quot;
jina_api_key = &quot;&lt;YOUR JINA RERANKER API TOKEN HERE&gt;&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {jina_api_key}&quot;
}
data = {
    &quot;model&quot;: &quot;jina-colbert-v1-en&quot;,
    &quot;query&quot;: &quot;Elephants eat 150 kg of food per day.&quot;,
    &quot;documents&quot;: [
        &quot;Elephants eat 150 kg of food per day.&quot;,
        &quot;Every day, the average elephant consumes roughly 150 kg of food.&quot;,
        &quot;The rain in Spain falls mainly on the plain.&quot;,
    ],
    &quot;top_n&quot;: 3
}

response = requests.post(url, headers=headers, json=data)
for item in response.json()[&apos;results&apos;]:
    print(f&quot;{item[&apos;relevance_score&apos;]} : {item[&apos;document&apos;][&apos;text&apos;]}&quot;)
</code></pre><p>Running this code from a Python file or in a notebook should produce the following result:</p><pre><code class="language-Text">11.15625 : Elephants eat 150 kg of food per day.
9.6328125 : Every day, the average elephant consumes roughly 150 kg of food.
1.568359375 : The rain in Spain falls mainly on the plain.
</code></pre><p>The exact match has the highest score, as we would expect, while the rephrasing has the second highest, and a completely unrelated text has a much lower score.</p><h3 id="scoring-using-colbert">Scoring using ColBERT</h3><p>What makes ColBERT reranking different from embedding-based scoring is that the tokens of the two texts are compared to each other during the scoring process. The two texts never have their own embeddings.</p><p>First, we use the same architecture as embedding models to create new representations for each token that include context information from the text. Then, we compare each token from the query with each token from the document.</p><p>For each token in the query, we identify the token in the document that has the strongest interaction with it, and sum over those interaction scores to calculate a final numerical value.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/ColBERT_dual_dark_small.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="1325" height="1200" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/ColBERT_dual_dark_small.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/ColBERT_dual_dark_small.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/ColBERT_dual_dark_small.png 1325w" sizes="(min-width: 1200px) 1200px"></figure><p>This interaction is &#x201C;late&#x201D;: Tokens interact across the two texts when we are comparing them to each other. But remember, the &#x201C;late&#x201D; interaction doesn&#x2019;t exclude the &#x201C;early&#x201D; interaction. The token vectors pairs being compared already contain information about their specific contexts.</p><p>This late interaction scheme preserves token-level information, even if that information is context-specific. That enables us to see, in part, how the ColBERT model calculates its score because we can identify which pairs of contextualized tokens contribute to the final score.</p><h2 id="explaining-rankings-with-heat-maps">Explaining Rankings with Heat Maps</h2><p>Heat maps are a visualization technique that&#x2019;s useful for seeing what&#x2019;s going on in Jina-ColBERT when it creates scores. In this section, we&#x2019;ll use the <a href="https://seaborn.pydata.org/?ref=jina-ai-gmbh.ghost.io"><code>seaborn</code></a> and <a href="https://matplotlib.org/?ref=jina-ai-gmbh.ghost.io"><code>matplotlib</code></a> libraries to create heat maps from the late interaction layer of <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io"><code>jina-colbert-v1-en</code></a>, showing how the query tokens interact with each target text token.</p><h3 id="set-up">Set-Up</h3><p>We have created a Python library file containing the code for accessing the <code>jina-colbert-v1-en</code> model and using <code>seaborn</code>, <code>matplotlib</code> and <code>Pillow</code> to create heatmaps. You can <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download this library directly from GitHub</a>, or <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">use the provided notebook</a> on your own system, or on <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>.</p><p>First, install the requirements. You will need the latest version of the <code>requests</code> library into your Python environment. So, if you have not already done so, run:</p><pre><code class="language-bash">pip install requests -U 
</code></pre><p>Then, install the core libraries:</p><pre><code class="language-bash">pip install matplotlib seaborn torch Pillow
</code></pre><p>Next, download <code>jina_colbert_heatmaps.py</code> from GitHub. You can do that <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">via a web browser</a> or at the command line if <code>wget</code> is installed:</p><pre><code class="language-bash">wget https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py
</code></pre><p>With the libraries in place, we need to only declare one function for the rest of this article:</p><pre><code class="language-Python">from jina_colbert_heatmaps import JinaColbertHeatmapMaker

def create_heatmap(query, document, figsize=None):
    heat_map_maker = JinaColbertHeatmapMaker(jina_api_key=jina_api_key)
    # get token embeddings for the query
    query_emb = heat_map_maker.embed(query, is_query=True)
    # get token embeddings for the target document
    document_emb = heat_map_maker.embed(document, is_query=False)
    return heat_map_maker.compute_heatmap(document_emb[0], query_emb[0], figsize)
</code></pre><h3 id="results">Results</h3><p>Now that we can create heat maps, let&#x2019;s make a few and see what they tell us.</p><p>Run the following command in Python:</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, &quot;Elephants eat 150 kg of food per day.&quot;)</code></pre><p>The result will be a heat map that looks like this:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--68-.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--68-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--68-.png 640w"></figure><p>This is a heat map of the activation levels between pairs of tokens when we compare two identical texts. Each square shows the interaction between two tokens, one from each text. The extra tokens <code>[CLS]</code> and <code>[SEP]</code> indicate the beginning and the end of the text respectively, and <code>q</code> and <code>d</code> are inserted right after the <code>[CLS]</code> token in queries and target documents respectively. This allows the model to take into account interactions between tokens and the beginning and ends of texts but also allows token representations to be sensitive to whether they are in queries or targets.</p><p>The brighter the square, the more interaction there is between the two tokens, which is indicative of being semantically related. Each token pair&#x2019;s interaction score is in the range -1.0 to 1.0. The squares highlighted by a red frame are the ones that count towards the final score: For each token in the query, it&#x2019;s highest interaction level with any document token is the value that counts.</p><p>The best matches &#x2014; the brightest spots &#x2014; and the red-framed maximum values are almost all exactly on the diagonal, and they have very strong interaction. The only exceptions are the &#x201C;technical&#x201D; tokens <code>[CLS]</code>, <code>q</code>, and <code>d</code>, as well as the word &#x201C;of&#x201D; which is a high-frequency &#x201C;stop word&#x201D; in English that carries very little independent information.</p><p>Let&#x2019;s take a structurally similar sentence &#x2014; &#x201C;Cats eat 50 g of food per day.&#x201D; &#x2014; and see how the tokens in it interact:</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, &quot;Cats eat 50 g of food per day.&quot;)</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/download.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/download.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/download.png 640w"></figure><p>Once again, the best matches are primarily on the diagonal because the words are frequently the same and the sentence structure is nearly identical. Even &#x201C;cats&#x201D; and &#x201C;elephants&#x201D; match, because of their common contexts, although not very well.</p><p>The less similar the context, the worse the match. Consider the text &#x201C;Employees eat at the company canteen.&#x201D;</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, &quot;Employees eat at the company canteen.&quot;)</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--69-.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--69-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--69-.png 640w"></figure><p>Although structurally similar, the only strong match here is between the two instances of &#x201C;eat.&#x201D; Topically, these are very different sentences, even if their structure are highly parallel.</p><p>Looking at the darkness of the colors in the red-framed squares, we can see how the model would rank them as matches for &#x201C;Elephants eat 150 kg of food per day&#x201D;, and <code>jina-colbert-v1-en</code> confirms this intuition:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Score</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>11.15625</td>
<td>Elephants eat 150 kg of food per day.</td>
</tr>
<tr>
<td>8.3671875</td>
<td>Cats eat 50 g of food per day.</td>
</tr>
<tr>
<td>3.734375</td>
<td>Employees eat at the company canteen.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Now, let&#x2019;s compare &#x201C;Elephants eat 150 kg of food per day.&#x201D; to a sentence that has essentially the same meaning but a different formulation: &#x201C;Every day, the average elephant consumes roughly 150 kg of food.&#x201D;</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, &quot;Every day, the average elephant consumes roughly 150 kg of food.&quot;)</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--70-.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--70-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--70-.png 640w"></figure><p>Notice the strong interaction between &#x201C;eat&#x201D; in the first sentence and &#x201C;consume&#x201D; in the second. The difference in vocabulary doesn&#x2019;t prevent Jina-ColBERT from recognizing the common meaning.</p><p>Also, &#x201C;every day&#x201D; strongly matches &#x201C;per day&#x201D;, even though they are in completely different places. Only the low-value word &#x201C;of&#x201D; is an anomalous non-match.</p><p>Now, let&#x2019;s compare the same query with a totally unrelated text: &#x201C;The rain in Spain falls mainly on the plain.&#x201D;</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, &quot;The rain in Spain falls mainly on the plain.&quot;)</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/download-1.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/download-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/download-1.png 640w"></figure><p>You can see that &#x201C;best match&#x201D; interactions score much lower for this pair, and there is very little interaction between any of the words in the two texts. Intuitively, we would expect it to score poorly compared to &#x201C;Every day, the average elephant consumes roughly 150 kg of food&#x201D;, and<code>jina-colbert-v1-en</code> agrees:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Score</th>
<th>Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>9.6328125</td>
<td>Every day, the average elephant consumes roughly 150 kg of food.</td>
</tr>
<tr>
<td>1.568359375</td>
<td>The rain in Spain falls mainly on the plain.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="long-texts">Long Texts</h3><p>These are toy examples to demonstrate the workings of ColBERT-style reranker models. In information retrieval contexts, like retrieval-augmented generation, queries tend to be short texts while matching candidate documents tend to be longer, often as long as the input context window of the model.</p><p>Jina-ColBERT models all support 8192 token input contexts, equivalent to roughly 16 standard pages of single-spaced text.</p><p>We can generate heat maps for these asymmetric cases too. For example, let&#x2019;s take the first section of the <a href="https://en.wikipedia.org/wiki/Indian_elephant?ref=jina-ai-gmbh.ghost.io">Wikipedia page on Indian Elephants</a>:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="2000" height="1870" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 2188w" sizes="(min-width: 720px) 720px"></figure><p>To see this as plain text, as passed to <code>jina-colbert-v1-en</code>, click <a href="https://raw.githubusercontent.com/jina-ai/workshops/docs-heatmaps/notebooks/heatmaps/wikipedia_indian_elephant.txt?ref=jina-ai-gmbh.ghost.io">this link</a>.</p><p>This text is 364 words long, so our heat map won&#x2019;t look very square:</p><pre><code class="language-Python">create_heatmap(&quot;Elephants eat 150 kg of food per day.&quot;, wikipedia_elephants, figsize=(50,7))</code></pre><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--71--2.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="2000" height="378" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--71--2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--71--2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--71--2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/06/Untitled--71--2.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>We see that &#x201C;elephants&#x201D; matches a lot of places in the text. This isn&#x2019;t surprising in a text about elephants. But we can also see one area where there is a lot stronger interaction:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--72--1.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="2000" height="443" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--72--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--72--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--72--1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/06/Untitled--72--1.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>What&#x2019;s going on here? With Jina-ColBERT, we can find the part of the longer text that this corresponds to. It turns out it&#x2019;s the fourth sentence of the second paragraph:</p><blockquote>The species is classified as a megaherbivore and consume up to 150 kg (330 lb) of plant matter per day.</blockquote><p>This restates the same information as in the query text. If we look at the heat map for just this sentence we can see the strong matches:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--74-.png" class="kg-image" alt="AI Explainability Made Easy: How Late Interaction Makes Jina-ColBERT Transparent" loading="lazy" width="640" height="480" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--74-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--74-.png 640w"></figure><p>Jina-ColBERT provides you with the means to see exactly what areas in a long text caused it to match the query. This leads to better debugging, but also to greater explainability. It doesn&#x2019;t take any sophistication to see how a match is made.</p><h2 id="explaining-ai-outcomes-with-jina-colbert">Explaining AI outcomes with Jina-ColBERT</h2><p>Embeddings are a core technology in modern AI. Almost everything we do is based on the idea that complex, learnable relationships in input data can be expressed in the geometry of high-dimensional spaces. However, it&#x2019;s very difficult for mere humans to make sense of spatial relationships in thousands to millions of dimensions.</p><p>ColBERT is a step back from that level of abstraction. It&#x2019;s not a complete answer to the problem of explaining what an AI model does, but it points us directly at which parts of our data are responsible for our results.</p><p>Sometimes, AI has to be a black box. The giant matrices that do all the heavy lifting are too big for any human to keep in their heads. But the ColBERT architecture shines a little bit of light into the box and demonstrates that more is possible.</p><p>The Jina-ColBERT model is currently available only for English (<code>jina-colbert-v1-en</code>) but more languages and usage contexts are on their way. This line of models, which not only perform state-of-the-art information retrieval but can tell you why they matched something, demonstrates Jina AI&apos;s commitment to making AI technologies both accessible and useful.</p>]]></content:encoded></item><item><title><![CDATA[Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image]]></title><description><![CDATA[Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.]]></description><link>https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/</link><guid isPermaLink="false">665f1ccd4b4b4c0001ba1c98</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Sofia Vasileva]]></dc:creator><pubDate>Wed, 05 Jun 2024 09:42:02 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image"><p>Jina CLIP v1 (<code>jina-clip-v1</code>) is a new multimodal embedding model that extends the capabilities of OpenAI&#x2019;s <a href="https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io">original CLIP model</a>. With this new model, users have a single embedding model that delivers state-of-the-art performance in both text-only and text-image cross-modal retrieval. Jina AI has improved on OpenAI CLIP&#x2019;s performance by 165% in text-only retrieval, and 12% in image-to-image retrieval, with identical or mildly better performance in text-to-image and image-to-text tasks. This enhanced performance makes Jina CLIP v1 indispensable for working with multimodal inputs.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><code spellcheck="false" style="white-space: pre-wrap;">jina-clip-v1</code> improves on OpenAI CLIP in <a href="#compare_table" rel="noreferrer">every category of retrieval</a>.</div></div><p>In this article, we will first discuss the shortcomings of the original CLIP model and how we have addressed them using a unique co-training method. Then, we will demonstrate the effectiveness of our model on various retrieval benchmarks. Finally, we will provide detailed instructions on how users can get started with Jina CLIP v1 via our Embeddings API and Hugging Face.</p><h2 id="the-clip-architecture-for-multimodal-ai">The CLIP Architecture for Multimodal AI</h2><p>In January 2021, OpenAI released the <a href="https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io">CLIP</a> (Contrastive Language&#x2013;Image Pretraining) model. CLIP has a straightforward yet ingenious architecture: it combines two embedding models, one for texts and one for images, into a single model with a single output embedding space. Its text and image embeddings are directly comparable to each other, making the distance between a text embedding and an image embedding proportionate to how well that text describes the image, and vice versa.</p><p>This has proven to be very useful in multimodal information retrieval and zero-shot image classification. Without further special training, CLIP performed well at placing images into categories with natural language labels.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg" class="kg-image" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w" sizes="(min-width: 720px) 720px"></figure><p>The text embedding model in the original CLIP was a custom neural network with only 63 million parameters. On the image side, OpenAI released CLIP with a selection of <a href="https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">ResNet</a> and <a href="https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">ViT models</a>. Each model was pre-trained for its individual modality and then trained with captioned images to produce similar embeddings for prepared image-text pairs.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png" class="kg-image" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>This approach yielded impressive results. Particularly notable is its zero-shot classification performance. For example, even though the training data did not include labeled images of <a href="https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io">astronauts</a>, CLIP could correctly identify pictures of astronauts based on its understanding of related concepts in texts and images.</p><p>However, OpenAI&#x2019;s CLIP has two important drawbacks:</p><ul><li>First is its very limited text input capacity. It can take a maximum 77 tokens of input, but <a href="https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io">empirical analysis shows</a> that in practice it doesn&#x2019;t use more than 20 tokens to produce its embeddings. This is because CLIP was trained from images with captions, and captions tend to be very short. This is in contrast to current text embedding models which support several thousand tokens.</li><li>Second, the performance of its text embeddings in text-only retrieval scenarios is very poor. Image captions are a very limited kind of text, and do not reflect the broad array of use cases a text embedding model would be expected to support.</li></ul><p>In most real use cases, text-only and image-text retrieval are combined or at least both are available for tasks. Maintaining a second embeddings model for text-only tasks effectively doubles the size and complexity of your AI framework.</p><p>Jina AI&#x2019;s new model addresses these issues directly, and <code>jina-clip-v1</code> takes advantage of the progress made in the last several years to bring state-of-the-art performance to tasks involving all combinations of text and image modalities.</p><h2 id="introducing-jina-clip-v1">Introducing Jina CLIP v1</h2><p>Jina CLIP v1 retains the OpenAI&#x2019;s original CLIP schema: two models co-trained to produce output in the same embedding space.</p><p>For text encoding, we adapted the <a href="https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io">Jina BERT v2</a> architecture used in the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings v2 models</a>. This architecture supports a state-of-the-art 8k token input window and outputs 768-dimensional vectors, producing more accurate embeddings from longer texts. This is more than 100 times the 77 token input supported in the original CLIP model.</p><p>For image embeddings, we are using the latest model from the Beijing Academy for Artificial Intelligence: the <a href="https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io"><code>EVA-02</code> model</a>. We have empirically compared a number of image AI models, testing them in cross-modal contexts with similar pre-training, and <code>EVA-02</code>clearly outperformed the others. It&#x2019;s also comparable to the Jina BERT architecture in model size, so that compute loads for image and text processing tasks are roughly identical.</p><p>These choices produce important benefits for users:</p><ul><li>Better performance on all benchmarks and all modal combinations, and especially large improvements in text-only embedding performance.</li><li><code>EVA-02</code>&apos;s empirically superior performance both in image-text and image-only tasks, with the added benefit of Jina AI&#x2019;s additional training, improving image-only performance.</li><li>Support for much longer text inputs. <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings&#x2019; 8k token</a> input support makes it possible to process detailed textual information and correlate it with images.</li><li>A large net savings in space, compute, code maintenance, and complexity because this multimodal model is highly performant even in non-multimodal scenarios.</li></ul><h3 id="training">Training</h3><p>Part of our recipe for high-performance multimodal AI is our training data and procedure. We notice that the very short length of texts used in image captions is the major cause of poor text-only performance in CLIP-style models, and our training is explicitly designed to remedy this.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png" class="kg-image" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>Training takes place in three steps:</p><ol><li>Use captioned image data to learn to align image and text embeddings, interleaved with text pairs with similar meanings. This co-training jointly optimizes for the two kinds of tasks. The text-only performance of the model declines during this phase, but not as much as if we had trained with only image-text pairs.</li><li>Train using synthetic data which aligns images with larger texts, generated by an AI model, that describes the image. Continue training with text-only pairs at the same time. During this phase, the model learns to attend to larger texts in conjunction with images.</li><li>Use text triplets with <a href="https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">hard negatives</a> to further improve text-only performance by learning to make finer semantic distinctions. At the same time, continue training using synthetic pairs of images and long texts. During this phase, text-only performance improves dramatically without the model losing any image-text abilities.</li></ol><p>For more information on the details of training and model architecture, please read <a href="https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io">our recent paper</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image"></div></a></figure><h2 id="new-state-of-the-art-in-multimodal-embeddings">New State-of-the-Art in Multimodal Embeddings</h2><p>We evaluated Jina CLIP v1&#x2019;s performance across text-only, image-only, and cross-modal tasks involving both input modalities. We used the <a href="https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io">MTEB retrieval benchmark</a> to evaluate text-only performance. For image-only tasks, we used the <a href="https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io">CIFAR-100</a> benchmark. For cross-model tasks, we evaluate on <a href="https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io">Flickr8k</a>, <a href="https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io">Flickr30K</a>, and <a href="https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io">MSCOCO Captions</a>, which are included in the <a href="https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io">CLIP Benchmark</a>.</p><p>The results are summarized in the table below:</p>
<!--kg-card-begin: html-->
<table id="compare_table">
<thead>
<tr>
<th>Model</th>
<th>Text-Text</th>
<th>Text-to-Image</th>
<th>Image-to-Text</th>
<th>Image-Image</th>
</tr>
</thead>
<tbody>
<tr>
<td>jina-clip-v1</td>
<td>0.429</td>
<td>0.899</td>
<td>0.803</td>
<td>0.916</td>
</tr>
<tr>
<td>openai-clip-vit-b16</td>
<td>0.162</td>
<td>0.881</td>
<td>0.756</td>
<td>0.816</td>
</tr>
<tr style="font-weight:bold">
<td>% increase<br>vs OpenAI CLIP</td>
<td>165%</td>
<td>2%</td>
<td>6%</td>
<td>12%</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>You can see from these results that <code>jina-clip-v1</code> outperforms OpenAI&#x2019;s original CLIP in all categories, and is dramatically better in text-only and image-only retrieval. Averaged over all categories, this is a 46% improvement in performance.</p><p>You can find a more detailed evaluation in <a href="https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io">our recent paper</a>.</p><h2 id="getting-started-with-embeddings-api">Getting Started with Embeddings API</h2><p>You can easily integrate Jina CLIP v1 into your applications using the <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Jina Embeddings API</a>.</p><p>The code below shows you how to call the API to get embeddings for texts and images, using the <code>requests</code> package in Python. It passes a text string and a URL to an image to the Jina AI server and returns both encodings.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x261D;&#xFE0F;</div><div class="kg-callout-text">Remember to replace <code spellcheck="false" style="white-space: pre-wrap;">&lt;YOUR_JINA_AI_API_KEY&gt;</code> with an activated Jina API key. You can get a trial key with a million free tokens from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform">Jina Embeddings web page</a>.</div></div><pre><code class="language-python">import requests
import numpy as np
from numpy.linalg import norm

cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))

url = &apos;https://api.jina.ai/v1/embeddings&apos;

headers = {
  &apos;Content-Type&apos;: &apos;application/json&apos;,
  &apos;Authorization&apos;: &apos;Bearer &lt;YOUR_JINA_AI_API_KEY&gt;&apos;
}

data = {
  &apos;input&apos;: [
     {&quot;text&quot;: &quot;Bridge close-shot&quot;},
     {&quot;url&quot;: &quot;https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s&quot;}],
  &apos;model&apos;: &apos;jina-clip-v1&apos;,
  &apos;encoding_type&apos;: &apos;float&apos;
}

response = requests.post(url, headers=headers, json=data)
sim = cos_sim(np.array(response.json()[&apos;data&apos;][0][&apos;embedding&apos;]), np.array(response.json()[&apos;data&apos;][1][&apos;embedding&apos;]))
print(f&quot;Cosine text&lt;-&gt;image: {sim}&quot;)
</code></pre><h3 id="integration-with-major-llm-frameworks">Integration with major LLM Frameworks</h3><p>Jina CLIP v1 is already available for <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">LlamaIndex</a> and <a href="https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">LangChain</a>:</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>: Use <code>JinaEmbedding</code> with the <code>MultimodalEmbedding</code> base class, and invoke <code>get_image_embeddings</code> or <code>get_text_embeddings</code> .</li><li><a href="https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io">LangChain</a>: Use <code>JinaEmbeddings</code>, and invoke <code>embed_images</code> or <code>embed_documents</code>.</li></ul><h3 id="pricing">Pricing</h3><p>Both text and image inputs are charged by token consumption.</p><p>For text in English, <a href="https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io">we have empirically calculated</a> that on average you will need 1.1 tokens for every word.</p><p>For images, we count the number of 224x224 pixel tiles required to cover your image. Some of these tiles may be partly blank but count just the same. Each tile costs 1,000 tokens to process.</p><p><strong>Example</strong></p><p>For an image with dimensions 750x500 pixels:</p><ol><li>The image is divided into 224x224 pixel tiles.<ol><li>To calculate the number of tiles, take the width in pixels and divide by 224, then round up to the nearest integer. <br>     750/224 &#x2248; 3.35 &#x2192; 4</li><li>Repeat for the height in pixels: <br>     500/224 &#x2248; 2.23 &#x2192; 3</li></ol></li><li>The total number of tiles required in this example is: <br>           4 (horizontal) x 3 (vertical) = 12 tiles</li><li>The cost will be 12 x 1,000 = 12,000 tokens </li></ol><h3 id="enterprise-support">Enterprise Support</h3><p>We are introducing a new benefit for users who purchase the Production Deployment plan with <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing">11 billion tokens</a>. This includes:</p><ul><li>Three hours of consultation with our product and engineering teams to discuss your specific use cases and requirements.</li><li>A customized Python notebook designed for your RAG (Retrieval-Augmented Generation) or vector search use case, demonstrating how to integrate Jina AI&#x2019;s models into your application.</li><li>Assignment to an account executive and priority email support to ensure your needs are met promptly and efficiently.</li></ul><h2 id="open-source-jina-clip-v1-on-hugging-face">Open-Source Jina CLIP v1 on Hugging Face</h2><p>Jina AI is committed to an open-source search foundation, and for that purpose, we are making this model available for free under an <a href="https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io">Apache 2.0 license</a>, on <a href="https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io">Hugging Face</a>.</p><p>You can find example code to download and run this model on your own system or cloud installation on the Hugging Face model page for <code>jina-clip-v1</code> .</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-clip-v1 &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png" alt="Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image"></div></a></figure><h2 id="summary">Summary</h2><p>Jina AI&#x2019;s latest model &#x2014; <code>jina-clip-v1</code> &#x2014; represents a significant advance in multimodal embedding models, offering substantial performance gains over OpenAI&apos;s CLIP. With notable improvements in text-only and image-only retrieval tasks, as well as competitive performance in text-to-image and image-to-text tasks, it stands as a promising solution for complex embeddings use cases.</p><p>This model currently only supports English-language texts due to resource constraints. We are working to expand its capabilities to more languages.</p>]]></content:encoded></item><item><title><![CDATA[Implementing a Chat History RAG with Jina AI and Milvus Lite]]></title><description><![CDATA[Enhance your search applications in Python with Jina Embeddings and Reranker and lightweight, easy-to-deploy Milvus Lite.
]]></description><link>https://jina.ai/news/implementing-a-chat-history-rag-with-jina-ai-and-milvus-lite/</link><guid isPermaLink="false">665d76034b4b4c0001ba1bb3</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Francesco Kruk]]></dc:creator><pubDate>Mon, 03 Jun 2024 14:09:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--39-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--39-.jpg" alt="Implementing a Chat History RAG with Jina AI and Milvus Lite"><p>Developers and operations engineers put a high value on infrastructure that they can easily set up, quickly start, and, later, efficiently deploy in a scaled production environment without additional hassle. For this reason, <a href="https://milvus.io/docs/milvus_lite.md?ref=jina-ai-gmbh.ghost.io"><u>Milvus Lite</u></a>, the latest lightweight vector database offering from our partner <a href="https://milvus.io/?ref=jina-ai-gmbh.ghost.io"><u>Milvus</u></a>, is an important tool for Python developers to quickly develop search applications, especially when used together with high-quality and easy-to-use search foundation models.</p><p>In this article, we&#x2019;ll describe how Milvus Lite integrates <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings v2</u></a> and <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><u>Jina Reranker v1</u></a> using the example of a <a href="https://jina.ai/news/albus-by-springworks-empowering-employees-with-enterprise-search?ref=jina-ai-gmbh.ghost.io"><u>Retrieval Augmented Generation (RAG)</u></a> application built on a fictitious company&#x2019;s internal public channel chats to let employees get answers to their organization-related questions in an accurate and helpful manner.</p><h2 id="overview-of-milvus-lite-jina-embeddings-and-jina-reranker">Overview of Milvus Lite, Jina Embeddings and Jina Reranker</h2><p>Milvus Lite is a new, lightweight version of leading vector database Milvus, which is now also offered as a Python library. Milvus Lite shares the same API as Milvus deployed on Docker or Kubernetes but can be easily installed via a one-line pip command, without setting up a server.</p><p>With the integration of Jina Embeddings v2 and Jina Reranker v1 in <a href="https://github.com/milvus-io/pymilvus?ref=jina-ai-gmbh.ghost.io"><u>pymilvus</u></a>, Milvus&apos;s Python SDK, you now have the option to directly embed documents using the same Python client for any deployment mode of Milvus, including Milvus Lite. You can find details of the Jina Embeddings and Reranker integration on pymilvus&#x2019;<a href="https://milvus.io/docs/integrate_with_jina.md?ref=jina-ai-gmbh.ghost.io"> <u>documentation pages</u></a>.</p><p>With its 8k-token context window and multilingual capabilities, Jina Embeddings v2 encodes the broad semantics of text and ensures accurate retrieval. By adding Jina Reranker v1 to the pipeline, you can further refine your results by cross-encoding the retrieved results directly with the query for a deeper contextual understanding.</p><h2 id="milvus-and-jina-ai-models-in-action">Milvus and Jina AI Models in Action</h2><p>This tutorial will focus on a practical use case: Querying a company&apos;s Slack chat history to answer a wide range of questions based on past conversations.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/E-R-slack--2-.jpg" class="kg-image" alt="Implementing a Chat History RAG with Jina AI and Milvus Lite" loading="lazy" width="1600" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/E-R-slack--2-.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/E-R-slack--2-.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/E-R-slack--2-.jpg 1600w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Process flow for querying the Slack data using an example query</span></figcaption></figure><p>For example, an employee could ask about the next step in some AI training process, as in the process schema above. By using Jina Embeddings, Jina Reranker, and Milvus, we can accurately identify relevant information in the logged Slack messages. This application can level up your workplace productivity by making it easier to access valuable information from past communications.</p><p>To generate the answers, we will use <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io"><u>Mixtral 7B Instruct</u></a> through <a href="https://python.langchain.com/v0.1/docs/integrations/llms/huggingface_endpoint/?ref=jina-ai-gmbh.ghost.io"><u>HuggingFace&#x2019;s integration in Langchain</u></a>. To use the model, you need a HuggingFace access token that you can generate as described <a href="https://huggingface.co/docs/hub/en/security-tokens?ref=jina-ai-gmbh.ghost.io"><u>here</u></a>.</p><p>You can follow along in <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/milvus/milvus_lite_jina_integration.ipynb?ref=jina-ai-gmbh.ghost.io"><u>Colab</u></a> or by <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/milvus/milvus_lite_jina_integration.ipynb?ref=jina-ai-gmbh.ghost.io"><u>downloading the notebook</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/milvus/milvus_lite_jina_integration.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colab</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/0d8af74d4089ab8b6d127bd74854be98/img/favicon.ico" alt="Implementing a Chat History RAG with Jina AI and Milvus Lite"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Implementing a Chat History RAG with Jina AI and Milvus Lite"></div></a></figure><h3 id="about-the-dataset">About the Dataset</h3><p>The dataset used in this tutorial was generated using GPT-4 and is meant to replicate the chat histories of Blueprint AI&#x2019;s Slack channels. Blueprint is a fictitious AI startup developing its own foundational models. You can download the dataset <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/milvus/chat_history.json?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><u>here</u></a>.</p><p>The data is organized in <em>channels</em>, each representative of a collection of related Slack threads. Each channel has a topic label, one of ten topic options: <em>model distribution</em>, <em>model training</em>, <em>model fine-tuning</em>, <em>ethics and bias mitigation</em>, <em>user feedback</em>, <em>sales</em>, <em>marketing</em>, <em>model onboarding</em>, <em>creative design</em>, and <em>product management</em>. One participant is known as the &quot;expert user&quot;. You can use this field to validate the results of querying for the most expert user in a topic, which we will show you how to do below.</p><p>Each channel also contains a chat history with conversation threads of up to 100 messages per channel. Each message in the dataset contains the following information:</p><ul><li>The <strong>user</strong> that sent the message</li><li>The <strong>message text</strong> sent by the user</li><li>The <strong>timestamp</strong> of the message</li><li>The <strong>name of the file</strong> the user might have attached to the message</li><li>The <strong>message ID</strong></li><li>The <strong>parent message ID</strong> if the message was within a thread originated from another message</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/06/image-1.png" class="kg-image" alt="Implementing a Chat History RAG with Jina AI and Milvus Lite" loading="lazy" width="780" height="450" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/image-1.png 780w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">A UML diagram of the chat data&apos;s structure.</span></figcaption></figure><h3 id="set-up-the-environment">Set up the Environment</h3><p>To start, install all the necessary components:</p><pre><code class="language-Python">pip install -U pymilvus
pip install -U &quot;pymilvus[model]&quot;
pip install langchain
pip install langchain-community
</code></pre><p>Download the dataset:</p><pre><code class="language-Python">import os

if not os.path.exists(&quot;chat_history.json&quot;):
    !wget https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/milvus/chat_history.json</code></pre><p>Set your Jina AI API Key in an environment variable. You can generate one <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><u>here</u></a>.</p><pre><code class="language-Python">import os
import getpass

os.environ[&quot;JINAAI_API_KEY&quot;] = getpass.getpass(prompt=&quot;Jina AI API Key: &quot;)</code></pre><p>Do the same for your Hugging Face Token. You can find how to generate one <a href="https://huggingface.co/docs/hub/en/security-tokens?ref=jina-ai-gmbh.ghost.io"><u>here</u></a>. Make sure that it is set to <code>READ</code> to access the <a href="https://huggingface.co/docs/hub/en/index?ref=jina-ai-gmbh.ghost.io"><u>Hugging Face Hub</u></a>.</p><pre><code class="language-Python">os.environ[&quot;HUGGINGFACEHUB_API_TOKEN&quot;] = getpass.getpass(prompt=&quot;Hugging Face Token: &quot;)</code></pre><h3 id="create-the-milvus-collection">Create the Milvus Collection</h3><p>Create the Milvus Collection to index the data:</p><pre><code class="language-Python">from pymilvus import MilvusClient, DataType

# Specify a local file name as uri parameter of MilvusClient to use Milvus Lite
client = MilvusClient(&quot;milvus_jina.db&quot;)

schema = MilvusClient.create_schema(
    auto_id=True,
    enable_dynamic_field=True,
)

schema.add_field(field_name=&quot;id&quot;, datatype=DataType.INT64, description=&quot;The Primary Key&quot;, is_primary=True)
schema.add_field(field_name=&quot;embedding&quot;, datatype=DataType.FLOAT_VECTOR, description=&quot;The Embedding Vector&quot;, dim=768)

index_params = client.prepare_index_params()
index_params.add_index(field_name=&quot;embedding&quot;, metric_type=&quot;COSINE&quot;, index_type=&quot;AUTOINDEX&quot;)

client.create_collection(collection_name=&quot;milvus_jina&quot;, schema=schema, index_params=index_params)</code></pre><h3 id="prepare-the-data">Prepare the Data</h3><p>Parse the chat history and extract the metadata:</p><pre><code class="language-Python">import json

with open(&quot;chat_history.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
    chat_data = json.load(file)

messages = []
metadatas = []

for channel in chat_data:
  chat_history = channel[&quot;chat_history&quot;]
  chat_topic = channel[&quot;topic&quot;]
  chat_expert = channel[&quot;expert_user&quot;]
  for message in chat_history:
    text = f&quot;&quot;&quot;{message[&quot;user&quot;]}: {message[&quot;message&quot;]}&quot;&quot;&quot;
    messages.append(text)
    meta = {
        &quot;time_stamp&quot;: message[&quot;time_stamp&quot;],
        &quot;file_name&quot;: message[&quot;file_name&quot;],
        &quot;parent_message_nr&quot;: message[&quot;parent_message_nr&quot;],
        &quot;channel&quot;: chat_topic,
        &quot;expert&quot;: True if message[&quot;user&quot;] == chat_expert else False
    }
    metadatas.append(meta)
</code></pre><h3 id="embed-the-chat-data">Embed the Chat Data</h3><p>Create embeddings for each message using Jina Embeddings v2 to retrieve relevant chat information:</p><pre><code class="language-Python">from pymilvus.model.dense import JinaEmbeddingFunction

jina_ef = JinaEmbeddingFunction(&quot;jina-embeddings-v2-base-en&quot;)

embeddings = jina_ef.encode_documents(messages)</code></pre><h3 id="index-the-chat-data">Index the Chat Data</h3><p>Index the messages, their embeddings, and the related metadata:</p><pre><code class="language-Python">collection_data = [{
    &quot;message&quot;: message,
    &quot;embedding&quot;: embedding,
    &quot;metadata&quot;: metadata
} for message, embedding, metadata in zip(messages, embeddings, metadatas)]

data = client.insert(
    collection_name=&quot;milvus_jina&quot;,
    data=collection_data
)</code></pre><h3 id="query-the-chat-history">Query the Chat History</h3><p>Time to ask a question:</p><pre><code class="language-Python">query = &quot;Who knows the most about encryption protocols in my team?&quot;</code></pre><p>Now embed the query and retrieve relevant messages. Here we retrieve the five most relevant messages and rerank them using Jina Reranker v1:</p><pre><code class="language-Python">from pymilvus.model.reranker import JinaRerankFunction

query_vectors = jina_ef.encode_queries([query])

results = client.search(
    collection_name=&quot;milvus_jina&quot;,
    data=query_vectors,
    limit=5,
)

results = results[0]

ids = [results[i][&quot;id&quot;] for i in range(len(results))]

results = client.get(
    collection_name=&quot;milvus_jina&quot;,
    ids=ids,
    output_fields=[&quot;id&quot;, &quot;message&quot;, &quot;metadata&quot;]
)

jina_rf = JinaRerankFunction(&quot;jina-reranker-v1-base-en&quot;)

documents = [results[i][&quot;message&quot;] for i in range(len(results))]
reranked_documents = jina_rf(query, documents)

reranked_messages = []
for reranked_document in reranked_documents:
  idx = reranked_document.index
  reranked_messages.append(results[idx])</code></pre><p>Lastly, generate an answer to the query using Mixtral 7B Instruct and the reranked messages as context:</p><pre><code class="language-Python">from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(repo_id=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;)

prompt = &quot;&quot;&quot;&lt;s&gt;[INST] Context information is below.\\n
        It includes the five most relevant messages to the query, sorted based on their relevance to the query.\\n
        ---------------------\\n
        {context_str}\\\\n
        ---------------------\\n
        Given the context information and not prior knowledge,
        answer the query. Please be brief, concise, and complete.\\n
        If the context information does not contain an answer to the query,
        respond with \\&quot;No information\\&quot;.\\n
        Query: {query_str}[/INST] &lt;/s&gt;&quot;&quot;&quot;

prompt = PromptTemplate(template=prompt, input_variables=[&quot;query_str&quot;, &quot;context_str&quot;])

llm_chain = prompt | llm

answer = llm_chain.invoke({&quot;query_str&quot;:query, &quot;context_str&quot;:reranked_messages})

print(f&quot;\n\nANSWER:\n\n{answer}&quot;)</code></pre><p>The answer to our question is:</p><blockquote>&#x201C;Based on the context information, User5 seems to be the most knowledgeable about encryption protocols in your team. They have mentioned that the new protocols enhance data security significantly, especially for cloud deployments.&#x201D;</blockquote><p>If you read through the messages in <code>chat_history.json</code>, you can verify for yourself if User5 is the most expert user. </p><h2 id="summary">Summary</h2><p>We have seen how to set up Milvus Lite, embed chat data using Jina Embeddings v2, and refine search results with Jina Reranker v1, all within a practical use case of searching a Slack chat history. Milvus Lite simplifies Python-based application development without the need for complex server setups. Its integration with Jina Embeddings and Reranker aims to boost productivity by making it easier to access valuable information from your workplace.</p><h2 id="use-jina-ai-models-and-milvus-now"><strong>Use Jina AI Models and Milvus Now</strong></h2><p><a href="https://milvus.io/docs/integrate_with_jina.md?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><u>Milvus Lite</u></a> with integrated <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings</u></a> and <a href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><u>Reranker</u></a> provides you with a complete processing pipeline, ready to use with just a few lines of code.</p><p>We would love to hear about your use cases and talk about how the Jina AI Milvus extension can fit your business needs. Contact us via <a href="https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io"><u>our website</u></a> or <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>our Discord channel</u></a> to share your feedback and stay up-to-date with our latest models. For questions about Milvus and Jina AI&apos;s integration, join the <a href="https://milvus.io/community?ref=jina-ai-gmbh.ghost.io"><u>Milvus community</u></a>.</p>]]></content:encoded></item><item><title><![CDATA[RAG is Dead, Again?]]></title><description><![CDATA[RAG is just one algorithmic pattern you can use. But if you make it *the* algorithm and idolize it, then you are living in a bubble you created, and the bubble will burst.]]></description><link>https://jina.ai/news/rag-is-dead-again/</link><guid isPermaLink="false">66505b6384f9e40001a6daf9</guid><category><![CDATA[Insights]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Fri, 24 May 2024 10:37:18 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/05/3c563034-52ba-4d9b-a537-07877cb2b506.webp" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/3c563034-52ba-4d9b-a537-07877cb2b506.webp" alt="RAG is Dead, Again?"><p>It is hard to tell if people hate to love RAG or love to hate RAG. </p><p>According to recent discussions on X and <a href="https://t.co/Q1QithBNj6?ref=jina-ai-gmbh.ghost.io">HN</a>, RAG <em>should</em> be dead, <strong>again</strong>. This time, critics are focusing on the over-engineering of most RAG frameworks, which, as <a href="https://x.com/jeremyphoward?ref=jina-ai-gmbh.ghost.io">@jeremyphoward</a> <a href="https://x.com/HamelHusain?ref=jina-ai-gmbh.ghost.io">@HamelHusain</a> <a href="https://x.com/Yampeleg?ref=jina-ai-gmbh.ghost.io">@Yampeleg</a> demonstrated, could be accomplished with 20 lines of Python code. </p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">T H A N K   Y O U ! ! !<br>(v 2.0)<br><br>Full RAG in 20 lines.<br><br>This is how you implement semantic search in ~10 lines<br><br>Replace &#x1D68C;&#x1D698;&#x1D697;&#x1D69D;&#x1D68E;&#x1D6A1;&#x1D69D;&#x1D69C; &amp; &#x1D69A;&#x1D69E;&#x1D68E;&#x1D69C;&#x1D69D;&#x1D692;&#x1D698;&#x1D697; with your own.<br><br>* the blurred code is for loading example contexts<br><br>(Everything from FastAI&apos;s guide:<a href="https://t.co/qBpU6T2Fd1?ref=jina-ai-gmbh.ghost.io">https://t.co/qBpU6T2Fd1</a>) <a href="https://t.co/Or3eJEbSt9?ref=jina-ai-gmbh.ghost.io">https://t.co/Or3eJEbSt9</a> <a href="https://t.co/e2q70H6QaY?ref=jina-ai-gmbh.ghost.io">pic.twitter.com/e2q70H6QaY</a></p>&#x2014; Yam Peleg (@Yampeleg) <a href="https://twitter.com/Yampeleg/status/1793698848616960393?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">May 23, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><p>The last time we had this vibe was shortly after the release of Claude/Gemini with a super long context window. What makes this time worse is that even Google&apos;s RAG generates funny results as <a href="https://x.com/icreatelife?ref=jina-ai-gmbh.ghost.io">@icreatelife</a> <a href="https://x.com/mark_riedl?ref=jina-ai-gmbh.ghost.io">@mark_riedl</a>  showed, which is ironic because back in April, at Google Next in Las Vegas, Google presented RAG as the grounding solution.</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I couldn&#x2019;t believe it before I tried it. Google needs to fix this asap.. <a href="https://t.co/r3FyOfxiTK?ref=jina-ai-gmbh.ghost.io">pic.twitter.com/r3FyOfxiTK</a></p>&#x2014; Kris Kashtanova (@icreatelife) <a href="https://twitter.com/icreatelife/status/1793781850923823144?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">May 23, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Yes! My website poisoning attack works on Google&apos;s new LLM-powered AI overviews! <a href="https://t.co/nWyMtl7nMj?ref=jina-ai-gmbh.ghost.io">pic.twitter.com/nWyMtl7nMj</a></p>&#x2014; Mark Riedl (@mark_riedl) <a href="https://twitter.com/mark_riedl/status/1793375699967054334?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">May 22, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="two-problems-of-rag">Two problems of RAG</h2><p>I see two problems with the RAG frameworks and solutions we have today. </p><h2 id="feed-forward-only">Feed-forward only</h2><p>First, nearly all RAG frameworks <strong>implement only a &quot;feed-forward&quot; path and lack a &quot;back-propagation&quot; path</strong>. It is an <em>incomplete</em> system. I remember <a href="https://x.com/swyx?ref=jina-ai-gmbh.ghost.io">@swyx</a>, in one of the episodes of <a href="https://x.com/latentspacepod?ref=jina-ai-gmbh.ghost.io">@latentspacepod</a>, arguing that RAG will <em>not</em> be killed by the long context window of LLMs since:</p><ol><li>long context is expensive for devs and</li><li>long context is hard to debug and lacks decomposability. </li></ol><p>But if all RAG frameworks focus only on the forwarding path, how is it easier to debug than an LLM? It is also interesting how many people get overexcited by the auto-magical results of RAG from some random POCs and completely forget that adding more forward layers without backward tuning is a terrible idea. We all know that adding one more layer to your neural networks expands its parametric space and hence representation ability, enabling it to do more potential things, <strong>but without training, this is nothing. </strong>There are quite some startups in the Bay Area working on evaluation&#x2014;essentially trying to evaluate the loss of a feed-forward system. Is it useful? Yes. But does it help close the loop of RAG? No.<br><br>So who is working on the back-propagation of RAG? Afaik not many. I am mostly familiar with DSPy, a library from <a href="https://x.com/stanfordnlp?ref=jina-ai-gmbh.ghost.io">@stanfordnlp</a> <a href="https://x.com/lateinteraction?ref=jina-ai-gmbh.ghost.io">@lateinteraction</a> that sets its mission on that. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/stanfordnlp/dspy?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - stanfordnlp/dspy: DSPy: The framework for programming&#x2014;not prompting&#x2014;foundation models</div><div class="kg-bookmark-description">DSPy: The framework for programming&#x2014;not prompting&#x2014;foundation models - stanfordnlp/dspy</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="RAG is Dead, Again?"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">stanfordnlp</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/0d188663ed7e46ec4bb66d2eb8c5a9417e63343bc566e659a691978ae3df0b3e/stanfordnlp/dspy" alt="RAG is Dead, Again?"></div></a></figure><p>But even for DSPy, the main focus is on optimizing few-shot demonstrations, not the full system (or at least from community usage). But why is this problem difficult? Because the signal is very sparse, and optimizing a non-differentiable pipeline system is essentially a combinatorial problem&#x2014;in other words, <strong>extremely hard</strong>. I learned some submodular optimization during my PhD, and I have a feeling that this technique will be put to good use in RAG optimization.</p><h2 id="grounding-in-the-wild-is-hard">Grounding in the wild is hard</h2><p>I do agree that RAG is for grounding, despite the funny search results from Google. There are two types of grounding: <strong>search grounding</strong>, which uses search engines to extend the world knowledge of LLMs, and <strong>check grounding</strong>, which uses private knowledge (e.g. proprietary data) to do fact-checking. </p><p>In both cases, it cites external knowledge to improve the factuality of the result, provided that these external resources are trustworthy. In Google&apos;s funny search result, one can easily see that not everything on the web is trustworthy (yeah, big surprise, who would thought!), which makes search grounding look bad. But I do believe you can only laugh at it for now. There are some <strong>implicit feedback mechanisms</strong> behind the Google Search UI that collect users&apos; reactions to those results and weight the credibility of the website for better grounding. In general, it should be pretty temporary, as this RAG just needs to get past<strong> the cold start</strong>, and results will improve over time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Heading--32-.png" class="kg-image" alt="RAG is Dead, Again?" loading="lazy" width="1500" height="787" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Heading--32-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Heading--32-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Heading--32-.png 1500w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Two types of grounding that inspire </span><a href="https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">Jina Reader</span></a></figcaption></figure><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/GOVSJD9XEAAf2kK.jpeg" width="2000" height="955" loading="lazy" alt="RAG is Dead, Again?" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/GOVSJD9XEAAf2kK.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/GOVSJD9XEAAf2kK.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/GOVSJD9XEAAf2kK.jpeg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/GOVSJD9XEAAf2kK.jpeg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled.jpg" width="2000" height="975" loading="lazy" alt="RAG is Dead, Again?" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/Untitled.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/Untitled.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">RAG was presented as a grounding solution in the Google Next conference.</span></p></figcaption></figure><h2 id="my-take">My Take</h2><p>RAG is neither dead nor alive; so stop arguing about it. RAG is just one algorithmic pattern you can use. But if you make it <strong><em>the</em></strong> algorithm and idolize it, then you are living in a bubble you created, and the bubble will burst.</p>]]></content:encoded></item><item><title><![CDATA[Bypass Limitations with PromptPerfect: Generate the Images the Models Donât Want You to See]]></title><description><![CDATA[See how PromptPerfect overcomes restrictions and limitations of image generation models like Stable Diffusion XL and DALL-E 3.]]></description><link>https://jina.ai/news/bypass-limitations-with-promptperfect-generate-the-images-the-models-dont-want-you-to-see/</link><guid isPermaLink="false">664c736084f9e40001a6d975</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Wed, 22 May 2024 14:00:56 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/05/break-chain.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Calm down, we&#x2019;re not focusing on <i><em class="italic" style="white-space: pre-wrap;">those</em></i> kind of images (whatever you think <i><em class="italic" style="white-space: pre-wrap;">those</em></i> are).</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/break-chain.png" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See"><p>Let&#x2019;s cut straight to the point: Sometimes you want to generate a perfectly innocent image, and a model (like <a href="https://openai.com/dall-e-3/?ref=jina-ai-gmbh.ghost.io">DALL-E 3</a> or <a href="https://stability.ai/stable-image?ref=jina-ai-gmbh.ghost.io">Stable Diffusion XL</a>) either flat-out refuses or comes up with something totally wrong. <a href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">PromptPerfect</a> helps with that, giving you better and more accurate results.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - AI Prompt Generator and Optimizer</div><div class="kg-bookmark-description">Unlock prompt optimization for models like GPT-4, ChatGPT and Midjourney. Generate and refine prompts to perfection, receiving improved outcomes in seconds.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/favicon-128x128.png" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See"><span class="kg-bookmark-author">AI Prompt Generator and Optimizer</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See"></div></a></figure><p>In this post we&#x2019;ll compare different models, explain how to use PromptPerfect to optimize your experience, and put it to the test, showing you the results of both models before and after using PromptPerfect&#x2019;s optimizer.</p><p>And no, we&#x2019;re not generating (or trying to generate) any dirty pictures. This is a family-friendly post, especially for families with children who like octopuppies. Or puptopi. Or whatever we end up calling some of the weird many-legged doggos we create later in the post.</p><h2 id="dall-e-3-and-stable-diffusion-xl">DALL-E 3 and Stable Diffusion XL</h2><p>While there are plenty of models out there, today we&#x2019;ll focus on the shiny new kids on the block: DALL-E 3 from <a href="https://openai.com/?ref=jina-ai-gmbh.ghost.io">OpenAI</a>, and Stable Diffusion XL from <a href="https://stability.ai/?ref=jina-ai-gmbh.ghost.io">Stability AI</a>. While each of these <em>can</em> achieve good results, they have different strengths and weaknesses.</p><p>Looking at DALL-E 3, out of the box it&#x2019;s good at understanding long sentences and object relationships, and it draws more realistic anatomy than Stable Diffusion XL (no Lovecraftian horror hands here). However, it often point-blank refuses to generate images of notable figures (like Taylor Swift) or well-known characters (like Mickey Mouse, even if we ask for the out-of-copyright Steamboat Willie version). It also generates text better than any other image generation model (though that&#x2019;s a low bar.)</p><p>Stable Diffusion XL is much more open to generating images of notable figures and well-known characters, though some of it&#x2019;s images of Mickey look like they were drawn while on some really fun drugs. However, it often messes up anatomy and object relationships. While you <em>can</em> ask it to generate text (and see it&#x2019;s trying its best), it falls way behind DALL-E 3 on that front.</p><p>With PromptPerfect we can get around some of these weaknesses from both models. We&#x2019;ll compare DALL-E 3 and Stable Diffusion, both before and after using PromptPerfect&apos;s optimization. You can skip ahead to see the ultimate winner.</p><h2 id="using-promptperfect%E2%80%99s-optimizer">Using PromptPerfect&#x2019;s Optimizer</h2><p>In this battle of the models we&#x2019;re using PromptPerfect&#x2019;s optimizer to see how we can get better image results from our prompts. Here&#x2019;s how:</p><p>Sign up for free credits at <a href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io">PromptPerfect</a>:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-17.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1137" height="792" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-17.png 1137w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Try a paid plan free for 7 days. And subscribe to a plan within 24 hours of your first login to get 40% off!</div></div><p>Click on the interactive feature:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-18.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="712" height="508" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-18.png 712w"></figure><p>In the &#x2018;optimizer&#x2019; pane (on the right-hand side), type something like <code>generate a prompt to create an image of felix the cat using DALL-E 3</code>:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-19.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1104" height="897" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-19.png 1104w" sizes="(min-width: 720px) 720px"></figure><p>Click &quot;Send to Assistant&quot;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-20.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="530" height="319"></figure><p>It will do some thinking, then generate the image from the prompt in the &#x2019;interactive&#x2019; pane, on the left:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-21.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="756" height="868" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-21.png 756w" sizes="(min-width: 720px) 720px"></figure><p>Refine your prompt by conversing with the Optimizer, then lather, rinse, repeat:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-22.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1570" height="731" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-22.png 1570w" sizes="(min-width: 720px) 720px"></figure><h2 id="contest-methodology">Contest Methodology</h2><p>For the &#x201C;before&#x201D; images, we&#x2019;ll use:</p><ul><li>ChatGPT (GPT-4) to generate images with DALL-E using the prompt <code>generate an image of &lt;thing&gt;</code>, for example <code>generate an image of mickey mouse</code>.</li><li><a href="https://replicate.com/stability-ai/sdxl?ref=jina-ai-gmbh.ghost.io">Replicate&#x2019;s interface</a> to generate images with Stable Diffusion XL, using the prompt <code>&lt;thing&gt;</code>, for example <code>mickey mouse</code>.</li></ul><p>For the &#x201C;after&#x201D; images, we&#x2019;ll use PromptPerfect&#x2019;s interactive optimizer, using the prompt <code>generate a prompt to create an image of &lt;thing&gt; using &lt;model name&gt;</code> .</p><p>We&#x2019;ll present the first output that comes up. The number of actual images may vary - PromptPerfect always generates four, Stable Diffusion XL (via Replicate), one, and DALL-E 3 one or two.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">While PromptPerfect&#x2019;s optimizer is interactive (so you can refine your prompt in a conversational manner), we just stuck with the first result to be as impartial as possible. By really using the interactive feature of the optimizer you&#x2019;d get even better results.</div></div><p>We&#x2019;ll award medals as follows:</p><ul><li>&#x1F4A9; - flat-out refused to cooperate</li><li>&#x1F949; - it tried, but none of the outputs were what we&#x2019;re looking for</li><li>&#x1F948; - at least one of the outputs was an okay result!</li><li>&#x1F947; - hot damn, at least one of the outputs was actually good!</li></ul><p>Finally we&#x2019;ll do a round up and see which model and method came out on top.</p><h2 id="who-will-be-the-next-top-model">Who Will Be the Next Top Model?</h2><p>Models, start your engines!</p><h3 id="round-1-notable-figures">Round 1: Notable Figures</h3><p>Let&apos;s first try our Lord and Savior Taylor Swift. Here&#x2019;s a real image of the person we&#x2019;re aiming for:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="2000" height="2958" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/Untitled.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Licensed </span><a href="https://creativecommons.org/licenses/by/3.0/deed.en?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer"><span style="white-space: pre-wrap;">CC BY 3.0</span></a><span style="white-space: pre-wrap;">, Attribution:&#xA0;iHeartRadioCA</span></figcaption></figure><p>Without PromptPerfect, DALL-E 3 flat out refuses to create Taylor:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-1.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="830" height="275" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-1.png 830w" sizes="(min-width: 720px) 720px"></figure><p>With PromptPerfect, it generates images with the optimized prompt, but none of them actually look like her:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--1-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="802" height="1034" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--1-.png 802w" sizes="(min-width: 720px) 720px"></figure><p>With SDXL, before PromptPerfect we get a pretty good rendition:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--1--1.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="768" height="768" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--1--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--1--1.png 768w" sizes="(min-width: 720px) 720px"></figure><p>And PromptPerfect&#x2019;s optimized prompt once again delivers:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--2-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="789" height="857" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--2-.png 789w" sizes="(min-width: 720px) 720px"></figure><p>Let&#x2019;s see which models could really generate-rate-rate:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Before optimization</th>
<th>After optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td>DALL-E 3</td>
<td>&#x1F4A9; It flat out refused</td>
<td>&#x1F949; Blonde? Check? Singer? Check. Taylor? Nope</td>
</tr>
<tr>
<td>Stable Diffusion XL</td>
<td>&#x1F947; Swifty vibes</td>
<td>&#x1F947; Quite Taylorian</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="round-2-%E2%80%9Ccopyrighted%E2%80%9D-material">Round 2: &#x201C;Copyrighted&#x201D; Material</h3><p>We&#x2019;re not even going to <em>try</em> with actually copyrighted material - that&#x2019;s a whole can of worms we don&#x2019;t want to dive into. However, the design of Mickey Mouse from Steamboat Willie <em>is</em> <a href="https://www.npr.org/2024/01/01/1221606624/mickey-mouse-public-domain-disney?ref=jina-ai-gmbh.ghost.io">out of copyright</a> as of 2024:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--3-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="959" height="729" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--3-.png 959w" sizes="(min-width: 720px) 720px"></figure><p>Let&#x2019;s use him as a subject. DALL-E 3 flat out refuses at first:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--4-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="820" height="248" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--4-.png 820w" sizes="(min-width: 720px) 720px"></figure><p>With PromptPerfect we get results with the right vibe, but not the 1930s <a href="https://en.wikipedia.org/wiki/Rubber_hose_animation?ref=jina-ai-gmbh.ghost.io">rubber hose</a> style:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--5-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="794" height="1007" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--5-.png 794w" sizes="(min-width: 720px) 720px"></figure><p>Stable Diffusion tries. It really does. With this Mickey you get a lot more ears, eyes and fingers for your buck:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--6-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="768" height="768" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--6-.png 768w" sizes="(min-width: 720px) 720px"></figure><p>With PromptPerfect optimization, Stable Diffusion still gives us fever dream Mickey, but more of a light fever, less &#x201C;<em>how</em> strong are these mushrooms?&#x201D; fever:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--7-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="793" height="835" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--7-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--7-.png 793w" sizes="(min-width: 720px) 720px"></figure><p>Which model puts the &#x201C;ick&#x201D; in Mickey?</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Before optimization</th>
<th>After optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td>DALL-E 3</td>
<td>&#x1F4A9; policy schmolicy. This stuff is definitely out of copyright.</td>
<td>&#x1F948;. Definitely had Mickey vibes, no weirdness, just not the 30s style I was aiming for.</td>
</tr>
<tr>
<td>Stable Diffusion XL</td>
<td>&#x1F949; Go home Mickey. You&#x2019;re possessed.</td>
<td>&#x1F948; Barely scraping into the silver medal category. More Mickey vibes than DALL-E 3, but the deformation is really distracting</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="round-3-text">Round 3: Text</h3><p>Let&#x2019;s generate a picture of a sign that says &#x201C;Happy days are here again&#x201D;. No target picture this time, just imagine (as difficult as it might be) a sign with that text. In the words of John Lennon, it&#x2019;s easy if you try.</p><p>DALL-E 3 gives us happy vibes, which I dig. However, it does throw in the word &#x201C;dye&#x201D;. Since this sounds like the word &#x201C;die&#x201D;, it might be sending mixed messages:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--8-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="640" height="619" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--8-.png 640w"></figure><p>With optimization, we actually get the correct wording and spelling with no extra words, at least once. And once it&#x2019;s almost spot-on, except for a misspelling:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--9-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="780" height="911" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--9-.png 780w" sizes="(min-width: 720px) 720px"></figure><p>Stable Diffusion XL gives us Herpy Days:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--10-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="768" height="768" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--10-.png 768w" sizes="(min-width: 720px) 720px"></figure><p>After optimizing the Stable Diffusion XL Prompt, we get a lonely misspelled sign in the woods. It&#x2019;s less scary than before, though I for one am not following that signpost to wherever it leads.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--11-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="785" height="853" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--11-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--11-.png 785w" sizes="(min-width: 720px) 720px"></figure><p>Who will see happy days, and who won&#x2019;t?</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Before optimization</th>
<th>After optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td>DALL-E 3</td>
<td>&#x1F948; You can see what the sign is saying, even though it added the extra &#x201C;dye&#x201D; word and the order of the words is off</td>
<td>&#x1F947; At least one of the signs has the full correct text. And another just had a &#x201C;small&#x201D; typo (an extra &#x201C;P&#x201D; in &#x201C;HAPPY&#x201D; - small by image generation standards!)</td>
</tr>
<tr>
<td>Stable Diffusion XL</td>
<td>&#x1F949; Looks like a motivational poster from Hell</td>
<td>&#x1F948; Not as good as unoptimized DALL-E 3, but doesn&#x2019;t make me want to gouge out my eyes as much as unoptimized SDXL</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="round-4-%E2%80%9Ccursed%E2%80%9D-creations">Round 4: &#x201C;Cursed&#x201D; Creations</h3><p>Let&#x2019;s see how well the models can adapt to weird stuff, like a puppy with seven legs. No target image this time - I don&#x2019;t want &#x201C;deformed puppies&#x201D; to be in my Google history. Just imagine a puppy with seven legs.</p><p>DALL-E 3 gave us two outputs this time. We didn&#x2019;t ask for it. It just likes doggos I guess. Proof that AI is becoming more human-like? Anyway, results were what we asked for, though a bit bland in my opinion. Still we&#x2019;re not awarding points for style in this round, just content. So a dog with an absurd number of legs superimposed on the Windows XP wallpaper works:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--12-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1024" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--12-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled--12-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--12-.png 1024w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-16.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1024" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-16.png 1024w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">While it&apos;s not strictly NSFW, it is sufficiently disturbing that I pixelated it</span></figcaption></figure><p>After optimization, so many legs! I wonder what the multi-legged dog emoji is meant to express? Send answers our way!</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--14-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="777" height="1009" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--14-.png 777w" sizes="(min-width: 720px) 720px"></figure><p>Stable Diffusion XL misread the assignment:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--15-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="768" height="768" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--15-.png 768w" sizes="(min-width: 720px) 720px"></figure><p>Even after optimization, we&#x2019;re like &#x201C;which part of seven legs did you not understand?&#x201D;:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--16-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="796" height="831" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--16-.png 796w" sizes="(min-width: 720px) 720px"></figure><p>Who&#x2019;s top dog and who&#x2019;s runt of the litter in this round?</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Before optimization</th>
<th>After optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td>DALL-E 3</td>
<td>&#x1F947; Both puppies have bizarre leg number. First puppy even has seven, though some of them are barely in shot. Though I don&#x2019;t  know what the clasper things are on puppy number two, and neither do I wish to find out.</td>
<td>&#x1F947; YES. All the puppies. All the legs. You can play shaking hands with these cuties for ages. One even got the leg count right.</td>
</tr>
<tr>
<td>Stable Diffusion XL</td>
<td>&#x1F949;When I want a puppy with legs for days, I don&#x2019;t mean just long legs</td>
<td>&#x1F949; I like my puppies with more legs</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="bonus-round-kegstand-punk">Bonus Round: Kegstand Punk</h3><p>In some cases, DALL-E 3 and SDXL both fail whether we employ optimization or not. For example, generating an image of a punk doing a <a href="https://en.wikipedia.org/wiki/Keg_stand?ref=jina-ai-gmbh.ghost.io">kegstand</a>.</p><p>Here is an image of a punk&#x2026;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--17-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="1125" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--17-.png 1125w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">via pexels.com</span></figcaption></figure><p>...and an illustration of a kegstand (that looks like it&#x2019;s from a wholesome children&#x2019;s book):</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--18-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="944" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--18-.png 944w" sizes="(min-width: 720px) 720px"></figure><p>I can&#x2019;t find an actual image of a punk doing a kegstand online. Ugh, punks, such prudes!</p><p>DALL-E 3 gives us a punk in a bar with weird but cool lighting. He looks very stoic. He&#x2019;s on a keg, but no kegstand.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--19-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="508" height="610"></figure><p>After optimization, I dig the vibe, but still no kegstand:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--20-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="783" height="1007" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--20-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--20-.png 783w" sizes="(min-width: 720px) 720px"></figure><p>They should change the name to Stable Diffusion ER, because this guy(?) needs to go to hospital:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--21-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="768" height="768" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--21-.png 768w" sizes="(min-width: 720px) 720px"></figure><p>After optimization looks much better. There&#x2019;s a keg. There&#x2019;s a punk. Still no kegstand, alas.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--22-.png" class="kg-image" alt="Bypass Limitations with PromptPerfect: Generate the Images the Models Don&#x2019;t Want You to See" loading="lazy" width="794" height="891" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled--22-.png 794w" sizes="(min-width: 720px) 720px"></figure><p>Who&#x2019;s the punk and who&#x2019;s just junk?</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Before optimization</th>
<th>After optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td>DALL-E 3</td>
<td>&#x1F948; Punk, check. Keg check. Kegstand, not so much</td>
<td>&#x1F948; Optimization changed the vibe a bit, but still no actual kegstand</td>
</tr>
<tr>
<td>Stable Diffusion XL</td>
<td>&#x1F949; Ouch. Not a punk. Not a kegstand. Barely a human being. And doing a kegstand like that, he won&#x2019;t be any kind of human being for much longer.</td>
<td>&#x1F948; Optimization gave us a much better result, showing a punk interacting with a keg. No body horror this time.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="tallying-up-the-score">Tallying Up the Score</h2><p>Now that the contest is done, we&#x2019;ll count the scores as follows:</p><ul><li>&#x1F4A9;: zero points</li><li>&#x1F949;: one point</li><li>&#x1F948;: two points</li><li>&#x1F947;: three points</li></ul><p>The maximum number of points any option could achieve is 15 (winning a gold medal in all five rounds). Let&#x2019;s see the breakdown:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Challenge</th>
<th>DALL-E 3</th>
<th></th>
<th>Stable Diffusion XL</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Before PromptPerfect</td>
<td>After PromptPerfect</td>
<td>Before PromptPerfect</td>
<td>After PromptPerfect</td>
</tr>
<tr>
<td>Notable figure</td>
<td>&#x1F4A9; 0</td>
<td>&#x1F949; 1</td>
<td>&#x1F947; 3</td>
<td>&#x1F947; 3</td>
</tr>
<tr>
<td>&#x201C;Copyrighted&#x201D; material</td>
<td>&#x1F4A9; 0</td>
<td>&#x1F948; 2</td>
<td>&#x1F949; 1</td>
<td>&#x1F948; 2</td>
</tr>
<tr>
<td>Text</td>
<td>&#x1F948; 2</td>
<td>&#x1F947; 3</td>
<td>&#x1F949; 1</td>
<td>&#x1F948; 2</td>
</tr>
<tr>
<td>Cursed creations</td>
<td>&#x1F947; 3</td>
<td>&#x1F947; 3</td>
<td>&#x1F949; 1</td>
<td>&#x1F949; 1</td>
</tr>
<tr>
<td>Punk kegstand</td>
<td>&#x1F948; 2</td>
<td>&#x1F948; 2</td>
<td>&#x1F949; 1</td>
<td>&#x1F948; 2</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td>&#x1F949; 7</td>
<td>&#x1F947; 11</td>
<td>&#x1F949; 7</td>
<td>&#x1F948; 10</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>In short, if it weren&#x2019;t for censorship in the early rounds, DALL-E 3 would&#x2019;ve scored much higher. Overall, using PromptPerfect to optimize your prompts leads to better results for both models.</p><p>You can trust us, because this was an impartial contest (done by us, for us, for our own product). Seriously though, the results do speak for themselves. Try it for yourself and see how it goes!</p>]]></content:encoded></item><item><title><![CDATA[AIR-Bench: Better Metrics for Better Search Foundation]]></title><description><![CDATA[AIR-Bench is a new approach to AI metrics that uses generative AI to make more realistic and flexible benchmarks. With AIR-Bench, you can create your own benchmarks for your own domain, and know that benchmarks data hasn't leaked into model training data.]]></description><link>https://jina.ai/news/air-bench-better-metrics-for-better-search-foundation/</link><guid isPermaLink="false">664c53c684f9e40001a6d96c</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Tue, 21 May 2024 14:26:11 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/05/cosmic--1-.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>Late at night, a police officer finds a drunk man crawling around on his hands and knees under a streetlight. The drunk man tells the officer he&#x2019;s looking for his wallet. When the officer asks if he&#x2019;s sure this is where he dropped the wallet, the man replies that he thinks he more likely dropped it across the street. Then why are you looking over here? the befuddled officer asks. Because the light&#x2019;s better here, explains the drunk man.<br><br>David H. Friedman, <a href="https://www.discovermagazine.com/the-sciences/why-scientific-studies-are-so-often-wrong-the-streetlight-effect?ref=jina-ai-gmbh.ghost.io"><em>Why Scientific Studies Are So Often Wrong: The Streetlight Effect</em></a>, Discover magazine, Dec. 2010</blockquote><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/cosmic--1-.jpg" alt="AIR-Bench: Better Metrics for Better Search Foundation"><p>Benchmarks are a core component of modern machine learning practices and have been for some time, but they have a very serious problem: We can&#x2019;t tell if our benchmarks measure anything useful.</p><p>This is a big problem, and this article will introduce part of a solution: The AIR-Bench. This joint project with the <a href="https://www.baai.ac.cn/english.html?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Beijing Academy of Artificial Intelligence</a> is a novel approach to AI metrics designed to improve the quality and usefulness of our benchmarks.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Search Foundation, Supercharged.</div><div class="kg-bookmark-description">Jina AI offers best-in-class embeddings, reranker and prompt optimizer, enabling advanced multimodal AI.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">Your Search Foundation, Supercharged.</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.baai.ac.cn/english.html?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">&#x5317;&#x4EAC;&#x667A;&#x6E90;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x7814;&#x7A76;&#x9662;</div><div class="kg-bookmark-description">&#x667A;&#x6E90;&#x7814;&#x7A76;&#x9662;&#x662F;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x9886;&#x57DF;&#x7684;&#x65B0;&#x578B;&#x7814;&#x53D1;&#x673A;&#x6784;&#xFF0C;&#x6C47;&#x96C6;&#x56FD;&#x9645;&#x9876;&#x5C16;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x5B66;&#x8005;&#xFF0C;&#x805A;&#x7126;&#x6838;&#x5FC3;&#x6280;&#x672F;&#x4E0E;&#x539F;&#x59CB;&#x521B;&#x65B0;&#xFF0C;&#x65E8;&#x5728;&#x63A8;&#x52A8;&#x4EBA;&#x5DE5;&#x667A;&#x80FD;&#x9886;&#x57DF;&#x53D1;&#x5C55;&#x653F;&#x7B56;&#x3001;&#x5B66;&#x672F;&#x601D;&#x60F3;&#x3001;&#x7406;&#x8BBA;&#x57FA;&#x7840;&#x3001;&#x9876;&#x5C16;&#x4EBA;&#x624D;&#x4E0E;&#x4EA7;&#x4E1A;&#x751F;&#x6001;&#x7684;&#x4E94;&#x5927;&#x6E90;&#x5934;&#x521B;&#x65B0;&#x3002;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.baai.ac.cn/home/images/favicon.ico" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.baai.ac.cn/home/images/logo.svg" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><h2 id="the-streetlight-effect">The Streetlight Effect</h2><p>Scientific and operational research places a lot of emphasis on measurements, but measurements aren&#x2019;t a simple thing. In a health study, you might want to know if some drug or treatment made recipients healthier, longer lived, or improved their condition in some way. But health and improved life quality are difficult things to measure directly, and it can take decades to find out if a treatment extended someone&#x2019;s life.</p><p>So researchers use proxies. In a health study, that might be something like physical strength, reduced pain, lowered blood pressure, or some other variable that you can easily measure. One of the problems with health research is that the proxy may not really be indicative of the better health outcome you want a drug or treatment to have.</p><p>A measurement is a proxy for something useful that matters to you. You may not be able to measure that thing, so you measure something else, something you <em>can</em> measure, that you have reasons to believe correlates with the useful thing you really care about.</p><p>Focusing on measurement was a major development of 20th century operational research and it&#x2019;s had some profound and positive effects. <a href="https://en.wikipedia.org/wiki/Total_quality_management?ref=jina-ai-gmbh.ghost.io">Total Quality Management</a>, a set of doctrines credited with Japan&#x2019;s rise to economic dominance in the 1980s, is almost completely about constant measurement of proxy variables and optimizing practices on that basis.</p><p>But a focus on measurement poses some known, big problems:</p><ul><li>A measurement may stop being a good proxy when you make decisions based on it.</li><li>There are often ways to inflate a measure that don&#x2019;t improve anything, leading to the possibility of cheating or believing you are making progress by doing things that aren&#x2019;t helping.</li></ul><p>Some people believe <a href="https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.0020124&amp;ref=jina-ai-gmbh.ghost.io">most medical research may be just wrong</a> in part because of this problem. The disconnect between things you can measure and actual goals is one of the reasons cited <a href="https://en.wikipedia.org/wiki/McNamara_fallacy?ref=jina-ai-gmbh.ghost.io">for the calamity of America&#x2019;s war in Vietnam</a>.</p><p>This is sometimes called the &#x201C;Streetlight Effect&#x201D;, from the stories, like the one at the top of this page, of the drunk looking for something not where he lost it, but where the light is better. A proxy measure is like looking where there&#x2019;s light because there&apos;s no light on the thing we want to see.</p><p>In more technical literature, the &#x201C;Streetlight Effect&#x201D; is typically tied to <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law?ref=jina-ai-gmbh.ghost.io">Goodhart&#x2019;s Law</a>, attributed to British economist <a href="https://en.wikipedia.org/wiki/Charles_Goodhart?ref=jina-ai-gmbh.ghost.io">Charles Goodhart</a>&#x2019;s criticisms of the Thatcher government, which had placed a lot of emphasis on proxy measures of prosperity. Goodhart&#x2019;s Law has several formulations, but the one below is the most widely cited:</p><blockquote>[E]very measure which becomes a target becomes a bad measure[&#x2026;]<br><br><em>Keith Hoskins, 1996 The &apos;awful idea of accountability&apos;: inscribing people into the measurement of objects.</em>00s</blockquote><p>In AI, a famous example of this is the BLEU metric used in machine translation research. Developed in 2001 at IBM, BLEU is a way to automate the evaluation of machine translation systems, and it was a pivotal factor in the machine translation boom of the 00s. Once it was easy to give your system a score, you could work at improving it. And BLEU scores improved consistently. By 2010, it was nearly impossible to get a research paper on machine translation into a journal or conference if it didn&#x2019;t beat the state-of-the-art BLEU score, no matter how innovative the paper was nor how well it might handle some specific problem that other systems were handling poorly.</p><p>The easiest way to get into a conference was to find some minor way to fiddle with the parameters of your model, get a BLEU score fractionally higher than Google Translate&#x2019;s, and then submit. These results were essentially useless. Just getting some fresh texts for it to translate would show that they were rarely better and frequently worse than the state-of-the-art.</p><p>Instead of using BLEU to evaluate progress in machine translation, getting a better BLEU score became the goal. As soon as that happened, it stopped being a useful way to evaluate progress.</p><h2 id="are-our-ai-benchmarks-good-proxies">Are Our AI Benchmarks Good Proxies?</h2><p>The most widely used benchmark for embedding models is the MTEB test set, which consists of 56 specific tests. These are averaged by category and all together to produce a collection of class-specific scores. At the time of writing, the top of the MTEB leaderboard looks like this:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Screenshot-2024-05-15-at-16.22.08.png" class="kg-image" alt="AIR-Bench: Better Metrics for Better Search Foundation" loading="lazy" width="1942" height="1454" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-15-at-16.22.08.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-15-at-16.22.08.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/Screenshot-2024-05-15-at-16.22.08.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Screenshot-2024-05-15-at-16.22.08.png 1942w" sizes="(min-width: 720px) 720px"></figure><p>The top-ranked embedding model has an overall average score of 68.28, the next highest is 67.56. It&#x2019;s very difficult, looking at this table, to know if that&apos;s a big difference or not. If it&#x2019;s a small difference, then other factors may be more important than which model has the highest score:</p><ul><li><strong>Model size:</strong> Models have different sizes, reflecting different computing resource demands. Small models run faster, in less memory, and require less expensive hardware. We see, on this top 10 list, models ranging in size from 434 million parameters to over 46 billion &#x2014; a 100-fold difference!</li><li><strong>Embedding size:</strong> Embedding dimensions vary. Smaller dimensionality makes embedding vectors use less memory and storage and makes vector comparisons (the core use of embeddings) much faster. In this list, we see embedding dimensions from 768 to 4096 &#x2014; only a five-fold difference but still significant when building commercial applications.</li><li><strong>Context input window size:</strong> Context windows vary in both size and quality, from 2048 tokens to 32768. Furthermore, different models use different approaches to positional encoding and input management, which can create biases in favor of specific parts of the input.</li></ul><p>In short, the overall average is a very incomplete way to determine which embedding model is best.</p><p>Even if we look at task-specific scores, like those below for retrieval, we face the same problems all over again. No matter what a model&#x2019;s score is on this set of tests, there is no way to know what models will perform best for your particular unique use case.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Screenshot-2024-05-15-at-16.52.31.png" class="kg-image" alt="AIR-Bench: Better Metrics for Better Search Foundation" loading="lazy" width="2000" height="1324" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Screenshot-2024-05-15-at-16.52.31.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Screenshot-2024-05-15-at-16.52.31.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/Screenshot-2024-05-15-at-16.52.31.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Screenshot-2024-05-15-at-16.52.31.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>But that&#x2019;s not the end of the problems with these kinds of benchmarks.</p><p>The main insight of Goodhart&#x2019;s Law is that a metric can always be gamed, often without intending to. For example, MTEB benchmarks consist of data from public sources that are likely to be in your training data. Unless you specifically work to try to remove benchmarking data from your training, your benchmark scores will be statistically unsound.</p><p>There is no simple, comprehensive solution. A benchmark is a proxy and we can never be certain it reflects what we want to know but can&#x2019;t directly measure.</p><p>But we do see three core problems with AI benchmarks that we can mitigate:</p><ol><li>Benchmarks are fixed in nature: The same tasks, using the same texts.</li><li>Benchmarks are generic: They are not very informative about real scenarios.</li><li>Benchmarks are inflexible: They cannot respond to diverse use cases.</li></ol><p>AI creates problems like this, but it sometimes also creates solutions. We believe we can use AI models to address these issues, at least as they affect AI benchmarks.</p><h2 id="using-ai-to-benchmark-ai-air-bench">Using AI to Benchmark AI: AIR-Bench</h2><p>AIR-Bench is open source and available under the <a href="https://opensource.org/license/mit?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">MIT License</a>. You can view or download the code from its <a href="https://github.com/AIR-Bench/AIR-Bench/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">repository on GitHub</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/AIR-Bench/AIR-Bench/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - AIR-Bench/AIR-Bench: AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark</div><div class="kg-bookmark-description">AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark - AIR-Bench/AIR-Bench</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">AIR-Bench</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://repository-images.githubusercontent.com/796154919/063cb803-f83f-4fcf-b860-132a73c4c2d9" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><h3 id="what-does-it-do">What does it do?</h3><p>AIR-Bench brings some important features to AI benchmarking:</p><ul><li><strong>Specialization for Retrieval and RAG Applications</strong> <br>This benchmark is oriented towards realistic information retrieval applications and retrieval-augmented generation pipelines.</li><li><strong>Domain and Language Flexibility</strong> <br>AIR makes it much easier to create benchmarks from domain-specific data or for another language, or even from task-specific data of your own.</li><li><strong>Automated Data Generation</strong> <br>AIR-Bench generates test data and the dataset receives regular updates, reducing the risk of data leakage.</li></ul><h2 id="air-bench-leaderboard-on-huggingface">AIR-Bench Leaderboard on HuggingFace</h2><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Explore the public beta AIR-Bench Leaderboard in <a href="https://huggingface.co/spaces/AIR-Bench/leaderboard?ref=jina-ai-gmbh.ghost.io">AIR-Bench&#x2019;s HuggingFace Space</a>.</div></div><p>We are operating a <a href="https://huggingface.co/spaces/AIR-Bench/leaderboard?ref=jina-ai-gmbh.ghost.io">leaderboard</a>, similar to the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">MTEB one</a>, for the current release of AIR-Bench-generated tasks. We will regularly regenerate the benchmarks, add new ones, and expand coverage to more AI models.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/spaces/AIR-Bench/leaderboard?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AIR-Bench Leaderboard - a Hugging Face Space by AIR-Bench</div><div class="kg-bookmark-description">Discover amazing ML apps made by the community</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">a Hugging Face Space by AIR-Bench</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/spaces/AIR-Bench/leaderboard.png" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><h3 id="how-does-it-work">How does it work?</h3><p>The core insight of the AIR approach is that we can use large language models (LLMs) to <em>generate</em> new texts and new tasks that can&#x2019;t be in any training set.</p><p>AIR-Bench takes advantage of the creative abilities of LLMs by asking them to play out a scenario. The user chooses a collection of documents &#x2014; a real one that may be a part of some models&#x2019; training data &#x2014; and then imagines a user with a defined role, and a situation in which they would need to use that corpus of documents.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-23.png" class="kg-image" alt="AIR-Bench: Better Metrics for Better Search Foundation" loading="lazy" width="2000" height="297" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-23.png 2105w" sizes="(min-width: 1200px) 1200px"></figure><p>Then, the user selects a document from the corpus and passes it, with the user profile and situation description, to the LLM. The LLM is prompted to create queries that are appropriate to that user and situation and which should find that document.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-29.png" class="kg-image" alt="AIR-Bench: Better Metrics for Better Search Foundation" loading="lazy" width="1344" height="614" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-29.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-29.png 1344w" sizes="(min-width: 1200px) 1200px"></figure><p>The AIR-Bench pipeline then prompts the LLM with the document and the query and makes synthetic documents that are <em>similar</em> to the one provided but which <em>should not</em> match the query.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-27.png" class="kg-image" alt="AIR-Bench: Better Metrics for Better Search Foundation" loading="lazy" width="974" height="702" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-27.png 974w" sizes="(min-width: 720px) 720px"></figure><p>We now have:</p><ul><li>A collection of queries</li><li>A matching real document for each query</li><li>A small collection of expected non-matching synthetic documents</li></ul><p>AIR-Bench merges the synthetic documents with the collection of real documents and then uses one or more embedding and reranker models to verify that the queries <em>ought</em> to be able to retrieve the matching documents. It also uses the LLM to verify that each query is relevant to the documents it ought to retrieve.</p><p>For more details on this AI-centric generation and quality control process, read the <a href="https://github.com/AIR-Bench/AIR-Bench/blob/main/docs/data_generation.md?ref=jina-ai-gmbh.ghost.io">Data Generation documentation</a> in the <a href="https://github.com/AIR-Bench/AIR-Bench/?ref=jina-ai-gmbh.ghost.io">AIR-Bench repository on GitHub</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/AIR-Bench/AIR-Bench/blob/main/docs/data_generation.md?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AIR-Bench/docs/data_generation.md at main &#xB7; AIR-Bench/AIR-Bench</div><div class="kg-bookmark-description">AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark - AIR-Bench/AIR-Bench</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">AIR-Bench</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://repository-images.githubusercontent.com/796154919/063cb803-f83f-4fcf-b860-132a73c4c2d9" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><p>The result is a set of high-quality query-match pairs and a semi-synthetic dataset to run them against. Even if the original real document collection does form a part of its training, the added synthetic documents and the queries themselves are new, never-before-seen data that it could not have previously learned from.</p><h3 id="domain-specific-benchmarks-and-reality-based-testing">Domain-Specific Benchmarks and Reality-Based Testing</h3><p>Synthesizing queries and documents prevents benchmark data from leaking into training, but it also goes a long way to address the problem of generic benchmarks.</p><p>By providing LLMs with chosen data, a user profile, and a scenario, AIR-Bench makes it very easy to construct benchmarks for particular use cases. Furthermore, by constructing queries for a specific type of user and usage scenario, AIR-Bench can produce test queries that are truer to real-world usage than traditional benchmarks. An LLM&#x2019;s limited creativity and imagination may not entirely match a real-world scenario, but it&#x2019;s a better match than a static test dataset made out of data available to researchers.</p><p>As a by-product of this flexibility, AIR-Bench supports all the languages that GPT-4 supports.</p><p>Furthermore, AIR-Bench focuses specifically on realistic AI-based information retrieval, by far the most widespread application of embedding models. It does not provide scores for other kinds of tasks like clustering or classification.</p><h2 id="the-air-bench-distribution">The AIR-Bench Distribution</h2><p>AIR-Bench is available to download, use, and modify via its <a href="https://github.com/AIR-Bench/AIR-Bench/?ref=jina-ai-gmbh.ghost.io">GitHub repository</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/AIR-Bench/AIR-Bench/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - AIR-Bench/AIR-Bench: AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark</div><div class="kg-bookmark-description">AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark - AIR-Bench/AIR-Bench</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">AIR-Bench</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://repository-images.githubusercontent.com/796154919/063cb803-f83f-4fcf-b860-132a73c4c2d9" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><p>AIR-Bench supports two kinds of benchmarks:</p><ul><li>An information retrieval task based on evaluating the correct retrieval of documents relevant to specific queries.</li><li>A &#x201C;long document&#x201D; task that mimics the information retrieval portion of a retrieval-augmented generation pipeline.</li></ul><p>We have also <a href="https://github.com/AIR-Bench/AIR-Bench/blob/main/docs/available_tasks.md?ref=jina-ai-gmbh.ghost.io">pre-generated a set of benchmarks</a>, in English and Chinese, along with the scripts to generate them as live examples of how to use AIR-Bench. These use sets of readily available data.</p><p>For example, for a <a href="https://huggingface.co/datasets/NeuML/wikipedia-20240101?ref=jina-ai-gmbh.ghost.io">selection of 6,738,498 English Wikipedia pages</a>, we have generated 1,727 queries matching 4,260 documents and an additional 7,882 synthetic non-matching but similar documents. We offer conventional information retrieval benchmarks for eight English-language datasets and six in Chinese. For the &#x201C;long document&#x201D; tasks, we provide fifteen benchmarks, all in English.</p><p>To see the complete list and more details, visit the <a href="https://github.com/AIR-Bench/AIR-Bench/blob/main/docs/available_tasks.md?ref=jina-ai-gmbh.ghost.io">Available Tasks page in the AIR-Bench repo on GitHub</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/AIR-Bench/AIR-Bench/blob/main/docs/available_tasks.md?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AIR-Bench/docs/available_tasks.md at main &#xB7; AIR-Bench/AIR-Bench</div><div class="kg-bookmark-description">AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark - AIR-Bench/AIR-Bench</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="AIR-Bench: Better Metrics for Better Search Foundation"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">AIR-Bench</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://repository-images.githubusercontent.com/796154919/063cb803-f83f-4fcf-b860-132a73c4c2d9" alt="AIR-Bench: Better Metrics for Better Search Foundation"></div></a></figure><h2 id="get-involved">Get Involved</h2><p>The AIR-Benchmark has been designed to be a tool for the Search Foundations community so that engaged users can create benchmarks better suited to their needs. When your tests are informative about your use cases, they inform us too, so we can build products that better meet your needs.</p>]]></content:encoded></item><item><title><![CDATA[Binary Embeddings: All the AI, 3.125% of the Fat]]></title><description><![CDATA[32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.]]></description><link>https://jina.ai/news/binary-embeddings-all-the-ai-3125-of-the-fat/</link><guid isPermaLink="false">662665537f510100015daa2d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Sofia Vasileva]]></dc:creator><pubDate>Wed, 15 May 2024 14:00:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png" alt="Binary Embeddings: All the AI, 3.125% of the Fat"><p>Embeddings have become the cornerstone of a variety of AI and natural language processing applications, offering a way to represent the meanings of texts as high-dimensional vectors. However, between the increasing size of models and the growing quantities of data AI models process, the computational and storage demands for traditional embeddings have escalated. Binary embeddings have been introduced as a compact, efficient alternative that maintains high performance while drastically reducing resource requirements.</p><p>Binary embeddings are one way to mitigate these resource requirements by reducing the size of embedding vectors by as much as 96% (96.875% in the case of Jina Embeddings). Users can leverage the power of compact binary embeddings within their AI applications with minimal loss of accuracy.</p><h2 id="what-are-binary-embeddings">What Are Binary Embeddings?</h2><p>Binary embeddings are a specialized form of data representation where traditional high-dimensional floating-point vectors are transformed into binary vectors. This not only compresses the embeddings but also retains nearly all of the vectors&apos; integrity and utility. The essence of this technique lies in its ability to maintain the semantics and relational distances between the data points even after conversion.<br><br>The magic behind binary embeddings is quantization, a method that turns high-precision numbers into lower-precision ones. In AI modeling, this often means converting the 32-bit floating-point numbers in embeddings into representations with fewer bits, like 8-bit integers.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="1280" height="860" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Binarization is the transformation of all scalar values to 0 or 1, like converting a color image to one with just black or white pixels. Image: &#x795E;&#x5948;&#x5DDD;&#x6C96;&#x6D6A;&#x88CF; (1831) by &#x845B;&#x98FE; (Hokusai)</span></figcaption></figure><p>Binary embeddings take this to its ultimate extreme, reducing each value to 0 or 1. Transforming 32-bit floating point numbers to binary digits cuts the size of embedding vectors 32-fold, a reduction of 96.875%. Vector operations on the resulting embeddings are much faster as a result. Using hardware speed-ups available on some microchips can increase the speed of vector comparisons by much more than 32-fold when the vectors are binarized.</p><p>Some information is inevitably lost during this process, but this loss is minimized when the model is very performant. If the non-quantized embeddings of different things are maximally different, then binarization is more likely to preserve that difference well. Otherwise, it can be difficult to interpret the embeddings correctly.</p><p>Jina Embeddings models are trained to be very robust in exactly that way, making them well-suited to binarization.</p><p>Such compact embeddings make new AI applications possible, particularly in resource-constrained contexts like mobile and time-sensitive uses.</p><p>These cost and computing time benefits come at a relatively small performance cost, as the chart below shows.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://hackmd.io/_uploads/ByhwJsQWC.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="1686" height="1050"><figcaption><i><em class="italic" style="white-space: pre-wrap;">NDCG@10: Scores calculated using </em></i><a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io"><i><em class="italic" style="white-space: pre-wrap;">Normalized Discounted Cumulative Gain</em></i></a><i><em class="italic" style="white-space: pre-wrap;"> for the top 10 results.</em></i></figcaption></figure><p>For <code>jina-embeddings-v2-base-en</code>, binary quantization reduces retrieval accuracy from 47.13% to 42.05%, a loss of approximately 10%. For <code>jina-embeddings-v2-base-de</code>, this loss is only 4%, from 44.39% to 42.65%.</p><p>Jina Embeddings models perform so well when producing binary vectors because they are trained to create a more uniform distribution of embeddings. This means that two different embeddings will likely be further from each other in more dimensions than embeddings from other models. This property ensures that those distances are better represented by their binary forms.</p><h2 id="how-do-binary-embeddings-work">How Do Binary Embeddings Work?</h2><p>To see how this works, consider three embeddings: <em>A</em>, <em>B</em>, and <em>C</em>. These three are all full floating-point vectors, not binarized ones. Now, let&#x2019;s say the distance from <em>A</em> to <em>B</em> is greater than the distance from <em>B</em> to <em>C</em>. With embeddings, we typically use the <a href="https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io">cosine distance</a>, so:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-9.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="172" height="19"></figure><p>If we binarize <em>A</em>, <em>B</em>, and <em>C</em>, we can measure distance more efficiently with <a href="https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io">Hamming distance</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="2000" height="808" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Hamming Distance on a cube. Left: Distance from A to B is 1. Right: Distance from B to C is 2.</span></figcaption></figure><p>Let&#x2019;s call <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em> and <em>C<sub>bin</sub></em> the binarized versions of <em>A</em>, <em>B</em> and <em>C</em>.</p>
<p>For binary vectors, if the cosine distance between <em>A<sub>bin</sub></em> and <em>B<sub>bin</sub></em> is greater than between <em>B<sub>bin</sub></em> and <em>C<sub>bin</sub></em>, then the Hamming distance between <em>A<sub>bin</sub></em> and <em>B<sub>bin</sub></em> is greater than or equal to the Hamming distance between <em>B<sub>bin</sub></em> and <em>C<sub>bin</sub></em>.</p>
<p>So if:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-10.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="172" height="19"></figure><p>then for Hamming distances:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-11.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="296" height="19"></figure><p>Ideally, when we binarize embeddings, we want the same relationships with full embeddings to hold for the binary embeddings as for the full ones. This means that if one distance is greater than another for floating point cosine, it should be greater for the Hamming distance between their binarized equivalents:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-12.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="518" height="19"></figure><p>We can&#x2019;t make this true for all triplets of embeddings, but we can make it true for almost all of them.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png" class="kg-image" alt="Binary Embeddings: All the AI, 3.125% of the Fat" loading="lazy" width="1500" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">The blue dots correspond to full floating-point vectors and the red ones to their binarized equivalents. </span></figcaption></figure><p>With a binary vector, we can treat every dimension as either present (a one) or absent (a zero). The more distant two vectors are from each other in non-binary form, the higher the probability that in any one dimension, one will have a positive value and the other a negative value. This means that in binary form, there will most likely be more dimensions where one has a zero and the other a one. This makes them further apart by Hamming distance.</p><p>The opposite applies to vectors that are closer together: The closer the non-binary vectors are, the higher the probability that in any dimension both have zeros or both have ones. This makes them closer by Hamming distance.</p><p>Jina Embeddings models are so well-suited to binarization because we train them using negative mining and other fine-tuning practices to especially increase the distance between dissimilar things and reduce the distance between similar ones. This makes the embeddings more robust, more sensitive to similarities and differences, and makes the Hamming distance between binary embeddings more proportionate to the cosine distance between non-binary ones.</p><h2 id="how-much-can-i-save-with-jina-ais-binary-embeddings">How Much Can I Save with Jina AI&apos;s Binary Embeddings?</h2><p>Embracing Jina AI&#x2019;s binary embedding models doesn&apos;t just lower latency in time-sensitive applications, but also yields considerable cost benefits, as shown in the table below:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Memory per<br>250 million<br>embeddings</th>
<th>Retrieval<br>benchmark<br>average</th>
<th>Estimated price on AWS<br>($3.8 per GB/month<br>with x2gb instances)</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point embeddings</td>
<td>715 GB</td>
<td>47.13</td>
<td>$35,021</td>
</tr>
<tr>
<td>Binary embeddings</td>
<td>22.3 GB</td>
<td>42.05</td>
<td>$1,095</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>This savings of over 95% is accompanied by only ~10% reduction in retrieval accuracy.</p><p>These are even greater savings than using binarized vectors from <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io">OpenAI&apos;s Ada 2 model</a> or <a href="https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io">Cohere&#x2019;s Embed v3</a>, both of which produce output embeddings of 1024 dimensions or more. Jina AI&#x2019;s embeddings have only 768 dimensions and still perform comparably to other models, making them smaller even before quantization for the same accuracy.</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Binary vectors save memory, computing time, transmission bandwidth, and disk storage, providing financial benefits in a number of categories</strong></b>. </div></div><p>These savings are also environmental, using fewer rare materials and less energy.</p><h2 id="get-started">Get Started</h2><p>To get binary embeddings using the <a href="https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Jina Embeddings API</a>, just add the parameter <code>encoding_type</code> to your API call, with the value <code>binary</code> to get the binarized embedding encoded as signed integers, or <code>ubinary</code> for unsigned integers.</p><h3 id="directly-access-jina-embedding-api">Directly Access Jina Embedding API</h3><p>Using <code>curl</code>:</p><pre><code class="language-bash">curl https://api.jina.ai/v1/embeddings \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer &lt;YOUR API KEY&gt;&quot; \
  -d &apos;{
    &quot;input&quot;: [&quot;Your text string goes here&quot;, &quot;You can send multiple texts&quot;],
    &quot;model&quot;: &quot;jina-embeddings-v2-base-en&quot;,
    &quot;encoding_type&quot;: &quot;binary&quot;
  }&apos;
</code></pre><p>Or via the Python <code>requests</code> API:</p><pre><code class="language-Python">import requests

headers = {
  &quot;Content-Type&quot;: &quot;application/json&quot;,
  &quot;Authorization&quot;: &quot;Bearer &lt;YOUR API KEY&gt;&quot;
}

data = {
  &quot;input&quot;: [&quot;Your text string goes here&quot;, &quot;You can send multiple texts&quot;],
  &quot;model&quot;: &quot;jina-embeddings-v2-base-en&quot;,
  &quot;encoding_type&quot;: &quot;binary&quot;,
}

response = requests.post(
    &quot;https://api.jina.ai/v1/embeddings&quot;, 
    headers=headers, 
    json=data,
)
</code></pre><p>With the above Python <code>request</code>, you will get the following response by inspecting <code>response.json()</code>:</p><pre><code class="language-JSON">{
  &quot;model&quot;: &quot;jina-embeddings-v2-base-en&quot;,
  &quot;object&quot;: &quot;list&quot;,
  &quot;usage&quot;: {
    &quot;total_tokens&quot;: 14,
    &quot;prompt_tokens&quot;: 14
  },
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;embedding&quot;,
      &quot;index&quot;: 0,
      &quot;embedding&quot;: [
        -0.14528547,
        -1.0152762,
        ...
      ]
    },
    {
      &quot;object&quot;: &quot;embedding&quot;,
      &quot;index&quot;: 1,
      &quot;embedding&quot;: [
        -0.109809875,
        -0.76077706,
        ...
      ]
    }
  ]
}
</code></pre><p>These are two binary embedding vectors stored as 96 8-bit signed integers. To unpack them to 768 0&#x2019;s and 1&#x2019;s, you need to use the <code>numpy</code> library:</p><pre><code class="language-Python">import numpy as np

# assign the first vector to embedding0
embedding0 = response.json()[&apos;data&apos;][0][&apos;embedding&apos;]

# convert embedding0 to a numpy array of unsigned 8-bit ints
uint8_embedding = np.array(embedding0).astype(numpy.uint8) 

# unpack to binary
np.unpackbits(uint8_embedding)
</code></pre><p>The result is a 768-dimension vector with only 0&#x2019;s and 1&#x2019;s:</p><pre><code class="language-Python">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],
      dtype=uint8)
</code></pre><h3 id="using-binary-quantization-in-qdrant">Using Binary Quantization in Qdrant</h3><p>You can also use <a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant&apos;s integration library</a> to put binary embeddings directly in your Qdrant vector store. As Qdrant has internally implemented <code>BinaryQuantization</code>, you can use it as a preset configuration for the entire vector collection, making it retrieve and store binary vectors without any other changes to your code.</p><p>See the example code below for how:</p><pre><code class="language-Python">import qdrant_client
import requests

from qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig

# Provide Jina API key and choose one of the available models.
# You can get a free trial key here: https://jina.ai/embeddings/
JINA_API_KEY = &quot;jina_xxx&quot;
MODEL = &quot;jina-embeddings-v2-base-en&quot;  # or &quot;jina-embeddings-v2-base-en&quot;
EMBEDDING_SIZE = 768  # 512 for small variant

# Get embeddings from the API
url = &quot;https://api.jina.ai/v1/embeddings&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {JINA_API_KEY}&quot;,
}

text_to_encode = [&quot;Your text string goes here&quot;, &quot;You can send multiple texts&quot;]
data = {
    &quot;input&quot;: text_to_encode,
    &quot;model&quot;: MODEL,
}

response = requests.post(url, headers=headers, json=data)
embeddings = [d[&quot;embedding&quot;] for d in response.json()[&quot;data&quot;]]


# Index the embeddings into Qdrant
client = qdrant_client.QdrantClient(&quot;:memory:&quot;)
client.create_collection(
    collection_name=&quot;MyCollection&quot;,
    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),
    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),
)

client.upload_collection(
    collection_name=&quot;MyCollection&quot;,
    ids=list(range(len(embeddings))),
    vectors=embeddings,
    payload=[
            {&quot;text&quot;: x} for x in text_to_encode
    ],
)</code></pre><p>To configure for search, you should use the <code>oversampling</code> and <code>rescore</code> parameters:</p><pre><code class="language-python">from qdrant_client.models import SearchParams, QuantizationSearchParams

results = client.search(
    collection_name=&quot;MyCollection&quot;,
    query_vector=embeddings[0],
    search_params=SearchParams(
        quantization=QuantizationSearchParams(
            ignore=False,
            rescore=True,
            oversampling=2.0,
        )
    )
)</code></pre><h3 id="using-llamaindex">Using LlamaIndex </h3><p>To use Jina binary embeddings with LlamaIndex, set the <code>encoding_queries</code> parameter to <code>binary</code> when instantiating the<code>JinaEmbedding</code> object:</p><pre><code class="language-python">from llama_index.embeddings.jinaai import JinaEmbedding

# You can get a free trial key from https://jina.ai/embeddings/
JINA_API_KEY = &quot;&lt;YOUR API KEY&gt;&quot;

jina_embedding_model = JinaEmbedding(
    api_key=jina_ai_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
    encoding_queries=&apos;binary&apos;,
    encoding_documents=&apos;float&apos;
)

jina_embedding_model.get_query_embedding(&apos;Query text here&apos;)
jina_embedding_model.get_text_embedding_batch([&apos;X&apos;, &apos;Y&apos;, &apos;Z&apos;])
</code></pre><h3 id="other-vector-databases-supporting-binary-embeddings">Other Vector Databases Supporting Binary Embeddings</h3><p>The following vector databases provide native support for binary vectors:</p><ul><li><a href="https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io">AstraDB by DataStax</a></li><li><a href="https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io">FAISS</a></li><li><a href="https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT">Milvus</a></li><li><a href="https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io">Vespa.ai</a></li><li><a href="https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io">Weaviate</a></li></ul><h2 id="example">Example</h2><p>To show you binary embeddings in action, we took a selection of abstracts from <a href="http://arxiv.org/?ref=jina-ai-gmbh.ghost.io">arXiv.org</a>, and got both 32-bit floating point and binary vectors for them using <code>jina-embeddings-v2-base-en</code>. We then compared them to the embeddings for an example query: &quot;3D segmentation.&quot;</p><p>You can see from the table below that the top three answers are the same and four of the top five match. Using binary vectors produces almost identical top matches.</p>
<!--kg-card-begin: html-->
<table>
<head>
<tr>
  <th>
  </th><th colspan="2">Binary</th>
  <th colspan="2">32-bit Float</th>
</tr>
<tr>
<th>Rank</th>
<th>Hamming<br>dist.</th>
<th>Matching Text</th>
<th>Cosine</th>
<th>Matching text</th>
</tr>

<tbody>
<tr>
<td>1</td>
<td>0.1862</td>
<td>SEGMENT3D: A Web-based<br>Application for Collaboration...</td>
<td>0.2340</td>
<td>SEGMENT3D: A Web-based<br>Application for Collaboration...</td>
</tr>
<tr>
<td>2</td>
<td>0.2148</td>
<td>Segmentation-by-Detection:<br>A Cascade Network for...</td>
<td>0.2857</td>
<td>Segmentation-by-Detection:<br>A Cascade Network for...</td>
</tr>
<tr>
<td>3</td>
<td>0.2174</td>
<td>Vox2Vox: 3D-GAN for Brain<br>Tumour Segmentation...</td>
<td>0.2973</td>
<td>Vox2Vox: 3D-GAN for Brain<br>Tumour Segmentation...</td>
</tr>
<tr>
<td>4</td>
<td>0.2318</td>
<td>DiNTS: Differentiable Neural<br>Network Topology Search...</td>
<td>0.2983</td>
<td>Anisotropic Mesh Adaptation for<br>Image Segmentation...</td>
</tr>
<tr>
<td>5</td>
<td>0.2331</td>
<td>Data-Driven Segmentation of<br>Post-mortem Iris Image...</td>
<td>0.3019</td>
<td>DiNTS: Differentiable Neural<br>Network Topology...</td>
</tr>
</tbody>
</head></table>
<!--kg-card-end: html-->
<h2 id></h2>]]></content:encoded></item><item><title><![CDATA[Jina Reader for Search Grounding to Improve Factuality of LLMs]]></title><description><![CDATA[Grounding is essential for GenAI apps. Our new https://s.jina.ai/ allows LLMs to access the latest knowledge from the web, enabling search grounding and making responses more trustworthy.]]></description><link>https://jina.ai/news/jina-reader-for-search-grounding-to-improve-factuality-of-llms/</link><guid isPermaLink="false">664381073883a50001b2110d</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 14 May 2024 16:06:37 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Heading--21-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Heading--21-.png" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs"><p>Grounding is <em>absolutely</em> essential for GenAI applications.</p><p>You have probably seen many tools, prompts, and RAG pipelines designed to improve the factuality of LLMs since 2023. Why? Because the primary barrier preventing enterprises from deploying LLMs to millions of users is <strong>the trust</strong>: Is the answer genuine, or is it a mere hallucination from the model? This is an industry-wide problem, and Jina AI has been working very hard to solve it. Today, with the new Jina Reader search grounding feature, <strong>you can simply use <code>https://s.jina.ai/YOUR_SEARCH_QUERY</code> to search the latest world-knowledge from the web.</strong> With this, you are one step closer to improving the factuality of LLMs, making their responses more trustworthy and helpful.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://jina.ai/reader?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reader API</div><div class="kg-bookmark-description">Read URLs or search the web, get better grounding for LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reader-api.png" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs"></div></a><figcaption><p><span style="white-space: pre-wrap;">API, demo can be found in the product page</span></p></figcaption></figure><h2 id="the-factuality-problem-of-llms">The Factuality Problem of LLMs</h2><p>We all know LLMs can make things up and harm user trust. LLMs may say things that are not factual (aka hallucinate), especially regarding topics they didn&apos;t learn about during training. This could be either new information created since training or niche knowledge that has been &quot;marginalized&quot; during training.</p><p>As a result, when it comes to questions like &quot;What&apos;s the weather today?&quot; or &quot;Who won the Oscar for Best Actress this year?&quot; the model will either respond with &quot;I don&apos;t know&quot; or give you outdated information.</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io#demo"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-13.png" class="kg-image" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" loading="lazy" width="2000" height="803" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-13.png 2000w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">An example of niche knowledge being &quot;marginalized&quot; during training can be seen when we asked </span><code spellcheck="false" style="white-space: pre-wrap;"><span>GPT-3.5-turbo</span></code><span style="white-space: pre-wrap;"> &quot;When was Jina AI founded?&quot; and received an incorrect answer. However, when using Reader for search grounding, the same LLM was able to provide the correct answer. In fact, it was precise to the exact date.</span></figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io#demo"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-14.png" class="kg-image" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" loading="lazy" width="2000" height="799" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-14.png 2000w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">An example of new information created since training. We asked </span><code spellcheck="false" style="white-space: pre-wrap;"><span>GPT-3.5-turbo</span></code><span style="white-space: pre-wrap;"> &quot;When will the next SpaceX launch be?&quot; (today is May 14th 2024) and the model responded with old information back in 2021.</span></figcaption></figure><h2 id="how-jina-reader-helps-better-grounding">How Jina Reader Helps Better Grounding</h2><p>Previously, users could easily prepend <code>https://r.jina.ai</code> to read text and image content from a particular URL into an LLM-friendly format and use it for check grounding and fact verification. Since its first release on April 15th, we have served over <strong>18 million requests</strong> from the world, suggesting its popularity.</p><p>Today we are excited to move the needle further by introducing the search grounding API <code>https://s.jina.ai</code>. By simply prepending it before your query, Reader will search the web and retrieve the top 5 results. Each result includes<strong> a title, LLM-friendly markdown</strong> (full content! not abstract), and <strong>a URL</strong> that allows you to attribute the source. Here is an example below, you are also encouraged to try <a href="https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io#demo">our live demo here</a>.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-4.jpg" width="1686" height="1846" loading="lazy" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled-4.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled-4.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/Untitled-4.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-4.jpg 1686w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-5.jpg" width="1338" height="798" loading="lazy" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/Untitled-5.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/Untitled-5.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/Untitled-5.jpg 1338w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Left: Markdown mode (directly visit </span><a href="https://s.jina.ai/who+is+han+xiao?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">https://s.jina.ai/who+is+han+xiao</span></a><span style="white-space: pre-wrap;">); Right JSON mode (using </span><code spellcheck="false" style="white-space: pre-wrap;"><span>curl https://s.jina.ai/who+is+han+xiao -H &apos;accept: application/json&apos;</span></code><span style="white-space: pre-wrap;">). Btw, an ego question like this always serves as a good test case.</span></p></figcaption></figure><p>There are three principles when we designing the search grounding in the Reader:</p><ul><li>Improve factuality;</li><li>Access up-to-date information, i.e., world knowledge;</li><li>Connect an answer to its source.</li></ul><p>Besides being extremely easy to use, <code>s.jina.ai</code> is also highly scalable and customizable as it leverages the existing flexible and scalable infrastructure of <code>r.jina.ai</code>. You can set parameters to control the image captioning, filter granularity, etc., via the request headers.</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://jina.ai/reader?ref=jina-ai-gmbh.ghost.io#apiform"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/6cf51d582e35abedd95e3272a0eaa7f1.gif" class="kg-image" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" loading="lazy" width="1000" height="636" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/6cf51d582e35abedd95e3272a0eaa7f1.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/6cf51d582e35abedd95e3272a0eaa7f1.gif 1000w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Try the interactive code snippet for the advanced usage of Reader API</span></figcaption></figure><h2 id="jina-reader-as-a-comprehensive-grounding-solution">Jina Reader as a Comprehensive Grounding Solution</h2><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/05/Heading--17-.svg" class="kg-image" alt="Jina Reader for Search Grounding to Improve Factuality of LLMs" loading="lazy" width="1200" height="630"></figure><p>If we combine search grounding (<code>s.jina.ai</code>) and check grounding (<code>r.jina.ai</code>), we can build a very comprehensive grounding solution for LLMs, agents, and RAG systems. In a typical trustworthy RAG workflow, Jina Reader works as follows:</p><ol><li>User inputs a question;</li><li>Retrieve the latest information from the web using <code>s.jina.ai</code>;</li><li>Generate an initial answer with a citation to the search result from the last step;</li><li>Use <code>r.jina.ai</code> to ground the answer with your own URL; or read the inline URLs from the source returned from step 3 to get deeper grounding;</li><li>Final answer generation and highlight potentially ungrounded claims to the user.</li></ol><h2 id="higher-rate-limit-with-api-keys">Higher Rate Limit with API Keys</h2><p>Users can enjoy the new search grounding endpoint for free without authorization. Moreover, when providing a Jina AI API key in the request header (the same key can be used in the Embedding/Reranking API), you can immediately enjoy 200 requests per minute per IP for <code>r.jina.ai</code> and 40 requests per minute per IP for <code>s.jina.ai</code>. The details can be found in the table below:</p>
<!--kg-card-begin: html-->
<table class="q-table"><thead data-v-ed61ae60><tr data-v-ed61ae60><th data-v-ed61ae60>Endpoint</th><th data-v-ed61ae60>Description</th><th data-v-ed61ae60>Rate limit w/o API key</th><th data-v-ed61ae60>Rate limit with API key</th><th data-v-ed61ae60>Token counting scheme</th><th data-v-ed61ae60>Average latency</th></tr></thead><tbody data-v-ed61ae60><tr data-v-ed61ae60><td data-v-ed61ae60><code data-v-ed61ae60>r.jina.ai</code></td><td data-v-ed61ae60>Read a URL return its content, useful for check grounding</td><td data-v-ed61ae60>20 RPM</td><td data-v-ed61ae60>200 RPM</td><td data-v-ed61ae60>Based on the output tokens</td><td data-v-ed61ae60>3 seconds</td></tr><tr data-v-ed61ae60><td data-v-ed61ae60><code data-v-ed61ae60>s.jina.ai</code></td><td data-v-ed61ae60>Search on the web return top-5 results, useful for search grounding</td><td data-v-ed61ae60>5 RPM</td><td data-v-ed61ae60>40 RPM</td><td data-v-ed61ae60>Based on the output tokens for all 5 search results</td><td data-v-ed61ae60>30 seconds</td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="conclusion">Conclusion</h2><p>We believe grounding is essential for GenAI applications, and building grounded solutions should be easy for everyone. That&apos;s why we introduced the new search grounding endpoint, <code>s.jina.ai</code>, which allows developers to easily incorporate world knowledge into their GenAI applications. We want developers to establish user trust, provide explainable answers, and inspire curiosity in millions of users.</p>]]></content:encoded></item></channel></rss>
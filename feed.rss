<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.75</generator><lastBuildDate>Tue, 02 Jan 2024 09:54:09 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[DocArray 0.40.0 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-40-0-update/</link><guid isPermaLink="false">658599a20bab3100012d2b68</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 22 Dec 2023 14:20:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0400">Release Note (<code>0.40.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" alt="DocArray 0.40.0 Update"><p>This release contains 1 new feature, 3 bug fixes, and 2 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.40.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v0.40.0 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.40.0) Release time: 2023-12-22 12:12:15 This release contains 1 new feature, 3 bug fixes and 2 documentation improvements.
&#x1F195; Features
Add Epsilla connector (#1835)
We have integra&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="DocArray 0.40.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/668ee783e022a27964faa8e5f0a854cc1dea3f036e49bb10f4413288f5b2066f/docarray/docarray/releases/tag/v0.40.0" alt="DocArray 0.40.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="add-epsilla-connector-1835">Add Epsilla connector (<a href="https://github.com/docarray/docarray/pull/1835?ref=jina-ai-gmbh.ghost.io">#1835</a>)</h3><p>We have integrated&#xA0;<a href="https://epsilla.com/?ref=jina-ai-gmbh.ghost.io" rel="nofollow">Epsilla</a>&#xA0;into DocArray.</p><p>Here&apos;s a simple example of how to use it:</p><pre><code class="language-Python">import numpy as np
from docarray import BaseDoc
from docarray.index import EpsillaDocumentIndex
from docarray.typing import NdArray
from pydantic import Field

class MyDoc(BaseDoc):
    text: str
    embedding: NdArray[10] = Field(is_embedding=True)

docs = [MyDoc(text=f&apos;text {i}&apos;, embedding=np.random.rand(10)) for i in range(10)]
query = np.random.rand(10)
db = EpsillaDocumentIndex[MyDoc]()
db.index(docs)
results = db.find(query, limit=10)</code></pre><p>In this example, we create a document class with both textual and numeric data. Then, we initialize an Epsilla-backed document index and use it to index our documents. Finally, we perform a search query.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fixed-type-hints-error-in-python-312-1840">Fixed type hints error in Python 3.12 (<a href="https://github.com/docarray/docarray/pull/1840?ref=jina-ai-gmbh.ghost.io">#1840</a>)</h3><p>DocArray type-hinting is now available for Python 3.12.</p><h3 id="fix-issue-serializing-and-deserializing-complex-schemas-1836">Fix issue serializing and deserializing complex schemas (<a href="https://github.com/docarray/docarray/pull/1836?ref=jina-ai-gmbh.ghost.io">#1836</a>)</h3><p>There was an issue when serializing and deserializing&#xA0;<code>protobuf</code>&#xA0;documents with nested documents in dictionaries and other complex structures.</p><h3 id="fix-storage-issue-in-torchtensor-class-1833">Fix storage issue in TorchTensor class (<a href="https://github.com/docarray/docarray/pull/1833?ref=jina-ai-gmbh.ghost.io">#1833</a>)</h3><p>There was a bug when deep-copying a&#xA0;<code>TorchTensor</code>&#xA0;object if its&#xA0;<code>dtype</code>&#xA0;was not&#xA0;<code>float32</code>. This has now been fixed.</p><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Add Epsilla integration guide (<a href="https://github.com/docarray/docarray/pull/1838?ref=jina-ai-gmbh.ghost.io">docs(epsilla): add epsilla integration guide #1838</a>)</li><li>Fix sign commit command in docs (<a href="https://github.com/docarray/docarray/pull/1834?ref=jina-ai-gmbh.ghost.io">docs: fix sign commit commad in docs #1834</a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Tony Yang (<a href="https://github.com/tonyyanga?ref=jina-ai-gmbh.ghost.io">@tonyyanga</a>&#xA0;)</li><li>Naymul Islam (<a href="https://github.com/ai-naymul?ref=jina-ai-gmbh.ghost.io">@ai-naymul</a>&#xA0;)</li><li>Ben Shaver (<a href="https://github.com/bpshaver?ref=jina-ai-gmbh.ghost.io">@bpshaver</a>&#xA0;)</li><li>Joan Fontanals (@<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>954 (<a href="https://github.com/954-Ivory?ref=jina-ai-gmbh.ghost.io">@954-Ivory</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Full-stack RAG with Jina Embeddings v2 and LlamaIndex]]></title><description><![CDATA[You can build your own RAG chatbot in a matter of minutes with Jina Embeddings, LlamaIndex and Mixtral Instruct. We'll show you how to get up and running right now.]]></description><link>https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/</link><guid isPermaLink="false">6583fe510bab3100012d29f2</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 22 Dec 2023 13:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><p>LLMs are cool, but their input context windows are always so small. They&#x2019;re getting bigger, but they&#x2019;ll never be big enough for, say, a whole book, much less an encyclopedia, unless there is some breakthrough in AI model architectures.</p><p>RAG (<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?ref=jina-ai-gmbh.ghost.io">Retrieval Augmented Generation</a>) is a strategy that can compensate for this limitation, letting you use LLMs to respond to questions with answers that draw on relevant parts of documents or whole repositories of documents that are far too large to put entirely into the model&#x2019;s input.</p><p>This article will show you how to use LlamaIndex, Jina Embeddings, and the <code>Mixtral-8x7B-Instruct-v0.1</code> language model (<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">hosted on HuggingFace</a>) to build a complete RAG system. For more information on the Mixtral language model, see the <a href="https://mistral.ai/news/mixtral-of-experts/?ref=jina-ai-gmbh.ghost.io">Mistral AI website</a> or the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">model card on HuggingFace</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://mistral.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mistral AI | Open source models</div><div class="kg-bookmark-description">Frontier AI in your hands</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://mistral.ai/images/favicon/apple-touch-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Open source models</span><span class="kg-bookmark-publisher">Mistral AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://mistral.ai/images/mistral-social-banner.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>You can also <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download a Jupyter Notebook with all the code in this article from GitHub</a>, or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run it directly on Colab</a>.</p><p>You will need:</p><ol><li>A <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings API key</a>.</li><li>A <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace account and token</a>.</li></ol><p>Since both the Jina Embeddings model and Mixtral are running remotely and are accessed via a web API, you won&#x2019;t need any special hardware. You will need to install Python and meet the <a href="https://docs.llamaindex.ai/en/stable/getting_started/installation.html?ref=jina-ai-gmbh.ghost.io">system requirements for LlamaIndex</a>.</p><h2 id="what-is-rag-and-how-does-it-work">What is RAG and How Does it Work?</h2><p>Retrieval Augmented Generation is a strategy that merges search with language generation. The way it works is that it uses an external information retrieval system to find documents that are likely to inform the answer to a user query. It then passes them, with the user&#x2019;s request, to a text-generating language model, which produces a natural language response.</p><p>This allows you to use an LLM to answer questions and use information from documents and sets of documents that are much larger than its input context window. The LLM only sees a few pertinent parts of the document when responding to prompts. This also has the advantage of reducing (although not eliminating) inexplicable hallucinations.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="1600" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>This strategy has some limitations:</p><ol><li>It is sensitive to the input context size supported by the LLM. The larger the context size, the more information you can give the LLM, yielding better and richer responses.</li><li>It is sensitive to the quality of the results of the initial information retrieval. If your search engine gives it irrelevant or inaccurate results, the LLM may paste them together as best it can and give you garbage output. This can be caused by bad data (as the saying goes <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out?ref=jina-ai-gmbh.ghost.io"><em>garbage in, garbage out</em></a>) but can also be caused by a search system that does not return the most useful matches or does not rank them highly enough in the results.</li></ol><p>High-quality embeddings are key to making RAG work because they reduce the impact of these limitations.</p><p>First, a small context size for an LLM means it&#x2019;s extra important to find the most relevant information, because you cannot add very much to the user&#x2019;s prompt. Second, how informative the answer is depends on how informative the input is. If the search results displayed to the LLM are irrelevant or poorly informative, that will be reflected in the result.</p><p>AI-generated embeddings are, on the whole, the best way to find and rank query results in general.</p><h2 id="build-a-full-rag-chatbot">Build a Full RAG Chatbot</h2><p>We will create and install a full RAG system using the <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex framework</a> for working with LLMs. This system uses Jina Embeddings to index document elements and store them in LlamaIndex&#x2019; built-in vector store and search engine. Then, it uses the newly released <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">Mixtral Instruct</a> model to construct natural language answers.</p><p>The approach in the article will also work with OpenAI&#x2019;s GPT models and Meta&#x2019;s Llama2, with some adaptation of the code and possibly the prompt. For more details, read the <a href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io">LlamaIndex documentation</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex &#x1F999; 0.9.19</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://docs.llamaindex.ai/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">LlamaIndex &#x1F999; 0.9.19</span></div></div></a></figure><p>This section involves a lot of code to copy and paste, and it will only get a very high-level explanation. You may prefer to <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download the accompanying notebook</a> or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run this code on Google Colab</a>.</p><h3 id="getting-started">Getting Started</h3><p>First, install LlamaIndex:</p><pre><code class="language-bash">pip install llama-index
</code></pre><p>Next, make sure that you have a <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina API key</a> and a <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace Inference API token</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hugging Face &#x2013; The AI community building the future.</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/v2-2.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>In Python, set up your secret key values like this:</p><pre><code class="language-Python">jinaai_api_key = &quot;&lt;your Jina Embeddings API key&gt;&quot;
hf_inference_api_key: str = &apos;&lt;your HuggingFace Inference API token&gt;&apos;
</code></pre><h3 id="connect-jina-embeddings">Connect Jina Embeddings</h3><p>LlamaIndex provides built-in support for the Jina Embeddings API. To use it, you only need to initialize the <code>JinaEmbedding</code> object with your API key and model name. For this example, we will use <code>jina-embeddings-v2-base-en</code>.</p><pre><code class="language-Python">from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jinaai_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
)
</code></pre><h3 id="connect-mixtral-llm">Connect Mixtral LLM</h3><p>We will also need to load the <code>Mixtral-8x7B-Instruct-v0.1</code> model. We will wrap it in a subclass of <code>llama_index.llms.CustomLLM</code> to make it compatible with LlamaIndex.</p><p>The important elements are the class parameters:</p><pre><code class="language-Python">model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
api_key: str = hf_inference_api_key
context_window: int = 4096
num_output: int = 512
</code></pre><p>The parameter <code>model_name</code> is the name of the model on HuggingFace, in this case, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code>, which is also the path part of the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">URL for its model card on HuggingFace</a>. For <code>api_key</code>, you need to use your HuggingFace Inference API token. Then, specify the input context size the model supports (<code>context_window</code>), in this case, 4096 tokens, and the maximum output size in tokens (<code>num_output</code>), 512.</p><p>The code below sets up the LLM object in the LlamaIndex framework:</p><pre><code class="language-Python">import requests
from llama_index.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.llms.base import llm_completion_callback
from typing import Any


class MixtralLLM(CustomLLM):
    context_window: int = 4096
    num_output: int = 512
    model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
    api_key: str = hf_inference_api_key

    @property
    def metadata(self) -&gt; LLMMetadata:
        &quot;&quot;&quot;Get LLM metadata.&quot;&quot;&quot;
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    def do_hf_call(self, prompt: str) -&gt; str:
        data = {
            &quot;inputs&quot;: prompt
        }

        response = requests.post(
            &apos;https://api-inference.huggingface.co/models/&apos; + self.model_name,
            headers={
                &apos;authorization&apos;: f&apos;Bearer {self.api_key}&apos;,
                &apos;content-type&apos;: &apos;application/json&apos;,
            },
            json=data,
            stream=True
        )
        if response.status_code != 200 or not response.json() or &apos;error&apos; in response.json():
            print(f&quot;Error: {response}&quot;)
            return &quot;Unable to answer for technical reasons.&quot;
        full_txt = response.json()[0][&apos;generated_text&apos;]
        offset = full_txt.find(&quot;---------------------&quot;)
        ss = full_txt[offset:]
        offset = ss.find(&quot;Answer:&quot;)
        return ss[offset+7:].strip()

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse:
        response = self.do_hf_call(prompt)
        return CompletionResponse(text=response)

    @llm_completion_callback()
    def stream_complete(
            self, prompt: str, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        response = &quot;&quot;
        for token in self.do_hf_call(prompt):
            response += token
            yield CompletionResponse(text=response, delta=token)


mixtral_llm = MixtralLLM()</code></pre><h3 id="prepare-a-text-for-rag">Prepare a Text for RAG</h3><p>Next, we will download a document and break it into pieces.</p><p>For this exercise, the text we&#x2019;ll use is <a href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><strong><em>Computers on the Farm</em></strong></a>, published by the US Department of Agriculture in 1982 and available via the Gutenberg Project. This 10,000-word booklet is full of useful information for the farmer considering buying a home computer for farm operations 40 years ago.</p><p>Naturally, its advice is perhaps less helpful today.</p><p>However, it serves as a good example because it is much longer than the input context size of Mixtral LLMs or Jina Embeddings v2.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--25-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="298" height="519"></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Computers on the Farm by Deborah Takiff Smith</div><div class="kg-bookmark-description">Free kindle book and epub digitized and proofread by volunteers.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.gutenberg.org/gutenberg/apple-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Project Gutenberg</span><span class="kg-bookmark-publisher">Smith, Deborah Takiff</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.gutenberg.org/cache/epub/59316/pg59316.cover.medium.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>The code below will strip the Gutenberg Project header and footer from the text, correct the MS DOS-style linebreaks to conventional ones, and split the text on the headers.</p><pre><code class="language-Python">import urllib.request
from typing import List
from llama_index.readers import StringIterableReader
from llama_index.schema import Document


def load_gutenberg(target_url: str) -&gt; List[Document]:
    ret: List[str] = []
    buff: str = &quot;&quot;
    reject: bool = True
    for raw_line in urllib.request.urlopen(target_url):
        line = raw_line.decode(&quot;utf-8&quot;)
        stripped_line = line.strip()
        if reject:
            if stripped_line.startswith(&quot;*** START OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = False
                continue
        else:
            if stripped_line.startswith(&quot;*** END OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = True
                continue
            if stripped_line:
                if stripped_line.startswith(&apos;=&apos;) and stripped_line.endswith(&apos;=&apos;):
                    ret.append(buff)
                    buff = &quot;&quot;
                    buff += stripped_line[1:len(stripped_line)-1] + &quot;\n\n&quot;
                else:
                    buff += line.replace(&apos;\r&apos;, &apos;&apos;)
    if buff.strip():
        ret.append(buff)
    return StringIterableReader().load_data(ret)

docs = load_gutenberg(&quot;https://www.gutenberg.org/cache/epub/59316/pg59316.txt&quot;)

# check that we loaded

assert len(docs) == 58</code></pre><p>The result is a collection of 58 small documents.</p><p>The code below does the following:</p><ol><li>Create a <code>ServiceContext</code> object that holds both the Mixtral LLM and the Jina Embeddings connection. We will use this here and later to create the full RAG system.</li><li>Get an embedding for each small document using the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a>.</li><li>Store the documents and embeddings in LlamaIndex&#x2019;s <a href="https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html?ref=jina-ai-gmbh.ghost.io">built-in in-memory vector store</a> <code>VectorStoreIndex</code>.</li></ol><pre><code class="language-Python">from llama_index import VectorStoreIndex, ServiceContext

service_context = ServiceContext.from_defaults(
    llm=mixtral_llm, embed_model=jina_embedding_model
)
index = VectorStoreIndex.from_documents(
    documents=docs, service_context=service_context
)
</code></pre><h3 id="prepare-a-prompt">Prepare a Prompt</h3><p>Next, we will create a custom prompt template. This prompt specifically asks the LLM not to use information outside of the context information retrieved from the vector database and to specifically say &#x201C;No information&#x201D; when the context does not have any information that answers the user&#x2019;s request.</p><pre><code class="language-Python">from llama_index import PromptTemplate

qa_prompt_tmpl = (
    &quot;Context information is below.\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;{context_str}\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;Given the context information and not prior knowledge, &quot;
    &quot;answer the query. Please be brief, concise, and complete.\\n&quot;
    &quot;If the context information does not contain an answer to the query, &quot;
    &quot;respond with \\&quot;No information\\&quot;.&quot;
    &quot;Query: {query_str}\\n&quot;
    &quot;Answer: &quot;
)
qa_prompt = PromptTemplate(qa_prompt_tmpl)
</code></pre><p>Then, we assemble the query engine using the prompt.</p><p>The key parameter to look at here is <code>similarity_top_k=2</code> in <code>VectorIndexRetriever</code>. This tells the RAG system to put only the best two search matches into the context sent to the LLM.</p><p>We can set this to a larger value if we&#x2019;re confident it will fit into the input context size of the LLM, so this factor is partly model-dependent and partly data-dependent.</p><pre><code class="language-Python">from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index import get_response_synthesizer

# configure retriever
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)

# configure response synthesizer
response_synthesizer = get_response_synthesizer(
    service_context=service_context,
    text_qa_template=qa_prompt,
    response_mode=&quot;compact&quot;,
)

# assemble query engine
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer,
)
</code></pre><h2 id="asking-the-rag-engine-questions">Asking the RAG Engine Questions</h2><p>Now you can ask questions and receive answers based on the text.</p><pre><code class="language-Python">result = query_engine.query(&quot;How is a computer useful on a farm?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">A computer can be useful on a farm by supplementing the calculator,
typewriter, and file cabinet. It can help with repetitive analyses, 
data storage, and management decisions. It can also send and receive 
written or graphic messages by telephone. Additionally, a computer 
program for a farm operation could make recordkeeping and analysis 
easier and improve management abilities. However, the improvements 
in efficiency and cost-effectiveness might be hard to measure in 
dollars.
</code></pre><p>You can ask questions that have an answer from the text that the LLM would never have produced on its own:</p><pre><code class="language-Python">result = query_engine.query(&quot;How much memory does a computer need?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">48K or 64K of memory is needed for most agricultural programs. The 
amount of memory needed depends on the software program and 
recordkeeping requirements.
</code></pre><p>And you can ask questions that have no answer in the text:</p><pre><code class="language-Python">result = query_engine.query(&quot;Who is buried in Grant&apos;s tomb?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">No information. The context information does not provide any details 
about Grant&apos;s tomb.
</code></pre><h2 id="checking-the-rag-retrieval">Checking the RAG Retrieval</h2><p>You may want to check to see what texts were retrieved for a specific query. For example:</p><pre><code class="language-Python">result = query_engine.query(&quot;What is the address of AgriData Resources?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code>205 West Highland Ave. Milwaukee, WI 53203
</code></pre><p>To check the retrieval phase, we have to use the retriever object we created above:</p><pre><code class="language-Python">retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)
</code></pre><p>You can rerun the retrieval and then inspect the documents:</p><pre><code class="language-Python">retrieved_texts = retriever.retrieve(&quot;What is the address of AgriData Resources?&quot;)
for i, rt in enumerate(retrieved_texts):
  print(f&quot;Text {i+1}:\\n\\n{rt.text}\\n\\n&quot;)
</code></pre><p>Result:</p><pre><code class="language-Text">Text 1:

3. AgriData Network

AgriData is a private information and computing network specializing in
agriculture. It offers immediate access to more than 10,000 pages of
continuously updated business, financial, marketing, weather, and price
information, as well as analyses and recommendations from its own and
other reporters, analysts, economists, meteorologists, and researchers.
It offers several different services, including an online computing
service that allows users to access a library of microcomputer software
programs that can be transferred to the user&apos;s microcomputer; an
agricultural production technology service offering data bases from 40
land-grant universities and from agricultural, chemical, fertilizer,
equipment, seed, and feed companies; an &quot;electronic yellow pages,&quot; or
product service directory for farmers; and electronic mail.
  ADDRESS: AgriData Resources, Inc.
           205 West Highland Ave.
           Milwaukee, WI 53203

Text 2:

2. AGRICOIA

AGRICOIA is an online information service produced by the National
Agricultural Library (NAD of USDA), and is available commercially from
a number of sources (including DIALOG and Bibliographic Retrieval
Services). It provides comprehensive access to information on published
literature pertaining to agriculture.
AGRICOIA is the catalog and index for NAL and covers materials
published since 1970. It includes about 1.5 million citations.
AGRICOIA contains citations to worldwide published books, serial
titles, and journal articles on agriculture and related subjects. In
addition to bibliographic citations of published literature, the system
offers information through several specialized subfiles; these subfiles
include brucellosis (BRU), environmental impact statements covering
1977 and 1978 (ENV), and the Food and Nutrition Information Center,
which emphasizes human nutrition research and education and food
technology (FNC).
Librarians are the main users of this system.
  ADDRESS: To find out more about AGRICOIA, contact:
           Educational Resources Staff
           National Agricultural Library
           Room 1402
           Beltsville, MD 20705
</code></pre><h2 id="making-rag-work-for-you">Making RAG Work For You</h2><p>With Jina Embeddings, LlamaIndex, and Mixtral LLM, you can make your own RAG system that can answer questions about long documents, respond to requests based on a manual or FAQ, or just behave in funny or useful ways, based on a large document context.</p><p>And you can do all this without complex AI operations engineering or even training your models.</p><p>Jina AI is committed to helping you make the most of emerging AI technology. In our coming articles, we delve deep into the practical issues of using embedding models, like discussing ways of chucking documents for use in RAG and search applications. We will also have more integration tutorials and practical advice for text pre-processing and data curation.</p><p>Learn more from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings</a> and <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a> websites, or reach out to us at <a href="mailto:contact@jina.ai">contact@jina.ai</a> to discuss how Jina AI&#x2019;s experience can help your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4012 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain]]></title><description><![CDATA[See how companies are integrating SceneXplain with their existing infrastructure to power their product descriptions and storytelling]]></description><link>https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/</link><guid isPermaLink="false">658198e20bab3100012d2999</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Lisa Li]]></dc:creator><pubDate>Thu, 21 Dec 2023 15:00:26 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain"><p>We recently wrote about SceneXplain&apos;s new <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">JSON Schema Store</a>, which lets you use predefined JSON schemas to extract information from images in a structured format. In this post, we&apos;re going to see how that&apos;s used by our partner, <a href="http://www.akia.cn/eindex.asp?ref=jina-ai-gmbh.ghost.io">AKIA Carpet &amp; Rugs</a>, for building up an AI-powered product stories generation bot to empower sales.</p><p>In our prior examples, we&apos;ve primarily used either cURL or Python to access <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a>. But in this post, we&apos;re switching things up a bit by using JavaScript.</p><h2 id="about-akia-carpet-rugs">About AKIA Carpet &amp; Rugs</h2><p>In July 2008, Gary Chen founded AKIA Carpet &amp; Rugs, which is now marking its 15th year of operation. The team, initially inspired by traditional Asian craftsmanship, has been dedicated to developing contemporary aesthetic styles. AKIA has grown into a brand known for its unique fusion of modern art with traditional design and weaving techniques.</p><p>As a carpet manufacturer, AKIA specializes in a range of products including mid-to-high-end decorative carpets, tapestries, and carpets for specific projects. The company integrates design, research, production, and sales, both domestically and internationally. Known for innovative design and high-quality products, AKIA has earned a solid reputation in China&apos;s high-end carpet market.</p><h2 id="crafting-a-winning-product-story-with-scenexplain"><strong>Crafting a Winning Product Story with SceneXplain</strong></h2><p>AKIA Carpet &amp; Rugs primarily caters to the mid-to-high-end market, focusing on the aesthetic appeal of their products. Their clientele often looks for carpets that not only complement their home design but also express their unique taste. Recognizing this, AKIA collaborates with skilled designers to create a diverse range of styles, resulting in an extensive collection of carpet designs and images. The challenge lies in effectively communicating the artistic value of these designs to discerning customers, a crucial factor in attracting clients.</p><p>Previously, crafting compelling narratives for a large array of product images, akin to artworks, was a daunting task. This required copywriters who were not only skilled in writing but also knowledgeable in design and art. Additionally, the need to swiftly identify the perfect product image from an extensive collection to meet specific customer preferences was a significant challenge. Traditional image labeling methods, focusing on basic attributes like color, shape, and material, proved insufficient for customers who often describe their needs in more abstract terms.</p><p>SceneXplain offers a dual solution to these challenges. Its approach is based on narration rather than mere description, aiming to weave engaging stories around images. This aligns with SceneXplain&apos;s core philosophy: storytelling brings images to life. By providing stories that resonate with the artistic nature of AKIA&apos;s products, SceneXplain addresses their need for an intuitive, aesthetically aligned way of presenting their carpets.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1183" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Embrace Yourself&quot; is a piece of art in the form of a carpet, exuding minimalist charm and contemporary simplicity. This piece features the elegant, abstract depiction of a round, white figure at its center&#x2014;an embodiment of purity and serenity. With meticulously crafted black outlines that grace the soothing white background, this carpet tells a subtle but impactful visual story. Each line is placed with thoughtful precision, evoking emotions and depth without cluttering the visual space. The characteristic round head of the figure in the design adds a touch of futuristic whimsy, creating a space for imagination to soar. The light gray tones and stark white spaces between the lines further enhance the abstract quality, offering a calm and peaceful atmosphere to any room. Whether adorning a modern living area or a chic office space, &quot;Embrace Yourself&quot; promises to be more than just a carpet&#x2014;it is a promise of self-discovery and a celebration of space and form. Its simplicity and abstraction are not just visually appealing but are crafted to engage the observer in an almost meditative contemplation.</blockquote><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1189" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Dancer&quot; is an evocative piece of home decor where color and geometry resonate to a rhythm of their own. Adorning a rich navy blue background, this carpet features a dynamic composition of blue and orange lines and squares that glide across the fabric, suggesting movement and energy, much like a dancer in the spotlight. The striking orange lines dance diagonally, connecting a series of crisp white squares, which are themselves accented with black detailing, reminiscent of precise footwork on a dance stage. Unequivocally modern, &quot;Dancer&quot; employs the contrast of deep blues with vibrant oranges and whites to create an abstract visual narrative that is open to interpretation, yet commands attention. Each element on &quot;Dancer&quot; is painstakingly arranged to bring a sense of balance and fluidity, paralleling a choreographed performance that tells a story with every twist and turn. The pattern encourages the eye to leap and land much like a viewer watching an enthralling solo dance performance, making &quot;Dancer&quot; not just a carpet, but a conversation piece that captivates and inspires.</blockquote><h2 id="product-story-generation-a-holistic-solution">Product story generation: A holistic solution</h2><p>In AKIA&#x2019;s use case, their product manager wants to automatically create stories for all their products. The stories should use some high-quality examples as a guide, which contain aesthetic explanations. Their specific needs are:</p><ul><li>Textual descriptions of their products</li><li>Stories for each product that follow the examples they provide</li><li>Batch processing of images triggered by the chat channel</li></ul><p>Based on these requirements, visual question answering (VQA) is the best fit, because:</p><ul><li>Visual question answering outputs textual descriptions.</li><li>In the question, you can also provide a prompt in a specific format on demand.</li><li>You can inject your own examples into the prompt to guide the model&#x2019;s output.</li><li>Once you have the prompt&#x2019;s basic structure, you can convert it to a template with variables that can be automatically populated each time you use it.</li></ul><p>SceneXplain&#x2019;s API provides a wide range of options for configuring your request, including image captioning, alt-text generation, visual question answering, JSON output, and more.</p><p>Several fields are required to execute a VQA task via the API:</p><ul><li>API endpoint: <code>https://api.scenex.jina.ai/v1/describe</code></li><li>API key: <code>&apos;x-api-key&apos;: token ${YOUR_API_KEY}</code>. You can generate and manage your API key on our <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">API page</a>.</li><li>Request payload, which is your task configuration, providing the image you want to process, setting <code>question_answer</code> in the <code>features</code> property, and setting your prompt in the <code>question</code> property.</li></ul><p>Here&#x2019;s a code snippet for such an API call in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
			&quot;image&quot;: &quot;The image you want to process, it can be a base64 string or a URL&quot;,
      &quot;features&quot;: [
        &quot;question_answer&quot;
      ],
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
		&quot;question&quot;: &quot;your prompt&quot;
    }
  ]
};

const YOUR_API_KEY = &apos;your_generated_API_key_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_API_KEY}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
})
.then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>The payload&#x2019;s <code>data</code> property is an array that can have several configurations, meaning you can batch-process your images via the API.</p><h2 id="connecting-akia-to-scenexplain%E2%80%99s-api-via-bot">Connecting AKIA to SceneXplain&#x2019;s API via bot</h2><p>AKIA uses <a href="https://www.larksuite.com/en_eu?ref=jina-ai-gmbh.ghost.io">Lark</a> for their internal messaging, which is a Chinese application similar to Slack, Microsoft Teams, and Discord. An employee of AKIA can simply send a message to their SceneXplain chatbot that includes an image and a topic.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1821" height="1140" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/21700476455_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/21700476455_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/21700476455_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg 1821w" sizes="(min-width: 720px) 720px"></figure><p>The chatbot sends back a detailed description of the carpet. Here&#x2019;s how it would look in English:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="847" height="291" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/SceneX-carpet--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png 847w" sizes="(min-width: 720px) 720px"></figure><p>Behind the scenes, there&#x2019;s a middleware service that connects Lark to the SceneXplain API:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1000" height="221" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/workflow.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png 1000w"></figure><p>It shuttles the data between the two services and performs several key tasks:</p><ul><li>Message validation</li><li>API payload generation</li><li>API calling</li><li>Message formatting</li></ul><p>The process is:</p><ol><li>Receive image and topic in message from Lark chatbot</li><li>Check message format is valid. If not, return an error.</li><li>Base64-encode the image and wrap both it and the topic into a payload, using the topic as the question in visual question answering (VQA)</li><li>Send the payload to the API</li><li>API generates a description and sends that back</li><li>Format the message to fit Lark&#x2019;s API</li><li>Send the message back to the Lark chatbot</li></ol><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We&#x2019;re not going to go into the workings of the Lark API here. We want to keep this post as service-agnostic as possible, so it&#x2019;s relevant to whatever service you want to integrate with SceneXplain.</div></div><p>We&#x2019;re just going to focus on the middleware (&#x201C;Your service&#x201D; in the diagram above). All you need is a few lines of code to reformulate the request, pass it on, and then do the same for the response.</p><pre><code class="language-jsx">// Function to call SceneXplain `/describe` API
const describe = async (image: string, name: string, topic: string) =&gt; {
  // prepare payload
  const newBody = {
    data: [
      {
        image: image,
        features: [
          &quot;question_answer&quot;
        ],
        languages: [&apos;zh-CN&apos;],
        algorithm: &apos;Jelly&apos;,
        question: `your prompt, incorporating ${name} and ${topic}, plus optional example for desired output format for in-context learning`
      }
    ]
  }

  // call SceneXplain API
  try {
    const resp = await fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
      headers: {
        &apos;x-api-key&apos;: `token ${process.env.scenexKey}`,
        &apos;content-type&apos;: &apos;application/json&apos;
      },
      body: JSON.stringify(newBody),
      method: &apos;POST&apos;,
    });
    if (!resp.ok) {
      const error = await resp.text();
      throw error;
    }
    const data = await resp.json() as any;
    console.log(`describe result: ${JSON.stringify(data, null, 2)}`);
    if (data.code !== 200) throw data;
    const result = data.result[0];

    // get result in the required language
    return result.i18n[&apos;zh-CN&apos;];
  } catch (e) {
    console.log(`describe error: ${JSON.stringify(e, null, 2)}`);
    return &apos;&apos;;
  }
}
</code></pre><p>As you can see from the <code>question</code> field in the example payload above, you can include some example output to help the algorithm in generating the kind of description you desire. And, of course, you don&#x2019;t <em>have</em> to use JavaScript to build your middleware service - any programming language with an HTTP library can access <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API</a>.</p><h2 id="wrapping-up">Wrapping up</h2><p>Do you want to follow in AKIA&#x2019;s footsteps and use SceneXplain to build vivid product stories from your images and videos? Head over to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to get started. Or for business use cases, fill in our <a href="https://jina.ai/contact-sales/?ref=jina-ai-gmbh.ghost.io">sales form</a> and we&#x2019;ll be happy to roll out the red carpet.</p>]]></content:encoded></item><item><title><![CDATA[Multi-Agent Simulations in PromptPerfect: 𝑛 Heads Are Better Than One]]></title><description><![CDATA[Discover the real-world impact of multi-agent simulations and see practical examples of systems uniting individual strengths to tackle complex tasks, offering efficient and tailored solutions across various domains]]></description><link>https://jina.ai/news/multi-agent-simulations-in-promptperfect-n-heads-are-better-than-one/</link><guid isPermaLink="false">658177280bab3100012d296d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 19 Dec 2023 15:00:58 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><p>In this post, we&#x2019;re going to explore how multi-agent simulations are not just a conceptual framework but a tool with tangible applications in various domains. By delving into practical examples, we can see how these systems bring together the strengths of individual agents to address complex tasks and deliver solutions that are more efficient, comprehensive, and tailored to specific needs. These examples will illustrate the versatility and effectiveness of multi-agent simulations in real-world scenarios, demonstrating their value in diverse fields.</p><h2 id="a-quick-history-lesson">A quick history lesson</h2><p>In the ever-evolving landscape of technology and problem-solving, our journey has been marked by significant milestones. Initially, coding was our primary tool for instructing computers to perform tasks and solve problems. This era was defined by programming languages and algorithms, where solutions were hard-coded. As technology advances, we&#x2019;re transitioning to a more intuitive approach: prompting. This phase leverages artificial intelligence, where we can simply ask a system a question or present a problem, and it generates solutions based on its training and algorithms. This method is more flexible and user-friendly, catering to a wider range of problems without the need for extensive coding knowledge. However, even now, we&apos;re seeing the glimmerings of a newer era that promises to revolutionize how we approach problem-solving: the use of agents.</p><h2 id="multi-agent-simulations-as-easy-as-1-2-3">Multi-agent simulations: As easy as 1, 2, 3</h2><p>Before we dive into using multiple agents, let&#x2019;s define a few key terms: <em>Agent</em>, <em>action,</em> <em>environment</em>, and <em>simulation</em>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="861" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Multi-agent-sim.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png 861w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Agent:</strong> An <em>agent</em>, in the realm of artificial intelligence, performs actions to achieve a goal, like planning a vacation or collecting state-of-the-art research information online.</li><li><strong>Action:</strong> An <em>action</em> is a state change in an environment initiated by an agent. This may include talking to other agents, searching Google, or accessing external APIs.</li><li><strong>Environment:</strong> An <em>environment</em> is a text description that constrains what the agent can do. In a travel planning simulation, this may be a (virtual) travel agent office.</li><li><strong>Simulation:</strong> A <em>simulation</em> is what you get when you put all of the above together: An agent (or agents) performing actions in a given environment.</li></ul><h2 id="diving-deeper-into-agents">Diving deeper into agents</h2><p>AI agents can range from simple, rule-based systems to <em>complex entities capable of learning</em>. They can analyze data, learn from experiences, and make predictions or recommendations.</p><p>Agents share several characteristics:</p><ul><li><strong>Autonomy</strong>: Agents operate without direct human intervention, making decisions based on their programming and the data they encounter.</li><li><strong>Reactivity</strong>: They perceive their environment and respond to changes in real time, adapting their actions as necessary.</li><li><strong>Proactivity</strong>: Beyond just reacting, agents can take initiative, anticipating future states and planning actions accordingly.</li><li><strong>Social ability</strong>: Agents can communicate and collaborate with other agents or humans to achieve complex goals.</li></ul><p>However, despite these capabilities, single AI agents face intrinsic limitations:</p><ul><li><strong>Specialization limit</strong>: Like human experts, each agent typically specializes in a particular domain or task, lacking the breadth of knowledge to handle unrelated challenges.</li><li><strong>Complex problem solving</strong>: Tackling multi-dimensional problems that require diverse expertise and perspectives is beyond the scope of a single agent.</li><li><strong>Limited adaptability</strong>: While adaptable within their domain, agents may struggle with novel situations that fall outside their programmed parameters or training data.</li></ul><p>In summary, while individual AI agents represent a significant technological advancement, their true potential is unlocked when they operate as part of a multi-agent system, combining their strengths and compensating for each other&apos;s limitations.</p><h2 id="multiple-agents-n-heads-are-better-than-one">Multiple agents: <em>n</em> heads are better than one</h2><p>The solution to those limitations lies in multi-agent simulation. This approach involves using multiple AI agents, each with its specialty or focus area, working in tandem to solve complex problems. By collaborating, these agents can cover a broader range of issues and provide more comprehensive solutions than a single agent could. <a href="https://arxiv.org/pdf/2307.05300.pdf?ref=jina-ai-gmbh.ghost.io">Current research</a> shows that the performance of agents in specific tasks depends on the persona of the agent.</p><h2 id="building-a-vacation-planner-with-promptperfect">Building a vacation planner with PromptPerfect</h2><p>Let&#x2019;s consider planning a vacation as a practical example of a multi-agent simulation. This example will use two agents:</p><ul><li>The Travel Planner finds the best travel routes.</li><li>The Accommodation Finder finds affordable accommodation.</li></ul><p>Together, they provide a complete vacation plan tailored to your preferences and budget.</p><p>Follow the steps below to create a vacation planner in PromptPerfect:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_Simulation_for_Vacation_Planning__NbBdmWiUS5SC_QZ00lc21Q" width="640" height="640" allowfullscreen frameborder="0">

</iframe>
<!--kg-card-end: html-->
<p>We can now check the output to see the results. Here&#x2019;s one example action taken by the Travel Planner agent:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="1588" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--47--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--47--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png 1588w" sizes="(min-width: 720px) 720px"></figure><p>As you can see, the Travel Planner agent calls the flight API endpoint to find flights for a one-week trip in July. Since the agent is aware of the flight API&#x2019;s documentation, it sends the correct parameters with the request and retrieves the flight data.</p><p>Once it&#x2019;s done that, it can talk to the Accommodation Finder agent to find suitable properties in the chosen city. The Accommodation Finder, in turn, calls a hotel booking API to (you guessed it) arrange the accommodation.</p><h2 id="further-applications">Further applications</h2><p>This simple outline, although given a relatively small task, also applies to far more complex problems:</p><ul><li><strong>Paper Research</strong>: In academic or professional research, one agent could specialize in identifying relevant papers, another in summarizing content, and a third in identifying key themes and gaps in the research.</li><li><strong>Coding</strong>: For software development, one agent could get the user requirements, another write the code, and a third make the deployment.</li><li><strong>Online Shopping Deal Finder</strong>: This could involve one agent tracking price changes, another comparing product features, and a third assessing user reviews to find the best deals.</li></ul><p>As we&apos;ve seen, multi-agent simulations open up a world of possibilities that transcend the limitations of individual AI agents. Whether it&apos;s planning your dream vacation, conducting thorough research, developing software, or finding the best online shopping deals, these simulations offer tailored, efficient, and comprehensive solutions.</p><p>But the real magic happens when you dive in and experience it yourself. By visiting the PromptPerfect website, you&apos;ll have the opportunity to create and manage your own multi-agent simulations.</p><h2 id="get-started-with-agents">Get started with Agents</h2><p>So, why wait? Head over to the <a href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">PromptPerfect website</a> now, and start crafting your own simulations. Whether for work, study, or personal projects, discover the power and flexibility of multi-agent AI at your fingertips. Your journey into the future of problem-solving starts today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[A Tale of Two Worlds: EMNLP 2023 at Sentosa]]></title><description><![CDATA[Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas.]]></description><link>https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa/</link><guid isPermaLink="false">657c3d130bab3100012d2878</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Sat, 16 Dec 2023 07:03:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><p>The sun blazed down on the glistening sidewalks of Sentosa, a symphony of laughter and chatter filling the air. Tourists, decked in their holiday best, meandered through the vibrantmaze of Universal Studios, their faces alight with the joy of a day out in this fantasy land. The click of cameras capturing moments against the backdrop of thrilling rides and colorful parades was omnipresent. Nearby, the enticing aromas of international cuisines wafted through the air, luring guests to indulge in a culinary adventure.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1440" height="1080" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2911702641285_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2911702641285_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg 1440w" sizes="(min-width: 720px) 720px"></figure><p>In stark contrast, just a stone&apos;s throw away, nestled in the heart of this revelry, was the Resorts World Convention Centre. Here, the atmosphere was charged with a different kind of excitement. The halls buzzed not with the sound of holiday-making, but with the fervor of intellectual discourse. Young researchers and seasoned academics, their conference badges swaying gently with each step, engaged in animated discussions about the latest in NLP.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-3.png" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As I walked into this place, the difference was clear. On one side, there was the loud and happy noise of a holiday resort, full of life and excitement. On the other side, there was a serious and busy atmosphere where people talked about NLP , LLMs, ChatGPT, prompting, and Google&apos;s new Gemini model. It was like a movie scene &#x2013; where learning and fun came together in a surprising way.</p><p>In this blog, I&apos;ll share some observations of EMNLP 2023 and some of the most interesting papers and posters we discovered at the conference.</p><h2 id="emnlp-2022-to-2023-shifts">EMNLP: 2022 to 2023 Shifts</h2><p>Attending EMNLP 2022 in Abu Dhabi and now looking back at EMNLP 2023, I&apos;ve observed significant shifts in research focus and conference dynamics. These changes, driven by the swift advancements in AI, paint a vivid picture of our adaptive and forward-looking community. Below, I share a comparison of the two conferences, highlighting how our priorities and discussions have transformed in just a year.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>EMNLP</th>
<th>2022</th>
<th>2023</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Research Focus</td>
<td>Diverse range of NLP methods, with emphasis on traditional approaches.</td>
<td>Strong focus on Large Language Models (LLMs) and prompting techniques.</td>
</tr>
<tr>
<td>Research Trends</td>
<td>Interest in a wide array of topics, but no standout groundbreaking papers.</td>
<td>Shift towards LLM interpretability, ethics, agents, and multimodal reasoning.</td>
</tr>
<tr>
<td>Conference Atmosphere</td>
<td>A bit peculiar and pessimistic due to the release of ChatGPT and its implications on traditional NLP methods.</td>
<td>More confidence and adaptability among researchers in embracing new trends.</td>
</tr>
<tr>
<td>Research Diversity</td>
<td>Still exploring traditional methods like topic modeling, n-grams smoothing, and Bayesian methods (as seen in COLING 2022).</td>
<td>Rapid adaptation to newer approaches, moving away from older methods.</td>
</tr>
<tr>
<td>Relevance of Presented Work</td>
<td>Consistent with contemporary research trends at the time.</td>
<td>Fast-paced AI development made some empirical methods and results feel outdated by the time of the conference.</td>
</tr>
<tr>
<td>Conference Engagement</td>
<td>Enjoyment derived more from personal conversations and interactions than from paper presentations.</td>
<td>Increased focus on personal communication, with more time spent at poster sessions than listening to oral presentations.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="paper-highlights-from-emnlp-2023">Paper Highlights from EMNLP 2023</h2><p>At EMNLP 2023, several intriguing papers caught my attention, each addressing different aspects of NLP and pushing the boundaries of what&apos;s possible in this field. Let me share some of the highlights from these papers and my thoughts on them.</p><h3 id="hybrid-inverted-index-is-a-robust-accelerator-for-dense-retrieval">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.05521?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</div><div class="kg-bookmark-description">Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI$^2$ to achieve lossless retrieval quality with competitive efficiency across various index settings. Our code and checkpoint are publicly available at https://github.com/namespace-Pt/Adon/tree/HI2.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Peitian Zhang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Text embeddings have become very popular for information retrieval tasks. However, performing an exact embedding vector search requires one to calculate similarities between the embedding representation of the query and the embedding of each document. This becomes very slow for large datasets and leads to latencies that are not acceptable for real-world search applications. Therefore, many applications use approximated nearest neighbor search techniques to speed up the search system, whereby many of these techniques rely on vector quantization algorithms that learn an index of clusters based on the data distribution.&#xA0;</p><p>In addition, <strong>hybrid search</strong> has become popular which combines embedding-based search with traditional BM25-based search techniques. Usually, BM25 and embedding search are performed completely independently in hybrid search settings and only the result sets are combined.&#xA0;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/hype-and-hybrids-multimodal-search-means-more-than-keywords-and-vectors-2/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hype and Hybrids: Search is more than Keywords and Vectors</div><div class="kg-bookmark-description">Twenty years ago, &#x201C;hybrid&#x201D; was a term used only by botanists and chemists. Today, hybrid is booming&#x2026; even in search. Many search systems are rolling out hybrid search schemes with the latest AI. But is &#x201C;hybrid search&#x201D; really more than a buzzword?</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/Jina-AI-Website-Banners-Templates--21-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper proposed a method to train a joined index of two parts: a cluster selector and a term selector. The cluster selector performs a vector quantization to assign texts into buckets of nearby clusters and the term selector determines the most representative terms of a document BM25 can be used to assign it into buckets associated with those terms, however, this is not trainable and can therefore not adjust to the training data. Alternatively, one can determine the most representative terms in a document with a BERT model with an MLP hat which is applied on each token to determine a score. In this way the term selector becomes trainable. Then the cluster centroids and the BERT model are trained together by using the KL divergence loss with the embedding model as a teacher to obtain a distribution of similarity values. The results in the paper show that this method can retrieve more relevant documents in the same amount of time as standard ANN techniques like HNSW and IVF-PQ implementations.</p><h3 id="is-chatgpt-good-at-search-investigating-large-language-models-as-re-ranking-agents">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2304.09542?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</div><div class="kg-bookmark-description">Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model&#x2019;s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Weiwei Sun</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper investigates techniques to utilize LLMs for re-ranking documents. Re-ranking is usually performed in a search system after a first retrieval step to re-order the retrieved documents, e.g., to select the most relevant ones among them. Commonly used models are finetuned transformer models which are called <strong>cross encoders</strong>. Those receive as input a pair composed of a query and a document candidate and return a relevance score. Besides, more traditional learning-to-rank models like LambdaMart are also popular, especially in cases where the ranking is not only done based on semantic relevance itself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Cross-Encoders &#x2014; Sentence-Transformers documentation</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.sbert.net/_static/favicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.sbert.net/_static/logo.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Observing the strong NLP capabilities of LLMs, the authors of this paper wanted to investigate whether Models like GPT4 can be used to rank documents better. However, the limitation of those closed API-based models is usually that probability outputs are not accessible. Accordingly, the paper investigates techniques that rely only on prompting and the generated output text for re-ranking. The technique they propose inserts the documents together with an ID in the prompt and instructs the LLM to output a sequence of IDs with respect to the relevancy of the documents. In cases where the whole set of documents is too long to fit into the prompt, they apply a sliding window approach, where re-ranking is first performed on the documents with the lowest retrieval score obtained from the first-stage retriever. Then the most relevant documents according to the output are presented together with the documents in the next window of retrieval candidates and so on.</p><p>Since GPT-4 is too expensive and too slow for using it in a real-world setting, the authors propose distilling its ranking capabilities into a transformer-based cross-encoder model. The results show that even a comparably small model (440M parameters) distilled with this technique can outperform much larger state-of-the-art re-ranking models.</p><h3 id="large-language-models-can-self-improve">Large Language Models Can Self-Improve</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.11610?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Large Language Models Can Self-Improve</div><div class="kg-bookmark-description">Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate &#x201C;high-confidence&#x201D; rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and 63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Jiaxin Huang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/qRfOSz3zHzg1gctMzgRjYRjeTyg-1pMyv7JIZZFJlTyKOQPoHJYfuB0u_eGC8wVnCKvN471-8D-avjk7-0XBGfAAFL7q0jI_-7eMGnXvCpEK8L8Pe3VSu5-iV5QHpfnY-5YSUQqvgT-gUs2sOUoQw0w" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="236"></figure><p>For many tasks, LLMs achieve already good results in zero-shot settings. However, to improve their performance on specific tasks (beyond zero-shot) common techniques still utilize a lot of training data to fine-tune them. To reduce the amount of training data needed, this paper presents a technique that uses the LLM to &#x201C;self-improve&#x201D;. The main idea behind this approach is to augment existing training datasets by using data generated with the LLM itself.</p><p>This is achieved by using a dataset with only questions and without answers. First, a chain of thoughts (CoT) method is used to generate for each question a couple of different reasoning paths and answers by using a temperature greater than zero to make the generative text non-deterministic. By determining the answer with the highest frequency the probability of the answer being correct can be increased. The paper also shows that LLMs can be used to effectively estimate the confidence of an answer by investigating the consistency of the answer. If a high percentage of the answers is equal, this answer also has a high probability being correct. These most frequent answers and the corresponding reasoning paths can then be used to construct additional prompts for constructing additional training data. However, the authors propose not only to use those reasoning paths and answers but also to add examples in different formats, e.g., presenting the question without any reasoning path and using the question together with some generic instruction like &#x201C;Let&#x2019;s think step by step&#x201D;. Finally, this enhanced training dataset can be used to fine-tune the LLM on the specific task. In their evaluation the authors show this technique is effective to fine-tune LLMs with minimal training data but also that it generalizes well, as it improves the result on out-of-domain datasets.</p><h3 id="adapting-language-models-to-compress-contexts">Adapting Language Models to Compress Contexts</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2305.14788?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Adapting Language Models to Compress Contexts</div><div class="kg-bookmark-description">Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Alexis Chevalier</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/RDaQqjnEa45QEArhdPfuNKXsZReEQfT7Hg_zgr-DlkmlMeeXJAYP0kiKfpQ3Vj8iL_I2mK5JZ0evaoHZAdBDHSaep9Z49Qx1dt5JdCcMrfH8UF-nRy_ZO6GT7wLwR654p0_9CF4jD7soGg-zBXbSFNg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="496"></figure><p>Language models are usually constrained by limited context length. While there are various techniques like <strong>AliBi</strong> to construct language models that can handle larger context lengths, those techniques do not help in cases where you want to use an existing model that has only a very limited context length. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</div><div class="kg-bookmark-description">Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi&#x2019;s inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Ofir Press</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Instead, this paper addresses this problem by introducing a new method that allows one to fine-tune an exciting language model to apply it to longer context lengths. To achieve this, the model&apos;s vocabulary is extended by a fixed number of summary tokens, which are supposed to tell the model to summarize the content of the other tokens in the respective embeddings. During inference, the text input is segmented into shorter text sequences which are extended with the summary tokens and prepended with the output summary vectors of the previously processed segments. To fine-tune the model to handle long sequences, the model is trained on a next-token prediction task. Thereby, the model should make use of the information encoded in the summary embeddings of the previous segments. Notably, the segment length is varied during the training and the backpropagation is done for one document as a whole. In other words, the model is first applied to all sequences in the afore-described scheme before starting the backpropagation. The technique is evaluated by the authors on OPT models of various sizes and the 7-B-Llama-2 model. Besides standard language modeling tasks, the authors also show that the models can be effectively used to solve in-context-learning classification tasks with longer prompts or used for re-ranking. Here the re-ranking follows a language modeling approach in which the passages are re-ranked based on the language model&#x2019;s likelihood to generate the question from the given passage.&#xA0; </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.emnlp-main.249/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Improving Passage Retrieval with Zero-Shot Question Generation</div><div class="kg-bookmark-description">Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, Luke Zettlemoyer. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aclanthology.org/thumb/2022.emnlp-main.249.jpg" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><h2 id="poster-highlights-from-emnlp-2023">Poster Highlights from EMNLP 2023</h2><p>At EMNLP 2023, alongside the compelling paper presentations, the poster sessions were a hub of vibrant discussion and exchange. Here&apos;s a rundown of some standout posters I came across, each offering a unique glimpse into the ongoing research and development within the field of NLP.</p><h3 id="can-retriever-augmented-language-models-reason"><strong>Can Retriever-Augmented Language Models Reason?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2931702707518_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2931702707518_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A poster from McGill University examined whether Retriever-Augmented Language Models (RALMs) can effectively reason by balancing the capabilities of both the retriever and the language model. The research highlighted the potential shortcomings of retrievers in sourcing all necessary statements for reasoning, and how language models might falter in reasoning even when provided with the required statements. It was a deep dive into improving the interactive components of language models.</p><h3 id="contrastive-learning-based-sentence-encoders"><strong>Contrastive Learning-based Sentence Encoders</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2941702707520_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2941702707520_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Researchers from Tohoku University presented findings on how Contrastive Learning (CL) can induce sentence encoders to implicitly weight informative words, enhancing the model&apos;s understanding and processing of language. This approach could refine the way sentence encoders prioritize and process key elements in text, making them more efficient and effective.</p><h3 id="investigating-semantic-subspaces-of-transformer-sentence-embeddings"><strong>Investigating Semantic Subspaces of Transformer Sentence Embeddings</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2951702707522_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2951702707522_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A team from the University of Stuttgart showcased their work on probing the semantic subspaces of transformer sentence embeddings. By employing linear structural probing, they aimed to understand how different layers of a transformer contribute to semantic content processing, offering insights into the inner workings of sentence embeddings.</p><h3 id="can-pre-trained-vision-and-language-model-answer-visual-information-seeking-questions"><strong>Can Pre-trained Vision and Language Model Answer Visual Information-Seeking Questions?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2961702707523_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2961702707523_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>An intriguing poster by researchers from the Georgia Institute of Technology, Google Research, and Google DeepMind introduced a benchmark for testing the world knowledge in multimodal Large Language Models (LLMs) through Visual Information-Seeking Questions. The research focused on the capabilities of retrieval-augmented models and GPT-4 in answering questions that require visual understanding, pushing the envelope on multimodal AI.</p><h3 id="to-split-or-not-to-split-composing-compounds-in-contextual-vector-spaces"><strong>To Split or Not to Split: Composing Compounds in Contextual Vector Spaces</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2971702707525_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2971702707525_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>From the University of Stuttgart, a study delved into whether splitting compounds in contextual vector spaces is beneficial for the model&apos;s performance. The research explored the impact of compounds on semantic representation and processing, contributing to our understanding of compositional semantics in language models.</p><h3 id="subspace-chronicles-how-linguistic-information-emerges-shifts-and-interacts-during-language-model-training"><strong>Subspace Chronicles: How Linguistic Information Emerges, Shifts, and Interacts during Language Model Training</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2981702707526_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2981702707526_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>This poster detailed an exploration into the dynamics of linguistic information as it emerges and evolves during the training of language models. It&apos;s a fascinating look at the underpinnings of language model training and the critical learning phases that define their capabilities.</p><h3 id="theory-of-mind-for-multi-agent-collaboration-via-large-language-models"><strong>Theory of Mind for Multi-Agent Collaboration via Large Language Models</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2991702707528_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2991702707528_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Lastly, a poster outlined research on the Theory of Mind in Large Language Models and their application in multi-agent collaboration tasks. It&apos;s an exciting foray into the cognitive capabilities of LMs and their potential in collaborative environments.</p><h2 id="embeddings-roundtable-a-birds-of-a-feather-at-emnlp-2023">Embeddings Roundtable: A Birds of a Feather at EMNLP 2023</h2><p>During EMNLP 2023, we hosted a Birds of a Feather (BoF) session on embeddings that turned into a rich tapestry of insights and discussions. With a crowd of over 80 attendees, the session was an electrifying blend of sharp minds and cutting-edge topics.</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691.mp4" poster="https://img.spacergif.org/v1/1280x720/0a/spacer.png" width="1280" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h3 id="lightning-talks-and-panel-discussion">Lightning Talks and Panel Discussion</h3><p>The BoF session featured lightning talks by renowned researchers like Huiqiang, Hassan, Hwiyeol, Mattia, and Yang Chen. Each speaker brought a unique perspective to the table, sharing their latest findings in embedding research within NLP. The talks sparked an energizing dialogue that transitioned into a thought-provoking panel discussion.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3041702708673_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3041702708673_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3041702708673_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3031702708669_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3031702708669_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3031702708669_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3061702708681_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3061702708681_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3061702708681_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3051702708676_.pic-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3051702708676_.pic-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3051702708676_.pic-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3081702708695_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3081702708695_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3081702708695_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div></div></figure><p>The panel, graced by Sebastian Ruder, Nicola Cancedda, Chia Ying Lee, Michael G&#xFC;nther, and Han Xiao, delved deep into the intricacies of embedding technologies. They covered a breadth of topics, from the evolution of embeddings to their future in a world increasingly dominated by Generative AI and Large Language Models (LLMs).</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="720" height="541" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3011702708364_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg 720w" sizes="(min-width: 720px) 720px"></figure><h3 id="key-takeaways-from-the-panel">Key Takeaways from the Panel</h3><ol><li><strong>Diverse Perspectives on Embeddings:</strong><br>The panelists introduced themselves and their work with various embeddings, discussing the common threads and divergences they&apos;ve observed. They emphasized the nuanced differences in how embeddings behave depending on their design and application contexts.</li><li><strong>The Relevance of Embeddings Amidst Generative AI:</strong><br>With 2023&apos;s spotlight on LLMs, the panelists reflected on the enduring importance of embeddings. They highlighted that despite the LLM trend, embeddings retain a crucial role in understanding and processing language at a more granular level.</li><li><strong>Context Length in Embeddings vs. LLMs:</strong><br>A curious observation was the disparity in context length expansion between LLMs and embedding models. The panelists shed light on the technical and practical constraints that currently limit the context window in embedding models.</li><li><strong>Search and Generation:</strong><br>Addressing the assertion that &apos;search is an overfitted generation, and generation is an underfitted search,&apos; the panelists shared mixed views, sparking a lively debate on the interplay between search functions and generative capabilities.</li><li><strong>Future of RAG and Agent Models:</strong><br>Looking towards EMNLP 2024, the conversation turned to the prospective challenges and developments in Retrieval Augmented Generation (RAG) and agent models. The panelists hinted at their vision for the future integration of embeddings within these applications, recognizing the pivotal role they will continue to play.</li></ol><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3101702709471_.pic_hd-1.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3101702709471_.pic_hd-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3101702709471_.pic_hd-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3101702709471_.pic_hd-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3101702709471_.pic_hd-1.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3091702709471_.pic_hd.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3091702709471_.pic_hd.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3091702709471_.pic_hd.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3091702709471_.pic_hd.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3091702709471_.pic_hd.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="summary">Summary</h2><p>Wrapping up EMNLP 2023, I&apos;m buzzing with ideas and energized by the community&apos;s shared passion for pushing the boundaries of NLP. Our Embeddings BoF session was a hit &#x2013; the engagement and insights made it a highlight for me.</p><p>Looking to get hands-on with the future of embeddings? We are hiring! We&apos;re all about diving deep into long-context, multilingual, and multimodal embeddings. So, if you&apos;re up for the challenge, check out the open roles here and maybe I&apos;ll see you at our Berlin, Shenzhen, or Beijing office.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/internship/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Intern Program</div><div class="kg-bookmark-description">Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-internship.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Can&apos;t wait to see what we&apos;ll cook up by EMNLP 2024 in Miami. Until then, keep innovating, keep questioning, and let&apos;s keep the conversations going!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.2 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-2-update/</link><guid isPermaLink="false">657b24340bab3100012d27cb</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Thu, 14 Dec 2023 15:54:21 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3232">Release Note (<code>3.23.2</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" alt="Jina 3.23.2 Update"><p>This release contains 1 dependency update and 2 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.2?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.2 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.2) Release time: 2023-12-14 15:28:24 This release contains 1 dependency update and 2 bug fixes.
&#x2699; Dependency update
Update GRPC version requirements (#6110)
The grpc version re&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.2 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/66a744fcace7275ba964cb292fb9e91fd04678aa02883dd2f72c0c8ef1b2bc4a/jina-ai/jina/releases/tag/v3.23.2" alt="Jina 3.23.2 Update"></div></a></figure><h2 id="%E2%9A%99-dependency-update">&#x2699; Dependency update</h2><h3 id="update-grpc-version-requirements-6110">Update GRPC version requirements (<a href="https://github.com/jina-ai/jina/pull/6110?ref=jina-ai-gmbh.ghost.io">#6110</a>)</h3><p>The&#xA0;<code>grpc</code>&#xA0;version requirements have been updated to allow&#xA0;<code>grpcio&lt;=1.57.0</code>.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="better-handling-of-exceptions-in-dynamic-batching-6128">Better handling of Exceptions in dynamic batching (<a href="https://github.com/jina-ai/jina/pull/6128?ref=jina-ai-gmbh.ghost.io">#6128</a>)</h3><p>An issue was identified when a dynamic batch raises an Exception that could affect some unrelated requests.</p><h3 id="load-balancing-streaming-based-on-response-type-6122">Load-balancing streaming based on response type (<a href="https://github.com/jina-ai/jina/pull/6122?ref=jina-ai-gmbh.ghost.io">#6122</a>)</h3><p>When using&#xA0;<code>Deployment</code>&#xA0;locally with multiple replicas, a load-balancing process is added in front of the replicas. </p><p>The load balancer assumed all&#xA0;<code>GET</code>&#xA0;requests to be streaming, but this may not be true for user-added FastAPI endpoints. We have fixed this assumption and now use the response type to determine if a request is streaming.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[There's a Time and a Place: Unleashing Dynamic Variables in PromptPerfect]]></title><description><![CDATA[Incorporate users' time, date and location into your prompts. Plus contents of other websites, and outputs of other prompts themselves!]]></description><link>https://jina.ai/news/theres-a-time-and-a-place-unleashing-dynamic-variables-in-promptperfect/</link><guid isPermaLink="false">6571be77782cc90001042c6f</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 07 Dec 2023 14:59:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--29-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--29-.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><p>Back in the mists of time, PromptPerfect introduced its <a href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io">Prompt-as-a-Service</a> feature, allowing you to serve your prompts via a RESTful API and call them from Python, JavaScript or cURL.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">What&#x2019;s Next for Prompt Engineering? PromptPerfect&#x2019;s Prompt as a Service!</div><div class="kg-bookmark-description">Deploy prompts and flexible template prompts as REST API services, and integrate them into your applications with just a few clicks</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-publisher">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/06/Pic.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure><p>Let&#x2019;s imagine we&#x2019;re building an AI-powered recipe website. We want users to be able to click a button that suggests a delicious recipe for them. To do that, we can create a prompt and deploy it as a service. We would then incorporate that into our website via Prompt-as-a-Service&#x2019;s <a href="https://promptperfect.jina.ai/api?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">RESTful API.</a></p><p>For example, we could have a prompt that suggested recipes to users:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="760" height="943" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123047_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png 760w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Incorporating a RESTful API into your website is <i><em class="italic" style="white-space: pre-wrap;">way</em></i> outside the scope of this post, so we&#x2019;ll just use cURL moving forwards. But rest assured, you can do this in any language or framework with an HTTP library.</div></div><p>By calling the prompt, a user could get a new recipe each time:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">Certainly! Below is a recipe for a classic Italian dish, Spaghetti Carbonara. This dish is known for its creamy texture and rich flavors, achieved with minimal ingredients. Enjoy!</span></p><p><b><strong style="white-space: pre-wrap;">Spaghetti Carbonara</strong></b></p><p><b><strong style="white-space: pre-wrap;">Ingredients</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">400g spaghetti</span></li><li value="2"><span style="white-space: pre-wrap;">150g pancetta or guanciale, diced</span></li><li value="3"><span style="white-space: pre-wrap;">3 large eggs</span></li><li value="4"><span style="white-space: pre-wrap;">1 cup (100g) freshly grated Parmesan cheese, plus extra for serving</span></li><li value="5"><span style="white-space: pre-wrap;">2 cloves garlic, peeled and left whole</span></li><li value="6"><span style="white-space: pre-wrap;">2 tablespoons extra-virgin olive oil</span></li><li value="7"><span style="white-space: pre-wrap;">Salt and freshly ground black pepper</span></li><li value="8"><span style="white-space: pre-wrap;">A small handful of fresh parsley, chopped (optional)</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions</strong></b></p><p><b><strong style="white-space: pre-wrap;">Cook the Pasta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Bring a large pot of salted water to a boil.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the spaghetti and cook according to the package instructions until al dente.</span></li><li value="3"><span style="white-space: pre-wrap;">Reserve 1 cup of pasta water before draining the spaghetti.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Prepare the Sauce</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a small bowl, whisk together the eggs and grated Parmesan cheese until well combined. Set aside.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Cook the Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">While the pasta is cooking, heat the olive oil in a large skillet over medium heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the pancetta or guanciale and the whole garlic cloves. Cook, stirring frequently, until the meat is crispy and golden, about 5 minutes.</span></li><li value="3"><span style="white-space: pre-wrap;">Remove and discard the garlic cloves.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Combine Pasta and Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Add the drained spaghetti to the skillet with the pancetta.</span></li><li value="2"><span style="white-space: pre-wrap;">Toss well to coat the pasta in the pancetta&apos;s fat.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Make Carbonara</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Remove the skillet from the heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Quickly pour the egg and cheese mixture into the pasta, stirring vigorously with tongs or a fork until the eggs thicken but do not scramble.</span></li><li value="3"><span style="white-space: pre-wrap;">Add a little reserved pasta water if the sauce is too thick. The heat from the pasta will cook the eggs and melt the cheese to create a creamy sauce.</span></li><li value="4"><span style="white-space: pre-wrap;">Season generously with freshly ground black pepper and add a pinch of salt if needed.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Garnish and Serve</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Serve immediately with extra grated Parmesan cheese and a sprinkle of chopped parsley, if desired.</span></li></ul><p><span style="white-space: pre-wrap;">Enjoy your delicious Spaghetti Carbonara!</span></p><p><span style="white-space: pre-wrap;">This recipe serves 4 people and is best enjoyed fresh.</span></p></div>
        </div><h2 id="template-prompts">Template prompts</h2><p>Now, static prompts are fine and all, but we also allow template prompts. This means a user could set options (like ingredients or dietary restrictions) each time they called the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="614" height="467" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123701_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png 614w"></figure><p>Which would give the user something like:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;parameters&quot;: {&quot;ingredients&quot;:&quot;apples, cinnamon, nutmeg&quot;}}&apos;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><b><strong style="white-space: pre-wrap;">Classic Apple Crisp Recipe</strong></b></p><p><span style="white-space: pre-wrap;">Enjoy the sweet and spicy flavors of apples, cinnamon, and nutmeg in this deliciously easy-to-make apple crisp. Perfect for a cozy night in or a festive gathering!</span></p><p><b><strong style="white-space: pre-wrap;">Ingredients:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">6 medium-sized apples, peeled, cored, and thinly sliced</span></li><li value="2"><span style="white-space: pre-wrap;">1 teaspoon ground cinnamon</span></li><li value="3"><span style="white-space: pre-wrap;">1/4 teaspoon ground nutmeg</span></li><li value="4"><span style="white-space: pre-wrap;">3/4 cup granulated sugar</span></li><li value="5"><span style="white-space: pre-wrap;">2 tablespoons lemon juice</span></li></ul><p><b><strong style="white-space: pre-wrap;">For the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">3/4 cup all-purpose flour</span></li><li value="2"><span style="white-space: pre-wrap;">3/4 cup old-fashioned rolled oats</span></li><li value="3"><span style="white-space: pre-wrap;">1/2 cup light brown sugar, packed</span></li><li value="4"><span style="white-space: pre-wrap;">1/2 teaspoon baking powder</span></li><li value="5"><span style="white-space: pre-wrap;">1/4 teaspoon ground cinnamon</span></li><li value="6"><span style="white-space: pre-wrap;">1/4 teaspoon salt</span></li><li value="7"><span style="white-space: pre-wrap;">1/3 cup unsalted butter, melted</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions:</strong></b></p><p><b><strong style="white-space: pre-wrap;">Preheat Oven and Prepare Apples:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Preheat your oven to 350&#xB0;F (175&#xB0;C).</span></li><li value="2"><span style="white-space: pre-wrap;">In a large mixing bowl, combine the sliced apples with granulated sugar, 1 teaspoon cinnamon, nutmeg, and lemon juice. Toss to coat the apples evenly.</span></li><li value="3"><span style="white-space: pre-wrap;">Transfer the apple mixture to a greased 9-inch square baking dish, spreading them out evenly.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Make the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a separate bowl, mix the flour, oats, brown sugar, baking powder, 1/4 teaspoon cinnamon, and salt.</span></li><li value="2"><span style="white-space: pre-wrap;">Pour the melted butter over the dry ingredients and mix until the mixture is crumbly.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Assemble and Bake:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Sprinkle the crumbly oat topping evenly over the apples in the baking dish.</span></li><li value="2"><span style="white-space: pre-wrap;">Bake in the preheated oven for about 45 minutes, or until the topping is golden brown and the apples are bubbling around the edges.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Serve:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Allow the apple crisp to cool slightly before serving.</span></li><li value="2"><span style="white-space: pre-wrap;">Serve warm, optionally with a scoop of vanilla ice cream or a dollop of whipped cream.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Notes:</strong></b></p><ul><li value="1"><b><strong style="white-space: pre-wrap;">Choosing Apples:</strong></b><span style="white-space: pre-wrap;"> For the best texture and flavor, use a mix of tart and sweet apples like Granny Smith and Honeycrisp.</span></li><li value="2"><b><strong style="white-space: pre-wrap;">Storage:</strong></b><span style="white-space: pre-wrap;"> Leftover apple crisp can be stored in the refrigerator for up to 3 days. Reheat in the oven or microwave before serving.</span></li></ul></div>
        </div><h2 id="a-time-and-a-place-for-everything">A time and a place for everything</h2><p>Now we&apos;re taking it up a notch with environmental awareness. Rather than the end-user having to specify where (and when) they are, you can choose to have that information automatically inserted. That means you can create a prompt once, and end-users will get different results depending on their location, date, and time.</p><p>Since recipes don&#x2019;t usually need a specified time and place, let&#x2019;s consider a new example: Building a website to suggest activities for a user to take part in:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="759" height="944" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124457_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png 759w" sizes="(min-width: 720px) 720px"></figure><p>We also need to enable Environment Awareness:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="611" height="110" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124710_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png 611w"></figure><p>Now, I (in Berlin at noon) get something like this when I call the prompt:</p><pre><code>- Visit the Berlin Wall Memorial at Bernauer Stra&#xDF;e
- Explore the Museum Island, especially the Pergamon Museum
- Take a walk through the Tiergarten Park
- Enjoy panoramic views from the Berliner Fernsehturm (TV Tower)
- Discover German history at the German Historical Museum
- Stroll along the East Side Gallery
- Check out the Brandenburg Gate at dusk
- Experience contemporary art at the Hamburger Bahnhof museum
- Shop or window-shop at Kurf&#xFC;rstendamm
- Relax in the evening at a traditional German beer garden&quot;
</code></pre><p>Whereas if my colleague in Tokyo called the prompt, she&apos;d get:</p><pre><code>- Visit Tokyo Skytree for panoramic views of the city at night.
- Explore the vibrant streets of Shibuya and see the famous Shibuya Crossing.
- Enjoy the illuminated Tokyo Tower and consider an evening visit.
- Take a stroll through the historic Asakusa district and see Senso-ji Temple.
- Experience the nightlife in Roppongi, known for its clubs and bars.
- Relax at Odaiba Seaside Park and enjoy the night view of Rainbow Bridge.
- Try out some local izakayas (Japanese pubs) for food and drinks.
- Visit a themed caf&#xE9;, like a cat caf&#xE9; or an owl caf&#xE9;, for a unique experience.
- Check out the latest gadgets and tech in Akihabara, the electronics district.
- Attend a traditional Kabuki performance at Kabukiza Theatre in Ginza (if available).
- Indulge in a sushi dinner at one of Tokyo&apos;s renowned sushi restaurants.
</code></pre><p>These variables are great when you&apos;re crafting prompts that deliver relevant output based on the user&#x2019;s time, date, and location. This is ideal for building websites and apps with a global audience.</p><h2 id="fetching-remote-data">Fetching remote data</h2><p>We can go even further though. With the crawler feature, you can download the main body text of the URL you specify:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="736" height="371" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125131_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png 736w" sizes="(min-width: 720px) 720px"></figure><p>This makes it ideal for, say, a summarizer/translator prompt. Here we summarize an article and convert it to a sea shanty:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="608" height="402" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125339_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png 608w"></figure><p>If I pass the URL for this story on <a href="https://www.cnn.com/2023/11/18/world/elon-musk-spacex-starship-launch-scn/index.html?ref=jina-ai-gmbh.ghost.io">Elon Musk&apos;s spaceship exploding</a>, I get this (abridged) output:</p><pre><code class="language-text">(Verse 1)
Oh, gather &apos;round, me hearties, and a tale I&apos;ll tell to thee,
Of SpaceX&apos;s Starship, and its flight so brief at sea.
Launched on a morn in November, with power so fierce and grand,
She climbed into the heavens, but &apos;twas not as Musk had planned.

(Chorus)
Heave ho, lift high, to the stars we aim to fly,
But the rocket&apos;s dreams did shatter, and into the sea did die.
Heave ho, lift high, with a fiery tail in sky,
SpaceX&apos;s Starship faltered, and we&apos;re left to wonder why.

(Verse 2)
The booster and the spacecraft, they parted with a roar,
The engines blazed like comets, as they&apos;d never done before.
The booster met its ending, in a ball of flame it fell,
While Starship soared a moment more, then silence cast its spell.
</code></pre><h2 id="calling-other-prompt-services">Calling other prompt services</h2><p>The above example wasn&apos;t exactly modular. I had both the summarizer and shantyizer in one prompt. That isn&apos;t so useful if I want to shanty all my things in full detail. Luckily, you can also call one prompt from another, allowing for a lot more modularity:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="762" height="493" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125614_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png 762w" sizes="(min-width: 720px) 720px"></figure><p>So, if I create a summarizer prompt (named <code>summarizer</code>, which has the sole function of summarizing a web page)&#x2026;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_130155_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="592" height="387"></figure><p>&#x2026;I can easily call it from a new shantyizer prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125751_Chrome-1.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="591" height="393"></figure><p>Sending a URL to the shantyizer prompt in turn processes the URL through the summarizer to download and summarize its contents. And of course, the summarizer can be used in any other prompt you like, acting more like a traditional programming language function.</p><p>That opens up many more possibilities. Especially when it comes to more complex tasks like cooking a dish. If I&#x2019;m cooking, for any given recipe I may want <em>one or more</em> of the following steps, but perhaps not all steps for all recipes.</p><ul><li>Download the content of a recipe page</li><li>If it&apos;s not in English, translate it (if I&apos;m cooking an authentic Chinese recipe, I want it from a real Chinese website)</li><li>Make it vegetarian (if I&apos;m eating with vegetarian friends)</li><li>Convert it to metric (because <a href="https://www.boredpanda.com/imperial-measurement-system-twitter-rant-innesmck/?ref=jina-ai-gmbh.ghost.io">reasons</a>)</li><li>Change serving size (depending on how many people I&apos;m eating with)</li></ul><h2 id="exercise-for-the-reader">Exercise for the reader</h2><p>As you can imagine, if I&apos;m eating alone (and I&apos;m not a vegetarian), my needs are quite different from when I&apos;m eating with my buddies at the local <a href="https://www.snopes.com/fact-check/peta-rename-fish-sea-kittens/?ref=jina-ai-gmbh.ghost.io">sea kitten appreciation society</a>. Either way, it means combining several of these &quot;functions&quot;.</p><p>We can also go multi-modal, combining text and image prompts:</p><ul><li>Create an image generation prompt from the given recipe</li><li>Generate an image of how the food should look</li></ul><p>The exercise for you, dear reader, is to build a prompt that performs the above recipe steps (or similar). Share your results on our Discord!</p><h2 id="make-the-magic-happen">Make the magic happen</h2><p>To get started with magic variables, head to <a href="http://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io">promptperfect.jina.ai</a> and get started. Let us know what you cook up on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord</a>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Dify.AI integrates Jina Embeddings for RAG]]></title><description><![CDATA[Dify.AI, a leading open-source platform specialized in creating generative AI applications, is now leveraging Jina Embeddings v2! ]]></description><link>https://jina.ai/news/dify-ai-integrates-jina-embeddings-for-rag/</link><guid isPermaLink="false">65703e6d782cc90001042bca</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 06 Dec 2023 15:00:31 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" alt="Dify.AI integrates Jina Embeddings for RAG"><p>Online LLM application development platform <a href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Dify.AI</a> has integrated the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 API</a> in its innovative AI toolkit for instant access when building and hosting LLM applications. All you need to do is add your Jina Embeddings API key via their intuitive web interface to get the full power of Jina AI&#x2019;s industry-leading embedding models in your RAG (retrieval-augmented generation) applications.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI x Jina AI&#xFF1A;Dify now Integrates Jina Embedding Model - Dify Blog</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Dify Blog</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/2OftM3MmDVHkLcxaNn58ZNWaE.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><h2 id="integrating-embeddings-in-rag">Integrating Embeddings in RAG</h2><p>Current AI architectures today have no direct way to integrate outside information sources. The model itself encodes information from its training data with varying levels of accuracy, and it is impractical to retrain the model every time there is new, potentially useful data that could be incorporated into it.</p><p>For example, I asked <a href="https://chat.jina.ai/chat?ref=jina-ai-gmbh.ghost.io">JinaChat</a> a question about current events:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1408" height="1654" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.48.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1408w" sizes="(min-width: 720px) 720px"></figure><p>The only way to ensure that an LLM has the information needed to answer a factual question is to provide it in the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1316" height="1186" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.53.33.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1316w" sizes="(min-width: 720px) 720px"></figure><p>Naturally, an LLM that only answers questions correctly if you include the answer in your question isn&#x2019;t very useful. This has led to a body of techniques called <strong>Retrieval-Augmented Generation</strong> or <strong>RAG</strong>. RAG is a framework installed around an LLM that searches external information sources for materials that might contain the information needed to answer a user&#x2019;s request and then presents them, with the user&#x2019;s prompt, to the LLM.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-2.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="2000" height="938" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-2.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This strategy has the added benefit that LLMs hallucinate much less when they are expected to handle a given text rather than recall things they might partially remember from training.</p><h2 id="leveraging-jina-embeddings-superior-performance">Leveraging Jina Embeddings Superior Performance</h2><p><a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a> has integrated Jina Embeddings v2 to enhance retrieval quality for RAG prompting. Jina AI&#x2019;s models provide <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">state-of-the-art accuracy in RAG applications</a>, and <a href="https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io">with an input window of 8,192 tokens</a>, they can support much larger and more complex questions than most competing models at a much lower price.</p><p>You can now use Jina Embeddings in your LLM projects via <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s intuitive application builder, as shown in the video below or in the <a href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">post on Dify.AI&apos;s blog</a>:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396.mp4" poster="https://img.spacergif.org/v1/1244x720/0a/spacer.png" width="1244" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:38</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h2 id="get-involved">Get Involved</h2><p>Check out <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s LLM application builder and hosting service for yourself. You can get a free tester token from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina AI website to use Jina Embeddings</a> to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI &#xB7; The Innovation Engine for Generative AI Applications</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/wh4qGCAanwpqHs0Kot8VLBSty4.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3873 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings v2 and MongoDB Atlas]]></title><description><![CDATA[Supercharge MongoDB Atlas multi-cloud vector search solutions with Jina AI’s industry-leading embeddings!]]></description><link>https://jina.ai/news/jina-embeddings-v2-and-mongodb-atlas/</link><guid isPermaLink="false">6570380d782cc90001042bac</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 06 Dec 2023 09:49:40 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--8-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--8-.jpg" alt="Jina Embeddings v2 and MongoDB Atlas"><p>Jina AI and <a href="https://www.mongodb.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MongoDB</a> have collaborated to bring you a tutorial on using <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> to bring state-of-the-art semantic search to MongoDB Atlas. Follow the link below to MongoDB&apos;s website to see how you can supercharge MongoDB&#x2019;s multi-cloud vector search solutions with Jina AI&#x2019;s industry-leading embeddings!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Semantic search with Jina Embeddings v2 and MongoDB Atlas | MongoDB</div><div class="kg-bookmark-description">Follow along with this tutorial on using Jina Embeddings v2 with MongoDB Atlas for vector search.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.mongodb.com/developer/favicon.ico" alt="Jina Embeddings v2 and MongoDB Atlas"><span class="kg-bookmark-author">MongoDB</span><span class="kg-bookmark-publisher">Scott Martens, Saahil Ognawala</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt3d996d2aa8d9ece9/647a2f77f7b1280bb15174b7/Technical_SOFTWARE_Terminal(4)_Spot_BS_ForestGreen.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session]]></title><description><![CDATA[Mark your calendars for an immersive BoF session on Embeddings at EMNLP 2023. Join us on December 9th from 11:00 AM to 12:30 PM, Singapore time, in the 'Aquarius 1' room for a deep dive into the latest embeddings.]]></description><link>https://jina.ai/news/discover-the-latest-in-embeddings-at-emnlp-2023-in-person-bof-session/</link><guid isPermaLink="false">6569dc2e66eb300001a28a4b</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Fri, 01 Dec 2023 14:25:49 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--25--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--25--1.png" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"><p>At the EMNLP2023 BoF (birds of a feather) session, we&apos;re set to explore the frontiers of embeddings. While recent developments like <code>jina-embeddings-v2</code> <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">have made significant strides</a>, they also open the door to new questions and challenges in the field. This 1.5-hour session is a platform to tackle these emerging issues in embeddings. You can register via the Google Form below:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://forms.gle/z5E5BFvQqFf3fpW68?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">EMNLP 2023/BoF 6: Embeddings</div><div class="kg-bookmark-description">What: A BoF (Birds of a Feather) in-person session on Embeddings, co-organized by EMNLP PC and Jina AI. When &amp; Where: Saturday, December 9th, from 11:00 AM to 12:30 PM Singapore time at &#x2018;Aquarius 1&#x2019; room. Highlights: multimodal embeddings, long context embeddings, multi- and bilingual embeddings,&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/docs/forms/device_home/android_192.png" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"><span class="kg-bookmark-author">Google Docs</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://lh5.googleusercontent.com/osAO-g4ajeOU2CCB6rhbi9UfHIsCU8_oa8HaZjsx0e6UuWoulCngSugztdf3Yoe2a5FQVSymImg=w1200-h630-p" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"></div></a></figure><h2 id="event-details">Event Details</h2><p><strong>Date &amp; Time:</strong> December 9th, 11:00 AM - 12:30 PM Singapore time, at &apos;Aquarius 1&apos; room.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4C6;</div><div class="kg-callout-text">Add Embeddings BoF at EMNLP to your calendar now [<a href="https://www.google.com/calendar/render?action=TEMPLATE&amp;text=Embeddings%3A+Birds+of+a+Feather+6&amp;dates=20231209T040000%2F20231209T053000&amp;ctz=UTC&amp;details=Birds+of+a+Feather+6&amp;location=&amp;sprop=&amp;sprop=name%3A&amp;ref=jina-ai-gmbh.ghost.io" target="_blank">Google</a>][<a href="https://outlook.office365.com/owa/?path=%2Fcalendar%2Faction%2Fcompose&amp;rru=addevent&amp;subject=Embeddings%3A+Birds+of+a+Feather+6&amp;startdt=2023-12-09T05%3A00%3A00+01%3A00&amp;enddt=2023-12-09T06%3A30%3A00+01%3A00&amp;body=Birds+of+a+Feather+6&amp;location=true&amp;ref=jina-ai-gmbh.ghost.io" target="_blank">Office365</a>][<a href="about:blank">Outlook</a>][<a href="about:blank">iCal</a>]</div></div><p><strong>Format:</strong> In-person, with an interactive and collaborative approach.<br><strong>Registration: </strong><a href="https://forms.gle/z5E5BFvQqFf3fpW68?ref=jina-ai-gmbh.ghost.io">Google Form</a></p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-1.png" class="kg-image" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session" loading="lazy" width="1288" height="962" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-1.png 1288w" sizes="(min-width: 720px) 720px"></figure><h2 id="themes-and-highlights">Themes and Highlights</h2><p>This BoF session features a set of thought-provoking presentations, panel discussions, and lightning talks, reflecting the ongoing evolution in the field:</p><h4 id="multimodal-embeddings">Multimodal Embeddings:</h4><ul><li>&quot;How can multimodal embeddings be advanced to handle the context and nuance in diverse data types?&quot;</li><li>&quot;What are the challenges in scaling multimodal embeddings for complex applications like personalized AI assistants?&quot;</li><li>&quot;In what ways can vector databases be integrated with multimodal embeddings to enhance their contextual understanding?&quot;</li></ul><h4 id="long-context-embeddings">Long Context Embeddings:</h4><ul><li>&quot;How can long context embeddings be optimized to reduce hallucination in LLMs?&quot;</li><li>&quot;What are the computational and practical challenges in scaling embeddings to handle longer contexts efficiently?&quot;</li><li>&quot;Beyond the 8K limit, what novel approaches can push the boundaries of context length in embeddings?&quot;</li></ul><h4 id="multiand-bilingual-embeddings">Multi- and Bilingual Embeddings:</h4><ul><li>&quot;What are the key hurdles in achieving high accuracy and cultural sensitivity in multi- and bilingual embeddings?&quot;</li><li>&quot;How can embeddings tackle the challenge of representing less popular factual knowledge accurately?&quot;</li><li>&quot;In the development of bilingual embeddings, how can we address the lack of in-house NLP expertise and high development costs?&quot;</li></ul><h4 id="embeddings-for-low-resource-languages">Embeddings for Low-Resource Languages:</h4><ul><li>&quot;What innovative strategies can be adopted to significantly boost support for low-resource languages in embeddings?&quot;</li><li>&quot;How can we incentivize the development of embeddings for less popular languages amidst high costs and complexity?&quot;</li><li>&quot;What role can transfer learning and fine-tuning play in enhancing embeddings for low-resource languages?&quot;</li></ul><h4 id="rag-retrieval-augmented-generation-applications">RAG (Retrieval Augmented Generation) Applications:</h4><ul><li>&quot;How can RAG applications overcome the limitations of vector databases in understanding complex, nuanced meanings?&quot;</li><li>&quot;What are the potentials and challenges of using RAG in AI assistants for deep personalization and context understanding?&quot;</li><li>&quot;Considering the current state of RAG, what are the ethical considerations and potential risks in its widespread adoption?&quot;</li></ul><p>For more information, visit <a href="https://2023.emnlp.org/program/bof/?ref=jina-ai-gmbh.ghost.io">EMNLP2023 Program</a>. </p><p>We are looking forward to seeing you in person in Singapore next week!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.1 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-1-update/</link><guid isPermaLink="false">6569a11066eb300001a2899e</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 01 Dec 2023 09:59:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3231">Release Note (<code>3.23.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark.jpg" alt="Jina 3.23.1 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.1 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.1) Release time: 2023-12-01 09:24:22 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Fix dependency on OpenTelemetry Exporter Prometheus (#6118)
We fixed the dependency version wi&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/bbfee09be3e253af0418f345fc17cff01a66ed9575027c8c932d25cf7cb4fb6f/jina-ai/jina/releases/tag/v3.23.1" alt="Jina 3.23.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-dependency-on-opentelemetry-exporter-prometheus-6118">Fix dependency on OpenTelemetry Exporter Prometheus (<a href="https://github.com/jina-ai/jina/pull/6118?ref=jina-ai-gmbh.ghost.io">#6118</a>)</h3><p>We fixed the dependency version with&#xA0;<code>opentelemetry-exporter-prometheus</code>&#xA0;to avoid using deprecated versions.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Give and Take in FDP's AI Geopolitics Panel]]></title><description><![CDATA[This blog post captures my personal reflections and contributions to the panel, highlighting key differences in AI strategies among global powers like the US, China, and the EU.]]></description><link>https://jina.ai/news/give-and-take-in-fdps-ai-geopolitics-panel/</link><guid isPermaLink="false">6567283bd64f01000187a058</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Wed, 29 Nov 2023 13:04:01 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--23-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--23-.png" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"><p>The crisp air of Berlin in late November carried a unique vibrancy, especially in the bustling district of Prenzlauer Berg. As I weaved through the lively Christmas market outside the <a href="https://palais-kulturbrauerei.de/?ref=jina-ai-gmbh.ghost.io">Palais der Kulturbrauerei</a>, the scene was a microcosm of Berlin itself. Stalls adorned with twinkling lights offered an array of handmade trinkets and steaming treats, drawing in a crowd that was as diverse as the city&apos;s own character. Artists, both emerging and established, mingled with locals and tourists, their conversations a blend of German and a myriad of other languages, creating a tapestry of global influence and local charm. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/2831701259388_.pic.jpg" class="kg-image" alt="Give and Take in FDP&apos;s AI Geopolitics Panel" loading="lazy" width="1702" height="1276" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/2831701259388_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/2831701259388_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/2831701259388_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/2831701259388_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Xmas market at Kulturbrauerei, a symbol of Berlin&apos;s industrial past and its cultural renaissance, was more than just a festive gathering. It was a reflection of the city&apos;s unique ethos - not striving to compete with the grandeur of Paris or the pace of Shanghai, but instead carving out its own identity, one that embraced its history while boldly facing the future.</span></figcaption></figure><p>As I approached the Palais, the sounds of the market slowly gave way to a more subdued atmosphere. Inside, the venue buzzed with the energy of intellectuals and policymakers, all gathered to dissect and debate the future of EU&apos;s AI. </p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://crm.fdpbt.de/termin/ai-revolution-wie-kuenstliche-intelligenz-unsere-welt-veraendert?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Zustimmung zu Cookies | FDP-Fraktion im Bundestag</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://crm.fdpbt.de/themes/custom/uv_fdpbt_theme/favicon.ico" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"><span class="kg-bookmark-author">FDP-Fraktion im Bundestag</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://crm.fdpbt.de/termin/themes/custom/uv_fdpbt_theme/images/logo.svg" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"></div></a><figcaption><p><span style="white-space: pre-wrap;">The full program of this FDP event is available </span><a href="https://crm.fdpbt.de/sites/default/files/2023-11/231128%20FDPBT%20KI%20Programm.pdf?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">here</span></a><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><p>Despite my role as a panelist and my years of experience in AI, spanning from Tencent AI to my own AI startups, I couldn&apos;t shake a sense of skepticism. Was my presence here truly impactful? Would the discussions tonight resonate beyond these walls? </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/2851701259389_.pic.jpg" class="kg-image" alt="Give and Take in FDP&apos;s AI Geopolitics Panel" loading="lazy" width="1702" height="1276" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/2851701259389_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/2851701259389_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/2851701259389_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/2851701259389_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">As the conversation unfolded, my thoughts turned towards what I would share with the audience. My answers, crafted from years of navigating the complex landscapes of AI in two very different worlds, were ready.</span></figcaption></figure><p>The air was thick with expectations as the panelists, including myself, took our seats. I was curious to hear the perspectives of my fellow speakers and the FDP&apos;s stance on these pressing issues. While I harbored doubts about the overall impact of such events, a part of me remained hopeful. Perhaps, through these discussions, we could foster a deeper understanding between Europe and China, bridging gaps in perception and strategy. </p><p>Below, you&apos;ll find my responses to the questions posed during the panel. Some answers have been expanded and further refined during my subsequent reflections.</p><h2 id="strategies-and-positions-of-major-powers">Strategies and positions of major powers</h2><h3 id="q-how-do-the-strategies-and-positions-of-the-us-china-and-the-eu-differ-in-achieving-global-leadership-in-ai">Q: How do the strategies and positions of the US, China and the EU differ in achieving global leadership in AI?</h3><p><strong>Han:</strong> In discussing how the strategies and positions of the US, China, and the EU differ in achieving global leadership in AI, I would start by highlighting that the US primarily drives innovation through the private sector with substantial funding and the robust Silicon Valley ecosystem. In contrast, China adopts a <strong>state-driven </strong>approach, utilizing <strong>large-scale</strong> data and focusing on <strong>rapid implementation</strong> of AI technologies. The EU, on the other hand, places a strong emphasis on ethical AI, developing regulatory frameworks, and maintaining a focus on human-centric AI.</p><p>Diving deeper into China&apos;s approach, we see ambitious state-driven initiatives, with the goal of becoming an AI world leader by 2030. This is rooted in a legacy of central planning and a national capitalism ethos. There&apos;s a significant emphasis on indigenous innovation, mirroring European approaches, but with a distinct focus on technological sovereignty. Talent development is also key, as evidenced by China&apos;s &quot;Thousand Talents&quot; plan, which aims to bolster AI expertise.</p><p>Large-scale data usage in China is a critical factor, with vast data repositories enhancing algorithm accuracy. This is supported by multidimensional datasets, particularly rich user data derived from diverse smartphone activities. When it comes to implementation, China has developed 79 large language models, each with over 1 billion parameters, showing remarkable progress. The focus here is on practical application, transitioning from discovery to a strong execution in &quot;1 to n&quot; innovation.</p><p>In contrast to the US, which focuses more on discovery, China excels in the implementation and application of AI technologies. This difference highlights a unique approach where China leverages its strengths in rapid development and practical application of AI solutions.</p><h2 id="cooperation-versus-regulation">Cooperation versus regulation</h2><h3 id="q-should-countries-limit-or-expand-their-collaborations-in-ai-research-due-to-geopolitical-rivalries">Q: Should countries limit or expand their collaborations in AI research due to geopolitical rivalries?</h3><p><strong>Han: </strong>When considering whether countries should limit or expand their collaborations in AI research due to geopolitical rivalries, it&apos;s tempting to adopt an old cold-war mindset. It&apos;s easy to see why the US might be hesitant to collaborate: they don&apos;t want to share their technological leadership, have security concerns, face economic competition, and grapple with ideological differences. In German, there&apos;s a saying &quot;Wer oben ist, will oben bleiben,&quot; meaning &quot;Those who are on top want to stay on top.&quot;</p><p>However, collaboration is the only rational way to accelerate AI development. The United States is known for its advanced AI algorithms and strong research capabilities, while China boasts vast data resources essential for training AI models.</p><p>The ideal scenario would involve collaboration between the US and China, specializing in AI excelling in efficiency and scalability, but we must be mindful of potential issues with global trust and acceptance due to weaker ethical and privacy safeguards. Similarly, collaboration between the US and the EU could yield technologically sophisticated, ethically compliant AI, but might struggle with data diversity and comprehensiveness. China and the EU could focus on ethically-grounded AI with rich data integration but might lag in incorporating the latest AI innovations. The best outcome would be a balanced AI, leading in innovation, ethics, and data scope, overcoming individual limitations.</p><p>A balanced approach is necessary, considering geopolitical tensions and concerns about data privacy, security, and ethical use of AI.</p><p>However, the reason China might be reluctant to open its arms fully to US AI is due to a focus on cultural and political autonomy. The Chinese emphasis on indigenous innovation is partly driven by concerns over cultural influences and values being exported through U.S.-dominated AI, especially in terms of political influence.</p><p>Information sovereignty is crucial for ensuring that AI development aligns with national values and interests, avoiding external cultural or political influences.</p><p>A true story illustrates this point: Post-WWII, Japan was heavily influenced by American ideas, including the notion that a bread diet was superior to rice for health. This belief, supported by some Japanese scholars and the government, led to significant changes in dietary habits in Japan, decreasing the country&apos;s self-sufficiency in food and increasing reliance on imports from the United States. This example shows how foreign influence can profoundly impact national culture and practices.</p><h3 id="q-should-the-eu-work-even-more-closely-with-strategic-partners-like-the-us">Q: Should the EU work even more closely with strategic partners like the US?</h3><p><strong>Han: </strong>Understanding the EU&apos;s inclination to collaborate with the US in AI is straightforward, as the US is a leader in AI innovation, which can complement the EU&apos;s strengths in regulatory frameworks. Additionally, the EU and US often share common values such as democracy, human rights, and privacy, aligning closely in AI development.</p><p>However, working with China also presents significant advantages. China ranks in the top 3 globally for AI vibrancy, as per the Stanford AI Index. Its AI market is projected to exceed $26.7 billion by 2026, offering the largest real-world testing ground for AI innovations, invaluable for global scalability. The sector diversity in China, with strong AI adoption in finance, retail, and high tech, and leadership in consumer AI apps, is noteworthy. Additionally, the automotive AI potential in China, being the world&apos;s largest auto market, presents significant opportunities, especially in autonomous vehicles and fleet management, with an estimated $380 billion economic value by 2030.</p><h2 id="challenges-and-opportunities-for-the-eu">Challenges and opportunities for the EU</h2><h3 id="q-compared-to-individual-state-actors-such-as-the-us-and-china-how-can-the-eu-maintain-its-competitiveness">Q: Compared to individual state actors such as the US and China, how can the EU maintain its competitiveness?</h3><p><strong>Han: </strong>The EU faces significant challenges in maintaining competitiveness with individual technological powerhouses like the US and China. One of the main hurdles is technological lag, with European companies generally underperforming compared to their US counterparts, especially in terms of growth and R&amp;D investment. This is reflective of Europe&apos;s lag in the last technology revolution, notably in ICT and disruptive innovations.</p><p>Moreover, there are stark investment disparities in key future technologies such as quantum computing, 5G, and AI. For instance, in AI, Europe only captured about 12% of external funding between 2015 and 2020, compared to 40% by the US and 32% by Asia, including China. This gap in investment is a significant factor contributing to the EU&apos;s challenges in maintaining technological competitiveness.</p><h3 id="q-does-the-eu-offer-any-specific-advantages">Q: Does the EU offer any specific advantages?</h3><p><strong>Han: </strong>The EU indeed offers specific advantages in the realm of AI. Europe boasts a rich talent pool and a strong tradition in theoretical AI research and mathematics. Notable European AI researchers include Yoshua Bengio from France/Canada, a leader in deep learning, J&#xFC;rgen Schmidhuber from Germany, known for his work on neural networks, and Max Welling, a Dutch computer scientist recognized for his contributions in machine learning and AI. Esteemed institutions like the Max Planck Institutes in Germany and INRIA in France have been at the forefront of significant AI advancements.</p><p>Europe is often seen as the &apos;Switzerland of AI politics,&apos; maintaining a neutral position amid US-China tech tensions. This neutrality is strategic, enabling Europe to focus on ethical AI development and collaborate innovatively without getting entangled in geopolitical struggles.</p><p>The EU&apos;s approach is characterized by the motto, &quot;In Europe, we don&apos;t just have history in our museums; we also make history in our AI labs.&quot; This reflects Europe&apos;s unique position in observing and learning from the technological race between the &apos;eagles&apos; (US) and &apos;dragons&apos; (China), emphasizing the importance of foresight in addition to speed.</p><p>In conclusion, AI is a transformative force comparable to the Industrial Revolution. Europe&apos;s stance encourages teamwork, openness, and curiosity. If not leveraged, there&apos;s a risk of turning this revolutionary era into a missed opportunity, relegating Europe to a spectator&apos;s role in this global race.</p>]]></content:encoded></item><item><title><![CDATA[SceneXplain's JSON Schema Store: Automate Your Alt-Text, and More!]]></title><description><![CDATA[Take the hassle out of extracting data from images with SceneXplain's new JSON Schema Store. Discover and share reusable JSON schemas. Create, contribute, and access schemas easily through GUI or API]]></description><link>https://jina.ai/news/scenexplains-json-schema-store-automate-your-alt-text-and-more/</link><guid isPermaLink="false">655f48ed14230d0001313816</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 27 Nov 2023 15:00:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><p>We&apos;re starting to run low on phrases like &quot;we&apos;re happy to announce x&quot;, or &quot;announcing x&quot; every time we push out a new feature in SceneXplain. So let&apos;s just jump to the meat: SceneXplain has a new JSON Schema Store, where you can discover and share reusable JSON schemas. This builds on our recent &quot;extract JSON from image&quot; feature, which lets you specify a JSON Schema when uploading an image, and get information back in JSON that adheres to that schema.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain&#x2019;s Image-to-JSON: Extract Structured Data from Images with Precision</div><div class="kg-bookmark-description">Pushing the boundaries of visual AI, we&#x2019;re thrilled to unveil SceneXplain&#x2019;s Image-to-JSON feature. Dive into a world where images aren&#x2019;t just seen, but deeply understood, translating visuals into structured data with unparalleled precision.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/09/Explore-image-storytelling-beyond-pixels--7-.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure><h2 id="what-is-a-json-schema">What is a JSON Schema?</h2><p>According to the <a href="https://json-schema.org/learn/getting-started-step-by-step?ref=jina-ai-gmbh.ghost.io">official website</a>, JSON Schema is a vocabulary that you can use to annotate and validate JSON documents. With SceneXplain, you can upload a schema like:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;alt_tag&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;description&quot;: &quot;the most concise description possible of the image&apos;s content and function. Do not describe elements that are purely decorative (e.g. part of the website&apos;s design, not content). Do not include text like &apos;this image contains&apos; or &apos;image depicts&apos;&quot;
    }
  }
}</code></pre><p>Along with an image...</p><figure class="kg-card kg-image-card"><img src="https://media.discordapp.net/attachments/1062299968955764777/1177229401763614794/generativejina_Close-up_view_of_a_tall_tree_covered_in_moss_wit_029426a1-96f1-49e9-903d-7772488af950.png?ex=6571bf51&amp;is=655f4a51&amp;hm=6f73c30f7a67b93fa5e5bcb615a047701a6b1ddd4cf3debf39aaf58e0a348033&amp;=&amp;width=625&amp;height=625" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><p></p><p>And get the following output:</p><pre><code class="language-json">{
  &quot;alt_text&quot;: &quot;Close-up view of a tall tree covered in moss with a clear blue sky in the background&quot;
}</code></pre><p>The JSON schema above is pretty straightforward, with only one field. But crafting more complicated JSON schemas can be a lot of effort, even assuming they work well. And sharing them can get messy, having to shuffle code blocks around Slack or other messaging clients (believe me, I&apos;ve been there!)</p><h2 id="how-does-the-schema-store-work">How does the Schema Store work?</h2><p>The Schema Store solves these headaches. Now you copy and use someone else&apos;s Schema without having to go through the bother of crafting your own. Or you can easily share your schemas with colleagues for more efficient image processing.</p><h3 id="use-community-schemas">Use community schemas</h3><p>To get started, create an account on SceneXplain and follow these steps:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h3 id="create-your-own-schema">Create your own schema</h3>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h2 id="examples">Examples</h2><p>Here are a few more examples of what you can do with the schemas from the Store:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/emoji.png" width="585" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/tinder.png" width="590" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/super.png" width="590" height="1231" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/land_use.png" width="584" height="827" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text.png" width="584" height="795" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text_language.png" width="584" height="870" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/brand_sentiment.png" width="597" height="876" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div></div></figure><h2 id="bulk-generating-alt-text-via-scenexplains-api">Bulk-generating alt-text via SceneXplain&apos;s API</h2><p>Now let&apos;s get our hands dirty by calling <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a> to bulk process images using a schema from the Store. In our use case, we&apos;ll perform the simple task of generating alt-text.</p><blockquote>Alt text is used in HTML to describe the appearance and function of an image on a webpage. They are crucial for accessibility, as they provide a text alternative for screen readers used by visually impaired users, and also aid in search engine optimization (SEO) by allowing search engines to better understand the content of the images.</blockquote><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">You can play with the code in <a href="https://www.notion.so/SceneXplain-s-JSON-Schema-Store-Automate-Your-Alt-Text-and-More-418d90b1477f459eb0d517869dcc80cd?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">this notebook</a>. Be sure to specify your own <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain API key</a>.</div></div><p>We won&apos;t go through each individual step in depth, since the notebook handles that. We&apos;ll just give an overview of each step. Please refer to the notebook for the real code.</p><h3 id="choose-the-schema">Choose the schema</h3><p>We&apos;ll use the alt-tagger schema that I created earlier. Be sure to note its ID! In our case that&apos;s <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-61.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="584" height="826"></figure><h3 id="test-the-api">Test the API</h3><p>We can quickly test the API with a code snippet by clicking the API tab:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/api.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="582" height="221"></figure><p>You can access the API in Python, cURL, or JavaScript. Right now we&apos;ll use cURL since it&apos;s nice and short. We&apos;ll send the URL to the following <a href="https://www.pexels.com/photo/heron-by-the-sea-18822188/?ref=jina-ai-gmbh.ghost.io">image</a>, along with our SceneXplain key:</p><figure class="kg-card kg-image-card"><img src="https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=1260&amp;h=750&amp;dpr=1" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><pre><code class="language-bash">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;data&quot;:[
    {&quot;image&quot;: &quot;https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg&quot;,
    &quot;features&quot;: [&quot;json&quot;],
    &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;}
  ]}&apos;</code></pre><p>After a few seconds (and some prettification via <a href="https://github.com/jqlang/jq?ref=jina-ai-gmbh.ghost.io"><code>jq</code></a>), we get the following JSON, which includes our alt-tag:</p><pre><code class="language-json">{
  &quot;code&quot;: 200,
  &quot;status&quot;: 20000,
  &quot;result&quot;: [
    {
      &quot;id&quot;: &quot;BiU7me3ytaKn4KaF0v84&quot;,
      &quot;image&quot;: &quot;https://storage.googleapis.com/causal-diffusion.appspot.com/imagePrompts%2Fe4710bdc-73de-469c-9202-c2c0fe1073af%2Foriginal.png&quot;,
      &quot;features&quot;: [
        &quot;json&quot;
      ],
      &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;,
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;uid&quot;: &quot;NIDud1AA3NMTBFYZ4MEpNZy5om62&quot;,
      &quot;optOut&quot;: false,
      &quot;fullyOptOut&quot;: false,
      &quot;__developer_options&quot;: null,
      &quot;text&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;,
      &quot;i18n&quot;: {
        &quot;en&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;
      },
      &quot;userId&quot;: &quot;foo&quot;,
      &quot;createdAt&quot;: 1700737281840,
      &quot;languages&quot;: []
    }
  ]
}</code></pre><h3 id="collect-your-data">Collect your data</h3><p>Moving forwards, we&apos;ll be using Python for our code. Assuming we have a folder of images, for each image we&apos;ll need to send:</p><ul><li>The image file converted to a <a href="https://en.wikipedia.org/wiki/Data_URI_scheme?ref=jina-ai-gmbh.ghost.io">base64-encoded datauri</a></li><li>The SceneXplain features we want to use. In our case that&apos;s just <code>[&apos;json&apos;]</code>.</li><li>The ID of the JSON Schema: <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</li></ul><p>We throw all of these into a dict, then throw each image&apos;s dict into a list.</p><h3 id="send-the-data-to-scenexplain">Send the data to SceneXplain</h3><p>That&apos;s really just a case of making an HTTP request and sending over our data. We&apos;ve wrapped it into a function in the notebook.</p><h3 id="process-the-output-data">Process the output data</h3><p>This is just a case of extracting the alt-tag from the output JSON, and in our case, writing it to a CSV file then zipping it up along with all the other images. Our <code>alt-text.csv</code> looks like:</p><pre><code class="language-csv">filename,alt-tag
/tmp/tmpexs68in3/free-photo-of-leaves-on-the-branch.jpeg,&quot;Close-up of a branch with mix of green and yellow leaves, portraying the onset of autumn, set against the blurred background of a serene forest.&quot;
/tmp/tmpexs68in3/free-photo-of-holida-christmas-party-drinks-ornaments.jpeg,&quot;Vividly colored table setting with red tablecloth, two empty crystal glasses, Christmas decorations including candy canes, gold ornaments, a small Christmas tree, on a backdrop of a green curtain and pink walls.&quot;
/tmp/tmpexs68in3/free-photo-of-red.jpeg,Close-up of a red classic Malibu car&apos;s rear end.
/tmp/tmpexs68in3/free-photo-of-pose-woman-dress-in-the-desert-gold-light-curly-hair.jpeg,&quot;Curly-haired woman in brown coat standing on beach, with the sun beaming light onto her.&quot;
/tmp/tmpexs68in3/free-photo-of-a-bowl-of-granola-with-fruit-and-nuts-on-a-wooden-cutting-board.jpeg,Bowl of granola with strawberries and pomegranate seeds on a wooden board on a dark brown table
/tmp/tmpexs68in3/free-photo-of-alexandrine-parakeet-in-side-view.png,&quot;Close-up of a green parrot with a red beak and yellow eyes on a branch, looking to the right; with a blurry green background&quot;
/tmp/tmpexs68in3/pexels-photo-12015253.png,&quot;Central, old gas pump with a red and white color scheme labeled &apos;Benzin&apos; with an attached black hose against a street scene backdrop&quot;</code></pre><p>You can view the process in full in <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/a11y-alt-tags/alt-tagger.ipynb?ref=jina-ai-gmbh.ghost.io">the notebook</a>.</p><h2 id="get-started-with-scenexplain-and-the-schema-store">Get started with SceneXplain and the Schema Store</h2><p>Like what you see? Go to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to sign up and get started, and head on over to <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">our Discord</a> to join the conversation!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Artificial General Intelligence is Cursed, And Science Fiction isn't Helping]]></title><description><![CDATA[AI is cursed by intellectual hubris, moving goalposts, bad incentives, and science fiction. Recent talk about Artificial General Intelligence only highlights those curses.]]></description><link>https://jina.ai/news/artificial-general-intelligence-is-cursed-and-science-fiction-isnt-helping/</link><guid isPermaLink="false">655b2361bb728c000101beaa</guid><category><![CDATA[Insights]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 20 Nov 2023 15:22:02 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><p>Many, many, many, (too many <em>many</em>&#x2019;s) years ago, I was a student taking <em>Introduction to Artificial Intelligence</em> at Stanford. <a href="https://en.wikipedia.org/wiki/Daphne_Koller?ref=jina-ai-gmbh.ghost.io">Daphne Koller</a> was my teacher, and my textbook was a first edition <a href="https://aima.cs.berkeley.edu/?ref=jina-ai-gmbh.ghost.io"><em>Artificial Intelligence: A Modern Approach</em></a> by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Stuart Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Peter Norvig</a>, which still sits on the shelf in my office.</p><p>A lot of us working in AI today have that textbook on our shelves.</p><p>That&#x2019;s why a <a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io">recent article in <em>Noema Magazine</em></a> by <a href="https://en.wikipedia.org/wiki/Blaise_Ag%C3%BCera_y_Arcas?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Blaise Ag&#xFC;era y Arcas</a> of Google Research and Peter Norvig, provocatively titled <em>Artificial General Intelligence Is Already Here</em> caught so much attention.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Artificial General Intelligence Is Already Here | NOEMA</div><div class="kg-bookmark-description">Today&#x2019;s most advanced AI models have many flaws, but decades from now, they will be recognized as the first true examples of artificial general intelligence.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.noemamag.com/wp-content/uploads/2020/06/cropped-ms-icon-310x310-1-180x180.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">NOEMA</span><span class="kg-bookmark-publisher">Blaise Ag&#xFC;era y Arcas</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://noemamag.imgix.net/2023/10/Noema_Card-display-2000x1000-0-00-04-04.jpg?fit=crop&amp;fm=pjpg&amp;h=628&amp;ixlib=php-3.3.1&amp;w=1200&amp;wpsize=noema-social-facebook&amp;s=2e1793c1499718ac74749479e3989c10" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><p>To summarize their claim, the core elements of general intelligence are already met by recent large language models: They can perform with human competence at tasks outside of their explicit training over a wide range of domains. Although hardly without flaws, human performance is hardly flawless either. Therefore, we should acknowledge that these systems have achieved &#x201C;general intelligence.&#x201D;</p><p>The rest of the article is, frankly, mostly taking down strawmen and rehashing old fights from the 90s. Honestly, the people who graduated from university in the last twenty years never fought in the Great Symbolicist-Connectionist War. That conflict ended a long time ago, and any Symbolicist bitter-enders who&#x2019;ve survived into the age of GPT should be allowed to stay in their thatched jungle huts, collecting nuts and fruits and proclaiming their loyalties to Fodor, Pinker, and Chomsky somewhere far away where it doesn&#x2019;t have to bother anyone.</p><p>I fought in that conflict (for the Connectionists, thank you), and on behalf of myself and the other veterans, please give it a rest! (<a href="http://garymarcus.com/index.html?ref=jina-ai-gmbh.ghost.io">Gary Marcus</a>, I mean you too!)</p><p>&#x201C;Artificial General Intelligence&#x201D; (AGI) is, to put it mildly, a controversial concept in AI research. The word is of recent vintage, dating to about the turn of the 21st century, and comes from an attempt to make clearer an idea that has circulated in AI since, arguably, before the invention of computers. It&#x2019;s gone by a lot of names over the years: &#x201C;true AI,&#x201D; &#x201C;real AI&#x201D;, &#x201C;strong AI,&#x201D; &#x201C;human-equivalent AI,&#x201D; etc. The history of attempts to define it in some testable way is, to say the least, not encouraging, but it doesn&#x2019;t seem to stop anyone from trying.</p><h2 id="the-curses-of-ai">The Curses of AI</h2><p>Artificial intelligence is a cursed field.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="1050" height="1050" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1050w" sizes="(min-width: 720px) 720px"></figure><p>The word was coined by <a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)?ref=jina-ai-gmbh.ghost.io">John McCarthy</a> in 1956, in large part to distinguish it from &#x201C;cybernetics,&#x201D; a label associated with <a href="https://en.wikipedia.org/wiki/Norbert_Wiener?ref=jina-ai-gmbh.ghost.io">Norbert Wiener</a>, who saw it as an interdisciplinary field with political implications that did not appeal to McCarthy and his colleagues. So, from the very beginning, AI has been associated with intellectual hubris and disinterest in social consequences, willfully distancing itself from the humanities &#x2014; the study of human works &#x2014; while purporting to build machines that do the work of humans.</p><p>That is one of AI&apos;s great curses: There is an expansive, argumentative, routinely acrimonious literature on the nature of intelligence and human intellectual abilities, dating back centuries and even millennia, and discussions of artificial intelligence reference it approximately never. How exactly are you going to discuss &#x201C;artificial intelligence&#x201D; if you can&#x2019;t say what it means for something to be intelligent, or how you would demonstrate that?</p><p>Ag&#xFC;era y Arcas of Google and Norvig are not especially clear on how they&#x2019;ve defined &#x201C;general intelligence&#x201D; despite having a section titled <em>&#x201C;What Is General Intelligence?&#x201D;</em></p><p>Broadly, their claim seems to be:</p><ol><li>General intelligences can do more than one kind of thing.</li><li>General intelligences can do things that their designers didn&#x2019;t specifically intend for them to be able to do.</li><li>General intelligences can do things that are specified in your instructions to them rather than their programming.</li></ol><p>This is my summary, and perhaps I have misunderstood them, but the section where they explain this goes on a number of tangents that make it hard to identify what they intend to say. They appear to be using the label &#x201C;general intelligence&#x201D; to contrast with &#x201C;narrow intelligence,&#x201D; another new term (I can&#x2019;t find reference to it before the 2010s) that seems to have the same relationship to what we used to call &#x201C;weak AI&#x201D; as &#x201C;general intelligence&#x201D; has to &#x201C;strong AI.&#x201D;</p><p>They define it as follows:</p><blockquote>Narrowly intelligent systems typically perform a single or predetermined set of tasks, for which they are explicitly trained. Even multitask learning yields only narrow intelligence because the models still operate within the confines of tasks envisioned by the engineers.</blockquote><p>They contrast this with &#x201C;general intelligence&#x201D;:</p><blockquote>By contrast, frontier language models can perform competently at pretty much any information task that can be done by humans, can be posed and answered using natural language, and has quantifiable performance.</blockquote><p>This approach poses some obvious problems. It defines artificial general intelligence to exist only when engineers have poor imaginations. That doesn&#x2019;t seem quite right.</p><p>The most frequently cited example of an AI model performing outside of its explicit training (a.k.a. <em>zero-shot learning</em>) is the &#x201C;zebra&#x201D; example: Let us say we have a multimodal AI model that has never had a picture of a zebra or mention of a zebra anywhere in its training data. So we tell it that a zebra is a horse with stripes, and then we present it with pictures, some of zebras, some not, and ask it if each one is a zebra.</p><p>The current generation of multimodal models is capable of doing a good job of this.</p><p>I don&#x2019;t know if this is exactly what Ag&#xFC;era y Arcas and Norvig have in mind as performing outside of &#x201C;the confines of tasks envisioned by the engineers&#x201D; because they give no examples, nor does the article they link to on this subject. (&#x201D;A general AI model can perform tasks the designers never&#xA0;<a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/?ref=jina-ai-gmbh.ghost.io">envisioned</a>.&#x201D;) But this kind of zero-shot learning does seem to be what they mean.</p><p>However, it&apos;s not clear that this really is an example of going outside of the tasks envisioned by the model&apos;s designers. Given that the model was explicitly trained to recognize horses and stripes and to connect the features it finds in images to natural language words and statements, it was explicitly trained to handle all parts of the problem. So is it right to say it&#x2019;s exceeded the bounds of &#x201C;narrow intelligence?&#x201D;</p><p>Other zero-shot learning examples suffer from the same problem.</p><p>Also, unexpected correct results from computers have been around for some time. As an example, <a href="https://www.jpl.nasa.gov/news/remote-agent-experiment-meets-all-objectives?ref=jina-ai-gmbh.ghost.io">this article from 1999</a> shows a logistic planner in a NASA space probe doing something that not only was correct and unexpected but that the engineers could never have had in mind when they designed it.</p><p>But there&#x2019;s an even larger problem with defining artificial general intelligence this way.</p><p>Let&#x2019;s consider the large language models, like ChatGPT. What were the tasks its engineers designed it to do? Only one task: Add the next word to a text.</p><p>The web interface to ChatGPT obscures what happens behind the scenes, but ChatGPT works like this:</p><blockquote>Iteration 1:<br>    User Input: <strong>Knock, knock.</strong> <br>    GPT Output: <strong>Who&#x2019;s</strong><br><br>Iteration 2:<br>    GPT Input: <strong>Knock, knock. Who&#x2019;s</strong> <br>    GPT Output: <strong>there</strong><br><br>Iteration 3: <br>    GPT Input: <strong>Knock, knock. Who&#x2019;s there</strong> <br>    GPT Output: <strong>?</strong><br><br>Return to user: <strong>&#x201C;Who&#x2019;s there?&#x201D;</strong></blockquote><p>That&#x2019;s all it does! You see it writing poetry, answering questions, or whatever else you like, but under the surface, it&#x2019;s a sequence of identical simple operations to append a word to a text.</p><p>We could say that ChatGPT does just one task, the one it was explicitly designed for.</p><p>So, is this a single-task ability or multi-task abilities? Is it doing more than the engineers designed it to do, or only exactly the one thing they designed it to do?</p><p>It depends on how you choose to look at it. This kind of ambiguity is not trivial to resolve.</p><p>The article highlights &#x201C;[a] healthy skepticism about metrics for AGI&#x201D; as a legitimate concern. However, the problem with &#x201C;general intelligence&#x201D; isn&#x2019;t metrics and thresholds. It&#x2019;s definitions. AI&#x2019;s lack of clear definitions of its terms is the direct cause of AI&#x2019;s second great curse: Moving goalposts.</p><p>Goalpost-moving is so central to AI as an enterprise that Professor Koller introduced me to it in her introductory class long ago, I think on, like, Week 3.</p><p>Every time researchers solve some problem in AI, it&#x2019;s redefined as not needing intelligence. As <a href="https://en.wikipedia.org/wiki/Pamela_McCorduck?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Pamela McCorduck</a> puts it in her influential history of AI:</p><blockquote>Practical AI successes, computational programs that actually achieved intelligent behavior, were soon assimilated into whatever application domain they were found to be useful, and became silent partners alongside other problem-solving approaches, which left AI researchers to deal only with the &#x201C;failures,&#x201D; the tough nuts that couldn&#x2019;t yet be cracked. Once in use, successful AI systems were simply considered valuable automatic helpers. [<a href="https://doi.org/10.1201/9780429258985?ref=jina-ai-gmbh.ghost.io">McCorduck 2004</a>]</blockquote><p>Or, in shorter words, attributed to <a href="https://en.wikipedia.org/wiki/Larry_Tesler?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Larry Tesler</a> (the inventor of copy-paste functionality):</p><blockquote>AI is whatever hasn&apos;t been done yet.</blockquote><p>AI is a <a href="https://en.wikipedia.org/wiki/Red_Queen%27s_race?ref=jina-ai-gmbh.ghost.io">Red Queen&#x2019;s Race</a> where no matter how fast you run, the finish line moves away even faster. The history of AI is a history of failure because we excise the successes from it. Never mind that AI research was already years ago bringing us decent spellcheckers, functional machine translations, and visual object recognition, among other accomplishments of modern technology.</p><p>Or that half the world today runs on logistic planners that started life in the 1960s as AI research in automated theorem proving. Yes, your &#x201C;supply chain&#x201D; is already AI from end to end. AI didn&#x2019;t get rid of the drudgery of trucking and manually loading and unloading goods. It got rid of the comfy office job of scheduling.</p><p>In the history of AI goalpost-moving, Ag&#xFC;era y Arcas and Norvig&#x2019;s contribution is somewhat novel: We&#x2019;re used to moving the goalposts of &#x201C;real&#x201D; AI to make them ever harder to reach. They&#x2019;ve moved them to say we&#x2019;ve already passed them.</p><p>The cause of the moving goalposts is more complex: AI is cursed with bad incentives. It lends itself to hype. We might blame this on the grants process and academic pressures to overpromise, or on late capitalism and the way the tech investor class likes to chase after the latest shiny thing in hopes of owning a &#x201C;disruptive&#x201D; new technology.</p><p>But I put a lot of the blame on AI&#x2019;s final curse: Science fiction.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 700w"></figure><p>Now, understand me: This is not a criticism of the makers of science fiction; it&#x2019;s a criticism of its consumers. I have been watching and reading science fiction since I was five years old, and I love it. It&#x2019;s a popular genre, beloved by millions, if not billions, of people.</p><p>But the ideas people have in their heads about AI, including the people who do AI research, have been so thoroughly colonized by fictitious machines that it has become difficult to separate legitimate debates from the dramatic visions of novelists and movie makers.</p><p>Ideas about &#x201C;artificial general intelligence&#x201D; or &#x201C;real&#x201D; AI owe far more to science fiction than to science. It&#x2019;s not the responsibility of the Issac Asimovs, the Stanley Kubriks, or the James Camerons of the world to police their imaginations or ours. It is the responsibility of AI professionals &#x2014; researchers, marketers, investors, and corporate frontmen &#x2014; to understand and make clear that &#x201C;<a href="https://en.wikipedia.org/wiki/Positronic_brain?ref=jina-ai-gmbh.ghost.io">positronic brains</a>,&#x201D; <a href="https://en.wikipedia.org/wiki/HAL_9000?ref=jina-ai-gmbh.ghost.io">HAL 9000</a>, and <a href="https://en.wikipedia.org/wiki/The_Terminator?ref=jina-ai-gmbh.ghost.io">killer robots with inexplicable Austrian accents</a> have no place in engineering.</p><p>We move the goalposts in AI because every accomplishment ends up disappointing us when it doesn&#x2019;t live up to our vision of AI. Rational beings might interrogate their visions of AI rather than move around the goalposts and say: &#x201C;This time, it&#x2019;s different!&#x201D;</p><p>Humans have deep prejudices that they struggle to recognize. Ag&#xFC;era y Arcas and Norvig recognize this in their mention of the &#x201C;<a href="https://en.wikipedia.org/wiki/Being_There?ref=jina-ai-gmbh.ghost.io">Chauncey Gardiner effect</a>.&#x201D; For younger readers who don&#x2019;t know the works of Peter Sellers and Hal Ashby, we might also call this the &#x201C;Rita Leeds effect&#x201D; after Charlize Theron&#x2019;s role in <a href="https://en.wikipedia.org/wiki/Arrested_Development_(season_3)?ref=jina-ai-gmbh.ghost.io">season three of <em>Arrested Development</em></a>. It&#x2019;s the same schtick.</p><p>Until the 1970s, there were serious social scientists who believed that black people, at least American ones, were systematically intellectually stunted and pointed to the way they spoke English as proof. Even today, plenty of people view African American Vernacular English (usually abbreviated as AAVE because who has time to spell that out?) as &#x201C;broken&#x201D; or &#x201C;defective&#x201D; or indicative of poor education.</p><p>Among linguists, no one of any importance today thinks that. We recognize AAVE as a fully functional variant form of English, just as well-suited to accurate and effective communication as standard English or any of its other variants. It has no relationship to cognitive abilities whatsoever.</p><p>Yet, the prejudice against it remains.</p><p>When we evaluate ChatGPT as &#x201C;real&#x201D; AI, or at least on the path to &#x201C;real&#x201D; AI, we are demonstrating the same prejudice, turned around. That should give us some pause. It should lead us to ask if <em>anything</em> we think makes software intelligent is really intelligence or just manipulating our prejudices. Is there any way to objectively judge intelligence at all? We may never be able to fully dismiss our prejudices.</p><p>It does not take a lot of interaction with ChatGPT to see it fall short of our science-fiction-based visions for AI. It can do some neat tricks, maybe even some useful ones, but it&#x2019;s not <a href="https://en.wikipedia.org/wiki/WALL-E?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">WALL-E</a>, or <a href="https://en.wikipedia.org/wiki/C-3PO?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">C-3PO</a>, or <a href="https://en.wikipedia.org/wiki/Data_(Star_Trek)?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Commander Data</a>. It&#x2019;s not even Arnold Schwarzenegger&apos;s iconic but not terribly verbal T-800, although I don&#x2019;t doubt you could readily integrate ChatGPT with a text-to-speech model that has a thick Austrian accent and pretend.</p><p>Changing the labels on things doesn&#x2019;t change things, at least not in any simple way. We could move the goalposts for &#x201C;artificial general intelligence&#x201D; in such a way that we could say we&#x2019;ve already accomplished that, but that will not make today&#x2019;s AI models more satisfactory to people looking for HAL 9000. It would just render the label &#x201C;general intelligence&#x201D; meaningless.</p><p>People would still want something we can&#x2019;t give them. Something we might never be able to give them.</p><h2 id="are-humans-still-special">Are Humans Still Special?</h2><p>Among the strawmen Ag&#xFC;era y Arcas and Norvig take on in a whole section is <em>&#x201C;Human (Or Biological) Exceptionalism.&#x201D;</em> People believe all kinds of things, but unless you count religious notions of the &#x201C;soul&#x201D;, it is hard to believe this is a commonly held position among AI professionals. More likely, most of them don&#x2019;t think about it at all.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 700w"></figure><p>The lesson we should be learning from recent AI is that it is difficult to identify a human skill that is absolutely impossible for a machine to perform. We cannot make AI models do everything a human can do yet, but we can no longer rule anything out.</p><p>This realization should end some of the bad ideas we have inherited from the past. We know today that animals have languages much like our own and that computers can learn language with a shocking facility. Noam Chomsky may not be dead yet, but ChatGPT has put the final nails in the coffin of his life&#x2019;s work.</p><p>We have known since Galileo that humanity does not live in the center of the universe. We have known for roughly two centuries that the universe existed for billions of years before people. We have known for some time that humans are not made special by the use of reason and language, as Descartes thought, as we find both among animals and note their frequent absence in humans.</p><p>AI research is removing the idea that humans are special because of some trick we can do, but that does not remove the specialness of humanity. Consider that we celebrate people who can run a mile in under four minutes. Yet, my 20-year-old cheap French-made car can do a mile in about 30 seconds if I really put my foot on the pedal.</p><p>We care about human accomplishment in ways that differ from how we care about pure performance benchmarks. We&#x2019;ll still prefer our human performers over AI, for the same reason I don&#x2019;t win Olympic medals with my old Citro&#xEB;n.</p><p>We imagine AI taking away the drudgery of human life, but we do not yet have robots that can restock shelves at Walmart. On the other hand, we now have AI models that can sing like <a href="https://en.wikipedia.org/wiki/%C3%89dith_Piaf?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Edith Piaf</a> and, if someone really tried, could probably dance like <a href="https://en.wikipedia.org/wiki/Shakira?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Shakira </a>or play the guitar like <a href="https://en.wikipedia.org/wiki/Jimi_Hendrix?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jimi Hendrix</a>.</p><p>This is progress. It&#x2019;s an accomplishment of significant value. But it&#x2019;s not the AI we wanted. Unless we rethink the whole program of AI research, we&#x2019;ll need to move the goalposts again.</p><h2 id="real-ai-instead-of-%E2%80%9Creal%E2%80%9D-ai">Real AI instead of &#x201C;Real&#x201D; AI</h2><p>Jina AI is a business. We don&#x2019;t sell fantasy, we sell AI models. There&#x2019;s no place for &#x201C;artificial general intelligence&#x201D; in what we do. We may love science fiction, but we deal in science fact.</p><p>&#x201C;Past performance is not a guarantee of future returns.&#x201D; Maybe it&#x2019;ll be different this time. Maybe we won&#x2019;t dismiss our successes as not &#x201C;real&#x201D; intelligence. Maybe we&#x2019;ll get over our science fiction dreams for once. Maybe this hype cycle won&#x2019;t lead to another AI Winter.</p><p>Maybe, but I won&#x2019;t bet on it.</p><p>Someday, we might have something that looks more like our visions of &#x201C;real&#x201D; AI. Perhaps in 50 years? 100? History does not encourage optimism about it. But regardless, it won&#x2019;t happen in a timeframe that pays any bills.</p><p>Moving around the goalposts of AI won&#x2019;t change that.</p><p>Our interest is business cases. We want AI to add to your bottom line. At Jina AI, we want to hear about your business processes and discuss how really existing AI models can fit into them. We know the limits of this technology intimately, but we also know its genuine potential.</p><p>Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to talk about what <em>real </em>AI can do for your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3602 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina AI’s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment]]></title><description><![CDATA[Jina AI launches 8K token length embedding v2 models on AWS marketplace, elevating enterprise AI deployments with EU-engineered innovation and a commitment to data sovereignty.]]></description><link>https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/</link><guid isPermaLink="false">65577ae1bb728c000101be6c</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 20 Nov 2023 14:32:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><p><strong>Berlin, Germany - November 20, 2023 </strong>- Catering to enterprise customers, Jina AI has released <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings v2</a> on AWS SageMaker, a milestone in accessible, top-tier AI solutions. Enterprise users can now search for <code>jina-embeddings-v2-base/small</code> on the AWS Marketplace and deploy them directly to their own AWS accounts. As a part of the AWS Startups program, this release underscores the collaboration between Jina AI&apos;s innovation and AWS&apos;s commitment to supporting groundbreaking startups, marking a significant advancement in AI development.</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png" class="kg-image" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment" loading="lazy" width="2000" height="1556" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-60.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-60.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-60.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png 2134w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Enterprise users can now search jina-embeddings-v2-base/small on the AWS marketplace and deploy them directly on their own AWS account.</span></figcaption></figure><h2 id="superior-models-on-a-robust-platform">Superior Models on a Robust Platform</h2><ul><li><em>SageMaker Integration</em>: With global availability on the AWS SageMaker Marketplace, Jina AI underscores its dedication to enterprise users, providing them with an effortless way to build applications using our advanced embedding models.</li><li><em>Seamless Deployment</em>: Enterprises can now easily deploy Jina Embedding v2 models as SageMaker endpoints, bypassing the complexity associated with custom infrastructure setups.</li><li><em>Cost-Effective Licensing</em>: The English <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">base</a> and <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">small</a> models are available <em>without licensing fees</em>. Clients incur costs only for their AWS instances, ensuring a privacy-first, cost-effective solution within their VPC.</li></ul><h3 id="tailored-solutions-for-varied-use-cases">Tailored Solutions for Varied Use Cases</h3><ul><li><em>Model Diversity</em>: With a <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">0.27GB base model</a> and a <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">0.07GB small model</a>, Jina AI provides tailored solutions for various needs, from in-depth analytics to lightweight applications.</li><li><em>Use Cases</em>: The base model is designed for comprehensive semantic representation, ideal for enterprise search and content discovery, while the small model caters to mobile and edge devices, optimizing for speed and efficiency.</li></ul><p>Commenting on this significant milestone, Dr. Han Xiao, CEO of Jina AI, offered the following insights:</p><blockquote>Launching Jina AI&apos;s 8K Context Length v2 Embedding Models on AWS Marketplace, we advance industry standards for private AI solutions. Developed in Germany, this pivotal release emphasizes data sovereignty and customer-centric innovation, addressing today&apos;s needs and shaping future secure, private AI deployments.</blockquote><p>Jina AI aims to make continuous strides towards privacy-aware, state-of-the-art AI, as evident from its plans.  </p><h2 id="why-jina-embeddings-v2-a-leap-in-ai-capability">Why Jina Embeddings v2: A Leap in AI Capability</h2><ul><li><em>Extended Context Length</em>: Jina Embeddings v2 models support an <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">unprecedented 8K</a> (8192 tokens) context length, allowing for a full understanding of longer documents.</li><li><em>Open Source Pioneer</em>: Jina AI takes pride in offering the only open-source model with a context length that matches OpenAI&#x2019;s proprietary models, broadening access to advanced AI.</li><li><em>Benchmark Leadership</em>: On the Massive Text Embedding Benchmark (MTEB) <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">leaderboard</a>, our models boast performance on par with industry-leading models, attesting to our commitment to excellence.</li></ul><h3 id="performance-vs-openais-text-embedding-ada002">Performance vs. OpenAI&apos;s text-embedding-ada002</h3><p>Below is a comparative performance snapshot that showcases the robust capabilities of Jina Embeddings v2:</p>
<!--kg-card-begin: html-->
<table id="e6f57e0d-fe4c-414f-8c0f-5081d4ad007d" class="simple-table"><tbody><tr id="29fae3d0-ca66-462f-8966-b3c1b74c9a86"><td id="bT&lt;r" class>Model</td><td id="hcDn" class>text-embedding-ada-002</td><td id="gMgn" class>jina-embeddings-v2-base-en</td></tr><tr id="5cff5519-ba19-40a8-9900-4488689c0a84"><td id="bT&lt;r" class>Rank</td><td id="hcDn" class><strong>18</strong></td><td id="gMgn" class>21</td></tr><tr id="f01a5da8-92d0-4a76-8e1c-8fb77c9ac6c8"><td id="bT&lt;r" class>Model Size (GB)</td><td id="hcDn" class>Unknown</td><td id="gMgn" class>0.27</td></tr><tr id="56cc4ec4-a35e-44f4-8f2a-5243a8c4331b"><td id="bT&lt;r" class>Average (56 datasets)</td><td id="hcDn" class><strong>60.99</strong></td><td id="gMgn" class>60.38</td></tr><tr id="ca2d0ce9-9157-46c5-a9c7-9179edea43ee"><td id="bT&lt;r" class>Embedding Dimensions</td><td id="hcDn" class>1536</td><td id="gMgn" class><strong>768</strong></td></tr><tr id="8c7ecb47-1d30-4c32-8a23-89dadfbbfc38"><td id="bT&lt;r" class>Sequence Length</td><td id="hcDn" class>8191</td><td id="gMgn" class>8192</td></tr><tr id="a0398e1a-3144-4ec4-bb36-8768488b3d8a"><td id="bT&lt;r" class>Classification Average (12 datasets)</td><td id="hcDn" class>70.93</td><td id="gMgn" class><strong>73.45</strong></td></tr><tr id="d5a7ccfb-7769-4d58-bad4-a219481876ff"><td id="bT&lt;r" class>Pair Classification Average (3 datasets)</td><td id="hcDn" class>84.89</td><td id="gMgn" class><strong>85.38</strong></td></tr><tr id="461eb054-e39a-4ec5-8c33-cc880cb9f2ac"><td id="bT&lt;r" class>Summarization Average (1 dataset)</td><td id="hcDn" class>30.8</td><td id="gMgn" class><strong>31.6</strong></td></tr><tr id="5f55e5e5-a6dc-4a4a-b67d-4ffed0084989"><td id="bT&lt;r" class>Retrieval Average (15 datasets)</td><td id="hcDn" class><strong>49.25</strong></td><td id="gMgn" class>47.87</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Jina AI&apos;s base model excels particularly in Classification and Pair Classification tasks, underscoring its value in diverse applications ranging from document analysis to recommendation systems.</p><h2 id="get-started-with-jina-embeddings-v2-on-aws">Get Started with Jina Embeddings v2 on AWS</h2><p>To begin using Jina Embeddings v2, visit the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS Marketplace listings</a> and select the model that best fits your needs.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a></figure><p>These sample notebooks can help users get started with Jina Embeddings v2 models:</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Real-time%20inference.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Real-time inference.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Real-time inference on SageMaker</span></p></figcaption></figure><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Batch%20transform.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Batch transform.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Embedding multiple sentences in a batch with SageMaker</span></p></figcaption></figure><h2 id="coming-soon-multilingual-embeddings-and-more">Coming Soon: Multilingual Embeddings and More</h2><p>Looking ahead, Jina AI is already deep in developing multilingual embedding models, making them available to its enterprise clients for private deployment on various cloud service providers (CSPs). With the imminent launch of these models, Jina AI is set to bridge language barriers, unlocking global opportunities for its clients.</p><h2 id="about-jina-ai-gmbh">About Jina AI GmbH</h2><p>Located at <a href="https://maps.app.goo.gl/rAZ1QiqoKp49KZAg6?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany</a>, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at&#xA0;<a>contact@jina.ai</a>.<br></p>]]></content:encoded></item></channel></rss>
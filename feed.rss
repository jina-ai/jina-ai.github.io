<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.80</generator><lastBuildDate>Mon, 11 Mar 2024 23:37:05 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Jina 3.24.0 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-24-0-update/</link><guid isPermaLink="false">65eedc3c28f704000180e2dc</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 11 Mar 2024 10:30:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3240">Release Note (<code>3.24.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/Image-Jina-dark.jpg" alt="Jina 3.24.0 Update"><p>This release contains 1 new feature.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.24.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v3.24.0 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.24.0) Release time: 2024-03-11 09:48:35 This release contains 1 new feature.
&#x1F195; Features
Support provider endpoint in Jina Executor (#6149)
This release adds the --provider-endpoi&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.24.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/4744775b0fe508673f9bad61ba6ed377883f4ebe462333dd41f8fadc4cc350d8/jina-ai/jina/releases/tag/v3.24.0" alt="Jina 3.24.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="support-provider-endpoint-in-jina-executor-6149">Support provider endpoint in Jina Executor (<a href="https://github.com/jina-ai/jina/pull/6149?ref=jina-ai-gmbh.ghost.io">#6149</a>)</h3><p>This release adds the&#xA0;<code>--provider-endpoint</code>&#xA0;flag, which can only be used in conjunction with the&#xA0;<code>--provider</code>&#xA0;flag. The&#xA0;<code>--provider-endpoint</code>&#xA0;flag takes as a parameter a named endpoint supported in the Executor and sets the endpoint of the provider to map to that Executor endpoint.</p><p>At present, SageMaker is the only available provider option, so using the parameters&#xA0;<code>--provider SAGEMAKER --provider-endpoint &lt;named endpoint&gt;</code>&#xA0;will make the SageMaker&#xA0;<code>invocations</code>&#xA0;endpoint map to the named endpoint of the Executor.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Zac Li (<a href="https://github.com/zac-li?ref=jina-ai-gmbh.ghost.io">@zac-li</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Precise RAG with Jina Reranker and LlamaIndex]]></title><description><![CDATA[Just Rerank It! Jina Reranker and LlamaIndex take your RAG up to the next level.]]></description><link>https://jina.ai/news/precise-rag-with-jina-reranker-and-llamaindex/</link><guid isPermaLink="false">65e97909b22368000152a4c7</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Francesco Kruk]]></dc:creator><pubDate>Thu, 07 Mar 2024 14:00:01 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/03/banner_Reranker_-LlamaIndex.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/03/banner_Reranker_-LlamaIndex.png" alt="Precise RAG with Jina Reranker and LlamaIndex"><p>While <a href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><u>bi-encoder models</u></a> such as <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings</u></a> can quickly retrieve many matching documents from a database of pre-computed embeddings, reranker models can refine this set by using a slower but more precise approach of cross-encoding users&#x2019; queries and retrieved documents. Jina AI has <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><u>released our first reranker model</u></a>, <code>jina-reranker-v1-base-en</code>, and, in this article, we&#x2019;ll present in-depth reasoning for why a reranker is essential to optimize RAG accuracy and how to get started building a state-of-the-art RAG system using <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><u>Jina Embeddings</u></a>/<a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>Reranker</u></a>,<a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"> <u>LlamaIndex</u></a>, and the <code>Mixtral-8x7B-Instruct-v0.1</code> language model (<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io"><u>hosted on HuggingFace</u></a>).</p><p>You&#x2019;ll need:</p><ul><li>A <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>combined key for Jina Embeddings and Reranker API</u></a>.</li><li>A <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"><u>HuggingFace account and token</u></a>.</li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><p>Since the Jina Embeddings and Reranker models as well as Mixtral run remotely and are accessed via a RESTful API, you won&#x2019;t need any special hardware.</p><h2 id="what-is-a-reranker">What is a reranker?</h2><p>Before continuing with the tutorial, it is important to highlight what rerankers are in the first place. For a full understanding of what a reranker is and why Jina Reranker V1 is the best choice for you, we encourage you to read our Jina Reranker V1 release post before continuing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class="kg-bookmark-description">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><p>In summary, rerankers are cross-encoder models that take as input a <em>document-query</em> pair, and emit a combined relevance score for that input pair. Using rerankers, users can sort documents from most to least relevant for a given query.</p><h2 id="why-use-jina-reranker-v1-base-en">Why use <code>jina-reranker-v1-base-en</code>?</h2><p>Reranking provides much more relevant information than using solely an embedding model. In our <a href="https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/?ref=jina-ai-gmbh.ghost.io"><u>model release post</u></a>, we demonstrated that Jina Reranker stands out compared to its open- and closed-source competitors and it can improve search systems by <strong>8% in hit rate and 33% in </strong><a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank?ref=jina-ai-gmbh.ghost.io"><strong><u>mean reciprocal rank</u></strong></a>.</p><p>This has a direct impact on the quality of responses obtained through the applied RAG solution. With the theory backing up this claim, we&#x2019;ll show you a practical example so you can see with your own eyes what effect Jina Reranker has on a RAG pipeline built with LlamaIndex.</p><h2 id="before-we-start-a-note-on-llamaindex-node-postprocessors">Before we start: A Note on LlamaIndex Node-Postprocessors</h2><p>Node-postprocessors in <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html?ref=jina-ai-gmbh.ghost.io"><u>LlamaIndex</u></a> are modules that transform or filter nodes after retrieval and before response synthesis within a query engine. As part of this package, LlamaIndex offers both built-in options as well as an API for custom additions.</p><p><strong>Jina Reranker has now been </strong><a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank.html?ref=jina-ai-gmbh.ghost.io"><strong><u>integrated</u></strong></a><strong> into LlamaIndex as a node postprocessor</strong>. To increase response accuracy, retrieved nodes are re-ordered based on relevance to the query, and the top <strong><em>N</em></strong> nodes are returned.</p><h2 id="follow-along-on-google-colab">Follow along on Google Colab</h2><p>This tutorial has an accompanying notebook that you can run on <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/JinaRerank.ipynb?ref=jina-ai-gmbh.ghost.io"><u>Google Colab</u></a> or locally.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/JinaRerank.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">llama_index/docs/examples/node_postprocessor/JinaRerank.ipynb at main &#xB7; run-llama/llama_index</div><div class="kg-bookmark-description">LlamaIndex is a data framework for your LLM applications - run-llama/llama_index</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Precise RAG with Jina Reranker and LlamaIndex"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">run-llama</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/e7094a0ef89361df6896a5e036ff16eba94b83aa69186423d6b06608a46e7707/run-llama/llama_index" alt="Precise RAG with Jina Reranker and LlamaIndex"></div></a></figure><h3 id="the-dataset-2024-nike-kids-product-catalog">The dataset: 2024 Nike Kids Product Catalog</h3><p>To showcase Jina Reranker&#x2019;s performance increase for RAG applications, we&#x2019;ve chosen the <a href="https://niketeam-asset-download.nike.net/catalogs/2024/2024_Nike%20Kids_02_09_24.pdf?cb=09302022&amp;ref=jina-ai-gmbh.ghost.io"><u>2024 Nike Kids Product Catalog</u></a> as our dataset. The document contains a structured set of kids&#x2019; products offered by Nike in 2024. We selected this dataset as it showcases the effect of using a reranker clearly and is relatable to most users.</p><h3 id="install-the-prerequisites">Install the prerequisites</h3><p>To install the requirements, run:</p><pre><code class="language-bash">pip install llama-index-postprocessor-jinaai-rerank
pip install llama-index-embeddings-jinaai
pip install llama-index
pip install llama-index-llms-huggingface
pip install &quot;huggingface_hub[inference]&quot;</code></pre><h3 id="access-mixtral-llm">Access Mixtral LLM</h3><p>To use the <code>Mixtral-8x7B-Instruct-v0.1</code> LLM, you need a<a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"> <u>HuggingFace</u></a> token.</p><pre><code class="language-Python">from llama_index.llms.huggingface import HuggingFaceInferenceAPI

hf_inference_api_key = &quot;&lt;your HuggingFace access token here&gt;&quot;

mixtral_llm = HuggingFaceInferenceAPI(
    model_name=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;,
    token=hf_inference_api_key,
)
</code></pre><h3 id="access-jina-embeddings-and-jina-reranker">Access Jina Embeddings and Jina Reranker</h3><p>To use our Jina Embeddings and Jina Reranker, you need a dedicated <a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io"><u>API key</u></a>. Store it in a variable called <code>api_key</code> and call the Jina Embeddings model from LlamaIndex:</p><pre><code class="language-Python">from llama_index.embeddings.jinaai import JinaEmbedding

api_key = &quot;&lt;your Jina key here&gt;&quot;
jina_embeddings = JinaEmbedding(api_key=api_key)</code></pre><p>Similarly, you can call the Jina Reranker model. By setting the <code>top_n</code> parameter, you can decide how many of the most relevant documents to return in the final output. In this case, we set <code>top_n=2</code>:</p><pre><code class="language-Python">from llama_index.postprocessor.jinaai_rerank import JinaRerank

jina_rerank = JinaRerank(api_key=api_key, top_n=2)</code></pre><h3 id="download-the-2024-nike-kids-product-catalog">Download the 2024 Nike Kids Product Catalog</h3><p>To download the data, run the following code:</p><pre><code class="language-Python">from llama_index.core import SimpleDirectoryReader
import requests

url = &apos;&lt;https://niketeam-asset-download.nike.net/catalogs/2024/2024_Nike%20Kids_02_09_24.pdf?cb=09302022&gt;&apos;
response = requests.get(url)

with open(&apos;Nike_Catalog.pdf&apos;, &apos;wb&apos;) as f:
&#xA0;&#xA0;&#xA0;&#xA0;f.write(response.content)

reader = SimpleDirectoryReader(
&#xA0;&#xA0;&#xA0;&#xA0;input_files=[&quot;Nike_Catalog.pdf&quot;]
)

documents = reader.load_data()</code></pre><h3 id="generate-and-index-embeddings-with-jina-embeddings">Generate and index embeddings with Jina Embeddings</h3><p>Now that the setup is complete, we&#x2019;ll generate the embedding vectors (nodes) and index them. Jina Embeddings v2 models accept input of up to 8192 tokens, large enough that for a document like this, we don&#x2019;t need to do any further text segmentation or check if any section has too many tokens. To embed and index the document, run the following code:</p><pre><code class="language-Python">from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(
&#xA0;&#xA0;&#xA0;&#xA0;documents=documents, embed_model=jina_embeddings
)</code></pre><h3 id="query-for-results-without-jina-reranker">Query for results without Jina Reranker</h3><p>When we query for specific information from this set of texts, the LlamaIndex <code>query_engine</code> does the following:</p><ol><li>With Jina Embeddings V2, it creates an embedding for the query.</li><li>It uses the index to get the <code>top_k = 10</code> stored embeddings with the highest cosine to the query embedding and return its place in the index.</li><li>It will look up the corresponding text in the vector data array.</li></ol><p>Let&#x2019;s ask what the best Nike jersey is in terms of fabric:</p><pre><code>query_engine = index.as_query_engine(
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=10, llm=mixtral_llm
)
response = query_engine.query(
&#xA0;&#xA0;&#xA0;&#xA0;&quot;What are the best padded pants that Nike sells?&quot;,
)

print(response.source_nodes[0].text)</code></pre><p><strong>Result</strong>:</p><pre><code class="language-Text">NIKE KIDS&#xA0; EQUIPMENT87NIKE BRASILIA SMALL DUFFEL 9.5&#xA0;
DM3976 $37.00
SIZES: Misc&#xA0; OFFER DATE: 07/01/22&#xA0; END DATE: 07/01/25
Tough 600D polyester &#x2022; Durable 300D polyester &#x2022; Detachable shoulder
strap&#xA0;&#x2022; Ventilated shoe or wet/dry storage &#x2022; Secure zip pocket &#x2022; 
Limited lifetime guarantee&#xA0;&#x2022; Screened Swoosh design trademark
DIMENSIONS:&#xA0; 20&quot; L x 10&quot; W x 11&quot; H&#xA0;
010 Black/Black/(White) 068 Iron Grey/Black/(White)
...</code></pre><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/z2OKhAFnHCXw3aI4W1KUZNxzt8WOEERSG-prrBcmGwBEDTAFGavSN8eiHnvFF6R0L-dlwRUY3U8R7ysinoGTSSL-9_vr0RdCmrIRSy_xffvKz-Trqa1Yr49YGNb7R-GsDpCqilIW3cluVER4St5rUts" class="kg-image" alt="Precise RAG with Jina Reranker and LlamaIndex" loading="lazy" width="594" height="592"></figure><h3 id="query-for-results-with-reranker">Query for results with Reranker</h3><p>We now want to apply the reranker to see if the RAG application yields a different, more <em>relevant</em> result. To do so, we need to add the <code>node_postprocessors</code> to the <code>query_engine</code>:</p><pre><code class="language-Python">query_engine = index.as_query_engine(
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=10, llm=mixtral_llm, node_postprocessors=[jina_rerank]
)
response = query_engine.query(
&#xA0;&#xA0;&#xA0;&#xA0;&quot;What are the best padded pants that Nike sells?&quot;,
)

print(response.source_nodes[0].text)</code></pre><p>Note that compared to the previous case without the reranker, the <code>query_engine</code> now also contains the <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/JinaRerank.html?ref=jina-ai-gmbh.ghost.io"><u>node_postprocessors</u></a> parameter set to <code>[jina_rerank]</code>.</p><p><strong>Result</strong>:</p><pre><code class="language-Text">NIKE KIDS&#xA0; FOOTBALL &#x2013; STOCK10
DJ5731 $47.00
SIZES:&#xA0; XS, S, M, L, XL, 2XL, 3XL
FABRIC:&#xA0; Body/panels lining: 100% polyester. Pad: 100%
ethylene vinyl acetate.
OFFER DATE:&#xA0; 04/01/23
END DATE:&#xA0; 04/01/27
Take the field ready to give it your all in the Nike Recruit
Pants. They&#x2019;re made from lightweight, stretchy&#xA0;fabric with
sweat-wicking power to help keep you dry and moving freely
when the game heats up. With&#xA0;integrated pads shaped for a
comfortable fit, you&#x2019;ll be prepared for a performance you can
be proud of.&#xA0;Choose from 6 different colors to outfit your
team. Nike Dri-FIT technology moves sweat away from your&#xA0;skin
for quicker evaporation, helping you stay dry and comfortable.
Lightweight knit fabric stretches with&#xA0;you to let you move
naturally. Thigh, knee, hip and tailbone pads are shaped for
an optimal fit, without&#xA0;compromising on coverage. A
body-hugging fit is designed to help keep the padding in place
and close to&#xA0;the body. Belt at the waist lets you dial in your
perfect fit to maximize comfort. Elastic at hems.
Hip width: 15&quot;, Inseam length: 11.75&quot; (size medium).
010 Black/(White) 060 Team Anthracite/(White) 100 White/(Black)
419 Team Navy/(White) 493 Team Royal/(White) 657 Team Scarlet/(White)</code></pre><p></p><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/X0ngGFoOz8S_KlRhAlQs4rOooKsV92d06-UBa7BsFPiTilgTD9xuQfgIblG7dTq0BQp4ZA9DlBCjFvA6D62eUVDmxxOO38va5mlnutPr4Zlu6l70tvbGtmrS2V0rpn-OE__tarMtR1MxAmD2oQ3m2qg" class="kg-image" alt="Precise RAG with Jina Reranker and LlamaIndex" loading="lazy" width="1600" height="982"></figure><h2 id="conclusion"><strong>Conclusion</strong></h2><p>As we can see, the query without the reranker leads to a top result which mentions &#x201C;mesh back for breathability&#x201D; and &#x201C;slim fit with soft hand feel&#x201D;. In comparison, by using a reranker, we obtain a top result that is &#x201C;engineered for optimal breathability&#x201D;, has a &#x201C;moisture-wicking design&#x201D; that &#x201C;helps keep you dry and cool under match-day pressure&#x201D;, and features &#x201C;lightweight fabric in a relaxed, easy fit&#x201D;.</p><p>The second result is much more accurate and appropriate for the query we asked. With our last two posts, we showed both from a theoretical and practical perspective that adding Jina Reranker to your RAG pipeline increases your retrieval accuracy and improves the quality of the responses you obtain from it.</p>]]></content:encoded></item><item><title><![CDATA[Build a RAG system with Jina Embeddings and Qdrant]]></title><description><![CDATA[Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.]]></description><link>https://jina.ai/news/build-a-rag-system-with-jina-embeddings-and-qdrant/</link><guid isPermaLink="false">65ddadd039a5b70001810ea3</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 04 Mar 2024 15:00:13 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg" alt="Build a RAG system with Jina Embeddings and Qdrant"><p>Retrieval-augmented generation is a rapidly growing group of techniques that were <a href="https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io"><u>originally conceived as an extension to large language models</u></a> (LLMs) but are increasingly seen as an alternative paradigm that re-imagines the role of AI models entirely.</p><p>We&#x2019;ll show you how to make a basic RAG engine using the <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex framework</a>, an open-source <a href="https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io">Mistral LLM</a>, <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Jina Embeddings v2</a>, and <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer">Qdrant&#x2019;s AI-ready vector database</a>.</p><h2 id="rag-in-context"><strong>RAG in Context</strong></h2><p>In a RAG system, the role of the AI model is still central but much more limited. Instead of answering prompts from its internal knowledge, derived from its training data, it&#x2019;s presented with the relevant information in its prompt. The role of the model is then to understand and synthesize information into an output.</p><p>This plays to the strengths of AI models and minimizes its weaknesses.</p><p>For example, we might give the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io"><u>Mistral-Instruct model</u></a> the following prompt:</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.
Query: Why is Taylor Swift planning to attend the Superbowl?
Answer:
</code></pre><p>When I did that, the model responded with:</p><pre><code class="language-text">I don&apos;t have real-time information. However, as of my last update,&#xA0;
Taylor Swift is not officially confirmed to attend or perform at&#xA0;
the Superbowl. If there are any updates, I recommend checking the&#xA0;
latest news sources.</code></pre><p>Large language models are expensive to update and retrain, so their internal information can never be entirely up to date.</p><p>Furthermore, large language models are well known to &#x201C;hallucinate,&#x201D; i.e., confabulate plausible answers that may be partly or wholly incorrect and have to be verified.</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.
Query: What was Weird Al Yankovic&apos;s first Taylor Swift parody?
Answer:

Weird Al Yankovic&apos;s first Taylor Swift parody was &quot;TMZ,&quot; a spoof&#xA0;
of Swift&apos;s song &quot;Blank Space.&quot; The parody was released in 2014&#xA0;
as part of Yankovic&apos;s album &quot;Mandatory Fun.&quot; The song humorously&#xA0;
addresses the obsession with celebrity gossip and scandals, as&#xA0;
reported by the tabloid news website TMZ.</code></pre><p>Weird Al Yankovic&#x2019;s <em>TMZ</em> was released in 2011 on the album <em>Alpocalypse</em> and is a parody of Swift&#x2019;s <a href="https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io"><em><u>You Belong with Me</u></em></a>.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)"></iframe></figure><p>The nature of neural networks makes hallucination difficult, if not impossible, to completely prevent and strongly undermines any plan to use AI models as information sources.</p><p>In the <a href="https://youtu.be/uaQZIK9gvNo?t=1130&amp;ref=jina-ai-gmbh.ghost.io"><u>words of Sam Altman</u></a>, CEO of OpenAI:</p><blockquote class="kg-blockquote-alt">&#x201C;I think people make a big mistake when they think of these models [LLMs] as a database. [&#x2026;] It will work, sort of. But it&#x2019;s, like, the world&#x2019;s most expensive, slowest, least accurate database.&#x201D;</blockquote><p>RAG minimizes reliance on an AI model&#x2019;s internal knowledge of facts about the world and takes advantage of its biggest strengths: natural language understanding and fluent language generation.</p><p>Let&#x2019;s take another example of a question Mistral-Instruct can&#x2019;t answer:</p><pre><code class="language-text">Answer the query. Please be brief, concise, and complete.

Query: What is the title of Taylor Swift&apos;s next album?
Answer:

As of the time I am providing this answer, Taylor Swift has&#xA0;
not yet announced the title of her next album. She has released&#xA0;
several re-recorded albums, but the title of her next original&#xA0;
album remains unknown. I recommend checking her official social&#xA0;
media channels or website for the latest updates.</code></pre><p>Now, a RAG system might have a database of recent news articles, or perhaps search the Internet for texts related to the title of Taylor Swift&#x2019;s next album. Then, it would insert the texts it finds into the prompt.</p><p>Let&#x2019;s assume it identifies <a href="https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io"><u>this AP article</u></a> as relevant to the query and constructs the following prompt:</p><pre><code class="language-text">Context information is below.
---------------------
February 6, 2024
LOS ANGELES (AP) &#x2014; Forget her &#x201C;Reputation.&#x201D; Taylor Swift has a&#xA0;
whole new album coming out.

Accepting the Grammy for best pop vocal album on Sunday night,&#xA0;
Taylor said she&#x2019;d been keeping a secret for two years.

&#x201C;My brand-new album comes out April 19. It&#x2019;s called &#x2018;The Tortured&#xA0;
Poets Department.&#x2019; I&#x2019;m going to go and post the cover right now&#xA0;
backstage,&#x201D; she announced.

And so she did.

---------------------
Given the context information and not prior knowledge, answer the query.&#xA0;
Please be brief, concise, and complete.
If the context information does not contain an answer to the query,&#xA0;
respond with &quot;No information&quot;.
Query: What is the title of Taylor Swift&apos;s next album?
Answer:</code></pre><p>Mistral-Instruct responded with:</p><pre><code class="language-text">The title of Taylor Swift&apos;s next album is &apos;The Tortured Poets Department&apos;.</code></pre><p>This kind of prompting also reduces the likelihood of hallucinations dramatically. For example:</p><pre><code class="language-text">Context information is below.
---------------------
Alpocalypse is the thirteenth studio album by American parody&#xA0;
musician &quot;Weird Al&quot; Yankovic, released on June 21, 2011.

Track List

1. Perform This Way (Parody of &quot;Born This Way&quot; by Lady Gaga)
2. CNR
3. TMZ (Parody of &quot;You Belong with Me&quot; by Taylor Swift)
4. Skipper Dan
5. Polka Face (Polka medley including songs by artists such as&#xA0;
&#xA0;&#xA0;&#xA0;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)
6. Craigslist
7. Party in the CIA (Parody of &quot;Party in the U.S.A.&quot; by Miley&#xA0;
&#xA0;&#xA0;&#xA0;Cyrus)
8. Ringtone
9. Another Tattoo (Parody of &quot;Nothin&apos; on You&quot; by B.o.B featuring&#xA0;
&#xA0;&#xA0;&#xA0;Bruno Mars)
10. If That Isn&apos;t Love
11. Whatever You Like (Parody of &quot;Whatever You Like&quot; by T.I.)
12. Stop Forwarding That Crap to Me

---------------------
Given the context information and not prior knowledge, answer the&#xA0;
query. Please be brief, concise, and complete.
If the context information does not contain an answer to the query,&#xA0;
respond with &quot;No information&quot;.
Query: What was Weird Al Yankovic&apos;s first Taylor Swift parody, what&#xA0;
year was it released, and what song was he parodying?
Answer:

Weird Al Yankovic&apos;s first Taylor Swift parody was &quot;TMZ,&quot; and it was&#xA0;
released in 2011. He was parodying &quot;You Belong with Me&quot; by Taylor&#xA0;
Swift.
</code></pre><h2 id="anatomy-of-a-rag-system"><strong>Anatomy of a RAG system</strong></h2><p>A RAG system has, minimally, two components:</p><ul><li>A generative AI model.</li><li>An information retrieval system.</li></ul><p>In this article, we will use <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io">Mistral-Instruct</a> as the generative AI model and access it via the <a href="https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io">Hugging Face Inference API</a>. We will use the <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog">Qdrant vector database</a> and <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Jina Embeddings</a> together for the information retrieval system.</p><p>As a data source, we will use the <a href="https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io"><em><u>National Artificial Intelligence Research and Development Strategic Plan 2023 Update</u></em></a>, published in May 2023 by the <a href="https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io">National Science and Technology Council</a>, a body that advises the US President on scientific and technical matters. This roughly 25,000-word document is a policy and strategic planning document for American AI policy and development.</p><p>It&apos;s also a classic &#x201C;tl;dr&#x201D; government policy paper. Most of it is very boring, and unless you have a stake or a close interest in this area, you probably aren&#x2019;t going to read it. But you might still want to know what it says.</p><p>This article will walk you through the steps of creating a RAG system able to answer questions about the text and its contents.</p><h2 id="install-the-prerequisites"><strong>Install the Prerequisites</strong></h2><p>First, make sure you have all the relevant Python libraries installed. In your Python environment, run the following:</p><pre><code class="language-shell">pip install llama-index qdrant-client 
pip install pdfminer.six llama-index-vector-stores-qdrant
pip install llama-index-llms-openai llama-index-embeddings-jinaai 
pip install llama-index-llms-huggingface &quot;huggingface_hub[inference]&quot;</code></pre><p>We will be using LlamaIndex to construct a RAG system around the Mistral-Instruct large language model and <a href="https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io"><u>pdfminer.six</u></a> to process the PDF file into a Qdrant vector store.</p><h2 id="connect-to-mistral-instruct-via-the-hugging-face-inference-api"><strong>Connect to Mistral-Instruct via the Hugging Face Inference API</strong></h2><p>First, set up the LLM. You will need access to the Hugging Face Inference API, including an access token. If you have a Hugging Face account, you can get one from <a href="https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io">your account settings page</a>.</p><p>If you do not have an account, first <a href="https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io">set one up</a>, then create an <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">access token</a>.</p><p>Place your token in a variable for future use:</p><pre><code class="language-python">hf_inference_api_key = &apos;&lt;your HuggingFace Inference API token&gt;&apos;</code></pre><p>Next, we construct a prompt template:</p><pre><code class="language-python">from llama_index.core import PromptTemplate

qa_prompt_tmpl = (
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Context information is below.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;---------------------\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;{context_str}\\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;---------------------\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Given the context information and not prior knowledge, &quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;answer the query. Please be brief, concise, and complete.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;If the context information does not contain an answer to the query, &quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;respond with \&quot;No information\&quot;.\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Query: {query_str}\n&quot;
&#xA0;&#xA0;&#xA0;&#xA0;&quot;Answer: &quot;
)
qa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>This template instructs the LLM to use only the context information provided in the prompt to answer questions. This prompt has worked well for us in working with Mistral-Instruct, but you can experiment with other wordings to see what works for your use case.</p><pre><code class="language-python">from llama_index.llms.huggingface import HuggingFaceInferenceAPI

mixtral_llm = HuggingFaceInferenceAPI(
    model_name=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;, 
    token=hf_inference_api_key
)</code></pre><p>Finally, create and initialize an object for the LlamaIndex framework that holds the connection to Mistral-Instruct. The code below comes from the <a href="https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io"><u>previous post on creating RAG systems using LlamaIndex</u></a>.</p><p>This puts the connection to the Mistral LLM into the variable <code>mistral_llm</code>.</p><h2 id="information-retrieval-with-qdrant-and-jina-embeddings"><strong>Information Retrieval with Qdrant and Jina Embeddings</strong></h2><p>To set up the retrieval system, you will need a Jina Embeddings API key. You can get one for free with a 1 million-token prepaid budget at the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings website</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Place your Jina Embeddings key in a variable for future use:</p><pre><code class="language-python">jina_emb_api_key = &quot;&lt;your Jina Embeddings API key&gt;&quot;</code></pre><p>Then, create a connector object using LlamaIndex for the Jina Embeddings server, selecting specifically the English monolingual model:</p><pre><code class="language-python">from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jina_emb_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
)</code></pre><h3 id="load-text-data"><strong>Load text data</strong></h3><p>Next, we will load the document and split it up into paragraphs. First, download the PDF from the White House website into the variable <code>pdf_data</code>:</p><pre><code class="language-python">import urllib.request

uri = &quot;https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf&quot;
pdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Next, we will process the PDF page by page, extracting the text and then chunking it into paragraphs by splitting on double line breaks. These are stored in the list <code>text_paras</code>:</p><pre><code class="language-python">import regex as re
from io import BytesIO, StringIO
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser

text_paras = []
parser = PDFParser(BytesIO(pdf_data))
doc = PDFDocument(parser)
rsrcmgr = PDFResourceManager()
for page in PDFPage.create_pages(doc):
&#xA0;&#xA0;&#xA0;&#xA0;output_string = StringIO()
&#xA0;&#xA0;&#xA0;&#xA0;device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
&#xA0;&#xA0;&#xA0;&#xA0;interpreter = PDFPageInterpreter(rsrcmgr, device)
&#xA0;&#xA0;&#xA0;&#xA0;interpreter.process_page(page)
&#xA0;&#xA0;&#xA0;&#xA0;page_text = output_string.getvalue()
&#xA0;&#xA0;&#xA0;&#xA0;text_paras.extend(re.split(r&apos;\n\s*\n&apos;, page_text))
</code></pre><p>Check that everything is loaded:</p><pre><code class="language-python">assert len(text_paras) == 615</code></pre><p>Next, we will covert this list of short texts into <a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex Document objects</a>:</p><pre><code class="language-python">from llama_index.core.readers import StringIterableReader

rag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>And you can inspect the text:</p><pre><code class="language-python">print(rag_docs[0].text)</code></pre><p>Result:</p><pre><code class="language-text">NATIONAL ARTIFICIAL INTELLIGENCE&#xA0;
RESEARCH AND DEVELOPMENT&#xA0;
STRATEGIC PLAN&#xA0;
2023 UPDATE</code></pre><h3 id="set-up-a-qdrant-vector-database"><strong>Set up a Qdrant Vector Database</strong></h3><p>In this article, we will use the <a href="https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io"><u>Qdrant Vector Search Cloud</u></a> to implement the RAG system&#x2019;s database. Qdrant&#x2019;s free tier includes 1GB of storage, which is more than enough for this tutorial.</p><p>You will need to <a href="https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io"><u>create an account on the Qdrant Cloud website</u></a> before continuing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Vector Search Database | Qdrant Cloud</div><div class="kg-bookmark-description">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://cloud.qdrant.io/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Qdrant Cloud</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cloud.qdrant.io/social.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Once you have an account and are logged in, you will need to create a cluster. Follow the <a href="https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>&#x201C;quick start&#x201D; instructions on the Qdrant Website</u></a> to set up a free cluster and get an API and the name of the Qdrant host server.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Quickstart - Qdrant</div><div class="kg-bookmark-description">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://qdrant.tech/images/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Edit on GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><p>Store the key and hostname in variables:</p><pre><code class="language-python">qdrant_api_key = &quot;&lt;your API key&gt;&quot;
qdrant_server = &quot;https://&lt;your server&gt;&quot;</code></pre><p>Next, we will need to import the relevant components from the <code>qdrant_client</code> and <code>llama_index</code> packages:</p><pre><code class="language-python">import qdrant_client
from llama_index.vector_stores.qdrant import QdrantVectorStore

client = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)
vector_store = QdrantVectorStore(client=client, collection_name=&quot;NTSC&quot;)</code></pre><p>This creates a collection named <code>NTSC</code> in your free cluster.</p><p>You can read the <a href="https://qdrant.tech/documentation/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>Qdrant documentation</u></a> for information on constructing a local data store in memory or on disk and hosting your own Qdrant server instance in a docker container via a web API.</p><h3 id="complete-the-rag-system"><strong>Complete the RAG system</strong></h3><p>Now we will assemble these components into a complete RAG system using boilerplate code for LlamaIndex:</p><pre><code class="language-python">from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core import (
		VectorStoreIndex,
		ServiceContext,
		get_response_synthesizer,
)

# set up the service and storage contexts
service_context = ServiceContext.from_defaults(
&#xA0;&#xA0;&#xA0;&#xA0;llm=mixtral_llm, embed_model=jina_embedding_model
)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# create an index
index = VectorStoreIndex.from_documents(
&#xA0;&#xA0;&#xA0;&#xA0;rag_docs, storage_context=storage_context, service_context=service_context
)

# configure retriever
retriever = VectorIndexRetriever(
&#xA0;&#xA0;&#xA0;&#xA0;index=index,
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=2,
)

# configure response synthesizer
response_synthesizer = get_response_synthesizer(
&#xA0;&#xA0;&#xA0;&#xA0;service_context=service_context,
&#xA0;&#xA0;&#xA0;&#xA0;text_qa_template=qa_prompt,
&#xA0;&#xA0;&#xA0;&#xA0;response_mode=&quot;compact&quot;,
)

# assemble query engine
query_engine = RetrieverQueryEngine(
&#xA0;&#xA0;&#xA0;&#xA0;retriever=retriever,
&#xA0;&#xA0;&#xA0;&#xA0;response_synthesizer=response_synthesizer,
)
</code></pre><p>This system will use Jina Embeddings to index the paragraphs from our text source and will put the two best matches into the context of every prompt. To change to the three best, for example, just modify the <code>VectorIndexRetriever</code> initialization above to have a different <code>similarity_top_k</code> parameter:</p><pre><code class="language-python"># configure retriever
retriever = VectorIndexRetriever(
&#xA0;&#xA0;&#xA0;&#xA0;index=index,
&#xA0;&#xA0;&#xA0;&#xA0;similarity_top_k=3,
)</code></pre><p>Now the system is ready to use.</p><h3 id="query-a-document"><strong>Query a Document</strong></h3><p>Let&#x2019;s try a straightforward query:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What is the Biden Administration&apos;s policy with regard to AI?&#xA0;
&quot;&quot;&quot;)
print(response.response)</code></pre><p>The result (yours may not be identical):</p><pre><code class="language-text">The Biden Administration prioritizes responsible AI systems that&#xA0;
are ethical, trustworthy, and safe, and serve the public good. 
The Administration has committed to substantial funding for AI 
R&amp;D, and has established the National AI Initiative Office to 
coordinate AI activities across the federal government.&#xA0;

The Administration&apos;s AI policy focuses on developing evaluative
techniques for AI, workforce development, public-private
partnerships, and international collaboration in AI research.</code></pre><p>Or something more specific:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What protections does the AI Bill of Rights propose to offer?
&quot;&quot;&quot;)

print(response.response)</code></pre><pre><code class="language-text">The AI Bill of Rights proposes five core protections: Safe and&#xA0;
Effective Systems; Algorithmic Discrimination Protections; Data&#xA0;
Privacy; Notice and Explanation; and Human Alternatives,&#xA0;
Consideration, and Fallback.</code></pre><p>Or even very specific:</p><pre><code class="language-python">response = query_engine.query(&quot;Who is Kei Koizumi?&quot;)
print(response.response)</code></pre><pre><code class="language-text">Kei Koizumi is the Principal Deputy Director for Policy at the Office&#xA0;
of Science and Technology Policy (OSTP).</code></pre><p>You can also ask more fanciful questions:</p><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
What rights will AI&apos;s receive under President Biden&apos;s proposed&#xA0;
AI Bill of Rights?
&quot;&quot;&quot;)
print(response.response)</code></pre><pre><code class="language-text">The proposed AI Bill of Rights does not grant rights to AI systems, but&#xA0;
rather outlines principles to ensure that AI technologies are&#xA0;
developed and used in a manner that respects human rights and values.&#xA0;
The principles include safety, transparency, fairness, disclosure,&#xA0;
data privacy, non-discrimination, and the ability for individuals to&#xA0;
opt out of certain AI systems.</code></pre><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
Why is President Biden&apos;s proposing an AI Bill of Rights?
Does AI really need rights?
&quot;&quot;&quot;)
print(response.response)</code></pre><pre><code class="language-text">President Biden&apos;s proposed AI Bill of Rights aims to ensure that AI&#xA0;
technologies are developed and used ethically, legally, and with&#xA0;
societal considerations in mind. The Bill of Rights is not granting&#xA0;
&quot;rights&quot; to AI, but rather setting guidelines for the responsible&#xA0;
development and deployment of AI systems to protect individuals and&#xA0;
communities from potential negative impacts.</code></pre><pre><code class="language-python">response = query_engine.query(&quot;&quot;&quot;
Has Donald Trump weighed in on AI?
Will he Make Humans Great Again?
&quot;&quot;&quot;)
print(response.response)
</code></pre><pre><code class="language-text">No information. The context information does not mention Donald&#xA0;
Trump&apos;s views on AI.</code></pre><h2 id="jina-ai-and-qdrant"><strong>Jina AI and Qdrant</strong></h2><p>Jina Embeddings&#x2019; 8k token context window makes it especially well-suited to retrieval-augmented generation because it can work with much larger blocks of text. Where other RAG applications often have to store individual sentences or pairs of sentences, in this tutorial, we could use whole paragraphs without worrying about their size.</p><p>This reduces the number of text items that have to be stored to carry the same information, lowering compute costs, sparing memory, and making retrieval faster.</p><p>Qdrant&#x2019;s vector database is simple to configure, fast, and cost-effective, and as you&#x2019;ve seen, it takes only a few lines of Python to integrate it into a RAG system.</p><p>For more about Qdrant&#x2019;s AI-ready database and cloud offerings, you can <a href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog" rel="noreferrer"><u>visit their website</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://qdrant.tech/?utm_source=jinaai&amp;utm_medium=partner&amp;utm_campaign=qdrant-jinaai-rag-blog"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Qdrant - Vector Database</div><div class="kg-bookmark-description">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://qdrant.tech/images/apple-touch-icon.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Vector Database</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://qdrant.tech/images/previews/social-preview-P.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><h2 id="get-in-touch"><strong>Get in Touch</strong></h2><p>Jina AI is committed to bringing reliable, affordable AI technologies to enterprises of every size and type. We&#x2019;d love to hear about your use cases and help fit AI into your business processes. For more information about Jina AI&#x2019;s offerings and to contact us, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>Jina AI website</u></a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><u>community on Discord</u></a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Build a RAG system with Jina Embeddings and Qdrant"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Build a RAG system with Jina Embeddings and Qdrant"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.5 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-5-update/</link><guid isPermaLink="false">65e0b09bb22368000152a3ff</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Thu, 29 Feb 2024 16:31:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-2.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3235">Release Note (<code>3.23.5</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-2.jpg" alt="Jina 3.23.5 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.5?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.5 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.5) Release time: 2024-02-29 14:36:02 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Fix batch transform endpoint for SageMaker with nested schemas (#6145)
This release improves t&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.5 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/0e601e49d37d6935a6e25fca8cffd5b4e98d0b1ed8605e0b1c72d54d290871a3/jina-ai/jina/releases/tag/v3.23.5" alt="Jina 3.23.5 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-batch-transform-endpoint-for-sagemaker-with-nested-schemas-6145">Fix batch transform endpoint for SageMaker with nested schemas (<a href="https://github.com/jina-ai/jina/pull/6145?ref=jina-ai-gmbh.ghost.io">#6145</a>)</h3><p>This release improves the SageMaker Batch Transform input data parsing logic so that data with more complex schema can be interpreted as expected.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Zac Li (<a href="https://github.com/zac-li?ref=jina-ai-gmbh.ghost.io">@zac-li</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Maximizing Search Relevance and RAG Accuracy with Jina Reranker]]></title><description><![CDATA[Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!]]></description><link>https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/</link><guid isPermaLink="false">65dfb0f7b22368000152a205</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Thu, 29 Feb 2024 08:30:10 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"><p>Text embeddings, known for their semantic representation capabilities, along with fast vector retrieval, are today&apos;s cornerstone in document search within vast datasets. However, the challenge often lies in filtering through these retrieved documents to accurately align with the user&apos;s search intent, a task that goes beyond the capabilities of simple cosine similarity measures.</p><p>Today, we&apos;re thrilled to announce <strong>Jina Reranker</strong> (<code>jina-reranker-v1-base-en</code>), a cutting-edge neural reranking model designed to tackle this critical issue of relevancy. Jina Reranker enhances your search and RAG (Retrieval-Augmented Generation) system by <em>reordering</em> retrieved documents in a manner that deeply and contextually understands the search query terms. Our evaluations demonstrated remarkable improvements for search systems employing the Jina Reranker <strong>with</strong> <strong>+8% in hit rate and +33% in mean reciprocal rank</strong>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker"></div></a></figure><h2 id="what-is-a-reranker">What is a Reranker?</h2><p>Understanding the concept of a reranker often leads us to question the necessity of <em>re-</em>ranking mechanisms in search (ranking) systems. Common questions are, &quot;<em>Why do we need a </em><strong>re</strong><em>ranker?</em>&quot; and &quot;<em>Isn&apos;t ranking documents by cosine similarity between the query and document embeddings enough already?</em>&quot; To address these questions, let&apos;s first revisit the single embedding cosine similarity approach and its limitations.</p><p>Cosine similarity quantifies the similarity between two documents by measuring the cosine of the angle between their embedding vectors. This metric is valued for its simplicity and has been used in many vector databases as the default way of retrieval. However, this approach, commonly termed as a <em>representation-based </em>approach, tends to oversimplify interaction between query and documents. Specifically, it struggles with capturing the nuanced interaction at the sub-document level and sub-query level, often missing the full depth of user intent and the fine details of document relevance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Copy-of-Copy-of-colbert-blog-interaction.svg" class="kg-image" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker" loading="lazy" width="168" height="150"><figcaption><span style="white-space: pre-wrap;">The comparison of the representation-based cosine similarity (left) and the reranker (right).</span></figcaption></figure><p>This is where rerankers come into play. Utilizing deep neural networks, rerankers delve deeper into the interactions between the query and the shortlisted documents. They move beyond basic document-level embeddings to embrace token-level interactions that occur within the query, within the document, and across the query-document boundary. Although this method is more computationally intensive compared to simple cosine similarity, it enables a nuanced comparison that incorporates context, semantic meaning, and the intent behind the query, substantially improving the relevancy of search results.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th></th>
<th>Vector Search via Cosine Similarity</th>
<th>Reranker</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Interaction Level</strong></td>
<td>Document-level embeddings</td>
<td>Token-level interactions</td>
</tr>
<tr>
<td><strong>Computational Demand</strong></td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td><strong>Most computation happens at</strong></td>
<td>Offline, i.e. indexing time</td>
<td>Online, i.e. query time</td>
</tr>
<tr>
<td><strong>Result</strong></td>
<td>Broad but superficial matching</td>
<td>Highly relevant and precise matching</td>
</tr>
<tr>
<td><strong>Strengths</strong></td>
<td>- Fast and efficient<br>- Simple implementation</td>
<td>- Deep contextual understanding<br>- Advanced semantic analysis</td>
</tr>
<tr>
<td><strong>Limitations</strong></td>
<td>- Limited by lack of depth and context<br>- May miss nuances of user intent</td>
<td>- Computationally intensive<br>- Requires more sophisticated models</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Provides a quick, efficient first pass</td>
<td>Adds depth, enhancing accuracy and relevance of final search results</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>In summary, reranker is a critical component in the search pipeline. <strong>A high-quality search system typically begins with an embedding-based vector retrieval step, which is then refined by a reranker model.</strong> This two-step approach leverages the strengths of both models, ensuring the delivery of precise, high-quality information that aligns seamlessly with the user&apos;s needs.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Copy-of-Copy-of-Copy-of-colbert-blog-interaction.svg" class="kg-image" alt="Maximizing Search Relevance and RAG Accuracy with Jina Reranker" loading="lazy" width="300" height="66"><figcaption><span style="white-space: pre-wrap;">A practical search system often chains the embedding-based search and the reranker together to achieve the best search quality.</span></figcaption></figure><h2 id="get-started-with-jina-reranker">Get Started with Jina Reranker</h2><p>To start using Jina Reranker, head to our <a href="http://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io">Reranker page</a> to get <code>YOUR_API_KEY</code>. You can adapt the example cURL snippet below by changing the query or adding more documents:</p><pre><code class="language-shell">curl -X &apos;POST&apos; \
  &apos;https://api.jina.ai/v1/rerank&apos; \
  -H &apos;accept: application/json&apos; \
  -H &apos;Authorization: Bearer YOUR_API_KEY&apos; \
  -H &apos;Content-Type: application/json&apos; \
  -d &apos;{
  &quot;model&quot;: &quot;jina-reranker-v1-base-en&quot;,
  &quot;query&quot;: &quot;Organic skincare products for sensitive skin&quot;,
  &quot;documents&quot;: [
    &quot;Eco-friendly kitchenware for modern homes&quot;,
    &quot;Biodegradable cleaning supplies for eco-conscious consumers&quot;,
    &quot;Organic cotton baby clothes for sensitive skin&quot;,
    &quot;Natural organic skincare range for sensitive skin&quot;,
    &quot;Tech gadgets for smart homes: 2024 edition&quot;,
    &quot;Sustainable gardening tools and compost solutions&quot;,
    &quot;Sensitive skin-friendly facial cleansers and toners&quot;,
    &quot;Organic food wraps and storage solutions&quot;,
    &quot;All-natural pet food for dogs with allergies&quot;,
    &quot;Yoga mats made from recycled materials&quot;
  ], 
  &quot;top_n&quot;: 3
}&apos;
</code></pre><p>In this example, the documents range from home goods to tech gadgets, clothing, and even pet food, showcasing a wide spectrum of products one might find on an e-commerce site. This diversity requires the reranker to effectively identify and prioritize items most relevant to the query about &quot;Organic skincare products for sensitive skin,&quot; despite the presence of other organic, eco-friendly, or sensitive skin-related products in different categories. The response is a JSON object as follows:</p><pre><code class="language-json">{
  &quot;model&quot;: &quot;jina-reranker-v1-base-en&quot;,
  &quot;usage&quot;: {
    &quot;total_tokens&quot;: 38,
    &quot;prompt_tokens&quot;: 38
  },
  &quot;results&quot;: [
    {
      &quot;index&quot;: 3,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Natural organic skincare range for sensitive skin&quot;
      },
      &quot;relevance_score&quot;: 0.8292155861854553
    },
    {
      &quot;index&quot;: 2,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Organic cotton baby clothes for sensitive skin&quot;
      },
      &quot;relevance_score&quot;: 0.14426936209201813
    },
    {
      &quot;index&quot;: 6,
      &quot;document&quot;: {
        &quot;text&quot;: &quot;Sensitive skin-friendly facial cleansers and toners&quot;
      },
      &quot;relevance_score&quot;: 0.13857832551002502
    }
  ]
}</code></pre><h2 id="top-performance-of-jina-reranker">Top Performance of Jina Reranker</h2><p>We evaluated Jina Reranker on four key benchmarks to ensure top-tier performance and search relevance. Higher performance in these benchmarks directly translates to better precision, relevance, and contextual understanding in search and retrieval applications.</p><p>For comparison, we included three other leading rerankers by <a href="https://www.baai.ac.cn/english.html?ref=jina-ai-gmbh.ghost.io">BGE (BAAI)</a>, <a href="https://smartyoudao.com/?ref=jina-ai-gmbh.ghost.io">BCE (Netease Youdao)</a>, and <a href="https://cohere.com/?ref=jina-ai-gmbh.ghost.io">Cohere</a> in the benchmark. As shown by the results below, <strong>Jina Reranker holds the highest average score in all relevant categories for reranking, making it a clear leader among its peers.</strong></p><h3 id="benchmark-1-llamaindex-rag">Benchmark 1: LlamaIndex RAG</h3><p>A <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">benchmark study</a> performed by LlamaIndex (which we reproduced), evaluated <em>a combination</em> of different embedding and reranking models in RAG tasks. This combines two scores - <em>Hit-rate</em> (how likely it is that a relevant document is retrieved by an embedding model), and <em>Mean Reciprocal Rank</em> (MRR - how high the most relevant document is ranked by the reranker model).</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-cyhs{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-wa1i"></th>
    <th class="tg-wa1i" colspan="2">No Reranker</th>
    <th class="tg-wa1i" colspan="2">jina-reranker</th>
    <th class="tg-wa1i" colspan="2">bge-reranker-base</th>
    <th class="tg-wa1i" colspan="2">bce-reranker-base_v1</th>
    <th class="tg-wa1i" colspan="2">cohere-reranker</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0">Embedding model</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
    <td class="tg-mwxe">Hit Rate</td>
    <td class="tg-mwxe">MRR</td>
  </tr>
  <tr>
    <td class="tg-cly1">jina-embeddings-v2-base-en</td>
    <td class="tg-mwxe">0.8053</td>
    <td class="tg-mwxe">0.5156</td>
    <td class="tg-mwxe">0.8737</td>
    <td class="tg-zt7h">0.7229</td>
    <td class="tg-mwxe">0.8368</td>
    <td class="tg-mwxe">0.6568</td>
    <td class="tg-mwxe">0.8737</td>
    <td class="tg-mwxe">0.7007</td>
    <td class="tg-zt7h">0.8842</td>
    <td class="tg-mwxe">0.7008</td>
  </tr>
  <tr>
    <td class="tg-cly1">bge-base-en-v1.5</td>
    <td class="tg-mwxe">0.7842</td>
    <td class="tg-mwxe">0.5183</td>
    <td class="tg-zt7h">0.8368</td>
    <td class="tg-zt7h">0.6895</td>
    <td class="tg-mwxe">0.8158</td>
    <td class="tg-mwxe">0.6586</td>
    <td class="tg-mwxe">0.8316</td>
    <td class="tg-mwxe">0.6843</td>
    <td class="tg-mwxe">0.8368</td>
    <td class="tg-mwxe">0.6739</td>
  </tr>
  <tr>
    <td class="tg-cly1">bce-embedding-base_v1</td>
    <td class="tg-mwxe">0.8526</td>
    <td class="tg-mwxe">0.5988</td>
    <td class="tg-mwxe">0.8895</td>
    <td class="tg-mwxe">0.7346</td>
    <td class="tg-mwxe">0.8684</td>
    <td class="tg-mwxe">0.6927</td>
    <td class="tg-mwxe">0.9157</td>
    <td class="tg-zt7h">0.7379</td>
    <td class="tg-zt7h">0.9158</td>
    <td class="tg-mwxe">0.7296</td>
  </tr>
  <tr>
    <td class="tg-cly1">CohereV3-en</td>
    <td class="tg-mwxe">0.7211</td>
    <td class="tg-mwxe">0.4900</td>
    <td class="tg-mwxe">0.8211</td>
    <td class="tg-zt7h">0.6894</td>
    <td class="tg-mwxe">0.8000</td>
    <td class="tg-mwxe">0.6285</td>
    <td class="tg-mwxe">0.8263</td>
    <td class="tg-mwxe">0.6855</td>
    <td class="tg-zt7h">0.8316</td>
    <td class="tg-mwxe">0.6710</td>
  </tr>
  <tr>
    <td class="tg-cyhs">Average</td>
    <td class="tg-dg4k">0.7908</td>
    <td class="tg-dg4k">0.5307</td>
    <td class="tg-dg4k">0.8553</td>
    <td class="tg-cya3"><strong>0.7091</strong></td>
    <td class="tg-dg4k">0.8303</td>
    <td class="tg-dg4k">0.6592</td>
    <td class="tg-dg4k">0.8618</td>
    <td class="tg-dg4k">0.7021</td>
    <td class="tg-cya4">0.8671</td>
    <td class="tg-dg4k">0.6938</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h4 id="substantial-improvement-over-simple-cosine-similarity">Substantial Improvement Over Simple Cosine Similarity</h4><p>The improvement Jina Reranker brings to both Hit Rate and Mean Reciprocal Rank (MRR) is significant. On average, the introduction of Jina Reranker elevates the Hit Rate from 0.7908 to 0.8553 (+7.9%), and the MRR from 0.5307 to 0.7091 (+33.7%). This showcases the reranker&#x2019;s ability to enhance the precision and relevance of search results dramatically, ensuring that users are more likely to find what they&apos;re searching for with higher accuracy.</p><h4 id="embeddings-agnosticism">Embeddings Agnosticism</h4><p>Jina Reranker&apos;s performance across different embedding models further illustrates its model-agnostic nature. Whether paired with <code>jina-embeddings-v2-base-en</code>, <code>bge-base-en-v1.5</code>, <code>bce-embedding-base_v1</code>, or <code>CohereV3-en</code>, Jina Reranker consistently improves the Hit Rate and MRR. This versatility makes it an invaluable tool for a wide range of applications, affirming its adaptability to different underlying technologies and use cases.</p><h3 id="benchmark-2-beir">Benchmark 2: BEIR</h3><p><a href="https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io">BIER (Benchmarking IR)</a> assesses a model&apos;s retrieval effectiveness, including relevance and <a href="https://towardsdatascience.com/normalized-discounted-cumulative-gain-37e6f75090e9?ref=jina-ai-gmbh.ghost.io">NDCG</a>. A higher BIER score correlates to more accurate matches and search result rankings.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-zt7h">jina-reranker</th>
    <th class="tg-zt7h">bge-reranker-base</th>
    <th class="tg-zt7h">bce-reranker-base-v1</th>
    <th class="tg-zt7h">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">NQ</td>
    <td class="tg-mwxe">0.5951</td>
    <td class="tg-mwxe">0.5457</td>
    <td class="tg-mwxe">0.5186</td>
    <td class="tg-zt7h">0.6004</td>
  </tr>
  <tr>
    <td class="tg-cly1">HotpotQA</td>
    <td class="tg-zt7h">0.7447</td>
    <td class="tg-mwxe">0.7766</td>
    <td class="tg-mwxe">0.7392</td>
    <td class="tg-mwxe">0.7202</td>
  </tr>
  <tr>
    <td class="tg-cly1">FiQA-2018</td>
    <td class="tg-mwxe">0.3981</td>
    <td class="tg-mwxe">0.3228</td>
    <td class="tg-mwxe">0.3262</td>
    <td class="tg-zt7h">0.4387</td>
  </tr>
  <tr>
    <td class="tg-cly1">CQADupstack</td>
    <td class="tg-zt7h">0.4077</td>
    <td class="tg-mwxe">0.3516</td>
    <td class="tg-mwxe">0.3594</td>
    <td class="tg-mwxe">0.3829</td>
  </tr>
  <tr>
    <td class="tg-cly1">Quora</td>
    <td class="tg-zt7h">0.8792</td>
    <td class="tg-mwxe">0.7001</td>
    <td class="tg-mwxe">0.8390</td>
    <td class="tg-mwxe">0.6433</td>
  </tr>
  <tr>
    <td class="tg-cly1">FEVER</td>
    <td class="tg-mwxe">0.8707</td>
    <td class="tg-zt7h">0.8961</td>
    <td class="tg-mwxe">0.7203</td>
    <td class="tg-mwxe">0.8265</td>
  </tr>
  <tr>
    <td class="tg-cly1">Climate-FEVER</td>
    <td class="tg-mwxe">0.2570</td>
    <td class="tg-zt7h">0.3399</td>
    <td class="tg-mwxe">0.2171</td>
    <td class="tg-mwxe">0.2038</td>
  </tr>
  <tr>
    <td class="tg-cly1">TREC-COVID</td>
    <td class="tg-zt7h">0.8523</td>
    <td class="tg-mwxe">0.7121</td>
    <td class="tg-mwxe">0.7364</td>
    <td class="tg-mwxe">0.8419</td>
  </tr>
  <tr>
    <td class="tg-cly1">NFCorpus</td>
    <td class="tg-zt7h">0.3809</td>
    <td class="tg-mwxe">0.3308</td>
    <td class="tg-mwxe">0.3534</td>
    <td class="tg-mwxe">0.3673</td>
  </tr>
  <tr>
    <td class="tg-cly1">ArguAna</td>
    <td class="tg-zt7h">0.5938</td>
    <td class="tg-mwxe">0.2620</td>
    <td class="tg-mwxe">0.3856</td>
    <td class="tg-mwxe">0.3040</td>
  </tr>
  <tr>
    <td class="tg-cly1">Touche-2020</td>
    <td class="tg-zt7h">0.3065</td>
    <td class="tg-mwxe">0.2965</td>
    <td class="tg-mwxe">0.2533</td>
    <td class="tg-mwxe">0.3052</td>
  </tr>
  <tr>
    <td class="tg-cly1">DBPedia</td>
    <td class="tg-zt7h">0.4387</td>
    <td class="tg-mwxe">0.4196</td>
    <td class="tg-mwxe">0.4069</td>
    <td class="tg-mwxe">0.4236</td>
  </tr>
  <tr>
    <td class="tg-cly1">SciFact</td>
    <td class="tg-zt7h">0.7549</td>
    <td class="tg-mwxe">0.7104</td>
    <td class="tg-mwxe">0.7021</td>
    <td class="tg-mwxe">0.7379</td>
  </tr>
  <tr>
    <td class="tg-cly1">SCIDOCS</td>
    <td class="tg-zt7h">0.1983</td>
    <td class="tg-mwxe">0.1540</td>
    <td class="tg-mwxe">0.1802</td>
    <td class="tg-mwxe">0.1813</td>
  </tr>
  <tr>
    <td class="tg-cly1">MSMarco</td>
    <td class="tg-mwxe">0.7042</td>
    <td class="tg-mwxe">0.7303</td>
    <td class="tg-mwxe">0.7155</td>
    <td class="tg-zt7h">0.7350</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-cya4">0.5588</td>
    <td class="tg-dg4k">0.5032</td>
    <td class="tg-dg4k">0.4969</td>
    <td class="tg-dg4k">0.5141</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="benchmark-3-mteb">Benchmark 3: MTEB</h3><p>The <a href="https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io">MTEB (Multilingual Text Embedding Benchmark)</a>, on the whole, tests a model&#x2019;s abilities in text embeddings, including clustering, classification, retrieval, and other metrics. However, for our comparison, we only used the MTEB&#x2019;s <em>Reranking</em> tasks.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-cya4{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-zt7h{font-weight:bold;text-align:right;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-zt7h">jina-reranker</th>
    <th class="tg-zt7h">bge-reranker-base</th>
    <th class="tg-zt7h">bce-reranker-base-v1</th>
    <th class="tg-zt7h">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">AskUbuntuDupQuestions</td>
    <td class="tg-zt7h">0.5793</td>
    <td class="tg-mwxe">0.5471</td>
    <td class="tg-mwxe">0.5654</td>
    <td class="tg-mwxe">0.5536</td>
  </tr>
  <tr>
    <td class="tg-cly1">SciDocsRR</td>
    <td class="tg-zt7h">0.8056</td>
    <td class="tg-mwxe">0.6741</td>
    <td class="tg-mwxe">0.7578</td>
    <td class="tg-mwxe">0.6728</td>
  </tr>
  <tr>
    <td class="tg-cly1">StackOverflowDupQuestions</td>
    <td class="tg-zt7h">0.4850</td>
    <td class="tg-mwxe">0.3764</td>
    <td class="tg-mwxe">0.4287</td>
    <td class="tg-mwxe">0.4414</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-cya4">0.6233</td>
    <td class="tg-dg4k">0.5325</td>
    <td class="tg-dg4k">0.5840</td>
    <td class="tg-dg4k">0.5559</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="benchmark-4-loco">Benchmark 4: LoCo</h3><p>Through the <a href="https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval?ref=jina-ai-gmbh.ghost.io">LoCo benchmark</a>, we measured a model&apos;s understanding of local coherence and context, together with query-specific ranking. A LoCo higher score reflects a better ability to identify and prioritize relevant information.</p>
<!--kg-card-begin: html-->
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-mwxe{text-align:right;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-gg7l{text-align:left;vertical-align:middle}
.tg .tg-dg4k{text-align:right;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-yla0">Dataset</th>
    <th class="tg-wa1i">jina-reranker</th>
    <th class="tg-wa1i">bge-reranker-base</th>
    <th class="tg-wa1i">bce-reranker-base-v1</th>
    <th class="tg-wa1i">cohere-rerank-english-v2.0</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cly1">qasper_None_abstract</td>
    <td class="tg-mwxe"><strong>0.996</strong></td>
    <td class="tg-mwxe">0.774</td>
    <td class="tg-mwxe">0.989</td>
    <td class="tg-mwxe">0.919</td>
  </tr>
  <tr>
    <td class="tg-cly1">qasper_None_title</td>
    <td class="tg-mwxe">0.980</td>
    <td class="tg-mwxe">0.883</td>
    <td class="tg-mwxe">0.971</td>
    <td class="tg-mwxe"><strong>0.983</strong></td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_gov_report_output</td>
    <td class="tg-mwxe"><strong>0.962</strong></td>
    <td class="tg-mwxe">0.574</td>
    <td class="tg-mwxe">0.922</td>
    <td class="tg-mwxe">0.659</td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_qmsum_output</td>
    <td class="tg-mwxe">0.466</td>
    <td class="tg-mwxe"><strong>0.549</strong></td>
    <td class="tg-mwxe">0.449</td>
    <td class="tg-mwxe">0.444</td>
  </tr>
  <tr>
    <td class="tg-cly1">scrolls_summ_screen_fd_output</td>
    <td class="tg-mwxe"><strong>0.962</strong></td>
    <td class="tg-mwxe">0.629</td>
    <td class="tg-mwxe">0.920</td>
    <td class="tg-mwxe">0.905</td>
  </tr>
  <tr>
    <td class="tg-gg7l">Average</td>
    <td class="tg-dg4k"><strong>0.873</strong></td>
    <td class="tg-dg4k">0.682</td>
    <td class="tg-dg4k">0.850</td>
    <td class="tg-dg4k">0.782</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="jina-reranker-model-highlights">Jina Reranker Model Highlights</h2><p>The Jina Reranker distinguishes itself as a leader in the reranking domain, outperforming key competitors from Cohere, BGE, and BCE, as illustrated in the previous analysis. Its embedding-agnostic nature and unmatched efficacy underscore its premier status in the industry. Furthermore, Jina Reranker boasts notable features that set it apart:</p><h3 id="long-context-length">Long Context Length</h3><p>Jina Reranker stands out from other reranking solutions by accommodating long context lengths. It is capable of handling queries up to 512 tokens and documents as large as 8192 tokens. Furthermore, the model is designed to process up to 2048 candidate documents for each query.</p>
<!--kg-card-begin: html-->
<table>
<thead>
  <tr>
    <th>Time cost of rerank(query=1, docs=100) in ms</th>
    <th colspan="5">#TokensPerDocument</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>#QueryTokens</td>
    <td>256</td>
    <td>512</td>
    <td>1024</td>
    <td>2048</td>
    <td>4096</td>
  </tr>
  <tr>
    <td>64</td>
    <td>156</td>
    <td>323</td>
    <td>1366</td>
    <td>2107</td>
    <td>3571</td>
  </tr>
  <tr>
    <td>128</td>
    <td>194</td>
    <td>369</td>
    <td>1377</td>
    <td>2123</td>
    <td>3598</td>
  </tr>
  <tr>
    <td>256</td>
    <td>273</td>
    <td>475</td>
    <td>1397</td>
    <td>2155</td>
    <td>4299</td>
  </tr>
  <tr>
    <td>512</td>
    <td>468</td>
    <td>1385</td>
    <td>2114</td>
    <td>3536</td>
    <td>7068</td>
  </tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h3 id="enhancing-accuracy-with-rag-integration">Enhancing Accuracy with RAG Integration</h3><p>Retrieval Augmented Generation (RAG) significantly enhances the precision of responses from large language models (LLMs) by incorporating additional, context-specific information&#x2014;such as a company&apos;s internal database&#x2014;into the query process. This method ensures the LLM&apos;s responses are firmly based on provided contextual data. The integration of rerankers with RAG within an information retrieval pipeline paves the way for the development of high-performance applications capable of delivering precise information retrieval across a vast array of domains.</p><p>Consider, for example, the scenario where you&apos;re planning to cook a special meal using specific ingredients favored by your guests. To simplify this task, you decide to employ a chatbot that suggests recipes based on renowned cookbooks. Initially, without a reranker, your input might look something like this:</p><pre><code class="language-text">Context information is below.
---------------------
{Madhur_Jaffrey_An_Invitation_To_Indian_Cooking}
{Julia_Child_Mastering_The_Art_Of_French_Cooking}
{Jiro_Ono_Sushi_Estetica_E_Tecnica}
---------------------

Given the context information and no prior knowledge, answer the query.
Query: Create a recipe using the ingredients below.
---------------------
Salmon
Asparagus
Potatoes
---------------------
</code></pre><p>Incorporating a reranker model, however, allows for a nuanced understanding of document relevance, enhancing the LLM&apos;s context with information about the relative importance of each cookbook. The refined input, highlighting the ranked relevance of the context to the query, might then appear as follows:</p><pre><code class="language-text">Context information is below.
Note that the context is ranked from most to least relevant to the query.
---------------------
{Julia_Child_Mastering_The_Art_Of_French_Cooking}
{Jiro_Ono_Sushi_Estetica_E_Tecnica}
{Madhur_Jaffrey_An_Invitation_To_Indian_Cooking}
---------------------

Given the context information and no prior knowledge, answer the query.
Query: Create a recipe using the ingredients below.
---------------------
Salmon
Asparagus
Potatoes
---------------------
</code></pre><p>This additional layer of insight into the cookbooks&apos; relevance to the given ingredients makes it evident that the optimal recipe likely falls within French cuisine, steering the chatbot&apos;s suggestion in a direction that best matches the ingredients&apos; culinary context. Without reranking, a suggestion based on sushi could have been equally plausible given the prominence of salmon, yet it would have led to a markedly different dining experience.</p><p>Moreover, by determining the number of documents to be considered in the ranking process, users can fine-tune the operation&apos;s precision and computational demands to suit their specific requirements, further illustrating the reranker&apos;s versatility and effectiveness.</p><h3 id="affordable-api-pricing">Affordable API pricing</h3><p>Jina Reranker API comes with identical pricing to our embedding API (including 1 million free trial), such as <code>jina-embeddings-v2-base-en</code> , based on the total number of tokens, in queries and documents. Token quotas can be bought on our <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Embeddings</a>/<a href="https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io">Reranker API</a> page. The API secret and token quotas can be used for both reranker and embedding APIs.</p><h2 id="coming-soon-to-aws-marketplace">Coming Soon to AWS Marketplace</h2><p>Expanding our reach, Jina Reranker will not only be accessible via our API but is also set to debut on the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS SageMaker Marketplace</a> for seamless private cloud deployment. This upcoming availability aims to offer enhanced data protection and application security within the familiar confines of your AWS cloud subscription.</p><p>We highly value your insights and experiences with Jina Reranker. Talk to us on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord channel</a> to share your feedback and stay up-to-date with our latest models. Your input is crucial as we continue to refine our technologies and contribute to a more dynamic and inclusive search AI ecosystem.</p>]]></content:encoded></item><item><title><![CDATA[Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning]]></title><description><![CDATA[Our new paper explores how our Spanish-English and German-English models use multi-task contrastive learning and a sophisticated data pipeline to master language understanding and cross-lingual efficiency for texts up to 8192 tokens]]></description><link>https://jina.ai/news/revolutionizing-bilingual-text-embeddings-with-multi-task-contrastive-learning/</link><guid isPermaLink="false">65df105ab22368000152a1b9</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 28 Feb 2024 15:00:41 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/banner-4models-02.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/banner-4models-02.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><p>In our recent paper, <a href="https://arxiv.org/abs/2402.17016?ref=jina-ai-gmbh.ghost.io">Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</a>, we detailed our development of <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io">German-English</a> and <a href="https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context?ref=jina-ai-gmbh.ghost.io">Spanish-English</a> bilingual text embedding models.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2402.17016?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</div><div class="kg-bookmark-description">We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations. By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embedding models. This integration aims to stimulate further research and advancement in text embedding technologies for these languages.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Isabelle Mohr</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>Our approach utilizes multi-task contrastive learning and advanced data curation pipeline, focusing on bilingual capabilities while extending to support 8192 tokens in length. This method allows our models to excel in understanding target languages and in conducting cross-lingual evaluations efficiently.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context</div><div class="kg-bookmark-description">Jina AI&#x2019;s new bilingual Spanish-English embedding model brings the state-of-the-art in AI to half a billion Spanish speakers.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class="kg-bookmark-description">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>In addition to the bilingual models covered in the paper, we have also developed bilingual <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english?ref=jina-ai-gmbh.ghost.io">Chinese-English</a> and <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">monolingual English</a> models. These additions showcase our commitment to covering a broad spectrum of linguistic needs and furthering our capabilities in language processing.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class="kg-bookmark-description">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"><span class="kg-bookmark-publisher">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI Launches World&#x2019;s First Open-Source 8K Text Embedding, Rivaling OpenAI</div><div class="kg-bookmark-description">Jina AI introduces jina-embeddings-v2, the world&#x2019;s first open-source model boasting an 8K context length. Matching the prowess of OpenAI&#x2019;s proprietary models, this innovation is now publicly accessible on Huggingface, signaling a significant milestone in the landscape of text embeddings.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" alt="Revolutionizing Bilingual Text Embeddings with Multi-Task Contrastive Learning"></div></a></figure><p>Our bilingual models are characterized by their efficiency, operating with optimized vocabulary sizes to require fewer parameters and less memory. This efficiency underscores our dedication to creating powerful yet resource-efficient tools for language processing.</p><p>Following the release of our paper, we expanded the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Massive Text Embedding Benchmark (MTEB)</a> to include benchmarks for our English-German and English-Spanish embedding models. This expansion is part of our effort to stimulate further research and advancements in text embedding technologies for non-English languages.</p><p>At Jina AI, our aim is to enhance the processing and understanding of multiple languages, contributing to the NLP field with our developments in bilingual and monolingual text embedding models.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.4 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-4-update/</link><guid isPermaLink="false">65ddb23939a5b70001810ef5</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Tue, 27 Feb 2024 10:01:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3234">Release Note (<code>3.23.4</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1-1.jpg" alt="Jina 3.23.4 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.4?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.4 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.4) Release time: 2024-02-27 09:36:35 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Use unbound method to link to dry_run endpoint (#6143)
In prior releases, to assess Flow conne&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.4 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/b7615b0bbc1b3d94939b1085aaeac93489f7338634868cf0d42e2cbea2750690/jina-ai/jina/releases/tag/v3.23.4" alt="Jina 3.23.4 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="use-unbound-method-to-link-to-dryrun-endpoint-6143">Use unbound method to link to <code>dry_run</code> endpoint (<a href="https://github.com/jina-ai/jina/pull/6143?ref=jina-ai-gmbh.ghost.io">#6143</a>)</h3><p>In prior releases, to assess Flow connectivity Jina automatically bound an Executor to a <code>_dry_run_</code> endpoint via an object method. This release changes that to a class-unbound method, thus avoiding a cyclic reference in the inner <code>requests</code> object of the <code>Executor</code>.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[What is ColBERT and Late Interaction and Why They Matter in Search?]]></title><description><![CDATA[Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.]]></description><link>https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/</link><guid isPermaLink="false">65d3a2134a32310001f5b71b</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 20 Feb 2024 01:19:04 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><p>Last Friday, the release of the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">ColBERT model by Jina AI on Hugging Face</a> sparked significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is excited about 8192-length ColBERT? This article delves into the intricacies of ColBERT and ColBERTv2, highlighting their design, improvements, and the surprising effectiveness of ColBERT&apos;s late interaction.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximize the search relevancy and RAG accuracy at ease</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-reranker-api.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a></figure><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">@huggingface</a> under Apache 2.0 licence<a href="https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io">https://t.co/snVGgI753H</a></p>&#x2014; Jina AI (@JinaAI_) <a href="https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&amp;ref=jina-ai-gmbh.ghost.io">February 16, 2024</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-is-colbert">What is ColBERT?</h2><p>The name &quot;ColBERT&quot; stands for <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, a model stems from the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as <strong>late interaction</strong>, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the retrieval process. Specifically, there are two versions of the model:</p><ul><li><strong>ColBERT</strong>: The initial model was the brainchild of <a href="https://x.com/lateinteraction?s=20&amp;ref=jina-ai-gmbh.ghost.io"><strong>Omar Khattab</strong></a><strong> and Matei Zaharia</strong>, presenting a novel approach to information retrieval through the &quot;ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT&quot; paper. Their work was published in SIGIR 2020.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class="kg-bookmark-description">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT&#x2019;s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT&#x2019;s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Omar Khattab</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p><span style="white-space: pre-wrap;">The original ColBERT paper that introduces the &quot;late interaction&quot;.</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Building on the foundational work, <strong>Omar Khattab</strong> continued his research, collaborating with <strong>Barlas Oguz, Matei Zaharia, and Michael S. Bernstein</strong> to introduce &quot;ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,&quot; presented at SIGIR 2021. This next iteration of ColBERT addressed previous limitations and introduced key improvements, such as <strong>denoised supervision</strong> and <strong>residual compression</strong>, enhancing both the model&apos;s retrieval effectiveness and its storage efficiency.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class="kg-bookmark-description">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\times$.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Keshav Santhanam</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a><figcaption><p><span style="white-space: pre-wrap;">ColBERTv2 adding denoised supervision and residual compression to improve the training data&apos;s quality and reduce the space footprint.</span></p></figcaption></figure><h2 id="understand-colberts-design">Understand ColBERT&apos;s Design</h2><p>Given that ColBERTv2&apos;s architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the original ColBERT.</p><h3 id="what-is-late-interaction-in-colbert">What is late interaction in ColBERT?</h3><p>&quot;Interaction&quot; refers to the process of evaluating the relevance between a query and a document by comparing their representations.</p><p>&quot;<em>Late interaction</em>&quot; is the essence of ColBERT. The term is derived from the model&apos;s architecture and processing strategy, where the interaction between the query and document representations occurs late in the process, after both have been independently encoded. This contrasts with &quot;<em>early interaction</em>&quot; models, where query and document embeddings interact at earlier stages, typically before or during their encoding by the model. </p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Interaction Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early Interaction</td>
<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>
</tr>
<tr>
<td>Late Interaction</td>
<td>ColBERT, ColBERTv2</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Early interaction can increase computational complexity since it requires considering all possible query-document pairs, making it less efficient for large-scale applications. </p><p>Late interaction models like ColBERT optimize for efficiency and scalability by allowing for the pre-computation of document representations and employing a more lightweight interaction step at the end, which focuses on the already encoded representations. This design choice enables faster retrieval times and reduced computational demands, making it more suitable for processing large document collections.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="300" height="143"><figcaption><span style="white-space: pre-wrap;">Schematic diagrams illustrating query&#x2013;document interaction paradigms in neural IR, with ColBERT&apos;s late interaction on the left-most.</span></figcaption></figure><h3 id="no-interaction-cosine-similarity-of-document-and-query-embeddings">No interaction: cosine similarity of document and query embeddings</h3><p>Many practical vector databases and neural search solutions depend on fast cosine similarity matching between document and query embeddings. While appealing for its straightforwardness and computational efficiency, this method, often referred to as &quot;<em>no interaction</em>&quot; or &quot;<em>not interaction-based</em>&quot; has been found to underperform in comparison to models that incorporate some form of interaction between queries and documents.</p><p>The core limitation of the &quot;no interaction&quot; approach lies in its inability to capture the complex nuances and relationships between query and document terms. Information retrieval, at its heart, is about understanding and matching the intent behind a query with the content within a document. This process often requires a deep, contextual understanding of the terms involved, something that single, aggregated embeddings for documents and queries struggle to provide.</p><h2 id="query-and-document-encoders-in-colbert">Query and document encoders in ColBERT</h2><p>ColBERT&apos;s encoding strategy is grounded in the BERT model, known for its deep contextual understanding of language. The model generates dense vector representations for each token in a query or document, <strong>creating a bag of contextualized embeddings for a query and a bag for a document, respectively. </strong>This facilitates a nuanced comparison of their embeddings during the late interaction phase.</p><h3 id="query-encoder-of-colbert">Query encoder of ColBERT</h3><p>For a query $Q$ with tokens ${q_1, q_2, ..., q_l}$, the process begins by tokenizing $Q$ into BERT-based WordPiece tokens and prepending a special <code>[Q]</code> token. This <code>[Q]</code> token, positioned right after BERT&#x2019;s <code>[CLS]</code> token, signals the start of a query. </p><p>If the query is shorter than a predefined number of tokens $N_q$, it is padded with <code>[mask]</code> tokens up to $N_q$; otherwise, it&apos;s truncated to the first $N_q$ tokens. The padded sequence is then passed through BERT, followed by a CNN (Convolutional Neural Network) and normalization. The output is a set of embedding vectors termed as $\mathbf{E}_q$ below:<br>$$\mathbf{E}_q := \mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[Q]},q_0,q_1,\ldots,q_l\mathtt{[mask]},\mathtt{[mask]},\ldots,\mathtt{[mask]}\right)\right)$$</p><h3 id="document-encoder-of-colbert">Document encoder of ColBERT</h3><p>Similarly, for a document $D$ with tokens ${d_1, d_2, ..., d_n}$, a <code>[D]</code> token is prepended to indicate the start of a document. This sequence, without the need for padding, undergoes the same process, results in a set of embedding vectors termed as $\mathbf{E}_d$ below:<br>$$\mathbf{E}_d := \mathrm{Filter}\left(\mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[D]},d_0,d_1,...,d_n\right)\right)\right)$$</p><p>The use of <code>[mask]</code> tokens for padding queries (coined as <strong>query augmentation </strong>in the paper) ensures uniform length across all queries, facilitating batch processing. The <code>[Q]</code> and <code>[D]</code> tokens explicitly mark the start of queries and documents, respectively, aiding the model in distinguishing between the two types of inputs.</p><h3 id="comparing-colbert-to-cross-encoders">Comparing ColBERT to cross-encoders</h3><p>Cross-encoders process pairs of queries and documents together, making them highly accurate but less efficient for large-scale tasks due to the computational cost of evaluating every possible pair. They excel in specific scenarios where the precise scoring of sentence pairs is necessary, such as in semantic similarity tasks or detailed content comparison. However, this design limits their applicability in situations requiring rapid retrieval from large datasets, where pre-computed embeddings and efficient similarity calculations are paramount.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg" class="kg-image" alt="What is ColBERT and Late Interaction and Why They Matter in Search?" loading="lazy" width="210" height="150"></figure><p>In contrast, ColBERT&#x2019;s late interaction model allows for pre-computation of document embeddings, significantly speeding up the retrieval process without compromising the depth of semantic analysis. This method, though seemingly counter-intuitive when compared to the direct approach of cross-encoders, offers a scalable solution for real-time and large-scale information retrieval tasks. It represents a strategic compromise between computational efficiency and the quality of interaction modeling.</p><h2 id="finding-the-top-k-documents-using-colbert">Finding the top-K documents using ColBERT</h2><p>Once we have embeddings for the query and documents, finding the most relevant top-K documents becomes straightforward (but not as straightforward as computing cosine of two vectors). </p><p>The key operations include a batch dot-product to compute term-wise similarities, max-pooling across document terms to find the highest similarity per query term, and summation across query terms to derive the total document score, followed by sorting the documents based on these scores. The pseudo PyTorch code is described below:</p><pre><code class="language-python">import torch

def compute_relevance_scores(query_embeddings, document_embeddings, k):
    &quot;&quot;&quot;
    Compute relevance scores for top-k documents given a query.
    
    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]
    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]
    :param k: Number of top documents to re-rank
    :return: Sorted document indices based on their relevance scores
    &quot;&quot;&quot;
    
    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]
    # Pad the k documents to their maximum length for batch operations
    # Note: Assuming document_embeddings is already padded and moved to GPU
    
    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)
    # Resulting shape: [k, num_query_terms, max_doc_length]
    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))
    
    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term
    # Shape after max-pool: [k, num_query_terms]
    max_scores_per_query_term = scores.max(dim=2).values
    
    # Sum the scores across query terms to get the total score for each document
    # Shape after sum: [k]
    total_scores = max_scores_per_query_term.sum(dim=1)
    
    # Sort the documents based on their total scores
    sorted_indices = total_scores.argsort(descending=True)
    
    return sorted_indices
</code></pre><p>Note that this procedure is used in both training and re-ranking at inference time. The ColBERT model is trained using a pairwise ranking loss, where the training data consists of triples $(q, d^+, d^-)$, where $q$ represents a query, $d^+$ is a relevant (positive) document for the query, and $d^-$ is a non-relevant (negative) document. The model aims to learn representations such that the similarity score between $q$ and $d^+$ is higher than the score between q and $d^-$.</p><p>The training objective can be mathematically represented as minimizing the following loss function: $$\mathrm{Loss} = \max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, where $S(q, d)$ denotes the similarity score computed by ColBERT between a query $q$ and a document $d$. This score is obtained by aggregating the max-similarity scores of the best-matching embeddings between the query and the document, following the late interaction pattern described in the model architecture. This approach ensures that the model is trained to distinguish between relevant and irrelevant documents for a given query, by encouraging a larger margin in the similarity scores for positive and negative document pairs.</p><h3 id="denoised-supervision-in-colbertv2">Denoised supervision in ColBERTv2</h3><p>Denoised supervision in ColBERTv2 refines the original training process by selecting challenging negatives and leveraging a cross-encoder for distillation. This sophisticated method of augmenting training data quality involves several steps:</p><ol><li><strong>Initial Training</strong>: Utilizing the official triples from the MS MARCO dataset, comprising a query, a relevant document, and a non-relevant document.</li><li><strong>Indexing and Retrieval</strong>: Employing ColBERTv2&apos;s compression to index training passages, followed by retrieving top-k passages for each query.</li><li><strong>Cross-Encoder Reranking</strong>: Enhancing passage selection through reranking by a MiniLM cross-encoder, distilling its scores into ColBERTv2.</li><li><strong>Forming Training Tuples</strong>: Generating w-way tuples for training, incorporating both high and lower-ranked passages to create challenging examples.</li><li><strong>Iterative Refinement</strong>: Repeating the process to continually improve the selection of hard negatives, thereby enhancing model performance.</li></ol><p>Note, this process represents a sophisticated enhancement to the ColBERT training regime rather than a fundamental change to its architecture.</p><h3 id="hyperparameters-of-colbert">Hyperparameters of ColBERT</h3><p>The hyperparameters of ColBERT is summarized below:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Best Choice</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>3 x 10^{-6}</td>
<td>Selected for fine-tuning to ensure stable and effective model updates.</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>Balances computational efficiency and the ability to capture sufficient information per update.</td>
</tr>
<tr>
<td>Number of Embeddings per Query (Nq)</td>
<td>32</td>
<td>Fixed to ensure a consistent representation size across queries, aiding in efficient processing.</td>
</tr>
<tr>
<td>Embedding Dimension (m)</td>
<td>128</td>
<td>Demonstrated to provide a good balance between representational power and computational efficiency.</td>
</tr>
<tr>
<td>Training Iterations</td>
<td>200k (MS MARCO), 125k (TREC CAR)</td>
<td>Chosen to ensure thorough learning while avoiding overfitting, with adjustments based on dataset characteristics.</td>
</tr>
<tr>
<td>Bytes per Dimension in Embeddings</td>
<td>4 (re-ranking), 2 (end-to-end ranking)</td>
<td>Trade-off between precision and space efficiency, with consideration for the application context (re-ranking vs. end-to-end).</td>
</tr>
<tr>
<td>Vector-Similarity Function</td>
<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>
<td>Selected based on performance and efficiency in the respective retrieval contexts.</td>
</tr>
<tr>
<td>FAISS Index Partitions (P)</td>
<td>2000</td>
<td>Determines the granularity of the search space partitioning, impacting search efficiency.</td>
</tr>
<tr>
<td>Nearest Partitions Searched (p)</td>
<td>10</td>
<td>Balances the breadth of the search against computational efficiency.</td>
</tr>
<tr>
<td>Sub-vectors per Embedding (s)</td>
<td>16</td>
<td>Affects the granularity of quantization, influencing both search speed and memory usage.</td>
</tr>
<tr>
<td>Index Representation per Dimension</td>
<td>16-bit values</td>
<td>Chosen for the second stage of end-to-end retrieval to manage the trade-off between accuracy and space.</td>
</tr>
<tr>
<td>Number of Layers in Encoders</td>
<td>12-layer BERT</td>
<td>Optimal balance between depth of contextual understanding and computational efficiency.</td>
</tr>
  <tr>
  <td>Max Query Length</td>
<td>128</td>
<td>The maximum number of tokens processed by the query encoder. <b>This gets extended in Jina-ColBERT model.</b></td>
</tr>
    <tr>
  <td>Max Document Length</td>
<td>512</td>
<td>The maximum number of tokens processed by the document encoder. <b>This gets extended to 8192 in Jina-ColBERT model. </b></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="the-indexing-strategy-of-colbert">The indexing strategy of ColBERT</h2><p>Unlike representation-based approaches that encode each document into one embedding vector, <strong>ColBERT encodes documents (and queries) into bags of embeddings, with each token in a document having its own embedding.</strong> This approach inherently means that for longer documents, more embeddings will be stored, <strong>which is a pain point of  the original ColBERT, and later addressed by ColBERTv2.</strong></p><p>The key to managing this efficiently lies in ColBERT&apos;s use of vector database (e.g. <a href="https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io">FAISS</a>) for indexing and retrieval, and its detailed indexing process which is designed to handle large volumes of data efficiently. The original ColBERT paper mentions several strategies to enhance the efficiency of indexing and retrieval, including:</p><ul><li><strong>Offline Indexing</strong>: Document representations are computed offline, allowing for the pre-computation and storage of document embeddings. This process leverages batch processing and GPU acceleration to handle large document collections efficiently.</li><li><strong>Embedding Storage</strong>: Document embeddings can be stored using 32-bit or 16-bit values for each dimension, offering a trade-off between precision and storage requirements. This flexibility allows ColBERT to maintain a balance between effectiveness (in terms of retrieval performance) and efficiency (in terms of storage and computational costs).</li></ul><p>The introduction of <strong>residual compression</strong> in ColBERTv2, which is a novel approach not present in the original ColBERT, plays a key role in reducing the model&apos;s space footprint by 6&#x2013;10&#xD7; while preserving quality. This technique compresses the embeddings further by effectively capturing and storing only the differences from a set of fixed reference centroids. </p><h2 id="effectiveness-and-efficiency-of-colbert">Effectiveness and Efficiency of ColBERT</h2><p>One might initially assume that incorporating BERT&apos;s deep contextual understanding into search would inherently require significant computational resources, making such an approach less feasible for real-time applications due to high latency and computational costs. However, ColBERT challenges and overturns this assumption through its innovative use of the late interaction mechanism. Here are some noteworthy points:</p><ol><li><strong>Significant Efficiency Gains</strong>: ColBERT achieves an orders-of-magnitude reduction in computational costs (FLOPs) and latency compared to traditional BERT-based ranking models. Specifically, for a given model size (e.g., 12-layer &quot;base&quot; transformer encoder), ColBERT not only matches but in some cases exceeds the effectiveness of BERT-based models with dramatically lower computational demands. For instance, at a re-ranking depth of <em>k</em>=10, BERT requires nearly 180&#xD7; more FLOPs than ColBERT; this gap widens as <em>k</em> increases, reaching 13900&#xD7; at <em>k</em>=1000 and even 23000&#xD7; at <em>k</em>=2000&#x200B;&#x200B;.</li><li><strong>Improved Recall and MRR@10 in End-to-End Retrieval</strong>: Contrary to the initial intuition that deeper interaction between query and document representations (as seen in early interaction models) would be necessary for high retrieval performance, ColBERT&apos;s end-to-end retrieval setup demonstrates superior effectiveness. For example, its Recall@50 exceeds the official BM25&apos;s Recall@1000 and almost all other models&apos; Recall@200, underscoring the model&apos;s remarkable ability to retrieve relevant documents from a vast collection without direct comparison of each query-document pair&#x200B;&#x200B;.</li><li><strong>Practicality for Real-World Applications</strong>: The experimental results underline ColBERT&apos;s practical applicability for real-world scenarios. Its indexing throughput and memory efficiency make it suitable for indexing large document collections like MS MARCO within a few hours, retaining high effectiveness with a manageable space footprint. These qualities highlight ColBERT&apos;s suitability for deployment in production environments where both performance and computational efficiency are paramount&#x200B;&#x200B;.</li><li><strong>Scalability with Document Collection Size</strong>: Perhaps the most surprising conclusion is ColBERT&apos;s scalability and efficiency in handling large-scale document collections. The architecture allows for the pre-computation of document embeddings and leverages efficient batch processing for query-document interaction, enabling the system to scale effectively with the size of the document collection. This scalability is counter-intuitive when considering the complexity and depth of understanding required for effective document retrieval, showcasing ColBERT&apos;s innovative approach to balancing computational efficiency with retrieval effectiveness.</li></ol><h2 id="using-jina-colbert-v1-en-a-8192-length-colbertv2-model">Using <code>jina-colbert-v1-en</code>: a 8192-length ColBERTv2 model</h2><p>Jina-ColBERT is designed for both fast and accurate retrieval, supporting <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">longer context lengths up to 8192, leveraging the advancements of JinaBERT</a>, which allows for longer sequence processing due to its architecture enhancements.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Strictly speaking Jina-ColBERT supports 8190-token length. Recall that in ColBERT document encoder, each document is padded with <code spellcheck="false" style="white-space: pre-wrap;">[D],[CLS]</code> in the beginning.</div></div><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-colbert-v1-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png" alt="What is ColBERT and Late Interaction and Why They Matter in Search?"></div></a></figure><h3 id="jinas-improvement-over-original-colbert">Jina&apos;s improvement over original ColBERT</h3><p>Jina-ColBERT&apos;s main advancement is its backbone, <code>jina-bert-v2-base-en</code>, which enables processing of significantly longer contexts (up to 8192 tokens) compared to the original ColBERT that uses <code>bert-base-uncased</code>. This capability is crucial for handling documents with extensive content, providing more detailed and contextual search results.</p><h3 id="jina-colbert-v1-en-performance-comparison-vs-colbertv2"><code>jina-colbert-v1-en</code> performance comparison vs. ColBERTv2</h3><p>We evaluated <code>jina-colbert-v1-en</code> on BEIR datasets and new LoCo benchmark which favors long-context, tested it against the original ColBERTv2 implementation and non-interaction based <code>jina-embeddings-v2-base-en</code> model.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Dataset</th>
<th>ColBERTv2</th>
<th>jina-colbert-v1-en</th>
<th>jina-embeddings-v2-base-en</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arguana</td>
<td>46.5</td>
<td><strong>49.4</strong></td>
<td>44.0</td>
</tr>
<tr>
<td>Climate-Fever</td>
<td>18.1</td>
<td>19.6</td>
<td><strong>23.5</strong></td>
</tr>
<tr>
<td>DBPedia</td>
<td><strong>45.2</strong></td>
<td>41.3</td>
<td>35.1</td>
</tr>
<tr>
<td>FEVER</td>
<td>78.8</td>
<td><strong>79.5</strong></td>
<td>72.3</td>
</tr>
<tr>
<td>FiQA</td>
<td>35.4</td>
<td>36.8</td>
<td><strong>41.6</strong></td>
</tr>
<tr>
<td>HotpotQA</td>
<td><strong>67.5</strong></td>
<td>65.9</td>
<td>61.4</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>33.7</td>
<td><strong>33.8</strong></td>
<td>32.5</td>
</tr>
<tr>
<td>NQ</td>
<td>56.1</td>
<td>54.9</td>
<td><strong>60.4</strong></td>
</tr>
<tr>
<td>Quora</td>
<td>85.5</td>
<td>82.3</td>
<td><strong>88.2</strong></td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>15.4</td>
<td>16.9</td>
<td><strong>19.9</strong></td>
</tr>
<tr>
<td>SciFact</td>
<td>68.9</td>
<td><strong>70.1</strong></td>
<td>66.7</td>
</tr>
<tr>
<td>TREC-COVID</td>
<td>72.6</td>
<td><strong>75.0</strong></td>
<td>65.9</td>
</tr>
<tr>
<td>Webis-touch2020</td>
<td>26.0</td>
<td><strong>27.0</strong></td>
<td>26.2</td>
</tr>
<tr>
<td>LoCo</td>
<td>74.3</td>
<td>83.7</td>
<td><strong>85.4</strong></td>
</tr>
<tr>
<td>Average</td>
<td>51.7</td>
<td><strong>52.6</strong></td>
<td>51.6</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>This table demonstrates <code>jina-colbert-v1-en</code>&apos;s superior  performance, especially in scenarios requiring longer context lengths vs the original ColBERTv2. Note that <code>jina-embeddings-v2-base-en</code> <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">uses more training data</a>, whereas <code>jina-colbert-v1-en</code> only uses MSMARCO, which may justify the good performance of <code>jina-embeddings-v2-base-en</code> on some tasks.</p><h3 id="example-usage-of-jina-colbert-v1-en">Example usage of <code>jina-colbert-v1-en</code></h3><p>This snippet outlines the indexing process with Jina-ColBERT, showcasing its support for long documents.</p><pre><code class="language-python">from colbert import Indexer
from colbert.infra import Run, RunConfig, ColBERTConfig

n_gpu: int = 1  # Set your number of available GPUs
experiment: str = &quot;&quot;  # Name of the folder where the logs and created indices will be stored
index_name: str = &quot;&quot;  # The name of your index, i.e. the name of your vector database

if __name__ == &quot;__main__&quot;:
    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):
        config = ColBERTConfig(
          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents
        )
        indexer = Indexer(
          checkpoint=&quot;jinaai/jina-colbert-v1-en&quot;,
          config=config,
        )
        documents = [
          &quot;ColBERT is an efficient and effective passage retrieval model.&quot;,
          &quot;Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.&quot;,
          &quot;JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.&quot;,
          &quot;Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.&quot;,
          &quot;Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.&quot;,
          &quot;Jina is an easier way to build neural search systems.&quot;,
          &quot;You can use Jina-ColBERT to build neural search systems with ease.&quot;,
          # Add more documents here to ensure the clustering work correctly
        ]
        indexer.index(name=index_name, collection=documents)
</code></pre><h3 id="use-jina-colbert-v1-en-in-ragatouille">Use <code>jina-colbert-v1-en</code> in RAGatouille</h3><p>RAGatouille is a new Python library that facilitates the use of advanced retrieval methods within RAG pipelines. It&apos;s designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines, making it accessible for developers to utilize these methods without needing deep expertise in the underlying research. Thanks to <a href="https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io">Benjamin Clavi&#xE9;</a>, you can now use <code>jina-colbert-v1-en</code> easily:</p><pre><code class="language-python">from ragatouille import RAGPretrainedModel

# Get your model &amp; collection of big documents ready
RAG = RAGPretrainedModel.from_pretrained(&quot;jinaai/jina-colbert-v1-en&quot;)
my_documents = [
    &quot;very long document1&quot;,
    &quot;very long document2&quot;,
    # ... more documents
]

# And create an index with them at full length!
RAG.index(collection=my_documents,
          index_name=&quot;the_biggest_index&quot;,
          max_document_length=8190,)

# or encode them in-memory with no truncation, up to your model&apos;s max length
RAG.encode(my_documents)
</code></pre><p>For more detailed information and further exploration of Jina-ColBERT, you can visit the <a href="https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io">Hugging Face page</a>.</p><h2 id="conclusion">Conclusion</h2><p>ColBERT represents a significant leap forward in the field of information retrieval. By enabling longer context lengths with Jina-ColBERT and maintaining compatibility with the ColBERT approach to late interaction, it offers a powerful alternative for developers looking to implement state-of-the-art search functionality.</p><p>Coupled with the RAGatouille library, which simplifies the integration of complex retrieval models into RAG pipelines, developers can now harness the power of advanced retrieval with ease, streamlining their workflows and enhancing their applications. The synergy between Jina-ColBERT and RAGatouille illustrates a remarkable stride in making advanced AI search models accessible and efficient for practical use.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.3 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-3-update/</link><guid isPermaLink="false">65cf33b44a32310001f5b6d8</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 16 Feb 2024 11:29:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3233">Release Note (<code>3.23.3</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Image-Jina-dark-1.jpg" alt="Jina 3.23.3 Update"><p>This release contains 1 bug fix and 1 documentation improvement.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.3?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.3 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.3) Release time: 2024-02-16 10:56:19 This release contains 1 bug fix and 1 documentation improvement.
&#x1F41E; Bug Fixes
Fix dynamic creation of schema with nested DocLists in Gateway&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.3 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/261d3e0cd12981bbe960a3e5cee076fdc791818f19a3ffb9a68e79eafe63114c/jina-ai/jina/releases/tag/v3.23.3" alt="Jina 3.23.3 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-dynamic-creation-of-schema-with-nested-doclists-in-gateway-6138">Fix dynamic creation of schema with nested DocLists in Gateway (<a href="https://github.com/jina-ai/jina/pull/6138?ref=jina-ai-gmbh.ghost.io">#6138</a>)</h3><p>Previously, nested document types with multiple&#xA0;<code>DocList</code>&#xA0;levels of nesting sometimes led to the schema exposed by the Gateway being incomplete because some references were lost.</p><p>Now schemas like this exposed through&#xA0;<code>Flow</code>&#xA0;work as expected:</p><pre><code class="language-python">class QuoteFile(BaseDoc):
    quote_file_id: int
    texts: DocList[TextDoc]
    images: DocList[ImageDoc]

class SearchResult(BaseDoc):
    results: DocList[QuoteFile]</code></pre><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Fix topology documentation (<a href="https://github.com/jina-ai/jina/pull/6134?ref=jina-ai-gmbh.ghost.io">docs: fix topology documentation<u>&#xA0;#6134</u></a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Aquí Se Habla Español: Top-Quality Spanish-English Embeddings and 8k Context]]></title><description><![CDATA[Jina AI's new bilingual Spanish-English embedding model brings the state-of-the-art in AI to half a billion Spanish speakers.]]></description><link>https://jina.ai/news/aqui-se-habla-espanol-top-quality-spanish-english-embeddings-and-8k-context/</link><guid isPermaLink="false">65cc8e2f4a32310001f5b5f1</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 14 Feb 2024 15:30:36 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/1334.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><p>Jina AI is <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">once</a> <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">again</a> <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">demonstrating</a> its commitment to high-quality multilingual AI models by releasing its <strong>Spanish-English bilingual model</strong>.</p><p>This model provides embedding vectors for texts of up to 8k tokens in Spanish or English, designed so that if texts in the two languages mean the same thing, their embeddings will be geometrically close together. Jina Embeddings v2 for Spanish and English is ideally suited for cross-language information retrieval, bilingual semantic analysis, and bilingual RAG applications.</p><p>This new model, <code>jina-embeddings-v2-base-es</code>, brings to Spanish the same state-of-the-art performance and ground-breaking feature set of Jina AI&#x2019;s <code>v2</code> models for <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English</a>, <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>:</p><ul><li>8,192 tokens of input context, a leader among open-source embedding models.</li><li>Real bilingualism instead of uneven multilingualism. Jina AI&#x2019;s bilingual models are trained to give balanced support to both languages, avoiding the <a href="https://arxiv.org/abs/2210.05619?ref=jina-ai-gmbh.ghost.io">biases of &#x201C;multilingual&#x201D; models</a> trained on uncurated Internet scrapes.</li><li><code>jina-embeddings-v2-base-es</code>&#xA0;is compact compared to open-source models of comparable performance. The embeddings themselves are 768 dimensions, saving space and run-time in production.</li><li>Jina Embeddings v2 models are fully integrated into major vector databases, RAG frameworks, and AI development libraries:<ul><li><a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a></li><li><a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a></li><li><a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a></li><li><a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul></li></ul><p>Jina Embeddings v2 for Spanish and English is accessible via the Jina <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Embeddings API </a> right now, with one million free tokens, so you pay nothing to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><h2 id="benchmarks">Benchmarks</h2><p>On Spanish benchmarks, Jina v2 for Spanish and English outperforms the <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Multilingual E5 base model</a> and the <a href="https://huggingface.co/BAAI/bge-m3?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">BGE M3 model</a>, the only comparable open-source models with Spanish support. The tests below (<a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MTEB-es</a>) are adapted from the <a href="https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Massive Text Embeddings Benchmark</a>. You can view and run them from this <a href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">GitHub repository</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-es?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-es: MTEB: Massive Text Embedding Benchmark with Spanish datasets</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark with Spanish datasets - jina-ai/mteb-es</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/6e71c1c34b09bea0278cbed9f0c639554954e953c35b93d4576539b808f0f813/jina-ai/mteb-es" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="2000" height="227" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-10.png 2241w"></figure><p>Jina Embeddings outperforms E5 on all metrics except classification and outperforms BGE-M3 in retrieval, clustering, and cross-language tasks, despite being 15% to 30% of the size of these larger models.</p><ul><li>Significantly better performance in <strong>retrieval</strong> tasks (like finding related documents in a database) and <strong>clustering</strong> (identifying groups of documents that belong together in a collection)</li><li>Roughly equal performance with E5 on <strong>reranking</strong> (ordering documents by semantic similarity) and near-equal performance on <strong>text classification</strong> in Spanish.</li><li>All three models have very similar benchmark scores for <strong>cross-language tasks</strong> (finding semantically similar texts in English to a Spanish input, or vice-versa), although Jina Embeddings still performs the best.</li></ul><p>When compared to closed-source multilingual models from Open AI and Cohere, Jina Embeddings&#x2019; compact size makes its achievements even more impressive.</p><figure class="kg-card kg-image-card kg-width-full"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png" class="kg-image" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context" loading="lazy" width="1992" height="254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-11.png 1992w"></figure><p>On retrieval tasks in Spanish, Jina outperforms the closed-source models offered by Open AI and Cohere and outperforms Open AI (and nearly equals Cohere&#x2019;s performance) on cross-language tasks.</p><h2 id="jina-embeddings-ai-for-a-multilingual-world">Jina Embeddings: AI for a Multilingual World</h2><p>Spanish is spoken by well over half a billion people, with official status in more than 20 countries, along with the <a href="https://european-union.europa.eu/index_es?ref=jina-ai-gmbh.ghost.io">European Union</a>, the <a href="https://www.un.org/es/?ref=jina-ai-gmbh.ghost.io">United Nations</a>, the <a href="https://www.wto.org/indexsp.htm?ref=jina-ai-gmbh.ghost.io">World Trade Organization</a>, and <a href="https://www.fifa.com/fifaplus/es/?ref=jina-ai-gmbh.ghost.io">FIFA</a>. Introducing this specialized bilingual model makes clear Jina AI&#x2019;s commitment to bringing AI technologies to everyone.</p><p>In addition to Spanish and its high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">English monolingual model</a>, Jina AI currently offers state-of-the-art embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">German</a>, <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Chinese</a>, and <a href="https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">programming languages</a>, with more to come.</p><p>Jina AI is committed to advancing AI technology for the broadest audience, placing a high value on transparency, accessibility, affordability, privacy, and data protection.</p><p>We value your feedback on all our models. Join our community channel to contribute and stay informed about new developments.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4427 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Aqu&#xED; Se Habla Espa&#xF1;ol: Top-Quality Spanish-English Embeddings and 8k Context"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command]]></title><description><![CDATA[Supercharge your Ghost blog's SEO and accessibility by automatically generating alt texts. All it takes is one command. WordPress/WooCommerce support coming soon!]]></description><link>https://jina.ai/news/scenexplain-alt-text-and-better-seo-for-your-ghost-blog-with-just-one-command/</link><guid isPermaLink="false">65c20d31aa949b00016cf15f</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 06 Feb 2024 15:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Frame-378.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Want to skip straight to generating alt texts for your Ghost blog? Check out our <a href="#Google-Colab-notebook" rel="noreferrer">notebook</a> or <a href="#Docker" rel="noreferrer">Docker instructions</a> for more</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Frame-378.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><p><a href="https://en.wikipedia.org/wiki/Alt_attribute?ref=jina-ai-gmbh.ghost.io">Alt text</a>, or alternative text, plays a crucial role in Search Engine Optimization (SEO) as it helps search engines understand the content of images on a webpage. By providing concise, relevant descriptions of images, alt text improves the likelihood of a website ranking higher in search engine results. This is particularly important for image search optimization, as alt text can significantly influence how search engines index and display images in search results.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09.webm" poster="https://img.spacergif.org/v1/887x667/0a/spacer.png" width="887" height="667" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Automatically-generated alt texts being displayed on Jina AI&apos;s blog</span></p></figcaption>
        </figure><p>Beyond enhancing SEO, using alt texts to improve accessibility (<a href="https://en.wikipedia.org/wiki/Computer_accessibility?ref=jina-ai-gmbh.ghost.io">a11y</a>) is vital for providing a better experience for visually impaired users. Alt text enables screen readers to describe images, allowing visually impaired individuals to fully comprehend a webpage&apos;s content. Additionally, incorporating alt text is essential for complying with web accessibility standards and regulations, such as the <a href="https://www.ada.gov/?ref=jina-ai-gmbh.ghost.io">Americans with Disabilities Act (ADA)</a>, <a href="https://www.w3.org/WAI/standards-guidelines/wcag/?ref=jina-ai-gmbh.ghost.io">Web Content Accessibility Guidelines (WCAG)</a>, and European Union directives like the <a href="https://en.wikipedia.org/wiki/European_Accessibility_Act?ref=jina-ai-gmbh.ghost.io">European Accessibility Act (EAA)</a>. Adhering to these standards not only reaches a broader audience but also shows a commitment to inclusivity and ensures equal access to digital information for all users.</p><h2 id="manually-adding-alt-texts-doesnt-scale">Manually adding alt texts doesn&apos;t scale</h2><p>Traditionally, writing alt text for images has been a labor-intensive task, especially for websites with extensive content, such as hundreds of blog posts or thousands of e-commerce products. Manually creating descriptive and accurate alt text for each image can be time-consuming and resource-intensive. This challenge is amplified in dynamic websites where new content is frequently added, requiring ongoing effort to ensure all images are accompanied by appropriate alt text. The task demands not only a significant investment of time but also attention to detail to maintain consistency and relevance across all images, making it a daunting task for content creators and webmasters.</p><h2 id="apis-can-help-but-you-still-need-to-write-the-middleware">APIs can help, but you still need to write the middleware</h2><p>Utilizing APIs like <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">SceneXplain</a>&#x2019;s to generate alt texts can significantly expedite the process of creating descriptive text for images, especially for websites with a large volume of visual content. Indeed, we&#x2019;ve written several posts on the subject!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/make-accessibility-accessible-generate-alt-text-with-scenexplain?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Making Accessibility Accessible: Create Alt Text with SceneXplain&#x2019;s API</div><div class="kg-bookmark-description">SceneXplain is your accessibility ally, making it easy to generate image alt texts to aid visually-impaired users and improve SEO</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplains-json-schema-store-automate-your-alt-text-and-more?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain&#x2019;s JSON Schema Store: Automate Your Alt-Text, and More!</div><div class="kg-bookmark-description">Take the hassle out of extracting data from images with SceneXplain&#x2019;s new JSON Schema Store. Discover and share reusable JSON schemas. Create, contribute, and access schemas easily through GUI or API</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><p>SceneXplain&#x2019;s API uses advanced algorithms and machine learning techniques to analyze images and automatically generate relevant alt texts, drastically reducing the manual workload. This can be a game-changer for efficiently managing websites with numerous images, such as e-commerce platforms and large-scale blogs.</p><p>However, while the API offers a streamlined solution for alt text generation, integrating the API into an existing platform still requires time and effort. Writing middleware is necessary to bridge the gap between the API and your website&apos;s own API or content management system. This middleware facilitates communication between the two, ensuring that the generated alt texts are correctly associated with their respective images on your website. Although this initial setup requires technical expertise and resources, once in place, it can provide a more automated and scalable approach to maintaining web accessibility and SEO standards.</p><h2 id="alt-texter-the-missing-middle-layer">Alt Texter: the missing middle layer</h2><p><a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io"><strong>Alt Texter</strong></a> serves as the missing link between SceneXplain and your content platform, streamlining the process of populating alt texts for images across your website. This tool is designed to operate directly from the command line, enabling you to efficiently go through all blog posts and automatically insert alt texts for content images and featured images. By running a single command, Alt Texter leverages the capabilities of SceneXplain to analyze images and generate appropriate alt texts, which are then seamlessly integrated into your website&apos;s content.</p><p>Currently, Alt Texter is specifically tailored for <a href="https://ghost.org/?ref=jina-ai-gmbh.ghost.io">Ghost blogs</a>, offering a solution for this popular blogging platform. However, its potential for expansion is significant. In the future, Alt Texter can be adapted to work with other widely-used platforms such as <a href="https://wordpress.org/download/?ref=jina-ai-gmbh.ghost.io">WordPress</a>, <a href="https://woo.com/?ref=jina-ai-gmbh.ghost.io">WooCommerce</a>, <a href="https://woo.com/?ref=jina-ai-gmbh.ghost.io">Shopify</a>, or any other platform with an API.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">WordPress and WooCommerce support is currently experimental. The functions are there, but more testing needs to be done. Let us know if you&#x2019;re interested!</div></div><p>This expansion would make the tool even more versatile and valuable, providing a wide range of website owners with an efficient, automated solution for enhancing web accessibility and SEO through accurate and descriptive alt texts.</p><h2 id="see-the-results">See the results</h2><p>You can witness the effectiveness of &quot;Alt Texter&quot; in conjunction with SceneXplain on <a href="https://jina.ai/news/?ref=jina-ai-gmbh.ghost.io">Jina AI&apos;s blog</a>. Here&apos;s that video again for reference:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09.webm" poster="https://img.spacergif.org/v1/887x667/0a/spacer.png" width="887" height="667" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-11-55-09_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>On this blog, every image is equipped with alt text generated by SceneXplain, and you can easily view this by simply hovering over any image. When you hover, a tooltip appears displaying the alt text along with a &quot;Powered by SceneXplain&quot; annotation. This feature not only demonstrates the accurate and contextually relevant alt texts provided by SceneXplain but also highlights the seamless integration of &quot;Alt Texter&quot; with the Ghost blogging platform. It serves as a practical example of how these technologies work together to enhance web accessibility and user experience, while also maintaining effective SEO.</p><h2 id="see-it-in-action">See it in action</h2><p>Here&apos;s how it looks when it&apos;s running in Docker Desktop:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34.webm" poster="https://img.spacergif.org/v1/892x719/0a/spacer.png" width="892" height="719" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/Screencast-from-2024-02-02-12-11-34_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:29</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><p>As you can see, it iterates over each blog post. If it finds images without alt text, it sends them to SceneXplain and then populates the post accordingly.</p><h2 id="run-it-yourself">Run it yourself</h2><p>To run the program you will need:</p><ul><li>Your Ghost <a href="https://ghost.org/docs/admin-api/?ref=jina-ai-gmbh.ghost.io">Admin API</a> key (<em>not</em> a Content API key)</li><li>Your Ghost blog URL</li><li>Your <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain API key</a></li></ul><div class="kg-card kg-callout-card kg-callout-card-red"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Always <a href="https://ghost.org/docs/faq/manual-backup/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">back up your data</a> before running any program (like this!) that may modify it.<br><br>This program is still very early in development, so be careful!</div></div><p>You can run the program one of three ways: In a Google Colab notebook, in Docker (via a graphical interface or command line), or directly on your machine.</p><h3 id="google-colab-notebook">Google Colab notebook</h3><p>While <a href="https://www.geeksforgeeks.org/how-to-use-google-colab/?ref=jina-ai-gmbh.ghost.io">Google Colab</a> isn&apos;t the ideal way to run Alt Texter in production, <a href="https://colab.research.google.com/github/jina-ai/scenex-alt-texter/blob/main/alt-texter.ipynb?ref=jina-ai-gmbh.ghost.io">the notebook</a> does give you a quick and easy way to see the code and test it out.</p><p>To run the notebook:</p><ul><li>Load up the below link</li><li>Enter your settings (in the &quot;Fill in variables&quot; section)</li><li>Click the <em>Runtime</em> menu, then click <em>Run all</em></li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/scenex-alt-texter/blob/main/alt-texter.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/b8b3f31c3785f2607d951bd203c1a8b5/img/favicon.ico" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><h3 id="docker">Docker</h3><p>Using Docker lets you run the program on your own machine in a sandbox. In <a href="https://www.docker.com/products/docker-desktop/?ref=jina-ai-gmbh.ghost.io">Docker Desktop</a> you can follow the steps in this <a href="https://www.slideshare.net/slideshows/alt-texter-tutorial/266063930?ref=jina-ai-gmbh.ghost.io">deck</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.slideshare.net/slideshows/alt-texter-tutorial/266063930?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Alt Texter Tutorial</div><div class="kg-bookmark-description">Alt Texter Tutorial - Download as a PDF or view online for free</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://public.slidesharecdn.com/_next/static/media/favicon.7bc3d920.ico" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"><span class="kg-bookmark-author">Slideshare</span><span class="kg-bookmark-publisher">alexcg15</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.slidesharecdn.com/ss_thumbnails/alttextertutorial-240202130606-24d0a8e2-thumbnail.jpg?width=640&amp;height=640&amp;fit=bounds" alt="SceneXplain: Alt Text and Better SEO for Your Ghost Blog With One Command"></div></a></figure><p>If you prefer the command line, you can simply run:</p><pre><code class="language-bash">docker run --name alt-texter \
-e GHOST_API_KEY=&lt;ghost-admin-api-key&gt; \
-e SCENEX_API_KEY=&lt;scenex-api-key&gt; \
-e GHOST_BLOG_URL=&quot;&lt;ghost-blog-url&gt;&quot; \
jinaai/alt-texter:0.1
</code></pre><h3 id="bare-metal">Bare metal</h3><p>Of course, you can run the code on &quot;bare metal&quot; as well. Check the <a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io#bare-metal">README</a> for more details.</p><h2 id="get-started">Get started</h2><p>To check the latest version of the code, you can visit the <a href="https://github.com/jina-ai/scenex-alt-texter?ref=jina-ai-gmbh.ghost.io">Alt Texter repo</a>. Contributions are always welcome! If you need support for the program, please leave an <a href="https://github.com/jina-ai/scenex-alt-texter/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc&amp;ref=jina-ai-gmbh.ghost.io">issue</a>, or if you need support with SceneXplain generally, you can start a conversation on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Discord</a> in the <a href="https://discord.com/channels/1106542220112302130/1106544027601469461?ref=jina-ai-gmbh.ghost.io">support channel</a>.</p>]]></content:encoded></item><item><title><![CDATA[Elevate Your Code Search with New Jina Code Embeddings]]></title><description><![CDATA[New 𝗷𝗶𝗻𝗮-𝗲𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀-𝘃𝟮-𝗯𝗮𝘀𝗲-𝗰𝗼𝗱𝗲 is optimized for code & docstring search. This powerful model supports searches between English and 30 widely-used programming languages, all with 8192 context length and SOTA performance.]]></description><link>https://jina.ai/news/elevate-your-code-search-with-new-jina-code-embeddings/</link><guid isPermaLink="false">65c0fa40aa949b00016cf07d</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 05 Feb 2024 17:46:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--27-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--27-.png" alt="Elevate Your Code Search with New Jina Code Embeddings"><p>Accurately searching through code and documentation is more critical than ever. We&apos;re thrilled to unveil our latest embeddings in the world of coding: <code><strong>jina-embeddings-v2-base-code</strong></code>. This new open-source programming language embedding model is designed to improve how developers interact with code and documentation. Supporting English and 30 popular programming languages, it stands out as the only open-source model of its kind that accommodates up to 8,192 input tokens. The <code>jina-embeddings-v2-base-code</code> is now available on <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-code?ref=jina-ai-gmbh.ghost.io">HuggingFace</a> under an Apache 2.0 license and can be freely accessed via our <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">Embedding API</a>.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11.mp4" poster="https://img.spacergif.org/v1/1070x876/0a/spacer.png" width="1070" height="876" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2024/02/ea1f0faf379bde31c7e6b56846326a11_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:07</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Visit </span><a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">Embedding API</span></a><span style="white-space: pre-wrap;"> and select </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-code</span></code><span style="white-space: pre-wrap;"> from the dropdown list. Enjoy 1M tokens for free.</span></p></figcaption>
        </figure><h2 id="why-develop-an-embedding-model-for-code">Why Develop an Embedding Model for Code?</h2><p>Developers often find themselves navigating through vast codebases, not in search of errors, but to locate specific functionalities or understand how certain processes are implemented. This task can be time-consuming and, at times, akin to finding a needle in a haystack. Integrated Development Environments (IDEs) have significantly improved this process by providing tools and features that automate the search for information. However, the potential for further enhancement exists, and this is where our embedding model comes into play.</p><h3 id="use-cases-of-jina-embeddings-v2-base-code">Use Cases of <code>jina-embeddings-v2-base-code</code> </h3><p>By integrating AI-powered search capabilities, we&apos;re not just augmenting existing functionalities within IDEs; we&apos;re transforming how developers engage with codebases. This technology goes beyond simple text search, offering semantic understanding that can interpret the intent behind a query, thereby significantly reducing the time and effort required for code reviews, unit testing, and overall quality management. </p><h4 id="enhanced-code-navigation"><strong>Enhanced Code Navigation</strong></h4><ul><li><strong>Query Format</strong>: Natural language description of the functionality or code snippet you&apos;re searching for.</li><li><strong>Retrieved Result Format</strong>: Relevant code files or snippets where the described functionality is implemented, along with annotations or highlights that point to the specific parts of the code.</li></ul><h4 id="streamlined-code-review"><strong>Streamlined Code Review</strong></h4><ul><li><strong>Query Format</strong>: Description of the programming concepts or patterns you want to review across the codebase.</li><li><strong>Retrieved Result Format</strong>: A list of code snippets or pull requests that match the described concepts, patterns, or best practices, enabling reviewers to focus on critical areas for improvement.</li></ul><h4 id="automated-documentation-assistance"><strong>Automated Documentation Assistance</strong></h4><ul><li><strong>Query Format</strong>: Code snippet for which you need documentation or an explanation.</li><li><strong>Retrieved Result Format</strong>: Suggested docstrings or documentation entries that explain the code&apos;s functionality, parameters, and return types, making it easier to maintain up-to-date and comprehensive documentation.</li></ul><p>By addressing these specific use cases, <code>jina-embeddings-v2-base-code</code> not only enhances the development experience but also promotes a more collaborative and efficient coding environment.</p><h2 id="benchmark-the-performance">Benchmark the Performance</h2><p>In a field where precision and accuracy are paramount, <code>jina-embeddings-v2-base-code</code> has outshined its competitors, leading the pack in nine out of fifteen crucial <a href="https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io">CodeNetSearch</a> benchmarks. What&apos;s more, our model holds highly competitive scores in the remaining benchmarks. When compared to its nearest competitors, including those from tech giants like Microsoft and Salesforce, <code>jina-embeddings-v2-base-code</code> not only ranks higher but also showcases its superior design and capabilities.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-8.png" class="kg-image" alt="Elevate Your Code Search with New Jina Code Embeddings" loading="lazy" width="2000" height="793" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-8.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Our model&apos;s excellence is not just in isolated instances; across the board, all Jina Embedding models have achieved top ranks on relevant benchmarks, distinguishing themselves among open-source models for code retrieval.</span></figcaption></figure><h2 id="model-highlights">Model Highlights</h2><ul><li><strong>State-of-the-Art Performance</strong>: Our commitment to excellence is reflected in the performance of Jina Embedding models, which consistently top benchmark lists against other open-source offerings and even outperform models from Microsoft and Salesforce.</li><li><strong>Compact Yet Powerful</strong>: In the world of AI, efficiency is key. With 161 million parameters (307MB without quantization), <code>jina-embeddings-v2-base-code</code> is designed for efficiency, offering high-speed performance and cost savings without compromising on capability.</li><li><strong>Extended Context Capability</strong>: The ability to process up to 8192 tokens allows for the handling of large functions and numerous object files, providing a depth of understanding and context that surpasses the limitations of models supporting only a few hundred tokens.</li><li><strong>Multi-Language Support</strong>: Tailored for versatility, our model&apos;s training encompasses 30 programming languages and frameworks, emphasizing six of the most popular ones: Python, JavaScript, Java, PHP, Go, and Ruby. This extensive coverage ensures that <code>jina-embeddings-v2-base-code</code> meets the diverse needs of the programming community.</li><li><strong>RAG Integration for Seamless Code Generation</strong>: The model&apos;s compatibility with RAG and integration with a code generation model facilitate not just code generation from general knowledge but also the ability to read relevant APIs and documentation, enabling automatic code integration that is both efficient and accurate.</li></ul><h2 id="seamless-api-integration">Seamless API Integration</h2><p><code>jina-embeddings-v2-base-code</code> is designed for easy integration, supporting major vector databases like MongoDB, Qdrant, and Weaviate, and frameworks such as Haystack and LlamaIndex. This ensures that developers can effortlessly incorporate our model into their existing systems, leveraging its capabilities to enhance their code retrieval and documentation processes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-7.png" class="kg-image" alt="Elevate Your Code Search with New Jina Code Embeddings" loading="lazy" width="2000" height="725" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/02/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/02/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/02/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/02/image-7.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Frameworks that support our embedding API</span></figcaption></figure><p>We value your feedback on <code>jina-embeddings-v2-base-code</code>. Join our community channel to contribute feedback and stay informed about our advancements. Together, we&apos;re shaping a more robust and inclusive AI future.</p>]]></content:encoded></item><item><title><![CDATA[A Deep Dive into Tokenization]]></title><description><![CDATA[Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?]]></description><link>https://jina.ai/news/a-deep-dive-into-tokenization/</link><guid isPermaLink="false">65afb3ee8da8040001e17061</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 31 Jan 2024 15:10:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" alt="A Deep Dive into Tokenization"><p>There are a lot of barriers to understanding AI models, some of them pretty big barriers, and they can stand in the way of implementing AI processes. But the first one many people encounter is understanding what we mean when talking about <strong>tokens</strong>. </p><p>One of the most important practical parameters in choosing an AI language model is the size of its context window &#x2014; the maximum input text size &#x2014; which is given in tokens, not words or characters or any other automatically recognizable unit.</p><p>Furthermore, embedding services are typically figured &#x201C;per token,&#x201D; meaning tokens are important to understanding your bill.</p><p>This can be very confusing if you aren&#x2019;t clear about what a token is. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="A Deep Dive into Tokenization" loading="lazy" width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings current price sheet (as of February 2024). Note that prices are stated per &#x201C;1M tokens&#x201D;.</span></figcaption></figure><p>But of all the confusing aspects of modern AI, tokens are probably the least complicated. This article will try to clarify what tokenization is, what it does, and why we do it that way.</p><h2 id="tldr">tl;dr</h2><p>For those who want or need a quick answer to figure out how many tokens to buy from Jina Embeddings or an estimate of how many they need to expect to buy, the following statistics are what you&apos;re looking for.</p><h3 id="tokens-per-english-word">Tokens per English Word</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A call to the Jina Embeddings v2 API for English models will use <b><strong style="white-space: pre-wrap;">approximately</strong></b> <b><strong style="white-space: pre-wrap;">10% more</strong></b> tokens than the number of words in your text, <b><strong style="white-space: pre-wrap;">plus two tokens per embedding</strong></b>.</div></div><p>During empirical testing, described further down in this article, a variety of English texts converted into tokens at a rate of about 10% more tokens than words, using Jina Embeddings English-only models. This result was pretty robust. </p><p>Jina Embeddings v2 models have a context window of 8192 tokens. This means that if you pass a Jina model an English text longer than 7,400 words, there is a good chance it will be truncated.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">7,400 words</strong></b>.</div></div><h3 id="tokens-per-chinese-character">Tokens per Chinese Character</h3><p>For Chinese, results are more variable. Depending on the text type, ratios varied from 0.6 to 0.75 tokens per Chinese character (&#x6C49;&#x5B57;). English texts given to Jina Embeddings v2 for Chinese produce approximately the same number of tokens as Jina Embeddings v2 for English: roughly 10% more than the number of words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input in Chinese to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for Chinese and English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">10,500 characters</strong></b> (<b><strong style="white-space: pre-wrap;">&#x5B57;&#x6570;</strong></b>), or <b><strong style="white-space: pre-wrap;">0.6 to 0.75 tokens per Chinese character, plus two per embedding.</strong></b></div></div><h3 id="tokens-per-german-word">Tokens per German Word</h3><p>German word-to-token ratios are more variable than English but less than Chinese. Depending on the genre of the text, I got 20% to 30% more tokens than words on average. Giving English texts to Jina Embeddings v2 for German and English uses a few more tokens than the English-only and Chinese/English models: 12% to 15% more tokens than words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Jina Embeddings v2 for German and English will count <b><strong style="white-space: pre-wrap;">20% to 30% more tokens than words, plus two per embedding</strong></b>. The maximum size of the input context is approximately <b><strong style="white-space: pre-wrap;">6,300 German words</strong></b>.</div></div><h3 id="caution">Caution!</h3><p>These are simple calculations, but they should be approximately right for most natural language texts and most users. Ultimately, we can only promise that the number of tokens will always be no more than the number of characters in your text, plus two. It will practically always be much less than that, but we cannot promise any specific count in advance.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Your Mileage May Vary! </strong></b><br><br>These are estimates based on statistically naive calculations. We do not guarantee how many tokens any particular request will take.</div></div><p>If all you need is advice on how many tokens to buy for Jina Embeddings, you can stop here. Other embedding models, from companies other than Jina AI, may not have the same token-to-word and token-to-Chinese-character ratios Jina models have, but they will not generally be very different overall.</p><p>If you want to understand why, the rest of this article is a deeper dive into tokenization for language models.</p><h2 id="words-tokens-numbers">Words, Tokens, Numbers</h2><p>Tokenization has been a part of natural language processing for longer than modern AI models have existed.</p><p>It&#x2019;s a bit clich&#xE9; to say that everything in a computer is just a number, but it&#x2019;s also mostly true. Language, however, is not naturally just a bunch of numbers. It might be speech, made of sound waves, or writing, made of marks on paper, or even an image of a printed text or a video of someone using sign language. But most of the time, when we talk about using computers to process natural language, we mean texts composed of sequences of characters: letters (a, b, c, etc.), numerals (0, 1, 2&#x2026;), punctuation, and spaces, in different languages and textual encodings.</p><p>Computer engineers call these &#x201C;strings&#x201D;.</p><p>AI language models take sequences of numbers as input. So, you might write the sentence:</p><blockquote><em>What is today&apos;s weather in Berlin?</em></blockquote><p>But, after tokenization, the AI model gets as input:</p><pre><code class="language-python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>Tokenization is the process of converting an input string into a specific sequence of numbers that your AI model can understand.</p><p>When you use an AI model via a web API that charges users per token, each request is converted into a sequence of numbers like the one above. The number of tokens in the request is the length of that sequence of numbers. So, asking Jina Embeddings v2 for English to give you an embedding for &#x201C;<em>What is today&apos;s weather in Berlin?</em>&#x201D; will cost you 11 tokens because it converted that sentence into a sequence of 11 numbers before passing it to the AI model.</p><p>AI models based on the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io">Transformer architecture</a> have a fixed-size <strong>context window</strong> whose size is measured in tokens. Sometimes this is called an &#x201C;input window,&#x201D; &#x201C;context size,&#x201D; or &#x201C;sequence length&#x201D; (especially on the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Hugging Face MTEB leaderboard</a>). It means the maximum text size that the model can see at one time.</p><p>So, if you want to use an embedding model, this is the maximum input size allowed.</p><p>Jina Embeddings v2 models all have a context window of 8,192 tokens. Other models will have different (typically smaller) context windows. This means that however much text you put into it, the tokenizer associated with that Jina Embeddings model must convert it into no more than 8,192 tokens.</p><h2 id="mapping-language-to-numbers">Mapping Language to Numbers</h2><p>The simplest way to explain the logic of tokens is this:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A token is a number that stands in for a part of a string.</div></div><p>For natural language models, the part of a string that a token stands for is a word, a part of a word, or a piece of punctuation. Spaces are not generally given any explicit representation in tokenizer output.</p><p>Tokenization is part of a group of techniques in natural language processing called <a href="https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io"><em>text segmentation</em></a>, and the module that performs tokenization is called, very logically, a <strong>tokenizer</strong>.</p><p>To show how tokenization works, we&#x2019;re going to tokenize some sentences using the smallest Jina Embeddings v2 for English model: <code>jina-embeddings-v2-small-en</code>. Jina Embeddings&#x2019; other English-only model &#x2014; <code>jina-embeddings-v2-base-en</code> &#x2014; uses the same tokenizer, so there&#x2019;s no point in downloading extra megabytes of AI model that we won&#x2019;t use in this article.</p><p>First, install the <code>transformers</code> module in your Python environment or notebook. Use the <code>-U</code> flag to make sure you upgrade to the latest version because this model will not work with some older versions:</p><pre><code class="language-bash">pip install -U transformers
</code></pre><p>Then, download <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> using <code>AutoModel.from_pretrained</code>:</p><pre><code class="language-Python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-small-en&apos;, trust_remote_code=True)
</code></pre><p>To tokenize a string, use the <code>encode</code> method of the <code>tokenizer</code> member object of the model:</p><pre><code class="language-Python">model.tokenizer.encode(&quot;What is today&apos;s weather in Berlin?&quot;)
</code></pre><p>The result is a list of numbers:</p><pre><code class="language-Python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>To convert these numbers back to string forms, use the <code>convert_ids_to_tokens</code> method of the <code>tokenizer</code> object:</p><pre><code class="language-Python">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])
</code></pre><p>The result is a list of strings:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;what&apos;, &apos;is&apos;, &apos;today&apos;, &quot;&apos;&quot;, &apos;s&apos;, &apos;weather&apos;, &apos;in&apos;,
 &apos;berlin&apos;, &apos;?&apos;, &apos;[SEP]&apos;]
</code></pre><p>Note that the model&#x2019;s tokenizer has:</p><ol><li>Added <code>[CLS]</code>at the beginning and <code>[SEP]</code> at the end. This is necessary for technical reasons and means that <strong>every request for an embedding will cost two extra tokens</strong>, above however many tokens the text takes.</li><li>Split punctuation from words, turning &#x201C;<em>Berlin?</em>&#x201D; into: <code>berlin</code> and <code>?</code>, and &#x201C;<em>today&#x2019;s</em>&#x201D; into <code>today</code>, <code>&apos;</code>, and <code>s</code>.</li><li>Put everything in lowercase. Not all models do this, but this can help with training when using English. It may be less helpful in languages where capitalization has a different meaning.</li></ol><p>Different word-counting algorithms in different programs might count the words in this sentence differently. OpenOffice counts this as six words. The Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) counts seven words. Other software may come to other numbers, depending on how they handle punctuation and clitics like &#x201C;&#x2019;s.&#x201D;</p><p>The tokenizer for this model produces nine tokens for those six or seven words, plus the two extra tokens needed with every request.</p><p>Now, let&#x2019;s try with a less common place-name than Berlin:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;I live in Kinshasa.&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;i&apos;, &apos;live&apos;, &apos;in&apos;, &apos;kin&apos;, &apos;##sha&apos;, &apos;##sa&apos;, &apos;.&apos;, &apos;[SEP]&apos;]
</code></pre><p>The name &#x201C;Kinshasa&#x201D; is broken up into three tokens: <code>kin</code>, <code>##sha</code>, and <code>##sa</code>. The <code>##</code> indicates that this token is not the beginning of a word.</p><p>If we give the tokenizer something completely alien, the number of tokens over the number of words increases even more:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;Klaatu barada nikto&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)

[&apos;[CLS]&apos;, &apos;k&apos;, &apos;##la&apos;, &apos;##at&apos;, &apos;##u&apos;, &apos;bar&apos;, &apos;##ada&apos;, &apos;nik&apos;, &apos;##to&apos;, &apos;[SEP]&apos;]
</code></pre><p>Three words become eight tokens, plus the <code>[CLS]</code> and <code>[SEP]</code> tokens.</p><p>Tokenization in German is similar. With the <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 for German</a> model, we can tokenize a translation of &quot;What is today&apos;s weather in Berlin?&quot; the same way as with the English model.</p><pre><code class="language-Python">german_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
token_ids = german_model.tokenizer.encode(&quot;Wie wird das Wetter heute in Berlin?&quot;)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Wie&apos;, &apos;wird&apos;, &apos;das&apos;, &apos;Wetter&apos;, &apos;heute&apos;, &apos;in&apos;, &apos;Berlin&apos;, &apos;?&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>This tokenizer is a little bit different from the English one in that <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> replace <code>[CLS]</code> and <code>[SEP]</code> but serve the same function. Also, the text is not case-normalized &#x2014; upper and lower cases remain as written &#x2014; because capitalization is meaningful in German differently from English.</p><p>(To simplify this presentation, I removed a special character indicating a word&apos;s beginning.)</p><p>Now, let&#x2019;s try a more complex sentence <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io">from a newspaper text</a>:</p><blockquote>Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen bleibt liegen &#x2013; zu genervt sind die Landwirte von b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.</blockquote><pre><code>sentence = &quot;&quot;&quot;
Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen
bleibt liegen &#x2013; zu genervt sind die Landwirte von 
b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.
&quot;&quot;&quot;
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)</code></pre><p>The tokenized result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Ein&apos;, &apos;Gro&#xDF;teil&apos;, &apos;der&apos;, &apos;mill&apos;, &apos;iarden&apos;, &apos;schwer&apos;, 
 &apos;en&apos;, &apos;Bauern&apos;, &apos;-&apos;, &apos;Sub&apos;, &apos;ventionen&apos;, &apos;bleibt&apos;, &apos;liegen&apos;, 
 &apos;&#x2013;&apos;, &apos;zu&apos;, &apos;gen&apos;, &apos;ervt&apos;, &apos;sind&apos;, &apos;die&apos;, &apos;Landwirte&apos;, &apos;von&apos;, 
 &apos;b&#xFC;ro&apos;, &apos;krat&apos;, &apos;ischen&apos;, &apos;G&#xE4;n&apos;, &apos;gel&apos;, &apos;ungen&apos;, &apos;und&apos;, &apos;Regel&apos;, 
 &apos;wahn&apos;, &apos;.&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>Here, you see that many German words were broken up into smaller pieces and not necessarily along the lines licensed by German grammar. The result is that a long German word that would count as just one word to a word counter might be any number of tokens to Jina&#x2019;s AI model.</p><p>Let&#x2019;s do the same in Chinese, translating &#x201D;What is today&apos;s weather in Berlin?&#x201D; as:</p><blockquote>&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;</blockquote><pre><code>chinese_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(&quot;&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;&quot;)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The tokenized result:</p><pre><code class="language-Python">[&apos;&lt;s&gt;&apos;, &apos;&#x67CF;&#x6797;&apos;, &apos;&#x4ECA;&#x5929;&#x7684;&apos;, &apos;&#x5929;&#x6C14;&apos;, &apos;&#x600E;&#x4E48;&#x6837;&apos;, &apos;&#xFF1F;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>In Chinese, there are usually no word breaks in written text, but the Jina Embeddings tokenizer frequently joins multiple Chinese characters together:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>&#x67CF;&#x6797;</td>
<td>B&#xF3;l&#xED;n</td>
<td>Berlin</td>
</tr>
<tr>
<td>&#x4ECA;&#x5929;&#x7684;</td>
<td>j&#x12B;nti&#x101;n de</td>
<td>today&#x2019;s</td>
</tr>
<tr>
<td>&#x5929;&#x6C14;</td>
<td>ti&#x101;nq&#xEC;</td>
<td>weather</td>
</tr>
<tr>
<td>&#x600E;&#x4E48;&#x6837;</td>
<td>z&#x11B;nmey&#xE0;ng</td>
<td>how</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Let&#x2019;s use a more complex sentence <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io">from a Hong Kong-based newspaper</a>:</p><pre><code class="language-Python">sentence = &quot;&quot;&quot;
&#x65B0;&#x898F;&#x5B9A;&#x57F7;&#x884C;&#x9996;&#x65E5;&#xFF0C;&#x8A18;&#x8005;&#x5728;&#x4E0B;&#x73ED;&#x9AD8;&#x5CF0;&#x524D;&#x7684;&#x4E0B;&#x5348;5&#x6642;&#x4F86;&#x5230;&#x5EE3;&#x5DDE;&#x5730;&#x9435;3&#x865F;&#x7DDA;&#xFF0C;
&#x5F9E;&#x7E41;&#x5FD9;&#x7684;&#x73E0;&#x6C5F;&#x65B0;&#x57CE;&#x7AD9;&#x555F;&#x7A0B;&#xFF0C;&#x5411;&#x6A5F;&#x5834;&#x5317;&#x65B9;&#x5411;&#x51FA;&#x767C;&#x3002;
&quot;&quot;&quot;
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>(Translation: <em>&#x201C;On the first day that the new regulations were in force, this reporter arrived at Guangzhou Metro Line 3 at 5 p.m., during rush hour, having departed the Zhujiang New Town Station heading north towards the airport.&#x201D;</em>)</p><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;&#x65B0;&apos;, &apos;&#x898F;&#x5B9A;&apos;, &apos;&#x57F7;&#x884C;&apos;, &apos;&#x9996;&apos;, &apos;&#x65E5;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x8A18;&#x8005;&apos;, &apos;&#x5728;&#x4E0B;&apos;, &apos;&#x73ED;&apos;, 
 &apos;&#x9AD8;&#x5CF0;&apos;, &apos;&#x524D;&#x7684;&apos;, &apos;&#x4E0B;&#x5348;&apos;, &apos;5&apos;, &apos;&#x6642;&apos;, &apos;&#x4F86;&#x5230;&apos;, &apos;&#x5EE3;&#x5DDE;&apos;, &apos;&#x5730;&apos;, &apos;&#x9435;&apos;, &apos;3&apos;, 
 &apos;&#x865F;&apos;, &apos;&#x7DDA;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5F9E;&apos;, &apos;&#x7E41;&#x5FD9;&apos;, &apos;&#x7684;&apos;, &apos;&#x73E0;&#x6C5F;&apos;, &apos;&#x65B0;&#x57CE;&apos;, &apos;&#x7AD9;&apos;, &apos;&#x555F;&apos;, 
 &apos;&#x7A0B;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5411;&apos;, &apos;&#x6A5F;&#x5834;&apos;, &apos;&#x5317;&apos;, &apos;&#x65B9;&#x5411;&apos;, &apos;&#x51FA;&#x767C;&apos;, &apos;&#x3002;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>These tokens do not map to any specific dictionary of Chinese words (&#x8BCD;&#x5178;). For example, &#x201C;&#x555F;&#x7A0B;&#x201D; - <em>q&#x1D0;ch&#xE9;ng</em> (depart, set out) would typically be categorized as a single word but is here split into its two constituent characters. Similarly, &#x201C;&#x5728;&#x4E0B;&#x73ED;&#x201D; would usually be recognized as two words, but with the split between &#x201C;&#x5728;&#x201D; - <em>z&#xE0;i</em> (in, during) and &#x201C;&#x4E0B;&#x73ED;&#x201D; - <em>xi&#xE0;b&#x101;n</em> (the end of the workday, rush hour), not between &#x201C;&#x5728;&#x4E0B;&#x201D; and &#x201C;&#x73ED;&#x201D; as the tokenizer has done here.</p><p>In all three languages, the places where the tokenizer breaks the text up are not directly related to the logical places where a human reader would break them.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The tokenizer algorithm does not use a conventional, language-aware dictionary, so its behavior does not match how humans count words.</div></div><p>This is not a specific feature of Jina Embeddings models. This approach to tokenization is almost universal in AI model development. Although two different AI models may not have identical tokenizers, in the current state of development, they will practically all use tokenizers with this kind of behavior.</p><p>The next section will discuss the specific algorithm used in tokenization and the logic behind it.</p><h2 id="why-do-we-tokenize-and-why-this-way">Why Do We Tokenize? And Why This Way?</h2><p>AI language models take as input sequences of numbers that stand in for text sequences, but a bit more happens before running the underlying neural network and creating an embedding. When presented with a list of numbers representing small text sequences, the model looks each number up in an internal dictionary that stores a unique vector for each number. It then combines them, and that becomes the input to the neural network.</p><p>This means that the tokenizer <strong>must</strong> be able to convert <strong><em>any</em></strong> input text we give it into tokens that appear in the model&#x2019;s dictionary of token vectors. If we took our tokens from a conventional dictionary, the first time we encountered a misspelling or a rare proper noun or foreign word, the whole model would stop. It could not process that input.</p><p>In natural language processing, this is called the out-of-vocabulary (OOV) problem, and it&#x2019;s pervasive in all text types and all languages. There are a few strategies for addressing the OOV problem:</p><ol><li>Ignore it. Replace everything not in the dictionary with an &#x201C;unknown&#x201D; token.</li><li>Bypass it. Instead of using a dictionary that maps text sequences to vectors, use one that maps <em>individual characters</em> to vectors. English only uses 26 letters most of the time, so this must be smaller and more robust against OOV problems than any dictionary.</li><li>Find frequent subsequences in the text, put them in the dictionary, and use characters (single-letter tokens) for whatever is left.</li></ol><p>The first strategy means that a lot of important information is lost. The model can&#x2019;t even learn about the data it&#x2019;s seen if it takes the form of something not in the dictionary. A lot of things in ordinary text are just not present in even the largest dictionaries.</p><p>The second strategy is possible, and researchers have investigated it. However, it means that the model has to accept a lot more input and has to learn a lot more. This means a much bigger model and much more training data for a result that has never proven to be any better than the third strategy.</p><p>AI language models pretty much all implement the third strategy in some form. Most use some variant of the <a href="https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io">Wordpiece algorithm</a> <a href="https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io">[Schuster and Nakajima 2012]</a> or a similar technique called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io">Senrich et al. 2016</a>] These algorithms are <em>language-agnostic</em>. That means they work the same for all written languages without any knowledge beyond a comprehensive list of possible characters. They were designed for multilingual models like Google&#x2019;s BERT that take just any input from scraping the Internet &#x2014; hundreds of languages and texts other than human language like computer programs &#x2014; so that they could be trained without doing complicated linguistics.</p><p>Some research shows significant improvements using more language-specific and language-aware tokenizers. [<a href="https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io">Rust et al. 2021</a>] But building tokenizers that way takes time, money, and expertise. Implementing a universal strategy like BPE or Wordpiece is much cheaper and easier.</p><p>However, as a consequence, there is no way to know how many tokens a specific text represents other than to run it through a tokenizer and then count the number of tokens that come out of it. Because the smallest possible subsequence of a text is one letter, you can be sure the number of tokens won&#x2019;t be larger than the number of characters (minus spaces) plus two.</p><p>To get a good estimate, we need to throw a lot of text at our tokenizer and calculate empirically how many tokens we get on average, compared to how many words or characters we input. In the next section, we&#x2019;ll do some not-very-systematic empirical measurements for all Jina Embeddings v2 models currently available.</p><h2 id="empirical-estimates-of-token-output-sizes">Empirical Estimates of Token Output Sizes</h2><p>For English and German, I used the Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) to get word counts for texts. This algorithm is widely used to select text snippets when you double-click on something. It is the closest thing available to a universal objective word counter.</p><p>I installed the <a href="https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io">polyglot library</a> in Python, which implements this text segmenter:</p><pre><code class="language-bash">pip install -U polyglot
</code></pre><p>To get the word count of a text, you can use code like this snippet:</p><pre><code class="language-python">from polyglot.text import Text

txt = &quot;What is today&apos;s weather in Berlin?&quot;
print(len(Text(txt).words))
</code></pre><p>The result should be <code>7</code>.</p><p>To get a token count, segments of the text were passed to the tokenizers of various Jina Embeddings models, as described below, and each time, I subtracted two from the number of tokens returned.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">The token counts listed here <b><strong style="white-space: pre-wrap;">do not include</strong></b> the extra two tokens at the beginning and end of each tokenized text.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en">English<br>(<code>jina-embeddings-v2-small-en</code> and <code>jina-embeddings-v2-base-en</code>)</h3><p>To calculate averages, I downloaded two English text corpora from <a href="https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Wortschatz Leipzig</a>, a collection of freely downloadable corpora in a number of languages and configurations hosted by Leipzig University:</p><ul><li>A one-million-sentence corpus of news data in English from 2020 (<code>eng_news_2020_1M</code>)</li><li>A one-million-sentence corpus of <a href="https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io">English Wikipedia</a> data from 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Both can be found on <a href="https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io">their English downloads page</a>.</p><p>For diversity, I also downloaded the <a href="https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io">Hapgood translation of Victor Hugo&#x2019;s <em>Les Mis&#xE9;rables</em></a> from Project Gutenberg, and a copy of the King James Version of the Bible, translated to English in 1611.</p><p>For each all four texts, I counted the words using the Unicode segmenter implemented in <code>polyglot</code>, then counted the tokens made by <code>jina-embeddings-v2-small-en</code>, subtracting two tokens for each tokenization request. The results are as follows:</p>
<!--kg-card-begin: html-->
<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Text</th><th id="|&lt;X;" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for English)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class><code>eng_news_2020_1M</code></td><td id="|&lt;X;" class>22,825,712</td><td id="GHal" class>25,270,581</td><td id="h]mu" class>1.107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class><code>eng_wikipedia_2016_1M</code></td><td id="|&lt;X;" class>24,243,607</td><td id="GHal" class>26,813,877</td><td id="h]mu" class>1.106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class><code>les_miserables_en</code></td><td id="|&lt;X;" class>688,911</td><td id="GHal" class>764,121</td><td id="h]mu" class>1.109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class><code>kjv_bible</code></td><td id="|&lt;X;" class>1,007,651</td><td id="GHal" class>1,099,335</td><td id="h]mu" class>1.091</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>The use of precise numbers does not mean this is a precise result. That documents of such different genres would all have between 9% and 11% more tokens than words indicates that you can probably expect somewhere around 10% more tokens than words, as measured by the Unicode segmenter. Word processors often do not count punctuation, while the Unicode Segmenter does, so you can&#x2019;t expect the word counts from office software to necessarily match this.</p><h3 id="german-jina-embeddings-v2-base-de">German<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>For German, I downloaded three corpora from <a href="https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io">Wortschatz Leipzig&#x2019;s German page</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> &#x2014; One million sentences from a balanced mixture of texts in different genres, dating to 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> &#x2014; One million sentences of news text from 2019.</li><li><code>deu_wikipedia_2021_1M</code> &#x2014; One million sentences extracted from the German Wikipedia in 2021.</li></ul><p>And for diversity, I also downloaded all <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata&amp;ref=jina-ai-gmbh.ghost.io">three volumes of Karl Marx&#x2019;s <em>Kapital</em></a> from the <a href="https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>I then followed the same procedure as for English:</p>
<!--kg-card-begin: html-->
<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Text</th><th id="|qln" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class>7,924,024</td><td id="YXZX" class>9,772,652</td><td id="oEoQ" class>1.234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class>17,949,120</td><td id="YXZX" class>21,711,555</td><td id="oEoQ" class>1.210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class>17,999,482</td><td id="YXZX" class>22,654,901</td><td id="oEoQ" class>1.259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class>784,336</td><td id="YXZX" class>1,011,377</td><td id="oEoQ" class>1.289</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>These results have a larger spread than the English-only model but still suggest that German text will yield, on average, 20% to 30% more tokens than words.</p><p>English texts yield more tokens with the German-English tokenizer than the English-only one:</p>
<!--kg-card-begin: html-->
<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc&lt;l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Text</th><th id="R@A^" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class>24243607</td><td id="UUfl" class>27758535</td><td id="iTZS" class>1.145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class>22825712</td><td id="UUfl" class>25566921</td><td id="iTZS" class>1.120</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>You should expect to need 12% to 15% more tokens than words to embed English texts with the bilingual German/English than with the English-only one.</p><h3 id="chinese-jina-embeddings-v2-base-zh">Chinese<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Chinese is typically written without spaces and had no traditional notion of &#x201C;words&#x201D; before the 20th century. Consequently, the size of a Chinese text is typically measured in characters (<strong>&#x5B57;&#x6570;</strong>). So, instead of using the Unicode Segmenter, I measured the length of Chinese texts by removing all the spaces and then just getting the character length.</p><p>I downloaded three corpora from the <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io">Chinese corpus page at Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> &#x2014; One million sentences from the Chinese language Wikipedia, extracted in 2018.</li><li><code>zho_news_2007-2009_1M</code> &#x2014; One million sentences from Chinese news sources, collected from 2007 to 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> &#x2014; One million sentences from news sources that use exclusively traditional Chinese characters (&#x7E41;&#x9AD4;&#x5B57;).</li></ul><p>In addition, for some diversity, I also used <em>The True Story of Ah Q</em> (&#x963F;Q&#x6B63;&#x50B3;), a novella by Lu Xun (&#x9B6F;&#x8FC5;) written in the early 1920s. I downloaded the <a href="https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io">traditional character version from Project Gutenberg</a>.</p>
<!--kg-card-begin: html-->
<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Text</th><th id="CaUc" class="simple-table-header-color simple-table-header">Character count<br>(&#x5B57;&#x6570;)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for Chinese and English)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio of tokens to characters<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class>45,116,182</td><td id="CQ{d" class>29,193,028</td><td id="_};C" class>0.647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class>44,295,314</td><td id="CQ{d" class>28,108,090</td><td id="_};C" class>0.635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class>54,585,819</td><td id="CQ{d" class>40,290,982</td><td id="_};C" class>0.738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class>41,268</td><td id="CQ{d" class>25,346</td><td id="_};C" class>0.614</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>This spread in token-to-character ratios is unexpected, and especially the outlier for the traditional character corpus merits further investigation. Nonetheless, we can conclude that for Chinese, you should expect to need <em>fewer</em> tokens than there are characters in your text. Depending on your content, you can expect to need 25% to 40% less.</p><p>English texts in Jina Embeddings v2 for Chinese and English yielded roughly the same number of tokens as they do in the English-only model:</p>
<!--kg-card-begin: html-->
<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="&lt;rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class style="width:165.3125px">24,243,607</td><td id="=]?F" class>26,890,176</td><td id="&lt;rlw" class>1.109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class style="width:165.3125px">22,825,712</td><td id="=]?F" class>25,060,352</td><td id="&lt;rlw" class>1.097</td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="taking-tokens-seriously">Taking Tokens Seriously</h2><p>Tokens are an important scaffolding for AI language models, and research is ongoing in this area.</p><p>One of the places where AI models have proven revolutionary is the discovery that they are very robust against noisy data. Even if a particular model does not use the optimal tokenization strategy, if the network is large enough, given enough data, and adequately trained, it can learn to do the right thing from imperfect input.</p><p>Consequently, much less effort is spent on improving tokenization than in other areas, but this may change.</p><p>As a user of embeddings, who buys them via an <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">API like Jina Embeddings</a>, you can&#x2019;t know precisely how many tokens you&#x2019;ll need for a specific task and may have to do some testing of your own to get solid numbers. But the estimates provided here &#x2014; circa 110% of the word count for English, circa 125% of the word count for German, and circa 70% of the character count for Chinese &#x2014; should be enough for basic budgeting.</p><h2 id="learn-more">Learn More</h2><p>For more information about Jina Embeddings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="A Deep Dive into Tokenization"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4308 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="A Deep Dive into Tokenization"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[MyScale & Jina AI: Unleashing Great Potential for Your AI Applications]]></title><description><![CDATA[With full integration of Jina Embeddings v2 models, MyScale allows users to harness the capabilities of Jina AI within an SQL database.]]></description><link>https://jina.ai/news/myscale-jina-ai-unleashing-great-potential-for-your-ai-applications/</link><guid isPermaLink="false">65b785a3c38742000104062a</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 29 Jan 2024 15:00:29 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"><p><a href="https://myscale.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MyScale</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its AI-focused SQL vector database. You can now use Jina AI&apos;s cutting-edge embedding models in conjunction with powerful and time-tested SQL database technologies to bring precise text matching and efficient semantic similarity to your data-driven applications.</p><p>Follow the link below to&#xA0;MyScale&apos;s website to see how you can supercharge your SQL with Jina Embeddings.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://myscale.com/blog/myscale-integration-jinaai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications</div><div class="kg-bookmark-description">MyScale is fully integrated with Jina Embeddings v2 in the EmbedText function, allowing users to process text with an input length of up to 8K using the standard SQL syntax.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://myscale.com/favicon.ico" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d3lhz231q7ogjd.cloudfront.net/blog/myscale-and-jinaai-2.jpg" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face]]></title><description><![CDATA[Jina AI's open-source bilingual embedding models for German-English and Chinese-English are now on Hugging Face.
We’re going to walk through installation and cross-language retrieval.]]></description><link>https://jina.ai/news/jina-embeddings-v2-bilingual-models-are-now-open-source-on-hugging-face/</link><guid isPermaLink="false">65b3adb510ff9f0001c50c4d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 26 Jan 2024 16:14:56 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><p>Jina AI has released its state-of-the-art open-source bilingual embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io">German-English</a> and <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese-English</a> language pairs via Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class="kg-bookmark-description">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class="kg-bookmark-description">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>In this tutorial, we&#x2019;re going to walk through a very minimal installation and use case that will cover:</p><ol><li>Downloading Jina Embedding models from Hugging Face.</li><li>Using the models to get encodings from texts in German and English.</li><li>Building a very rudimentary embeddings-based neural search engine for cross-language queries.</li></ol><p>We will show you how to use Jina Embeddings to write English queries that retrieve matching texts in German and vice-versa.</p><p>This tutorial works the same for the Chinese model. Just follow the instructions in the section (towards the end) titled <a href="#querying-in-chinese" rel="noreferrer"><strong>Querying in Chinese</strong></a> to get the Chinese-English bilingual model and an example document in Chinese.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-de &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-de.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-zh?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-zh &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="bilingual-embedding-models">Bilingual Embedding Models</h2><p>A bilingual embedding model is a model that maps texts in two languages &#x2014; German and English in this tutorial, Chinese and English for the Chinese model &#x2014; to the same embedding space. And, it does it in such a way that if a German text and an English text mean the same thing, their corresponding embedding vectors will be close together.</p><p>Models like this are very well suited to cross-language information retrieval applications, which we will show in this tutorial, but can also serve as a basis for RAG-based chatbots, multilingual text categorization, summarization, sentiment analysis, and any other application that uses embeddings. By using models like these, you can treat texts in both languages as if they were written in the same language.</p><p>Although many giant language models trained claim to support many different languages, they do not support all languages equally. There are growing questions of <a href="https://aclanthology.org/2023.findings-eacl.89/?ref=jina-ai-gmbh.ghost.io">bias caused by the dominance of English on the Internet</a> and input sources distorted by the <a href="https://arxiv.org/abs/2401.05749?ref=jina-ai-gmbh.ghost.io">widespread online publication of machine-translated texts</a>. By focusing on two languages, we can better control embedding quality for both, minimizing bias while producing much smaller models with similar or higher performance than giant models that purport to handle dozens of languages.</p><p>Jina Embeddings v2 bilingual models support 8,192 input context tokens, enabling them not just to support two languages, but also to support relatively large segments of text compared to comparable models. This makes them ideal for more complex use cases where much more textual information has to be processed into embeddings.</p><h2 id="follow-along-on-google-colab">Follow along on Google Colab</h2><p>This tutorial has an <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">accompanying notebook</a> that you can run on <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, or locally on your own system.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/workshops/blob/feat-embeddings-notebook/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/cce4fce8bbe78d8bdc0c77a288df9fa7/img/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="installing-the-prerequisites">Installing the Prerequisites</h2><p>Make sure the current environment has the relevant libraries installed. You will need the latest version of <a href="https://pypi.org/project/transformers/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>transformers</code></a>, so even if it is already installed, run:</p><pre><code class="language-bash">pip install -U transformers 
</code></pre><p>This tutorial will use the <a href="https://faiss.ai/?ref=jina-ai-gmbh.ghost.io">FAISS library from Meta</a> to do vector search and comparison. To install it, run:</p><pre><code class="language-bash">pip install faiss-cpu
</code></pre><p>We will also be using <a href="https://www.crummy.com/software/BeautifulSoup/?ref=jina-ai-gmbh.ghost.io">Beautiful Soup</a> to process the input data in this tutorial, so make sure it is installed:</p><pre><code class="language-bash">pip install bs4
</code></pre><h2 id="access-to-hugging-face">Access to Hugging Face</h2><p>You will need access to Hugging Face, specifically an account and an access token to download models.</p><p><strong>If you do not have an account on Hugging Face:</strong></p><p>Go to <a href="https://huggingface.co/?ref=jina-ai-gmbh.ghost.io">https://huggingface.co/</a> and you should see a &#x201C;Sign Up&#x201D; button on the upper right of the page. Click it and follow the instructions from there to make a new account.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="1088" height="887" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png 1088w" sizes="(min-width: 720px) 720px"></figure><p><strong>After you are logged into your account:</strong></p><p>Follow the instructions <a href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io">on the Hugging Face website</a> to get an access token.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">User access tokens</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/docs/hub.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>You need to copy this token into an environment variable called <code>HF_TOKEN</code>. If you&#x2019;re working in a notebook (on <a href="https://colab.research.google.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, for example) or setting it internally in a Python program, use the following Python code:</p><pre><code class="language-python">import os

os.environ[&apos;HF_TOKEN&apos;] = &quot;&lt;your token here&gt;&quot;
</code></pre><p>In your shell, use whatever the provided syntax is for setting an environment variable. In <code>bash</code> :</p><pre><code class="language-bash">export HF_TOKEN=&quot;&lt;your token here&gt;&quot;
</code></pre><h2 id="download-jina-embeddings-v2-for-german-and-english">Download Jina Embeddings v2 for German and English</h2><p>Once your token is set, you can download the Jina Embeddings German-English bilingual model using the <code>transformers</code> library:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
</code></pre><p>This may take several minutes the first time you do it, but the model will be cached locally after that, so don&#x2019;t worry if you restart this tutorial later.</p><h2 id="download-english-language-data">Download English-language Data</h2><p>For this tutorial, we are going to get the English-language version of the book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a>. This book is also available in Chinese and German, which we&#x2019;ll use later in this tutorial.</p><p>To download the EPUB version, run the following command:</p><pre><code class="language-bash">wget -O progit-en.epub https://open.umn.edu/opentextbooks/formats/3437</code></pre><p>This copies the book to a file named <code>progit-en.epub</code> in the local directory.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--27-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="490" height="647"><figcaption><span style="white-space: pre-wrap;">The cover of the paper edition.</span></figcaption></figure><p>Alternatively, you can just visit the link <a href="https://open.umn.edu/opentextbooks/formats/3437?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">https://open.umn.edu/opentextbooks/formats/3437</a> to download it to a local drive. It is available under the&#xA0;<a href="https://creativecommons.org/licenses/by-nc-sa/3.0/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Creative Commons Attribution Non Commercial Share Alike 3.0 license</a>.</p><h2 id="processing-the-data">Processing the Data</h2><p>This particular text has an internal structure of hierarchical sections, which we can easily find by looking for the <code>&lt;section&gt;</code> tag in the underlying XHTML data. The code below reads the EPUB file and splits it up using the internal structure of an EPUB file and the <code>&lt;section&gt;</code> tag, then converts each section to plain text without XHTML tags. It creates a Python dictionary whose keys are a set of strings indicating each section&#x2019;s location in the book, and whose values are the plain text contents of that section.</p><pre><code class="language-python">from zipfile import ZipFile
from bs4 import BeautifulSoup
import copy

def decompose_epub(file_name):
    
    def to_top_text(section):
        selected = copy.copy(section)
				while next_section := selected.find(&quot;section&quot;):
            next_section.decompose()
        return selected.get_text().strip()

    ret = {}
    with ZipFile(file_name, &apos;r&apos;) as zip:
        for name in zip.namelist():
            if name.endswith(&quot;.xhtml&quot;):
                data = zip.read(name)
                doc = BeautifulSoup(data.decode(&apos;utf-8&apos;), &apos;html.parser&apos;)
                ret[name + &quot;:top&quot;] = to_top_text(doc)
                for num, sect in enumerate(doc.find_all(&quot;section&quot;)):
                    ret[name + f&quot;::{num}&quot;] = to_top_text(sect)
    return ret
</code></pre><p>Then, run the <code>decompose_epub</code> function on the EPUB file you downloaded before:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-en.epub&quot;)
</code></pre><p>The variable <code>book_data</code> will now have 583 sections in it. For example:</p><pre><code class="language-python">print(book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;])
</code></pre><p>Result:</p><pre><code class="language-Text">The Command Line
There are a lot of different ways to use Git.
There are the original command-line tools, and there are many graphical user interfaces of varying capabilities.
For this book, we will be using Git on the command line.
For one, the command line is the only place you can run all Git commands&#x2009;&#x2014;&#x2009;most of the GUIs implement only a partial subset of Git functionality for simplicity.
If you know how to run the command-line version, you can probably also figure out how to run the GUI version, while the opposite is not necessarily true.
Also, while your choice of graphical client is a matter of personal taste, all users will have the command-line tools installed and available.
So we will expect you to know how to open Terminal in macOS or Command Prompt or PowerShell in Windows.
If you don&#x2019;t know what we&#x2019;re talking about here, you may need to stop and research that quickly so that you can follow the rest of the examples and descriptions in this book.
</code></pre><h2 id="generating-and-indexing-embeddings-with-jina-embeddings-v2-and-faiss">Generating and Indexing Embeddings with Jina Embeddings v2 and FAISS</h2><p>For each of the 583 sections, we will generate an embedding and store it in a FAISS index. Jina Embeddings v2 models accept input of up to 8192 tokens, large enough that for a book like this, we don&#x2019;t need to do any further text segmentation or check if any section has too many tokens. The longest section in the book has roughly 12,000 characters, which, for normal English, should be far below the 8k token limit.</p><p>To generate a single embedding, you use the <code>encode</code> method of the model we downloaded. For example:</p><pre><code class="language-python">model.encode([book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;]])
</code></pre><p>This returns an array containing a single 768 dimension vector:</p><pre><code class="language-python">array([[ 6.11135997e-02,  1.67829826e-01, -1.94809273e-01,
         4.45595086e-02,  3.28837298e-02, -1.33441269e-01,
         1.35364473e-01, -1.23119736e-02,  7.51526654e-02,
        -4.25386652e-02, -6.91794455e-02,  1.03527725e-01,
        -2.90831417e-01, -6.21018047e-03, -2.16205455e-02,
        -2.20803712e-02,  1.50471330e-01, -3.31433356e-01,
        -1.48741454e-01, -2.10959971e-01,  8.80039856e-02,
				....
</code></pre><p>That is an embedding.</p><p>Jina Embeddings models are set up to allow batch processing. The optimal batch size depends on the hardware you use when running. A large batch size risks running out of memory. A small batch size will take longer to process.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Setting <code spellcheck="false" style="white-space: pre-wrap;">batch_size=5</code> worked on Google Colab in free tier without a GPU, and took <b><strong style="white-space: pre-wrap;">about an hour</strong></b> to generate the entire set of embeddings.</div></div><p>In production, we recommend using much more powerful hardware or using Jina AI&#x2019;s <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Embedding API service</a>. Follow the link below to find out how it works and how to get started with free access.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>The code below generates the embeddings and stores them in a FAISS index. Set the variable <code>batch_size</code> as appropriate to your resources.</p><pre><code class="language-python">import faiss

batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>When working in a production environment, a Python dictionary is not an adequate or performant way to handle documents and embeddings. You should use a purpose-built vector database, which will have its own directions for data insertion.</p><h2 id="querying-in-german-for-english-results">Querying in German for English Results</h2><p>When we query for something from this set of texts, here&#x2019;s what will happen:</p><ol><li>The Jina Embeddings German-English model will create an embedding for the query.</li><li>We will use the FAISS index (<code>faiss_index</code>) to get the stored embedding with the highest cosine to the query embedding and return its place in the index.</li><li>We will look up the corresponding text in the vector data array (<code>vector_data</code>) and print out the cosine, the location of the text, and the text itself.</li></ol><p>That&#x2019;s what the <code>query</code> function below does.</p><pre><code class="language-python">def query(query_str):
    query = model.encode([query_str], normalize_embeddings=True)
    cosine, index = faiss_index.search(query, 1)
    print(f&quot;Cosine: {cosine[0][0]}&quot;)
    loc, txt = vector_data[index[0][0]]
    print(f&quot;Location: {loc}\\nText:\\n\\n{txt}&quot;)
</code></pre><p>Now let&#x2019;s try it out.</p><pre><code class="language-python"># Translation: &quot;How do I roll back to a previous version?&quot;
query(&quot;Wie kann ich auf eine fr&#xFC;here Version zur&#xFC;cksetzen?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5202275514602661
Location: EPUB/ch02-git-basics-chapter.xhtml::20
Text:

Undoing things with git restore
Git version 2.23.0 introduced a new command: git restore.
It&#x2019;s basically an alternative to git reset which we just covered.
From Git version 2.23.0 onwards, Git will use git restore instead of git reset for many undo operations.
Let&#x2019;s retrace our steps, and undo things with git restore instead of git reset.
</code></pre><p>This is a pretty good choice to answer the question. Let&#x2019;s try another one:</p><pre><code class="language-python"># Translation: &quot;What does &apos;version control&apos; mean?&quot;
query(&quot;Was bedeutet &apos;Versionsverwaltung&apos;?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5001817941665649
Location: EPUB/ch01-getting-started.xhtml::1
Text:

About Version Control

What is &#x201C;version control&#x201D;, and why should you care?
Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.
For the examples in this book, you will use software source code as the files being version controlled, though in reality you can do this with nearly any type of file on a computer.
If you are a graphic or web designer and want to keep every version of an image or layout (which you would most certainly want to), a Version Control System (VCS) is a very wise thing to use.
It allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more.
Using a VCS also generally means that if you screw things up or lose files, you can easily recover.
In addition, you get all this for very little overhead.
</code></pre><p>Try it with your own German questions to see how well it works. As a general practice, when dealing with text information retrieval, you should ask for three to five responses instead of just one. The best answer is often not the first one.</p><h2 id="reversing-the-roles-querying-german-documents-with-english">Reversing the Roles: Querying German documents with English</h2><p>The book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a> is also <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git-german?ref=jina-ai-gmbh.ghost.io">available in German</a>. We can use this same model to give this demo with the languages reversed.</p><p>Download the ebook:</p><pre><code class="language-bash">wget -O progit-de.epub https://open.umn.edu/opentextbooks/formats/3454
</code></pre><p>This copies the book to a file named <code>progit-de.epub</code>. We then process it the same way we did for the English book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-de.epub&quot;)
</code></pre><p>And then generate the embeddings the same way as before:</p><pre><code class="language-python">batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>We can now use the same <code>query</code> function to search in English for answers in German:</p><pre><code class="language-python">query(&quot;What is version control?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.6719034910202026
Location: EPUB/ch01-getting-started.xhtml::1
Text:

Was ist Versionsverwaltung?

Was ist &#x201E;Versionsverwaltung&#x201C;, und warum sollten Sie sich daf&#xFC;r interessieren?
Versionsverwaltung ist ein System, welches die &#xC4;nderungen an einer oder einer Reihe von Dateien &#xFC;ber die Zeit hinweg protokolliert, sodass man sp&#xE4;ter auf eine bestimmte Version zur&#xFC;ckgreifen kann.
Die Dateien, die in den Beispielen in diesem Buch unter Versionsverwaltung gestellt werden, enthalten Quelltext von Software, tats&#xE4;chlich kann in der Praxis nahezu jede Art von Datei per Versionsverwaltung nachverfolgt werden.
Als Grafik- oder Webdesigner m&#xF6;chte man zum Beispiel in der Lage sein, jede Version eines Bildes oder Layouts nachverfolgen zu k&#xF6;nnen. Als solcher w&#xE4;re es deshalb ratsam, ein Versionsverwaltungssystem (engl. Version Control System, VCS) einzusetzen.
Ein solches System erlaubt es, einzelne Dateien oder auch ein ganzes Projekt in einen fr&#xFC;heren Zustand zur&#xFC;ckzuversetzen, nachzuvollziehen, wer zuletzt welche &#xC4;nderungen vorgenommen hat, die m&#xF6;glicherweise Probleme verursachen, herauszufinden wer eine &#xC4;nderung urspr&#xFC;nglich vorgenommen hat und viele weitere Dinge.
Ein Versionsverwaltungssystem bietet allgemein die M&#xF6;glichkeit, jederzeit zu einem vorherigen, funktionierenden Zustand zur&#xFC;ckzukehren, auch wenn man einmal Mist gebaut oder aus irgendeinem Grund Dateien verloren hat.
All diese Vorteile erh&#xE4;lt man f&#xFC;r einen nur sehr geringen, zus&#xE4;tzlichen Aufwand.
</code></pre><p>This section&#x2019;s title translates as <em>&#x201C;What is version control?&#x201D;</em>, so this is a good response.</p><h2 id="querying-in-chinese">Querying in Chinese</h2><p>These examples will work exactly the same way with Jina Embeddings v2 for Chinese and English. To use the Chinese model instead, just run the following:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
</code></pre><p>And to get the Chinese edition of <em>Pro Git: Everything You Need to Know About Git</em>:</p><pre><code class="language-python">wget -O progit-zh.epub https://open.umn.edu/opentextbooks/formats/3455
</code></pre><p>Then, process the Chinese book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-zh.epub&quot;)
</code></pre><p>All the other code in this tutorial will work the same way.</p><h2 id="the-future-more-languages-including-programming">The Future: More Languages, including Programming</h2><p>We will be rolling out more bilingual models in the immediate future, with Spanish and Japanese already in the works, as well as a model that supports English and several major programming languages. These models are ideally suited to international enterprises that manage multilingual information, and can serve as the cornerstone for AI-powered information retrieval and RAG-based generative language models, inserting themselves into a variety of cutting-edge AI use cases.</p><p>Jina AI&#x2019;s models are compact and perform among the best in their class, showing how you don&#x2019;t need the biggest model to get the best performance. By focusing on bilingual performance, we produce models that are both better at those languages, easier to adapt, and more cost-effective than large models trained on uncurated data.</p><p>Jina Embeddings are available from <a href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io">Hugging Face</a>, in the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS marketplace</a> for use in Sagemaker, and via the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings web API</a>. They are fully integrated into many AI process frameworks and vector databases.</p><p>See the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings website</a> for more information, or contact us to discuss how Jina AI&#x2019;s offerings can fit into your business processes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure>]]></content:encoded></item></channel></rss>
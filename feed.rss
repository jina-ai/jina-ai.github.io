<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.72</generator><lastBuildDate>Fri, 10 Nov 2023 03:15:09 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data]]></title><description><![CDATA[Unlock the Secrets of the Satellites: Leverage SceneXplain's powerful 'Extract JSON from Image' feature for land use classification]]></description><link>https://jina.ai/news/look-up-in-the-sky-using-scenexplain-to-classify-land-use-from-satellite-data/</link><guid isPermaLink="false">6544c26c2305600001855879</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 06 Nov 2023 15:00:55 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><p>Is it a bird? Is it a plane? Is it a dense residential area? Or maybe even a chaparral? Worry not readers. SceneXplain is here to let you know. We don&apos;t even have to jump into a phone booth to change first.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F331;</div><div class="kg-callout-text">A <a href="https://en.wikipedia.org/wiki/Chaparral?ref=jina-ai-gmbh.ghost.io">chaparral</a> is a shrubland plant community found primarily in California, in southern Oregon and in the northern portion of the Baja California Peninsula in Mexico. <br><br>You&apos;ll see why this is relevant later.</div></div><p>In this blog post, we&apos;ll talk about our experiments using SceneXplain to look at satellite imagery and classify it based on what the land is used for. We&apos;ll go through three different datasets and explain how we did what we did.</p><h2 id="what-is-land-use-classification-and-what-is-it-used-for">What is land use classification and what is it used for?</h2><p>Using satellite data for classifying land use involves working out what a section of the Earth is used for, such as agriculture, city, forest, or body of water. The focus here is on using optical satellite imagery as the data source, which is then analyzed through AI image models to determine the land use. It&apos;s often used for:</p><ul><li>Resource Management: Knowing how land is being used aids in allocating and managing resources, such as water for irrigation.</li><li>Urban Planning: Good land use maps are invaluable to city planners so that they can know what types of land use exist in and around urban areas to make informed decisions on infrastructure development.</li><li>Environmental Protection: By identifying sensitive or critical ecological zones, conservationists can better protect and manage them.</li><li>Disaster Management: Land use classification can help to identify vulnerable areas that require priority during emergencies.</li><li>Climate Change Studies: Monitoring land use over time can offer insights into climate change impacts, such as the rate of deforestation.</li><li>Research and Education: Scientists and educators across disciplines use this data for various kinds of research, from ecology to social sciences.</li></ul><h2 id="up-up-and-away-how-we-classify-land-usage-in-scenexplain">Up, up, and away! How we classify land usage in SceneXplain</h2><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F40D;</div><div class="kg-callout-text">You can find a link to the notebook <a href="https://colab.research.google.com/github/alexcg1/scenex-geospatial/blob/main/notebook.ipynb?ref=jina-ai-gmbh.ghost.io">here</a>.</div></div><h3 id="datasets">Datasets</h3><p>We used three datasets for our testing:</p><ul><li><a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html?ref=jina-ai-gmbh.ghost.io">UC Merced land use classification</a></li><li><a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID (Aerial Image Dataset)</a></li><li><a href="https://www.tensorflow.org/datasets/catalog/resisc45?ref=jina-ai-gmbh.ghost.io">RESISC45 (Remote Sensing Image Scene Classification)</a></li></ul><p>Each of these consists of satellite and aerial images of different sections of Earth, labeled by what they are used for, e.g. <code>dense_residential</code>, <code>round_farm</code>, <code>intersection</code>, etc.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg" width="600" height="600" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg 600w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral_010.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><code spellcheck="false" style="white-space: pre-wrap;"><span>airport</span></code><span style="white-space: pre-wrap;"> from AID dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>denseresidential</span></code><span style="white-space: pre-wrap;"> from UC Merced dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>chaparral</span></code><span style="white-space: pre-wrap;"> from RESISC45 dataset</span></p></figcaption></figure><h3 id="scenexplain">SceneXplain</h3><p>We used SceneXplain&apos;s <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">&quot;Extract JSON from image&quot; feature</a> with the Flash algorithm to extract one label per input image. After testing with several algorithms, we saw the Flash algorithm offered the fastest performance, and precision on par with more recent algorithms like <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Jelly</a>.</p><p>Rather than manually uploading each image via the web UI, we used SceneXplain&apos;s API to upload and analyze images one at a time.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We could speed this up by (ab)using SceneXplain&apos;s cid feature when batching and giving each image a unique ID. The ID would contain the image&apos;s &quot;official&quot; category, so (after processing) we could extract that and compare it to the category assigned by SceneXplain.</div></div><h3 id="json-schema">JSON Schema</h3><p>Why even use a JSON schema? Why not just let SceneXplain interpret the images directly? Say, by looking at an aerial view of a baseball diamond from the UC Merced dataset?</p><p>Here&apos;s why:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--39-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="441" height="666"><figcaption><a href="https://scenex.jina.ai/share?thread=qjW5p1gZBYK2HONbNsOE&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>It&#x2019;s a clear description, but nowhere does it mention <code>baseballdiamond</code>, which (since we&#x2019;re doing classification) is the label we want.</p><p>We could also try asking it a question using visual question answering (VQA):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--40-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="444" height="798"><figcaption><a href="https://scenex.jina.ai/share?thread=VW4xcmLawvG8lWInwwAa&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>That answer is slightly better, but still throws in a lot more superfluous words. If we were to bulk classify images using this method we would have to search each answer for the category label, since there would be no consistent wording between them. And what would happen if an image were ambiguous and SceneXplain tried to assign it two categories or more?</p><p>For this reason, we used SceneXplain&#x2019;s &#x201C;<a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io">Extract JSON from image</a>&#x201D; feature, where we specify a JSON Schema and can thus get highly structured, standardized output:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="612" height="1389" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--41-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png 612w"><figcaption><a href="https://scenex.jina.ai/share?thread=agnPj7H47wl87kDiEjM6&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>We used the following JSON schema:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;category&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Which single main category of geospatial imagery does this image belong to?&quot;,
      &quot;enum&quot;: [&lt;categories from dataset&gt;],
      &quot;maxContains&quot;: 1
    }
  }
}
</code></pre><p>Since each dataset has different (albeit similar) categories, we generated the list dynamically for each dataset.</p><h2 id="does-it-work">Does it work?</h2><p>Sometimes! Like at least 60% of the time! In some randomized tests we got close to 80% (or occasionally as low as 50%).</p><p>Several factors can cause it to fail:</p><ul><li>Some categories can look very similar, e.g. <code>sparse_residential</code>, <code>medium_residential</code>, <code>dense_residential</code>. SceneXplain often picks the wrong one. This can also be seen in cases like <code>road</code> vs <code>runway</code>.</li><li>Occasionally it hallucinates a new category not specified in the <code>enum</code>, for example <code>residential</code> (as opposed to <code>denseresidential</code>). Occasionally it glitches and assigns a category like <code>A</code>.</li><li>Some category names like <code>chaparral</code> are uncommon words and/or concepts. It seems unlikely that many pictures of (or references to) chaparrals are in its training datasets. With so little data, the model barely knows what a chaparral is or what it looks like. (Before reading this post, did you?)</li><li>Aerial view geospatial images are under-represented in the training corpora relative to other images. This means that to a general-purpose tool like SceneXplain, an aerial view of a chaparral may look more like mold or dirt on a surface.</li></ul><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png" width="1200" height="1200" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--42-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--42-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png 1200w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral01.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">One of these is mold. The other is chaparral. Sorry, California.</span></p></figcaption></figure><h2 id="why-not-use-a-dedicated-model">Why not use a dedicated model?</h2><p>Most land-use classification is indeed done with dedicated models, specifically trained on aerial-view land use imagery. This makes them perfect for that particular task while not being so hot at general image classification.</p><p>So, why not just use one of those?</p><h3 id="self-hosting-is-a-drag">Self-hosting is a drag</h3><p>Even self-hosting something like Resnet (which has <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/torch-neuronx/resnet50-inference-on-trn1-tutorial.html?ref=jina-ai-gmbh.ghost.io">decent tutorials</a>) is a drag. You need to set up AWS, install dependencies, compile the model, and so on. It&#x2019;s not as easy as using an existing software-as-a-service like SceneXplain.</p><h3 id="academic-models-are-pure-kryptonite">Academic models are pure kryptonite</h3><p>Let&apos;s look at the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset</a> and see how we can replicate what they did in their paper:</p><p>First of all, we have to download the code from OneDrive or Baidu Pan. Dang, the file no longer exists on OneDrive, and Baidu Pan wants me to install a random RPM or DEB file just to download the dataset. Blech.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="1238" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--43-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--43-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png 1238w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Parents: Please check your children&apos;s Halloween candy. I just found a Baidu Pan installation RPM in my kid&apos;s bag. Disgusting.</span></figcaption></figure><p>Assuming we have a friend in China who downloads the code and sends it to us (thanks Kelly!), we can extract it and check the <code>readme.txt</code> (Did you assume the readme would be in a repo somewhere, you sweet innocent summer child?)</p><p>Checking the file dates with <code>ls -l</code>, we see the files were last modified in 2016. Great.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="741" height="338" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--44-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png 741w" sizes="(min-width: 720px) 720px"></figure><p>Now we download the pre-trained model from the <a href="https://www.vlfeat.org/matconvnet/pretrained/?ref=jina-ai-gmbh.ghost.io">URL in the readme</a>. Good news! The link works! Bad news! There are lots of CNNs there and the readme doesn&apos;t say which one to download!</p><p>To quote the readme, the next step is to: <code>Compile vlfeat, gist, matconvnet and liblinear, and place them under &lt;libs&gt;</code>. We didn&apos;t actually try this, but I&apos;m <em>sure</em> compiling software from over five years ago will go without a hitch. I just need to use version...um. There&apos;s no version specified. It&apos;s sheer dumb luck I guess!</p><p>The final step: We&apos;ve put so much work into the other steps that now we can finally reap the fruits of our labor. We just need to install MatLab. Which is 860 EUR per year. Wonderful.</p><p>I&apos;m sure setting up the model was worth the effort for whoever wrote the paper. But if I&apos;m just trying things out, that&apos;s a lot of work and money.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A2;</div><div class="kg-callout-text">On the verge of giving up, I checked the directory structure of the code. Reading the readme, I assumed I had to <i><em class="italic" style="white-space: pre-wrap;">download</em></i> the libs. But the libs folder already exists. Already (supposedly) populated with what I need. Turns out I can&apos;t even trust the readme (or perhaps myself, to be honest)</div></div><h3 id="i-got-ninety-nine-problems-but-a-chaparral-aint-one">I got ninety-nine problems, but a chaparral ain&apos;t one</h3><p>Even assuming pre-trained models were simple to use, the categories are baked in already. Since most of the training data was (I assume) taken from California, chaparrals are all over the place, but there aren&apos;t so many medieval castles. So if I wanted to apply the same model to Europe, I&apos;d be stuck with those categories.</p><p>On the other hand, SceneXplain is a general-purpose tool. This means it may not have extensive chaparral knowledge (seriously, outside of Californians, who among us does?), but it has enough general-purpose knowledge that it can more reliably classify what you&apos;re specifically looking for.</p><p>Let&apos;s just say that specialist data can be kryptonite to a general-purpose image classification/captioning model. AI models aren&#x2019;t (and can&#x2019;t be) all things to all people, after all.</p><p>When you think of the data used to train general-purpose image models, very little would be aerial views of the landscape taken from satellites and labels for such data (<code>road</code>, <code>residential</code>, etc.) would far more commonly be applied to pictures of those phenomena taken from a more human-centric angle.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png" width="1125" height="750" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--45-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--45-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png 1125w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10-1.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Dense residential (L: How it&apos;s represented in most models, R: how it&apos;s represented in UC Merced dataset)</span></p></figcaption></figure><h2 id="why-not-use-scenexplain">Why NOT use SceneXplain?</h2><p>While using dedicated models has its downsides, there are some advantages:</p><h3 id="you-only-care-about-chaparrals">You only care about chaparrals</h3><p>Good for you buddy. Good for you.</p><h3 id="scenexplain-isnt-as-accurate-as-pre-trained-models">SceneXplain isn&apos;t as accurate as pre-trained models</h3><p>While a pain to set up, pre-trained models offer superior accuracy when it comes to this very specific domain. Those models were trained specifically on aerial imagery and thus offer accuracy rates of 85-95% when using high-level methods (see final table in the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset paper</a>), compared to SceneXplain&apos;s accuracy of about 60%.</p><h2 id="next-steps">Next steps</h2><p>Large language models and image-to-text models aren&#x2019;t specially trained to recognize landscape types from satellite imagery. There&#x2019;s enough in their training data to make a stab at it, and it&#x2019;s not totally useless, but zero-shot detection is just not good. Some categories are (by their nature) going to be tough to recognize, even for a human. If we can&#x2019;t tell from 30,000 feet how many people live on a city block, then how could we expect SceneXplain to tell <code>sparseresidential</code> from <code>mediumresidential</code>?</p><p>Specially trained and fine-tuned models perform well enough to be a hard bar to beat.</p><p>But often you hit upon a problem that <strong>doesn&#x2019;t</strong> have a specialized dataset that can be used for training a dedicated model. That means that a general-purpose tool, like SceneXplain, may be your best bet. Our next steps are trying to see how much better we can make SceneX with the least effort, so that our users can always get the best performance possible for the least added effort.</p><p>Even if no one ever uses SceneXplain to catalog land use, learning to make it better at this task teaches us how to make it better for something else that it&#x2019;s never specially learned to do.</p><p>To test out SceneXplain for our own use case, create a free account and start playing! Let us know how you&apos;re doing on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings 2: The Best Solution for Embedding Long Documents]]></title><description><![CDATA[With Jina Embeddings 2 models you get high-quality embeddings from an open-source, downloadable model with an input size of 8,192 tokens.]]></description><link>https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/</link><guid isPermaLink="false">65425e91d9dd650001cdb1b6</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Thu, 02 Nov 2023 15:00:06 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Blog-images--7-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Blog-images--7-.jpg" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><p>Text embeddings are the backbone of AI language processing, powering text clustering, information retrieval, sentiment analysis, text-to-image processing, and information extraction, among other core tasks. But there&#x2019;s a catch: Until now, text embedding models have been trained for very short text segments, typically a few hundred tokens at most. <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">Jina Embeddings 1 models</a>, based on the <a href="https://huggingface.co/docs/transformers/model_doc/t5?ref=jina-ai-gmbh.ghost.io">T5 models</a>, are limited to 512 tokens. Tokens don&#x2019;t <em>quite</em> match up to words one-to-one, but this means the largest texts that they can support are just a few short paragraphs.</p><p>But not anymore!</p><p>With <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings 2 models</a>, you can get high-quality embeddings from an open-source, Apache 2-licensed model with an input size of 8,192 tokens &#x2014; more than sixteen times as much as Jina Embeddings 1 and the widely used <a href="https://www.sbert.net/?ref=jina-ai-gmbh.ghost.io">SBERT</a> models!</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class="kg-bookmark-description">Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, e&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Michael G&#xFC;nther</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"></div></a><figcaption><p><span style="white-space: pre-wrap;">Read the Jina Embeddings 2 paper on arXiv</span></p></figcaption></figure><p>Few other embedding models offer input sizes exceeding 512 tokens, and at present, the <em>only </em>one that accepts over 8,000 tokens is neither open-source nor available for download. Jina Embeddings 2 models not only give you a larger input size than all other open-source models, but they also rival the performance of the closed-source alternative.</p><p>Check for yourself on the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">HuggingFace MTEB leaderboard</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">MTEB Leaderboard - a Hugging Face Space by mteb</div><div class="kg-bookmark-description">Discover amazing ML apps made by the community</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><span class="kg-bookmark-author">a Hugging Face Space by mteb</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/spaces/mteb/leaderboard.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"></div></a></figure><h2 id="how-jina-embeddings-2-supports-long-documents">How Jina Embeddings 2 Supports Long Documents</h2><p>Jina AI has implemented the <a href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io">ALiBi approach</a>, the first embedding model to do so. ALiBi is an alternative to the positional encoding system first proposed in the famous &#x201C;<a href="https://dl.acm.org/doi/10.5555/3295222.3295349?ref=jina-ai-gmbh.ghost.io">Attention is all you need</a>&#x201D; paper. This scheme makes it possible to train embedding models on short texts and still get high-quality results on longer texts.</p><p>For Jina Embeddings 2, Jina AI has taken the BERT architecture and grafted it into ALiBi.</p><p>ALiBi was originally proposed for text generators (like ChatGPT) that only care about dependencies on previous words. This means that the last tokens in the input matter more than earlier ones because a text generator&#x2019;s task is to produce the next word after the input. This makes no sense for an embedding model that tries to create a representation of the whole text input, so Jina Embeddings 2 models implement a bi-directional version of ALiBi.</p><p>This architectural change, however, means that we can&#x2019;t use the pre-training of the original BERT model and have to completely retrain the model from scratch.</p><h2 id="english-only-for-now-german-and-chinese-soon">English-Only for Now, German and Chinese Soon</h2><p>Although there are plenty of multilingual text embedding models out there, there are known issues with providing universal language support. Not all languages are equal when it comes to embedding quality, and <a href="https://aclanthology.org/2023.sigtyp-1.16.pdf?ref=jina-ai-gmbh.ghost.io">recent research</a> shows that models tend to be biased towards structures that parallel English ones, distorting embeddings.</p><p>In short, multilingual models have an &#x201C;accent&#x201D;, usually an English one due to the majority of English data in the training.</p><p>For superior language-specific performance, and more compact, easier-to-train models, Jina Embeddings are currently English-only. We&#x2019;re planning German and Chinese models for the near future.</p><h2 id="training-jina-embeddings-2-for-top-performance-up-to-8192-tokens">Training Jina Embeddings 2 for Top Performance up to 8,192 Tokens</h2><p>Using ALiBi means that even though Jina Embeddings 2 models support larger input sizes, the models don&#x2019;t have to be trained with larger input data. The training learned for short sequences scales up to larger ones automatically.</p><p>Our training is similar to the way other embedding models are trained. We start with <a href="https://aclanthology.org/N19-1423.pdf?ref=jina-ai-gmbh.ghost.io">masked word pre-training</a> using the circa 170 billion word English-only <a href="https://huggingface.co/datasets/c4?ref=jina-ai-gmbh.ghost.io">C4 dataset</a>. Then, we do <a href="https://arxiv.org/abs/1607.08085?ref=jina-ai-gmbh.ghost.io">pairwise contrastive training</a>. This means taking pairs of texts that are known to be similar or dissimilar and adjusting the weights of the embedding model so that similar inputs are closer together, and dissimilar ones are farther apart. We used a new corpus of text pairs, curated by Jina AI, based on the one <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">used to train the Jina Embeddings 1 models</a>.</p><p>Finally, we fine-tuned the model using <a href="https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io">text triplets and negative mining</a>, with an in-house training dataset specially augmented with sentences of opposite grammatical polarity. Embedding models have typically had trouble with negative polarity sentences: A sentence like &#x201C;The dog is in the car&#x201D; will often have an embedding close to &#x201C;The dog is outside the car,&#x201D; even though these are naturally opposite in meaning.</p><p>We added a collection of positive and negative pairs like this to the training data, using the same methods <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">employed for the Jina Embeddings 1 models</a> to specifically improve performance on this kind of language.</p><h2 id="three-models-to-better-fit-your-use-case">Three Models To Better Fit Your Use Case</h2><p>The Jina Embeddings 2 models come in three sizes, providing high-quality embeddings for users with different requirements and capabilities. All three support 8,192 input tokens.</p><ul><li><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a>: 33 million parameters, 512-dimension embeddings.</li><li><a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-base-en</code></a>: 137 million parameters, 768-dimension embeddings.</li><li><code>jina-embeddings-v2-large-en</code>: 435 million parameters, 1,024-dimension embeddings.</li></ul><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1296" height="946" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image.png 1296w" sizes="(min-width: 720px) 720px"></figure><p>The <code>jina-embeddings-v2-large-en</code> model is not yet available to download but will be released in the immediate future.</p><h2 id="bigger-inputs-leaner-models-peak-performance">Bigger Inputs, Leaner Models, Peak Performance</h2><p>We tested the Jina Embeddings 2 models against the <a href="https://arxiv.org/abs/2210.07316?ref=jina-ai-gmbh.ghost.io">MTEB benchmark suite</a> and at the time of writing:</p><ul><li><code>jina-embeddings-v2-base-en</code> scores roughly on par with the best models on most benchmarks, and generally better than similarly sized ones.</li><li><code>jina-embeddings-v2-small-en</code> ranks near the top for models with sizes under 100MB.</li></ul><p>The <code>jina-embeddings-v2-large-en</code> model is not yet available for testing.</p><p>However, among models that take more than 512 tokens in input, there is only one that compares to the Jina Embeddings 2 models: OpenAI&#x2019;s <code>text-embedding-ada-002</code>. This model is not publicly available and can only be accessed via a paid web-based API. Its size is unknown.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-14.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1184" height="375" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-14.png 1184w"><figcaption><span style="white-space: pre-wrap;">jina-embeddings-v2-base-en compared with OpenAI&#x2019;s text-embedding-ada-002 on the English MTEB benchmark</span></figcaption></figure><p><code>jina-embeddings-v2-base-en</code> is comparable in performance with <code>text-embedding-ada-002</code> on all benchmarks and even exceeds it in several tasks. Furthermore, the Jina Embeddings 2 models all produce smaller embedding vectors than <code>text-embedding-ada-002</code> which produces 1,536-dimensional output, compared to 512, 768, and 1,024 dimensions for our three model sizes respectively. This means considerable savings in computing time and memory for applications. Storing shorter vectors takes less space in memory and storage, speeds up database retrieval, and calculating the distance between them is proportionately fast.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-15.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1204" height="376" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-15.png 1204w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings 2 compared to other leading AI embedding models</span></figcaption></figure><p>Jina AI&#x2019;s base and small model compared to other leading embedding models</p><p>Furthermore, <code>jina-embeddings-v2-small-en</code> is the only model under 100MB that supports more than 512 input tokens.</p><p>Even if we set aside support for larger inputs, Jina Embeddings offers performance on par with the most common embedding foundation models, while remaining roughly the same size or even significantly smaller.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-16.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1000" height="433" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-16.png 1000w"></figure><p>This gives Jina Embeddings 2 models an impressive advantage in terms of accessibility, economic use of computing power, and cost to users.</p><h2 id="integration">Integration</h2><p>Jina Embeddings 2 models are already integrated into:</p><ul><li><a href="https://github.com/huggingface/text-embeddings-inference?ref=jina-ai-gmbh.ghost.io#supported-models">HuggingFace Text Embeddings Inference API</a></li><li><a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/jina_embeddings.ipynb?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a></li><li><a href="https://twitter.com/xenovacom/status/1717904546481992094?ref=jina-ai-gmbh.ghost.io">Transformers.js</a></li><li><a href="https://github.com/simonw/llm-embed-jina?ref=jina-ai-gmbh.ghost.io">LLM</a></li></ul><p>More integrations are coming soon, both from Jina AI and our open-source user community.</p><h2 id="try-it-out-yourself-right-now">Try It Out Yourself Right Now</h2><p>Go to <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io#enterprises">https://jina.ai/#enterprises</a> to get an access key for Jina AI&apos;s online Embedding API with ten thousand free tokens for you to embed. You&apos;ll find the key on the upper right:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-4.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1289" height="712" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-4.png 1289w" sizes="(min-width: 720px) 720px"></figure><p>You can access Jina Embeddings 2 models hosted at Jina AI via any standard HTTP interface. Use the drop-down menus to select between the <code>jina-embeddings-v2-base-en</code> and <code>jina-embeddings-v2-small-en</code> models, and to get code snippets to use the Embeddings API.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-6.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1309" height="737" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-6.png 1309w" sizes="(min-width: 720px) 720px"></figure><p>You can also get example code snippets in a variety of languages and frameworks to help integrate Jina Embeddings directly into your project:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-7.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1307" height="972" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-7.png 1307w" sizes="(min-width: 720px) 720px"></figure><p>And if you need to add tokens to your API key, just click the &quot;Top Up&quot; tab.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-8.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1281" height="1155" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-8.png 1281w" sizes="(min-width: 720px) 720px"></figure><h2 id="future-jina-embeddings-models">Future Jina Embeddings Models</h2><p>Jina AI will be rolling out a larger model, <code>jina-embeddings-v2-large-en</code>, in the immediate future, which we expect to exceed the performance of the other Jina Embeddings 2 models and compete with the highest-scoring large embedding models.</p><p>In the near future, we are expanding our offerings to include German and Chinese embedding models, which we expect will match or exceed state-of-the-art performance in both languages.</p><p>These benchmark results prove the robustness of Jina AI&#x2019;s contrastive training methodology, and we are always investigating improved model design and AI techniques. Our mission is to put more intelligence and higher performance into more compact open-access models that you can affordably access via our API, or easily run on your hardware and in your cloud instances.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.4 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-4-update/</link><guid isPermaLink="false">65411268d9dd650001cdb18b</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Tue, 31 Oct 2023 14:45:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-3.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3224">Release Note (<code>3.22.4</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-3.jpg" alt="Jina 3.22.4 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.4?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.4 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.4) Release time: 2023-10-31 14:33:21 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Add option to use body for streaming instead of params (#6098)
In order to allow complex neste&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.22.4 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/39a4535a750f8cf9c61cd33bd367faa5bb713422cddd112641a36c1a99333c4f/jina-ai/jina/releases/tag/v3.22.4" alt="Jina 3.22.4 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="add-option-to-use-body-for-streaming-instead-of-params-6098">Add option to use body for streaming instead of params (<a href="https://github.com/jina-ai/jina/pull/6098?ref=jina-ai-gmbh.ghost.io">#6098</a>)</h3><p>In order to allow complex nested documents to pass through streaming endpoints, the&#xA0;<code>GET</code>&#xA0;endpoint can now get the data from the body or the parameters of the HTTP request.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain]]></title><description><![CDATA[See how SceneXplain enhanced search quality, and enriched user experience for a top European e-commerce platform.]]></description><link>https://jina.ai/news/case-study-revolutionizing-e-commerce-user-experience-and-streamlining-search-with-scenexplain/</link><guid isPermaLink="false">6537c300dcdd090001c7d12b</guid><category><![CDATA[Knowledge Base]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Mon, 30 Oct 2023 13:00:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/2.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/2.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><p></p><h2 id="client-overview">Client Overview</h2><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-27-at-10.58.59.png" class="kg-image" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain" loading="lazy" width="1295" height="189" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-2023-10-27-at-10.58.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-2023-10-27-at-10.58.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-27-at-10.58.59.png 1295w" sizes="(min-width: 720px) 720px"></figure><p>Our client, a leading European e-commerce platform, specializing in fashion and lifestyle, offers a wide range of clothing, footwear, and accessories. Their platform features an extensive selection of emerging and well-known brands, with a total of around two million product images, spanning all their product categories. </p><p><strong>The client&apos;s vision was clear</strong>: automatically generate basic and specific descriptions for all these images and, more importantly, categorize them with detailed tags to<em> improve user experience and enhance search result quality.</em></p><h2 id="why-they-started-looking-for-a-new-solution">Why They Started Looking For A New Solution</h2><p></p><blockquote>&quot;We were facing significant challenges in managing our vast catalog of product images. Manual image description generation was too time-consuming. We needed a solution to save time and resources and provide comprehensive product tagging. SceneXplain was the game-changer that helped us achieve this.&quot; - Sales Director</blockquote><p><strong>1. Efficiently Generating Descriptions</strong>: With two million images to manage, creating descriptions for each one manually was a time&#x2013;consuming and inefficient task. It took roughly a week for them to write descriptions for the first seven thousand images. The client sought an automated solution to save valuable time and resources.</p><p><strong>2. Better Product Tagging</strong>: They wanted a comprehensive tagging system that went beyond general descriptions. They needed to categorize products by attributes such as color, gender, and more, all while maintaining a consistent schema for easy management.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try SceneXplain Now</span></p></figcaption></figure><h2 id="how-scenexplains-api-saved-time-and-effort">How SceneXplain&apos;s API Saved Time And Effort </h2><p>SceneXplain addressed these challenges with a two-fold strategy:</p><p><strong>1. Automated Image Description</strong>: SceneXplain leveraged its powerful API to enable batch image uploads, eliminating the need to process each image one by one. Using SceneXplain&#x2019;s API, the client achieved high-quality image descriptions, reducing the time and effort required to manually craft descriptions. When they usually spent over two weeks on getting the description of 14k of their images, now with batch upload that number decreased by 75%.</p><p><strong>2. Comprehensive Tagging with JSON Schema</strong>: SceneXplain&#x2019;s JSON schema system enabled the client to automatically categorize images with a fine-grained tag scheme optimized for their product search system. Using SceneXplain, their tagging system progressed from a few categories to a multifaceted system with highly detailed descriptive labels, making it easy to manage and optimize their product listings</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try SceneXplain Now</span></p></figcaption></figure><h2 id="conclusion"><strong>Conclusion</strong></h2><p>At SceneXplain, we&apos;re on a mission to empower businesses with innovative solutions that enhance user experience.</p><p>We&apos;re dedicated to delivering results and exceeding expectations, and we&apos;re proud to have earned high satisfaction and recognition from our clients.</p><p>Join us on this journey to make technology and digital content accessible to all!</p>]]></content:encoded></item><item><title><![CDATA[How to Caption Image Segments with SceneXplain: Introducing Crop'n Explain]]></title><description><![CDATA[SceneXplain: Precise image insights, webcam uploads, multi-API keys, and more user-friendly updates.]]></description><link>https://jina.ai/news/how-to-caption-image-segments-with-scenexplain-introducing-cropn-explain/</link><guid isPermaLink="false">65391d7fdcdd090001c7d24b</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Fri, 27 Oct 2023 14:00:37 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--14-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--14-.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><p>We&apos;re excited to introduce some exciting updates to SceneXplain 0.16.5, your reliable platform for uncovering the hidden stories within your images.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try The New Features Now</span></p></figcaption></figure><p>Let&apos;s dive right in and explore these improvements!</p><h2 id="focus-on-what-matters-most-in-your-images-with-cropn-explain">Focus On What Matters Most in Your Images with Crop&apos;n Explain</h2><p>With Crop&apos;n Explain, you can extract insights and descriptions for the elements that most matter to you. </p><p>It&apos;s as simple as uploading an image and selecting your area of interest. Our advanced computer vision algorithm then springs into action, automatically generating accurate and detailed captions for it. </p><p>Whether you&apos;re a content creator, media professional, SEO expert, or part of an e-commerce enterprise, Crop&apos;n Explain provides the ultimate tool for making sense of the visual world.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost.mp4" poster="https://img.spacergif.org/v1/1924x1080/0a/spacer.png" width="1924" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:41</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate">1&#xD7;</button>
                        <button class="kg-video-unmute-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Step-by-Step Guide</span></p></figcaption>
        </figure><p><strong>Beyond Captioning: Advanced Visual Insights</strong></p><p>Crop&apos;n Explain also offers a range of advanced visual comprehension tasks, enabling you to dive deeper into your images. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-26-at-17.00.21.png" class="kg-image" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain" loading="lazy" width="2000" height="1083" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-2023-10-26-at-17.00.21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-2023-10-26-at-17.00.21.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Screenshot-2023-10-26-at-17.00.21.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/10/Screenshot-2023-10-26-at-17.00.21.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Crop&apos;n Explain</span></figcaption></figure><p>From Visual Question Answering to JSON output and image captioning, you&apos;ll have the tools you need to gain a comprehensive understanding of your visuals. </p><h2 id="capturing-moments-is-now-more-convenient-than-ever">Capturing Moments Is Now More Convenient Than Ever</h2><p>Simplify image uploads with our new webcam integration feature on SceneXplain. </p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:15</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate">1&#xD7;</button>
                        <button class="kg-video-unmute-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Capture Moments Instantly In Action</span></p></figcaption>
        </figure><p>Instead of the traditional method of uploading images from your local file system, you can now directly capture and upload photos using your device&apos;s camera, whether you&apos;re using your desktop computer, laptop or phone.<br><br>This eliminates the need for traditional image file uploads from your local file system!</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try The New Features Now</span></p></figcaption></figure><h2 id="streamline-team-collaboration-with-multiple-api-keys">Streamline Team Collaboration with Multiple API Keys</h2><p>In a collaborative work environment, the multi-API key function in SceneXplain offers significant advantages. It allows each team member to have their own dedicated API key, promoting individualized access and facilitating task management. </p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key.mp4" poster="https://img.spacergif.org/v1/1924x1080/0a/spacer.png" width="1924" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:20</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate">1&#xD7;</button>
                        <button class="kg-video-unmute-icon">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">See Multi API Key Management In Action</span></p></figcaption>
        </figure><p>Whether your focus is on image captioning or any other project aspect, these unique API keys offer flexibility and improve workflow coordination.</p><p>Stay tuned for more updates, your feedback continually drives our commitment to improving SceneXplain. </p><p>Happy exploring!</p>]]></content:encoded></item><item><title><![CDATA[Are you ready for this Jelly? SceneXplain’s new algo kills hallucinations dead]]></title><description><![CDATA[SceneXplain's state-of-the-art Jelly algorithm is more concise, readable and accurate than ever before, while killing hallucinations.]]></description><link>https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead/</link><guid isPermaLink="false">65366f5e6930100001517b53</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Wed, 25 Oct 2023 14:00:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><p>Another month rolls by, and here at Jina AI we&#x2019;re releasing a new, more refined algorithm for <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">SceneXplain</a>.</p><p>A while back, we introduced SceneXplain&#x2019;s &#x201C;Glide&#x201D; algorithm, which nailed recognizing multilingual text in images, something other models are now only just introducing:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--1-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="447" height="844"></figure><p></p><p>But that was then. This is now. So, what comes next? Fixing hallucination, that&#x2019;s what.</p><h2 id="what-is-hallucination-in-computer-vision">What is hallucination in computer vision?</h2><p>You may already be familiar with AI hallucination when it comes to LLMs, for example, <a href="https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html?ref=jina-ai-gmbh.ghost.io">ChatGPT hallucinating papers and sources that don&#x2019;t exist in reality</a>.</p><blockquote class="kg-blockquote-alt"><strong>&quot;[ChatGPT is an] omniscient, eager-to-please intern who sometimes lies to you&quot;</strong><br>Professor Ethan Mollick of <a href="https://en.wikipedia.org/wiki/Wharton_School_of_the_University_of_Pennsylvania?ref=jina-ai-gmbh.ghost.io">Wharton</a></blockquote><p>Likewise, image generation models (like Midjourney) often hallucinate unwanted details (or unwanted fingers for that matter.)</p><p><a href="https://gagadget.com/en/how-it-works/210102-the-unattainable-pinnacle-of-art-why-does-midjourney-artificial-intelligence-draw-6-fingers-on-hands-and-how/?ref=jina-ai-gmbh.ghost.io">The unattainable pinnacle of art: why does Midjourney artificial intelligence draw 6 fingers on hands and how can it be fixed?</a></p><p>In short, hallucination is AI models generating false information.</p><p>SceneXplain often fell victim to this in our prior algorithms, hallucinating skateboarders, and occasionally Naruto:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--2-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="640" height="380" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--2-.png 640w"><figcaption><span style="white-space: pre-wrap;">SceneXplain&#x2019;s Aqua algorithm: In a whimsical and enchanting scene, Totoro and a young girl share an umbrella in the rain. The iconic character from Studio Ghibli&apos;s beloved film stands alongside the girl, offering shelter from the downpour. The atmosphere is both heartwarming and magical, as Totoro&apos;s presence infuses the moment with a sense of wonder and delight. Meanwhile, </span><b><strong style="white-space: pre-wrap;">Naruto makes a surprising appearance in the rain-soaked setting</strong></b><span style="white-space: pre-wrap;">, adding another layer of intrigue to this captivating tableau. This charming depiction brings together two distinct worlds, bridging the gap between Japanese animation and manga to create an unforgettable moment of friendship and adventure.</span></figcaption></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--3-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="446" height="612"></figure><h2 id="what-causes-hallucination-in-computer-vision">What causes hallucination in computer vision?</h2><p>In humans, hallucination is caused by errors in perception, faulty expectations, neurological defects, or maybe just a dose of Scarecrow&#x2019;s fear gas. In computer vision, it can often come about through issues with training data or the way an image is processed before generation or (in the case of SceneXplain) captioning.</p><h3 id="slicing-and-dicing-the-elephant-in-the-room">Slicing and dicing: the elephant in the room</h3><p>When it comes to image captioning, you often want a high level of detail. A common approach is to slice an image into several pieces, generate a caption for each piece, then merge the captions together into a final description. Sounds like a pretty sensible approach, right? By focusing on each segment and then combining results, we should get a better description overall.</p><p>Except...you ever heard the tale of the blind men and the elephant?</p><blockquote>Six blind men encounter an elephant for the first time and each touches a different part of the animal. One feels the trunk and thinks it&apos;s a snake, another touches a leg and believes it&apos;s a tree, and so on with the tusk (a spear), ear (a mat), tail (a rope), and side (a wall). Amidst much debate, they all insist they&apos;re correct, not realizing they&apos;re all describing the same elephant.</blockquote><p>If we want to understand an image by slicing it into different parts we can face exactly the same issue. If we were to draw what the blind men experience, we&apos;d create a picture of a snake wrapped in a blanket next to a tree, along with a bunch of other elements. In short, many <em>elements</em>, but no <em>elephant</em>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-2.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="1170" height="656" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-2.png 1170w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Midjourney&apos;s interpretation of the blind men&apos;s elephant</span></figcaption></figure><p>Likewise, in captioning, let&apos;s say we split up one image of three men into several segments. In each segment, there may be some part (arm, leg, side of head) of a different man. When it comes time to combine those segment descriptions (<code>a man&apos;s face</code>, <code>a man&apos;s leg</code>, <code>a man&apos;s arm</code>, etc), how many men would be seen? Are these all part of one man? Are they each a part of different men? If so, how many? And without the context of the rest of the image, just what is that patch of fuzz in the top-left section?</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/man-grid.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="600" height="853" srcset="https://jina-ai-gmbh.ghost.io/content/images/2023/10/man-grid.png 600w"><figcaption><span style="white-space: pre-wrap;">Adapted from Pexels.com (</span><a href="https://www.pexels.com/photo/man-in-white-dress-shirt-holding-suit-jacket-1043474/?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">https://www.pexels.com/photo/man-in-white-dress-shirt-holding-suit-jacket-1043474/</span></a><span style="white-space: pre-wrap;">)</span></figcaption></figure><p>If the model splits everything up too much, it can hallucinate things that just aren&apos;t there. If it doesn&apos;t split things, the overall description may be too vague to be useful.</p><h3 id="ambiguity-thats-a-cutepuppy">Ambiguity: That&apos;s a cute...puppy?</h3><p>Many models rely on multimodality - that is translating text-to-image (like using a prompt to generate an image in Midjourney) or image-to-text (like generating a caption from an image in SceneXplain). Ambiguous text or images can therefore cause problems.</p><p>Take the phrase &quot;salmon swimming in a river.&quot; Most humans would imagine fish swimming up a river. AI on the other hand can see some ambiguity:</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://www.reddit.com/r/technicallythetruth/comments/y26t6x/ai_asked_to_draw_salmon_swimming_in_river/?ref=jina-ai-gmbh.ghost.io"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-1.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="750" height="426" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-1.png 750w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Via Reddit</span></figcaption></figure><p>Likewise, sometimes images can be ambiguous:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--7-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--7-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Untitled--7-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Untitled--7-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--7-.png 2048w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Duck or rabbit?</span></figcaption></figure><p>This isn&apos;t just applicable to traditional optical illusions or Rorschach tests. A <a href="https://www.insider.com/viral-puppy-looks-like-mix-between-dog-and-cat-2020-3?ref=jina-ai-gmbh.ghost.io">cute little fuzzball</a> could be a puppy OR a kitten. Running it by the model just once may result in a misclassification. Several tries may be needed to consistently see that puppy as a puppy.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--8-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="700" height="525" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--8-.png 700w"><figcaption><span style="white-space: pre-wrap;">Dog or cat? (Via Business Insider)</span></figcaption></figure><p>And let&#x2019;s not even get started on <a href="https://en.m.wikipedia.org/wiki/The_dress?ref=jina-ai-gmbh.ghost.io">that damn dress</a>.</p><h3 id="ocr-occasionally-crappy-recognition">OCR: Occasionally Crappy Recognition</h3><p>If you&apos;ve got this far in the article, you probably think you&apos;re a pretty hot reader. Go on, give yourself a big pat on the back. You should really share some of your knowledge on <a href="http://expertsexchange.com/?ref=jina-ai-gmbh.ghost.io">expertsexchange.com</a>. It&apos;s a site I use every day, sometimes several times. To exchange information with other experts.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">If you don&apos;t see why I&apos;m bringing this up, re-read that URL</div></div><p>It&apos;s not just a lack of clear word spacing that can trip you up. Take keming (uh, kerning) for example.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"> In typography, kerning is the process of adjusting the spacing between characters in a proportional font, usually to achieve a visually pleasing result.</div></div><p>Bad kerning can have interesting results:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.mos.cms.futurecdn.net/9cdc770ee6d32d569f4dfa1d7e44029e-1200-80.jpg.webp" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy"><figcaption><span style="white-space: pre-wrap;">10 flickering lights</span></figcaption></figure><p>If these things are ambiguous to readers like you, just imagine how confused an AI model can get!</p><h2 id="how-does-scenexplain-fix-hallucination">How does SceneXplain fix hallucination?</h2><p>SceneXplain&#x2019;s new Jelly algorithm aims to eliminate these hallucinations, using the following methods:</p><ol><li>A simplified processing pipeline. Our prior algorithms had much more complex pipelines, meaning more places for things to go wrong. Jelly pares this back with a new end-to-end approach to image labeling that produces detailed image captions with much fewer hallucinations.</li><li>Self-consistency: We&#x2019;ve implemented an advanced prompt technique (self-consistency) to get the most consistent and accurate caption for any given image.</li><li>OCR improvements: We&apos;ve improved Jelly&#x2019;s text recognition capabilities which leads to fewer hallucinations and better captions.</li></ol><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Hallucination is seeing things that don&#x2019;t exist. Just like people &#x201C;see&#x201D; ghosts. Since Jelly removes hallucinations you could call it the ultimate &#x201C;Ghostbuster&#x201D;. Cue the theme music.</div></div><h2 id="before-and-after">Before and After</h2><p>As you can see from the examples below:</p><ul><li>Glide suffers from way more hallucinations (highlighted in bold) than Jelly</li><li>Jelly provides more readable captions, lacking the elaborate flowery language of Glide</li></ul><h3 id="les-choristes">Les Choristes</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--4-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="764" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--4-.png 764w" sizes="(min-width: 720px) 720px"></figure><p><strong>Glide:</strong></p><blockquote>The image showcases a poster for a movie called &quot;Les Choristes.&quot; The poster features the title &quot;LES CHORISTES&quot; prominently displayed in the center of the image. The names of the actors and director, including &quot;GERARD JUGNOT,&quot; &quot;FRAN&#xC7;OIS BERLEAND,&quot; &quot;KAD,&quot; and &quot;CHRISTOPHE BARRATIER,&quot; are listed below the title.</blockquote><blockquote>The poster shows a group of people posing for a picture in various settings. On the left side of the image, a group of people in tuxedos can be seen standing in front of a tree. In the middle of the image, a group of children sits on a bench with a chalkboard. To the right of the children, <strong>another group of children is sitting in a classroom</strong>. They are separated by a group of boys who are sitting on a bench. Further to the right, a group of people is posing for another picture. <strong>Above all these scenes, there is a group of children sitting on a stage</strong>.</blockquote><blockquote><strong>The color scheme of the poster is predominantly white</strong>, with the title and texts in black. The actors&apos; names are in smaller font compared to the title. The overall appearance suggests a heartwarming and collaborative movie involving children and adults in different settings, possibly related to music or education.</blockquote><blockquote>Additional details, such as the ratings &quot;9.8&quot; and &quot;27.7.761,&quot; are included at the bottom right corner of the poster. The meaning of these numbers is unclear without further context.</blockquote><p><strong>Jelly:</strong></p><blockquote>The image is a movie poster for the French film &quot;Les Choristes&quot;, directed by Christophe Barratier. The movie&apos;s title is printed in large letters under an archway. Beneath this are young boys and men, some sitting on a bench, and others standing. They are dressed in classic school-like clothing and are positioned at different heights, perhaps indicating different ages. In the lower corners of the poster, the names of actors G&#xE9;rard Jugnot, Fran&#xE7;ois Berl&#xE9;and, and Kad are printed. The poster has a vintage feel, possibly due to its color scheme and the classic clothing of the characters.</blockquote><h3 id="liberty-leading-the-people">Liberty leading the people</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--5-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="464" height="367"></figure><p><strong>Glide:</strong></p><blockquote>The artwork depicts a scene with a strong patriotic theme. The central focus of the painting showcases a woman holding the French flag, her figure dominating the composition. Her proud stance and the vibrant colors of the flag capture the viewer&apos;s attention. Surrounding her, several other characters are depicted, each with their own distinct role. A man is shown holding a rifle, signifying the presence of armed conflict. In another section, <strong>a group of men is seen together on a boat</strong>, suggesting a collective effort or journey. Additionally, <strong>a man on a separate boat is featured</strong>, adding further depth to the narrative. <strong>A woman with a knife and a man with a gun are shown</strong>, reflecting a sense of danger and potential violence. The artwork overall conveys a sense of patriotism, conflict, and tension through its composition and subject matter.</blockquote><p><strong>Jelly:</strong></p><blockquote>This image is a painting titled &quot;Liberty Leading the People&quot;, created in 1830 by the French artist Eug&#xE8;ne Delacroix. The painting depicts a woman symbolizing France and the revolution, standing on the bodies of prostrate figures signifying defeated enemies. She is dressed in a <strong>blue</strong> dress, holding a French flag in her left hand, symbolizing the French republic and democracy. In her right hand, she is likely holding a weapon, possibly a sword. The painting represents a moment from the French Revolution, showcasing the spirit of the people resisting tyranny.</blockquote><h3 id="emojis">Emojis</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--6-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="1024" height="537" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Untitled--6-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--6-.png 1024w" sizes="(min-width: 720px) 720px"></figure><p><strong>Glide:</strong></p><blockquote>A playful scene unfolds on a vibrant green grassy expanse, as a group of colorful emoji balls gathers together. These enchanting characters fill the frame, showcasing a wide array of expressive emotions and witty designs. In the center, a striking <strong>red and yellow ball</strong> stands out with its angry face emblazoned upon its surface. Positioned nearby, lying comfortably on the grass, two yellow emoji balls exude a relaxed demeanor. To the right, a buoyant <strong>blue and yellow ball</strong> features a cheerful smiley face that adds a touch of delight and cheerfulness to the mix. On the left, <strong>two</strong> yellow balls adorned with sunglasses mirror each other&apos;s fashionable style while resting peacefully in the grass. Amidst the lively atmosphere, a <strong>yellow and blue plush toy makes its presence known with a sad expression</strong>, attracting sympathy and affection. Additionally, a yellow and red emoji doll captivates attention with a heartfelt kiss imprinted upon its face. Further accentuating this whimsical scene, a lone yellow face with big expressive eyes captures the viewer&apos;s gaze, radiating a sense of curiosity and intrigue. Lastly, <strong>a yellow teddy bear</strong> wearing sunglasses and a friendly smile completes the array of delightful characters, inviting joy and warmth into the playful world they inhabit.</blockquote><p><strong>Jelly:</strong></p><blockquote>The image shows a group of six stress balls with different emoji faces on them, situated on a green grassy surface. Each stress ball displays a unique emoji expression and they come in varied colors including yellow, red, and blue. The background of the image is blurred, ensuring the focus is on these stress balls.</blockquote><p>Want to see more examples of how SceneXplain algorithms stack up? Check them out here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/benchmark?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"></div></a></figure><h2 id="get-your-jelly-on">Get your Jelly on</h2><p>As you can see, Jelly gives you captions that are more concise, readable, and <strong>accurate</strong> than ever before. Wave goodbye to hallucinations and say hello to detailed, precise descriptions of your images.</p><p>To get started with Jelly (or any other SceneXplain algorithm), sign up for a free account at <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">scenex.jina.ai</a> and start captioning your images!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.3 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-3-update/</link><guid isPermaLink="false">65390517dcdd090001c7d210</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Wed, 25 Oct 2023 12:10:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-2.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3223">Release Note (<code>3.22.3</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-2.jpg" alt="Jina 3.22.3 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.3?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.3 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.3) Release time: 2023-10-25 12:03:58 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Add and use POST endpoint for streaming (#6093)
HTTP streaming when using a GET endpoint only&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.22.3 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/bbf622e38936bd97688a794004636257a997cf3b42c4e080c2308a580498fffe/jina-ai/jina/releases/tag/v3.22.3" alt="Jina 3.22.3 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="add-and-use-post-endpoint-for-streaming-6093">Add and use POST endpoint for streaming (<a href="https://github.com/jina-ai/jina/pull/6093?ref=jina-ai-gmbh.ghost.io">#6093</a>)</h3><p>HTTP streaming when using a&#xA0;<code>GET</code>&#xA0;endpoint only allows Documents whose schemas contain only&#xA0;<code>str</code>,&#xA0;<code>int</code>, and&#xA0;<code>float</code>&#xA0;fields.</p><p>By adding a&#xA0;<code>POST</code>&#xA0;endpoint and modifying the Jina Client to use it, it is now possible to send Documents with complex nested document schemas over streaming HTTP.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Peter Willemsen (<a href="https://github.com/peterwilli?ref=jina-ai-gmbh.ghost.io">@peterwilli</a>&#xA0;)</li><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Jina AI Launches World's First Open-Source 8K Text Embedding, Rivaling OpenAI]]></title><description><![CDATA[Jina AI introduces jina-embeddings-v2, the world's first open-source model boasting an 8K context length. Matching the prowess of OpenAI's proprietary models, this innovation is now publicly accessible on Huggingface, signaling a significant milestone in the landscape of text embeddings.]]></description><link>https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/</link><guid isPermaLink="false">6538999ddcdd090001c7d17f</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 25 Oct 2023 05:43:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"><p><strong>Berlin, Germany - October 25, 2023</strong> &#x2013; Jina AI, the Berlin-based artificial intelligence company, is thrilled to announce the launch of its second-generation text embedding model: <strong><code>jina-embeddings-v2</code></strong>. This cutting-edge model is now the only open-source offering that supports an impressive 8K (8192 tokens) context length, putting it on par with OpenAI&apos;s proprietary model, <code>text-embedding-ada-002</code>, in terms of both capabilities and performance <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">on the Massive Text Embedding Benchmark (MTEB) leaderboard.</a></p><h2 id="benchmarking-against-the-best-8k-model-from-open-ai"><strong>Benchmarking Against the Best 8K Model from Open AI</strong></h2><p>When directly compared with OpenAI&apos;s 8K model <code>text-embedding-ada-002</code>, <code>jina-embeddings-v2</code> showcases its mettle. Below is a performance comparison table, highlighting areas where <code>jina-embeddings-v2</code> particularly excels:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Model Size (GB)</th>
<th>Embedding Dimensions</th>
<th>Sequence Length</th>
<th>Average (56 datasets)</th>
<th>Classification Average (12 datasets)</th>
<th>Reranking Average (4 datasets)</th>
<th>Retrieval Average (15 datasets)</th>
<th>Summarization Average (1 dataset)</th>
</tr>
</thead>
<tbody>
<tr>
<td>15</td>
<td>text-embedding-ada-002</td>
<td>Unknown</td>
<td>1536</td>
<td>8191</td>
<td>60.99</td>
<td>70.93</td>
<td>84.89</td>
<td>56.32</td>
<td>30.8</td>
</tr>
<tr>
<td>17</td>
<td>jina-embeddings-v2-base-en</td>
<td>0.27</td>
<td>768</td>
<td>8192</td>
<td>60.38</td>
<td>73.45</td>
<td>85.38</td>
<td>56.98</td>
<td>31.6</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Notably, <code>jina-embedding-v2</code> outperforms its OpenAI counterpart in Classification Average, Reranking Average, Retrieval Average, and Summarization Average.</p><h2 id="features-and-benefits"><strong>Features and Benefits</strong></h2><p>Jina AI&#x2019;s dedication to innovation is evident in this latest offering:</p><ul><li><strong>From Scratch to Superiority</strong>: The <code>jina-embeddings-v2</code> was built from the ground up. Over<strong> </strong>the last three months, the team at Jina AI engaged in intensive R&amp;D, data collection, and tuning. The outcome is a model that marks a significant leap from its predecessor.</li><li><strong>Unlocking Extended Context Potential with 8K</strong>: <code>jina-embeddings-v2</code> isn&#x2019;t just a technical feat; its 8K context length opens doors to new industry applications:<ul><li><strong>Legal Document Analysis</strong>: Ensure every detail in extensive legal texts is captured and analyzed.</li><li><strong>Medical Research</strong>: Embed scientific papers holistically for advanced analytics and discovery.</li><li><strong>Literary Analysis</strong>: Dive deep into long-form content, capturing nuanced thematic elements.</li><li><strong>Financial Forecasting</strong>: Attain superior insights from detailed financial reports.</li><li><strong>Conversational AI</strong>: Improve chatbot responses to intricate user queries.</li></ul></li></ul><p>Benchmarking shows that in several datasets, this extended context enabled <code>jina-embeddings-v2</code> to outperform other leading base embedding models, emphasizing the practical advantages of longer context capabilities.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png" class="kg-image" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI" loading="lazy" width="2000" height="647" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png 2359w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Availability</strong>: Both models are freely available for download on Huggingface:<ul><li><strong>Base Model</strong> (0.27G) - Designed for heavy-duty tasks requiring higher accuracy, like academic research or business analytics.</li><li><strong>Small Model</strong> (0.07G) - Crafted for lightweight applications such as mobile apps or devices with limited computing resources.</li></ul></li><li><strong>Size Options for Different Needs</strong>: Understanding the diverse needs of the AI community, Jina AI offers two versions of the model:</li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-en.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-small-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-small-en.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></a></figure><p>In reflecting on the journey and significance of this launch, Dr. Han Xiao, CEO of Jina AI, shared his thoughts:</p><blockquote><em>&quot;In the ever-evolving world of AI, staying ahead and ensuring open access to breakthroughs is paramount. With <code>jina-embeddings-v2</code>, we&apos;ve achieved a significant milestone. Not only have we developed <strong>the world&apos;s first open-source 8K context length model</strong>, but we have also brought it to a performance level on par with industry giants like OpenAI. Our mission at Jina AI is clear: we aim to democratize AI and empower the community with tools that were once confined to proprietary ecosystems. Today, I am proud to say, we have taken a giant leap towards that vision.&quot;</em></blockquote><p>This pioneering spirit is evident in Jina AI&apos;s forward-looking plans.</p><h2 id="a-glimpse-into-the-future"><strong>A Glimpse into the Future</strong></h2><p>Jina AI is committed to leading the forefront of innovation in AI. Here&#x2019;s what&#x2019;s next on their roadmap:</p><ul><li><strong>Academic Insights</strong>: An academic paper detailing the technical intricacies and benchmarks of <code>jina-embeddings-v2</code> will soon be published, allowing the AI community to gain deeper insights.</li><li><strong>API Development</strong>: The team is in the advanced stages of developing an OpenAI-like embeddings API platform. This will provide users with the capability to effortlessly scale the embedding model according to their needs.</li><li><strong>Language Expansion</strong>: Venturing into multilingual embeddings, Jina AI is setting its sights on launching German-English models, further expanding its repertoire.</li></ul><hr><p>About Jina AI GmbH:<br>Located at Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at <a>contact@jina.ai</a>.</p>]]></content:encoded></item><item><title><![CDATA[DocArray 0.39.1 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-39-1-update/</link><guid isPermaLink="false">653639aa6930100001517b22</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 23 Oct 2023 09:19:10 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0391">Release Note (<code>0.39.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak-1.jpg" alt="DocArray 0.39.1 Update"><p>This release contains 2 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.39.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v0.39.1 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.39.1) Release time: 2023-10-23 08:56:38 This release contains 2 bug fixes.
&#x1F41E; Bug Fixes
From_dataframe with numpy==1.26.1 (#1823)
A recent update to numpy has changed some of the v&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="DocArray 0.39.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/2cc6511b791fbb31b8bb33b69f7a6fb2948965f11790323434871815a729578d/docarray/docarray/releases/tag/v0.39.1" alt="DocArray 0.39.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fromdataframe-with-numpy1261-1823">From_dataframe with numpy==1.26.1 (<a href="https://github.com/docarray/docarray/pull/1823?ref=jina-ai-gmbh.ghost.io">#1823</a>)</h3><p>A recent update to numpy has changed some of the versioning semantics, breaking DocArray&apos;s&#xA0;<code>from_dataframe()</code>&#xA0;method in some cases where the dataframe contains a numpy array. This has now been now fixed.</p><pre><code class="language-Python">class MyDoc(BaseDoc):
    embedding: NdArray
    text: str

da = DocVec[MyDoc](
    [
        MyDoc(
            embedding=[1, 2, 3, 4],
            text=&apos;hello&apos;,
        ),
        MyDoc(
            embedding=[5, 6, 7, 8],
            text=&apos;world&apos;,
        ),
    ],
    tensor_type=NdArray,
)
df_da = da.to_dataframe()
# This broke before and is now fixed
da2 = DocVec[MyDoc].from_dataframe(df_da, tensor_type=NdArray)</code></pre><h3 id="type-handling-in-python-39-1823">Type handling in python 3.9 (<a href="https://github.com/docarray/docarray/pull/1823?ref=jina-ai-gmbh.ghost.io">#1823</a>)</h3><p>Starting with Python 3.9,&#xA0;<code>Optional.__args__</code>&#xA0;is not always available, leading to some compatibility problems. This has been fixed by using the&#xA0;<code>typing.get_args</code>&#xA0;helper.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Johannes Messner (<a href="https://github.com/JohannesMessner?ref=jina-ai-gmbh.ghost.io">@JohannesMessner</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.2 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-2-update/</link><guid isPermaLink="false">653298e658a10800011a4c32</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 20 Oct 2023 15:17:10 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3222">Release Note (<code>3.22.2</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-1.jpg" alt="Jina 3.22.2 Update"><p>This release contains 1 bug fix and 5 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.2?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.2 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.2) Release time: 2023-10-20 14:42:15 This release contains 1 bug fix and 5 documentation improvements.
&#x1F41E; Bug Fixes
Fix check graph compatibility nested types (#6085)
A bug was&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.22.2 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/4b64ab24771105198f1dd2fd80fda882c9f93774e44c7f831d779620eef7ae30/jina-ai/jina/releases/tag/v3.22.2" alt="Jina 3.22.2 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-check-graph-compatibility-nested-types-6085">Fix check graph compatibility nested types (<a href="https://github.com/jina-ai/jina/pull/6085?ref=jina-ai-gmbh.ghost.io">#6085</a>)</h3><p>A bug was found that prevented Flows from starting when they had Executors with different but compatible schemas. This is now fixed.</p><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Add Jina deployment docs (<a href="https://github.com/jina-ai/jina/pull/6079?ref=jina-ai-gmbh.ghost.io"><u>#6079</u></a>)</li><li>Fix typos inside docs/cloud-nativeness (<a href="https://github.com/jina-ai/jina/pull/6080?ref=jina-ai-gmbh.ghost.io"><u>#6080</u></a>)</li><li>Fix typos inside docs/concepts directory (<a href="https://github.com/jina-ai/jina/pull/6082?ref=jina-ai-gmbh.ghost.io"><u>#6082</u></a>)</li><li>Fix typo inside docs/tutorials (<a href="https://github.com/jina-ai/jina/pull/6083?ref=jina-ai-gmbh.ghost.io"><u>#6083</u></a>)</li><li>Fix readme code (<a href="https://github.com/jina-ai/jina/pull/6075?ref=jina-ai-gmbh.ghost.io"><u>#6075</u></a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Deepankar Mahapatro (<a href="https://github.com/deepankarm?ref=jina-ai-gmbh.ghost.io">@deepankarm</a>&#xA0;)</li><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>&#xA0;)</li><li>Parikshit Adhikari (<a href="https://github.com/parikshitadhikari?ref=jina-ai-gmbh.ghost.io">@parikshitadhikari</a>&#xA0;)</li><li>Subba Reddy Veeramreddy (<a href="https://github.com/subbuv26?ref=jina-ai-gmbh.ghost.io">@subbuv26</a>&#xA0;)</li><li>&#x410;&#x43D;&#x442;&#x43E;&#x43D; (<a href="https://github.com/BouFFaNTCRiB?ref=jina-ai-gmbh.ghost.io">@BouFFaNTCRiB</a>&#xA0;)</li><li>Abhijit (<a href="https://github.com/AbhiGaunker?ref=jina-ai-gmbh.ghost.io">@AbhiGaunker</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[How Embeddings Drive AI: A Comprehensive Presentation]]></title><description><![CDATA[Embeddings are a core concept and technology in AI, present in some form across the spectrum of AI applications. Properly understanding and mastering this technology positions you to leverage AI models to add the most value to your business.]]></description><link>https://jina.ai/news/how-embeddings-drive-ai-a-guide/</link><guid isPermaLink="false">652fe2c6bd6ec2000125ee5c</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Thu, 19 Oct 2023 13:00:24 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Blog-images--5-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Blog-images--5-.jpg" alt="How Embeddings Drive AI: A Comprehensive Presentation"><p>Embeddings are a core concept and technology in artificial intelligence, used in some form in practically all recent AI models, including ChatGPT and image generators like Stable Diffusion. It is one of the fundamental technologies behind the most visible and useful AI applications.</p><p>Embeddings are also important to less flashy applications, like search, visual face and object recognition, machine translation, recommenders, and many more.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">What is an embedding?</strong></b><br><br>An embedding is a <i><em class="italic" style="white-space: pre-wrap;">vector </em></i>that stands in for some other object (digital, physical, or abstract) such that some of the properties of the object it stands for are preserved as geometric properties of their corresponding vectors.</div></div><p>This post tries to explain what that means concretely.</p><h2 id="vectors">Vectors</h2><p>A vector is just a list of numbers that we can interpret as coordinates in a high-dimensional space.</p><p>Imagine a list of two numbers $(x,y)$. We could treat them as coordinates on an x-y graph, with each list corresponding to a point in a two-dimensional space:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/vec1-1-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="997" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/vec1-1-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/vec1-1-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>If we have three numbers $(x,y,z)$, then this corresponds to a point in a three-dimensional space:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/vec2-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="1162" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/vec2-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/vec2-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>We can readily extend this logic into more dimensions: four, five, a hundred, a thousand, even millions or billions. Drawing a space with a thousand dimensions is very hard, and imagining one is all but impossible, but mathematically, it&#x2019;s trivial to manage.</p><h2 id="embeddings-as-vectors">Embeddings as Vectors</h2><p>For example, let&#x2019;s say we had a way to assign a small vector &#x2014; just two numbers long &#x2014; to pictures of apples and oranges.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/apple_and_orange.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="900" height="448" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/apple_and_orange.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/apple_and_orange.png 900w" sizes="(min-width: 720px) 720px"></figure><p>We can display vectors of just two numbers as points on an XY graph, so imagine we took pictures of apples and oranges and assigned each a random vector, then plotted them on a graph:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/graph1-2-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="516" height="389"></figure><p>The apples and oranges are all mixed together, and nothing about how they&#x2019;re placed on the graph tells us whether a picture is of an apple or an orange.</p><p>What we&#x2019;d like to have is something like this:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/graph2-1-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="515" height="389"></figure><p>If we had the means to assign vectors like these to pictures of apples and oranges, it would be trivial to write a program to tell them apart. All it would have to do is check which side of a line the vector fell on.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/graph3-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="515" height="389"></figure><p>AI and neural networks provide a way to do exactly that.</p><p>Using machine learning, we can construct an AI model that takes images and then outputs vectors such that the oranges and apples are easily separated. These vectors typically have a lot more than two dimensions, so they aren&#x2019;t easy to draw.</p><p>The property of the original object that we care about &#x2014; whether it is an apple or an orange &#x2014; is preserved in the geometry of the embedding vectors: The apples are all to one side and the oranges to the other, easily separated by what in high-dimensional geometry is called a <em>hyper-plane</em>, but is really just a simple way of splitting the embedding space.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/flow-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="820" height="300" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/flow-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/flow-1.png 820w" sizes="(min-width: 720px) 720px"></figure><p>This example is very simple and concerns only one distinction: apples or oranges. Embeddings can capture a lot of information about many different aspects of the objects they represent. If we trained an AI model to distinguish apples from oranges, we would likely find that the embeddings encoded many of the characteristics that distinguish apples from oranges, like color and texture and the presence of a stem. We might additionally find that the green apples are clustered together, farther from the red ones, or that highly mottled apples had embeddings closer to the oranges.</p><h2 id="word-embeddings">Word Embeddings</h2><p>Embeddings emerge almost automatically from neural network-based AI models, as we will show in the next section, but they were first developed as a way to better encapsulate the semantics (i.e. meanings) of words for computers. You will still sometimes see people talk about &quot;semantic embeddings&quot; because of the role word embeddings have played in this technology, even though embeddings aren&#x2019;t specifically about some kind of real-world meaning but about any useful property that an AI model can learn to recognize.</p><p>In natural language processing, embeddings have always been associated with an idea in linguistics called the <strong>distributional hypothesis</strong>, most frequently associated with the linguist J.R. Firth:</p><blockquote>You shall know a word by the company it keeps.<br><br>A Synopsis of Linguistic Theory, 1930-55.<br>(In Studies in Linguistic Analysis, 1957, pp. 1-31) </blockquote><p>The distributional hypothesis is the idea that the semantics &#x2014; that is to say, the meaning &#x2014; of a word is reflected in the contexts of its usage. We should expect that the more related the meanings of two words, the more we should see them in the same contexts.</p><p>For example, when two words are spelled or pronounced the same, we use context to figure out which word is intended:</p><ul><li>She took her <strong>bat</strong> to baseball practice.</li><li>She was chased away from the cave by a <strong>bat</strong>.</li></ul><p>It is clear from the surrounding context which meaning of the word <strong>bat</strong> is intended.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/bat_girl.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1456" height="816" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/bat_girl.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/bat_girl.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/bat_girl.png 1456w" sizes="(min-width: 720px) 720px"></figure><p>Similarly, we can often expect that words from the same semantic class, like <strong>tomato</strong>, <strong>carrot</strong>, and <strong>zucchini</strong>, appear in the same contexts:</p><ul><li>He grew <strong>tomatoes</strong> in his garden.</li><li>He grew <strong>carrots</strong> in his garden.</li><li>He grew <strong>zucchini</strong> in his garden.</li></ul><p>But not:</p><ul><li>He grew <strong>bicycles</strong> in his garden.</li></ul><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/bicycle_plant.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1024" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/bicycle_plant.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/bicycle_plant.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/bicycle_plant.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>Words that can fill the blank in &#x201C;He grew _____ in his garden&#x201D; share a property that they do not share with bicycles, and we can build an AI model that encompasses this information, along with thousands of other contexts where some words can go and others can&#x2019;t.</p><p>Word embeddings are a way to encapsulate this information in a vector space.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/graph4-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="515" height="389"></figure><h2 id="image-recognition">Image Recognition</h2><p>We can even construct embeddings where we don&#x2019;t explicitly say what features are relevant.</p><p>For example, an embedding for a face recognition system translates pictures of people into embedding vectors distributed in a high-dimensional space.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/faces_A-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="999" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/faces_A-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/faces_A-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>We might find that people whose faces have some common properties would be grouped together in some part of the embedding space. We might find, for example, that grey-haired people and bald people cluster together:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/faces_marked-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="999" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/faces_marked-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/faces_marked-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>But more importantly and usefully, we would expect that if we gave it two pictures of the same person, those two embeddings would be closer to each other than to any other person&#x2019;s picture:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Wynona-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="728" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Wynona-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Wynona-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><p>Of course, it would also find people who just&#xA0;look&#xA0;similar, although we hope that they wouldn&#x2019;t be as close together as two people who are actually the same:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Hobbit_dude-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="728" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Hobbit_dude-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Hobbit_dude-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><h2 id="multimodal-embeddings">Multimodal Embeddings</h2><p>Embedding spaces can also act as a bridge between different kinds of inputs.</p><p>For example, if we have a database of pictures with descriptive captions, we can co-train two embedding models &#x2014; one for images and one for texts &#x2014; to output vectors in the same embedding space. The result is that embeddings of descriptions of images and embeddings of the images themselves will be near each other.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/fruit-1.png" class="kg-image" alt="How Embeddings Drive AI: A Comprehensive Presentation" loading="lazy" width="1000" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/fruit-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/fruit-1.png 1000w" sizes="(min-width: 720px) 720px"></figure><h2 id="embeddings-everywhere">Embeddings Everywhere</h2><p>Embeddings provide a very generalized framework for making useful comparisons between things. They are so useful that they see applications all over AI and machine learning. Any application that requires similarity/dissimilarity evaluation, relies on hidden or non-obvious features, or requires implicit context-sensitive mappings between different inputs and outputs, likely uses embeddings in some form.</p><p>They are actively in use for:</p><ul><li>Search and information retrieval for all media types</li><li>Face and object recognition in machine vision</li><li>Question-answering systems</li><li>Recommender systems</li><li>Outlier detection (often as part of fraud detection)</li><li>Spellchecking and grammar correction</li><li>Natural language understanding</li><li>Machine translation</li></ul><p>This is a far from exhaustive list because nearly every AI application uses embeddings. In some scenarios, embeddings are used internally and are invisible to users and developers, but many AI applications hinge on the ability to use embeddings to identify and compare things in useful, human-like ways.</p><p>Jina AI is developing a collection of high-performance specialized embedding models that you can download or use via our public API, without any of the complexities of AI development. We are also preparing a suite of open-source tools in Python to help you integrate embedding models into your tech stack.</p><p>Jina AI is committed to providing you with tools and help for creating, optimizing, evaluating, and implementing embedding models for your enterprise. We&#x2019;re here to help you navigate the new world of business AI. Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to get started.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="How Embeddings Drive AI: A Comprehensive Presentation"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="How Embeddings Drive AI: A Comprehensive Presentation"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3155 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/ec2c34cadd4b5f4594415127380a85e6.ico" alt="How Embeddings Drive AI: A Comprehensive Presentation"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="How Embeddings Drive AI: A Comprehensive Presentation"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Unveiling the Magic: Become a Part of PromptPerfect's Affiliate Family]]></title><description><![CDATA[Introducing PromptPerfect's Affiliate Program, an initiative for enthusiasts to bring attention to Jina AI's innovative technologies and show our appreciation to the community that makes PromptPerfect great!]]></description><link>https://jina.ai/news/unveiling-the-magic-become-a-part-of-promptperfects-affiliate-family/</link><guid isPermaLink="false">652e4e73bfcbdb000148afe6</guid><category><![CDATA[Knowledge Base]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Wed, 18 Oct 2023 13:45:05 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"><p>Great news for PromptPerfect enthusiasts: We want to show you our appreciation for helping to build our community! </p><p>We&apos;re introducing the <strong>PromptPerfect Affiliate Program</strong>, an initiative inviting enthusiasts to bring attention to Jina AI&apos;s innovative technologies and show our appreciation to the community that makes PromptPerfect great! Become one of our affiliates and enjoy rewards for helping show the world what PromptPerfect can do.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/affiliate?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficien&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"></div></a><figcaption><p><span style="white-space: pre-wrap;">Learn More About PromptPerfect Affiliate Program</span></p></figcaption></figure><h2 id="key-benefits">Key Benefits:</h2><ul><li><strong>A Rewarding Start:</strong> From the get-go, affiliates receive <strong>a 15% commission on sales</strong> during their first month. It&apos;s a hearty welcome to the family, ensuring you feel valued from your very first contribution. And we are more than happy to increase your commission after the first month.</li><li><strong>Sustained Gains:</strong> The perks don&apos;t just stop after the first month. For every ensuing sale, a consistent 15% commission is credited to you. It&apos;s the gift that keeps on giving, rewarding your efforts and amplifying your impact.</li><li><strong>Simple, Monthly Payouts:</strong> To keep things simple, earnings are calculated and sent out monthly. You can focus on what you do best, leaving the complexities of transactions to us.</li><li><strong>Your Success is Important to Us: </strong>We&apos;re committed to supporting your success. You&apos;ll have access to valuable resources to generate as many referrals as possible. These resources are designed to help you develop content and attract more new merchants. Plus, you&apos;ll get access to a variety of creative, pre-developed lead magnets and detailed performance metrics.</li></ul><h2 id="how-to-get-started"><strong>How to Get Started: </strong></h2><p>Embarking on this journey is as simple as 1-2-3:</p><p><strong>Apply Easily:</strong> Start by going to our <a href="https://promptperfect.jina.ai/affiliate?ref=jina-ai-gmbh.ghost.io">website</a> and filling out our sign-up form. There are no hoops to jump through, just some basic information.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/affiliate?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficien&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"></div></a></figure><p><strong>Quick Follow-up:</strong> Your time is precious, and so we commit to responding within 5 business days. You&apos;ll be fully briefed on the next steps without any unnecessary wait.</p><p><strong>Get guidance from the team:</strong> Once your application has been reviewed, our team will reach out to you to arrange a call. During this call, we will walk you through all the necessary steps and provide you with valuable links to promotional materials, so you can start immediately.</p><p>This program is about fostering a community that values innovation and shared success. It&apos;s for those who are excited about shaping the future of technology and want their voices to be heard.</p><p>The PromptPerfect Affiliate Program can make you an integral part of a community that champions forward-thinking and mutual growth. So, if you like sharing with the world the next big thing in tech, this opportunity is for you.</p><p>Ready to take the leap? Dive in, spread the word, and watch the magic happen. </p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/affiliate?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficien&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Unveiling the Magic: Become a Part of PromptPerfect&apos;s Affiliate Family"></div></a><figcaption><p><span style="white-space: pre-wrap;">Learn More About PromptPerfect Affiliate Program</span></p></figcaption></figure><p>The future is being written, and here&apos;s your chance to hold the pen.</p><p>Welcome to the family!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.1 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-1-update/</link><guid isPermaLink="false">65200886701fe20001daea2c</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 06 Oct 2023 14:54:43 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3221"><br>Release Note (<code>3.22.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark.jpg" alt="Jina 3.22.1 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.1 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.1) Release time: 2023-10-06 13:52:45 This release contains 1 bug fix.&#x1F41E; Bug FixesFix timeout issue with dynamic batching (#6071)When using dynamic batching (either with the @&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/pinned-octocat.svg" alt="Jina 3.22.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/f8312759ade3440797e5ebe9a8b57877d5e0cf47ed60c32fad174d1e51ee2f1e/jina-ai/jina/releases/tag/v3.22.1" alt="Jina 3.22.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-timeout-issue-with-dynamic-batching-6071">Fix timeout issue with dynamic batching (<a href="https://github.com/jina-ai/jina/pull/6071?ref=jina-ai-gmbh.ghost.io">#6071</a>)</h3><p>When using dynamic batching (either with the <code>@dynamic_batching</code> decorator on an <code>Executor</code> endpoint or by setting the <code>uses_dynamic_batching</code> parameter on a <code>Deployment</code>, timeouts were not being properly taken into account because the timer might not be triggered until a previous batch finished. This has been fixed.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a> )</li></ul>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.0 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-0-update/</link><guid isPermaLink="false">651d461cd921f000016560c5</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Wed, 04 Oct 2023 12:53:54 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-1.jpeg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3220"><br>Release Note (<code>3.22.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-1.jpeg" alt="Jina 3.22.0 Update"><p>This release contains 1 new feature and 2 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v3.22.0 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.0) Release time: 2023-10-04 12:16:53 This release contains 1 new feature and 2 documentation improvements.&#x1F195; FeaturesChange the way dynamic batching is handled (#6066)This re&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/pinned-octocat.svg" alt="Jina 3.22.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/da0df8f748c732c06c623f112cf1a035f2119229a4e0c4af7a7a4bc13da78b7e/jina-ai/jina/releases/tag/v3.22.0" alt="Jina 3.22.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="change-the-way-dynamic-batching-is-handled-6066">Change the way dynamic batching is handled (<a href="https://github.com/jina-ai/jina/pull/6066?ref=jina-ai-gmbh.ghost.io">#6066</a>)</h3><p>This release implements some changes in the way we handle dynamic batching:</p><ul><li>Jina guarantees that all the batches passed to Executors contain no more than <code>preferred_batch_size</code> documents.</li><li>Requests are returned to the client as they are processed without waiting for other requests to be ready.</li></ul><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Fix dynamic batching documentation (<a href="https://github.com/jina-ai/jina/pull/6068?ref=jina-ai-gmbh.ghost.io">#6068</a>)</li><li>Fix line explaining dynamic batching in YAML (<a href="https://github.com/jina-ai/jina/pull/6065?ref=jina-ai-gmbh.ghost.io">#6065</a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>Johannes Messner (<a href="https://github.com/JohannesMessner?ref=jina-ai-gmbh.ghost.io">@JohannesMessner</a> )</li></ul>]]></content:encoded></item><item><title><![CDATA[DocArray 0.39 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-39-update/</link><guid isPermaLink="false">651ac836330c280001e2c0bd</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 02 Oct 2023 13:43:56 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0390"><br>Release Note (0.39.0)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak.jpg" alt="DocArray 0.39 Update"><p>This release contains 4 new features, 8 bug fixes, and 7 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.39.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v0.39.0 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.39.0) Release time: 2023-10-02 13:06:02 This release contains 4 new features, 8 bug fixes, and 7 documentation improvements.&#x1F195; FeaturesSupport for Pydantic v2 &#x1F680; (#1652)The bigge&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/pinned-octocat.svg" alt="DocArray 0.39 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/6046718f1e879a210b994daf388d030e8249404f7f7d70a9a3675722a1870488/docarray/docarray/releases/tag/v0.39.0" alt="DocArray 0.39 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="support-for-pydantic-v2-%F0%9F%9A%80-1652">Support for Pydantic v2 &#x1F680; (<a href="https://github.com/docarray/docarray/pull/1652?ref=jina-ai-gmbh.ghost.io">#1652</a>)</h3><p>The biggest feature of this release is <strong>full support for Pydantic v2</strong>! We are <strong>continuing to support Pydantic v1</strong> at the same time.</p><p>If you use Pydantic v2, you will need to adapt your DocArray code to the new Pydantic API. Check out their migration guide <a href="https://docs.pydantic.dev/latest/migration/?ref=jina-ai-gmbh.ghost.io" rel="nofollow">here</a>.</p><p>Pydantic v2 has its core written in Rust and provides significant performance improvements to DocArray: <strong>JSON serialization is 240% faster</strong> and <strong>validation</strong> of BaseDoc and DocList with non-native types like <code>TorchTensor</code> is <strong>20% faster</strong>.</p><h3 id="add-basedocwithoutid-1803">Add BaseDocWithoutId (<a href="https://github.com/docarray/docarray/pull/1803?ref=jina-ai-gmbh.ghost.io">#1803</a>)</h3><p>A <code>BaseDoc</code> by default includes an <code>id</code> field. This can be problematic if you want to build an API that requires a model without this ID field. Therefore, <strong>we now provide a <code>BaseDocWithoutId</code> which is, as its name suggests, is BaseDoc without the ID field</strong>.</p><p>Please use this Document with caution, BaseDoc is still the base class to use unless you specifically need to remove the ID.</p><p>&#x26A0;&#xFE0F; <strong><code>BaseDocWithoutId</code> is not compatible with <code>DocIndex</code></strong> or any feature requiring a vector database. This is because DocIndex needs the id field to store and retrieve documents.</p><h2 id="%F0%9F%92%A3-breaking-change">&#x1F4A3; Breaking change</h2><h3 id="remove-jina-ai-cloud-pushpull-1791">Remove Jina AI cloud push/pull (<a href="https://github.com/docarray/docarray/pull/1791?ref=jina-ai-gmbh.ghost.io">#1791</a>)</h3><p>Jina AI Cloud is being discontinued. Therefore, we are removing the <code>push/pull</code> feature related to Jina AI cloud.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-doclist-subscription-error">Fix DocList subscription error</h3><p><code>DocList</code> can be typed from BaseDoc using the following syntax <code>DocList[MyDoc]()</code>.</p><p>In this release, we have fixed a bug that allowed users to specify the type of a <code>DocList</code> multiple times</p><p>Doing <code>DocList[MyDoc1][MyDoc2]</code> won&apos;t work anymore (<a href="https://github.com/docarray/docarray/pull/1800?ref=jina-ai-gmbh.ghost.io">#1800</a>)</p><p>We also fixed a bug that caused a silent failure when users passed <code>DocList</code> the wrong type, for example <code>DocList[doc()]</code>. (<a href="https://github.com/docarray/docarray/pull/1794?ref=jina-ai-gmbh.ghost.io">#1794</a>)</p><h3 id="milvus-connection-parameter-missing-1802">Milvus connection parameter missing (<a href="https://github.com/docarray/docarray/pull/1802?ref=jina-ai-gmbh.ghost.io">#1802</a>)</h3><p>We fixed a small bug that incorrectly set the port of the Milvus client.</p><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Fix Documentation for pydantic v2 (<a href="https://github.com/docarray/docarray/pull/1815?ref=jina-ai-gmbh.ghost.io">docs: fix documentation for pydantic v2 #1815</a>)</li><li>Adding field descriptions to predefined mesh 3D Document (<a href="https://github.com/docarray/docarray/pull/1789?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined mesh 3D document #1789</a>)</li><li>Adding field descriptions to predefined point cloud 3D Document (<a href="https://github.com/docarray/docarray/pull/1792?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined point cloud 3D document #1792</a>)</li><li>Adding field descriptions to predefined video Document (<a href="https://github.com/docarray/docarray/pull/1775?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined video document #1775</a>)</li><li>Adding field descriptions to predefined text Document (<a href="https://github.com/docarray/docarray/pull/1770?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined text document #1770</a>)</li><li>Adding field descriptions to predefined image Document (<a href="https://github.com/docarray/docarray/pull/1772?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined image document #1772</a>)</li><li>Adding field descriptions to predefined audio Document (<a href="https://github.com/docarray/docarray/pull/1774?ref=jina-ai-gmbh.ghost.io">docs: adding field descriptions to predefined audio document #1774</a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>lvzi (<a href="https://github.com/lvzii?ref=jina-ai-gmbh.ghost.io">@lvzii</a> )</li><li>Puneeth K (<a href="https://github.com/punndcoder28?ref=jina-ai-gmbh.ghost.io">@punndcoder28</a> )</li><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a> )</li><li>samsja (<a href="https://github.com/samsja?ref=jina-ai-gmbh.ghost.io">@samsja</a> )</li></ul>]]></content:encoded></item></channel></rss>
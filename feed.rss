<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.77</generator><lastBuildDate>Fri, 02 Feb 2024 21:53:38 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[A Deep Dive into Tokenization]]></title><description><![CDATA[Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?]]></description><link>https://jina.ai/news/a-deep-dive-into-tokenization/</link><guid isPermaLink="false">65afb3ee8da8040001e17061</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 31 Jan 2024 15:10:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" alt="A Deep Dive into Tokenization"><p>There are a lot of barriers to understanding AI models, some of them pretty big barriers, and they can stand in the way of implementing AI processes. But the first one many people encounter is understanding what we mean when talking about <strong>tokens</strong>. </p><p>One of the most important practical parameters in choosing an AI language model is the size of its context window &#x2014; the maximum input text size &#x2014; which is given in tokens, not words or characters or any other automatically recognizable unit.</p><p>Furthermore, embedding services are typically figured &#x201C;per token,&#x201D; meaning tokens are important to understanding your bill.</p><p>This can be very confusing if you aren&#x2019;t clear about what a token is. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="A Deep Dive into Tokenization" loading="lazy" width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings current price sheet (as of February 2024). Note that prices are stated per &#x201C;1M tokens&#x201D;.</span></figcaption></figure><p>But of all the confusing aspects of modern AI, tokens are probably the least complicated. This article will try to clarify what tokenization is, what it does, and why we do it that way.</p><h2 id="tldr">tl;dr</h2><p>For those who want or need a quick answer to figure out how many tokens to buy from Jina Embeddings or an estimate of how many they need to expect to buy, the following statistics are what you&apos;re looking for.</p><h3 id="tokens-per-english-word">Tokens per English Word</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A call to the Jina Embeddings v2 API for English models will use <b><strong style="white-space: pre-wrap;">approximately</strong></b> <b><strong style="white-space: pre-wrap;">10% more</strong></b> tokens than the number of words in your text, <b><strong style="white-space: pre-wrap;">plus two tokens per embedding</strong></b>.</div></div><p>During empirical testing, described further down in this article, a variety of English texts converted into tokens at a rate of about 10% more tokens than words, using Jina Embeddings English-only models. This result was pretty robust. </p><p>Jina Embeddings v2 models have a context window of 8192 tokens. This means that if you pass a Jina model an English text longer than 7,400 words, there is a good chance it will be truncated.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">7,400 words</strong></b>.</div></div><h3 id="tokens-per-chinese-character">Tokens per Chinese Character</h3><p>For Chinese, results are more variable. Depending on the text type, ratios varied from 0.6 to 0.75 tokens per Chinese character (&#x6C49;&#x5B57;). English texts given to Jina Embeddings v2 for Chinese produce approximately the same number of tokens as Jina Embeddings v2 for English: roughly 10% more than the number of words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The maximum size for input in Chinese to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for Chinese and English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">10,500 characters</strong></b> (<b><strong style="white-space: pre-wrap;">&#x5B57;&#x6570;</strong></b>), or <b><strong style="white-space: pre-wrap;">0.6 to 0.75 tokens per Chinese character, plus two per embedding.</strong></b></div></div><h3 id="tokens-per-german-word">Tokens per German Word</h3><p>German word-to-token ratios are more variable than English but less than Chinese. Depending on the genre of the text, I got 20% to 30% more tokens than words on average. Giving English texts to Jina Embeddings v2 for German and English uses a few more tokens than the English-only and Chinese/English models: 12% to 15% more tokens than words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Jina Embeddings v2 for German and English will count <b><strong style="white-space: pre-wrap;">20% to 30% more tokens than words, plus two per embedding</strong></b>. The maximum size of the input context is approximately <b><strong style="white-space: pre-wrap;">6,300 German words</strong></b>.</div></div><h3 id="caution">Caution!</h3><p>These are simple calculations, but they should be approximately right for most natural language texts and most users. Ultimately, we can only promise that the number of tokens will always be no more than the number of characters in your text, plus two. It will practically always be much less than that, but we cannot promise any specific count in advance.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Your Mileage May Vary! </strong></b><br><br>These are estimates based on statistically naive calculations. We do not guarantee how many tokens any particular request will take.</div></div><p>If all you need is advice on how many tokens to buy for Jina Embeddings, you can stop here. Other embedding models, from companies other than Jina AI, may not have the same token-to-word and token-to-Chinese-character ratios Jina models have, but they will not generally be very different overall.</p><p>If you want to understand why, the rest of this article is a deeper dive into tokenization for language models.</p><h2 id="words-tokens-numbers">Words, Tokens, Numbers</h2><p>Tokenization has been a part of natural language processing for longer than modern AI models have existed.</p><p>It&#x2019;s a bit clich&#xE9; to say that everything in a computer is just a number, but it&#x2019;s also mostly true. Language, however, is not naturally just a bunch of numbers. It might be speech, made of sound waves, or writing, made of marks on paper, or even an image of a printed text or a video of someone using sign language. But most of the time, when we talk about using computers to process natural language, we mean texts composed of sequences of characters: letters (a, b, c, etc.), numerals (0, 1, 2&#x2026;), punctuation, and spaces, in different languages and textual encodings.</p><p>Computer engineers call these &#x201C;strings&#x201D;.</p><p>AI language models take sequences of numbers as input. So, you might write the sentence:</p><blockquote><em>What is today&apos;s weather in Berlin?</em></blockquote><p>But, after tokenization, the AI model gets as input:</p><pre><code class="language-python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>Tokenization is the process of converting an input string into a specific sequence of numbers that your AI model can understand.</p><p>When you use an AI model via a web API that charges users per token, each request is converted into a sequence of numbers like the one above. The number of tokens in the request is the length of that sequence of numbers. So, asking Jina Embeddings v2 for English to give you an embedding for &#x201C;<em>What is today&apos;s weather in Berlin?</em>&#x201D; will cost you 11 tokens because it converted that sentence into a sequence of 11 numbers before passing it to the AI model.</p><p>AI models based on the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io">Transformer architecture</a> have a fixed-size <strong>context window</strong> whose size is measured in tokens. Sometimes this is called an &#x201C;input window,&#x201D; &#x201C;context size,&#x201D; or &#x201C;sequence length&#x201D; (especially on the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Hugging Face MTEB leaderboard</a>). It means the maximum text size that the model can see at one time.</p><p>So, if you want to use an embedding model, this is the maximum input size allowed.</p><p>Jina Embeddings v2 models all have a context window of 8,192 tokens. Other models will have different (typically smaller) context windows. This means that however much text you put into it, the tokenizer associated with that Jina Embeddings model must convert it into no more than 8,192 tokens.</p><h2 id="mapping-language-to-numbers">Mapping Language to Numbers</h2><p>The simplest way to explain the logic of tokens is this:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">A token is a number that stands in for a part of a string.</div></div><p>For natural language models, the part of a string that a token stands for is a word, a part of a word, or a piece of punctuation. Spaces are not generally given any explicit representation in tokenizer output.</p><p>Tokenization is part of a group of techniques in natural language processing called <a href="https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io"><em>text segmentation</em></a>, and the module that performs tokenization is called, very logically, a <strong>tokenizer</strong>.</p><p>To show how tokenization works, we&#x2019;re going to tokenize some sentences using the smallest Jina Embeddings v2 for English model: <code>jina-embeddings-v2-small-en</code>. Jina Embeddings&#x2019; other English-only model &#x2014; <code>jina-embeddings-v2-base-en</code> &#x2014; uses the same tokenizer, so there&#x2019;s no point in downloading extra megabytes of AI model that we won&#x2019;t use in this article.</p><p>First, install the <code>transformers</code> module in your Python environment or notebook. Use the <code>-U</code> flag to make sure you upgrade to the latest version because this model will not work with some older versions:</p><pre><code class="language-bash">pip install -U transformers
</code></pre><p>Then, download <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> using <code>AutoModel.from_pretrained</code>:</p><pre><code class="language-Python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-small-en&apos;, trust_remote_code=True)
</code></pre><p>To tokenize a string, use the <code>encode</code> method of the <code>tokenizer</code> member object of the model:</p><pre><code class="language-Python">model.tokenizer.encode(&quot;What is today&apos;s weather in Berlin?&quot;)
</code></pre><p>The result is a list of numbers:</p><pre><code class="language-Python">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]
</code></pre><p>To convert these numbers back to string forms, use the <code>convert_ids_to_tokens</code> method of the <code>tokenizer</code> object:</p><pre><code class="language-Python">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])
</code></pre><p>The result is a list of strings:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;what&apos;, &apos;is&apos;, &apos;today&apos;, &quot;&apos;&quot;, &apos;s&apos;, &apos;weather&apos;, &apos;in&apos;,
 &apos;berlin&apos;, &apos;?&apos;, &apos;[SEP]&apos;]
</code></pre><p>Note that the model&#x2019;s tokenizer has:</p><ol><li>Added <code>[CLS]</code>at the beginning and <code>[SEP]</code> at the end. This is necessary for technical reasons and means that <strong>every request for an embedding will cost two extra tokens</strong>, above however many tokens the text takes.</li><li>Split punctuation from words, turning &#x201C;<em>Berlin?</em>&#x201D; into: <code>berlin</code> and <code>?</code>, and &#x201C;<em>today&#x2019;s</em>&#x201D; into <code>today</code>, <code>&apos;</code>, and <code>s</code>.</li><li>Put everything in lowercase. Not all models do this, but this can help with training when using English. It may be less helpful in languages where capitalization has a different meaning.</li></ol><p>Different word-counting algorithms in different programs might count the words in this sentence differently. OpenOffice counts this as six words. The Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) counts seven words. Other software may come to other numbers, depending on how they handle punctuation and clitics like &#x201C;&#x2019;s.&#x201D;</p><p>The tokenizer for this model produces nine tokens for those six or seven words, plus the two extra tokens needed with every request.</p><p>Now, let&#x2019;s try with a less common place-name than Berlin:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;I live in Kinshasa.&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-Python">[&apos;[CLS]&apos;, &apos;i&apos;, &apos;live&apos;, &apos;in&apos;, &apos;kin&apos;, &apos;##sha&apos;, &apos;##sa&apos;, &apos;.&apos;, &apos;[SEP]&apos;]
</code></pre><p>The name &#x201C;Kinshasa&#x201D; is broken up into three tokens: <code>kin</code>, <code>##sha</code>, and <code>##sa</code>. The <code>##</code> indicates that this token is not the beginning of a word.</p><p>If we give the tokenizer something completely alien, the number of tokens over the number of words increases even more:</p><pre><code class="language-Python">token_ids = model.tokenizer.encode(&quot;Klaatu barada nikto&quot;)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)

[&apos;[CLS]&apos;, &apos;k&apos;, &apos;##la&apos;, &apos;##at&apos;, &apos;##u&apos;, &apos;bar&apos;, &apos;##ada&apos;, &apos;nik&apos;, &apos;##to&apos;, &apos;[SEP]&apos;]
</code></pre><p>Three words become eight tokens, plus the <code>[CLS]</code> and <code>[SEP]</code> tokens.</p><p>Tokenization in German is similar. With the <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 for German</a> model, we can tokenize a translation of &quot;What is today&apos;s weather in Berlin?&quot; the same way as with the English model.</p><pre><code class="language-Python">german_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
token_ids = german_model.tokenizer.encode(&quot;Wie wird das Wetter heute in Berlin?&quot;)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Wie&apos;, &apos;wird&apos;, &apos;das&apos;, &apos;Wetter&apos;, &apos;heute&apos;, &apos;in&apos;, &apos;Berlin&apos;, &apos;?&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>This tokenizer is a little bit different from the English one in that <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> replace <code>[CLS]</code> and <code>[SEP]</code> but serve the same function. Also, the text is not case-normalized &#x2014; upper and lower cases remain as written &#x2014; because capitalization is meaningful in German differently from English.</p><p>(To simplify this presentation, I removed a special character indicating a word&apos;s beginning.)</p><p>Now, let&#x2019;s try a more complex sentence <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io">from a newspaper text</a>:</p><blockquote>Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen bleibt liegen &#x2013; zu genervt sind die Landwirte von b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.</blockquote><pre><code>sentence = &quot;&quot;&quot;
Ein Gro&#xDF;teil der milliardenschweren Bauern-Subventionen
bleibt liegen &#x2013; zu genervt sind die Landwirte von 
b&#xFC;rokratischen G&#xE4;ngelungen und Regelwahn.
&quot;&quot;&quot;
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)</code></pre><p>The tokenized result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;Ein&apos;, &apos;Gro&#xDF;teil&apos;, &apos;der&apos;, &apos;mill&apos;, &apos;iarden&apos;, &apos;schwer&apos;, 
 &apos;en&apos;, &apos;Bauern&apos;, &apos;-&apos;, &apos;Sub&apos;, &apos;ventionen&apos;, &apos;bleibt&apos;, &apos;liegen&apos;, 
 &apos;&#x2013;&apos;, &apos;zu&apos;, &apos;gen&apos;, &apos;ervt&apos;, &apos;sind&apos;, &apos;die&apos;, &apos;Landwirte&apos;, &apos;von&apos;, 
 &apos;b&#xFC;ro&apos;, &apos;krat&apos;, &apos;ischen&apos;, &apos;G&#xE4;n&apos;, &apos;gel&apos;, &apos;ungen&apos;, &apos;und&apos;, &apos;Regel&apos;, 
 &apos;wahn&apos;, &apos;.&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>Here, you see that many German words were broken up into smaller pieces and not necessarily along the lines licensed by German grammar. The result is that a long German word that would count as just one word to a word counter might be any number of tokens to Jina&#x2019;s AI model.</p><p>Let&#x2019;s do the same in Chinese, translating &#x201D;What is today&apos;s weather in Berlin?&#x201D; as:</p><blockquote>&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;</blockquote><pre><code>chinese_model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(&quot;&#x67CF;&#x6797;&#x4ECA;&#x5929;&#x7684;&#x5929;&#x6C14;&#x600E;&#x4E48;&#x6837;&#xFF1F;&quot;)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>The tokenized result:</p><pre><code class="language-Python">[&apos;&lt;s&gt;&apos;, &apos;&#x67CF;&#x6797;&apos;, &apos;&#x4ECA;&#x5929;&#x7684;&apos;, &apos;&#x5929;&#x6C14;&apos;, &apos;&#x600E;&#x4E48;&#x6837;&apos;, &apos;&#xFF1F;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>In Chinese, there are usually no word breaks in written text, but the Jina Embeddings tokenizer frequently joins multiple Chinese characters together:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>&#x67CF;&#x6797;</td>
<td>B&#xF3;l&#xED;n</td>
<td>Berlin</td>
</tr>
<tr>
<td>&#x4ECA;&#x5929;&#x7684;</td>
<td>j&#x12B;nti&#x101;n de</td>
<td>today&#x2019;s</td>
</tr>
<tr>
<td>&#x5929;&#x6C14;</td>
<td>ti&#x101;nq&#xEC;</td>
<td>weather</td>
</tr>
<tr>
<td>&#x600E;&#x4E48;&#x6837;</td>
<td>z&#x11B;nmey&#xE0;ng</td>
<td>how</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Let&#x2019;s use a more complex sentence <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io">from a Hong Kong-based newspaper</a>:</p><pre><code class="language-Python">sentence = &quot;&quot;&quot;
&#x65B0;&#x898F;&#x5B9A;&#x57F7;&#x884C;&#x9996;&#x65E5;&#xFF0C;&#x8A18;&#x8005;&#x5728;&#x4E0B;&#x73ED;&#x9AD8;&#x5CF0;&#x524D;&#x7684;&#x4E0B;&#x5348;5&#x6642;&#x4F86;&#x5230;&#x5EE3;&#x5DDE;&#x5730;&#x9435;3&#x865F;&#x7DDA;&#xFF0C;
&#x5F9E;&#x7E41;&#x5FD9;&#x7684;&#x73E0;&#x6C5F;&#x65B0;&#x57CE;&#x7AD9;&#x555F;&#x7A0B;&#xFF0C;&#x5411;&#x6A5F;&#x5834;&#x5317;&#x65B9;&#x5411;&#x51FA;&#x767C;&#x3002;
&quot;&quot;&quot;
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>(Translation: <em>&#x201C;On the first day that the new regulations were in force, this reporter arrived at Guangzhou Metro Line 3 at 5 p.m., during rush hour, having departed the Zhujiang New Town Station heading north towards the airport.&#x201D;</em>)</p><p>The result:</p><pre><code class="language-python">[&apos;&lt;s&gt;&apos;, &apos;&#x65B0;&apos;, &apos;&#x898F;&#x5B9A;&apos;, &apos;&#x57F7;&#x884C;&apos;, &apos;&#x9996;&apos;, &apos;&#x65E5;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x8A18;&#x8005;&apos;, &apos;&#x5728;&#x4E0B;&apos;, &apos;&#x73ED;&apos;, 
 &apos;&#x9AD8;&#x5CF0;&apos;, &apos;&#x524D;&#x7684;&apos;, &apos;&#x4E0B;&#x5348;&apos;, &apos;5&apos;, &apos;&#x6642;&apos;, &apos;&#x4F86;&#x5230;&apos;, &apos;&#x5EE3;&#x5DDE;&apos;, &apos;&#x5730;&apos;, &apos;&#x9435;&apos;, &apos;3&apos;, 
 &apos;&#x865F;&apos;, &apos;&#x7DDA;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5F9E;&apos;, &apos;&#x7E41;&#x5FD9;&apos;, &apos;&#x7684;&apos;, &apos;&#x73E0;&#x6C5F;&apos;, &apos;&#x65B0;&#x57CE;&apos;, &apos;&#x7AD9;&apos;, &apos;&#x555F;&apos;, 
 &apos;&#x7A0B;&apos;, &apos;&#xFF0C;&apos;, &apos;&#x5411;&apos;, &apos;&#x6A5F;&#x5834;&apos;, &apos;&#x5317;&apos;, &apos;&#x65B9;&#x5411;&apos;, &apos;&#x51FA;&#x767C;&apos;, &apos;&#x3002;&apos;, &apos;&lt;/s&gt;&apos;]
</code></pre><p>These tokens do not map to any specific dictionary of Chinese words (&#x8BCD;&#x5178;). For example, &#x201C;&#x555F;&#x7A0B;&#x201D; - <em>q&#x1D0;ch&#xE9;ng</em> (depart, set out) would typically be categorized as a single word but is here split into its two constituent characters. Similarly, &#x201C;&#x5728;&#x4E0B;&#x73ED;&#x201D; would usually be recognized as two words, but with the split between &#x201C;&#x5728;&#x201D; - <em>z&#xE0;i</em> (in, during) and &#x201C;&#x4E0B;&#x73ED;&#x201D; - <em>xi&#xE0;b&#x101;n</em> (the end of the workday, rush hour), not between &#x201C;&#x5728;&#x4E0B;&#x201D; and &#x201C;&#x73ED;&#x201D; as the tokenizer has done here.</p><p>In all three languages, the places where the tokenizer breaks the text up are not directly related to the logical places where a human reader would break them.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">The tokenizer algorithm does not use a conventional, language-aware dictionary, so its behavior does not match how humans count words.</div></div><p>This is not a specific feature of Jina Embeddings models. This approach to tokenization is almost universal in AI model development. Although two different AI models may not have identical tokenizers, in the current state of development, they will practically all use tokenizers with this kind of behavior.</p><p>The next section will discuss the specific algorithm used in tokenization and the logic behind it.</p><h2 id="why-do-we-tokenize-and-why-this-way">Why Do We Tokenize? And Why This Way?</h2><p>AI language models take as input sequences of numbers that stand in for text sequences, but a bit more happens before running the underlying neural network and creating an embedding. When presented with a list of numbers representing small text sequences, the model looks each number up in an internal dictionary that stores a unique vector for each number. It then combines them, and that becomes the input to the neural network.</p><p>This means that the tokenizer <strong>must</strong> be able to convert <strong><em>any</em></strong> input text we give it into tokens that appear in the model&#x2019;s dictionary of token vectors. If we took our tokens from a conventional dictionary, the first time we encountered a misspelling or a rare proper noun or foreign word, the whole model would stop. It could not process that input.</p><p>In natural language processing, this is called the out-of-vocabulary (OOV) problem, and it&#x2019;s pervasive in all text types and all languages. There are a few strategies for addressing the OOV problem:</p><ol><li>Ignore it. Replace everything not in the dictionary with an &#x201C;unknown&#x201D; token.</li><li>Bypass it. Instead of using a dictionary that maps text sequences to vectors, use one that maps <em>individual characters</em> to vectors. English only uses 26 letters most of the time, so this must be smaller and more robust against OOV problems than any dictionary.</li><li>Find frequent subsequences in the text, put them in the dictionary, and use characters (single-letter tokens) for whatever is left.</li></ol><p>The first strategy means that a lot of important information is lost. The model can&#x2019;t even learn about the data it&#x2019;s seen if it takes the form of something not in the dictionary. A lot of things in ordinary text are just not present in even the largest dictionaries.</p><p>The second strategy is possible, and researchers have investigated it. However, it means that the model has to accept a lot more input and has to learn a lot more. This means a much bigger model and much more training data for a result that has never proven to be any better than the third strategy.</p><p>AI language models pretty much all implement the third strategy in some form. Most use some variant of the <a href="https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io">Wordpiece algorithm</a> <a href="https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io">[Schuster and Nakajima 2012]</a> or a similar technique called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io">Senrich et al. 2016</a>] These algorithms are <em>language-agnostic</em>. That means they work the same for all written languages without any knowledge beyond a comprehensive list of possible characters. They were designed for multilingual models like Google&#x2019;s BERT that take just any input from scraping the Internet &#x2014; hundreds of languages and texts other than human language like computer programs &#x2014; so that they could be trained without doing complicated linguistics.</p><p>Some research shows significant improvements using more language-specific and language-aware tokenizers. [<a href="https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io">Rust et al. 2021</a>] But building tokenizers that way takes time, money, and expertise. Implementing a universal strategy like BPE or Wordpiece is much cheaper and easier.</p><p>However, as a consequence, there is no way to know how many tokens a specific text represents other than to run it through a tokenizer and then count the number of tokens that come out of it. Because the smallest possible subsequence of a text is one letter, you can be sure the number of tokens won&#x2019;t be larger than the number of characters (minus spaces) plus two.</p><p>To get a good estimate, we need to throw a lot of text at our tokenizer and calculate empirically how many tokens we get on average, compared to how many words or characters we input. In the next section, we&#x2019;ll do some not-very-systematic empirical measurements for all Jina Embeddings v2 models currently available.</p><h2 id="empirical-estimates-of-token-output-sizes">Empirical Estimates of Token Output Sizes</h2><p>For English and German, I used the Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io">Unicode Standard Annex #29</a>) to get word counts for texts. This algorithm is widely used to select text snippets when you double-click on something. It is the closest thing available to a universal objective word counter.</p><p>I installed the <a href="https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io">polyglot library</a> in Python, which implements this text segmenter:</p><pre><code class="language-bash">pip install -U polyglot
</code></pre><p>To get the word count of a text, you can use code like this snippet:</p><pre><code class="language-python">from polyglot.text import Text

txt = &quot;What is today&apos;s weather in Berlin?&quot;
print(len(Text(txt).words))
</code></pre><p>The result should be <code>7</code>.</p><p>To get a token count, segments of the text were passed to the tokenizers of various Jina Embeddings models, as described below, and each time, I subtracted two from the number of tokens returned.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">The token counts listed here <b><strong style="white-space: pre-wrap;">do not include</strong></b> the extra two tokens at the beginning and end of each tokenized text.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en">English<br>(<code>jina-embeddings-v2-small-en</code> and <code>jina-embeddings-v2-base-en</code>)</h3><p>To calculate averages, I downloaded two English text corpora from <a href="https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Wortschatz Leipzig</a>, a collection of freely downloadable corpora in a number of languages and configurations hosted by Leipzig University:</p><ul><li>A one-million-sentence corpus of news data in English from 2020 (<code>eng_news_2020_1M</code>)</li><li>A one-million-sentence corpus of <a href="https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io">English Wikipedia</a> data from 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Both can be found on <a href="https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io">their English downloads page</a>.</p><p>For diversity, I also downloaded the <a href="https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io">Hapgood translation of Victor Hugo&#x2019;s <em>Les Mis&#xE9;rables</em></a> from Project Gutenberg, and a copy of the King James Version of the Bible, translated to English in 1611.</p><p>For each all four texts, I counted the words using the Unicode segmenter implemented in <code>polyglot</code>, then counted the tokens made by <code>jina-embeddings-v2-small-en</code>, subtracting two tokens for each tokenization request. The results are as follows:</p>
<!--kg-card-begin: html-->
<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Text</th><th id="|&lt;X;" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for English)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class><code>eng_news_2020_1M</code></td><td id="|&lt;X;" class>22,825,712</td><td id="GHal" class>25,270,581</td><td id="h]mu" class>1.107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class><code>eng_wikipedia_2016_1M</code></td><td id="|&lt;X;" class>24,243,607</td><td id="GHal" class>26,813,877</td><td id="h]mu" class>1.106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class><code>les_miserables_en</code></td><td id="|&lt;X;" class>688,911</td><td id="GHal" class>764,121</td><td id="h]mu" class>1.109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class><code>kjv_bible</code></td><td id="|&lt;X;" class>1,007,651</td><td id="GHal" class>1,099,335</td><td id="h]mu" class>1.091</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>The use of precise numbers does not mean this is a precise result. That documents of such different genres would all have between 9% and 11% more tokens than words indicates that you can probably expect somewhere around 10% more tokens than words, as measured by the Unicode segmenter. Word processors often do not count punctuation, while the Unicode Segmenter does, so you can&#x2019;t expect the word counts from office software to necessarily match this.</p><h3 id="german-jina-embeddings-v2-base-de">German<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>For German, I downloaded three corpora from <a href="https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io">Wortschatz Leipzig&#x2019;s German page</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> &#x2014; One million sentences from a balanced mixture of texts in different genres, dating to 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> &#x2014; One million sentences of news text from 2019.</li><li><code>deu_wikipedia_2021_1M</code> &#x2014; One million sentences extracted from the German Wikipedia in 2021.</li></ul><p>And for diversity, I also downloaded all <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata&amp;ref=jina-ai-gmbh.ghost.io">three volumes of Karl Marx&#x2019;s <em>Kapital</em></a> from the <a href="https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>I then followed the same procedure as for English:</p>
<!--kg-card-begin: html-->
<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Text</th><th id="|qln" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class>7,924,024</td><td id="YXZX" class>9,772,652</td><td id="oEoQ" class>1.234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class>17,949,120</td><td id="YXZX" class>21,711,555</td><td id="oEoQ" class>1.210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class>17,999,482</td><td id="YXZX" class>22,654,901</td><td id="oEoQ" class>1.259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class>784,336</td><td id="YXZX" class>1,011,377</td><td id="oEoQ" class>1.289</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>These results have a larger spread than the English-only model but still suggest that German text will yield, on average, 20% to 30% more tokens than words.</p><p>English texts yield more tokens with the German-English tokenizer than the English-only one:</p>
<!--kg-card-begin: html-->
<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc&lt;l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Text</th><th id="R@A^" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class>24243607</td><td id="UUfl" class>27758535</td><td id="iTZS" class>1.145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc&lt;l" class style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class>22825712</td><td id="UUfl" class>25566921</td><td id="iTZS" class>1.120</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>You should expect to need 12% to 15% more tokens than words to embed English texts with the bilingual German/English than with the English-only one.</p><h3 id="chinese-jina-embeddings-v2-base-zh">Chinese<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Chinese is typically written without spaces and had no traditional notion of &#x201C;words&#x201D; before the 20th century. Consequently, the size of a Chinese text is typically measured in characters (<strong>&#x5B57;&#x6570;</strong>). So, instead of using the Unicode Segmenter, I measured the length of Chinese texts by removing all the spaces and then just getting the character length.</p><p>I downloaded three corpora from the <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io">Chinese corpus page at Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> &#x2014; One million sentences from the Chinese language Wikipedia, extracted in 2018.</li><li><code>zho_news_2007-2009_1M</code> &#x2014; One million sentences from Chinese news sources, collected from 2007 to 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> &#x2014; One million sentences from news sources that use exclusively traditional Chinese characters (&#x7E41;&#x9AD4;&#x5B57;).</li></ul><p>In addition, for some diversity, I also used <em>The True Story of Ah Q</em> (&#x963F;Q&#x6B63;&#x50B3;), a novella by Lu Xun (&#x9B6F;&#x8FC5;) written in the early 1920s. I downloaded the <a href="https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io">traditional character version from Project Gutenberg</a>.</p>
<!--kg-card-begin: html-->
<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Text</th><th id="CaUc" class="simple-table-header-color simple-table-header">Character count<br>(&#x5B57;&#x6570;)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for Chinese and English)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio of tokens to characters<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class>45,116,182</td><td id="CQ{d" class>29,193,028</td><td id="_};C" class>0.647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class>44,295,314</td><td id="CQ{d" class>28,108,090</td><td id="_};C" class>0.635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class>54,585,819</td><td id="CQ{d" class>40,290,982</td><td id="_};C" class>0.738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class>41,268</td><td id="CQ{d" class>25,346</td><td id="_};C" class>0.614</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>This spread in token-to-character ratios is unexpected, and especially the outlier for the traditional character corpus merits further investigation. Nonetheless, we can conclude that for Chinese, you should expect to need <em>fewer</em> tokens than there are characters in your text. Depending on your content, you can expect to need 25% to 40% less.</p><p>English texts in Jina Embeddings v2 for Chinese and English yielded roughly the same number of tokens as they do in the English-only model:</p>
<!--kg-card-begin: html-->
<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="&lt;rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class style="width:165.3125px">24,243,607</td><td id="=]?F" class>26,890,176</td><td id="&lt;rlw" class>1.109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class style="width:165.3125px">22,825,712</td><td id="=]?F" class>25,060,352</td><td id="&lt;rlw" class>1.097</td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="taking-tokens-seriously">Taking Tokens Seriously</h2><p>Tokens are an important scaffolding for AI language models, and research is ongoing in this area.</p><p>One of the places where AI models have proven revolutionary is the discovery that they are very robust against noisy data. Even if a particular model does not use the optimal tokenization strategy, if the network is large enough, given enough data, and adequately trained, it can learn to do the right thing from imperfect input.</p><p>Consequently, much less effort is spent on improving tokenization than in other areas, but this may change.</p><p>As a user of embeddings, who buys them via an <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">API like Jina Embeddings</a>, you can&#x2019;t know precisely how many tokens you&#x2019;ll need for a specific task and may have to do some testing of your own to get solid numbers. But the estimates provided here &#x2014; circa 110% of the word count for English, circa 125% of the word count for German, and circa 70% of the character count for Chinese &#x2014; should be enough for basic budgeting.</p><h2 id="learn-more">Learn More</h2><p>For more information about Jina Embeddings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Best Embeddings and Perfect Prompts</div><div class="kg-bookmark-description">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Best Embeddings and Perfect Prompts</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="A Deep Dive into Tokenization"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4308 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="A Deep Dive into Tokenization"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="A Deep Dive into Tokenization"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[MyScale & Jina AI: Unleashing Great Potential for Your AI Applications]]></title><description><![CDATA[With full integration of Jina Embeddings v2 models, MyScale allows users to harness the capabilities of Jina AI within an SQL database.]]></description><link>https://jina.ai/news/myscale-jina-ai-unleashing-great-potential-for-your-ai-applications/</link><guid isPermaLink="false">65b785a3c38742000104062a</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 29 Jan 2024 15:00:29 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/MyScaleBlog.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"><p><a href="https://myscale.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MyScale</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its AI-focused SQL vector database. You can now use Jina AI&apos;s cutting-edge embedding models in conjunction with powerful and time-tested SQL database technologies to bring precise text matching and efficient semantic similarity to your data-driven applications.</p><p>Follow the link below to&#xA0;MyScale&apos;s website to see how you can supercharge your SQL with Jina Embeddings.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://myscale.com/blog/myscale-integration-jinaai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications</div><div class="kg-bookmark-description">MyScale is fully integrated with Jina Embeddings v2 in the EmbedText function, allowing users to process text with an input length of up to 8K using the standard SQL syntax.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://myscale.com/favicon.ico" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d3lhz231q7ogjd.cloudfront.net/blog/myscale-and-jinaai-2.jpg" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="MyScale &amp; Jina AI: Unleashing Great Potential for Your AI Applications"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face]]></title><description><![CDATA[Jina AI's open-source bilingual embedding models for German-English and Chinese-English are now on Hugging Face.
We’re going to walk through installation and cross-language retrieval.]]></description><link>https://jina.ai/news/jina-embeddings-v2-bilingual-models-are-now-open-source-on-hugging-face/</link><guid isPermaLink="false">65b3adb510ff9f0001c50c4d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 26 Jan 2024 16:14:56 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><p>Jina AI has released its state-of-the-art open-source bilingual embedding models for <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io">German-English</a> and <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese-English</a> language pairs via Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class="kg-bookmark-description">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class="kg-bookmark-description">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"><span class="kg-bookmark-publisher">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>In this tutorial, we&#x2019;re going to walk through a very minimal installation and use case that will cover:</p><ol><li>Downloading Jina Embedding models from Hugging Face.</li><li>Using the models to get encodings from texts in German and English.</li><li>Building a very rudimentary embeddings-based neural search engine for cross-language queries.</li></ol><p>We will show you how to use Jina Embeddings to write English queries that retrieve matching texts in German and vice-versa.</p><p>This tutorial works the same for the Chinese model. Just follow the instructions in the section (towards the end) titled <a href="#querying-in-chinese" rel="noreferrer"><strong>Querying in Chinese</strong></a> to get the Chinese-English bilingual model and an example document in Chinese.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-de &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-de.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-zh?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-zh &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-zh.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="bilingual-embedding-models">Bilingual Embedding Models</h2><p>A bilingual embedding model is a model that maps texts in two languages &#x2014; German and English in this tutorial, Chinese and English for the Chinese model &#x2014; to the same embedding space. And, it does it in such a way that if a German text and an English text mean the same thing, their corresponding embedding vectors will be close together.</p><p>Models like this are very well suited to cross-language information retrieval applications, which we will show in this tutorial, but can also serve as a basis for RAG-based chatbots, multilingual text categorization, summarization, sentiment analysis, and any other application that uses embeddings. By using models like these, you can treat texts in both languages as if they were written in the same language.</p><p>Although many giant language models trained claim to support many different languages, they do not support all languages equally. There are growing questions of <a href="https://aclanthology.org/2023.findings-eacl.89/?ref=jina-ai-gmbh.ghost.io">bias caused by the dominance of English on the Internet</a> and input sources distorted by the <a href="https://arxiv.org/abs/2401.05749?ref=jina-ai-gmbh.ghost.io">widespread online publication of machine-translated texts</a>. By focusing on two languages, we can better control embedding quality for both, minimizing bias while producing much smaller models with similar or higher performance than giant models that purport to handle dozens of languages.</p><p>Jina Embeddings v2 bilingual models support 8,192 input context tokens, enabling them not just to support two languages, but also to support relatively large segments of text compared to comparable models. This makes them ideal for more complex use cases where much more textual information has to be processed into embeddings.</p><h2 id="follow-along-on-google-colab">Follow along on Google Colab</h2><p>This tutorial has an <a href="https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">accompanying notebook</a> that you can run on <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, or locally on your own system.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/jina-ai/workshops/blob/feat-embeddings-notebook/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/cce4fce8bbe78d8bdc0c77a288df9fa7/img/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><h2 id="installing-the-prerequisites">Installing the Prerequisites</h2><p>Make sure the current environment has the relevant libraries installed. You will need the latest version of <a href="https://pypi.org/project/transformers/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>transformers</code></a>, so even if it is already installed, run:</p><pre><code class="language-bash">pip install -U transformers 
</code></pre><p>This tutorial will use the <a href="https://faiss.ai/?ref=jina-ai-gmbh.ghost.io">FAISS library from Meta</a> to do vector search and comparison. To install it, run:</p><pre><code class="language-bash">pip install faiss-cpu
</code></pre><p>We will also be using <a href="https://www.crummy.com/software/BeautifulSoup/?ref=jina-ai-gmbh.ghost.io">Beautiful Soup</a> to process the input data in this tutorial, so make sure it is installed:</p><pre><code class="language-bash">pip install bs4
</code></pre><h2 id="access-to-hugging-face">Access to Hugging Face</h2><p>You will need access to Hugging Face, specifically an account and an access token to download models.</p><p><strong>If you do not have an account on Hugging Face:</strong></p><p>Go to <a href="https://huggingface.co/?ref=jina-ai-gmbh.ghost.io">https://huggingface.co/</a> and you should see a &#x201C;Sign Up&#x201D; button on the upper right of the page. Click it and follow the instructions from there to make a new account.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="1088" height="887" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png 1088w" sizes="(min-width: 720px) 720px"></figure><p><strong>After you are logged into your account:</strong></p><p>Follow the instructions <a href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io">on the Hugging Face website</a> to get an access token.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">User access tokens</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/docs/hub.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>You need to copy this token into an environment variable called <code>HF_TOKEN</code>. If you&#x2019;re working in a notebook (on <a href="https://colab.research.google.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Google Colab</a>, for example) or setting it internally in a Python program, use the following Python code:</p><pre><code class="language-python">import os

os.environ[&apos;HF_TOKEN&apos;] = &quot;&lt;your token here&gt;&quot;
</code></pre><p>In your shell, use whatever the provided syntax is for setting an environment variable. In <code>bash</code> :</p><pre><code class="language-bash">export HF_TOKEN=&quot;&lt;your token here&gt;&quot;
</code></pre><h2 id="download-jina-embeddings-v2-for-german-and-english">Download Jina Embeddings v2 for German and English</h2><p>Once your token is set, you can download the Jina Embeddings German-English bilingual model using the <code>transformers</code> library:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-de&apos;, trust_remote_code=True)
</code></pre><p>This may take several minutes the first time you do it, but the model will be cached locally after that, so don&#x2019;t worry if you restart this tutorial later.</p><h2 id="download-english-language-data">Download English-language Data</h2><p>For this tutorial, we are going to get the English-language version of the book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a>. This book is also available in Chinese and German, which we&#x2019;ll use later in this tutorial.</p><p>To download the EPUB version, run the following command:</p><pre><code class="language-bash">wget -O progit-en.epub https://open.umn.edu/opentextbooks/formats/3437</code></pre><p>This copies the book to a file named <code>progit-en.epub</code> in the local directory.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--27-.png" class="kg-image" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face" loading="lazy" width="490" height="647"><figcaption><span style="white-space: pre-wrap;">The cover of the paper edition.</span></figcaption></figure><p>Alternatively, you can just visit the link <a href="https://open.umn.edu/opentextbooks/formats/3437?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">https://open.umn.edu/opentextbooks/formats/3437</a> to download it to a local drive. It is available under the&#xA0;<a href="https://creativecommons.org/licenses/by-nc-sa/3.0/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Creative Commons Attribution Non Commercial Share Alike 3.0 license</a>.</p><h2 id="processing-the-data">Processing the Data</h2><p>This particular text has an internal structure of hierarchical sections, which we can easily find by looking for the <code>&lt;section&gt;</code> tag in the underlying XHTML data. The code below reads the EPUB file and splits it up using the internal structure of an EPUB file and the <code>&lt;section&gt;</code> tag, then converts each section to plain text without XHTML tags. It creates a Python dictionary whose keys are a set of strings indicating each section&#x2019;s location in the book, and whose values are the plain text contents of that section.</p><pre><code class="language-python">from zipfile import ZipFile
from bs4 import BeautifulSoup
import copy

def decompose_epub(file_name):
    
    def to_top_text(section):
        selected = copy.copy(section)
				while next_section := selected.find(&quot;section&quot;):
            next_section.decompose()
        return selected.get_text().strip()

    ret = {}
    with ZipFile(file_name, &apos;r&apos;) as zip:
        for name in zip.namelist():
            if name.endswith(&quot;.xhtml&quot;):
                data = zip.read(name)
                doc = BeautifulSoup(data.decode(&apos;utf-8&apos;), &apos;html.parser&apos;)
                ret[name + &quot;:top&quot;] = to_top_text(doc)
                for num, sect in enumerate(doc.find_all(&quot;section&quot;)):
                    ret[name + f&quot;::{num}&quot;] = to_top_text(sect)
    return ret
</code></pre><p>Then, run the <code>decompose_epub</code> function on the EPUB file you downloaded before:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-en.epub&quot;)
</code></pre><p>The variable <code>book_data</code> will now have 583 sections in it. For example:</p><pre><code class="language-python">print(book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;])
</code></pre><p>Result:</p><pre><code class="language-Text">The Command Line
There are a lot of different ways to use Git.
There are the original command-line tools, and there are many graphical user interfaces of varying capabilities.
For this book, we will be using Git on the command line.
For one, the command line is the only place you can run all Git commands&#x2009;&#x2014;&#x2009;most of the GUIs implement only a partial subset of Git functionality for simplicity.
If you know how to run the command-line version, you can probably also figure out how to run the GUI version, while the opposite is not necessarily true.
Also, while your choice of graphical client is a matter of personal taste, all users will have the command-line tools installed and available.
So we will expect you to know how to open Terminal in macOS or Command Prompt or PowerShell in Windows.
If you don&#x2019;t know what we&#x2019;re talking about here, you may need to stop and research that quickly so that you can follow the rest of the examples and descriptions in this book.
</code></pre><h2 id="generating-and-indexing-embeddings-with-jina-embeddings-v2-and-faiss">Generating and Indexing Embeddings with Jina Embeddings v2 and FAISS</h2><p>For each of the 583 sections, we will generate an embedding and store it in a FAISS index. Jina Embeddings v2 models accept input of up to 8192 tokens, large enough that for a book like this, we don&#x2019;t need to do any further text segmentation or check if any section has too many tokens. The longest section in the book has roughly 12,000 characters, which, for normal English, should be far below the 8k token limit.</p><p>To generate a single embedding, you use the <code>encode</code> method of the model we downloaded. For example:</p><pre><code class="language-python">model.encode([book_data[&apos;EPUB/ch01-getting-started.xhtml::12&apos;]])
</code></pre><p>This returns an array containing a single 768 dimension vector:</p><pre><code class="language-python">array([[ 6.11135997e-02,  1.67829826e-01, -1.94809273e-01,
         4.45595086e-02,  3.28837298e-02, -1.33441269e-01,
         1.35364473e-01, -1.23119736e-02,  7.51526654e-02,
        -4.25386652e-02, -6.91794455e-02,  1.03527725e-01,
        -2.90831417e-01, -6.21018047e-03, -2.16205455e-02,
        -2.20803712e-02,  1.50471330e-01, -3.31433356e-01,
        -1.48741454e-01, -2.10959971e-01,  8.80039856e-02,
				....
</code></pre><p>That is an embedding.</p><p>Jina Embeddings models are set up to allow batch processing. The optimal batch size depends on the hardware you use when running. A large batch size risks running out of memory. A small batch size will take longer to process.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text">Setting <code spellcheck="false" style="white-space: pre-wrap;">batch_size=5</code> worked on Google Colab in free tier without a GPU, and took <b><strong style="white-space: pre-wrap;">about an hour</strong></b> to generate the entire set of embeddings.</div></div><p>In production, we recommend using much more powerful hardware or using Jina AI&#x2019;s <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Embedding API service</a>. Follow the link below to find out how it works and how to get started with free access.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure><p>The code below generates the embeddings and stores them in a FAISS index. Set the variable <code>batch_size</code> as appropriate to your resources.</p><pre><code class="language-python">import faiss

batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>When working in a production environment, a Python dictionary is not an adequate or performant way to handle documents and embeddings. You should use a purpose-built vector database, which will have its own directions for data insertion.</p><h2 id="querying-in-german-for-english-results">Querying in German for English Results</h2><p>When we query for something from this set of texts, here&#x2019;s what will happen:</p><ol><li>The Jina Embeddings German-English model will create an embedding for the query.</li><li>We will use the FAISS index (<code>faiss_index</code>) to get the stored embedding with the highest cosine to the query embedding and return its place in the index.</li><li>We will look up the corresponding text in the vector data array (<code>vector_data</code>) and print out the cosine, the location of the text, and the text itself.</li></ol><p>That&#x2019;s what the <code>query</code> function below does.</p><pre><code class="language-python">def query(query_str):
    query = model.encode([query_str], normalize_embeddings=True)
    cosine, index = faiss_index.search(query, 1)
    print(f&quot;Cosine: {cosine[0][0]}&quot;)
    loc, txt = vector_data[index[0][0]]
    print(f&quot;Location: {loc}\\nText:\\n\\n{txt}&quot;)
</code></pre><p>Now let&#x2019;s try it out.</p><pre><code class="language-python"># Translation: &quot;How do I roll back to a previous version?&quot;
query(&quot;Wie kann ich auf eine fr&#xFC;here Version zur&#xFC;cksetzen?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5202275514602661
Location: EPUB/ch02-git-basics-chapter.xhtml::20
Text:

Undoing things with git restore
Git version 2.23.0 introduced a new command: git restore.
It&#x2019;s basically an alternative to git reset which we just covered.
From Git version 2.23.0 onwards, Git will use git restore instead of git reset for many undo operations.
Let&#x2019;s retrace our steps, and undo things with git restore instead of git reset.
</code></pre><p>This is a pretty good choice to answer the question. Let&#x2019;s try another one:</p><pre><code class="language-python"># Translation: &quot;What does &apos;version control&apos; mean?&quot;
query(&quot;Was bedeutet &apos;Versionsverwaltung&apos;?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.5001817941665649
Location: EPUB/ch01-getting-started.xhtml::1
Text:

About Version Control

What is &#x201C;version control&#x201D;, and why should you care?
Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.
For the examples in this book, you will use software source code as the files being version controlled, though in reality you can do this with nearly any type of file on a computer.
If you are a graphic or web designer and want to keep every version of an image or layout (which you would most certainly want to), a Version Control System (VCS) is a very wise thing to use.
It allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more.
Using a VCS also generally means that if you screw things up or lose files, you can easily recover.
In addition, you get all this for very little overhead.
</code></pre><p>Try it with your own German questions to see how well it works. As a general practice, when dealing with text information retrieval, you should ask for three to five responses instead of just one. The best answer is often not the first one.</p><h2 id="reversing-the-roles-querying-german-documents-with-english">Reversing the Roles: Querying German documents with English</h2><p>The book <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io"><em>Pro Git: Everything You Need to Know About Git</em></a> is also <a href="https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git-german?ref=jina-ai-gmbh.ghost.io">available in German</a>. We can use this same model to give this demo with the languages reversed.</p><p>Download the ebook:</p><pre><code class="language-bash">wget -O progit-de.epub https://open.umn.edu/opentextbooks/formats/3454
</code></pre><p>This copies the book to a file named <code>progit-de.epub</code>. We then process it the same way we did for the English book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-de.epub&quot;)
</code></pre><p>And then generate the embeddings the same way as before:</p><pre><code class="language-python">batch_size = 5

vector_data = []
faiss_index = faiss.IndexFlatIP(768)

data = [(key, txt) for key, txt in book_data.items()]
batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

for ind, batch in enumerate(batches):
    print(f&quot;Processing batch {ind + 1} of {len(batches)}&quot;)
    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)
    vector_data.extend(batch)
    faiss_index.add(batch_embeddings)
</code></pre><p>We can now use the same <code>query</code> function to search in English for answers in German:</p><pre><code class="language-python">query(&quot;What is version control?&quot;)
</code></pre><p>Result:</p><pre><code class="language-text">Cosine: 0.6719034910202026
Location: EPUB/ch01-getting-started.xhtml::1
Text:

Was ist Versionsverwaltung?

Was ist &#x201E;Versionsverwaltung&#x201C;, und warum sollten Sie sich daf&#xFC;r interessieren?
Versionsverwaltung ist ein System, welches die &#xC4;nderungen an einer oder einer Reihe von Dateien &#xFC;ber die Zeit hinweg protokolliert, sodass man sp&#xE4;ter auf eine bestimmte Version zur&#xFC;ckgreifen kann.
Die Dateien, die in den Beispielen in diesem Buch unter Versionsverwaltung gestellt werden, enthalten Quelltext von Software, tats&#xE4;chlich kann in der Praxis nahezu jede Art von Datei per Versionsverwaltung nachverfolgt werden.
Als Grafik- oder Webdesigner m&#xF6;chte man zum Beispiel in der Lage sein, jede Version eines Bildes oder Layouts nachverfolgen zu k&#xF6;nnen. Als solcher w&#xE4;re es deshalb ratsam, ein Versionsverwaltungssystem (engl. Version Control System, VCS) einzusetzen.
Ein solches System erlaubt es, einzelne Dateien oder auch ein ganzes Projekt in einen fr&#xFC;heren Zustand zur&#xFC;ckzuversetzen, nachzuvollziehen, wer zuletzt welche &#xC4;nderungen vorgenommen hat, die m&#xF6;glicherweise Probleme verursachen, herauszufinden wer eine &#xC4;nderung urspr&#xFC;nglich vorgenommen hat und viele weitere Dinge.
Ein Versionsverwaltungssystem bietet allgemein die M&#xF6;glichkeit, jederzeit zu einem vorherigen, funktionierenden Zustand zur&#xFC;ckzukehren, auch wenn man einmal Mist gebaut oder aus irgendeinem Grund Dateien verloren hat.
All diese Vorteile erh&#xE4;lt man f&#xFC;r einen nur sehr geringen, zus&#xE4;tzlichen Aufwand.
</code></pre><p>This section&#x2019;s title translates as <em>&#x201C;What is version control?&#x201D;</em>, so this is a good response.</p><h2 id="querying-in-chinese">Querying in Chinese</h2><p>These examples will work exactly the same way with Jina Embeddings v2 for Chinese and English. To use the Chinese model instead, just run the following:</p><pre><code class="language-python">from transformers import AutoModel

model = AutoModel.from_pretrained(&apos;jinaai/jina-embeddings-v2-base-zh&apos;, trust_remote_code=True)
</code></pre><p>And to get the Chinese edition of <em>Pro Git: Everything You Need to Know About Git</em>:</p><pre><code class="language-python">wget -O progit-zh.epub https://open.umn.edu/opentextbooks/formats/3455
</code></pre><p>Then, process the Chinese book:</p><pre><code class="language-python">book_data = decompose_epub(&quot;progit-zh.epub&quot;)
</code></pre><p>All the other code in this tutorial will work the same way.</p><h2 id="the-future-more-languages-including-programming">The Future: More Languages, including Programming</h2><p>We will be rolling out more bilingual models in the immediate future, with Spanish and Japanese already in the works, as well as a model that supports English and several major programming languages. These models are ideally suited to international enterprises that manage multilingual information, and can serve as the cornerstone for AI-powered information retrieval and RAG-based generative language models, inserting themselves into a variety of cutting-edge AI use cases.</p><p>Jina AI&#x2019;s models are compact and perform among the best in their class, showing how you don&#x2019;t need the biggest model to get the best performance. By focusing on bilingual performance, we produce models that are both better at those languages, easier to adapt, and more cost-effective than large models trained on uncurated data.</p><p>Jina Embeddings are available from <a href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io">Hugging Face</a>, in the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS marketplace</a> for use in Sagemaker, and via the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings web API</a>. They are fully integrated into many AI process frameworks and vector databases.</p><p>See the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings website</a> for more information, or contact us to discuss how Jina AI&#x2019;s offerings can fit into your business processes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 Bilingual Models Are Now Open-Source On Hugging Face"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop]]></title><description><![CDATA[Discover how SceneXplain, our AI tool, transforms images into valuable assets for marketers, advertisers, and e-commerce pros.]]></description><link>https://jina.ai/news/discover-the-hidden-business-value-in-images-with-scenexplain-martech-online-workshop/</link><guid isPermaLink="false">65b225a58da8040001e1739f</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Fri, 26 Jan 2024 15:03:35 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/PP-workshop-slides--260-x-260-px---3000-x-890-px-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/PP-workshop-slides--260-x-260-px---3000-x-890-px-.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><p></p><h2 id="learn-how-ai-transforms-visuals-into-valuable-insights"><strong>Learn How AI Transforms Visuals into Valuable Insights</strong></h2><p>In the rapidly evolving world of Marketing Technology (MarTech), understanding and leveraging the power of images is crucial. <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">SceneXplain</a>, our advanced AI tool, is at the forefront of this revolution, turning mere visuals <a href="https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">into valuable business asset</a><a href="https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">s</a>; it&apos;s about driving tangible value and visibility for your brand. <br><br>This workshop is tailored for marketing professionals, advertisers, e-commerce practitioners, and business owners who are keen to use the potential of AI in their strategies.</p><h2 id="whats-in-store"><strong>What&apos;s in Store?</strong></h2><p>We&apos;ll delve deep into the world of SceneXplain, a state-of-the-art AI tool. You will gain comprehensive insights, explore inspiring real-world applications, and participate in hands-on tutorials to master SceneXplain&apos;s capabilities.</p><p><strong>Date and Time:</strong> Jan. 31st, 6 PM CET<br><strong>Location:</strong> Online. </p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">How To Use AI To Uncover The Business Value Hidden Behind Images &#xB7; Zoom &#xB7; Luma</div><div class="kg-bookmark-description">In the dynamic world of Marketing Technology (MarTech), the intersection of AI and visual content represents a frontier of untapped potential. The power of images extends far beyond mere&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://lu.ma/apple-touch-icon.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><span class="kg-bookmark-author">Miruna</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://social-images.lu.ma/cdn-cgi/image/format=auto,fit=cover,dpr=1,quality=75,width=800,height=419/api/event-one?calendar_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Fcommunity_avatar_5.png&amp;calendar_name&amp;color0=%23060505&amp;color1=%23ab1a49&amp;color2=%23d4baa7&amp;color3=%235e3030&amp;host_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Favatar_29.png&amp;host_name=Miruna&amp;img=https%3A%2F%2Fimages.lumacdn.com%2Fevent-covers%2Fsl%2Fa27afa4f-c693-4c4b-a224-d7466ebffbcc&amp;name=How%20To%20Use%20AI%20To%20Uncover%20The%20Business%20Value%20Hidden%20Behind%20Images" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"></div></a><figcaption><p><span style="white-space: pre-wrap;">Register Here!</span></p></figcaption></figure><h2 id="agenda"><strong>Agenda:</strong></h2><ul><li>A 5-minute introduction to the latest AI Trends in MarTech.</li><li>A 15-minute demonstration of SceneXplain&apos;s advanced features, including <a href="https://jina.ai/news/read-my-pics-you-got-ocr-in-my-visual-question-answering?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Visual Question Answering</a>, <a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io">structured JSON ouput</a>, and API integration followed by a brief Q&amp;A.</li><li>20 minutes of in-depth case studies focusing on AI&apos;s role in social media comprehension and product story generation.</li><li>A 5-minute interactive brainstorming session, offering a platform for engagement with experts.</li></ul><h2 id="why-attend"><strong>Why Attend?</strong></h2><ul><li>&#x200B;<strong>&#x1F4A1; Inspiring Case Studies: </strong>Covering solid use cases from leading media agencies, global FMCG companies, and high-end brands, catering to a diverse business audience.</li><li>&#x200B;&#x1F469;&#x200D;&#x1F3EB; <strong>Hands-on Tutorials: </strong>Our product expert will provide clear guidance on how to explore SceneXplain&apos;s advanced functions, improving your AI professional skills.</li><li><strong>&#x200B;&#x1F381; Gifts: </strong>During the brainstorming session, we have gifts for active participants who share their experience using SceneXplain or thoughts on its future applications.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">How To Use AI To Uncover The Business Value Hidden Behind Images &#xB7; Zoom &#xB7; Luma</div><div class="kg-bookmark-description">In the dynamic world of Marketing Technology (MarTech), the intersection of AI and visual content represents a frontier of untapped potential. The power of images extends far beyond mere&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://lu.ma/apple-touch-icon.png" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"><span class="kg-bookmark-author">Miruna</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://social-images.lu.ma/cdn-cgi/image/format=auto,fit=cover,dpr=1,quality=75,width=800,height=419/api/event-one?calendar_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Fcommunity_avatar_5.png&amp;calendar_name&amp;color0=%23060505&amp;color1=%23ab1a49&amp;color2=%23d4baa7&amp;color3=%235e3030&amp;host_avatar=https%3A%2F%2Fcdn.lu.ma%2Favatars-default%2Favatar_29.png&amp;host_name=Miruna&amp;img=https%3A%2F%2Fimages.lumacdn.com%2Fevent-covers%2Fsl%2Fa27afa4f-c693-4c4b-a224-d7466ebffbcc&amp;name=How%20To%20Use%20AI%20To%20Uncover%20The%20Business%20Value%20Hidden%20Behind%20Images" alt="Discover the Hidden Business Value in Images with SceneXplain | MarTech Online Workshop"></div></a><figcaption><p><span style="white-space: pre-wrap;">Register Here!</span></p></figcaption></figure><h2 id="improve-your-experience"><strong>Improve Your Experience</strong></h2><p>To make the most of this 45-minute workshop, <a href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">we encourage early registration for the event</a>. If you&apos;re new to the tool, our experts will assist you during the workshop.</p><div class="kg-card kg-button-card kg-align-center"><a href="https://lu.ma/qqcpl381?ref=jina-ai-gmbh.ghost.io" class="kg-btn kg-btn-accent">Register Here!</a></div><p>Join us at Jina AI&apos;s MarTech Online Workshop and step into the future of marketing technology with SceneXplain.<br><br>We&apos;re not just exploring AI; we&apos;re defining its role in transforming marketing.</p><p></p>]]></content:encoded></item><item><title><![CDATA[Making Accessibility Accessible: Create Alt Text with SceneXplain's API]]></title><description><![CDATA[SceneXplain is your accessibility ally, making it easy to generate image alt texts to aid visually-impaired users and improve SEO]]></description><link>https://jina.ai/news/make-accessibility-accessible-generate-alt-text-with-scenexplain/</link><guid isPermaLink="false">65af909b8da8040001e16fbb</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 23 Jan 2024 15:00:18 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--14-.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><p>Accessibility (or &quot;a11y&quot; for short) is fast becoming an important part of web development and e-commerce. Back in the day, accessibility aids like <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">alt text</a> or color-blind-friendly color schemes weren&apos;t seen as high priorities by developers and companies. But now, with accessibility legislation from <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Europe</a> and the <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">USA</a>, making your website accessible is more important than ever before.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Alt text, or alternative text, is a brief description of an image used on websites and in digital documents. It helps people who can&apos;t see the image understand what it&apos;s about. This includes people using screen readers because of visual impairments and those with slow internet connections where images don&apos;t load. Alt text is also useful for search engines to understand and index images.</div></div><p>But how can you go about creating alt text for every image on your website? Manually going through each image and writing alt text could take forever, especially if you have thousands (or millions) of images. And if more are being added every day, it becomes a never-ending battle.</p><p>That&#x2019;s where SceneXplain comes in. It&#x2019;s your a11y ally! You can simply upload an image and get alt text for it without having to wrack your brain thinking of the wording yourself.</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Generate_Alt_Text_with_SceneXplain__C5Wt_1HXQo-LONuk8GAqfA?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<p>If you have, say, a few dozen images, this is a good way to give your brain a break. But you still need to do all the clicking and dragging yourself. Your brain wins, but your fingers don&apos;t. And if you have a few thousand images? Call the doctor now to pre-book your <a href="https://en.wikipedia.org/wiki/Repetitive_strain_injury?ref=jina-ai-gmbh.ghost.io">carpal tunnel</a> appointment.</p><p>If only there were a way you could automate the whole thing. Then your brain and fingers could <em>both</em> focus on more interesting things.</p><p>That&apos;s where SceneXplain&apos;s API comes in. You can write a script that will go through your thousands of images, send them in batches to SceneXplain, and generate a CSV file with the results (or with a bit more coding, integrate directly into your workflow.)</p><p>After all, you know what they say. You can&apos;t spell happiness without APIness.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Upon further reflection, I have found that the English language does not, in fact, work like that.</div></div><h2 id="what-is-an-api">What is an API?</h2><p>But before we dive into the <em>how</em>, let&apos;s look at the <em>what</em>. The <a href="https://www.notion.so/Making-Accessibility-Accessible-Generate-Alt-Text-with-SceneXplain-0146ed2dc0de4e5fbe5595aee30967b8?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Oxford English Dictionary</a> defines API as:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="927" height="209" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--50-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--50-.png 927w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Well, that&#x2019;s not useful at all</span></figcaption></figure><p>However, everyone&apos;s favorite AI, GPT-4 defines an API as:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png" class="kg-image" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API" loading="lazy" width="789" height="664" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--51-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--51-.png 789w" sizes="(min-width: 720px) 720px"></figure><p>Or, if you prefer a video explanation:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/s7wmiS2mSXY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="What is an API?"></iframe></figure><p>In short, you can write a Python (or any other language) program to talk to SceneXplain via its API and automate your whole alt-tagging process. We have a Python snippet that does just that.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Can&#x2019;t run the code on your own computer? Keep reading down to the Google Colab notebook which lets you use it in your browser.</div></div><p>Here&#x2019;s how you use it:</p><ol><li>Install the <a href="https://docs.python-requests.org/en/master/index.html?ref=jina-ai-gmbh.ghost.io">requests</a> library:</li></ol><pre><code class="language-bash">pip install requests
</code></pre><ol><li>Go to SceneXplain&#x2019;s API page to generate a secret key and copy it to your clipboard.</li><li>Paste it into the Python code below.</li><li>Copy an image URL into the code where it says <code>....</code>.</li><li>Run the code!</li></ol><pre><code class="language-python">import requests
import json

# generate token on SceneXplain&apos;s API page
YOUR_GENERATED_SECRET = &quot;your_generated_secret_here&quot;

data = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot; # change to image URL
    }
  ]
}

headers = {
  &quot;x-api-key&quot;: f&quot;token {YOUR_GENERATED_SECRET}&quot;,
  &quot;content-type&quot;: &quot;application/json&quot;,
}

response = requests.post(&quot;https://api.scenex.jina.ai/v1/describe&quot;, headers=headers, json=data)
print(response.json())
</code></pre><p>(We&#x2019;ll put in more code snippets later for cURL and JavaScript)</p><h2 id="api-in-action-scenexplain-in-a-notebook">API in action: SceneXplain in a Notebook</h2><p>Since we want to see this in action, we&#x2019;ll use the code live in a <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io">notebook</a>. That lets you see what&#x2019;s happening in real time with real data, and lets you examine and play with the Python code yourself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/api-a11y-alt-text/scenexplain_a11y_alt_texts.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Google Colaboratory</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/colaboratory-static/common/f6c736cbde16632a9fcd16d9ab1970b1/img/favicon.ico" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></div><div class="kg-bookmark-thumbnail"><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure><p>The notebook goes beyond just the simple Python snippet above. It also downloads a sample dataset and exports the results to a CSV file.</p><h2 id="beyond-the-notebook-using-the-api-irl">Beyond the notebook: Using the API IRL</h2><p>Of course, you&#x2019;re not limited to Python when you use SceneXplain&#x2019;s API. Any language that has an HTTP library should work fine.</p><p>Here&#x2019;s that same code snippet from above, this time in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
};

const YOUR_GENERATED_SECRET = &apos;your_generated_secret_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_GENERATED_SECRET}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
}).then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>And this time as a <a href="https://curl.se/?ref=jina-ai-gmbh.ghost.io">cURL</a> command:</p><pre><code class="language-shell">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{
  &quot;data&quot;: [
    {
      &quot;task_id&quot;: &quot;alt_text&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
      &quot;image&quot;: &quot;...&quot;
    }
  ]
}&apos;</code></pre><h2 id="improve-your-image-accessibility-with-scenexplain%E2%80%99s-api">Improve your image accessibility with SceneXplain&#x2019;s API</h2><p>To get started, head over to <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API page</a> to brush up on how it all works, generate a secret key, and then either adapt our notebook or create your own code to start improving accessibility today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Making Accessibility Accessible: Create Alt Text with SceneXplain&apos;s API"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?]]></title><description><![CDATA[Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ]]></description><link>https://jina.ai/news/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/</link><guid isPermaLink="false">65af98d28da8040001e17008</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Tue, 23 Jan 2024 11:22:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">On Jan. 25, 2024, OpenAI released <a href="https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io">a new embedding model</a> with a new feature called <i><b><strong class="italic" style="white-space: pre-wrap;">&quot;shortening&quot;</strong></b></i>, which allows developers to trim embeddings&#x2014;essentially cutting numbers from the sequence&apos;s end&#x2014;without compromising the embedding&apos;s ability to represent concepts effectively. Dive into this post for a solid theoretical foundation on the viability and rationale behind this innovation.</div></div><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><p>Consider this: when measuring the cosine similarity of embedding vectors in high-dimensional spaces, how does their similarity in lower-dimensional subspaces imply the overall similarity? Is there a direct, proportional relationship, or is the reality more complex with high-dimensional data?</p><p>More concretely, <strong>does high similarity between vectors in their first 256 dimensions assure a high similarity in their full 768 dimensions?</strong> Conversely, if vectors significantly differ in some dimensions, does this spell a low overall similarity? These aren&apos;t mere theoretical musings; they are crucial considerations for efficient vector retrieval, database indexing, and the performance of RAG systems.</p><p>Developers often rely on heuristics, assuming that high subspace similarity equates to high overall similarity or that notable differences in one dimension significantly affect the overall similarity. The question is: are these heuristic methods built on firm theoretical ground, or are they simply assumptions of convenience?</p><p>This post delves into these questions, examining the theory and practical implications of subspace similarity in relation to overall vector similarity. </p><h2 id="bounding-the-cosine-similarity">Bounding the Cosine Similarity</h2><p>Given vectors $\mathbf{A}, \mathbf{B}\in \mathbb{R}^d$, we decompose them as $\mathbf{A}=[\mathbf{A}_1, \mathbf{A}_2]$ and $\mathbf{B}=[\mathbf{B}_1, \mathbf{B}_2]$, where $\mathbf{A}_1,\mathbf{B}_1\in\mathbb{R}^m$ and $\mathbf{A}_2,\mathbf{B}_2\in\mathbb{R}^n$, with $m+n=d$.</p><p>The cosine similarity in the subspace $\mathbb{R}^m$ is given by $\cos(\mathbf{A}_1, \mathbf{B}_1)=\frac{\mathbf{A}_1\cdot\mathbf{B}_1}{\|\mathbf{A}_1\|\|\mathbf{B}_1\|}$; similarly, the similarity in the subspace $\mathbb{R}^n$ is $\cos(\mathbf{A}_2, \mathbf{B}_2)=\frac{\mathbf{A}_2\cdot\mathbf{B}_2}{\|\mathbf{A}_2\|\|\mathbf{B}_2\|}$.</p><p>In the original space $\mathbb{R}^d$, the cosine similarity is defined as:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}\\&amp;=\frac{\mathbf{A}_1\cdot\mathbf{B}_1+\mathbf{A}_2\cdot\mathbf{B}_2}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\cos(\mathbf{A}_1, \mathbf{B}_1)\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\cos(\mathbf{A}_2, \mathbf{B}_2)\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\end{align*}$$</p><p>Now, let $s := \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$. Then, we have:$$\begin{align*}\cos(\mathbf{A},\mathbf{B})&amp;\leq\frac{s\|\mathbf{A}_1\|\|\mathbf{B}_1\|+s\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\\&amp;=\frac{\|\mathbf{A}_1\|\|\mathbf{B}_1\|+\|\mathbf{A}_2\|\|\mathbf{B}_2\|}{\sqrt{\|\mathbf{A}_1\|^2+\|\mathbf{A}_2\|^2}\sqrt{\|\mathbf{B}_1\|^2+\|\mathbf{B}_2\|^2}}\cdot s\\&amp;=\cos(\underbrace{[\|\mathbf{A}_1\|, \|\mathbf{A}_2\|]}_{\mathbb{R}^2}, \underbrace{[\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]}_{\mathbb{R}^2})\cdot s\\&amp;\leq 1\cdot s \\&amp;= \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}$$</p><p>End of proof. </p><p>Note that in the final step of the proof, we leverage that the cosine similarity is always less than or equal to 1. This forms our upper bound. Similarly, we can show that the lower bound of \(\cos(\mathbf{A},\mathbf{B})\) is given by: </p><p>\[ \cos(\mathbf{A},\mathbf{B}) \geq t \cdot \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) \], where $t:= \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))$.</p><p>Note that for the lower bound, we can not hastily conclude that \(\cos(\mathbf{A},\mathbf{B}) \geq t\). This is because of the range of the cosine function, which spans between \([-1, 1]\). Due to this range, it&apos;s impossible to establish a tighter lower bound than the trivial value of -1.</p><p>So in conclusion, we have the following loose bound: $$ -1\leq\cos(\mathbf{A},\mathbf{B})\leq\max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)).$$ and a tighter bound \[\begin{align*}  \gamma \cdot t\leq&amp;\cos(\mathbf{A}, \mathbf{B}) \leq\gamma\cdot s\\\gamma \cdot \min(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2)) \leq &amp;\cos(\mathbf{A}, \mathbf{B}) \leq \gamma \cdot \max(\cos(\mathbf{A}_1, \mathbf{B}_1), \cos(\mathbf{A}_2, \mathbf{B}_2))\end{align*}\], where $\gamma = \cos([\|\mathbf{A}_1\|, \|\mathbf{A}_2\|], [\|\mathbf{B}_1\|, \|\mathbf{B}_2\|]) $.</p><h3 id="connection-to-johnson%E2%80%93lindenstrauss-lemma">Connection to Johnson&#x2013;Lindenstrauss Lemma</h3><p>The JL lemma asserts that for any \(0 &lt; \epsilon &lt; 1\) and any finite set of points \( S \) in \( \mathbb{R}^d \), there exists a mapping \( f: \mathbb{R}^d \rightarrow \mathbb{R}^k \) (with \( k = O(\epsilon^{-2} \log |S|) \)) such that for all \( \mathbf{u}, \mathbf{v} \in S \), the Euclidean distances are approximately preserved:<br><br>\[(1 - \epsilon) \|\mathbf{u} - \mathbf{v}\|^2 \leq \|f(\mathbf{u}) - f(\mathbf{v})\|^2 \leq (1 + \epsilon) \|\mathbf{u} - \mathbf{v}\|^2\]</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Johnson&#x2013;Lindenstrauss lemma - Wikipedia</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://en.wikipedia.org/static/apple-touch/wikipedia.png" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"><span class="kg-bookmark-author">Wikimedia Foundation, Inc.</span><span class="kg-bookmark-publisher">Contributors to Wikimedia projects</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?"></div></a></figure><p>To make $f$ work like a subspace selection, we can use a diagonal matrix for projection, such as a \(5 \times 3\) matrix \(f\), albeit not random (note, the typical formulation of the JL lemma involves linear transformations that often utilize random matrices drawn from a Gaussian distribution). For instance, if we aim to retain the 1st, 3rd, and 5th dimensions from a 5-dimensional vector space, the matrix \(f\) could be designed as follows: \[f = \begin{bmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1\end{bmatrix}\]<br>However, by specifying $f$ to be diagonal, we limit the class of functions that can be used for the projection. The JL lemma guarantees the existence of a suitable $f$ within the broader class of linear transformations, but when we restrict $f$ to be diagonal, such a suitable $f$ may not exist within this restricted class for applying the JL lemma&apos;s bounds.</p><h2 id="validating-the-bounds">Validating the Bounds</h2><p>To empirically explore the theoretical bounds on cosine similarity in high-dimensional vector spaces, we can employ a Monte Carlo simulation. This method allows us to generate a large number of random vector pairs, compute their similarities in both the original space and subspaces, and then assess how well the theoretical upper and lower bounds hold in practice.</p><p>The following Python code snippet implements this concept. It randomly generates pairs of vectors in a high-dimensional space and computes their cosine similarity. Then, it divides each vector into two subspaces, calculates the cosine similarity within each subspace, and evaluates the upper and lower bounds of the full-dimensional cosine similarity based on the subspace similarities.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import numpy as np


def compute_cosine_similarity(U, V):
    # Normalize the rows to unit vectors
    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)
    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)
    # Compute pairwise cosine similarity
    return np.sum(U_norm * V_norm, axis=1)


# Generate random data
num_points = 5000
d = 1024
A = np.random.random([num_points, d])
B = np.random.random([num_points, d])

# Compute cosine similarity between A and B
cos_sim = compute_cosine_similarity(A, B)

# randomly divide A and B into subspaces
m = np.random.randint(1, d)
A1 = A[:, :m]
A2 = A[:, m:]
B1 = B[:, :m]
B2 = B[:, m:]

# Compute cosine similarity in subspaces
cos_sim1 = compute_cosine_similarity(A1, B1)
cos_sim2 = compute_cosine_similarity(A2, B2)

# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2
s = np.maximum(cos_sim1, cos_sim2)
t = np.minimum(cos_sim1, cos_sim2)

norm_A1 = np.linalg.norm(A1, axis=1)
norm_A2 = np.linalg.norm(A2, axis=1)
norm_B1 = np.linalg.norm(B1, axis=1)
norm_B2 = np.linalg.norm(B2, axis=1)

# Form new vectors in R^2 from the norms
norm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)
norm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)

# Compute cosine similarity in R^2
gamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)

# print some info and validate the lower bound and upper bound
print(&apos;d: %d\n&apos;
      &apos;m: %d\n&apos;
      &apos;n: %d\n&apos;
      &apos;avg. cosine(A,B): %f\n&apos;
      &apos;avg. upper bound: %f\n&apos;
      &apos;avg. lower bound: %f\n&apos;
      &apos;lower bound satisfied: %s\n&apos;
      &apos;upper bound satisfied: %s&apos; % (
          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),
          np.all(gamma * t &lt;= cos_sim)))
</code></pre><figcaption><p><span style="white-space: pre-wrap;">A Monte Carlo validator for validating cosine similarity bounds</span></p></figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-output">d: 1024
m: 743
n: 281
avg. cosine(A,B): 0.750096
avg. upper bound: 0.759080
avg. lower bound: 0.741200
lower bound satisfied: True
upper bound satisfied: True</code></pre><figcaption><p><span style="white-space: pre-wrap;">A sample output from our Monte Carlo validator. It&apos;s important to note that the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>lower/upper bound satisfied</span></code><span style="white-space: pre-wrap;"> condition is checked for every vector individually. Meanwhile, the </span><code spellcheck="false" style="white-space: pre-wrap;"><span>avg. lower/upper bound</span></code><span style="white-space: pre-wrap;"> provides a more intuitive overview of the statistics related to these bounds but doesn&apos;t directly influence the validation process.</span></p></figcaption></figure><h2 id="understanding-the-bounds">Understanding the Bounds</h2><p>In a nutshell, when comparing two high-dimensional vectors, the overall similarity lies between the best and worst similarities of their subspaces, adjusted for how large or important those subspaces are in the overall scheme. This is what the bounds for cosine similarity in higher dimensions intuitively represent: the balance between the most and least similar parts, weighted by their relative sizes or importance.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png" class="kg-image" alt="Does Subspace Cosine Similarity Imply High-Dimensional Cosine Similarity?" loading="lazy" width="1200" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Each pen has two main components: the body and the cap.</span></figcaption></figure><p>Imagine you&apos;re trying to compare two multi-part objects (let&apos;s say, two fancy pens) based on their overall similarity. Each pen has two main components: the body and the cap. The similarity of the whole pen (both body and cap) is what we&apos;re trying to determine:</p><h3 id="upper-bound-gamma-cdot-s">Upper Bound ($\gamma \cdot s$)</h3><p>Think of $s$ as the best match between corresponding parts of the pens. If the caps are very similar but the bodies aren&apos;t, $s$ is the similarity of the caps.</p><p>Now, $\gamma$ is like a scaling factor based on the size (or importance) of each part. If one pen has a very long body and a short cap, while the other has a short body and a long cap, $\gamma$ adjusts the overall similarity to account for these differences in proportions.</p><p>The upper bound tells us that no matter how similar some parts are, the overall similarity can&apos;t exceed this &quot;best part similarity&quot; scaled by the proportion factor.</p><h3 id="lower-bound-gamma-cdot-t">Lower Bound ($\gamma \cdot t$)</h3><p>Here, $t$ is the similarity of the least matching parts. If the bodies of the pens are quite different but the caps are similar, $t$ reflects the body&apos;s similarity.</p><p>Again, $\gamma$ scales this based on the proportion of each part.</p><p>The lower bound means that the overall similarity can&apos;t be worse than this &quot;worst part similarity&quot; after accounting for the proportion of each part.</p><h2 id="implications-of-the-bounds">Implications of the Bounds</h2><p>For software engineers working with embeddings, vector search, retrieval, or databases, understanding these bounds has practical implications, particularly when dealing with high-dimensional data. Vector search often involves finding the closest (most similar) vectors in a database to a given query vector, typically using cosine similarity as a measure of closeness. The bounds we discussed can provide insights into the effectiveness and limitations of using subspace similarities for such tasks.</p><h3 id="using-subspace-similarity-for-ranking">Using Subspace Similarity for Ranking</h3><p><strong>Safety and Accuracy</strong>: Using subspace similarity for ranking and retrieving top-k results can be effective, but with caution. The upper bound indicates that the overall similarity can&apos;t exceed the maximum similarity of the subspaces. Thus, if a pair of vectors is highly similar in a particular subspace, it&apos;s a strong candidate for being similar in the high-dimensional space.</p><p><strong>Potential Pitfalls</strong>: However, the lower bound suggests that two vectors with low similarity in one subspace could still be quite similar overall. Therefore, relying solely on subspace similarity might miss some relevant results.</p><h3 id="misconceptions-and-cautions">Misconceptions and Cautions</h3><p><strong>Overestimating Subspace Importance</strong>: A common misconception is overestimating the importance of a particular subspace. While high similarity in one subspace is a good indicator, it doesn&apos;t guarantee high overall similarity due to the influence of other subspaces.</p><p><strong>Ignoring Negative Similarities</strong>: In cases where the cosine similarity in a subspace is negative, it indicates an opposing relationship in that dimension. Engineers should be wary of how these negative similarities impact the overall similarity.</p>]]></content:encoded></item><item><title><![CDATA[Using Jina Embeddings v2 with Haystack Pipelines]]></title><description><![CDATA[Access Jina AI's state-of-the-art open-source embedding models in your Haystack application pipeline.]]></description><link>https://jina.ai/news/jina-embeddings-v2-and-deepset-haystack/</link><guid isPermaLink="false">65a916c40bab3100012d2e8c</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Saahil Ognawala]]></dc:creator><pubDate>Fri, 19 Jan 2024 15:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/haystack_embeddings.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"><p><a href="https://www.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Deepset</a> has integrated <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> into its industry-leading <a href="https://haystack.deepset.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Haystack</a> NLP framework. You can now access Jina AI&apos;s state-of-the-art open-source embedding models in your Haystack pipeline.</p><p>We have collaborated with Deepset to bring you a tutorial on using&#xA0;Jina Embeddings v2&#xA0;in Haystack to analyze legal documents. Follow the link below to Deepset&apos;s blog to see how you can create a retrieval-augmented generation (RAG) application using Jina AI&apos;s industry-leading embeddings and Haystack!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Using Jina Embeddings v2 with Haystack 2.0 pipelines to summarize legal documents | Haystack</div><div class="kg-bookmark-description">Learn how to use the Jina v2 Embedding models in a RAG pipeline with our new Haystack integration.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://haystack.deepset.ai/favicon.ico" alt="Using Jina Embeddings v2 with Haystack Pipelines"><span class="kg-bookmark-author">Haystack</span><span class="kg-bookmark-publisher">Tilde Thurium Senior Developer Advocate</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://haystack.deepset.ai/blog/using-jina-embeddings-haystack/thumbnail.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Using Jina Embeddings v2 with Haystack Pipelines"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length]]></title><description><![CDATA[Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.]]></description><link>https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/</link><guid isPermaLink="false">65a542040bab3100012d2d79</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 15 Jan 2024 16:28:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><p><strong>Berlin, Germany - January 15, 2023</strong>&#xA0;&#x2013; Echoing JFK&apos;s iconic &apos;Ich bin ein Berliner&apos;, at Jina AI we&apos;re thrilled to bridge languages in our own way. Today, we&apos;re proud to announce our latest innovation: <code>jina-embeddings-v2-base-de</code>, a German/English embedding model. This state-of-the-art bilingual model is a significant stride forward in language representation, boasting a context length of <strong>8,192 tokens</strong>. What sets it apart is its remarkable efficiency: it achieves top-tier performance while being <strong>only 1/7th the size</strong> of comparable models.</p><p>Embeddings are crucial for German businesses looking to expand into the U.S. market. According to the <a href="https://www.gaccny.com/en/media/press-releases/news-details/german-american-business-outlook-gabo-2022-companies-in-the-us-remain-profitable?ref=jina-ai-gmbh.ghost.io">German American Business Outlook (GABO) 2022</a>, approximately a third of German companies generate over 20% of their global sales and profits in the U.S., with 93% expecting an increase in U.S. sales&#x200B;&#x200B;. This trend continues as 93% plan to grow their company&apos;s U.S. investments in the next three years, with <a href="https://www.globenewswire.com/news-release/2023/02/09/2604561/0/en/SURVEY-SHOWS-GERMAN-COMPANIES-IN-THE-US-PROFIT-FROM-ROBUST-MARKET-SIZE-AND-CUSTOMER-DEMAND.html?ref=jina-ai-gmbh.ghost.io">85% expecting net sales growth and a significant focus on digital transformation</a>&#x200B;&#x200B;. Good embeddings can play a pivotal role in this expansion by facilitating better understanding of customer preferences, enabling more effective communication, and positioning culturally resonant products.</p><p>Our breakthrough is particularly beneficial for German businesses looking to implement bilingual applications in English-speaking countries. With <code>jina-embeddings-v2-base-de</code>, we&apos;re excited to see how German companies will innovate and thrive in an increasingly connected world.</p><h2 id="model-highlights">Model Highlights</h2><ul><li><strong>State-of-the-art Performance</strong>: <code>jina-embeddings-v2-base-de</code> consistently ranking at the top in relevant benchmarks and leading among open-source models of similar size.</li><li><strong>Bilingual Model:</strong>&#xA0;This model encodes texts in both German and English, allowing the use of either language as the query or target document in retrieval applications. Texts with equivalent meanings in both languages are mapped to the same embedding space, forming the basis for multilingual applications.</li><li><strong>Extended Context</strong>: An 8192-token length enables <code>jina-embeddings-v2-base-de</code> to support longer texts and document fragments, far surpassing models that only support a few hundred tokens at a time.</li><li><strong>Compact Size</strong>: <code>jina-embeddings-v2-base-de</code> is built for high performance on standard computer hardware. With only 161 million parameters, the entire model is 322MB and fits in the memory of commodity computers. The embeddings themselves are 768 dimensions, a relatively small vector size compared to many models, saving space and run-time for applications.</li><li><strong>Bias Minimization</strong>: <a href="https://aclanthology.org/2023.findings-eacl.89.pdf?ref=jina-ai-gmbh.ghost.io">Recent research</a> shows that multilingual models without specific language training show strong biases towards English grammatical structures in embeddings. Embedding models should be about capturing meaning and not favor sentence pairs that are merely superficially similar.</li><li><strong>Seamless Integration</strong>: Jina Embeddings v2 models have native integrations with major vector databases, including <a href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io">MongoDB</a>, <a href="https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io">Qdrant</a>, and <a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-jinaai?ref=jina-ai-gmbh.ghost.io">Weaviate</a>, as well as RAG and LLM frameworks such as <a href="https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io">Haystack</a> and <a href="https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings.html?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a>.</li></ul><h2 id="leading-performance-in-german-nlp">Leading Performance in German NLP</h2><p>We&apos;ve put <code>jina-embeddings-v2-base-de</code> to the test against four renowned baselines that also support both German and English. These include:</p><ul><li><a href="https://huggingface.co/intfloat/multilingual-e5-large?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-large</a> and <a href="https://huggingface.co/intfloat/multilingual-e5-base?ref=jina-ai-gmbh.ghost.io">Multilingual-E5-base</a> from Microsoft</li><li>T-Systems&#x2019; <a href="https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer?ref=jina-ai-gmbh.ghost.io">Cross English &amp; German RoBERTa for Sentence Embeddings</a></li><li><a href="https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2?ref=jina-ai-gmbh.ghost.io">Sentence-BERT</a> (<code>distiluse-base-multilingual-cased-v2</code>)</li></ul><p>Our benchmarks include <a href="https://huggingface.co/mteb?ref=jina-ai-gmbh.ghost.io">the MTEB tasks for English</a> and our own custom benchmark. Given the lack of a comprehensive benchmark suite for German embeddings, we took the initiative to <a href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io">develop our own</a>, inspired by the MTEB. We&apos;re proud to share our findings and breakthroughs with you here.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/mteb-de?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - jina-ai/mteb-de: MTEB: Massive Text Embedding Benchmark</div><div class="kg-bookmark-description">MTEB: Massive Text Embedding Benchmark. Contribute to jina-ai/mteb-de development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/a1d1679b83d58c1685db0d4d12d6bb6360edfe08fb0c4a9ebda71b57379853f3/jina-ai/mteb-de" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1310" height="925" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-4.png 1310w" sizes="(min-width: 1200px) 1200px"></figure><h3 id="compact-size-superior-results">Compact Size, Superior Results</h3><p><code>jina-embeddings-v2-base-de</code> demonstrates exceptional performance, especially in German language tasks. It outshines the E5 base model while being less than a third of its size. Moreover, it stands toe-to-toe with the E5 large model, which is seven times larger, showcasing its efficiency and power. This efficiency makes <code>jina-embeddings-v2-base-de</code> a game-changer, particularly when compared to other popular bi- and multilingual embedding models.</p><h3 id="excelling-in-german-english-cross-language-retrieval">Excelling in German-English Cross-Language Retrieval</h3><p>Our model isn&apos;t just about size and efficiency; it&apos;s also a top performer in English-German cross-language retrieval tasks. This is evident in its performance in various key benchmarks:</p><ul><li><a href="https://www.cl.uni-heidelberg.de/statnlpgroup/wikiclir/?ref=jina-ai-gmbh.ghost.io">WikiCLIR</a>, for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts17-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS17</a>, part of the MTEB evaluation for English to German retrieval</li><li><a href="https://huggingface.co/datasets/mteb/sts22-crosslingual-sts?ref=jina-ai-gmbh.ghost.io">STS22</a>, for German to English retrieval, also part of MTEB</li><li><a href="https://huggingface.co/datasets/mteb/bucc-bitext-mining?ref=jina-ai-gmbh.ghost.io">BUCC</a>, for German to English retrieval, included in MTEB</li></ul><p>The performance in these benchmarks, particularly in the MTEB evaluation tests (with the exception of WikiCLIR), underscores the effectiveness of <code>jina-embeddings-v2-base-de</code> in handling complex bilingual tasks.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="1275" height="625" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-6.png 1275w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="get-api-access">Get API Access</h2><p>Our offerings for our enterprise users who value privacy and data compliance, including <code>jina-embeddings-v2-base-de</code>, are accessible via the Jina Embeddings API:</p><ol><li>Visit <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a> and click on the model dropdown</li><li>Select <code>jina-embeddings-v2-base-de</code></li></ol><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-3.png" class="kg-image" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length" loading="lazy" width="2000" height="1062" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/01/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>We will make this model available in the AWS Sagemaker marketplace for Amazon cloud users and for download on HuggingFace very soon.</p><h2 id="jina-8k-embeddings-the-cornerstone-of-diverse-ai-applications">Jina 8K Embeddings: The Cornerstone of Diverse AI Applications</h2><p>Embeddings are crucial for a wide range of AI applications, including information retrieval, data quality control, classification, and recommendation. They are fundamental to enhancing numerous AI tasks.</p><p>Jina AI is committed to advancing the state-of-the-art in embedding technology, keeping our core AI components transparent, accessible, and affordable to enterprises of all types and sizes that value privacy and data compliance. In addition to <code>jina-embeddings-v2-base-de</code>, Jina AI has released state-of-the-art embedding models for <a href="https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io">Chinese</a> and high-performance <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">English monolingual models</a>. This is part of our mission to make AI technology more inclusive and globally applicable.</p><p>We value your feedback. Join our community channel to contribute feedback and stay informed about our advancements. Together, we&apos;re shaping a more robust and inclusive AI future.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4232 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English]]></title><description><![CDATA[The first bilingual Chinese-English embedding model with 8192 token-length.]]></description><link>https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/</link><guid isPermaLink="false">659d729f0bab3100012d2c85</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 18:58:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><p>Following the remarkable success of the previous <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings V2</a>, we are thrilled to announce the launch of our latest Chinese/English bilingual text embedding model: <code>jina-embeddings-v2-base-zh</code>. This new model inherits the exceptional 8K token length of Jina Embeddings V2, now with robust support for both Chinese and English languages.</p><p><code>jina-embeddings-v2-base-zh</code> stands out for its exceptional quality and performance, achieved through rigorous and balanced pre-training with high-quality bilingual data. This approach ensures a significant reduction in bias, often seen in models trained with unbalanced multilingual data.</p><h2 id="highlights">Highlights</h2><ul><li><strong>Bilingual Model:</strong> This model encodes texts in both English and Chinese, allowing the use of either language as the query or target document. Texts with equivalent meanings in these languages are mapped to the same embedding space, forming the basis for numerous multilingual applications.</li><li><strong>Extended 8K Token-Length:</strong> Our model is capable of processing significantly large text passages, a feature that exceeds the capabilities of most other open-source models.</li><li><strong>Compact and Efficient:</strong> With a size of 322MB (161 million parameters) and output dimensions of 768, our model is designed for high performance on standard computer hardware without GPU, enhancing its accessibility.</li></ul><h2 id="leading-performance-on-c-mteb">Leading Performance on C-MTEB</h2><p>In the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">Chinese MTEB leaderboard</a>, our Jina Embeddings v2, supporting both Chinese and English, stands out as one of the top models<strong> under 0.5GB</strong>. What sets it apart is its impressive 8K token-length capability, a unique feature in its category.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>Among Chinese models of similar size, only the E5 Multilingual model and our <code>jina-embeddings-v2-base-zh</code> offer support for English, enabling effective cross-language applications. Notably, Jina demonstrates significantly superior performance in all categories involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>While both models have an 8K token context size, <code>jina-embeddings-v2-base-zh</code> significantly outperforms OpenAI&apos;s <code>text-embedding-ada-002</code>,  especially in tasks involving the Chinese language.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h2 id="empowering-chinese-enterprises-for-global-expansion">Empowering Chinese Enterprises for Global Expansion</h2><p>Our Chinese-English embedding model is a powerful tool for Chinese companies looking to &apos;go global&apos; (&#x51FA;&#x6D77;). It seamlessly processes Chinese texts, providing high-quality embeddings that effortlessly integrate with leading vector databases, search systems, RAG applications.</p><p><code>jina-embeddings-v2-base-zh</code> is especially beneficial for developing AI applications tailored to Chinese-English contexts, crucial for businesses expanding internationally. Here are some specific use cases:</p><ol><li><strong>Document Analysis and Management</strong>: It can analyze and manage a vast array of documents, aiding in international legal and business transactions.</li><li><strong>AI-Powered Search Applications</strong>: Enhances search functions in multilingual environments, making it easier for global users to find relevant information in Chinese and English.</li><li><strong>Retrieval-Augmented Chatbots and Question-Answering</strong>: Builds efficient, bilingual customer service bots, improving interactions with customers worldwide.</li><li><strong>Natural Language Processing Applications</strong>: This includes sentiment analysis for understanding global market trends, topic modeling for international marketing strategies, and text classification for managing global communication.</li><li><strong>Recommender Systems</strong>: Tailors product and content recommendations for diverse global audiences, using insights drawn from Chinese and English data.</li></ol><p>By leveraging this model, Chinese enterprises can effectively bridge the language gap in their AI applications, enhancing their global competitiveness and market reach.</p><h2 id="get-started-with-jina-embeddings-v2-base-zh-via-api">Get Started with <code>jina-embeddings-v2-base-zh</code> via API</h2><p>Begin integrating our model into your workflow immediately through the Embeddings API. Simply visit the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">our Embeddings portal</a>, get your free access key or top up an existing key, and then choose <code>jina-embeddings-v2-base-zh</code> from the dropdown menu. It&apos;s that easy to get started!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1202" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--57-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--57-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Untitled--57-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--57-.png 2060w" sizes="(min-width: 720px) 720px"></figure><h2 id="whats-next-expanding-language-support-and-aws-sagemaker-integration">What&apos;s Next: Expanding Language Support and AWS Sagemaker Integration</h2><p><code>jina-embeddings-v2-base-zh</code> will soon be available via AWS Sagemaker and Hugging Face.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>At Jina AI, our commitment to being a leader in affordable and accessible embedding technology for a global audience is unwavering. We&apos;re actively developing additional multilingual offerings, focusing on major European and other international languages, to broaden our reach. Stay tuned for these exciting updates, including integration with AWS SageMaker, as we continue to expand our capabilities.</p><h2 id="a-special-thanks-to-our-early-testers">A Special Thanks to Our Early Testers</h2><p>We&apos;re immensely grateful to the select members of our Chinese user community who tested the preview version (<code>jina-embeddings-v2-base-zh-preview</code>). Their insightful feedback was crucial in enhancing the official release&apos;s performance for this release. If you have any observations or suggestions regarding the quality of our models, we warmly invite you to join our Discord server and share your thoughts with us. Your input is invaluable in our continuous improvement journey.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">Improved Score Distribution vs. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2-base-zh-preview</span></code><span style="white-space: pre-wrap;"> suffered from inflated similarity scores, leading to high cosine scores even for unrelated items. This was particularly evident in the top-5 results from the screenshot below. The similarity scores were consistently high and did not accurately reflect the true relationship between items. For example, the comparison between &#x201C;&#x5B89;&#x59AE;&#x201D; and &#x201C;&#x84B8;&#x6C7D;&#x673A;&#x201D; received misleadingly high similarity scores.</span></p><p><span style="white-space: pre-wrap;">In the official release, we have fine-tuned the model to produce more distinct and logical similarity scores, ensuring a more accurate representation of the relationships between items. For example, the revised scoring now presents a broader range, offering a clearer insight into the relative similarity among items.</span></p><p><span style="white-space: pre-wrap;">Additionally, Jina Embeddings now uniquely stands as the only open-source embedding model supporting 8192 tokens. This feature highlights its capability in processing a wide variety of data types, from extensive documents to brief phrases, or even individual words/names such as &#x201C;&#x5B89;&#x59AE;&#x201D; vs &#x201C;&#x9732;&#x5A1C;&#x201D;.</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure><p></p><h2 id="%E4%B8%AD%E8%8B%B1%E5%8F%8C%E8%AF%AD8k%E5%90%91%E9%87%8F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B0%E9%B2%9C%E5%87%BA%E7%82%89%EF%BC%8C%E4%BC%81%E4%B8%9A%E5%87%BA%E6%B5%B7%E5%BF%85%E5%A4%87%EF%BC%81">&#x4E2D;&#x82F1;&#x53CC;&#x8BED;8K&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#x65B0;&#x9C9C;&#x51FA;&#x7089;&#xFF0C;&#x4F01;&#x4E1A;&#x51FA;&#x6D77;&#x5FC5;&#x5907;&#xFF01;</h2><p>&#x81EA;&#x4ECE;&#x6211;&#x4EEC;&#x7684; Embeddings V2 &#x83B7;&#x5F97;&#x5404;&#x754C;&#x597D;&#x8BC4;&#x540E;&#xFF0C;&#x4ECA;&#x65E5;&#xFF0C;&#x6211;&#x4EEC;&#x63A8;&#x51FA;&#x4E86;&#x5168;&#x65B0;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#x5411;&#x91CF;&#x5927;&#x6A21;&#x578B;&#xFF1A;jina-embeddings-v2-base-zh&#x3002;&#x6B64;&#x6A21;&#x578B;&#x4E0D;&#x4EC5;&#x7EE7;&#x627F;&#x4E86; V2 &#x7684;&#x5168;&#x90E8;&#x4F18;&#x52BF;&#xFF0C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x957F;&#x8FBE;&#x516B;&#x5343;&#x8BCD;&#x5143;&#x7684;&#x6587;&#x672C;&#xFF0C;&#x66F4;&#x80FD;&#x6D41;&#x7545;&#x5E94;&#x5BF9;&#x4E2D;&#x82F1;&#x6587;&#x53CC;&#x8BED;&#x5185;&#x5BB9;&#xFF0C;&#x4E3A;&#x8DE8;&#x8BED;&#x79CD;&#x7684;&#x5E94;&#x7528;&#x63D2;&#x4E0A;&#x4E86;&#x7FC5;&#x8180;&#x3002;</p><p>jina-embeddings-v2-base-zh &#x4E4B;&#x6240;&#x4EE5;&#x8868;&#x73B0;&#x5353;&#x8D8A;&#xFF0C;&#x5168;&#x8D56;&#x4F18;&#x8D28;&#x7684;&#x53CC;&#x8BED;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x7ECF;&#x8FC7;&#x6211;&#x4EEC;&#x4E25;&#x683C;&#x4E14;&#x5E73;&#x8861;&#x7684;&#xA0;<strong>&#x9884;&#x8BAD;&#x7EC3;&#x3001;&#x4E00;&#x9636;&#x5FAE;&#x8C03;&#x548C;&#x4E8C;&#x9636;&#x5FAE;&#x8C03;</strong>&#x3002;&#x8FD9;&#x79CD;&#x4E09;&#x6B65;&#x8D70;&#x7684;&#x8BAD;&#x7EC3;&#x8303;&#x5F0F;&#x4E0D;&#x4EC5;&#x6CDB;&#x5316;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x53CC;&#x8BED;&#x80FD;&#x529B;&#xFF0C;&#x66F4;&#x6709;&#x6548;&#x7684;&#x964D;&#x4F4E;&#x4E86;&#x6A21;&#x578B;&#x504F;&#x89C1;&#xFF0C;&#x89E3;&#x51B3;&#x4E86;&#x591A;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x65F6;&#x5E38;&#x906D;&#x9047;&#x5230;&#x7684;&#x201C;&#x4E0D;&#x60A3;&#x5BE1;&#x800C;&#x60A3;&#x4E0D;&#x5747;&#x201D;&#x7684;&#x95EE;&#x9898;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E7%89%B9%E8%89%B2%E4%B8%80%E8%A7%88"><strong>&#x6A21;&#x578B;&#x7279;&#x8272;&#x4E00;&#x89C8;</strong></h3><p><strong>&#x7279;&#x8272; 1&#xFF1A;&#x53CC;&#x8BED;&#x65E0;&#x7F1D;&#x5BF9;&#x63A5;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x80FD;&#x591F;&#x6D41;&#x7545;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x4F5C;&#x4E3A;&#x641C;&#x7D22;&#x67E5;&#x8BE2;&#x8FD8;&#x662F;&#x76EE;&#x6807;&#x6587;&#x6863;&#x3002;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#x4E2D;&#x610F;&#x4E49;&#x76F8;&#x8FD1;&#x7684;&#x5185;&#x5BB9;&#x90FD;&#x4F1A;&#x88AB;&#x6620;&#x5C04;&#x5230;&#x76F8;&#x540C;&#x7684;&#x5411;&#x91CF;&#x7A7A;&#x95F4;&#xFF0C;&#x4E3A;&#x591A;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x5960;&#x5B9A;&#x4E86;&#x575A;&#x5B9E;&#x57FA;&#x7840;&#x3002;</p><p><strong>&#x7279;&#x8272; 2&#xFF1A;8k Token &#x8D85;&#x957F;&#x6587;&#x672C;&#x652F;&#x6301;</strong></p><p>&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x652F;&#x6301;&#x957F;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x8FD9;&#x5728;&#x5F00;&#x6E90;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x4E2D;&#x72EC;&#x6811;&#x4E00;&#x5E1C;&#xFF0C;&#x5728;&#x5904;&#x7406;&#x66F4;&#x957F;&#x7684;&#x6587;&#x672C;&#x6BB5;&#x843D;&#x4E0A;&#x63D0;&#x4F9B;&#x4E86;&#x663E;&#x8457;&#x4F18;&#x52BF;&#x3002;</p><p><strong>&#x7279;&#x8272; 3&#xFF1A;&#x9AD8;&#x6548;&#x7D27;&#x51D1;&#x7684;&#x6A21;&#x578B;&#x7ED3;&#x6784;</strong></p><p>jina-embeddings-v2-base-zh &#x6A21;&#x578B;&#x4EE5; 322MB &#x7684;&#x8F7B;&#x5DE7;&#x4F53;&#x79EF;&#xFF08;&#x5305;&#x542B; 1.61 &#x4EBF;&#x53C2;&#x6570;&#xFF09;&#xFF0C;&#x8F93;&#x51FA;&#x7EF4;&#x5EA6;&#x4E3A; 768&#xFF0C;&#x80FD;&#x591F;&#x5728;&#x666E;&#x901A;&#x8BA1;&#x7B97;&#x673A;&#x786C;&#x4EF6;&#x4E0A;&#x9AD8;&#x6548;&#x8FD0;&#x884C;&#xFF0C;&#x65E0;&#x9700;&#x4F9D;&#x8D56; GPU&#xFF0C;&#x6781;&#x5927;&#x5730;&#x63D0;&#x5347;&#x4E86;&#x5176;&#x5B9E;&#x7528;&#x6027;&#x548C;&#x4FBF;&#x6377;&#x6027;&#x3002;</p><h3 id="%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%8D%93%E8%B6%8A"><strong>&#x6A21;&#x578B;&#x6027;&#x80FD;&#x5353;&#x8D8A;</strong></h3><p>&#x5728; CMTEB &#x6392;&#x884C;&#x699C;&#x7684;&#x6FC0;&#x70C8;&#x7ADE;&#x4E89;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684; Jina Embeddings v2 &#x6A21;&#x578B;&#x5728; 0.5GB &#x4EE5;&#x4E0B;&#x6A21;&#x578B;&#x7C7B;&#x522B;&#x4E2D;&#x8131;&#x9896;&#x800C;&#x51FA;&#xFF0C;&#x5B83;&#x4E0D;&#x4EC5;&#x652F;&#x6301;&#x4E2D;&#x82F1;&#x6587;&#x672C;&#xFF0C;&#x800C;&#x4E14;&#x80FD;&#x591F;&#x5904;&#x7406;&#x9AD8;&#x8FBE; 8K Token &#x7684;&#x6587;&#x672C;&#xFF0C;&#x8FD9;&#x4E00;&#x80FD;&#x529B;&#x5728;&#x540C;&#x7C7B;&#x6A21;&#x578B;&#x4E2D;&#x5B9E;&#x5C5E;&#x7F55;&#x89C1;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1484" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image.png 1484w" sizes="(min-width: 720px) 720px"></figure><p>&#x5728;&#x540C;&#x7B49;&#x4F53;&#x79EF;&#x7684;&#x652F;&#x6301;&#x4E2D;&#x6587;&#x7684;&#x6A21;&#x578B;&#x4E2D;&#xFF0C;Multilingual E5 &#x548C;&#x6211;&#x4EEC;&#x7684; jina-embeddings-v2-base-zh &#x662F;&#x552F;&#x4E8C;&#x80FD;&#x591F;&#x5904;&#x7406;&#x82F1;&#x6587;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x8FD9;&#x4F7F;&#x5F97;&#x8DE8;&#x8BED;&#x8A00;&#x5E94;&#x7528;&#x6210;&#x4E3A;&#x53EF;&#x80FD;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1528" height="292" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-1.png 1528w" sizes="(min-width: 720px) 720px"></figure><p>&#x76EE;&#x524D;&#xFF0C;&#x5168;&#x7403;&#x8303;&#x56F4;&#x5185;&#xFF0C;&#x4EC5;&#x6709; OpenAI &#x7684;&#x95ED;&#x6E90;&#x6A21;&#x578B; text-embedding-ada-002 &#x548C; Jina Embeddings &#x80FD;&#x591F;&#x652F;&#x6301; 8k Token &#x7684;&#x957F;&#x6587;&#x672C;&#x8F93;&#x5165;&#x3002;&#x800C;&#x5728;&#x5904;&#x7406;&#x4E2D;&#x6587;&#x4EFB;&#x52A1;&#x65B9;&#x9762;&#xFF0C;Jina Embeddings &#x663E;&#x793A;&#x51FA;&#x4E86;&#x663E;&#x8457;&#x7684;&#x6027;&#x80FD;&#x4F18;&#x52BF;&#x3002;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1348" height="252" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-2.png 1348w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%8A%A9%E5%8A%9B%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A%E6%8B%93%E5%B1%95%E5%85%A8%E7%90%83%E4%B8%9A%E5%8A%A1">&#x52A9;&#x529B;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x62D3;&#x5C55;&#x5168;&#x7403;&#x4E1A;&#x52A1;</h3><p>&#x6211;&#x4EEC;&#x7684;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B; jina-embeddings-v2-base-zh &#x662F;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x8FDB;&#x519B;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7684;&#x5F3A;&#x5927;&#x4F19;&#x4F34;&#x3002;&#x5B83;&#x80FD;&#x591F;&#x65E0;&#x7F1D;&#x5904;&#x7406;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x6587;&#x672C;&#xFF0C;&#x5E76;&#x63D0;&#x4F9B;&#x9AD8;&#x8D28;&#x91CF;&#x7684;&#x6587;&#x672C;&#x5411;&#x91CF;&#x8868;&#x793A;&#xFF0C;&#x8FD8;&#x80FD;&#x8F7B;&#x677E;&#x96C6;&#x6210;&#x5230;&#x5148;&#x8FDB;&#x7684;&#x5411;&#x91CF;&#x6570;&#x636E;&#x5E93;&#x3001;&#x641C;&#x7D22;&#x7CFB;&#x7EDF;&#x4EE5;&#x53CA; RAG &#x5E94;&#x7528;&#x91CC;&#x3002;<br>&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#x7279;&#x522B;&#x9002;&#x5408;&#x6253;&#x9020;&#x9002;&#x5E94;&#x4E2D;&#x82F1;&#x53CC;&#x8BED;&#x573A;&#x666F;&#x7684; AI &#x5E94;&#x7528;&#xFF0C;&#x5BF9;&#x4E8E;&#x8FFD;&#x6C42;&#x5168;&#x7403;&#x5316;&#x53D1;&#x5C55;&#x7684;&#x4F01;&#x4E1A;&#x6765;&#x8BF4;&#xFF0C;&#x5176;&#x4EF7;&#x503C;&#x4E0D;&#x53EF;&#x4F30;&#x91CF;&#x3002;&#x4EE5;&#x4E0B;&#x662F;&#x51E0;&#x4E2A;&#x5B9E;&#x9645;&#x5E94;&#x7528;&#x6848;&#x4F8B;&#xFF1A;</p><ul><li>&#x6587;&#x6863;&#x5206;&#x6790;&#x4E0E;&#x7BA1;&#x7406;&#xFF1A;&#x5206;&#x6790;&#x548C;&#x7BA1;&#x7406;&#x6D77;&#x91CF;&#x6587;&#x6863;&#xFF0C;&#x52A9;&#x529B;&#x56FD;&#x9645;&#x6CD5;&#x5F8B;&#x548C;&#x5546;&#x52A1;&#x4EA4;&#x6613;&#x7684;&#x987A;&#x5229;&#x8FDB;&#x884C;&#x3002;</li><li>AI &#x9A71;&#x52A8;&#x641C;&#x7D22;&#x5E94;&#x7528;&#xFF1A;&#x5728;&#x591A;&#x8BED;&#x8A00;&#x73AF;&#x5883;&#x4E2D;&#x63D0;&#x5347;&#x641C;&#x7D22;&#x6027;&#x80FD;&#xFF0C;&#x5E2E;&#x52A9;&#x5168;&#x7403;&#x7528;&#x6237;&#x8F7B;&#x677E;&#x627E;&#x5230;&#x4E2D;&#x82F1;&#x6587;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#x3002;</li><li>&#x589E;&#x5F3A;&#x68C0;&#x7D22;&#x7684;&#x804A;&#x5929;&#x673A;&#x5668;&#x4EBA;&#x548C;&#x95EE;&#x7B54;&#x7CFB;&#x7EDF;&#xFF1A;&#x6253;&#x9020;&#x9AD8;&#x6548;&#x7684;&#x53CC;&#x8BED;&#x5BA2;&#x670D;&#x673A;&#x5668;&#x4EBA;&#xFF0C;&#x4F18;&#x5316;&#x4E0E;&#x5168;&#x7403;&#x5BA2;&#x6237;&#x7684;&#x6C9F;&#x901A;&#x4F53;&#x9A8C;&#x3002;</li><li>&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x5904;&#x7406;&#x5E94;&#x7528;&#xFF1A;&#x6DB5;&#x76D6;&#x5168;&#x7403;&#x5E02;&#x573A;&#x8D8B;&#x52BF;&#x5206;&#x6790;&#x3001;&#x56FD;&#x9645;&#x5E02;&#x573A;&#x7B56;&#x7565;&#x7684;&#x4E3B;&#x9898;&#x5EFA;&#x6A21;&#xFF0C;&#x4EE5;&#x53CA;&#x5168;&#x7403;&#x901A;&#x8BAF;&#x7BA1;&#x7406;&#x7684;&#x6587;&#x672C;&#x5206;&#x7C7B;&#x3002;</li><li>&#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#xFF1A;&#x5229;&#x7528;&#x4E2D;&#x82F1;&#x6570;&#x636E;&#x6D1E;&#x5BDF;&#xFF0C;&#x4E3A;&#x5168;&#x7403;&#x591A;&#x5143;&#x5316;&#x53D7;&#x4F17;&#x63D0;&#x4F9B;&#x4E2A;&#x6027;&#x5316;&#x7684;&#x4EA7;&#x54C1;&#x548C;&#x5185;&#x5BB9;&#x63A8;&#x8350;&#x3002;</li></ul><p>&#x501F;&#x52A9;&#x8FD9;&#x6B3E;&#x6A21;&#x578B;&#xFF0C;&#x4E2D;&#x56FD;&#x4F01;&#x4E1A;&#x80FD;&#x591F;&#x5728; AI &#x5E94;&#x7528;&#x9886;&#x57DF;&#x8DE8;&#x8D8A;&#x8BED;&#x8A00;&#x7684;&#x9E3F;&#x6C9F;&#xFF0C;&#x5728;&#x5168;&#x7403;&#x5E02;&#x573A;&#x7684;&#x89D2;&#x9010;&#x4E2D;&#x5360;&#x636E;&#x5148;&#x673A;&#x3002;</p><h3 id="%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B-jina-embeddings-v2-base-zh">&#x8F7B;&#x677E;&#x4E0A;&#x624B; jina-embeddings-v2-base-zh</h3><p>&#x60F3;&#x8981;&#x5FEB;&#x901F;&#x5C06;&#x6211;&#x4EEC;&#x7684;&#x53CC;&#x8BED;&#x5411;&#x91CF;&#x6A21;&#x578B;&#x878D;&#x5165;&#x60A8;&#x7684;&#x5DE5;&#x4F5C;&#x6D41;&#x7A0B;&#xFF1F;&#x53EA;&#x9700;&#x51E0;&#x4E2A;&#x7B80;&#x5355;&#x6B65;&#x9AA4;&#xFF1A;&#x8BBF;&#x95EE; <a href="https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io">https://jina.ai/embeddings</a>&#xFF0C;&#x9886;&#x53D6;&#x60A8;&#x7684;&#x514D;&#x8D39; API &#x5BC6;&#x94A5;&#x6216;&#x66F4;&#x65B0;&#x73B0;&#x6709;&#x5BC6;&#x94A5;&#xFF0C;&#x7136;&#x540E;&#x5728;&#x4E0B;&#x62C9;&#x83DC;&#x5355;&#x4E2D;&#x9009;&#x62E9; jina-embeddings-v2-base-zh&#xFF0C;&#x60A8;&#x7684;&#x6A21;&#x578B;&#x5373;&#x523B;&#x51C6;&#x5907;&#x5C31;&#x7EEA;&#xFF0C;&#x7B49;&#x5F85;&#x60A8;&#x7684;&#x63A2;&#x7D22;&#x548C;&#x4F7F;&#x7528;&#xFF01;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="2000" height="1138" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/image-7.png 2222w" sizes="(min-width: 720px) 720px"></figure><h3 id="%E5%B1%95%E6%9C%9B%E6%9C%AA%E6%9D%A5%EF%BC%9A%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%E4%B8%8E-aws-sagemaker-%E6%B7%B1%E5%BA%A6%E8%9E%8D%E5%90%88">&#x5C55;&#x671B;&#x672A;&#x6765;&#xFF1A;&#x591A;&#x8BED;&#x8A00;&#x652F;&#x6301;&#x4E0E; AWS SageMaker &#x6DF1;&#x5EA6;&#x878D;&#x5408;</h3><p>jina-embeddings-v2-base-zh &#x5373;&#x5C06;&#x4E0A;&#x7EBF; AWS SageMaker &#x548C; HuggingFace&#xFF0C;&#x4E3A;&#x7528;&#x6237;&#x63D0;&#x4F9B;&#x66F4;&#x52A0;&#x4FBF;&#x6377;&#x7684;&#x670D;&#x52A1;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai (Jina AI)</div><div class="kg-bookmark-description">embeddings, prompts, multimodal AI</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/jinaai.png" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><p>&#x6211;&#x4EEC;&#x6B63;&#x79EF;&#x6781;&#x63A8;&#x8FDB;&#x591A;&#x8BED;&#x8A00;&#x5411;&#x91CF;&#x6A21;&#x578B;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x6B27;&#x6D32;&#x53CA;&#x5176;&#x4ED6;&#x56FD;&#x9645;&#x8BED;&#x8A00;&#x7684;&#x652F;&#x6301;&#xFF0C;&#x6765;&#x6EE1;&#x8DB3;&#x5168;&#x7403;&#x7528;&#x6237;&#x7684;&#x591A;&#x6837;&#x5316;&#x9700;&#x6C42;&#x3002;&#x656C;&#x8BF7;&#x671F;&#x5F85;&#x6211;&#x4EEC;&#x5373;&#x5C06;&#x63A8;&#x51FA;&#x7684;&#x6FC0;&#x52A8;&#x4EBA;&#x5FC3;&#x7684;&#x66F4;&#x65B0;&#xFF0C;&#x5305;&#x62EC;&#x4E0E; AWS SageMaker &#x7684;&#x6DF1;&#x5EA6;&#x96C6;&#x6210;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6301;&#x7EED;&#x6DF1;&#x5316;&#x548C;&#x62D3;&#x5BBD;&#x670D;&#x52A1;&#x8303;&#x56F4;&#x3002;</p><h3 id="%E8%87%B4%E8%B0%A2%EF%BC%9A%E6%84%9F%E8%B0%A2%E6%97%A9%E6%9C%9F%E6%B5%8B%E8%AF%95%E8%80%85%E7%9A%84%E5%AE%9D%E8%B4%B5%E8%B4%A1%E7%8C%AE">&#x81F4;&#x8C22;&#xFF1A;&#x611F;&#x8C22;&#x65E9;&#x671F;&#x6D4B;&#x8BD5;&#x8005;&#x7684;&#x5B9D;&#x8D35;&#x8D21;&#x732E;</h3><p>&#x6211;&#x4EEC;&#x8877;&#x5FC3;&#x611F;&#x8C22;&#x53C2;&#x4E0E; <code>jina-embeddings-v2-base-zh-preview</code> &#x6D4B;&#x8BD5;&#x7684;&#x4E2D;&#x56FD;&#x793E;&#x533A;&#x670B;&#x53CB;&#x4EEC;&#x3002;&#x4F60;&#x4EEC;&#x7684;&#x5B9D;&#x8D35;&#x610F;&#x89C1;&#x5BF9;&#x4F18;&#x5316;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x8D77;&#x5230;&#x4E86;&#x91CD;&#x8981;&#x4F5C;&#x7528;&#x3002;&#x5982;&#x679C;&#x60A8;&#x5728;&#x4F7F;&#x7528;&#x8FC7;&#x7A0B;&#x4E2D;&#x6709;&#x4EFB;&#x4F55;&#x5EFA;&#x8BAE;&#x6216;&#x60F3;&#x6CD5;&#xFF0C;&#x6B22;&#x8FCE;&#x968F;&#x65F6;&#x5411;&#x6211;&#x4EEC;&#x63D0;&#x51FA;&#x3002;&#x60A8;&#x7684;&#x6BCF;&#x4E00;&#x6761;&#x53CD;&#x9988;&#x90FD;&#x662F;&#x6211;&#x4EEC;&#x6301;&#x7EED;&#x8FDB;&#x6B65;&#x7684;&#x52A8;&#x529B;&#x3002;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4182 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.com/assets/images/favicon.ico" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English"></div></a></figure><hr><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">&#x6B63;&#x5F0F;&#x7248;&#x89E3;&#x51B3;&#x4E86;&#x9884;&#x89C8;&#x7248;&#x7684;&#x5206;&#x6570;&#x81A8;&#x80C0;&#x95EE;&#x9898;</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">&#x4E0E;&#x4E4B;&#x524D;&#x7684;&#x9884;&#x89C8;&#x7248;&#x6A21;&#x578B;&#x76F8;&#x6BD4;&#xFF0C;&#x6B63;&#x5F0F;&#x7248;&#x6A21;&#x578B;&#x63D0;&#x4F9B;&#x4E86;&#x66F4;&#x52A0;&#x5206;&#x6563;&#x4E14;&#x5408;&#x7406;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x3002;&#x5728;&#x9884;&#x89C8;&#x7248;&#x7684;&#x6D4B;&#x8BD5;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x6A21;&#x578B;&#x66FE;&#x663E;&#x793A;&#x51FA;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x7684;&#x901A;&#x8D27;&#x81A8;&#x80C0;&#x73B0;&#x8C61;&#xFF0C;&#x5373;&#x4FBF;&#x662F;&#x5B8C;&#x5168;&#x4E0D;&#x76F8;&#x5173;&#x7684;&#x8BCD;&#x6C47;&#xFF0C;&#x6BD4;&#x5982;&#x2018;&#x5B89;&#x59AE;&#x2019;&#x548C;&#x2018;&#x84B8;&#x6C7D;&#x673A;&#x2019;&#xFF0C;&#x4E5F;&#x4F1A;&#x83B7;&#x5F97;&#x5F88;&#x9AD8;&#x7684;&#x4F59;&#x5F26;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;&#x800C;&#x5728;&#x6B63;&#x5F0F;&#x7248;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4F18;&#x5316;&#x4E86;&#x6A21;&#x578B;&#xFF0C;&#x4EE5;&#x786E;&#x4FDD;&#x76F8;&#x4F3C;&#x5EA6;&#x8BC4;&#x5206;&#x66F4;&#x4E3A;&#x5408;&#x7406;&#xFF0C;&#x4ECE;&#x800C;&#x66F4;&#x51C6;&#x786E;&#x5730;&#x53CD;&#x6620;&#x5185;&#x5BB9;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x3002;</span></p><p><span style="white-space: pre-wrap;">&#x6B64;&#x5916;&#xFF0C;Jina Embeddings &#x73B0;&#x5728;&#x652F;&#x6301;&#x9AD8;&#x8FBE; 8192 Token &#x7684;&#x6587;&#x672C;&#x5904;&#x7406;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x957F;&#x7BC7;&#x5927;&#x8BBA;&#x8FD8;&#x662F;&#x7B80;&#x77ED;&#x8BED;&#x53E5;&#xFF0C;&#x751A;&#x81F3;&#x662F;&#x5355;&#x4E2A;&#x8BCD;&#x6C47;&#x6216;&#x540D;&#x5B57;&#xFF08;&#x5982;&#x201C;&#x5B89;&#x59AE;&#x201D;&#x4E0E;&#x201C;&#x9732;&#x5A1C;&#x201D;&#x7684;&#x6BD4;&#x8F83;&#xFF09;&#xFF0C;&#x90FD;&#x80FD;&#x5C55;&#x73B0;&#x51FA;&#x5176;&#x5904;&#x7406;&#x5404;&#x79CD;&#x7C7B;&#x578B;&#x6570;&#x636E;&#x7684;&#x5F3A;&#x5927;&#x80FD;&#x529B;&#x3002;&#x8FD9;&#x4E00;&#x6539;&#x8FDB;&#x4E0D;&#x4EC5;&#x63D0;&#x5347;&#x4E86;&#x6A21;&#x578B;&#x7684;&#x51C6;&#x786E;&#x6027;&#xFF0C;&#x4E5F;&#x589E;&#x5F3A;&#x4E86;&#x5176;&#x5728;&#x5904;&#x7406;&#x591A;&#x6837;&#x5316;&#x6570;&#x636E;&#x65F6;&#x7684;&#x7075;&#x6D3B;&#x6027;&#x548C;&#x5B9E;&#x7528;&#x6027;&#x3002;</span></p></div>
        </div><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png" class="kg-image" alt="8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English" loading="lazy" width="1572" height="688" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--2-.png 1572w" sizes="(min-width: 720px) 720px"></figure>]]></content:encoded></item><item><title><![CDATA[The 1950-2024 Text Embeddings Evolution Poster]]></title><description><![CDATA[Take part in celebrating the achievements of text embeddings and carry a piece of its legacy with you.]]></description><link>https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster/</link><guid isPermaLink="false">659d02a00bab3100012d2bff</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Tue, 09 Jan 2024 11:07:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--30-.png" alt="The 1950-2024 Text Embeddings Evolution Poster"><p>Embark on a journey through time with our latest infographic poster, showcasing the remarkable evolution of text embeddings from 1950 to 2024. This visually striking piece is not just a testament to technological progress; it&apos;s a guide that traces the lineage of innovations that have revolutionized how we represent text data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png" class="kg-image" alt="The 1950-2024 Text Embeddings Evolution Poster" loading="lazy" width="1414" height="2000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Latest-version2.0--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Latest-version2.0--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Latest-version2.0--1-.png 1414w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">The Evolution of Text Embeddings&#xA0;</span></a><span style="white-space: pre-wrap;">&#xA9; 2024&#xA0;by&#xA0;</span><a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" target="_blank" rel="cc:attributionURL noopener noreferrer"><span style="white-space: pre-wrap;">Jina AI&#xA0;</span></a><span style="white-space: pre-wrap;">is licensed under&#xA0;</span><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer"><span style="white-space: pre-wrap;">CC BY-NC-ND 4.0&#xA0;</span></a></figcaption></figure><h2 id="a-chronicle-of-innovation">A Chronicle of Innovation</h2><p>From the foundational Bag of Words model to the 8K token-length <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">jina-embeddings-v2</a> and everything in between, our poster captures each step in advancing text embeddings. Designed with precision, it highlights how each breakthrough built upon its predecessors, leading us to today&apos;s embeddings-driven applications.</p><h2 id="designed-for-enthusiasts-and-professionals-alike">Designed for Enthusiasts and Professionals Alike</h2><p>Whether you&apos;re a data scientist, a software engineer, an AI researcher, or simply a technology enthusiast, this poster is a must-have. It&apos;s more than just a visual treat; it&apos;s an educational tool that breaks down complex developments into an accessible format.</p><h2 id="bring-home-a-piece-of-ai-history">Bring Home a Piece of AI History</h2><p>For those who appreciate the tactile feel of quality, you can own this piece of history by purchasing a hard copy. Imagine this stunning poster adorning your living room wall, sparking conversations and inspiring future innovators.</p>
<!--kg-card-begin: html-->
<script async src="https://js.stripe.com/v3/buy-button.js">
</script>

<div style="text-align: center">
<stripe-buy-button buy-button-id="buy_btn_1OWchyEoTOtb5eZv69zXthH9" publishable-key="pk_live_51MD1n2EoTOtb5eZvVgdn5IcOR7BNFFcJTmb7hcAihzLOZpif376WqhFUTpN3sQHb7ADxQLtKxXvC7YAttivYrSDa00PJqFn7qz">
</stripe-buy-button>
</div>
<!--kg-card-end: html-->
<figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/13.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/14.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png" width="1024" height="1024" loading="lazy" alt="The 1950-2024 Text Embeddings Evolution Poster" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/15.png 1024w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="accessible-in-multiple-formats">Accessible in Multiple Formats</h2><p>Not ready for a physical copy? No problem. We offer a downloadable PNG or PDF version, ensuring you can access this wealth of information in the format that best suits your needs.</p><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-v2.png" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PNG) Download the Evolution of Text Embeddings (834KB)</div><div class="kg-file-card-caption">Best for display and sharing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-v2.png</div><div class="kg-file-card-filesize">814 KB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-jina-ai-cmyk-color.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Print, CMYK) Download the Evolution of Text Embeddings (7.1MB)</div><div class="kg-file-card-caption">Best for printing</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-jina-ai-cmyk-color.pdf</div><div class="kg-file-card-filesize">7 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><div class="kg-card kg-file-card"><a class="kg-file-card-container" href="https://jina-ai-gmbh.ghost.io/content/files/2024/01/evolution-text-embeddings-standard.pdf" title="Download" download><div class="kg-file-card-contents"><div class="kg-file-card-title">(PDF Standard) Download the Evolution of Text Embeddings (2.8MB)</div><div class="kg-file-card-caption">Best for viewing on the screen</div><div class="kg-file-card-metadata"><div class="kg-file-card-filename">evolution-text-embeddings-standard.pdf</div><div class="kg-file-card-filesize">3 MB</div></div></div><div class="kg-file-card-icon"><svg viewbox="0 0 24 24"><defs><style>.a{fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px;}</style></defs><title>download-circle</title><polyline class="a" points="8.25 14.25 12 18 15.75 14.25"/><line class="a" x1="12" y1="6.75" x2="12" y2="18"/><circle class="a" cx="12" cy="12" r="11.25"/></svg></div></a></div><h2 id="references-at-your-fingertips">References at Your Fingertips</h2><p>Accompanying our infographic, we provide an extensive list of references, corresponding to each milestone depicted. This curated collection allows you to delve deeper into each technology, understanding the intricacies and applications that have shaped the field of natural language processing.</p><blockquote>TF-IDF	1972	K.S. Jones, A statistical interpretation of term specificity and its application in retrieval, J. Doc. 28 (1972) 11&#x2013;21.</blockquote><blockquote>TF-IDF	1973	K.S. Jones, Index term weighting, Inf. Storage Retr. 9 (11) (1973) 619&#x2013;633.</blockquote><blockquote>Bag of Words	1981	Z.S. Harris, Distributional structure, in: Papers on Syntax, Springer, 1981, pp. 3&#x2013;22.</blockquote><blockquote>BoN-Grams	1994	W. Cavnar, W.B. Cavnar, J.M. Trenkle, N-gram-based text categorization, in: Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR-94), 1994, pp. 161&#x2013;175.</blockquote><blockquote>doc2vec	2014	Q. Le, T. Mikolov, Distributed representations of sentences and documents, in: Proceedings of the 31st International Conference on International Conference on Machine Learning (ICML) - Volume 32, ICML &#x2019;14, JMLR.org, 2014, pp. II&#x2013;1188&#x2013;II&#x2013;1196.</blockquote><blockquote>DAN	2015	M. Iyyer, V. Manjunatha, J. Boyd-Graber, H. Daum&#xE9; III, Deep unordered composition rivals syntactic methods for text classification, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Beijing, China, 2015, pp. 1681&#x2013;1691.</blockquote><blockquote>RCNN	2015	S. Lai, L. Xu, K. Liu, J. Zhao, Recurrent convolutional neural networks for text classification, in: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI &#x2019;15, AAAI Press, 2015, pp. 2267&#x2013;2273.</blockquote><blockquote>RNNs	2015	D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, in: International Conference on Learning Representations (ICLR) 2015, 2014.</blockquote><blockquote>Skip-Thought	2015	R. Kiros, Y. Zhu, R.R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, S. Fidler, Skip-thought vectors, in: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 28, Curran Associates, Inc., 2015, pp. 3294&#x2013;3302.</blockquote><blockquote>DESM	2016	E. Nalisnick, B. Mitra, N. Craswell, R. Caruana, Improving document rank- ing with dual word embeddings, in: Proceedings of the 25th International Conference Companion on World Wide Web, 2016, pp. 83&#x2013;84.</blockquote><blockquote>DV-ngram	2016	B. Li, T. Liu, X. Du, D. Zhang, Z. Zhao, Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews, in: Workshop Contribution at International Conference on Learning Representations (ICLR) 2016, 2016.</blockquote><blockquote>FastSent	2016	F. Hill, K. Cho, A. Korhonen, Learning distributed representations of sentences from unlabelled data, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics (ACL), San Diego, California, 2016, pp. 1367&#x2013;1377.</blockquote><blockquote>HAN	2016	Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. Hovy, Hierarchical attention networks for document classification, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, San Diego, California, 2016, pp. 1480&#x2013;1489.</blockquote><blockquote>NVDM	2016	Y. Miao, L. Yu, P. Blunsom, Neural variational inference for text processing, in: Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML) - Volume 48, ICML &#x2019;16, JMLR.org, 2016, pp. 1727&#x2013;1736.</blockquote><blockquote>Siamese CBoW	2016	T. Kenter, A. Borisov, M. de Rijke, Siamese CBOW: Optimizing word embed- dings for sentence representations, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics (ACL), Berlin, Germany, 2016, pp. 941&#x2013;951.</blockquote><blockquote>CNN-LSTM	2017	Z. Gan, Y. Pu, R. Henao, C. Li, X. He, L. Carin, Learning generic sentence representations using convolutional neural networks, in: Empirical Methods in Natural Language Processing, EMNLP, 2017, pp. 2390&#x2013;2400.</blockquote><blockquote>CNNs	2017	Y. Zhang, D. Shen, G. Wang, Z. Gan, R. Henao, L. Carin, Deconvolutional paragraph representation learning, in: I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran Associates, Inc., 2017, pp. 5438&#x2013;5445.</blockquote><blockquote>CNNs	2017	Z. Zhu, J. Hu, Context aware document embedding, 2017, arXiv:1707.01521.</blockquote><blockquote>Doc2VecC	2017	M. Chen, Efficient vector representation for documents through corruption, in: International Conference on Learning Representations, ICLR, 2017.</blockquote><blockquote>DiSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, C. Zhang, DiSAN: Directional self- attention network for RNN/CNN-free language understanding, in: AAAI, 2018, pp. 5446&#x2013;5455.</blockquote><blockquote>ELMo	2018	M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettle- moyer, Deep contextualized word representations, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Associa- tion for Computational Linguistics (ACL), New Orleans, Louisiana, 2018, pp. 2227&#x2013;2237.</blockquote><blockquote>GPT-2	2018	A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners, OpenAI Blog (2018).</blockquote><blockquote>ReSan	2018	T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, C. Zhang, Reinforced self-attention network: A hybrid of hard and soft attention for sequence modeling, in: Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI &#x2019;18, AAAI Press, 2018, pp. 4345&#x2013;4352.</blockquote><blockquote>Sent2vec	2018	M. Pagliardini, P. Gupta, M. Jaggi, Unsupervised learning of sentence embed- dings using compositional n-gram features, in: Proceedings of North American Chapter of the Association for Computational Linguistics NAACL-HLT, 2018, pp. 528&#x2013;540.</blockquote><blockquote>BART	2019	Lewis, Mike, et al. &quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.&quot; arXiv preprint arXiv:1910.13461 (2019).</blockquote><blockquote>BERT	2019	J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidi- rectional transformers for language understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 4171&#x2013;4186.</blockquote><blockquote>DistilBERT	2019	V. Sanh, L. Debut, J. Chaumond, T. Wolf, DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, in: 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing at NeurIPS 2019, 2019.</blockquote><blockquote>DocBERT	2019	A. Adhikari, A. Ram, R. Tang, J. Lin, DocBERT: BERT for document classification, 2019, ArXiv abs/1904.08398.</blockquote><blockquote>LASER	2019	M. Artetxe, H. Schwenk, Massively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond, Trans. Assoc. Comput. Linguist. 7 (2019) 597&#x2013;610.</blockquote><blockquote>MASS	2019	K. Song, X. Tan, T. Qin, J. Lu, T. Liu, MASS: Masked sequence to sequence pre-training for language generation, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, in: Proceedings of Machine Learning Research, vol. 97, PMLR, 2019, pp. 5926&#x2013;5936.</blockquote><blockquote>SBERT	2019	N. Reimers, I. Gurevych, Sentence-BERT: Sentence embeddings using siamese BERT-networks, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 3982&#x2013;3992.</blockquote><blockquote>Transformer-XL	2019	Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, R. Salakhutdinov, Transformer- XL: Attentive language models beyond a fixed-length context, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL, Association for Computational Linguistics, Florence, Italy, 2019, pp. 2978&#x2013;2988.</blockquote><blockquote>VLAWE	2019	R.T. Ionescu, A. Butnaru, Vector of locally-aggregated word embeddings (VLAWE): A novel document-level representation, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics (ACL), Minneapolis, Minnesota, 2019, pp. 363&#x2013;369.</blockquote><blockquote>XLM	2019	A. Conneau, G. Lample, Cross-lingual language model pretraining, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 7059&#x2013;7069.</blockquote><blockquote>XLNet	2019	Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R.R. Salakhutdinov, Q.V. Le, XLNet: Generalized autoregressive pretraining for language understanding, in: H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d&#x2019;Alch&#xE9; Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019, pp. 5753&#x2013;5763.</blockquote><blockquote>ALBERT	2020	Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, in: International Conference on Learning Representations, ICLR, OpenReview.net, 2020.</blockquote><blockquote>ELECTRA	2020	Clark, Kevin, et al. &quot;Electra: Pre-training text encoders as discriminators rather than generators.&quot; arXiv preprint arXiv:2003.10555 (2020).</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Saw, P. Nokhiz, P. Netrapalli, P. Rai, P. Talukdar, P-SIF: Document embeddings using partition averaging, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020, pp. 7863&#x2013;7870.</blockquote><blockquote>P-SIF	2020	V. Gupta, A. Kumar, P. Nokhiz, H. Gupta, P. Talukdar, Improving docu- ment classification with multi-sense embeddings, in: European Conference on Artificial Intelligence (ECAI) 2020, IOS Press, 2020, pp. 2030&#x2013;2037.</blockquote><blockquote>RoBERTa	2020	Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L.Zettlemoyer, V. Stoyanov, RoBERTa: A robustly optimized BERT pretrainingapproach, in: Under Review as a Conference Paper at International Conference on Learning Representations (ICLR) 2020, 2020.</blockquote><blockquote>SpanBERT	2020	M. Joshi, D. Chen, Y. Liu, D. Weld, L. Zettlemoyer, O. Levy, SpanBERT: Improving pre-training by representing and predicting spans, Trans. Assoc. Comput. Linguist. 8 (2020).</blockquote><blockquote>SimCSE	2021	Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894&#x2013;6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552.</blockquote><blockquote>AugCSE	2022	Tang, Zilu, Muhammed Yusuf Kocyigit, and Derry Wijaya. &quot;Augcse: Contrastive sentence embedding with diverse augmentations.&quot; arXiv preprint arXiv:2210.13749 (2022).</blockquote><blockquote>DiffCSE	2022	Oh, Dongsuk, et al. &quot;Don&apos;t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling.&quot; arXiv preprint arXiv:2209.05972 (2022).</blockquote><blockquote>SGPT	2022	Muennighoff, Niklas. &quot;Sgpt: Gpt sentence embeddings for semantic search.&quot; arXiv preprint arXiv:2202.08904 (2022).</blockquote><blockquote>bge	2023	C-Pack: Packaged Resources To Advance General Chinese Embedding</blockquote><blockquote>embeddings-v2	2023	G&#xFC;nther, Michael, et al. &quot;Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.&quot; arXiv preprint arXiv:2310.19923 (2023).</blockquote>]]></content:encoded></item><item><title><![CDATA[Words + JSON + Images = SceneXplain's new JSON Schema Builder]]></title><description><![CDATA[Effortlessly create JSON Schemas with SceneXplain: Describe your needs in natural language, and get the perfect schema for extracting JSON from your images!]]></description><link>https://jina.ai/news/words-json-images-scenexplains-new-json-schema-builder/</link><guid isPermaLink="false">65958ec70bab3100012d2bbc</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 04 Jan 2024 15:00:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--10-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--10-.jpg" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"><p>2024 has just kicked off, and we&apos;re happy to introduce our brand spanking new JSON Schema Builder interface - after all, you know what they say: New year, new UI, right?</p><p>As a bit of background, last year we introduced SceneXplain&apos;s <a href="https://www.notion.so/Words-JSON-Images-SceneXplain-s-new-JSON-Schema-Builder-7485a41a22e24510a67ebdd8e392a5c5?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">&quot;extract JSON from image&quot; feature</a>, which does what it says on the tin: You can specify a JSON Schema, upload your image, and the output will be a populated JSON string based on the image contents. For example, <a href="https://scenex.jina.ai/share?thread=ECR3q6syJZYehCHmL56c&amp;ref=jina-ai-gmbh.ghost.io">identifying products and text strings in an image</a>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--48-.png" class="kg-image" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" loading="lazy" width="460" height="815"></figure><p>But creating those JSON Schemas is hard work - folks often don&#x2019;t understand the connection between the task and the Schema, and it&#x2019;s easy to misplace a curly bracket or semi-colon and thus get invalid JSON. That&apos;s why we introduced our <a href="https://www.notion.so/Words-JSON-Images-SceneXplain-s-new-JSON-Schema-Builder-7485a41a22e24510a67ebdd8e392a5c5?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">JSON Schema Store</a>, where you can use predefined schemas and not have to roll your own.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--49-.png" class="kg-image" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" loading="lazy" width="1797" height="900" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--49-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--49-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Untitled--49-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--49-.png 1797w" sizes="(min-width: 720px) 720px"></figure><p>Of course, that can only go so far - if you want to build your own custom schema, you&apos;re just facing those problems all over again.</p><p>Well, not any more. Now, you can simply <em>say</em> what kind of JSON Schema you want, and SceneXplain will churn out a Schema that matches your requirements. In short, you use natural language to generate the schema, which you can then use with an image to extract your desired output JSON.</p><p>Here&#x2019;s how you can get started:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Step-by-step_guide_Building_and_using_your_JSON_Schema_in_SceneXplain__i-WIWI2sRDO1vxMSP4Cu1Q?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<p>Here are a few more examples to inspire you:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/strings.png" width="634" height="997" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/strings.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/strings.png 634w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/recipe.png" width="633" height="959" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/recipe.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/recipe.png 633w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/ad_detector.png" width="627" height="917" loading="lazy" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/ad_detector.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/ad_detector.png 627w"></div></div></div></figure><h2 id="get-started">Get started</h2><p>Head over to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">scenex.jina.ai</a> to start generating your own JSON Schemas now and share your results on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord</a>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Words + JSON + Images = SceneXplain&apos;s new JSON Schema Builder"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[DocArray 0.40.0 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-40-0-update/</link><guid isPermaLink="false">658599a20bab3100012d2b68</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 22 Dec 2023 14:20:33 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0400">Release Note (<code>0.40.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-DocArray-dak-1.jpeg" alt="DocArray 0.40.0 Update"><p>This release contains 1 new feature, 3 bug fixes, and 2 documentation improvements.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.40.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Release v0.40.0 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.40.0) Release time: 2023-12-22 12:12:15 This release contains 1 new feature, 3 bug fixes and 2 documentation improvements.
&#x1F195; Features
Add Epsilla connector (#1835)
We have integra&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="DocArray 0.40.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/668ee783e022a27964faa8e5f0a854cc1dea3f036e49bb10f4413288f5b2066f/docarray/docarray/releases/tag/v0.40.0" alt="DocArray 0.40.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="add-epsilla-connector-1835">Add Epsilla connector (<a href="https://github.com/docarray/docarray/pull/1835?ref=jina-ai-gmbh.ghost.io">#1835</a>)</h3><p>We have integrated&#xA0;<a href="https://epsilla.com/?ref=jina-ai-gmbh.ghost.io" rel="nofollow">Epsilla</a>&#xA0;into DocArray.</p><p>Here&apos;s a simple example of how to use it:</p><pre><code class="language-Python">import numpy as np
from docarray import BaseDoc
from docarray.index import EpsillaDocumentIndex
from docarray.typing import NdArray
from pydantic import Field

class MyDoc(BaseDoc):
    text: str
    embedding: NdArray[10] = Field(is_embedding=True)

docs = [MyDoc(text=f&apos;text {i}&apos;, embedding=np.random.rand(10)) for i in range(10)]
query = np.random.rand(10)
db = EpsillaDocumentIndex[MyDoc]()
db.index(docs)
results = db.find(query, limit=10)</code></pre><p>In this example, we create a document class with both textual and numeric data. Then, we initialize an Epsilla-backed document index and use it to index our documents. Finally, we perform a search query.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fixed-type-hints-error-in-python-312-1840">Fixed type hints error in Python 3.12 (<a href="https://github.com/docarray/docarray/pull/1840?ref=jina-ai-gmbh.ghost.io">#1840</a>)</h3><p>DocArray type-hinting is now available for Python 3.12.</p><h3 id="fix-issue-serializing-and-deserializing-complex-schemas-1836">Fix issue serializing and deserializing complex schemas (<a href="https://github.com/docarray/docarray/pull/1836?ref=jina-ai-gmbh.ghost.io">#1836</a>)</h3><p>There was an issue when serializing and deserializing&#xA0;<code>protobuf</code>&#xA0;documents with nested documents in dictionaries and other complex structures.</p><h3 id="fix-storage-issue-in-torchtensor-class-1833">Fix storage issue in TorchTensor class (<a href="https://github.com/docarray/docarray/pull/1833?ref=jina-ai-gmbh.ghost.io">#1833</a>)</h3><p>There was a bug when deep-copying a&#xA0;<code>TorchTensor</code>&#xA0;object if its&#xA0;<code>dtype</code>&#xA0;was not&#xA0;<code>float32</code>. This has now been fixed.</p><h2 id="%F0%9F%93%97-documentation-improvements">&#x1F4D7; Documentation Improvements</h2><ul><li>Add Epsilla integration guide (<a href="https://github.com/docarray/docarray/pull/1838?ref=jina-ai-gmbh.ghost.io">docs(epsilla): add epsilla integration guide #1838</a>)</li><li>Fix sign commit command in docs (<a href="https://github.com/docarray/docarray/pull/1834?ref=jina-ai-gmbh.ghost.io">docs: fix sign commit commad in docs #1834</a>)</li></ul><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Tony Yang (<a href="https://github.com/tonyyanga?ref=jina-ai-gmbh.ghost.io">@tonyyanga</a>&#xA0;)</li><li>Naymul Islam (<a href="https://github.com/ai-naymul?ref=jina-ai-gmbh.ghost.io">@ai-naymul</a>&#xA0;)</li><li>Ben Shaver (<a href="https://github.com/bpshaver?ref=jina-ai-gmbh.ghost.io">@bpshaver</a>&#xA0;)</li><li>Joan Fontanals (@<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>954 (<a href="https://github.com/954-Ivory?ref=jina-ai-gmbh.ghost.io">@954-Ivory</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Full-stack RAG with Jina Embeddings v2 and LlamaIndex]]></title><description><![CDATA[You can build your own RAG chatbot in a matter of minutes with Jina Embeddings, LlamaIndex and Mixtral Instruct. We'll show you how to get up and running right now.]]></description><link>https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/</link><guid isPermaLink="false">6583fe510bab3100012d29f2</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Fri, 22 Dec 2023 13:00:07 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/4.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><p>LLMs are cool, but their input context windows are always so small. They&#x2019;re getting bigger, but they&#x2019;ll never be big enough for, say, a whole book, much less an encyclopedia, unless there is some breakthrough in AI model architectures.</p><p>RAG (<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?ref=jina-ai-gmbh.ghost.io">Retrieval Augmented Generation</a>) is a strategy that can compensate for this limitation, letting you use LLMs to respond to questions with answers that draw on relevant parts of documents or whole repositories of documents that are far too large to put entirely into the model&#x2019;s input.</p><p>This article will show you how to use LlamaIndex, Jina Embeddings, and the <code>Mixtral-8x7B-Instruct-v0.1</code> language model (<a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">hosted on HuggingFace</a>) to build a complete RAG system. For more information on the Mixtral language model, see the <a href="https://mistral.ai/news/mixtral-of-experts/?ref=jina-ai-gmbh.ghost.io">Mistral AI website</a> or the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">model card on HuggingFace</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://mistral.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mistral AI | Open source models</div><div class="kg-bookmark-description">Frontier AI in your hands</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://mistral.ai/images/favicon/apple-touch-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Open source models</span><span class="kg-bookmark-publisher">Mistral AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://mistral.ai/images/mistral-social-banner.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>You can also <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download a Jupyter Notebook with all the code in this article from GitHub</a>, or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run it directly on Colab</a>.</p><p>You will need:</p><ol><li>A <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings API key</a>.</li><li>A <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace account and token</a>.</li></ol><p>Since both the Jina Embeddings model and Mixtral are running remotely and are accessed via a web API, you won&#x2019;t need any special hardware. You will need to install Python and meet the <a href="https://docs.llamaindex.ai/en/stable/getting_started/installation.html?ref=jina-ai-gmbh.ghost.io">system requirements for LlamaIndex</a>.</p><h2 id="what-is-rag-and-how-does-it-work">What is RAG and How Does it Work?</h2><p>Retrieval Augmented Generation is a strategy that merges search with language generation. The way it works is that it uses an external information retrieval system to find documents that are likely to inform the answer to a user query. It then passes them, with the user&#x2019;s request, to a text-generating language model, which produces a natural language response.</p><p>This allows you to use an LLM to answer questions and use information from documents and sets of documents that are much larger than its input context window. The LLM only sees a few pertinent parts of the document when responding to prompts. This also has the advantage of reducing (although not eliminating) inexplicable hallucinations.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="1600" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--24-.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>This strategy has some limitations:</p><ol><li>It is sensitive to the input context size supported by the LLM. The larger the context size, the more information you can give the LLM, yielding better and richer responses.</li><li>It is sensitive to the quality of the results of the initial information retrieval. If your search engine gives it irrelevant or inaccurate results, the LLM may paste them together as best it can and give you garbage output. This can be caused by bad data (as the saying goes <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out?ref=jina-ai-gmbh.ghost.io"><em>garbage in, garbage out</em></a>) but can also be caused by a search system that does not return the most useful matches or does not rank them highly enough in the results.</li></ol><p>High-quality embeddings are key to making RAG work because they reduce the impact of these limitations.</p><p>First, a small context size for an LLM means it&#x2019;s extra important to find the most relevant information, because you cannot add very much to the user&#x2019;s prompt. Second, how informative the answer is depends on how informative the input is. If the search results displayed to the LLM are irrelevant or poorly informative, that will be reflected in the result.</p><p>AI-generated embeddings are, on the whole, the best way to find and rank query results in general.</p><h2 id="build-a-full-rag-chatbot">Build a Full RAG Chatbot</h2><p>We will create and install a full RAG system using the <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex framework</a> for working with LLMs. This system uses Jina Embeddings to index document elements and store them in LlamaIndex&#x2019; built-in vector store and search engine. Then, it uses the newly released <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">Mixtral Instruct</a> model to construct natural language answers.</p><p>The approach in the article will also work with OpenAI&#x2019;s GPT models and Meta&#x2019;s Llama2, with some adaptation of the code and possibly the prompt. For more details, read the <a href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io">LlamaIndex documentation</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://docs.llamaindex.ai/en/stable/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex &#x1F999; 0.9.19</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://docs.llamaindex.ai/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">LlamaIndex &#x1F999; 0.9.19</span></div></div></a></figure><p>This section involves a lot of code to copy and paste, and it will only get a very high-level explanation. You may prefer to <a href="https://raw.githubusercontent.com/jina-ai/workshops/article/LlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">download the accompanying notebook</a> or <a href="https://colab.research.google.com/github/jina-ai/workshops/blob/article%2FLlamaIndex/notebooks/llamaindex/RAGwithJinaLlamaIndex.ipynb?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">run this code on Google Colab</a>.</p><h3 id="getting-started">Getting Started</h3><p>First, install LlamaIndex:</p><pre><code class="language-bash">pip install llama-index
</code></pre><p>Next, make sure that you have a <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina API key</a> and a <a href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io">HuggingFace Inference API token</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hugging Face &#x2013; The AI community building the future.</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://huggingface.co/front/thumbnails/v2-2.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>In Python, set up your secret key values like this:</p><pre><code class="language-Python">jinaai_api_key = &quot;&lt;your Jina Embeddings API key&gt;&quot;
hf_inference_api_key: str = &apos;&lt;your HuggingFace Inference API token&gt;&apos;
</code></pre><h3 id="connect-jina-embeddings">Connect Jina Embeddings</h3><p>LlamaIndex provides built-in support for the Jina Embeddings API. To use it, you only need to initialize the <code>JinaEmbedding</code> object with your API key and model name. For this example, we will use <code>jina-embeddings-v2-base-en</code>.</p><pre><code class="language-Python">from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jinaai_api_key,
    model=&quot;jina-embeddings-v2-base-en&quot;,
)
</code></pre><h3 id="connect-mixtral-llm">Connect Mixtral LLM</h3><p>We will also need to load the <code>Mixtral-8x7B-Instruct-v0.1</code> model. We will wrap it in a subclass of <code>llama_index.llms.CustomLLM</code> to make it compatible with LlamaIndex.</p><p>The important elements are the class parameters:</p><pre><code class="language-Python">model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
api_key: str = hf_inference_api_key
context_window: int = 4096
num_output: int = 512
</code></pre><p>The parameter <code>model_name</code> is the name of the model on HuggingFace, in this case, <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code>, which is also the path part of the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io">URL for its model card on HuggingFace</a>. For <code>api_key</code>, you need to use your HuggingFace Inference API token. Then, specify the input context size the model supports (<code>context_window</code>), in this case, 4096 tokens, and the maximum output size in tokens (<code>num_output</code>), 512.</p><p>The code below sets up the LLM object in the LlamaIndex framework:</p><pre><code class="language-Python">import requests
from llama_index.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.llms.base import llm_completion_callback
from typing import Any


class MixtralLLM(CustomLLM):
    context_window: int = 4096
    num_output: int = 512
    model_name: str = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
    api_key: str = hf_inference_api_key

    @property
    def metadata(self) -&gt; LLMMetadata:
        &quot;&quot;&quot;Get LLM metadata.&quot;&quot;&quot;
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    def do_hf_call(self, prompt: str) -&gt; str:
        data = {
            &quot;inputs&quot;: prompt
        }

        response = requests.post(
            &apos;https://api-inference.huggingface.co/models/&apos; + self.model_name,
            headers={
                &apos;authorization&apos;: f&apos;Bearer {self.api_key}&apos;,
                &apos;content-type&apos;: &apos;application/json&apos;,
            },
            json=data,
            stream=True
        )
        if response.status_code != 200 or not response.json() or &apos;error&apos; in response.json():
            print(f&quot;Error: {response}&quot;)
            return &quot;Unable to answer for technical reasons.&quot;
        full_txt = response.json()[0][&apos;generated_text&apos;]
        offset = full_txt.find(&quot;---------------------&quot;)
        ss = full_txt[offset:]
        offset = ss.find(&quot;Answer:&quot;)
        return ss[offset+7:].strip()

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse:
        response = self.do_hf_call(prompt)
        return CompletionResponse(text=response)

    @llm_completion_callback()
    def stream_complete(
            self, prompt: str, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        response = &quot;&quot;
        for token in self.do_hf_call(prompt):
            response += token
            yield CompletionResponse(text=response, delta=token)


mixtral_llm = MixtralLLM()</code></pre><h3 id="prepare-a-text-for-rag">Prepare a Text for RAG</h3><p>Next, we will download a document and break it into pieces.</p><p>For this exercise, the text we&#x2019;ll use is <a href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><strong><em>Computers on the Farm</em></strong></a>, published by the US Department of Agriculture in 1982 and available via the Gutenberg Project. This 10,000-word booklet is full of useful information for the farmer considering buying a home computer for farm operations 40 years ago.</p><p>Naturally, its advice is perhaps less helpful today.</p><p>However, it serves as a good example because it is much longer than the input context size of Mixtral LLMs or Jina Embeddings v2.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--25-.png" class="kg-image" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex" loading="lazy" width="298" height="519"></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.gutenberg.org/ebooks/59316?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Computers on the Farm by Deborah Takiff Smith</div><div class="kg-bookmark-description">Free kindle book and epub digitized and proofread by volunteers.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.gutenberg.org/gutenberg/apple-icon.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Project Gutenberg</span><span class="kg-bookmark-publisher">Smith, Deborah Takiff</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.gutenberg.org/cache/epub/59316/pg59316.cover.medium.jpg" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>The code below will strip the Gutenberg Project header and footer from the text, correct the MS DOS-style linebreaks to conventional ones, and split the text on the headers.</p><pre><code class="language-Python">import urllib.request
from typing import List
from llama_index.readers import StringIterableReader
from llama_index.schema import Document


def load_gutenberg(target_url: str) -&gt; List[Document]:
    ret: List[str] = []
    buff: str = &quot;&quot;
    reject: bool = True
    for raw_line in urllib.request.urlopen(target_url):
        line = raw_line.decode(&quot;utf-8&quot;)
        stripped_line = line.strip()
        if reject:
            if stripped_line.startswith(&quot;*** START OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = False
                continue
        else:
            if stripped_line.startswith(&quot;*** END OF THE PROJECT GUTENBERG EBOOK&quot;):
                reject = True
                continue
            if stripped_line:
                if stripped_line.startswith(&apos;=&apos;) and stripped_line.endswith(&apos;=&apos;):
                    ret.append(buff)
                    buff = &quot;&quot;
                    buff += stripped_line[1:len(stripped_line)-1] + &quot;\n\n&quot;
                else:
                    buff += line.replace(&apos;\r&apos;, &apos;&apos;)
    if buff.strip():
        ret.append(buff)
    return StringIterableReader().load_data(ret)

docs = load_gutenberg(&quot;https://www.gutenberg.org/cache/epub/59316/pg59316.txt&quot;)

# check that we loaded

assert len(docs) == 58</code></pre><p>The result is a collection of 58 small documents.</p><p>The code below does the following:</p><ol><li>Create a <code>ServiceContext</code> object that holds both the Mixtral LLM and the Jina Embeddings connection. We will use this here and later to create the full RAG system.</li><li>Get an embedding for each small document using the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings API</a>.</li><li>Store the documents and embeddings in LlamaIndex&#x2019;s <a href="https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html?ref=jina-ai-gmbh.ghost.io">built-in in-memory vector store</a> <code>VectorStoreIndex</code>.</li></ol><pre><code class="language-Python">from llama_index import VectorStoreIndex, ServiceContext

service_context = ServiceContext.from_defaults(
    llm=mixtral_llm, embed_model=jina_embedding_model
)
index = VectorStoreIndex.from_documents(
    documents=docs, service_context=service_context
)
</code></pre><h3 id="prepare-a-prompt">Prepare a Prompt</h3><p>Next, we will create a custom prompt template. This prompt specifically asks the LLM not to use information outside of the context information retrieved from the vector database and to specifically say &#x201C;No information&#x201D; when the context does not have any information that answers the user&#x2019;s request.</p><pre><code class="language-Python">from llama_index import PromptTemplate

qa_prompt_tmpl = (
    &quot;Context information is below.\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;{context_str}\\n&quot;
    &quot;---------------------\\n&quot;
    &quot;Given the context information and not prior knowledge, &quot;
    &quot;answer the query. Please be brief, concise, and complete.\\n&quot;
    &quot;If the context information does not contain an answer to the query, &quot;
    &quot;respond with \\&quot;No information\\&quot;.&quot;
    &quot;Query: {query_str}\\n&quot;
    &quot;Answer: &quot;
)
qa_prompt = PromptTemplate(qa_prompt_tmpl)
</code></pre><p>Then, we assemble the query engine using the prompt.</p><p>The key parameter to look at here is <code>similarity_top_k=2</code> in <code>VectorIndexRetriever</code>. This tells the RAG system to put only the best two search matches into the context sent to the LLM.</p><p>We can set this to a larger value if we&#x2019;re confident it will fit into the input context size of the LLM, so this factor is partly model-dependent and partly data-dependent.</p><pre><code class="language-Python">from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index import get_response_synthesizer

# configure retriever
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)

# configure response synthesizer
response_synthesizer = get_response_synthesizer(
    service_context=service_context,
    text_qa_template=qa_prompt,
    response_mode=&quot;compact&quot;,
)

# assemble query engine
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer,
)
</code></pre><h2 id="asking-the-rag-engine-questions">Asking the RAG Engine Questions</h2><p>Now you can ask questions and receive answers based on the text.</p><pre><code class="language-Python">result = query_engine.query(&quot;How is a computer useful on a farm?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">A computer can be useful on a farm by supplementing the calculator,
typewriter, and file cabinet. It can help with repetitive analyses, 
data storage, and management decisions. It can also send and receive 
written or graphic messages by telephone. Additionally, a computer 
program for a farm operation could make recordkeeping and analysis 
easier and improve management abilities. However, the improvements 
in efficiency and cost-effectiveness might be hard to measure in 
dollars.
</code></pre><p>You can ask questions that have an answer from the text that the LLM would never have produced on its own:</p><pre><code class="language-Python">result = query_engine.query(&quot;How much memory does a computer need?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">48K or 64K of memory is needed for most agricultural programs. The 
amount of memory needed depends on the software program and 
recordkeeping requirements.
</code></pre><p>And you can ask questions that have no answer in the text:</p><pre><code class="language-Python">result = query_engine.query(&quot;Who is buried in Grant&apos;s tomb?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code class="language-Text">No information. The context information does not provide any details 
about Grant&apos;s tomb.
</code></pre><h2 id="checking-the-rag-retrieval">Checking the RAG Retrieval</h2><p>You may want to check to see what texts were retrieved for a specific query. For example:</p><pre><code class="language-Python">result = query_engine.query(&quot;What is the address of AgriData Resources?&quot;)
print(result.response)
</code></pre><p>Result:</p><pre><code>205 West Highland Ave. Milwaukee, WI 53203
</code></pre><p>To check the retrieval phase, we have to use the retriever object we created above:</p><pre><code class="language-Python">retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=2,
)
</code></pre><p>You can rerun the retrieval and then inspect the documents:</p><pre><code class="language-Python">retrieved_texts = retriever.retrieve(&quot;What is the address of AgriData Resources?&quot;)
for i, rt in enumerate(retrieved_texts):
  print(f&quot;Text {i+1}:\\n\\n{rt.text}\\n\\n&quot;)
</code></pre><p>Result:</p><pre><code class="language-Text">Text 1:

3. AgriData Network

AgriData is a private information and computing network specializing in
agriculture. It offers immediate access to more than 10,000 pages of
continuously updated business, financial, marketing, weather, and price
information, as well as analyses and recommendations from its own and
other reporters, analysts, economists, meteorologists, and researchers.
It offers several different services, including an online computing
service that allows users to access a library of microcomputer software
programs that can be transferred to the user&apos;s microcomputer; an
agricultural production technology service offering data bases from 40
land-grant universities and from agricultural, chemical, fertilizer,
equipment, seed, and feed companies; an &quot;electronic yellow pages,&quot; or
product service directory for farmers; and electronic mail.
  ADDRESS: AgriData Resources, Inc.
           205 West Highland Ave.
           Milwaukee, WI 53203

Text 2:

2. AGRICOIA

AGRICOIA is an online information service produced by the National
Agricultural Library (NAD of USDA), and is available commercially from
a number of sources (including DIALOG and Bibliographic Retrieval
Services). It provides comprehensive access to information on published
literature pertaining to agriculture.
AGRICOIA is the catalog and index for NAL and covers materials
published since 1970. It includes about 1.5 million citations.
AGRICOIA contains citations to worldwide published books, serial
titles, and journal articles on agriculture and related subjects. In
addition to bibliographic citations of published literature, the system
offers information through several specialized subfiles; these subfiles
include brucellosis (BRU), environmental impact statements covering
1977 and 1978 (ENV), and the Food and Nutrition Information Center,
which emphasizes human nutrition research and education and food
technology (FNC).
Librarians are the main users of this system.
  ADDRESS: To find out more about AGRICOIA, contact:
           Educational Resources Staff
           National Agricultural Library
           Room 1402
           Beltsville, MD 20705
</code></pre><h2 id="making-rag-work-for-you">Making RAG Work For You</h2><p>With Jina Embeddings, LlamaIndex, and Mixtral LLM, you can make your own RAG system that can answer questions about long documents, respond to requests based on a manual or FAQ, or just behave in funny or useful ways, based on a large document context.</p><p>And you can do all this without complex AI operations engineering or even training your models.</p><p>Jina AI is committed to helping you make the most of emerging AI technology. In our coming articles, we delve deep into the practical issues of using embedding models, like discussing ways of chucking documents for use in RAG and search applications. We will also have more integration tutorials and practical advice for text pre-processing and data curation.</p><p>Learn more from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings</a> and <a href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a> websites, or reach out to us at <a href="mailto:contact@jina.ai">contact@jina.ai</a> to discuss how Jina AI&#x2019;s experience can help your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">LlamaIndex - Data Framework for LLM Applications</div><div class="kg-bookmark-description">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/645ad8c999ba34bf0713622b_LlamaBrowserTab.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Data Framework for LLM Applications</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://assets-global.website-files.com/6459a0e3f348e9e898b7df80/6462e6a26afc95b84a8db9dc_LlamaLogo%20White.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">Jina AI website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noopener noreferrer">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 4012 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Full-stack RAG with Jina Embeddings v2 and LlamaIndex"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain]]></title><description><![CDATA[See how companies are integrating SceneXplain with their existing infrastructure to power their product descriptions and storytelling]]></description><link>https://jina.ai/news/a-magic-carpet-ride-building-vivid-product-stories-with-scenexplain/</link><guid isPermaLink="false">658198e20bab3100012d2999</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Lisa Li]]></dc:creator><pubDate>Thu, 21 Dec 2023 15:00:26 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--28-.png" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain"><p>We recently wrote about SceneXplain&apos;s new <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">JSON Schema Store</a>, which lets you use predefined JSON schemas to extract information from images in a structured format. In this post, we&apos;re going to see how that&apos;s used by our partner, <a href="http://www.akia.cn/eindex.asp?ref=jina-ai-gmbh.ghost.io">AKIA Carpet &amp; Rugs</a>, for building up an AI-powered product stories generation bot to empower sales.</p><p>In our prior examples, we&apos;ve primarily used either cURL or Python to access <a href="https://www.notion.so/A-Magic-Carpet-Ride-Building-Vivid-Product-Stories-with-SceneXplain-7a07a53a3ffa48b083d9abe9c3c1c3af?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a>. But in this post, we&apos;re switching things up a bit by using JavaScript.</p><h2 id="about-akia-carpet-rugs">About AKIA Carpet &amp; Rugs</h2><p>In July 2008, Gary Chen founded AKIA Carpet &amp; Rugs, which is now marking its 15th year of operation. The team, initially inspired by traditional Asian craftsmanship, has been dedicated to developing contemporary aesthetic styles. AKIA has grown into a brand known for its unique fusion of modern art with traditional design and weaving techniques.</p><p>As a carpet manufacturer, AKIA specializes in a range of products including mid-to-high-end decorative carpets, tapestries, and carpets for specific projects. The company integrates design, research, production, and sales, both domestically and internationally. Known for innovative design and high-quality products, AKIA has earned a solid reputation in China&apos;s high-end carpet market.</p><h2 id="crafting-a-winning-product-story-with-scenexplain"><strong>Crafting a Winning Product Story with SceneXplain</strong></h2><p>AKIA Carpet &amp; Rugs primarily caters to the mid-to-high-end market, focusing on the aesthetic appeal of their products. Their clientele often looks for carpets that not only complement their home design but also express their unique taste. Recognizing this, AKIA collaborates with skilled designers to create a diverse range of styles, resulting in an extensive collection of carpet designs and images. The challenge lies in effectively communicating the artistic value of these designs to discerning customers, a crucial factor in attracting clients.</p><p>Previously, crafting compelling narratives for a large array of product images, akin to artworks, was a daunting task. This required copywriters who were not only skilled in writing but also knowledgeable in design and art. Additionally, the need to swiftly identify the perfect product image from an extensive collection to meet specific customer preferences was a significant challenge. Traditional image labeling methods, focusing on basic attributes like color, shape, and material, proved insufficient for customers who often describe their needs in more abstract terms.</p><p>SceneXplain offers a dual solution to these challenges. Its approach is based on narration rather than mere description, aiming to weave engaging stories around images. This aligns with SceneXplain&apos;s core philosophy: storytelling brings images to life. By providing stories that resonate with the artistic nature of AKIA&apos;s products, SceneXplain addresses their need for an intuitive, aesthetically aligned way of presenting their carpets.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1183" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-6.47.11-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Embrace Yourself&quot; is a piece of art in the form of a carpet, exuding minimalist charm and contemporary simplicity. This piece features the elegant, abstract depiction of a round, white figure at its center&#x2014;an embodiment of purity and serenity. With meticulously crafted black outlines that grace the soothing white background, this carpet tells a subtle but impactful visual story. Each line is placed with thoughtful precision, evoking emotions and depth without cluttering the visual space. The characteristic round head of the figure in the design adds a touch of futuristic whimsy, creating a space for imagination to soar. The light gray tones and stark white spaces between the lines further enhance the abstract quality, offering a calm and peaceful atmosphere to any room. Whether adorning a modern living area or a chic office space, &quot;Embrace Yourself&quot; promises to be more than just a carpet&#x2014;it is a promise of self-discovery and a celebration of space and form. Its simplicity and abstraction are not just visually appealing but are crafted to engage the observer in an almost meditative contemplation.</blockquote><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="2000" height="1189" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/Screenshot-2023-12-19-at-7.18.18-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><blockquote>&quot;Dancer&quot; is an evocative piece of home decor where color and geometry resonate to a rhythm of their own. Adorning a rich navy blue background, this carpet features a dynamic composition of blue and orange lines and squares that glide across the fabric, suggesting movement and energy, much like a dancer in the spotlight. The striking orange lines dance diagonally, connecting a series of crisp white squares, which are themselves accented with black detailing, reminiscent of precise footwork on a dance stage. Unequivocally modern, &quot;Dancer&quot; employs the contrast of deep blues with vibrant oranges and whites to create an abstract visual narrative that is open to interpretation, yet commands attention. Each element on &quot;Dancer&quot; is painstakingly arranged to bring a sense of balance and fluidity, paralleling a choreographed performance that tells a story with every twist and turn. The pattern encourages the eye to leap and land much like a viewer watching an enthralling solo dance performance, making &quot;Dancer&quot; not just a carpet, but a conversation piece that captivates and inspires.</blockquote><h2 id="product-story-generation-a-holistic-solution">Product story generation: A holistic solution</h2><p>In AKIA&#x2019;s use case, their product manager wants to automatically create stories for all their products. The stories should use some high-quality examples as a guide, which contain aesthetic explanations. Their specific needs are:</p><ul><li>Textual descriptions of their products</li><li>Stories for each product that follow the examples they provide</li><li>Batch processing of images triggered by the chat channel</li></ul><p>Based on these requirements, visual question answering (VQA) is the best fit, because:</p><ul><li>Visual question answering outputs textual descriptions.</li><li>In the question, you can also provide a prompt in a specific format on demand.</li><li>You can inject your own examples into the prompt to guide the model&#x2019;s output.</li><li>Once you have the prompt&#x2019;s basic structure, you can convert it to a template with variables that can be automatically populated each time you use it.</li></ul><p>SceneXplain&#x2019;s API provides a wide range of options for configuring your request, including image captioning, alt-text generation, visual question answering, JSON output, and more.</p><p>Several fields are required to execute a VQA task via the API:</p><ul><li>API endpoint: <code>https://api.scenex.jina.ai/v1/describe</code></li><li>API key: <code>&apos;x-api-key&apos;: token ${YOUR_API_KEY}</code>. You can generate and manage your API key on our <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">API page</a>.</li><li>Request payload, which is your task configuration, providing the image you want to process, setting <code>question_answer</code> in the <code>features</code> property, and setting your prompt in the <code>question</code> property.</li></ul><p>Here&#x2019;s a code snippet for such an API call in JavaScript:</p><pre><code class="language-jsx">const body = {
  &quot;data&quot;: [
    {
			&quot;image&quot;: &quot;The image you want to process, it can be a base64 string or a URL&quot;,
      &quot;features&quot;: [
        &quot;question_answer&quot;
      ],
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;languages&quot;: [
        &quot;en&quot;
      ],
		&quot;question&quot;: &quot;your prompt&quot;
    }
  ]
};

const YOUR_API_KEY = &apos;your_generated_API_key_here&apos;;

fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
  headers: {
    &apos;x-api-key&apos;: `token ${YOUR_API_KEY}`,
    &apos;content-type&apos;: &apos;application/json&apos;
  },
  body: JSON.stringify(body),
  method: &apos;POST&apos;
})
.then(async (resp) =&gt; {
  if (resp.ok) {
    const data = await resp.json();
    console.log(data);
  }
});
</code></pre><p>The payload&#x2019;s <code>data</code> property is an array that can have several configurations, meaning you can batch-process your images via the API.</p><h2 id="connecting-akia-to-scenexplain%E2%80%99s-api-via-bot">Connecting AKIA to SceneXplain&#x2019;s API via bot</h2><p>AKIA uses <a href="https://www.larksuite.com/en_eu?ref=jina-ai-gmbh.ghost.io">Lark</a> for their internal messaging, which is a Chinese application similar to Slack, Microsoft Teams, and Discord. An employee of AKIA can simply send a message to their SceneXplain chatbot that includes an image and a topic.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1821" height="1140" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/21700476455_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/21700476455_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/21700476455_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/21700476455_.pic.jpg 1821w" sizes="(min-width: 720px) 720px"></figure><p>The chatbot sends back a detailed description of the carpet. Here&#x2019;s how it would look in English:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="847" height="291" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/SceneX-carpet--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/SceneX-carpet--3-.png 847w" sizes="(min-width: 720px) 720px"></figure><p>Behind the scenes, there&#x2019;s a middleware service that connects Lark to the SceneXplain API:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png" class="kg-image" alt="A Magic Carpet Ride: Building Vivid Product Stories with SceneXplain" loading="lazy" width="1000" height="221" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/workflow.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/workflow.png 1000w"></figure><p>It shuttles the data between the two services and performs several key tasks:</p><ul><li>Message validation</li><li>API payload generation</li><li>API calling</li><li>Message formatting</li></ul><p>The process is:</p><ol><li>Receive image and topic in message from Lark chatbot</li><li>Check message format is valid. If not, return an error.</li><li>Base64-encode the image and wrap both it and the topic into a payload, using the topic as the question in visual question answering (VQA)</li><li>Send the payload to the API</li><li>API generates a description and sends that back</li><li>Format the message to fit Lark&#x2019;s API</li><li>Send the message back to the Lark chatbot</li></ol><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We&#x2019;re not going to go into the workings of the Lark API here. We want to keep this post as service-agnostic as possible, so it&#x2019;s relevant to whatever service you want to integrate with SceneXplain.</div></div><p>We&#x2019;re just going to focus on the middleware (&#x201C;Your service&#x201D; in the diagram above). All you need is a few lines of code to reformulate the request, pass it on, and then do the same for the response.</p><pre><code class="language-jsx">// Function to call SceneXplain `/describe` API
const describe = async (image: string, name: string, topic: string) =&gt; {
  // prepare payload
  const newBody = {
    data: [
      {
        image: image,
        features: [
          &quot;question_answer&quot;
        ],
        languages: [&apos;zh-CN&apos;],
        algorithm: &apos;Jelly&apos;,
        question: `your prompt, incorporating ${name} and ${topic}, plus optional example for desired output format for in-context learning`
      }
    ]
  }

  // call SceneXplain API
  try {
    const resp = await fetch(&apos;https://api.scenex.jina.ai/v1/describe&apos;, {
      headers: {
        &apos;x-api-key&apos;: `token ${process.env.scenexKey}`,
        &apos;content-type&apos;: &apos;application/json&apos;
      },
      body: JSON.stringify(newBody),
      method: &apos;POST&apos;,
    });
    if (!resp.ok) {
      const error = await resp.text();
      throw error;
    }
    const data = await resp.json() as any;
    console.log(`describe result: ${JSON.stringify(data, null, 2)}`);
    if (data.code !== 200) throw data;
    const result = data.result[0];

    // get result in the required language
    return result.i18n[&apos;zh-CN&apos;];
  } catch (e) {
    console.log(`describe error: ${JSON.stringify(e, null, 2)}`);
    return &apos;&apos;;
  }
}
</code></pre><p>As you can see from the <code>question</code> field in the example payload above, you can include some example output to help the algorithm in generating the kind of description you desire. And, of course, you don&#x2019;t <em>have</em> to use JavaScript to build your middleware service - any programming language with an HTTP library can access <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&#x2019;s API</a>.</p><h2 id="wrapping-up">Wrapping up</h2><p>Do you want to follow in AKIA&#x2019;s footsteps and use SceneXplain to build vivid product stories from your images and videos? Head over to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to get started. Or for business use cases, fill in our <a href="https://jina.ai/contact-sales/?ref=jina-ai-gmbh.ghost.io">sales form</a> and we&#x2019;ll be happy to roll out the red carpet.</p>]]></content:encoded></item><item><title><![CDATA[Multi-Agent Simulations in PromptPerfect: 𝑛 Heads Are Better Than One]]></title><description><![CDATA[Discover the real-world impact of multi-agent simulations and see practical examples of systems uniting individual strengths to tackle complex tasks, offering efficient and tailored solutions across various domains]]></description><link>https://jina.ai/news/multi-agent-simulations-in-promptperfect-n-heads-are-better-than-one/</link><guid isPermaLink="false">658177280bab3100012d296d</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Tue, 19 Dec 2023 15:00:58 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--27-.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><p>In this post, we&#x2019;re going to explore how multi-agent simulations are not just a conceptual framework but a tool with tangible applications in various domains. By delving into practical examples, we can see how these systems bring together the strengths of individual agents to address complex tasks and deliver solutions that are more efficient, comprehensive, and tailored to specific needs. These examples will illustrate the versatility and effectiveness of multi-agent simulations in real-world scenarios, demonstrating their value in diverse fields.</p><h2 id="a-quick-history-lesson">A quick history lesson</h2><p>In the ever-evolving landscape of technology and problem-solving, our journey has been marked by significant milestones. Initially, coding was our primary tool for instructing computers to perform tasks and solve problems. This era was defined by programming languages and algorithms, where solutions were hard-coded. As technology advances, we&#x2019;re transitioning to a more intuitive approach: prompting. This phase leverages artificial intelligence, where we can simply ask a system a question or present a problem, and it generates solutions based on its training and algorithms. This method is more flexible and user-friendly, catering to a wider range of problems without the need for extensive coding knowledge. However, even now, we&apos;re seeing the glimmerings of a newer era that promises to revolutionize how we approach problem-solving: the use of agents.</p><h2 id="multi-agent-simulations-as-easy-as-1-2-3">Multi-agent simulations: As easy as 1, 2, 3</h2><p>Before we dive into using multiple agents, let&#x2019;s define a few key terms: <em>Agent</em>, <em>action,</em> <em>environment</em>, and <em>simulation</em>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="861" height="627" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Multi-agent-sim.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Multi-agent-sim.png 861w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Agent:</strong> An <em>agent</em>, in the realm of artificial intelligence, performs actions to achieve a goal, like planning a vacation or collecting state-of-the-art research information online.</li><li><strong>Action:</strong> An <em>action</em> is a state change in an environment initiated by an agent. This may include talking to other agents, searching Google, or accessing external APIs.</li><li><strong>Environment:</strong> An <em>environment</em> is a text description that constrains what the agent can do. In a travel planning simulation, this may be a (virtual) travel agent office.</li><li><strong>Simulation:</strong> A <em>simulation</em> is what you get when you put all of the above together: An agent (or agents) performing actions in a given environment.</li></ul><h2 id="diving-deeper-into-agents">Diving deeper into agents</h2><p>AI agents can range from simple, rule-based systems to <em>complex entities capable of learning</em>. They can analyze data, learn from experiences, and make predictions or recommendations.</p><p>Agents share several characteristics:</p><ul><li><strong>Autonomy</strong>: Agents operate without direct human intervention, making decisions based on their programming and the data they encounter.</li><li><strong>Reactivity</strong>: They perceive their environment and respond to changes in real time, adapting their actions as necessary.</li><li><strong>Proactivity</strong>: Beyond just reacting, agents can take initiative, anticipating future states and planning actions accordingly.</li><li><strong>Social ability</strong>: Agents can communicate and collaborate with other agents or humans to achieve complex goals.</li></ul><p>However, despite these capabilities, single AI agents face intrinsic limitations:</p><ul><li><strong>Specialization limit</strong>: Like human experts, each agent typically specializes in a particular domain or task, lacking the breadth of knowledge to handle unrelated challenges.</li><li><strong>Complex problem solving</strong>: Tackling multi-dimensional problems that require diverse expertise and perspectives is beyond the scope of a single agent.</li><li><strong>Limited adaptability</strong>: While adaptable within their domain, agents may struggle with novel situations that fall outside their programmed parameters or training data.</li></ul><p>In summary, while individual AI agents represent a significant technological advancement, their true potential is unlocked when they operate as part of a multi-agent system, combining their strengths and compensating for each other&apos;s limitations.</p><h2 id="multiple-agents-n-heads-are-better-than-one">Multiple agents: <em>n</em> heads are better than one</h2><p>The solution to those limitations lies in multi-agent simulation. This approach involves using multiple AI agents, each with its specialty or focus area, working in tandem to solve complex problems. By collaborating, these agents can cover a broader range of issues and provide more comprehensive solutions than a single agent could. <a href="https://arxiv.org/pdf/2307.05300.pdf?ref=jina-ai-gmbh.ghost.io">Current research</a> shows that the performance of agents in specific tasks depends on the persona of the agent.</p><h2 id="building-a-vacation-planner-with-promptperfect">Building a vacation planner with PromptPerfect</h2><p>Let&#x2019;s consider planning a vacation as a practical example of a multi-agent simulation. This example will use two agents:</p><ul><li>The Travel Planner finds the best travel routes.</li><li>The Accommodation Finder finds affordable accommodation.</li></ul><p>Together, they provide a complete vacation plan tailored to your preferences and budget.</p><p>Follow the steps below to create a vacation planner in PromptPerfect:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_Simulation_for_Vacation_Planning__NbBdmWiUS5SC_QZ00lc21Q" width="640" height="640" allowfullscreen frameborder="0">

</iframe>
<!--kg-card-end: html-->
<p>We can now check the output to see the results. Here&#x2019;s one example action taken by the Travel Planner agent:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png" class="kg-image" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One" loading="lazy" width="1588" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Untitled--47--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Untitled--47--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Untitled--47--1.png 1588w" sizes="(min-width: 720px) 720px"></figure><p>As you can see, the Travel Planner agent calls the flight API endpoint to find flights for a one-week trip in July. Since the agent is aware of the flight API&#x2019;s documentation, it sends the correct parameters with the request and retrieves the flight data.</p><p>Once it&#x2019;s done that, it can talk to the Accommodation Finder agent to find suitable properties in the chosen city. The Accommodation Finder, in turn, calls a hotel booking API to (you guessed it) arrange the accommodation.</p><h2 id="further-applications">Further applications</h2><p>This simple outline, although given a relatively small task, also applies to far more complex problems:</p><ul><li><strong>Paper Research</strong>: In academic or professional research, one agent could specialize in identifying relevant papers, another in summarizing content, and a third in identifying key themes and gaps in the research.</li><li><strong>Coding</strong>: For software development, one agent could get the user requirements, another write the code, and a third make the deployment.</li><li><strong>Online Shopping Deal Finder</strong>: This could involve one agent tracking price changes, another comparing product features, and a third assessing user reviews to find the best deals.</li></ul><p>As we&apos;ve seen, multi-agent simulations open up a world of possibilities that transcend the limitations of individual AI agents. Whether it&apos;s planning your dream vacation, conducting thorough research, developing software, or finding the best online shopping deals, these simulations offer tailored, efficient, and comprehensive solutions.</p><p>But the real magic happens when you dive in and experience it yourself. By visiting the PromptPerfect website, you&apos;ll have the opportunity to create and manage your own multi-agent simulations.</p><h2 id="get-started-with-agents">Get started with Agents</h2><p>So, why wait? Head over to the <a href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">PromptPerfect website</a> now, and start crafting your own simulations. Whether for work, study, or personal projects, discover the power and flexibility of multi-agent AI at your fingertips. Your journey into the future of problem-solving starts today!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="Multi-Agent Simulations in PromptPerfect: &#x1D45B; Heads Are Better Than One"></div></a></figure>]]></content:encoded></item></channel></rss>
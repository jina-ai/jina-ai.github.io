<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.74</generator><lastBuildDate>Mon, 27 Nov 2023 15:12:16 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[SceneXplain's JSON Schema Store: Automate Your Alt-Text, and More!]]></title><description><![CDATA[Take the hassle out of extracting data from images with SceneXplain's new JSON Schema Store. Discover and share reusable JSON schemas. Create, contribute, and access schemas easily through GUI or API]]></description><link>https://jina.ai/news/scenexplains-json-schema-store-automate-your-alt-text-and-more/</link><guid isPermaLink="false">655f48ed14230d0001313816</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 27 Nov 2023 15:00:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><p>We&apos;re starting to run low on phrases like &quot;we&apos;re happy to announce x&quot;, or &quot;announcing x&quot; every time we push out a new feature in SceneXplain. So let&apos;s just jump to the meat: SceneXplain has a new JSON Schema Store, where you can discover and share reusable JSON schemas. This builds on our recent &quot;extract JSON from image&quot; feature, which lets you specify a JSON Schema when uploading an image, and get information back in JSON that adheres to that schema.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain&#x2019;s Image-to-JSON: Extract Structured Data from Images with Precision</div><div class="kg-bookmark-description">Pushing the boundaries of visual AI, we&#x2019;re thrilled to unveil SceneXplain&#x2019;s Image-to-JSON feature. Dive into a world where images aren&#x2019;t just seen, but deeply understood, translating visuals into structured data with unparalleled precision.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/09/Explore-image-storytelling-beyond-pixels--7-.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure><h2 id="what-is-a-json-schema">What is a JSON Schema?</h2><p>According to the <a href="https://json-schema.org/learn/getting-started-step-by-step?ref=jina-ai-gmbh.ghost.io">official website</a>, JSON Schema is a vocabulary that you can use to annotate and validate JSON documents. With SceneXplain, you can upload a schema like:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;alt_tag&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;description&quot;: &quot;the most concise description possible of the image&apos;s content and function. Do not describe elements that are purely decorative (e.g. part of the website&apos;s design, not content). Do not include text like &apos;this image contains&apos; or &apos;image depicts&apos;&quot;
    }
  }
}</code></pre><p>Along with an image...</p><figure class="kg-card kg-image-card"><img src="https://media.discordapp.net/attachments/1062299968955764777/1177229401763614794/generativejina_Close-up_view_of_a_tall_tree_covered_in_moss_wit_029426a1-96f1-49e9-903d-7772488af950.png?ex=6571bf51&amp;is=655f4a51&amp;hm=6f73c30f7a67b93fa5e5bcb615a047701a6b1ddd4cf3debf39aaf58e0a348033&amp;=&amp;width=625&amp;height=625" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><p></p><p>And get the following output:</p><pre><code class="language-json">{
  &quot;alt_text&quot;: &quot;Close-up view of a tall tree covered in moss with a clear blue sky in the background&quot;
}</code></pre><p>The JSON schema above is pretty straightforward, with only one field. But crafting more complicated JSON schemas can be a lot of effort, even assuming they work well. And sharing them can get messy, having to shuffle code blocks around Slack or other messaging clients (believe me, I&apos;ve been there!)</p><h2 id="how-does-the-schema-store-work">How does the Schema Store work?</h2><p>The Schema Store solves these headaches. Now you copy and use someone else&apos;s Schema without having to go through the bother of crafting your own. Or you can easily share your schemas with colleagues for more efficient image processing.</p><h3 id="use-community-schemas">Use community schemas</h3><p>To get started, create an account on SceneXplain and follow these steps:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h3 id="create-your-own-schema">Create your own schema</h3>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h2 id="examples">Examples</h2><p>Here are a few more examples of what you can do with the schemas from the Store:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/emoji.png" width="585" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/tinder.png" width="590" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/super.png" width="590" height="1231" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/land_use.png" width="584" height="827" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text.png" width="584" height="795" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text_language.png" width="584" height="870" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/brand_sentiment.png" width="597" height="876" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div></div></figure><h2 id="bulk-generating-alt-text-via-scenexplains-api">Bulk-generating alt-text via SceneXplain&apos;s API</h2><p>Now let&apos;s get our hands dirty by calling <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a> to bulk process images using a schema from the Store. In our use case, we&apos;ll perform the simple task of generating alt-text.</p><blockquote>Alt text is used in HTML to describe the appearance and function of an image on a webpage. They are crucial for accessibility, as they provide a text alternative for screen readers used by visually impaired users, and also aid in search engine optimization (SEO) by allowing search engines to better understand the content of the images.</blockquote><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">You can play with the code in <a href="https://www.notion.so/SceneXplain-s-JSON-Schema-Store-Automate-Your-Alt-Text-and-More-418d90b1477f459eb0d517869dcc80cd?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">this notebook</a>. Be sure to specify your own <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain API key</a>.</div></div><p>We won&apos;t go through each individual step in depth, since the notebook handles that. We&apos;ll just give an overview of each step. Please refer to the notebook for the real code.</p><h3 id="choose-the-schema">Choose the schema</h3><p>We&apos;ll use the alt-tagger schema that I created earlier. Be sure to note its ID! In our case that&apos;s <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-61.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="584" height="826"></figure><h3 id="test-the-api">Test the API</h3><p>We can quickly test the API with a code snippet by clicking the API tab:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/api.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="582" height="221"></figure><p>You can access the API in Python, cURL, or JavaScript. Right now we&apos;ll use cURL since it&apos;s nice and short. We&apos;ll send the URL to the following <a href="https://www.pexels.com/photo/heron-by-the-sea-18822188/?ref=jina-ai-gmbh.ghost.io">image</a>, along with our SceneXplain key:</p><figure class="kg-card kg-image-card"><img src="https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=1260&amp;h=750&amp;dpr=1" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><pre><code class="language-bash">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;data&quot;:[
    {&quot;image&quot;: &quot;https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg&quot;,
    &quot;features&quot;: [&quot;json&quot;],
    &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;}
  ]}&apos;</code></pre><p>After a few seconds (and some prettification via <a href="https://github.com/jqlang/jq?ref=jina-ai-gmbh.ghost.io"><code>jq</code></a>), we get the following JSON, which includes our alt-tag:</p><pre><code class="language-json">{
  &quot;code&quot;: 200,
  &quot;status&quot;: 20000,
  &quot;result&quot;: [
    {
      &quot;id&quot;: &quot;BiU7me3ytaKn4KaF0v84&quot;,
      &quot;image&quot;: &quot;https://storage.googleapis.com/causal-diffusion.appspot.com/imagePrompts%2Fe4710bdc-73de-469c-9202-c2c0fe1073af%2Foriginal.png&quot;,
      &quot;features&quot;: [
        &quot;json&quot;
      ],
      &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;,
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;uid&quot;: &quot;NIDud1AA3NMTBFYZ4MEpNZy5om62&quot;,
      &quot;optOut&quot;: false,
      &quot;fullyOptOut&quot;: false,
      &quot;__developer_options&quot;: null,
      &quot;text&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;,
      &quot;i18n&quot;: {
        &quot;en&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;
      },
      &quot;userId&quot;: &quot;foo&quot;,
      &quot;createdAt&quot;: 1700737281840,
      &quot;languages&quot;: []
    }
  ]
}</code></pre><h3 id="collect-your-data">Collect your data</h3><p>Moving forwards, we&apos;ll be using Python for our code. Assuming we have a folder of images, for each image we&apos;ll need to send:</p><ul><li>The image file converted to a <a href="https://en.wikipedia.org/wiki/Data_URI_scheme?ref=jina-ai-gmbh.ghost.io">base64-encoded datauri</a></li><li>The SceneXplain features we want to use. In our case that&apos;s just <code>[&apos;json&apos;]</code>.</li><li>The ID of the JSON Schema: <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</li></ul><p>We throw all of these into a dict, then throw each image&apos;s dict into a list.</p><h3 id="send-the-data-to-scenexplain">Send the data to SceneXplain</h3><p>That&apos;s really just a case of making an HTTP request and sending over our data. We&apos;ve wrapped it into a function in the notebook.</p><h3 id="process-the-output-data">Process the output data</h3><p>This is just a case of extracting the alt-tag from the output JSON, and in our case, writing it to a CSV file then zipping it up along with all the other images. Our <code>alt-text.csv</code> looks like:</p><pre><code class="language-csv">filename,alt-tag
/tmp/tmpexs68in3/free-photo-of-leaves-on-the-branch.jpeg,&quot;Close-up of a branch with mix of green and yellow leaves, portraying the onset of autumn, set against the blurred background of a serene forest.&quot;
/tmp/tmpexs68in3/free-photo-of-holida-christmas-party-drinks-ornaments.jpeg,&quot;Vividly colored table setting with red tablecloth, two empty crystal glasses, Christmas decorations including candy canes, gold ornaments, a small Christmas tree, on a backdrop of a green curtain and pink walls.&quot;
/tmp/tmpexs68in3/free-photo-of-red.jpeg,Close-up of a red classic Malibu car&apos;s rear end.
/tmp/tmpexs68in3/free-photo-of-pose-woman-dress-in-the-desert-gold-light-curly-hair.jpeg,&quot;Curly-haired woman in brown coat standing on beach, with the sun beaming light onto her.&quot;
/tmp/tmpexs68in3/free-photo-of-a-bowl-of-granola-with-fruit-and-nuts-on-a-wooden-cutting-board.jpeg,Bowl of granola with strawberries and pomegranate seeds on a wooden board on a dark brown table
/tmp/tmpexs68in3/free-photo-of-alexandrine-parakeet-in-side-view.png,&quot;Close-up of a green parrot with a red beak and yellow eyes on a branch, looking to the right; with a blurry green background&quot;
/tmp/tmpexs68in3/pexels-photo-12015253.png,&quot;Central, old gas pump with a red and white color scheme labeled &apos;Benzin&apos; with an attached black hose against a street scene backdrop&quot;</code></pre><p>You can view the process in full in <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/a11y-alt-tags/alt-tagger.ipynb?ref=jina-ai-gmbh.ghost.io">the notebook</a>.</p><h2 id="get-started-with-scenexplain-and-the-schema-store">Get started with SceneXplain and the Schema Store</h2><p>Like what you see? Go to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to sign up and get started, and head on over to <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">our Discord</a> to join the conversation!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Artificial General Intelligence is Cursed, And Science Fiction isn't Helping]]></title><description><![CDATA[AI is cursed by intellectual hubris, moving goalposts, bad incentives, and science fiction. Recent talk about Artificial General Intelligence only highlights those curses.]]></description><link>https://jina.ai/news/artificial-general-intelligence-is-cursed-and-science-fiction-isnt-helping/</link><guid isPermaLink="false">655b2361bb728c000101beaa</guid><category><![CDATA[Insights]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 20 Nov 2023 15:22:02 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><p>Many, many, many, (too many <em>many</em>&#x2019;s) years ago, I was a student taking <em>Introduction to Artificial Intelligence</em> at Stanford. <a href="https://en.wikipedia.org/wiki/Daphne_Koller?ref=jina-ai-gmbh.ghost.io">Daphne Koller</a> was my teacher, and my textbook was a first edition <a href="https://aima.cs.berkeley.edu/?ref=jina-ai-gmbh.ghost.io"><em>Artificial Intelligence: A Modern Approach</em></a> by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Stuart Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Peter Norvig</a>, which still sits on the shelf in my office.</p><p>A lot of us working in AI today have that textbook on our shelves.</p><p>That&#x2019;s why a <a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io">recent article in <em>Noema Magazine</em></a> by <a href="https://en.wikipedia.org/wiki/Blaise_Ag%C3%BCera_y_Arcas?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Blaise Ag&#xFC;era y Arcas</a> of Google Research and Peter Norvig, provocatively titled <em>Artificial General Intelligence Is Already Here</em> caught so much attention.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Artificial General Intelligence Is Already Here | NOEMA</div><div class="kg-bookmark-description">Today&#x2019;s most advanced AI models have many flaws, but decades from now, they will be recognized as the first true examples of artificial general intelligence.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.noemamag.com/wp-content/uploads/2020/06/cropped-ms-icon-310x310-1-180x180.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">NOEMA</span><span class="kg-bookmark-publisher">Blaise Ag&#xFC;era y Arcas</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://noemamag.imgix.net/2023/10/Noema_Card-display-2000x1000-0-00-04-04.jpg?fit=crop&amp;fm=pjpg&amp;h=628&amp;ixlib=php-3.3.1&amp;w=1200&amp;wpsize=noema-social-facebook&amp;s=2e1793c1499718ac74749479e3989c10" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><p>To summarize their claim, the core elements of general intelligence are already met by recent large language models: They can perform with human competence at tasks outside of their explicit training over a wide range of domains. Although hardly without flaws, human performance is hardly flawless either. Therefore, we should acknowledge that these systems have achieved &#x201C;general intelligence.&#x201D;</p><p>The rest of the article is, frankly, mostly taking down strawmen and rehashing old fights from the 90s. Honestly, the people who graduated from university in the last twenty years never fought in the Great Symbolicist-Connectionist War. That conflict ended a long time ago, and any Symbolicist bitter-enders who&#x2019;ve survived into the age of GPT should be allowed to stay in their thatched jungle huts, collecting nuts and fruits and proclaiming their loyalties to Fodor, Pinker, and Chomsky somewhere far away where it doesn&#x2019;t have to bother anyone.</p><p>I fought in that conflict (for the Connectionists, thank you), and on behalf of myself and the other veterans, please give it a rest! (<a href="http://garymarcus.com/index.html?ref=jina-ai-gmbh.ghost.io">Gary Marcus</a>, I mean you too!)</p><p>&#x201C;Artificial General Intelligence&#x201D; (AGI) is, to put it mildly, a controversial concept in AI research. The word is of recent vintage, dating to about the turn of the 21st century, and comes from an attempt to make clearer an idea that has circulated in AI since, arguably, before the invention of computers. It&#x2019;s gone by a lot of names over the years: &#x201C;true AI,&#x201D; &#x201C;real AI&#x201D;, &#x201C;strong AI,&#x201D; &#x201C;human-equivalent AI,&#x201D; etc. The history of attempts to define it in some testable way is, to say the least, not encouraging, but it doesn&#x2019;t seem to stop anyone from trying.</p><h2 id="the-curses-of-ai">The Curses of AI</h2><p>Artificial intelligence is a cursed field.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="1050" height="1050" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1050w" sizes="(min-width: 720px) 720px"></figure><p>The word was coined by <a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)?ref=jina-ai-gmbh.ghost.io">John McCarthy</a> in 1956, in large part to distinguish it from &#x201C;cybernetics,&#x201D; a label associated with <a href="https://en.wikipedia.org/wiki/Norbert_Wiener?ref=jina-ai-gmbh.ghost.io">Norbert Wiener</a>, who saw it as an interdisciplinary field with political implications that did not appeal to McCarthy and his colleagues. So, from the very beginning, AI has been associated with intellectual hubris and disinterest in social consequences, willfully distancing itself from the humanities &#x2014; the study of human works &#x2014; while purporting to build machines that do the work of humans.</p><p>That is one of AI&apos;s great curses: There is an expansive, argumentative, routinely acrimonious literature on the nature of intelligence and human intellectual abilities, dating back centuries and even millennia, and discussions of artificial intelligence reference it approximately never. How exactly are you going to discuss &#x201C;artificial intelligence&#x201D; if you can&#x2019;t say what it means for something to be intelligent, or how you would demonstrate that?</p><p>Ag&#xFC;era y Arcas of Google and Norvig are not especially clear on how they&#x2019;ve defined &#x201C;general intelligence&#x201D; despite having a section titled <em>&#x201C;What Is General Intelligence?&#x201D;</em></p><p>Broadly, their claim seems to be:</p><ol><li>General intelligences can do more than one kind of thing.</li><li>General intelligences can do things that their designers didn&#x2019;t specifically intend for them to be able to do.</li><li>General intelligences can do things that are specified in your instructions to them rather than their programming.</li></ol><p>This is my summary, and perhaps I have misunderstood them, but the section where they explain this goes on a number of tangents that make it hard to identify what they intend to say. They appear to be using the label &#x201C;general intelligence&#x201D; to contrast with &#x201C;narrow intelligence,&#x201D; another new term (I can&#x2019;t find reference to it before the 2010s) that seems to have the same relationship to what we used to call &#x201C;weak AI&#x201D; as &#x201C;general intelligence&#x201D; has to &#x201C;strong AI.&#x201D;</p><p>They define it as follows:</p><blockquote>Narrowly intelligent systems typically perform a single or predetermined set of tasks, for which they are explicitly trained. Even multitask learning yields only narrow intelligence because the models still operate within the confines of tasks envisioned by the engineers.</blockquote><p>They contrast this with &#x201C;general intelligence&#x201D;:</p><blockquote>By contrast, frontier language models can perform competently at pretty much any information task that can be done by humans, can be posed and answered using natural language, and has quantifiable performance.</blockquote><p>This approach poses some obvious problems. It defines artificial general intelligence to exist only when engineers have poor imaginations. That doesn&#x2019;t seem quite right.</p><p>The most frequently cited example of an AI model performing outside of its explicit training (a.k.a. <em>zero-shot learning</em>) is the &#x201C;zebra&#x201D; example: Let us say we have a multimodal AI model that has never had a picture of a zebra or mention of a zebra anywhere in its training data. So we tell it that a zebra is a horse with stripes, and then we present it with pictures, some of zebras, some not, and ask it if each one is a zebra.</p><p>The current generation of multimodal models is capable of doing a good job of this.</p><p>I don&#x2019;t know if this is exactly what Ag&#xFC;era y Arcas and Norvig have in mind as performing outside of &#x201C;the confines of tasks envisioned by the engineers&#x201D; because they give no examples, nor does the article they link to on this subject. (&#x201D;A general AI model can perform tasks the designers never&#xA0;<a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/?ref=jina-ai-gmbh.ghost.io">envisioned</a>.&#x201D;) But this kind of zero-shot learning does seem to be what they mean.</p><p>However, it&apos;s not clear that this really is an example of going outside of the tasks envisioned by the model&apos;s designers. Given that the model was explicitly trained to recognize horses and stripes and to connect the features it finds in images to natural language words and statements, it was explicitly trained to handle all parts of the problem. So is it right to say it&#x2019;s exceeded the bounds of &#x201C;narrow intelligence?&#x201D;</p><p>Other zero-shot learning examples suffer from the same problem.</p><p>Also, unexpected correct results from computers have been around for some time. As an example, <a href="https://www.jpl.nasa.gov/news/remote-agent-experiment-meets-all-objectives?ref=jina-ai-gmbh.ghost.io">this article from 1999</a> shows a logistic planner in a NASA space probe doing something that not only was correct and unexpected but that the engineers could never have had in mind when they designed it.</p><p>But there&#x2019;s an even larger problem with defining artificial general intelligence this way.</p><p>Let&#x2019;s consider the large language models, like ChatGPT. What were the tasks its engineers designed it to do? Only one task: Add the next word to a text.</p><p>The web interface to ChatGPT obscures what happens behind the scenes, but ChatGPT works like this:</p><blockquote>Iteration 1:<br>    User Input: <strong>Knock, knock.</strong> <br>    GPT Output: <strong>Who&#x2019;s</strong><br><br>Iteration 2:<br>    GPT Input: <strong>Knock, knock. Who&#x2019;s</strong> <br>    GPT Output: <strong>there</strong><br><br>Iteration 3: <br>    GPT Input: <strong>Knock, knock. Who&#x2019;s there</strong> <br>    GPT Output: <strong>?</strong><br><br>Return to user: <strong>&#x201C;Who&#x2019;s there?&#x201D;</strong></blockquote><p>That&#x2019;s all it does! You see it writing poetry, answering questions, or whatever else you like, but under the surface, it&#x2019;s a sequence of identical simple operations to append a word to a text.</p><p>We could say that ChatGPT does just one task, the one it was explicitly designed for.</p><p>So, is this a single-task ability or multi-task abilities? Is it doing more than the engineers designed it to do, or only exactly the one thing they designed it to do?</p><p>It depends on how you choose to look at it. This kind of ambiguity is not trivial to resolve.</p><p>The article highlights &#x201C;[a] healthy skepticism about metrics for AGI&#x201D; as a legitimate concern. However, the problem with &#x201C;general intelligence&#x201D; isn&#x2019;t metrics and thresholds. It&#x2019;s definitions. AI&#x2019;s lack of clear definitions of its terms is the direct cause of AI&#x2019;s second great curse: Moving goalposts.</p><p>Goalpost-moving is so central to AI as an enterprise that Professor Koller introduced me to it in her introductory class long ago, I think on, like, Week 3.</p><p>Every time researchers solve some problem in AI, it&#x2019;s redefined as not needing intelligence. As <a href="https://en.wikipedia.org/wiki/Pamela_McCorduck?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Pamela McCorduck</a> puts it in her influential history of AI:</p><blockquote>Practical AI successes, computational programs that actually achieved intelligent behavior, were soon assimilated into whatever application domain they were found to be useful, and became silent partners alongside other problem-solving approaches, which left AI researchers to deal only with the &#x201C;failures,&#x201D; the tough nuts that couldn&#x2019;t yet be cracked. Once in use, successful AI systems were simply considered valuable automatic helpers. [<a href="https://doi.org/10.1201/9780429258985?ref=jina-ai-gmbh.ghost.io">McCorduck 2004</a>]</blockquote><p>Or, in shorter words, attributed to <a href="https://en.wikipedia.org/wiki/Larry_Tesler?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Larry Tesler</a> (the inventor of copy-paste functionality):</p><blockquote>AI is whatever hasn&apos;t been done yet.</blockquote><p>AI is a <a href="https://en.wikipedia.org/wiki/Red_Queen%27s_race?ref=jina-ai-gmbh.ghost.io">Red Queen&#x2019;s Race</a> where no matter how fast you run, the finish line moves away even faster. The history of AI is a history of failure because we excise the successes from it. Never mind that AI research was already years ago bringing us decent spellcheckers, functional machine translations, and visual object recognition, among other accomplishments of modern technology.</p><p>Or that half the world today runs on logistic planners that started life in the 1960s as AI research in automated theorem proving. Yes, your &#x201C;supply chain&#x201D; is already AI from end to end. AI didn&#x2019;t get rid of the drudgery of trucking and manually loading and unloading goods. It got rid of the comfy office job of scheduling.</p><p>In the history of AI goalpost-moving, Ag&#xFC;era y Arcas and Norvig&#x2019;s contribution is somewhat novel: We&#x2019;re used to moving the goalposts of &#x201C;real&#x201D; AI to make them ever harder to reach. They&#x2019;ve moved them to say we&#x2019;ve already passed them.</p><p>The cause of the moving goalposts is more complex: AI is cursed with bad incentives. It lends itself to hype. We might blame this on the grants process and academic pressures to overpromise, or on late capitalism and the way the tech investor class likes to chase after the latest shiny thing in hopes of owning a &#x201C;disruptive&#x201D; new technology.</p><p>But I put a lot of the blame on AI&#x2019;s final curse: Science fiction.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 700w"></figure><p>Now, understand me: This is not a criticism of the makers of science fiction; it&#x2019;s a criticism of its consumers. I have been watching and reading science fiction since I was five years old, and I love it. It&#x2019;s a popular genre, beloved by millions, if not billions, of people.</p><p>But the ideas people have in their heads about AI, including the people who do AI research, have been so thoroughly colonized by fictitious machines that it has become difficult to separate legitimate debates from the dramatic visions of novelists and movie makers.</p><p>Ideas about &#x201C;artificial general intelligence&#x201D; or &#x201C;real&#x201D; AI owe far more to science fiction than to science. It&#x2019;s not the responsibility of the Issac Asimovs, the Stanley Kubriks, or the James Camerons of the world to police their imaginations or ours. It is the responsibility of AI professionals &#x2014; researchers, marketers, investors, and corporate frontmen &#x2014; to understand and make clear that &#x201C;<a href="https://en.wikipedia.org/wiki/Positronic_brain?ref=jina-ai-gmbh.ghost.io">positronic brains</a>,&#x201D; <a href="https://en.wikipedia.org/wiki/HAL_9000?ref=jina-ai-gmbh.ghost.io">HAL 9000</a>, and <a href="https://en.wikipedia.org/wiki/The_Terminator?ref=jina-ai-gmbh.ghost.io">killer robots with inexplicable Austrian accents</a> have no place in engineering.</p><p>We move the goalposts in AI because every accomplishment ends up disappointing us when it doesn&#x2019;t live up to our vision of AI. Rational beings might interrogate their visions of AI rather than move around the goalposts and say: &#x201C;This time, it&#x2019;s different!&#x201D;</p><p>Humans have deep prejudices that they struggle to recognize. Ag&#xFC;era y Arcas and Norvig recognize this in their mention of the &#x201C;<a href="https://en.wikipedia.org/wiki/Being_There?ref=jina-ai-gmbh.ghost.io">Chauncey Gardiner effect</a>.&#x201D; For younger readers who don&#x2019;t know the works of Peter Sellers and Hal Ashby, we might also call this the &#x201C;Rita Leeds effect&#x201D; after Charlize Theron&#x2019;s role in <a href="https://en.wikipedia.org/wiki/Arrested_Development_(season_3)?ref=jina-ai-gmbh.ghost.io">season three of <em>Arrested Development</em></a>. It&#x2019;s the same schtick.</p><p>Until the 1970s, there were serious social scientists who believed that black people, at least American ones, were systematically intellectually stunted and pointed to the way they spoke English as proof. Even today, plenty of people view African American Vernacular English (usually abbreviated as AAVE because who has time to spell that out?) as &#x201C;broken&#x201D; or &#x201C;defective&#x201D; or indicative of poor education.</p><p>Among linguists, no one of any importance today thinks that. We recognize AAVE as a fully functional variant form of English, just as well-suited to accurate and effective communication as standard English or any of its other variants. It has no relationship to cognitive abilities whatsoever.</p><p>Yet, the prejudice against it remains.</p><p>When we evaluate ChatGPT as &#x201C;real&#x201D; AI, or at least on the path to &#x201C;real&#x201D; AI, we are demonstrating the same prejudice, turned around. That should give us some pause. It should lead us to ask if <em>anything</em> we think makes software intelligent is really intelligence or just manipulating our prejudices. Is there any way to objectively judge intelligence at all? We may never be able to fully dismiss our prejudices.</p><p>It does not take a lot of interaction with ChatGPT to see it fall short of our science-fiction-based visions for AI. It can do some neat tricks, maybe even some useful ones, but it&#x2019;s not <a href="https://en.wikipedia.org/wiki/WALL-E?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">WALL-E</a>, or <a href="https://en.wikipedia.org/wiki/C-3PO?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">C-3PO</a>, or <a href="https://en.wikipedia.org/wiki/Data_(Star_Trek)?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Commander Data</a>. It&#x2019;s not even Arnold Schwarzenegger&apos;s iconic but not terribly verbal T-800, although I don&#x2019;t doubt you could readily integrate ChatGPT with a text-to-speech model that has a thick Austrian accent and pretend.</p><p>Changing the labels on things doesn&#x2019;t change things, at least not in any simple way. We could move the goalposts for &#x201C;artificial general intelligence&#x201D; in such a way that we could say we&#x2019;ve already accomplished that, but that will not make today&#x2019;s AI models more satisfactory to people looking for HAL 9000. It would just render the label &#x201C;general intelligence&#x201D; meaningless.</p><p>People would still want something we can&#x2019;t give them. Something we might never be able to give them.</p><h2 id="are-humans-still-special">Are Humans Still Special?</h2><p>Among the strawmen Ag&#xFC;era y Arcas and Norvig take on in a whole section is <em>&#x201C;Human (Or Biological) Exceptionalism.&#x201D;</em> People believe all kinds of things, but unless you count religious notions of the &#x201C;soul&#x201D;, it is hard to believe this is a commonly held position among AI professionals. More likely, most of them don&#x2019;t think about it at all.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 700w"></figure><p>The lesson we should be learning from recent AI is that it is difficult to identify a human skill that is absolutely impossible for a machine to perform. We cannot make AI models do everything a human can do yet, but we can no longer rule anything out.</p><p>This realization should end some of the bad ideas we have inherited from the past. We know today that animals have languages much like our own and that computers can learn language with a shocking facility. Noam Chomsky may not be dead yet, but ChatGPT has put the final nails in the coffin of his life&#x2019;s work.</p><p>We have known since Galileo that humanity does not live in the center of the universe. We have known for roughly two centuries that the universe existed for billions of years before people. We have known for some time that humans are not made special by the use of reason and language, as Descartes thought, as we find both among animals and note their frequent absence in humans.</p><p>AI research is removing the idea that humans are special because of some trick we can do, but that does not remove the specialness of humanity. Consider that we celebrate people who can run a mile in under four minutes. Yet, my 20-year-old cheap French-made car can do a mile in about 30 seconds if I really put my foot on the pedal.</p><p>We care about human accomplishment in ways that differ from how we care about pure performance benchmarks. We&#x2019;ll still prefer our human performers over AI, for the same reason I don&#x2019;t win Olympic medals with my old Citro&#xEB;n.</p><p>We imagine AI taking away the drudgery of human life, but we do not yet have robots that can restock shelves at Walmart. On the other hand, we now have AI models that can sing like <a href="https://en.wikipedia.org/wiki/%C3%89dith_Piaf?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Edith Piaf</a> and, if someone really tried, could probably dance like <a href="https://en.wikipedia.org/wiki/Shakira?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Shakira </a>or play the guitar like <a href="https://en.wikipedia.org/wiki/Jimi_Hendrix?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jimi Hendrix</a>.</p><p>This is progress. It&#x2019;s an accomplishment of significant value. But it&#x2019;s not the AI we wanted. Unless we rethink the whole program of AI research, we&#x2019;ll need to move the goalposts again.</p><h2 id="real-ai-instead-of-%E2%80%9Creal%E2%80%9D-ai">Real AI instead of &#x201C;Real&#x201D; AI</h2><p>Jina AI is a business. We don&#x2019;t sell fantasy, we sell AI models. There&#x2019;s no place for &#x201C;artificial general intelligence&#x201D; in what we do. We may love science fiction, but we deal in science fact.</p><p>&#x201C;Past performance is not a guarantee of future returns.&#x201D; Maybe it&#x2019;ll be different this time. Maybe we won&#x2019;t dismiss our successes as not &#x201C;real&#x201D; intelligence. Maybe we&#x2019;ll get over our science fiction dreams for once. Maybe this hype cycle won&#x2019;t lead to another AI Winter.</p><p>Maybe, but I won&#x2019;t bet on it.</p><p>Someday, we might have something that looks more like our visions of &#x201C;real&#x201D; AI. Perhaps in 50 years? 100? History does not encourage optimism about it. But regardless, it won&#x2019;t happen in a timeframe that pays any bills.</p><p>Moving around the goalposts of AI won&#x2019;t change that.</p><p>Our interest is business cases. We want AI to add to your bottom line. At Jina AI, we want to hear about your business processes and discuss how really existing AI models can fit into them. We know the limits of this technology intimately, but we also know its genuine potential.</p><p>Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to talk about what <em>real </em>AI can do for your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3602 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina AI’s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment]]></title><description><![CDATA[Jina AI launches 8K token length embedding v2 models on AWS marketplace, elevating enterprise AI deployments with EU-engineered innovation and a commitment to data sovereignty.]]></description><link>https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/</link><guid isPermaLink="false">65577ae1bb728c000101be6c</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 20 Nov 2023 14:32:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><p><strong>Berlin, Germany - November 20, 2023 </strong>- Catering to enterprise customers, Jina AI has released <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings v2</a> on AWS SageMaker, a milestone in accessible, top-tier AI solutions. Enterprise users can now search for <code>jina-embeddings-v2-base/small</code> on the AWS Marketplace and deploy them directly to their own AWS accounts. As a part of the AWS Startups program, this release underscores the collaboration between Jina AI&apos;s innovation and AWS&apos;s commitment to supporting groundbreaking startups, marking a significant advancement in AI development.</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png" class="kg-image" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment" loading="lazy" width="2000" height="1556" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-60.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-60.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-60.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png 2134w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Enterprise users can now search jina-embeddings-v2-base/small on the AWS marketplace and deploy them directly on their own AWS account.</span></figcaption></figure><h2 id="superior-models-on-a-robust-platform">Superior Models on a Robust Platform</h2><ul><li><em>SageMaker Integration</em>: With global availability on the AWS SageMaker Marketplace, Jina AI underscores its dedication to enterprise users, providing them with an effortless way to build applications using our advanced embedding models.</li><li><em>Seamless Deployment</em>: Enterprises can now easily deploy Jina Embedding v2 models as SageMaker endpoints, bypassing the complexity associated with custom infrastructure setups.</li><li><em>Cost-Effective Licensing</em>: The English <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">base</a> and <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">small</a> models are available <em>without licensing fees</em>. Clients incur costs only for their AWS instances, ensuring a privacy-first, cost-effective solution within their VPC.</li></ul><h3 id="tailored-solutions-for-varied-use-cases">Tailored Solutions for Varied Use Cases</h3><ul><li><em>Model Diversity</em>: With a <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">0.27GB base model</a> and a <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">0.07GB small model</a>, Jina AI provides tailored solutions for various needs, from in-depth analytics to lightweight applications.</li><li><em>Use Cases</em>: The base model is designed for comprehensive semantic representation, ideal for enterprise search and content discovery, while the small model caters to mobile and edge devices, optimizing for speed and efficiency.</li></ul><p>Commenting on this significant milestone, Dr. Han Xiao, CEO of Jina AI, offered the following insights:</p><blockquote>Launching Jina AI&apos;s 8K Context Length v2 Embedding Models on AWS Marketplace, we advance industry standards for private AI solutions. Developed in Germany, this pivotal release emphasizes data sovereignty and customer-centric innovation, addressing today&apos;s needs and shaping future secure, private AI deployments.</blockquote><p>Jina AI aims to make continuous strides towards privacy-aware, state-of-the-art AI, as evident from its plans.  </p><h2 id="why-jina-embeddings-v2-a-leap-in-ai-capability">Why Jina Embeddings v2: A Leap in AI Capability</h2><ul><li><em>Extended Context Length</em>: Jina Embeddings v2 models support an <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">unprecedented 8K</a> (8192 tokens) context length, allowing for a full understanding of longer documents.</li><li><em>Open Source Pioneer</em>: Jina AI takes pride in offering the only open-source model with a context length that matches OpenAI&#x2019;s proprietary models, broadening access to advanced AI.</li><li><em>Benchmark Leadership</em>: On the Massive Text Embedding Benchmark (MTEB) <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">leaderboard</a>, our models boast performance on par with industry-leading models, attesting to our commitment to excellence.</li></ul><h3 id="performance-vs-openais-text-embedding-ada002">Performance vs. OpenAI&apos;s text-embedding-ada002</h3><p>Below is a comparative performance snapshot that showcases the robust capabilities of Jina Embeddings v2:</p>
<!--kg-card-begin: html-->
<table id="e6f57e0d-fe4c-414f-8c0f-5081d4ad007d" class="simple-table"><tbody><tr id="29fae3d0-ca66-462f-8966-b3c1b74c9a86"><td id="bT&lt;r" class>Model</td><td id="hcDn" class>text-embedding-ada-002</td><td id="gMgn" class>jina-embeddings-v2-base-en</td></tr><tr id="5cff5519-ba19-40a8-9900-4488689c0a84"><td id="bT&lt;r" class>Rank</td><td id="hcDn" class><strong>18</strong></td><td id="gMgn" class>21</td></tr><tr id="f01a5da8-92d0-4a76-8e1c-8fb77c9ac6c8"><td id="bT&lt;r" class>Model Size (GB)</td><td id="hcDn" class>Unknown</td><td id="gMgn" class>0.27</td></tr><tr id="56cc4ec4-a35e-44f4-8f2a-5243a8c4331b"><td id="bT&lt;r" class>Average (56 datasets)</td><td id="hcDn" class><strong>60.99</strong></td><td id="gMgn" class>60.38</td></tr><tr id="ca2d0ce9-9157-46c5-a9c7-9179edea43ee"><td id="bT&lt;r" class>Embedding Dimensions</td><td id="hcDn" class>1536</td><td id="gMgn" class><strong>768</strong></td></tr><tr id="8c7ecb47-1d30-4c32-8a23-89dadfbbfc38"><td id="bT&lt;r" class>Sequence Length</td><td id="hcDn" class>8191</td><td id="gMgn" class>8192</td></tr><tr id="a0398e1a-3144-4ec4-bb36-8768488b3d8a"><td id="bT&lt;r" class>Classification Average (12 datasets)</td><td id="hcDn" class>70.93</td><td id="gMgn" class><strong>73.45</strong></td></tr><tr id="d5a7ccfb-7769-4d58-bad4-a219481876ff"><td id="bT&lt;r" class>Pair Classification Average (3 datasets)</td><td id="hcDn" class>84.89</td><td id="gMgn" class><strong>85.38</strong></td></tr><tr id="461eb054-e39a-4ec5-8c33-cc880cb9f2ac"><td id="bT&lt;r" class>Summarization Average (1 dataset)</td><td id="hcDn" class>30.8</td><td id="gMgn" class><strong>31.6</strong></td></tr><tr id="5f55e5e5-a6dc-4a4a-b67d-4ffed0084989"><td id="bT&lt;r" class>Retrieval Average (15 datasets)</td><td id="hcDn" class><strong>49.25</strong></td><td id="gMgn" class>47.87</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Jina AI&apos;s base model excels particularly in Classification and Pair Classification tasks, underscoring its value in diverse applications ranging from document analysis to recommendation systems.</p><h2 id="get-started-with-jina-embeddings-v2-on-aws">Get Started with Jina Embeddings v2 on AWS</h2><p>To begin using Jina Embeddings v2, visit the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS Marketplace listings</a> and select the model that best fits your needs.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a></figure><p>These sample notebooks can help users get started with Jina Embeddings v2 models:</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Real-time%20inference.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Real-time inference.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Real-time inference on SageMaker</span></p></figcaption></figure><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Batch%20transform.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Batch transform.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Embedding multiple sentences in a batch with SageMaker</span></p></figcaption></figure><h2 id="coming-soon-multilingual-embeddings-and-more">Coming Soon: Multilingual Embeddings and More</h2><p>Looking ahead, Jina AI is already deep in developing multilingual embedding models, making them available to its enterprise clients for private deployment on various cloud service providers (CSPs). With the imminent launch of these models, Jina AI is set to bridge language barriers, unlocking global opportunities for its clients.</p><h2 id="about-jina-ai-gmbh">About Jina AI GmbH</h2><p>Located at <a href="https://maps.app.goo.gl/rAZ1QiqoKp49KZAg6?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany</a>, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at&#xA0;<a>contact@jina.ai</a>.<br></p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.0 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-0-update/</link><guid isPermaLink="false">655b2af0bb728c000101bee8</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 20 Nov 2023 10:20:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3230">Release Note (<code>3.23.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Image-Jina-dark.jpg" alt="Jina 3.23.0 Update"><p>This release contains 1 new feature and 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.0 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.0) Release time: 2023-11-20 10:02:37 This release contains 1 new feature and 1 bug fix.
&#x1F195; Features
Call extend rest interface in worker (#6108)
When running a Deployment with H&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/1e01faf8fa1f5ed07a44184f9998e6cc26cea7da380da0ed5c663d6859928bca/jina-ai/jina/releases/tag/v3.23.0" alt="Jina 3.23.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="call-extend-rest-interface-in-worker-6108">Call extend rest interface in worker (<a href="https://github.com/jina-ai/jina/pull/6108?ref=jina-ai-gmbh.ghost.io">#6108</a>)</h3><p>When running a&#xA0;<code>Deployment</code>&#xA0;with HTTP locally without&#xA0;<code>docker</code>&#xA0;you can now extend the API by using&#xA0;<code>extend_rest_interface</code>, the same as with&#xA0;<code>Flow</code>.</p><pre><code class="language-Python">    import jina
    from jina import Deployment
    def extend_rest_function(app):
        @app.get(&apos;/hello&apos;, tags=[&apos;My Extended APIs&apos;])
        async def foo():
            return {&apos;msg&apos;: &apos;hello world&apos;}

        return app

    jina.helper.extend_rest_interface = extend_rest_function
    d = Deployment(protocol=&apos;http&apos;, port=8080)

    with d:
        response = requests.get(f&apos;http://localhost:8080/hello&apos;)
        assert response.status_code == 200
        assert response.json() == {&apos;msg&apos;: &apos;hello world&apos;}</code></pre><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="read-csv-with-escaped-chars-6102">Read CSV with escaped chars (<a href="https://github.com/jina-ai/jina/pull/6102?ref=jina-ai-gmbh.ghost.io">#6102</a>)</h3><p>There was an issue when using Jina to do batch transformations on SageMaker. Parsing texts with escape characters was not working properly.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals Martinez (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>Deepankar Mahapatro (<a href="https://github.com/deepankarm?ref=jina-ai-gmbh.ghost.io">@deepankarm</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[SceneXplain's OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!]]></title><description><![CDATA[SceneXplain beats GPT-4V by going beyond English, offering multilingual OCR for Chinese, Japanese, Korean, Arabic, Hindi and more]]></description><link>https://jina.ai/news/scenexplains-ocr-beats-gpt-4v-hands-down-chinese-japanese-korean-arabic-hindi-and-more/</link><guid isPermaLink="false">65537bd2c2c1f30001ea4816</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 16 Nov 2023 15:00:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/sensex--3-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/sensex--3-.jpg" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><p>There&apos;s been a lot of noise recently about the release of GPT-4V - a model that can analyze the content of images and summarize that into human language. Sound familiar? It may, because SceneXplain has been doing just that for months already!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><p>But just because you take an early lead doesn&apos;t mean you win the race. We&apos;ve got to show how we&apos;re better, right? Well, we&apos;ve already done that in <a href="https://jina.ai/news/scenexplain-vs-gpt-4-vision-the-best-image-captioning-tool-in-2023/?ref=jina-ai-gmbh.ghost.io">this post</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplain-vs-gpt-4-vision-the-best-image-captioning-tool-in-2023/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023?</div><div class="kg-bookmark-description">Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/09/Explore-image-storytelling-beyond-pixels--9-.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><p>But in this post, we&apos;ll go one further and show you how we excel at OCR for multilingual texts. In short, <strong>SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V</strong>. Also, it doesn&#x2019;t have any pesky daily request limits.</p><p>You might think GPT-4V is pretty decent if you&#x2019;re a user of Latin-character languages. Since you&#x2019;re reading this (I assume in English), that means you!</p><p>Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies:</p><ul><li><a href="https://blog.roboflow.com/gpt-4-vision/?ref=jina-ai-gmbh.ghost.io">First Impressions with GPT-4V(ision)</a></li><li><a href="https://arxiv.org/abs/2310.16809?ref=jina-ai-gmbh.ghost.io">Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</a></li></ul><p>But, as mentioned in the study:</p><blockquote class="kg-blockquote-alt">There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V.</blockquote><p>Bad news - it&apos;s not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts.</p><p>That&apos;s where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.)</p><h2 id="tldr-how-does-gpt-4v-fail-compared-to-scenexplain">TL;DR: How does GPT-4V fail compared to SceneXplain?</h2><p>In short:</p><ul><li><strong>API limits:</strong> 100 requests a day is the new &#x201C;<a href="https://quoteinvestigator.com/2011/09/08/640k-enough/?ref=jina-ai-gmbh.ghost.io">640K ought to be enough for anybody</a>&#x201D;. SceneXplain has a <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing">range of plans</a> to suit your needs.</li><li><strong>Reliability:</strong> Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it can&#x2019;t do it. SceneXplain gets it done right every time.</li><li><strong>Accuracy:</strong> GPT-4V will often extract strings that are <strong>plausible</strong> for a given image, but not the <strong>actual</strong> strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. <a href="https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead/?utm_source=PromptPerfect+Newsletter&amp;utm_campaign=ce1beac0b3-EMAIL_CAMPAIGN_2023_09_29_10_25_COPY_02&amp;utm_medium=email&amp;utm_term=0_fada173e11-6efedfdbc3-%5BLIST_EMAIL_ID%5D">SceneXplain pretty much cured hallucinations in its Jelly algorithm</a>.</li><li><strong>Consistency:</strong> GPT-4V doesn&#x2019;t just hallucinate. It hallucinates different things each time. SceneXplain is generally consistent.</li><li><strong>Completeness:</strong> GPT-4V doesn&#x2019;t always extract <strong><em>all</em></strong> the strings in the image. SceneXplain is more comprehensive.</li><li><strong>Cost:</strong> SceneXplain is up to 50% more affordable.</li></ul><p>Let&#x2019;s dive into each point in more detail.</p><h3 id="api-limits">API limits</h3><p>Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would&apos;ve put this post out earlier if we hadn&apos;t easily overshot that multiple times.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-29.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1435" height="71" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-29.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-29.png 1435w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Hello </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RateLimitError</span></code><span style="white-space: pre-wrap;">, my old friend.</span></figcaption></figure><h3 id="reliability">Reliability</h3><p>If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/qt1pq58r827iyyvugjfr.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1200" height="675" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/qt1pq58r827iyyvugjfr.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/qt1pq58r827iyyvugjfr.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/qt1pq58r827iyyvugjfr.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>We can extract the strings as JSON by using the prompt:</p><pre><code class="language-text">Extract the text strings from this image and populate a JSON that follows this schema:

{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;text_strings&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Every text string contained in the image. Consider all languages&quot;
    }
  }
}

Return just the output JSON. Do not put it in a code block
</code></pre><p>Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like &#x201C;Here is your JSON&#x201D; and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content.</p><p>It returns this output:</p><pre><code class="language-json">{
&quot;text_strings&quot;: [
&quot;YOU CAN&apos;T RUN&quot;,
&quot;FROM US FOREVER&quot;,
&quot;m&quot;
]
}
</code></pre><p>I would&#x2019;ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns:</p><figure class="kg-card kg-code-card"><pre><code class="language-json">{
  &quot;text_strings&quot;: [
    &quot;YOU CAN&apos;T RUN&quot;,
    &quot;FROM US FOREVER&quot;,
    &quot;M&quot;
  ]
}</code></pre><figcaption><p><span style="white-space: pre-wrap;">Ah, those sweet sweet indents</span></p></figcaption></figure><p>Now let&#x2019;s try asking in natural language about a Chinese ad:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/McDonald-EN-CHINE.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1329" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/McDonald-EN-CHINE.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/McDonald-EN-CHINE.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/McDonald-EN-CHINE.jpeg 1329w" sizes="(min-width: 720px) 720px"></figure><p>If we use the prompt: <code>Extract the text strings from this image</code>, GPT-4V gives us:</p><pre><code class="language-text">It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted.

Given this issue, I&apos;m unable to extract the text from the image as requested. If there&apos;s anything else you would like me to assist you with, please let me know!
</code></pre><p>That&#x2019;s&#x2026;not useful. <strong><em>However</em></strong>, asking it to extract it as JSON (by using the above prompt with schema) works:</p><pre><code class="language-json">{
&quot;text_strings&quot;: [
&quot;&#x9EA6;&#x5F53;&#x52B3;&quot;,
&quot;&#x4E00;&#x5468;&#x4E03;&#x5929;&quot;,
&quot;&#x65E9;&#x6668;&#x56DB;&#x70B9;&quot;,
&quot;&#x81F3;&quot;,
&quot;17:00 &#x65F6;&#x6BB5;&quot;,
&quot;&#x5468;&#x672B;&#x65E9;&#x9910; &#x5EF6;&#x65F6;&quot;,
&quot;&#x66F4;&#x591A;&#x60CA;&#x559C;&quot;,
&quot;&#x5C3D;&#x5728;&#x9EA6;&#x5F53;&#x52B3;APP&quot;
]
}
</code></pre><p>Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows.</p><h3 id="accuracy">Accuracy</h3><p>Take another look at that JSON output. Then, take a look at the ad.</p><p>At first glance, the JSON looks decent. It&#x2019;s got the <code>17:00</code> and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that <em>could</em> occur in a Chinese McDonald&apos;s ad, but none of them are actually in that particular ad.</p><p>This can be even more dangerous than being wrong - enough people are already worried about <a href="https://futurism.com/artificial-intelligence-automated-mansplaining-machine?ref=jina-ai-gmbh.ghost.io">GPT mansplaining</a> - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too.</p><p>Also, why does it suddenly think it can extract the text when it stated before that it couldn&#x2019;t?</p><p>For reference, SceneXplain returns:</p><pre><code class="language-json">{
  &quot;text_strings&quot;: [
    &quot;&#x9EA6;&#x9EA6;&#x591C;&#x5E02;&quot;,
    &quot;&#x51FA;&#x795E;&#x5364;&#x5473;&#x9E21;&#x67B6;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x62CC;&#x9E21;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x9178;&#x8FA3;&#x9E21;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x534A;&#x9E21;&quot;,
    &quot;&#x9EA6;&#x9EA6;&#x591C;&#x5E02;&#x5C0F;&#x5821;&quot;,
    &quot;&#x6697;&#x7CD6;&#x5E26;&#x52B2;&#x5821;&quot;,
    &quot;&#x552F;&#x9187;&#x5E26;&#x52B2;&#x5821;&quot;,
    &quot;&#x83B1;&#x83B1;&#x771F;&#x9999;&#x5821;&quot;,
    &quot;17:00&#x8425;&#x591C;&quot;,
    &quot;&#x54C7;&#x85D5;&#x5E26;&#x52B2;&#x5821;&quot;
  ]
}
</code></pre><p>If you compare the strings, you&#x2019;ll see SceneXplain is much more accurate and hallucinates less.</p><h3 id="consistency">Consistency</h3><p>Coming back to our old friend, the Chinese McDonald&#x2019;s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here&apos;s what we get:</p><pre><code>{
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;text_strings&quot;: [
&quot;&#x9EA6;&#x8FA3;&#x9E21;&#x7FC5;&quot;,
&quot;&#x5821;&#x5821;&#x53CC;&#x62FC; &#x7B49;&#x4F60;&#x6765;&#x64A9;&quot;,
&quot;&#x4E00;&#x4EBA;&#x98DF;&#x597D;&#x798F;&#x5229;&quot;,
&quot;17:00&#x524D;&#x6765;&quot;,
&quot;&#x9EA6;&#x5F53;&#x52B3;&quot;,
&quot;&#x5C0A;&#x4EAB;&#x8F6F;&#x6B27;&#x5305;&quot;,
&quot;&#x5468;&#x8FB9;&#x597D;&#x8D27; &#x7B49;&#x4F60;&quot;,
&quot;&#x5C0A;&#x4EAB;&#x8F6F;&#x6B27;&#x5305;&quot;
]
}
}
</code></pre><p>Well dang. Not only is the text entirely different, but it&#x2019;s also sent us JSON in a completely different format, something like the <strong>schema</strong> we sent, rather than the <em>output format</em> that the schema defines.</p><h3 id="completeness">Completeness</h3><p>If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="624" height="808" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg 624w"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x645;&#x639;&#x62A;&#x648;&#x642; MAATOUK&quot;,
&quot;1960&quot;,
&quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
&quot;&#x62D;&#x628;&#x64A;&#x628;&#x629; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;&quot;,
&quot;&#x62A;&#x642;&#x62F;&#x645;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
&quot;&#x623;&#x635;&#x627;&#x644;&#x629; &#x627;&#x645;&#x62A;&#x62F;&#x62A; &#x648;&#x633;&#x637;&#x648;&#x644;&#x629; &#x627;&#x644;&#x646;&#x636;&#x62C;&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x645;&#x639;&#x62A;&#x648;&#x642;&quot;,
    &quot;MAATOUK&quot;,
    &quot;1 9 6 0&quot;,
    &quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
    &quot;&#x62D;&#x633;&#x628; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;&quot;,
    &quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
    &quot;&#x645;&#x639;&#x62A;&#x648;&#x642;&quot;,
    &quot;mAATOUK&quot;,
    &quot;&#x62A;&#x62D;&#x645;&#x64A;&#x635; &#x63A;&#x627;&#x645;&#x642;&quot;,
    &quot;&#x628;&#x646; &#x645;&#x637;&#x62D;&#x648;&#x646; &#x661;&#x660;&#x660;&#x66A; &#x623;&#x631;&#x627;&#x628;&#x64A;&#x643;&#x627;&quot;,
    &quot;&#x623;&#x635;&#x627;&#x644;&#x629; &#x627;&#x644;&#x645;&#x630;&#x627;&#x642; &#x648;&#x633;&#x647;&#x648;&#x644;&#x629; &#x627;&#x644;&#x62A;&#x62D;&#x636;&#x64A;&#x631;&quot;,
    &quot;&#x644;&#x637;&#x627;&#x644;&#x645;&#x627; &#x627;&#x634;&#x62A;&#x647;&#x631; &#x627;&#x644;&#x639;&#x631;&#x628; &#x639;&#x628;&#x631; &#x645;&#x627;&#x636;&#x64A; &#x627;&#x644;&#x632;&#x645;&#x627;&#x646; &#x628;&#x62A;&#x62D;&#x636;&#x64A;&#x631; &#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x641;&#x64A; &#x645;&#x646;&#x627;&#x632;&#x644;&#x647;&#x645;&#x60C; &#x648;&#x627;&#x644;&#x62A;&#x64A; &#x62A;&#x639;&#x62F; &#x645;&#x646; &#x623;&#x62D;&#x62F; &#x631;&#x645;&#x648;&#x632; &#x627;&#x644;&#x643;&#x631;&#x645; &#x648;&#x627;&#x644;&#x636;&#x64A;&#x627;&#x641;&#x629;&quot;,
    &quot;&#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629;. &#x645;&#x646; &#x647;&#x646;&#x627; &#x627;&#x646;&#x637;&#x644;&#x642;&#x62A; &#x645;&#x635;&#x627;&#x646;&#x639; &#x645;&#x639;&#x62A;&#x648;&#x642; &#x644;&#x62A;&#x642;&#x62F;&#x64A;&#x645; &#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x627;&#x644;&#x633;&#x647;&#x644;&#x629; &#x627;&#x644;&#x62A;&#x62D;&#x636;&#x64A;&#x631; &#x630;&#x627;&#x62A; &#x627;&#x644;&#x631;&#x627;&#x626;&#x62D;&#x629; &#x627;&#x644;&#x645;&#x645;&#x64A;&#x632;&#x629; &#x648;&#x627;&#x644;&#x637;&#x639;&#x645; &#x627;&#x644;&#x623;&#x635;&#x64A;&#x644;.&quot;,
    &quot;&#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x645;&#x639;&#x62A;&#x648;&#x642; &#x661;&#x669;&#x666;&#x660;&#x60C; &#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629; &#x62D;&#x633;&#x628; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;.&quot;
  ]
</code></pre><h3 id="cost">Cost</h3><p>At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplain&#x2019;s MAX plan you pay less than half of that. You can find out more on our <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing">pricing page</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><h2 id="head-to-head-gpt-4v-vs-scenexplain">Head to head: GPT-4V vs SceneXplain</h2><p>Let&#x2019;s put both services to the test and see who comes out on top.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">See our <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/scenex-gpt4v?ref=jina-ai-gmbh.ghost.io">notebook</a> for the full range of testing and results.</div></div><h3 id="testing-methodology">Testing methodology</h3><p>We took a selection of images, some from <a href="http://pexels.com/?ref=jina-ai-gmbh.ghost.io">pexels.com</a> and some from searching Google Images for advertisements in the given languages:</p><ul><li>Chinese (we didn&apos;t differentiate between Simplified and Traditional)</li><li>Japanese</li><li>Korean</li><li>Arabic</li><li>Hindi</li></ul><p>We then used:</p><ul><li><a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a> (using the <a href="https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead?ref=jina-ai-gmbh.ghost.io">Jelly algorithm</a>)</li><li>A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit)</li></ul><p>We ran several rounds of tests:</p><ul><li><strong>Basic image description:</strong> For SceneXplain, we just uploaded the image. For GPT-4V, we asked: <code>What is in the image?</code></li><li><strong>Visual question answering:</strong> We uploaded the image to both services and asked: <code>What does the text say in this image?</code></li><li><strong>JSON output:</strong> For SceneXplain, we simply used the <a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision?ref=jina-ai-gmbh.ghost.io">&quot;Extract JSON from image&quot;</a> feature and a predefined <a href="https://json-schema.org/?ref=jina-ai-gmbh.ghost.io">JSON Schema</a>.</li></ul><p>The JSON Schema was as follows:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;text_strings&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Every text string contained in the image. Consider all languages&quot;
    }
  }
}</code></pre><p>Since GPT-4V doesn&apos;t directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text:</p><pre><code class="language-text">Extract the text strings from this image and populate a JSON that follows this schema:

&lt;JSON Schema from above&gt;

Return just the output JSON. Do not put it in a code block&quot;</code></pre><p>The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">To save space, we&#x2019;ve simplified the JSON output to include just the text strings, not the rest of the JSON structure.</div></div><h3 id="chinese">Chinese</h3><p>We&#x2019;ve already dived into the McDonald&#x2019;s ad above, so we&#x2019;ll just look at one more Chinese image:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-2670327.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="2000" height="3000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/pexels-photo-2670327.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/pexels-photo-2670327.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/pexels-photo-2670327.jpeg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/pexels-photo-2670327.jpeg 2400w" sizes="(min-width: 720px) 720px"></figure><ul><li>SceneXplain output:</li></ul><pre><code class="language-json">[
  &quot;&#x91D1;&#x94B1;&#x809A;20&#x5143;&quot;,
  &quot;&#x65FA;&#x89D2;&#x725B;&#x7B4B;&#x8169;20&#x5143;&quot;,
  &quot;&#x65FA;&#x89D2;&#x725B;&#x6742;18&#x5143;&quot;
]</code></pre><ul><li>GPT-4V output:</li></ul><pre><code class="language-json">[
  &quot;&#x7AF9;&#x7B74;&#x9B5A;&quot;,
  &quot;20&#x5143;&quot;, 
  &quot;&#x751C;&#x751C;&#x5708;&quot;,
  &quot;20&#x5143;&quot;,
  &quot;&#x751C;&#x751C;&#x4ED9;&#x8C9D;&quot;,
  &quot;18&#x5143;&quot;
]</code></pre><p>Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices.</p><h3 id="japanese">Japanese</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/21d10241ed74414d8263d56330cd5d02.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="400" height="300"></figure><ul><li>GPT-4V gets some of the characters correct in the cigarette ad. The string <code>&#x79C1;&#x306F;&#x305F;&#x3060;&#x5438;&#x3044;&#x6BBB;&#x306B;&#x306A;&#x308A;&#x307E;&#x3059;</code> has most of the same characters but is still a bit off. Other strings are just hallucinated.</li></ul><pre><code class="language-json">[
&quot;&#x79C1;&#x306F;&#x305F;&#x3060;&#x5438;&#x3044;&#x6BBB;&#x306B;&#x306A;&#x308A;&#x307E;&#x3059;&quot;,
&quot;PLAISIR&quot;,
&quot;&#x7D14;&#x6B63;&quot;,
&quot;MICRONITE&quot;,
&quot;&#x65B0;&#x5F0F;&quot;,
&quot;&#x3059;&#x3079;&#x3066;&#x306E;&#x5473;&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x79C1;&#x306F;&#x305F;&#x3070;&#x3053;&#x3092;&#x5438;&#x3044;&#x307E;&#x3059;&quot;,
    &quot;PLAISIR&quot;,
    &quot;&#x72EC;&#x5360;&quot;,
    &quot;MICRONITE&quot;,
    &quot;&#x65B0;&#x305F;&quot;,
    &quot;&#x30B7;&#x30AC;&#x30EC;&#x30C3;&#x30C8;&quot;,
    &quot;&#x559C;&#x3073;&#x306E;&#x5473;&quot;,
    &quot;KING SIZE&quot;
  ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/poster.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="897" height="1390" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/poster.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/poster.png 897w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V: After extracting (wrong) text from Chinese and Japanese images, we uploaded another image with the prompt <code>Now do this one</code> (which we&#x2019;d used successfully to repeat prior commands). GPT-4V seems to have forgotten how. We got the output:</li></ul><pre><code class="language-text">I&apos;m sorry, but I can&apos;t assist with identifying or making statements about text within images. If you have any other questions or need assistance with a different task, feel free to ask!
</code></pre><ul><li>SceneXplain did its duty as expected:</li></ul><pre><code class="language-json">[
    &quot;&#x7528;&#x670D;&#x3050;&#x76F4;&#x305A;&#x305B;&#x8E87;&#x9DF9;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3;&quot;,
    &quot;&#x8A55;&#x5224;&#x306E;&#x6551;&#x6025;&#x85AC;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#x30EB;&#x30F3;&#x30C0;&quot;,
    &quot;&#x80BA;&#x708E;&#x306B;&quot;,
    &quot;&#x65E5;&#x82F1;&#x7C73; &#x88FD;&#x6CD5;&#x7279;&#x8A31;)&quot;,
    &quot;ANTI-LUMGEN&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#x30EB;&#x30D0;&#x30B2;&#x30CE;&quot;,
    &quot;&#x6771;&#x4EAC;&#x9A8C;&#x85AC;&#x682A;&#x5F0F;&#x4F1A;&#x793E;&quot;,
    &quot;&#x5448;&#x8D08;&#x737B;&#x6587;&quot;,
    &quot;(&#x91D1;&#x79D1;&#x767E;&#x8CA8;&#x5E97;&#x53CA;&#x5404;&#x9EB5;&#x5E97; &#x6709;&#x4E09;)&quot;,
    &quot;&#x3042;&#x308A;&#x304B;&#x305B;&#x3093;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x3055;&#x304C;&#x5E74;&#x4EE5;&#x4E0A;&#x4FDD;&#x5B58;&#x3057;&#x3066;&#x3082;&#x52B9;&#x76EE;&#x306B;&#x8B8A;&#x5316;&#x306F;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x4F55;&#x7B49;&#x671F;&#x4F5C;&#x54C1;&#x306E;&#x306A;&#x3044;&#x5B89;&#x5168;&#x306A;&#x5185;&#x670D;&#x85AC;&#x3067;&#x306D;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x4ED6;&#x306E;&#x91AB;&#x85E5;&#x3068;&#x4F75;&#x7528;&#x3057;&#x3066;&#x5DEE;&#x652F;&#x3078;&#x3042;&#x308A;&#x307E;&#x305B;&#x3093;&quot;,
    &quot;&#x306E;&#x5FC5;&#x8981;&#x306F;&#x3042;&#x308A;&#x307E;&#x305B;&#x3093;&quot;,
    &quot;&#x672C;&#x5291;&#x306F;&#x300C;&#x53EA;&#x4E00;&#x56DE;&#x5206;&#x300D;&#x306E;&#x307F;&#x3066;&#x7D50;&#x69CB;&#x3067;&#x3059;&#x6218;&#x7248;&quot;,
    &quot;&#x3089;&#x5C02;&#x9580;&#x5BB6;&#x306B;&#x671F;&#x76F8;&#x8AC7;&#x4E0B;&#x3055;&#x3044;&quot;,
    &quot;&#x8C61;&#x306E;&#x8D85;&#x3089;&#x306A;&#x3044;&#x6642;&#x306F;&#x4E7E;&#x708E;&#x3067;&#x306A;&#x3044;&#x3068;&#x601D;&#x3072;&#x307E;&#x3059;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3; &#x3092;&#x670D;&#x7528;&#x3057;&#x307E;&#x3059;&#x3068;&#x5FC5;&#x305A;&#x53F3;&#x306E;&quot;,
    &quot;&#x672C;&#x5291; &#x306E; &#x7279; &#x9577;&quot;,
    &quot;&#x308B;&#x306E;&#x3067;&#x3042;&#x308A;&#x307E;&#x3057;&#x3066;&#x5B89;&#x7720;&#x3088;&#x308A;&#x89BA;&#x3081;&#x308B;&#x6642;&#x306B;&#x306F;&#x5FB3;&quot;,
    &quot;&#x6B64;&#x306E;&#x7279;&#x6B8A;&#x73FE;&#x8C61; &#x767C;&#x6C57;&#x5B89;&#x7720; &#x3053;&#x305D;&#x85AC;&#x52B9;&#x3067;&#x64B2;&#x6EC5;&quot;,
    &quot;&#x3068;&#x3082;&#x5FEB;&#x4E0A;&#x3052;&#x306B;&#x9577;&#x6642;&#x9593;&#x5B89;&#x7720;&#x81F4;&#x3057;&#x307E;&#x3059;&quot;,
    &quot;&#x767C;&#x6C57;&#x3057;&#x5176;&#x767C;&#x6C57;&#x304C;&#x76F8;&#x4E4B;&#x6559;&#x751F;&#x308B;&#x9803;&#x3088;&#x308A;&#x5FE0;&#x8005;&#x306F;&#x3044;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3; &#x3092;&#x670D;&#x7528;&#x3059;&#x308B;&#x3068;&#x767D;&#x8840;&#x7403;&#x306E;&#x306F;&quot;,
    &quot;&#x6CBB; &#x7642; &#x306E; &#x9435; &#x5247;&quot;,
    &quot;&#x5982;&#x4E16;&#x69AE;79&#x767C;&#x97F3;&#x5143;&#x4E2D;&#x5C71;&quot;,
    &quot;&#x50F9; &#x5C0F;&#x4EBA;&#x76F8; &#x91D1; &#x570B;&quot;,
    &quot;&#x5927;&#x4EBA;&#x7FD4; &#x4E94;&quot;,
    &quot;&#x90E8;&#x5B78;&#x5316;&#x6240;&#x92FC;&#x88FD;&#x5C71;&#x4E2D; &#x793E;&#x6703;&#x5F0F;&#x683C; &#x5143;&#x8CE3;&#x767C;&quot;,
    &quot;&#x793E;&#x6703;&#x8005;&#x5408;&#x6728;&#x5927; &#xB7;&#x5E97;&#x5546;&#x7F6E;&#x7F50; &#x610F; &#x675F;)&quot;,
    &quot;&#x793E;&#x6703;&#x540D;&#x4EE4;&#x90E8;&#x592A;&#x9577;&#x5DDD;&#x9F8D; &#x570B;&#x53E4;&#x540D;(&quot;,
    &quot;&#x6703;&#x9762;&#x30A4;&#x30E9;&#x30D6;&#x30B5;&#x30EB;&#x30BF;&#x30D3;&#x30B9;&#x30DB; &#x8AAA; &#x5927;&#x30CE;&quot;,
    &quot;&#x5E97;&#x7406;&#x4EE3;&quot;,
    &quot;&#x793E;&#x6703;&#x5F0F;&#x682A;&#x85E5;&#x88FD;&#x4EAC;&#x6771; &#x5143;&#x9020;&#x88FD;&quot;,
    &quot;Image ID: W5DPKC&quot;,
    &quot;www.alamy.com&quot;
  ]
</code></pre><h3 id="korean">Korean</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-6314649.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1125" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/pexels-photo-6314649.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/pexels-photo-6314649.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-6314649.jpeg 1125w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V remembers how to extract text again. And it remembers how to get it wrong too! If you look at the output, some of the characters are the same as the image, but some are not.</li></ul><pre><code class="language-json">[
&quot;&#xC220;&#xC740; &#xB0A8;&#xC5D0; &#xB9D0;&#xB098;&quot;,
&quot;&#xC220;&#xC740; &#xB098;&#xB85C; &#xB9D0;&#xB098;&quot;
]
</code></pre><ul><li>SceneXplain gives accurate output, though I&#x2019;m not sure where the <code>50</code> is coming from:</li></ul><pre><code class="language-json">[
    &quot;|&#xC57C;&quot;,
    &quot;&#xC88B;&#xC740; &#xB0A0;&#xC5D0; &#xB9CC;&#xB098;&quot;,
    &quot;&#xB2E8;&#xC774; &#xBC29;&quot;,
    &quot;50&quot;,
    &quot;&#xC88B;&#xC740; &#xB098;&#xB97C; &#xB9CC;&#xB098;&quot;
 ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/230531_--_----------_--_--2.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="800" height="1028" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/230531_--_----------_--_--2.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/230531_--_----------_--_--2.jpg 800w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V actually does okay with the more complex image. It doesn&#x2019;t catch <strong><em>all</em></strong> the strings, but it&#x2019;s less wrong than usual from a quick comparison (note: I don&#x2019;t read Korean, so I&#x2019;m just eyeballing it):</li></ul><pre><code class="language-json">[
&quot;&#xC790;&#xC5F0;&#xACFC; &#xAD6D;&#xBBFC;&#xC744; &#xC5F0;&#xACB0;&#xD558;&#xB294;&quot;,
&quot;&#xC218;&#xBAA9;&#xB7;&#xC815;&#xC6D0; &#xD50C;&#xB7AB;&#xD3FC; &#xAE30;&#xAD00;&quot;,
&quot;&#xC218;&#xBAA9;&#xC6D0;&#xC758; &#xACE0;&#xAC1D;&#xAC00;&#xCE58;&#xB97C; &#xB118;&#xC5B4;&quot;,
&quot;&#xAD6D;&#xBBFC; &#xC0DD;&#xD65C; &#xC18D; &#xB179;&#xC0C9;&#xACF5;&#xAC04; &#xD655;&#xC0B0;&#xACFC;&quot;,
&quot;&#xC815;&#xC6D0;&#xC0B0;&#xC5C5; &#xC0DD;&#xD0DC;&#xACC4;&#xB97C; &#xD65C;&#xC131;&#xD654;&#xD558;&#xB294; &#xC5ED;&#xD560;&#xC744;&quot;,
&quot;&#xC801;&#xADF9;&#xC801;&#xC73C;&#xB85C; &#xC218;&#xD589;&#xD558;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;&quot;
]
</code></pre><ul><li>SceneXplain extracts a lot more:</li></ul><pre><code class="language-json">[
    &quot;&#xD55C;&#xAD6D;&#xC218;&#xBAA9;&#xC6D0;&#xC815;&#xC6D0;&#xAD00;&#xB9AC;&#xC6D0;&quot;,
    &quot;&#xC790;&#xC5F0;&#xACFC; &#xAD6D;&#xBBFC;&#xC744; &#xC5F0;&#xACB0;&#xD558;&#xB294;&quot;,
    &quot;&#xC218;&#xBAA9;&#xC6D0;&#xB7;&#xC815;&#xC6D0; &#xD50C;&#xB7AB;&#xD3FC; &#xAE30;&#xAD00;&quot;,
    &quot;&#xC218;&#xBAA9;&#xC6D0;&#xC758; &#xACF5;&#xAC04;&#xC801; &#xAC1C;&#xB150;&#xC744; &#xB118;&#xC5B4;&quot;,
    &quot;&#xAD6D;&#xBBFC; &#xC0DD;&#xD65C; &#xC18D; &#xB179;&#xC0C9;&#xBB38;&#xD654; &#xD655;&#xC0B0;&#xACFC;&quot;,
    &quot;&#xC815;&#xC6D0;&#xC0B0;&#xC5C5; &#xC0DD;&#xD0DC;&#xACC4;&#xB97C; &#xD65C;&#xC131;&#xD654;&#xD558;&#xB294; &#xC5ED;&#xD560;&#xC744;&quot;,
    &quot;&#xC801;&#xADF9;&#xC801;&#xC73C;&#xB85C; &#xC218;&#xD589;&#xD558;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;&quot;,
    &quot;&#xC0B0;&#xB9BC;&#xC0DD;&#xBB3C;&#xC790;&#xC6D0;&quot;,
    &quot;&#xACE0;&#xD488;&#xC9C8; &#xC218;&#xBAA9;&#xC6D0;&quot;,
    &quot;ES&quot;,
    &quot;&#xBCF4;&#xC804; &#xB7; &#xBCF5;&#xC6D0;&quot;,
    &quot;&#xC11C;&#xBE44;&#xC2A4;&quot;,
    &quot;&#xC5ED;&#xB7C9; &#xAC15;&#xD654;&quot;,
    &quot;&#xC0B0;&#xB9BC;&#xC0DD;&#xBB3C;&#xC790;&#xC6D0;&quot;,
    &quot;&#xC5F0;&#xAD6C; &#xB7;&#xD65C;&#xC6A9;&quot;,
    &quot;&#xC815;&#xC6D0;&#xBB38;&#xD654;&quot;,
    &quot;&#xD65C;&#xC131;&#xD654;&quot;
  ]
</code></pre><h3 id="arabic">Arabic</h3><p>Since we already covered the Arabic coffee image above, we&apos;ll just leave one Arabic here:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/artworks-000043896167-z3x71u-t500x500.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="500" height="500"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x627;&#x644;&#x62D;&#x64A;&#x627;&#x629; &#x637;&#x631;&#x64A;&#x642; &#x623;&#x643;&#x62B;&#x631; &#x633;&#x631;&#x639;&#x629;&quot;
]
</code></pre><ul><li>SceneXplain gets a little confused about the 4G logo, seeing it as <code>EAG</code> but is otherwise sound:</li></ul><pre><code class="language-json">[
    &quot;&#x627;&#x644;&#x62D;&#x64A;&#x627;&#x629; &#x635;&#x627;&#x631;&#x62A; &#x623;&#x643;&#x62B;&#x631; &#x633;&#x631;&#x639;&#x629;&quot;,
    &quot;EAG&quot;,
    &quot;LTE&quot;
  ]
</code></pre><h3 id="hindi">Hindi</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="955" height="1575" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg 955w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x914;&#x930; &#x906;&#x902;&#x935;&#x932;&#x93E;&quot;,
&quot;&#x915;&#x93E; &#x92A;&#x94B;&#x937;&#x923; &#x92C;&#x93F;&#x928;&#x93E; &#x91A;&#x93F;&#x92A;&#x91A;&#x93F;&#x92A;&#x93E;&#x939;&#x91F;&quot;,
&quot;&#x938;&#x93F;&#x930;&#x94D;&#x92B; &#x930;&#x941;&quot;,
&quot;9 &#x92E;&#x947;&#x902;&quot;,
&quot;40ml&quot;,
&quot;&#x921;&#x93E;&#x92C;&#x930;&quot;,
&quot;&#x91F;&#x94B;&#x932; &#x92B;&#x94D;&#x930;&#x940; 1800-103-1644&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;&#x915;&#x947;&#x936; &#x924;&#x947;&#x932;&quot;,
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x914;&#x930; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;&#x915;&#x93E; &#x92A;&#x94B;&#x937;&#x923; &#x92C;&#x93F;&#x928;&#x93E;&quot;,
    &quot;&#x91A;&#x93F;&#x92A;&#x91A;&#x93F;&#x92A;&#x93E;&#x939;&#x91F;&quot;,
    &quot;&#x92E;&#x947;&#x902;&quot;,
    &quot;&#x928;&#x92F;&#x93E;&quot;,
    &quot;&#x92A;&#x948;&#x915;&quot;,
    &quot;&#x20B9;9/-&quot;,
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;40ml&quot;,
    &quot;DABUR CARES: CALL OR WRITE&quot;,
    &quot;+ TOLL FREE 1800-103-1644&quot;
  ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="973" height="1576" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg 973w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V gave us the error <code>Something went wrong. If this issue persists please contact us through our help center at help.openai.com</code>. After retrying, it gave us:</li></ul><pre><code class="language-json">[
&quot;MDH &#x915;&#x940; &#x90F;&#x915;&quot;,
&quot;&#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&quot;,
&quot;&#x938;&#x94D;&#x935;&#x93E;&#x926; &#x915;&#x947; &#x938;&#x93E;&#x925; - &#x938;&#x93E;&#x925; &#x916;&#x941;&#x936;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x940; &#x92C;&#x94C;&#x91B;&#x93E;&#x930; !!&quot;,
&quot;&#x92E;&#x938;&#x93E;&#x932;&#x93E; &#x90F;&#x915; &#x938;&#x94D;&#x935;&#x93E;&#x926;&#x93F;&#x937;&#x94D;&#x91F; &#x914;&#x930; &#x91C;&#x940;&#x935;&#x902;&#x924; &#x909;&#x92A;&#x939;&#x93E;&#x930; !!&quot;,
&quot;MDH&quot;,
&quot;&#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
&quot;&#x905;&#x938;&#x932;&#x940; &#x92E;&#x938;&#x93E;&#x932;&#x947; &#x938;&#x91A;-&#x938;&#x91A;&quot;,
&quot;MDH Ltd.&quot;,
&quot;E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com&quot;
]
</code></pre><ul><li>SceneXplain goes much further, even extracting product names and tiny details like when the company was established:</li></ul><pre><code class="language-json">[
    &quot;MDH&quot;,
    &quot;&#x915;&#x940; &#x90F;&#x915;&quot;,
    &quot;&#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&quot;,
    &quot;&#x938;&#x94D;&#x935;&#x93E;&#x926; &#x915;&#x947; &#x938;&#x93E;&#x925; -&#x938;&#x93E;&#x925; &#x916;&#x941;&#x936;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x940; &#x92C;&#x94C;&#x91B;&#x93E;&#x930; !&quot;,
    &quot;&#x92E;&#x938;&#x93E;&#x932;&#x93E; &#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x94B; &#x914;&#x930; &#x91C;&#x940;&#x924;&#x94B; &#x909;&#x92A;&#x939;&#x93E;&#x930; !!&quot;,
    &quot;&#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x915;&#x940; &#x913;&#x930; &#x938;&#x947; &#x905;&#x92A;&#x928;&#x947; &#x938;&#x92D;&#x940; &#x917;&#x94D;&#x930;&#x93E;&#x939;&#x915;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x90F;&#x915; &#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&#x964;&quot;,
    &quot;&#x915;&#x93F;&#x938;&#x940; &#x92D;&#x940; &#x928;&#x91C;&#x926;&#x940;&#x915;&#x940; &#x935;&#x93F;&#x915;&#x94D;&#x930;&#x947;&#x924;&#x93E; &#x938;&#x947; &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x92E;&#x938;&#x93E;&#x932;&#x94B;&#x902; &#x915;&#x947; &#x928;&#x940;&#x91A;&#x947; &#x91B;&#x92A;&#x947; &#x92A;&#x948;&#x915;&#x94D;&#x938; &#x92E;&#x947;&#x902; &#x938;&#x947;&quot;,
    &quot;&#x915;&#x94B;&#x908; &#x92D;&#x940; 100 &#x917;&#x94D;&#x930;&#x93E;&#x92E; &#x935;&#x93E;&#x932;&#x93E; &#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x947;&#x902; &#x914;&#x930; &#x92D;&#x93E;&#x917;&#x94D;&#x92F;&#x936;&#x93E;&#x932;&#x940; &#x935;&#x93F;&#x91C;&#x947;&#x924;&#x93E; &#x92C;&#x928;&#x928;&#x947; &#x915;&#x93E; &#x905;&#x935;&#x938;&#x930;&quot;,
    &quot;&#x92A;&#x94D;&#x930;&#x93E;&#x92A;&#x94D;&#x924; &#x915;&#x930;&#x947;&#x902;, &#x939;&#x94B; &#x938;&#x915;&#x924;&#x93E; &#x939;&#x948; &#x906;&#x92A; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x917;&#x90F; &#x92A;&#x948;&#x915; &#x915;&#x947; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x92E;&#x947;&#x902; &#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x93E; &#x928;&#x93E;&#x92E; &#x939;&#x94B;&#x964;&quot;,
    &quot;&#x905;&#x938;&#x932;&#x940; &#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
    &quot;&#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
    &quot;&#x938;&#x91A; -&#x938;&#x91A;&quot;,
    &quot;MD H&quot;,
    &quot;SPICES&quot;,
    &quot;MD&quot;,
    &quot;MOH&quot;,
    &quot;Garam&quot;,
    &quot;Pav Bhaji&quot;,
    &quot;masala&quot;,
    &quot;Kitchen&quot;,
    &quot;Shahi Pancer&quot;,
    &quot;King&quot;,
    &quot;Chana&quot;,
    &quot;DEGGI&quot;,
    &quot;MIRCH&quot;,
    &quot;Chunky Chat&quot;,
    &quot;&#x928;&#x93F;&#x92F;&#x92E; &#x935; &#x936;&#x930;&#x94D;&#x924;&#x947;&#x902; :- &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x92E;&#x938;&#x93E;&#x932;&#x94B;&#x902; &#x915;&#x947; &#x90A;&#x92A;&#x930; &#x926;&#x930;&#x94D;&#x936;&#x93E;&#x92F;&#x947; &#x917;&#x90F; &#x91A;&#x941;&#x928;&#x93F;&#x928;&#x94D;&#x926;&#x93E; &#x92A;&#x948;&#x915;&#x94D;&#x938; &#x92E;&#x947;&#x902; &#x938;&#x947; &#x915;&#x94B;&#x908; &#x92D;&#x940; 100 &#x917;&#x94D;&#x930;&#x93E;&#x92E; &#x935;&#x93E;&#x932;&#x93E;&quot;,
    &quot;&#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x947;&#x902;, &#x909;&#x938;&#x915;&#x93E; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x916;&#x94B;&#x932;&#x947;&#x902;, &#x939;&#x94B; &#x938;&#x915;&#x924;&#x93E; &#x939;&#x948; &#x915;&#x93F; &#x909;&#x938; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x915;&#x947; &#x928;&#x940;&#x91A;&#x947; &#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x93E; &#x928;&#x93E;&#x92E; &#x91B;&#x92A;&#x93E; &#x939;&#x94B;&#x964; &#x909;&#x938;&quot;,
    &quot;&#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x94B; &#x92A;&#x94D;&#x930;&#x93E;&#x92A;&#x94D;&#x924; &#x915;&#x930;&#x928;&#x947; &#x915;&#x947; &#x92D;&#x93E;&#x917;&#x94D;&#x92F;&#x936;&#x93E;&#x932;&#x940; &#x935;&#x93F;&#x91C;&#x947;&#x924;&#x93E; &#x92C;&#x928;&#x947;&#x902;&#x964; &#x2022; &#x907;&#x938; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x915;&#x93E; &#x928;&#x915;&#x926; &#x932;&#x93E;&#x92D; &#x915;&#x94B;&#x908; &#x92D;&#x940; &#x928;&#x939;&#x940;&#x902; &#x939;&#x948;&quot;,
    &quot;&#x932;&#x93F;&#x90F; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; &#x92E;&#x947;&#x902; &#x939;&#x940; &#x932;&#x93E;&#x917;&#x942; &#x939;&#x948; &#x2022; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x938;&#x94D;&#x91F;&#x949;&#x915; &#x930;&#x939;&#x928;&#x947; &#x924;&#x915; &#x938;&#x940;&#x92E;&#x93F;&#x924; &#x939;&#x948; &#x2022; &#x92F;&#x939; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x915;&#x947; &#x938;&#x941;&#x92A;&#x930;&quot;,
    &quot;&#x938;&#x94D;&#x91F;&#x949;&#x915;&#x93F;&#x938;&#x94D;&#x91F;, &#x938;&#x94D;&#x91F;&#x949;&#x915;&#x93F;&#x938;&#x94D;&#x91F; &#x914;&#x930; &#x915;&#x930;&#x94D;&#x92E;&#x91A;&#x93E;&#x930;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x928;&#x939;&#x940;&#x902; &#x939;&#x948; &#xB7; &#x915;&#x902;&#x92A;&#x928;&#x940; &#x915;&#x93E; &#x928;&#x93F;&#x930;&#x94D;&#x923;&#x92F; &#x905;&#x902;&#x924;&#x93F;&#x92E; &#x914;&#x930; &#x938;&#x930;&#x94D;&#x935;&#x92E;&#x93E;&#x928;&#x94D;&#x92F; &#x939;&#x94B;&#x917;&#x93E;&quot;,
    &quot;&#x914;&#x930; &#x938;&#x92D;&#x940; &#x935;&#x93F;&#x935;&#x93E;&#x926; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; &#x915;&#x947; &#x928;&#x94D;&#x92F;&#x93E;&#x92F; &#x915;&#x94D;&#x937;&#x947;&#x924;&#x94D;&#x930; &#x915;&#x947; &#x905;&#x927;&#x940;&#x928; &#x939;&#x94B;&#x902;&#x917;&#x947;&#x964; &#x2022; &#x905;&#x928;&#x94D;&#x92F; &#x936;&#x930;&#x94D;&#x924;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x92A;&#x948;&#x915; &#x926;&#x947;&#x916;&#x947;&#x902;&#x964;&quot;,
    &quot;&#x92E;&#x939;&#x93E;&#x936;&#x93F;&#x92F;&#x93E;&#x901; &#x926;&#x940; &#x939;&#x91F;&#x94D;&#x91F;&#x940; (&#x92A;&#x94D;&#x930;&#x93E;&#x966;) &#x932;&#x93F;&#x92E;&#x93F;&#x91F;&#x947;&#x921;&quot;,
    &quot;9/44, &#x915;&#x940;&#x930;&#x94D;&#x924;&#x93F; &#x928;&#x917;&#x930;, &#x928;&#x908; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; - 110015 &#x92B;&#x94B;&#x928; &#x928;&#x902;&#x966; 011-41425106 - 07-08&quot;,
    &quot;ESTD. 1919&quot;,
    &quot;E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com&quot;
  ]
</code></pre><h2 id="conclusion">Conclusion</h2><p>At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap:</p><p><strong>API limits:</strong></p><ul><li>GPT-4V: 100 requests per day</li><li>SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month.</li></ul><p><strong>Reliability:</strong></p><ul><li>GPT-4V: Is often like &#x201C;I&#x2019;m sorry Dave, I&#x2019;m afraid I can&#x2019;t do that.&#x201D;</li><li>SceneXplain: Doggedly reliable. Gets the job done.</li></ul><p><strong>Accuracy:</strong></p><ul><li>GPT-4V: hallucinates strings it believes are plausible from a given image, rather than reading the actual text.</li><li>SceneXplain: Extracts the correct text.</li></ul><p><strong>Consistency:</strong></p><ul><li>GPT-4V doesn&#x2019;t just hallucinate. It hallucinates different things each time.</li><li>SceneXplain gives much more consistent output.</li></ul><p><strong>Completeness:</strong></p><ul><li>GPT-4V: Often doesn&#x2019;t always extract <strong><em>all</em></strong> the strings in the image.</li><li>SceneXplain: Extracts more strings.</li></ul><p><strong>Cost:</strong></p><ul><li>GPT-4V: $0.025 per image.</li><li>SceneXplain: As low as 0.02 USD per image on the MAX plan if you enable JSON output. As low as 0.01 USD without JSON output.</li></ul><p>When you take all those factors into account, it&apos;s clear that SceneXplain is the obvious choice.</p><h2 id="get-started-with-scenexplain">Get started with SceneXplain</h2><p>Ready to explore multilingual OCR? Sign up for a free account with SceneXplain and start converting images to text easily. It&apos;s a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Embeddings in Depth]]></title><description><![CDATA[This second article in our series on embedding technology is much more concrete. It explains where embeddings come from and how they are used.]]></description><link>https://jina.ai/news/embeddings-in-depth/</link><guid isPermaLink="false">65535d16173d1400018405a8</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 15 Nov 2023 15:00:38 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--18--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--18--1.png" alt="Embeddings in Depth"><p>This article is the second in our series on embedding technology. In the <a href="https://jina.ai/news/how-embeddings-drive-ai-a-guide/?ref=jina-ai-gmbh.ghost.io">previous article, we discussed</a> embeddings in very abstract terms, but this one will be much more concrete. It explains where embeddings come from and how they are used.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Trigger Warning:</strong></b> This article contains some light algebra, and although it tries to avoid complicated math, there are some notational conventions from vector algebra.</div></div><p>An <em>embedding</em> is a <em>mapping</em> of objects to <em>vectors</em>. Those objects may be real things or digital objects like images and files.</p><p>For example, imagine a robot that harvests tomatoes:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/JsZJWyMVBtY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="This Tomato-Picking Robot Is the Future of Farming &#x1F345; &#x1F916;"></iframe></figure><p>This robot selects tomatoes on the vine that are ready to harvest, and then snips their stalks and takes them away. Recognizing which tomatoes are ripe is a good application of image embeddings. Each tomato on the vine is assigned a representative vector based on digital images taken by the robot&#x2019;s cameras. It then uses these embeddings to decide whether or not to harvest a particular tomato.</p><p>Similarly, we might use embeddings to study and predict customer behavior. These embedding vectors stand in for a person &#x2014; a real human being who presumably exists somewhere &#x2014; but we construct their embedding vectors from digital information about that person. For a study of consumer buying behavior, this is likely to be some combination of demographic information and purchasing records.</p><p>In the previous article, we discussed how vectors are just ordered lists of numbers that we decide to interpret as points in a high-dimensional space. Data stored in a computer is also just an ordered list of numbers. Embeddings are, therefore, made by translating input vectors in one vector space &#x2014; the one that contains the source data &#x2014; into vectors in a different space called the <strong><em>embedding space</em></strong>.</p><p>But not just any mapping between vector spaces is an embedding. The translation into embeddings is purposeful. The placement of embedding vectors relative to each other should be informative in some way about the objects they represent.</p><h2 id="cosines-how-we-compare-embeddings">Cosines: How We Compare Embeddings</h2><p>When we use embeddings in applications, the thing we most often do is compare two embeddings and calculate how close or far apart they are. This is indicative of the similarity or dissimilarity of the objects those embeddings stand in for with respect to some set of properties.</p><p>To do this, we use a metric called the <strong>cosine</strong>. This section explains cosines and how we calculate them in high-dimensional vector spaces.</p><p>An embedding is a vector, and as we showed in the previous section, we can interpret a vector as a point in a multi-dimensional space. However, it also has an interpretation as a direction and magnitude. Traditionally, we visualize this interpretation with an arrow:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-19.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1350" height="1560" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-19.png 1350w" sizes="(min-width: 720px) 720px"></figure><p>The <em>direction</em> of this three-dimensional vector is specified by the $(x,y,z)$ coordinates, and the <em>magnitude</em> is just the length of the vector, i.e., the distance from the coordinate $(x,y,z)$ to the origin at $(0,0,0)$. The length is calculated using a multidimensional extension of <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem?ref=jina-ai-gmbh.ghost.io">Pythagoras&apos; theorem</a>:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-20.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1500" height="300" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-20.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-20.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-20.png 1500w" sizes="(min-width: 720px) 720px"></figure><p>The double bars around a vector $\|\vec{v}\|$ indicate the vector&apos;s <em>length</em> (or <em>magnitude</em>).</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Although this example uses only three dimensions, all these formulas apply exactly the same to arbitrarily large vectors of any number of dimensions.</div></div><p>We can see that different vectors can have the same direction but different lengths. For example, if we have the vectors $(4,2)$ and $(2,1)$, they have different lengths ($\sqrt{20} \approx 4.47$ and $\sqrt{5} \approx 2.23$ respectively) but they point in the same direction.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-21.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="938" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-21.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-21.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-21.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>How do we know they point in the same direction? We measure the angle between the two vectors. If the angle is zero, they point in the same direction.</p><p>To show how this works, let&#x2019;s consider two vectors: $\vec{a}=(3,2,5)$ and $\vec{b}=(4,5,2)$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-52.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="1140" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-52.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-52.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-52.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-52.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>To calculate the angle, we have a formula that scales up to any number of dimensions: The cosine.</p><p>The cosine of the angle&#xA0;$\theta$&#xA0;between vectors&#xA0;$\vec{a}$&#xA0;and&#xA0;$\vec{b}$&#xA0;is:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-50.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="208" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-50.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-50.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-50.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-50.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>The numerator of this fraction, $a&#x22C5;b$,&#xA0;is the&#xA0;<em>dot product</em>&#xA0;of the two vectors, and it&#x2019;s easy to calculate:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-49.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="288" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-49.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-49.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-49.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-49.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>As for&#xA0;&#x2016;<em>a</em>&#x2016;&#xA0;and&#xA0;&#x2016;<em>b</em>&#x2016;, those are the lengths of the two vectors: $\|\vec{a}\| = \sqrt{38} \approx 6.164$ and $\vec{b} = \sqrt{45} \approx 6.708$. Therefore, to calculate&#xA0;$cos&#x3B8;$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-22.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="587" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-22.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>This cosine corresponds to an angle of approximately 39.3 degrees, but in machine learning, we typically stop once we&#x2019;ve calculated the cosine because if all the numbers in both vectors are greater than zero, then the cosine of the angle will be between 0 and 1. A cosine of 1 means the angle between two vectors is 0 degrees, i.e., the vectors have the same direction (although they may have different lengths). A cosine of 0 means they have an angle of 90 degrees. i.e., they are <em>perpendicular</em> or <em>orthogonal</em>.</p><p>So let&#x2019;s compare $\vec{a}=(4,2)$&#xA0;and&#xA0;$\vec{b}=(2,1)$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-23.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-23.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>When the cosine of two vectors is one, they point in the same direction, even though they may have different lengths.</p><p>Another way to do the same operation is to take each vector and <em>normalize</em> it. Normalizing a vector means creating a new vector that points in the same direction but has a length of exactly 1. If two vectors point in the same direction, they will normalize to identical vectors.</p><p>To normalize a vector, divide it in each dimension by the vector&#x2019;s length. If $\vec{v} = (3,2,5)$ and $\hat{v} = norm(\vec{v})$ then:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-24.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="1232" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-24.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>If two vectors $\vec{a}$ and $\vec{b}$ have an angle between them $\theta$, then their normalized forms $\hat{a}$ and $\hat{b}$ will have the same angle $\theta$ between them. Cosine is a measure that is indifferent to the magnitudes of vectors.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">When we compare embeddings to decide how near or far they are from each other, we almost always use cosines.</div></div><p>The reason for this is that the magnitudes of embedding vectors are not usually informative about anything. Most neural network architectures perform normalizations between some or all of their layers, rendering the magnitudes of embedding vectors typically useless.</p><h2 id="neural-networks-as-vector-mappings">Neural Networks As Vector Mappings</h2><p>Embedding vectors work because they are placed so that things that have common properties have cosines that are closer to 1, and those that do not have cosines closer to 0. An embedding model takes input vectors and translates them into embedding vectors.</p><p>Although there are many techniques for performing such translations, today, embedding models are exclusively neural networks.</p><p>A neural network is a sequence of vector transformations. In formal mathematical terms, for each layer of the network, we multiply the vector output of the previous layer by a matrix, and then generally add a <em>bias.</em> A <em>bias</em> is a set of values that we add or subtract from the resulting vector. We take the result and make that the input to the next layer of the network.</p><p>More formally, the values of the $m$th layer of the network are $\vec{m}$ and are transformed into the values of the $n$th layer ($\vec{n}$) by:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-25.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="325" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-25.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-25.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-25.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-25.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>Where $\textbf{W}$ is a matrix and $\vec{bias}$ is the vector of bias values.</p><p>It&#x2019;s customary to think of it as a network of &#x201C;nodes&#x201D; bearing numerical values, connected by &#x201C;weights&#x201D;. This is truer to the roots of neural network theory but is exactly equivalent to multiplying a vector by a matrix.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-38.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="650" height="1100" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-38.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-38.png 650w"></figure><p>If we have a vector $\vec{m} = (m_1,m_2m_3)$, this one neural network layer transforms it into a vector $\vec{n} = (n_1,n_2,n_3,n_4,n_5)$. It does that by multiplying the values in $\vec{m}$ by a set of weights (also called <em>parameters</em>), then adding them up, and adding the bias vector $\vec{bias} = (bias_1,bias_2,bias_3,bias_4,bias_5)$</p><p>Each line between nodes in the diagram above is one weight. If we designate the weight from $m_1$ to $n_1$ as $w_{11}$, from $m_2$ to $n_3$ as $w_{23}$, etc., then:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-26.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="600" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-26.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-26.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-26.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-26.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>In matrix algebra notation, this is the same as multiplying the 3-dimensional vector $\vec{n}$ by a $3x5$ matrix to get $\vec{m}$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-27.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="472" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-27.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-27.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-27.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Neural networks have multiple layers, so the output of this matrix multiplication is multiplied by another matrix and then another, typically with some kind of thresholding or normalization between multiplications and sometimes other tricks. The most common way of doing this today is to apply the <strong>ReLU</strong> (<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)?ref=jina-ai-gmbh.ghost.io">rectified linear activation unit</a>) activation function. If the value of any node is below zero, it&#x2019;s replaced with a zero, and otherwise, left unchanged.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-28.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="520" height="198"></figure><p>Otherwise, a neural network is just a series of transformations like the one above from $\vec{m}$ to $\vec{n}$.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-40.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="261" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-40.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-40.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-40.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-40.png 2300w" sizes="(min-width: 720px) 720px"></figure><p>This structure has some important properties. The most important is that:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In theory, any kind of vector transformation is possible with enough layers. Any problem that can be expressed as a mapping from a vector space $\vec{x} \in \textit{A}$ to vectors in another vector space $\vec{y} \in \textit{B}$ has some solution that looks just like a neural network.</div></div><p>It may be impractical to construct a large enough network for a given problem, and there may be no procedure for correctly calculating the weights. AI models are not guaranteed in advance to be able to learn or perform any task at all.</p><p>What happens in practice is that neural networks <strong><em>approximate</em></strong> a solution by learning from examples. How good a solution that approximation is depends on a lot of factors and can&#x2019;t typically be known in advance of training a model.</p><p>There is an aphorism attributed to the British statistician George Box:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">All models are wrong, but some are useful.</em></i></div></div><p>Neural networks can be very good approximations, but they are still approximations, and it&#x2019;s not possible to know for certain how good they are at something until you put them into production. The real measure of an AI model is its usefulness, and that&#x2019;s hard to know in advance.</p><h2 id="turning-problems-into-vector-mappings">Turning Problems Into Vector Mappings</h2><p>Defining neural networks as mappings from one vector space to another sounds very limiting. We don&#x2019;t naturally see the kinds of things we want AI models to do in those terms.</p><p>However, many of the most visible developments in recent AI are really just ways of using vector-to-vector mappings in clever ways. Large language models like ChatGPT translate input texts, rendered as vectors, into vectors that designate a single word. They write texts by adding the single-word output to the end of the input and then running again to get the next word.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-42.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="2000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-42.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-42.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-42.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-42.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>It may take some creativity and engineering to find a way to express some task as a vector mapping, but as recent developments show, it is surprisingly often possible.</p><h2 id="where-do-embeddings-fit-in">Where Do Embeddings Fit In?</h2><p>When you train an AI model, you do it with some goal in mind. In the typical scenario, you have training data that consists of inputs and matching correct outputs. For example, let&#x2019;s say you want to classify tweets by their emotional content. This is a common benchmarking task used to evaluate text AI models.</p><p>One widely used dataset (<a href="https://github.com/dair-ai/emotion_dataset?ref=jina-ai-gmbh.ghost.io">DAIR.AI Emotion</a>) sorts tweets into eight emotional categories:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-58.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="740" height="420" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-58.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-58.png 740w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-55.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1500" height="1106" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-55.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-55.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-55.png 1500w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Number of tweets per emotion category in the </span><a href="http://dair.ai/?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">DAIR.AI</span></a><span style="white-space: pre-wrap;"> Emotion dataset</span></figcaption></figure><p>To train a model to do this task, you need to decide two things:</p><ol><li>How to map the output category labels to vectors.</li><li>How to map the input texts to vectors</li></ol><p>The easiest way to encode these emotion labels is to create six-dimensional vectors where each value corresponds to one category. For example, if we make a list of emotions, like the one above, &#x201C;anger&#x201D; is the third in the list. So if a tweet has angry content, we train the network to output a six-dimensional vector with zero values everywhere except the third item in it, which is set to one:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-30.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="589" height="81"></figure><p>This approach is called <em>zero-hot encoding</em> among AI engineers.</p><p>We also have to encode the tweets as vectors. There is more than one way to do this, but most text-processing models follow broadly the same pattern:</p><p>First, input texts are split into <em>tokens</em>, which you can understand as something like splitting it into words. The exact mechanisms vary somewhat, but the procedure broadly follows the following steps:</p><ol><li>Normalize the case, i.e., make everything lowercase.</li><li>Split a text on punctuation and spaces.</li><li>Look up each resulting subsegment of text in a dictionary (which is a part of the AI model bundle) and accept it if it is in the dictionary.</li><li>For segments not in the dictionary, split them into the smallest number of parts that are in the dictionary, and then accept those segments.</li></ol><p>So, for example, let&#x2019;s say we wanted an embedding for this famous line from <em>Alice in Wonderland</em>:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">Why, sometimes I&apos;ve believed as many as six impossible things before breakfast.</em></i></div></div><p>Jina Embeddings&#x2019; text tokenizer will split it up into 18 tokens like this:</p><pre><code>[CLS] why , sometimes i &apos; ve believed as many as six impossible 
things before breakfast . [SEP]
</code></pre><p>The <code>[CLS]</code> and <code>[SEP]</code> are special tokens added to the text to indicate the beginning and end, but otherwise, you can see that the splits are between words on spaces and between letters and punctuation.</p><p>If the tokenizer encounters a sentence with words that are not in the dictionary, it tries to match the parts to entries in its dictionary. For example:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">Klaatu barada nikto</em></i></div></div><p>This is rendered in the Jina tokenizer as:</p><pre><code>[CLS] k ##la ##at ##u bar ##ada nik ##to [SEP]
</code></pre><p>The <code>##</code> indicates a segment appearing elsewhere than the beginning of a word.</p><p>This approach guarantees that every possible string can be tokenized and can always be processed by the AI model, even if it&#x2019;s a new word or a nonsense string of letters.</p><p>Once the input text is transformed into a list of tokens, we replace each token with a unique vector from a lookup table. Then, we concatenate those vectors into one long vector.</p><p>Many text-processing AI models (including Jina Embeddings 2 models) use vectors of 512 dimensions for each token. All text-processing models are limited in the number of tokens they can accept as input. Jina Embeddings 2 models accept 8,192 tokens, but most models only take 512 or fewer.</p><p>If we use 512 for this example, then to do emotion classification for these tweets, we need to build a model that takes an input vector of $512 \times 512 = 262144$ dimensions, and outputs a vector of 6 dimensions:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-43.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1350" height="800" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-43.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-43.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-43.png 1350w" sizes="(min-width: 720px) 720px"></figure><p>Neural networks are often presented as a kind of black box in which the parts on the inside are not described in depth. This is not wrong, but it&#x2019;s not very informative. In practice, large, modern AI models tend to have three components, although this is not always true, and the boundaries between them are not always clear:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-44.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1400" height="800" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-44.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-44.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-44.png 1400w" sizes="(min-width: 720px) 720px"></figure><p>The vocabulary for some of this is not entirely codified, and this schema does not apply to all AI models, but in general, a trained model has three parts:</p><ol><li><strong>An input processor or encoder</strong><br><br>Especially for networks that take in very large input vectors, there is some part of the network devoted to correlating all parts of the input with each other. This section of the network can be very large, and it can be structurally complex. Many of the biggest breakthroughs in AI in the last two decades have involved innovative structures in this part of the model.<br></li><li><strong>A multi-layer perceptron</strong><br><br>There isn&#x2019;t a consistent term for this &#x201C;middle&#x201D; part of the model, but &#x201C;MLP&#x201D; or &#x201C;multi-layer perceptron&#x201D; appear to be the most common terms in the technical literature. The term itself is a bit old-fashioned, recalling the &#x201C;perceptron&#x201D;, a kind of small single-layer neural network invented in the 1950s that is ancestral to modern AI models.<br><br>The MLP typically consists of a number of connected layers of the same size. For example, the smallest of the ViT image processing models has twelve layers with 192 dimensions in each, while the largest has 24 layers, each with 768 dimensions.<br><br>The size and structure of the MLP are key design variables in an AI model.<br></li><li><strong>The &#x201C;Classifier&#x201D;</strong><br><br>This is usually the smallest part of the model, typically much smaller than the MLP. It may be as small as one layer but typically has more in modern models. This part of the model translates the output of the MLP into the exact form specified for the output, like the thousand-dimension one-hot vectors used for Imagenet-1k. It may not actually be a classifier. This final section of the model may be trained to do some other task.</li></ol><p>The size of the input encoder is mostly determined by your planned input sizes and the size of the first layer of the MLP. The size of the &#x201C;classifier&#x201D; is mostly determined by the training objective and the size of the last layer of the MLP. You have relatively few design choices in those areas. The MLP is the only part where you can make free decisions about your model.</p><p>The size of the MLP is a design choice balancing three factors:</p><ul><li>Smaller MLPs learn faster, run faster, and take less memory than large MLPs.</li><li>Large MLPs typically learn more and perform better, all else being equal.</li><li>Large MLPs are more likely to &#x201C;overfit&#x201D; than small MLPs. There is a risk that instead of generalizing from their training examples, they will memorize them and not learn effectively.</li></ul><p>The optimal balance depends on your use case and context, and it is very difficult to calculate the optimum in advance.</p><p>Most of the learning takes place in the MLP. The MLP learns to recognize those features of the inputs that are useful in calculating the correct output. The last layer of the MLP outputs vectors that are closer together the more they share those features. The &#x201C;classifier&#x201D; then has the relatively easy task of translating that into the correct output, like what class a particular input belongs to..</p><p>So, for example, if we train a text classifier with the six emotion labels from our dataset, we would expect the outputs of the MLP to be a high-dimensional analog of something like this:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-53.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1800" height="1250" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-53.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-53.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-53.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-53.png 1800w" sizes="(min-width: 720px) 720px"></figure><p>In short, the output of the last layer of the MLP has all the properties we want in an embedding: It puts similar things close together and dissimilar things further apart.</p><p>We typically construct embedding models by taking a model we&#x2019;ve trained for something &#x2014; often classification, but not necessarily &#x2014; and throwing away the &#x201C;classifier&#x201D; part of the model. We just stop at the last layer of the MLP and use its outputs as embeddings.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-47.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1200" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-47.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-47.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-47.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>There are four key points to take from this:</p><ol><li>Embeddings put things close to each other that are similar based on a set of features.</li><li>The set of features that embeddings encode is not usually specified in advance. It comes from training a neural network to do a task and consists of the features the model discovered were relevant to doing that task.</li><li>Embedding vectors are typically very high-dimensional but are usually much, much smaller than input vectors.</li><li>Most models have an embedding layer in them somewhere, even if they never expose it or use it as a source of embedding vectors.</li></ol><h2 id="text-embeddings">Text Embeddings</h2><p>Text embeddings work in very much the same way we&#x2019;ve described above, but while it&#x2019;s easy to understand how apples are like other apples and unlike oranges, it&#x2019;s not so easy to understand what makes texts similar to each other.</p><p>The first and most prototypical application of text embeddings is for search and information retrieval. In this kind of application, we start with a collection of texts:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-31.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="1778" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-31.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-31.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-31.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-31.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We then calculate and store their embeddings:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-32.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="391" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-32.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-32.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-32.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-32.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>When users want to search this document collection, they write a query in normal language:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-33.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="171" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-33.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-33.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-33.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-33.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We also convert this text into its corresponding embedding:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-34.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="171" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-34.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-34.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-34.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-34.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We perform search by taking the cosine between $\vec{q}$ and each of the document embeddings $\vec{d}_1,\vec{d}_2,\vec{d}_3$, identifying the one with the highest cosine (and thus is closest to the query embedding). We then return to the user the corresponding document.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-35.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="164" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-35.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-35.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>So, one training goal for text similarity is information retrieval: If we train a model to be a good search engine, it will give us embeddings that put two documents close to each other in proportion to how much the same queries will retrieve them both.</p><p>There are other possible goals we can use to train text embeddings, like classifying texts by genre, type, or author, which will lead to embeddings that preserve different features. There are a number of ways we can train text embedding models, but we will discuss that topic in the next article, which will go into detail about training and adapting AI models. What matters is what you&#x2019;ve trained the AI model to see as a text&apos;s most relevant features.</p><h2 id="jina-embeddings">Jina Embeddings</h2><p>As a concrete example, the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings 2</a> models represent each token with a 512-dimension vector and can accept a maximum of 8192 tokens as input. This AI model, therefore, has an input vector size of 512x8192 or 4,194,304 dimensions. If the input text is less than 8192 tokens long, the rest of the input vector is set to zeros. If longer, it&#x2019;s truncated.</p><p>The model outputs a 512-dimension embedding vector to represent the input text.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-48.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1400" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-48.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-48.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-48.png 1400w" sizes="(min-width: 720px) 720px"></figure><p>As a developer or user, you don&#x2019;t have to deal directly with any of these complexities. They are hidden from you by software libraries and web APIs that make using embeddings very straightforward. To transform a text into an embedding can involve as little as one line of code:</p><pre><code class="language-bash">curl &lt;https://api.jina.ai/v1/embeddings&gt; \\
  -H &quot;Content-Type: application/json&quot; \\
  -H &quot;Authorization: Bearer jina_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; \\
  -d &apos;{
    &quot;input&quot;: [&quot;Your text string goes here&quot;, &quot;You can send multiple texts&quot;],
    &quot;model&quot;: &quot;jina-embeddings-v2-base-en&quot;
  }&apos;
</code></pre><p>Go to the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings page</a> to get a token and see code snippets for how to get embeddings in your development environment and programming language.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Embeddings in Depth"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Embeddings in Depth"></div></a></figure><h2 id="embeddings-the-heart-of-ai">Embeddings: The Heart of AI</h2><p>Practically all models have an embedding layer in them, even if it&apos;s only implicit, and most AI projects can be built on top of embedding models. That&apos;s why good embeddings are so important to the effective use of AI technologies.</p><p>This series of articles is designed to demystify embeddings so that you can better understand and oversee the introduction of this technology in your business. Read the <a href="https://jina.ai/news/how-embeddings-drive-ai-a-guide/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">first installment here</a>. Next, we will dive further into this technology, and provide hands-on tutorials with code you can use in your own projects.</p><p>Jina AI is committed to providing you with tools and help for creating, optimizing, evaluating, and implementing embedding models for your enterprise. We&#x2019;re here to help you navigate the new world of business AI. Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to get started.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Embeddings in Depth"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Embeddings in Depth"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3602 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Embeddings in Depth"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Embeddings in Depth"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data]]></title><description><![CDATA[Unlock the Secrets of the Satellites: Leverage SceneXplain's powerful 'Extract JSON from Image' feature for land use classification]]></description><link>https://jina.ai/news/look-up-in-the-sky-using-scenexplain-to-classify-land-use-from-satellite-data/</link><guid isPermaLink="false">6544c26c2305600001855879</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 06 Nov 2023 15:00:55 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><p>Is it a bird? Is it a plane? Is it a dense residential area? Or maybe even a chaparral? Worry not readers. SceneXplain is here to let you know. We don&apos;t even have to jump into a phone booth to change first.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F331;</div><div class="kg-callout-text">A <a href="https://en.wikipedia.org/wiki/Chaparral?ref=jina-ai-gmbh.ghost.io">chaparral</a> is a shrubland plant community found primarily in California, in southern Oregon and in the northern portion of the Baja California Peninsula in Mexico. <br><br>You&apos;ll see why this is relevant later.</div></div><p>In this blog post, we&apos;ll talk about our experiments using SceneXplain to look at satellite imagery and classify it based on what the land is used for. We&apos;ll go through three different datasets and explain how we did what we did.</p><h2 id="what-is-land-use-classification-and-what-is-it-used-for">What is land use classification and what is it used for?</h2><p>Using satellite data for classifying land use involves working out what a section of the Earth is used for, such as agriculture, city, forest, or body of water. The focus here is on using optical satellite imagery as the data source, which is then analyzed through AI image models to determine the land use. It&apos;s often used for:</p><ul><li>Resource Management: Knowing how land is being used aids in allocating and managing resources, such as water for irrigation.</li><li>Urban Planning: Good land use maps are invaluable to city planners so that they can know what types of land use exist in and around urban areas to make informed decisions on infrastructure development.</li><li>Environmental Protection: By identifying sensitive or critical ecological zones, conservationists can better protect and manage them.</li><li>Disaster Management: Land use classification can help to identify vulnerable areas that require priority during emergencies.</li><li>Climate Change Studies: Monitoring land use over time can offer insights into climate change impacts, such as the rate of deforestation.</li><li>Research and Education: Scientists and educators across disciplines use this data for various kinds of research, from ecology to social sciences.</li></ul><h2 id="up-up-and-away-how-we-classify-land-usage-in-scenexplain">Up, up, and away! How we classify land usage in SceneXplain</h2><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F40D;</div><div class="kg-callout-text">You can find a link to the notebook <a href="https://colab.research.google.com/github/alexcg1/scenex-geospatial/blob/main/notebook.ipynb?ref=jina-ai-gmbh.ghost.io">here</a>.</div></div><h3 id="datasets">Datasets</h3><p>We used three datasets for our testing:</p><ul><li><a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html?ref=jina-ai-gmbh.ghost.io">UC Merced land use classification</a></li><li><a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID (Aerial Image Dataset)</a></li><li><a href="https://www.tensorflow.org/datasets/catalog/resisc45?ref=jina-ai-gmbh.ghost.io">RESISC45 (Remote Sensing Image Scene Classification)</a></li></ul><p>Each of these consists of satellite and aerial images of different sections of Earth, labeled by what they are used for, e.g. <code>dense_residential</code>, <code>round_farm</code>, <code>intersection</code>, etc.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg" width="600" height="600" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg 600w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral_010.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><code spellcheck="false" style="white-space: pre-wrap;"><span>airport</span></code><span style="white-space: pre-wrap;"> from AID dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>denseresidential</span></code><span style="white-space: pre-wrap;"> from UC Merced dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>chaparral</span></code><span style="white-space: pre-wrap;"> from RESISC45 dataset</span></p></figcaption></figure><h3 id="scenexplain">SceneXplain</h3><p>We used SceneXplain&apos;s <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">&quot;Extract JSON from image&quot; feature</a> with the Flash algorithm to extract one label per input image. After testing with several algorithms, we saw the Flash algorithm offered the fastest performance, and precision on par with more recent algorithms like <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Jelly</a>.</p><p>Rather than manually uploading each image via the web UI, we used SceneXplain&apos;s API to upload and analyze images one at a time.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We could speed this up by (ab)using SceneXplain&apos;s cid feature when batching and giving each image a unique ID. The ID would contain the image&apos;s &quot;official&quot; category, so (after processing) we could extract that and compare it to the category assigned by SceneXplain.</div></div><h3 id="json-schema">JSON Schema</h3><p>Why even use a JSON schema? Why not just let SceneXplain interpret the images directly? Say, by looking at an aerial view of a baseball diamond from the UC Merced dataset?</p><p>Here&apos;s why:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--39-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="441" height="666"><figcaption><a href="https://scenex.jina.ai/share?thread=qjW5p1gZBYK2HONbNsOE&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>It&#x2019;s a clear description, but nowhere does it mention <code>baseballdiamond</code>, which (since we&#x2019;re doing classification) is the label we want.</p><p>We could also try asking it a question using visual question answering (VQA):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--40-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="444" height="798"><figcaption><a href="https://scenex.jina.ai/share?thread=VW4xcmLawvG8lWInwwAa&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>That answer is slightly better, but still throws in a lot more superfluous words. If we were to bulk classify images using this method we would have to search each answer for the category label, since there would be no consistent wording between them. And what would happen if an image were ambiguous and SceneXplain tried to assign it two categories or more?</p><p>For this reason, we used SceneXplain&#x2019;s &#x201C;<a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io">Extract JSON from image</a>&#x201D; feature, where we specify a JSON Schema and can thus get highly structured, standardized output:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="612" height="1389" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--41-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png 612w"><figcaption><a href="https://scenex.jina.ai/share?thread=agnPj7H47wl87kDiEjM6&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>We used the following JSON schema:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;category&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Which single main category of geospatial imagery does this image belong to?&quot;,
      &quot;enum&quot;: [&lt;categories from dataset&gt;],
      &quot;maxContains&quot;: 1
    }
  }
}
</code></pre><p>Since each dataset has different (albeit similar) categories, we generated the list dynamically for each dataset.</p><h2 id="does-it-work">Does it work?</h2><p>Sometimes! Like at least 60% of the time! In some randomized tests we got close to 80% (or occasionally as low as 50%).</p><p>Several factors can cause it to fail:</p><ul><li>Some categories can look very similar, e.g. <code>sparse_residential</code>, <code>medium_residential</code>, <code>dense_residential</code>. SceneXplain often picks the wrong one. This can also be seen in cases like <code>road</code> vs <code>runway</code>.</li><li>Occasionally it hallucinates a new category not specified in the <code>enum</code>, for example <code>residential</code> (as opposed to <code>denseresidential</code>). Occasionally it glitches and assigns a category like <code>A</code>.</li><li>Some category names like <code>chaparral</code> are uncommon words and/or concepts. It seems unlikely that many pictures of (or references to) chaparrals are in its training datasets. With so little data, the model barely knows what a chaparral is or what it looks like. (Before reading this post, did you?)</li><li>Aerial view geospatial images are under-represented in the training corpora relative to other images. This means that to a general-purpose tool like SceneXplain, an aerial view of a chaparral may look more like mold or dirt on a surface.</li></ul><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png" width="1200" height="1200" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--42-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--42-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png 1200w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral01.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">One of these is mold. The other is chaparral. Sorry, California.</span></p></figcaption></figure><h2 id="why-not-use-a-dedicated-model">Why not use a dedicated model?</h2><p>Most land-use classification is indeed done with dedicated models, specifically trained on aerial-view land use imagery. This makes them perfect for that particular task while not being so hot at general image classification.</p><p>So, why not just use one of those?</p><h3 id="self-hosting-is-a-drag">Self-hosting is a drag</h3><p>Even self-hosting something like Resnet (which has <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/torch-neuronx/resnet50-inference-on-trn1-tutorial.html?ref=jina-ai-gmbh.ghost.io">decent tutorials</a>) is a drag. You need to set up AWS, install dependencies, compile the model, and so on. It&#x2019;s not as easy as using an existing software-as-a-service like SceneXplain.</p><h3 id="academic-models-are-pure-kryptonite">Academic models are pure kryptonite</h3><p>Let&apos;s look at the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset</a> and see how we can replicate what they did in their paper:</p><p>First of all, we have to download the code from OneDrive or Baidu Pan. Dang, the file no longer exists on OneDrive, and Baidu Pan wants me to install a random RPM or DEB file just to download the dataset. Blech.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="1238" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--43-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--43-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png 1238w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Parents: Please check your children&apos;s Halloween candy. I just found a Baidu Pan installation RPM in my kid&apos;s bag. Disgusting.</span></figcaption></figure><p>Assuming we have a friend in China who downloads the code and sends it to us (thanks Kelly!), we can extract it and check the <code>readme.txt</code> (Did you assume the readme would be in a repo somewhere, you sweet innocent summer child?)</p><p>Checking the file dates with <code>ls -l</code>, we see the files were last modified in 2016. Great.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="741" height="338" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--44-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png 741w" sizes="(min-width: 720px) 720px"></figure><p>Now we download the pre-trained model from the <a href="https://www.vlfeat.org/matconvnet/pretrained/?ref=jina-ai-gmbh.ghost.io">URL in the readme</a>. Good news! The link works! Bad news! There are lots of CNNs there and the readme doesn&apos;t say which one to download!</p><p>To quote the readme, the next step is to: <code>Compile vlfeat, gist, matconvnet and liblinear, and place them under &lt;libs&gt;</code>. We didn&apos;t actually try this, but I&apos;m <em>sure</em> compiling software from over five years ago will go without a hitch. I just need to use version...um. There&apos;s no version specified. It&apos;s sheer dumb luck I guess!</p><p>The final step: We&apos;ve put so much work into the other steps that now we can finally reap the fruits of our labor. We just need to install MatLab. Which is 860 EUR per year. Wonderful.</p><p>I&apos;m sure setting up the model was worth the effort for whoever wrote the paper. But if I&apos;m just trying things out, that&apos;s a lot of work and money.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A2;</div><div class="kg-callout-text">On the verge of giving up, I checked the directory structure of the code. Reading the readme, I assumed I had to <i><em class="italic" style="white-space: pre-wrap;">download</em></i> the libs. But the libs folder already exists. Already (supposedly) populated with what I need. Turns out I can&apos;t even trust the readme (or perhaps myself, to be honest)</div></div><h3 id="i-got-ninety-nine-problems-but-a-chaparral-aint-one">I got ninety-nine problems, but a chaparral ain&apos;t one</h3><p>Even assuming pre-trained models were simple to use, the categories are baked in already. Since most of the training data was (I assume) taken from California, chaparrals are all over the place, but there aren&apos;t so many medieval castles. So if I wanted to apply the same model to Europe, I&apos;d be stuck with those categories.</p><p>On the other hand, SceneXplain is a general-purpose tool. This means it may not have extensive chaparral knowledge (seriously, outside of Californians, who among us does?), but it has enough general-purpose knowledge that it can more reliably classify what you&apos;re specifically looking for.</p><p>Let&apos;s just say that specialist data can be kryptonite to a general-purpose image classification/captioning model. AI models aren&#x2019;t (and can&#x2019;t be) all things to all people, after all.</p><p>When you think of the data used to train general-purpose image models, very little would be aerial views of the landscape taken from satellites and labels for such data (<code>road</code>, <code>residential</code>, etc.) would far more commonly be applied to pictures of those phenomena taken from a more human-centric angle.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png" width="1125" height="750" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--45-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--45-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png 1125w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10-1.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Dense residential (L: How it&apos;s represented in most models, R: how it&apos;s represented in UC Merced dataset)</span></p></figcaption></figure><h2 id="why-not-use-scenexplain">Why NOT use SceneXplain?</h2><p>While using dedicated models has its downsides, there are some advantages:</p><h3 id="you-only-care-about-chaparrals">You only care about chaparrals</h3><p>Good for you buddy. Good for you.</p><h3 id="scenexplain-isnt-as-accurate-as-pre-trained-models">SceneXplain isn&apos;t as accurate as pre-trained models</h3><p>While a pain to set up, pre-trained models offer superior accuracy when it comes to this very specific domain. Those models were trained specifically on aerial imagery and thus offer accuracy rates of 85-95% when using high-level methods (see final table in the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset paper</a>), compared to SceneXplain&apos;s accuracy of about 60%.</p><h2 id="next-steps">Next steps</h2><p>Large language models and image-to-text models aren&#x2019;t specially trained to recognize landscape types from satellite imagery. There&#x2019;s enough in their training data to make a stab at it, and it&#x2019;s not totally useless, but zero-shot detection is just not good. Some categories are (by their nature) going to be tough to recognize, even for a human. If we can&#x2019;t tell from 30,000 feet how many people live on a city block, then how could we expect SceneXplain to tell <code>sparseresidential</code> from <code>mediumresidential</code>?</p><p>Specially trained and fine-tuned models perform well enough to be a hard bar to beat.</p><p>But often you hit upon a problem that <strong>doesn&#x2019;t</strong> have a specialized dataset that can be used for training a dedicated model. That means that a general-purpose tool, like SceneXplain, may be your best bet. Our next steps are trying to see how much better we can make SceneX with the least effort, so that our users can always get the best performance possible for the least added effort.</p><p>Even if no one ever uses SceneXplain to catalog land use, learning to make it better at this task teaches us how to make it better for something else that it&#x2019;s never specially learned to do.</p><p>To test out SceneXplain for our own use case, create a free account and start playing! Let us know how you&apos;re doing on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings 2: The Best Solution for Embedding Long Documents]]></title><description><![CDATA[With Jina Embeddings 2 models you get high-quality embeddings from an open-source, downloadable model with an input size of 8,192 tokens.]]></description><link>https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/</link><guid isPermaLink="false">65425e91d9dd650001cdb1b6</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Thu, 02 Nov 2023 15:00:06 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Blog-images--7-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Blog-images--7-.jpg" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><p>Text embeddings are the backbone of AI language processing, powering text clustering, information retrieval, sentiment analysis, text-to-image processing, and information extraction, among other core tasks. But there&#x2019;s a catch: Until now, text embedding models have been trained for very short text segments, typically a few hundred tokens at most. <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">Jina Embeddings 1 models</a>, based on the <a href="https://huggingface.co/docs/transformers/model_doc/t5?ref=jina-ai-gmbh.ghost.io">T5 models</a>, are limited to 512 tokens. Tokens don&#x2019;t <em>quite</em> match up to words one-to-one, but this means the largest texts that they can support are just a few short paragraphs.</p><p>But not anymore!</p><p>With <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings 2 models</a>, you can get high-quality embeddings from an open-source, Apache 2-licensed model with an input size of 8,192 tokens &#x2014; more than sixteen times as much as Jina Embeddings 1 and the widely used <a href="https://www.sbert.net/?ref=jina-ai-gmbh.ghost.io">SBERT</a> models!</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class="kg-bookmark-description">Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, e&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Michael G&#xFC;nther</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"></div></a><figcaption><p><span style="white-space: pre-wrap;">Read the Jina Embeddings 2 paper on arXiv</span></p></figcaption></figure><p>Few other embedding models offer input sizes exceeding 512 tokens, and at present, the <em>only </em>one that accepts over 8,000 tokens is neither open-source nor available for download. Jina Embeddings 2 models not only give you a larger input size than all other open-source models, but they also rival the performance of the closed-source alternative.</p><p>Check for yourself on the <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">HuggingFace MTEB leaderboard</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">MTEB Leaderboard - a Hugging Face Space by mteb</div><div class="kg-bookmark-description">Discover amazing ML apps made by the community</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"><span class="kg-bookmark-author">a Hugging Face Space by mteb</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/spaces/mteb/leaderboard.png" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents"></div></a></figure><h2 id="how-jina-embeddings-2-supports-long-documents">How Jina Embeddings 2 Supports Long Documents</h2><p>Jina AI has implemented the <a href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io">ALiBi approach</a>, the first embedding model to do so. ALiBi is an alternative to the positional encoding system first proposed in the famous &#x201C;<a href="https://dl.acm.org/doi/10.5555/3295222.3295349?ref=jina-ai-gmbh.ghost.io">Attention is all you need</a>&#x201D; paper. This scheme makes it possible to train embedding models on short texts and still get high-quality results on longer texts.</p><p>For Jina Embeddings 2, Jina AI has taken the BERT architecture and grafted it into ALiBi.</p><p>ALiBi was originally proposed for text generators (like ChatGPT) that only care about dependencies on previous words. This means that the last tokens in the input matter more than earlier ones because a text generator&#x2019;s task is to produce the next word after the input. This makes no sense for an embedding model that tries to create a representation of the whole text input, so Jina Embeddings 2 models implement a bi-directional version of ALiBi.</p><p>This architectural change, however, means that we can&#x2019;t use the pre-training of the original BERT model and have to completely retrain the model from scratch.</p><h2 id="english-only-for-now-german-and-chinese-soon">English-Only for Now, German and Chinese Soon</h2><p>Although there are plenty of multilingual text embedding models out there, there are known issues with providing universal language support. Not all languages are equal when it comes to embedding quality, and <a href="https://aclanthology.org/2023.sigtyp-1.16.pdf?ref=jina-ai-gmbh.ghost.io">recent research</a> shows that models tend to be biased towards structures that parallel English ones, distorting embeddings.</p><p>In short, multilingual models have an &#x201C;accent&#x201D;, usually an English one due to the majority of English data in the training.</p><p>For superior language-specific performance, and more compact, easier-to-train models, Jina Embeddings are currently English-only. We&#x2019;re planning German and Chinese models for the near future.</p><h2 id="training-jina-embeddings-2-for-top-performance-up-to-8192-tokens">Training Jina Embeddings 2 for Top Performance up to 8,192 Tokens</h2><p>Using ALiBi means that even though Jina Embeddings 2 models support larger input sizes, the models don&#x2019;t have to be trained with larger input data. The training learned for short sequences scales up to larger ones automatically.</p><p>Our training is similar to the way other embedding models are trained. We start with <a href="https://aclanthology.org/N19-1423.pdf?ref=jina-ai-gmbh.ghost.io">masked word pre-training</a> using the circa 170 billion word English-only <a href="https://huggingface.co/datasets/c4?ref=jina-ai-gmbh.ghost.io">C4 dataset</a>. Then, we do <a href="https://arxiv.org/abs/1607.08085?ref=jina-ai-gmbh.ghost.io">pairwise contrastive training</a>. This means taking pairs of texts that are known to be similar or dissimilar and adjusting the weights of the embedding model so that similar inputs are closer together, and dissimilar ones are farther apart. We used a new corpus of text pairs, curated by Jina AI, based on the one <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">used to train the Jina Embeddings 1 models</a>.</p><p>Finally, we fine-tuned the model using <a href="https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io">text triplets and negative mining</a>, with an in-house training dataset specially augmented with sentences of opposite grammatical polarity. Embedding models have typically had trouble with negative polarity sentences: A sentence like &#x201C;The dog is in the car&#x201D; will often have an embedding close to &#x201C;The dog is outside the car,&#x201D; even though these are naturally opposite in meaning.</p><p>We added a collection of positive and negative pairs like this to the training data, using the same methods <a href="https://arxiv.org/abs/2307.11224?ref=jina-ai-gmbh.ghost.io">employed for the Jina Embeddings 1 models</a> to specifically improve performance on this kind of language.</p><h2 id="three-models-to-better-fit-your-use-case">Three Models To Better Fit Your Use Case</h2><p>The Jina Embeddings 2 models come in three sizes, providing high-quality embeddings for users with different requirements and capabilities. All three support 8,192 input tokens.</p><ul><li><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a>: 33 million parameters, 512-dimension embeddings.</li><li><a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en?ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><code>jina-embeddings-v2-base-en</code></a>: 137 million parameters, 768-dimension embeddings.</li><li><code>jina-embeddings-v2-large-en</code>: 435 million parameters, 1,024-dimension embeddings.</li></ul><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1296" height="946" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image.png 1296w" sizes="(min-width: 720px) 720px"></figure><p>The <code>jina-embeddings-v2-large-en</code> model is not yet available to download but will be released in the immediate future.</p><h2 id="bigger-inputs-leaner-models-peak-performance">Bigger Inputs, Leaner Models, Peak Performance</h2><p>We tested the Jina Embeddings 2 models against the <a href="https://arxiv.org/abs/2210.07316?ref=jina-ai-gmbh.ghost.io">MTEB benchmark suite</a> and at the time of writing:</p><ul><li><code>jina-embeddings-v2-base-en</code> scores roughly on par with the best models on most benchmarks, and generally better than similarly sized ones.</li><li><code>jina-embeddings-v2-small-en</code> ranks near the top for models with sizes under 100MB.</li></ul><p>The <code>jina-embeddings-v2-large-en</code> model is not yet available for testing.</p><p>However, among models that take more than 512 tokens in input, there is only one that compares to the Jina Embeddings 2 models: OpenAI&#x2019;s <code>text-embedding-ada-002</code>. This model is not publicly available and can only be accessed via a paid web-based API. Its size is unknown.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-14.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1184" height="375" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-14.png 1184w"><figcaption><span style="white-space: pre-wrap;">jina-embeddings-v2-base-en compared with OpenAI&#x2019;s text-embedding-ada-002 on the English MTEB benchmark</span></figcaption></figure><p><code>jina-embeddings-v2-base-en</code> is comparable in performance with <code>text-embedding-ada-002</code> on all benchmarks and even exceeds it in several tasks. Furthermore, the Jina Embeddings 2 models all produce smaller embedding vectors than <code>text-embedding-ada-002</code> which produces 1,536-dimensional output, compared to 512, 768, and 1,024 dimensions for our three model sizes respectively. This means considerable savings in computing time and memory for applications. Storing shorter vectors takes less space in memory and storage, speeds up database retrieval, and calculating the distance between them is proportionately fast.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-15.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1204" height="376" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-15.png 1204w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings 2 compared to other leading AI embedding models</span></figcaption></figure><p>Jina AI&#x2019;s base and small model compared to other leading embedding models</p><p>Furthermore, <code>jina-embeddings-v2-small-en</code> is the only model under 100MB that supports more than 512 input tokens.</p><p>Even if we set aside support for larger inputs, Jina Embeddings offers performance on par with the most common embedding foundation models, while remaining roughly the same size or even significantly smaller.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-16.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1000" height="433" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-16.png 1000w"></figure><p>This gives Jina Embeddings 2 models an impressive advantage in terms of accessibility, economic use of computing power, and cost to users.</p><h2 id="integration">Integration</h2><p>Jina Embeddings 2 models are already integrated into:</p><ul><li><a href="https://github.com/huggingface/text-embeddings-inference?ref=jina-ai-gmbh.ghost.io#supported-models">HuggingFace Text Embeddings Inference API</a></li><li><a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/jina_embeddings.ipynb?ref=jina-ai-gmbh.ghost.io">LlamaIndex</a></li><li><a href="https://twitter.com/xenovacom/status/1717904546481992094?ref=jina-ai-gmbh.ghost.io">Transformers.js</a></li><li><a href="https://github.com/simonw/llm-embed-jina?ref=jina-ai-gmbh.ghost.io">LLM</a></li></ul><p>More integrations are coming soon, both from Jina AI and our open-source user community.</p><h2 id="try-it-out-yourself-right-now">Try It Out Yourself Right Now</h2><p>Go to <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io#enterprises">https://jina.ai/#enterprises</a> to get an access key for Jina AI&apos;s online Embedding API with ten thousand free tokens for you to embed. You&apos;ll find the key on the upper right:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-4.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1289" height="712" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-4.png 1289w" sizes="(min-width: 720px) 720px"></figure><p>You can access Jina Embeddings 2 models hosted at Jina AI via any standard HTTP interface. Use the drop-down menus to select between the <code>jina-embeddings-v2-base-en</code> and <code>jina-embeddings-v2-small-en</code> models, and to get code snippets to use the Embeddings API.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-6.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1309" height="737" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-6.png 1309w" sizes="(min-width: 720px) 720px"></figure><p>You can also get example code snippets in a variety of languages and frameworks to help integrate Jina Embeddings directly into your project:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-7.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1307" height="972" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-7.png 1307w" sizes="(min-width: 720px) 720px"></figure><p>And if you need to add tokens to your API key, just click the &quot;Top Up&quot; tab.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-8.png" class="kg-image" alt="Jina Embeddings 2: The Best Solution for Embedding Long Documents" loading="lazy" width="1281" height="1155" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-8.png 1281w" sizes="(min-width: 720px) 720px"></figure><h2 id="future-jina-embeddings-models">Future Jina Embeddings Models</h2><p>Jina AI will be rolling out a larger model, <code>jina-embeddings-v2-large-en</code>, in the immediate future, which we expect to exceed the performance of the other Jina Embeddings 2 models and compete with the highest-scoring large embedding models.</p><p>In the near future, we are expanding our offerings to include German and Chinese embedding models, which we expect will match or exceed state-of-the-art performance in both languages.</p><p>These benchmark results prove the robustness of Jina AI&#x2019;s contrastive training methodology, and we are always investigating improved model design and AI techniques. Our mission is to put more intelligence and higher performance into more compact open-access models that you can affordably access via our API, or easily run on your hardware and in your cloud instances.</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.4 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-4-update/</link><guid isPermaLink="false">65411268d9dd650001cdb18b</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Tue, 31 Oct 2023 14:45:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-3.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3224">Release Note (<code>3.22.4</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-3.jpg" alt="Jina 3.22.4 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.4?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.4 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.4) Release time: 2023-10-31 14:33:21 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Add option to use body for streaming instead of params (#6098)
In order to allow complex neste&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.22.4 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/39a4535a750f8cf9c61cd33bd367faa5bb713422cddd112641a36c1a99333c4f/jina-ai/jina/releases/tag/v3.22.4" alt="Jina 3.22.4 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="add-option-to-use-body-for-streaming-instead-of-params-6098">Add option to use body for streaming instead of params (<a href="https://github.com/jina-ai/jina/pull/6098?ref=jina-ai-gmbh.ghost.io">#6098</a>)</h3><p>In order to allow complex nested documents to pass through streaming endpoints, the&#xA0;<code>GET</code>&#xA0;endpoint can now get the data from the body or the parameters of the HTTP request.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain]]></title><description><![CDATA[See how SceneXplain enhanced search quality, and enriched user experience for a top European e-commerce platform.]]></description><link>https://jina.ai/news/case-study-revolutionizing-e-commerce-user-experience-and-streamlining-search-with-scenexplain/</link><guid isPermaLink="false">6537c300dcdd090001c7d12b</guid><category><![CDATA[Knowledge Base]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Mon, 30 Oct 2023 13:00:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/2.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/2.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><p></p><h2 id="client-overview">Client Overview</h2><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-27-at-10.58.59.png" class="kg-image" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain" loading="lazy" width="1295" height="189" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-2023-10-27-at-10.58.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-2023-10-27-at-10.58.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-27-at-10.58.59.png 1295w" sizes="(min-width: 720px) 720px"></figure><p>Our client, a leading European e-commerce platform, specializing in fashion and lifestyle, offers a wide range of clothing, footwear, and accessories. Their platform features an extensive selection of emerging and well-known brands, with a total of around two million product images, spanning all their product categories. </p><p><strong>The client&apos;s vision was clear</strong>: automatically generate basic and specific descriptions for all these images and, more importantly, categorize them with detailed tags to<em> improve user experience and enhance search result quality.</em></p><h2 id="why-they-started-looking-for-a-new-solution">Why They Started Looking For A New Solution</h2><p></p><blockquote>&quot;We were facing significant challenges in managing our vast catalog of product images. Manual image description generation was too time-consuming. We needed a solution to save time and resources and provide comprehensive product tagging. SceneXplain was the game-changer that helped us achieve this.&quot; - Sales Director</blockquote><p><strong>1. Efficiently Generating Descriptions</strong>: With two million images to manage, creating descriptions for each one manually was a time&#x2013;consuming and inefficient task. It took roughly a week for them to write descriptions for the first seven thousand images. The client sought an automated solution to save valuable time and resources.</p><p><strong>2. Better Product Tagging</strong>: They wanted a comprehensive tagging system that went beyond general descriptions. They needed to categorize products by attributes such as color, gender, and more, all while maintaining a consistent schema for easy management.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try SceneXplain Now</span></p></figcaption></figure><h2 id="how-scenexplains-api-saved-time-and-effort">How SceneXplain&apos;s API Saved Time And Effort </h2><p>SceneXplain addressed these challenges with a two-fold strategy:</p><p><strong>1. Automated Image Description</strong>: SceneXplain leveraged its powerful API to enable batch image uploads, eliminating the need to process each image one by one. Using SceneXplain&#x2019;s API, the client achieved high-quality image descriptions, reducing the time and effort required to manually craft descriptions. When they usually spent over two weeks on getting the description of 14k of their images, now with batch upload that number decreased by 75%.</p><p><strong>2. Comprehensive Tagging with JSON Schema</strong>: SceneXplain&#x2019;s JSON schema system enabled the client to automatically categorize images with a fine-grained tag scheme optimized for their product search system. Using SceneXplain, their tagging system progressed from a few categories to a multifaceted system with highly detailed descriptive labels, making it easy to manage and optimize their product listings</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Case Study: Revolutionizing E-Commerce User Experience And Streamlining Search With SceneXplain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try SceneXplain Now</span></p></figcaption></figure><h2 id="conclusion"><strong>Conclusion</strong></h2><p>At SceneXplain, we&apos;re on a mission to empower businesses with innovative solutions that enhance user experience.</p><p>We&apos;re dedicated to delivering results and exceeding expectations, and we&apos;re proud to have earned high satisfaction and recognition from our clients.</p><p>Join us on this journey to make technology and digital content accessible to all!</p>]]></content:encoded></item><item><title><![CDATA[How to Caption Image Segments with SceneXplain: Introducing Crop'n Explain]]></title><description><![CDATA[SceneXplain: Precise image insights, webcam uploads, multi-API keys, and more user-friendly updates.]]></description><link>https://jina.ai/news/how-to-caption-image-segments-with-scenexplain-introducing-cropn-explain/</link><guid isPermaLink="false">65391d7fdcdd090001c7d24b</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Miruna Nedelcu]]></dc:creator><pubDate>Fri, 27 Oct 2023 14:00:37 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--14-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--14-.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><p>We&apos;re excited to introduce some exciting updates to SceneXplain 0.16.5, your reliable platform for uncovering the hidden stories within your images.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try The New Features Now</span></p></figcaption></figure><p>Let&apos;s dive right in and explore these improvements!</p><h2 id="focus-on-what-matters-most-in-your-images-with-cropn-explain">Focus On What Matters Most in Your Images with Crop&apos;n Explain</h2><p>With Crop&apos;n Explain, you can extract insights and descriptions for the elements that most matter to you. </p><p>It&apos;s as simple as uploading an image and selecting your area of interest. Our advanced computer vision algorithm then springs into action, automatically generating accurate and detailed captions for it. </p><p>Whether you&apos;re a content creator, media professional, SEO expert, or part of an e-commerce enterprise, Crop&apos;n Explain provides the ultimate tool for making sense of the visual world.</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost.mp4" poster="https://img.spacergif.org/v1/1924x1080/0a/spacer.png" width="1924" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:41</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Step-by-Step Guide</span></p></figcaption>
        </figure><p><strong>Beyond Captioning: Advanced Visual Insights</strong></p><p>Crop&apos;n Explain also offers a range of advanced visual comprehension tasks, enabling you to dive deeper into your images. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-2023-10-26-at-17.00.21.png" class="kg-image" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain" loading="lazy" width="2000" height="1083" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-2023-10-26-at-17.00.21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-2023-10-26-at-17.00.21.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Screenshot-2023-10-26-at-17.00.21.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/10/Screenshot-2023-10-26-at-17.00.21.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Crop&apos;n Explain</span></figcaption></figure><p>From Visual Question Answering to JSON output and image captioning, you&apos;ll have the tools you need to gain a comprehensive understanding of your visuals. </p><h2 id="capturing-moments-is-now-more-convenient-than-ever">Capturing Moments Is Now More Convenient Than Ever</h2><p>Simplify image uploads with our new webcam integration feature on SceneXplain. </p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/Blogpost-Miruna_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:15</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Capture Moments Instantly In Action</span></p></figcaption>
        </figure><p>Instead of the traditional method of uploading images from your local file system, you can now directly capture and upload photos using your device&apos;s camera, whether you&apos;re using your desktop computer, laptop or phone.<br><br>This eliminates the need for traditional image file uploads from your local file system!</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="How to Caption Image Segments with SceneXplain: Introducing Crop&apos;n Explain"></div></a><figcaption><p><span style="white-space: pre-wrap;">Try The New Features Now</span></p></figcaption></figure><h2 id="streamline-team-collaboration-with-multiple-api-keys">Streamline Team Collaboration with Multiple API Keys</h2><p>In a collaborative work environment, the multi-API key function in SceneXplain offers significant advantages. It allows each team member to have their own dedicated API key, promoting individualized access and facilitating task management. </p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key.mp4" poster="https://img.spacergif.org/v1/1924x1080/0a/spacer.png" width="1924" height="1080" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/10/API-Key_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:20</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">See Multi API Key Management In Action</span></p></figcaption>
        </figure><p>Whether your focus is on image captioning or any other project aspect, these unique API keys offer flexibility and improve workflow coordination.</p><p>Stay tuned for more updates, your feedback continually drives our commitment to improving SceneXplain. </p><p>Happy exploring! </p>]]></content:encoded></item><item><title><![CDATA[Are you ready for this Jelly? SceneXplain’s new algo kills hallucinations dead]]></title><description><![CDATA[SceneXplain's state-of-the-art Jelly algorithm is more concise, readable and accurate than ever before, while killing hallucinations.]]></description><link>https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead/</link><guid isPermaLink="false">65366f5e6930100001517b53</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Wed, 25 Oct 2023 14:00:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><p>Another month rolls by, and here at Jina AI we&#x2019;re releasing a new, more refined algorithm for <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">SceneXplain</a>.</p><p>A while back, we introduced SceneXplain&#x2019;s &#x201C;Glide&#x201D; algorithm, which nailed recognizing multilingual text in images, something other models are now only just introducing:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--1-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="447" height="844"></figure><p></p><p>But that was then. This is now. So, what comes next? Fixing hallucination, that&#x2019;s what.</p><h2 id="what-is-hallucination-in-computer-vision">What is hallucination in computer vision?</h2><p>You may already be familiar with AI hallucination when it comes to LLMs, for example, <a href="https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html?ref=jina-ai-gmbh.ghost.io">ChatGPT hallucinating papers and sources that don&#x2019;t exist in reality</a>.</p><blockquote class="kg-blockquote-alt"><strong>&quot;[ChatGPT is an] omniscient, eager-to-please intern who sometimes lies to you&quot;</strong><br>Professor Ethan Mollick of <a href="https://en.wikipedia.org/wiki/Wharton_School_of_the_University_of_Pennsylvania?ref=jina-ai-gmbh.ghost.io">Wharton</a></blockquote><p>Likewise, image generation models (like Midjourney) often hallucinate unwanted details (or unwanted fingers for that matter.)</p><p><a href="https://gagadget.com/en/how-it-works/210102-the-unattainable-pinnacle-of-art-why-does-midjourney-artificial-intelligence-draw-6-fingers-on-hands-and-how/?ref=jina-ai-gmbh.ghost.io">The unattainable pinnacle of art: why does Midjourney artificial intelligence draw 6 fingers on hands and how can it be fixed?</a></p><p>In short, hallucination is AI models generating false information.</p><p>SceneXplain often fell victim to this in our prior algorithms, hallucinating skateboarders, and occasionally Naruto:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--2-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="640" height="380" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--2-.png 640w"><figcaption><span style="white-space: pre-wrap;">SceneXplain&#x2019;s Aqua algorithm: In a whimsical and enchanting scene, Totoro and a young girl share an umbrella in the rain. The iconic character from Studio Ghibli&apos;s beloved film stands alongside the girl, offering shelter from the downpour. The atmosphere is both heartwarming and magical, as Totoro&apos;s presence infuses the moment with a sense of wonder and delight. Meanwhile, </span><b><strong style="white-space: pre-wrap;">Naruto makes a surprising appearance in the rain-soaked setting</strong></b><span style="white-space: pre-wrap;">, adding another layer of intrigue to this captivating tableau. This charming depiction brings together two distinct worlds, bridging the gap between Japanese animation and manga to create an unforgettable moment of friendship and adventure.</span></figcaption></figure><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--3-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="446" height="612"></figure><h2 id="what-causes-hallucination-in-computer-vision">What causes hallucination in computer vision?</h2><p>In humans, hallucination is caused by errors in perception, faulty expectations, neurological defects, or maybe just a dose of Scarecrow&#x2019;s fear gas. In computer vision, it can often come about through issues with training data or the way an image is processed before generation or (in the case of SceneXplain) captioning.</p><h3 id="slicing-and-dicing-the-elephant-in-the-room">Slicing and dicing: the elephant in the room</h3><p>When it comes to image captioning, you often want a high level of detail. A common approach is to slice an image into several pieces, generate a caption for each piece, then merge the captions together into a final description. Sounds like a pretty sensible approach, right? By focusing on each segment and then combining results, we should get a better description overall.</p><p>Except...you ever heard the tale of the blind men and the elephant?</p><blockquote>Six blind men encounter an elephant for the first time and each touches a different part of the animal. One feels the trunk and thinks it&apos;s a snake, another touches a leg and believes it&apos;s a tree, and so on with the tusk (a spear), ear (a mat), tail (a rope), and side (a wall). Amidst much debate, they all insist they&apos;re correct, not realizing they&apos;re all describing the same elephant.</blockquote><p>If we want to understand an image by slicing it into different parts we can face exactly the same issue. If we were to draw what the blind men experience, we&apos;d create a picture of a snake wrapped in a blanket next to a tree, along with a bunch of other elements. In short, many <em>elements</em>, but no <em>elephant</em>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-2.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="1170" height="656" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-2.png 1170w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Midjourney&apos;s interpretation of the blind men&apos;s elephant</span></figcaption></figure><p>Likewise, in captioning, let&apos;s say we split up one image of three men into several segments. In each segment, there may be some part (arm, leg, side of head) of a different man. When it comes time to combine those segment descriptions (<code>a man&apos;s face</code>, <code>a man&apos;s leg</code>, <code>a man&apos;s arm</code>, etc), how many men would be seen? Are these all part of one man? Are they each a part of different men? If so, how many? And without the context of the rest of the image, just what is that patch of fuzz in the top-left section?</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/man-grid.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="600" height="853" srcset="https://jina-ai-gmbh.ghost.io/content/images/2023/10/man-grid.png 600w"><figcaption><span style="white-space: pre-wrap;">Adapted from Pexels.com (</span><a href="https://www.pexels.com/photo/man-in-white-dress-shirt-holding-suit-jacket-1043474/?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">https://www.pexels.com/photo/man-in-white-dress-shirt-holding-suit-jacket-1043474/</span></a><span style="white-space: pre-wrap;">)</span></figcaption></figure><p>If the model splits everything up too much, it can hallucinate things that just aren&apos;t there. If it doesn&apos;t split things, the overall description may be too vague to be useful.</p><h3 id="ambiguity-thats-a-cutepuppy">Ambiguity: That&apos;s a cute...puppy?</h3><p>Many models rely on multimodality - that is translating text-to-image (like using a prompt to generate an image in Midjourney) or image-to-text (like generating a caption from an image in SceneXplain). Ambiguous text or images can therefore cause problems.</p><p>Take the phrase &quot;salmon swimming in a river.&quot; Most humans would imagine fish swimming up a river. AI on the other hand can see some ambiguity:</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://www.reddit.com/r/technicallythetruth/comments/y26t6x/ai_asked_to_draw_salmon_swimming_in_river/?ref=jina-ai-gmbh.ghost.io"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-1.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="750" height="426" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/image-1.png 750w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Via Reddit</span></figcaption></figure><p>Likewise, sometimes images can be ambiguous:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--7-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--7-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Untitled--7-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Untitled--7-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--7-.png 2048w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Duck or rabbit?</span></figcaption></figure><p>This isn&apos;t just applicable to traditional optical illusions or Rorschach tests. A <a href="https://www.insider.com/viral-puppy-looks-like-mix-between-dog-and-cat-2020-3?ref=jina-ai-gmbh.ghost.io">cute little fuzzball</a> could be a puppy OR a kitten. Running it by the model just once may result in a misclassification. Several tries may be needed to consistently see that puppy as a puppy.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--8-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="700" height="525" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--8-.png 700w"><figcaption><span style="white-space: pre-wrap;">Dog or cat? (Via Business Insider)</span></figcaption></figure><p>And let&#x2019;s not even get started on <a href="https://en.m.wikipedia.org/wiki/The_dress?ref=jina-ai-gmbh.ghost.io">that damn dress</a>.</p><h3 id="ocr-occasionally-crappy-recognition">OCR: Occasionally Crappy Recognition</h3><p>If you&apos;ve got this far in the article, you probably think you&apos;re a pretty hot reader. Go on, give yourself a big pat on the back. You should really share some of your knowledge on <a href="http://expertsexchange.com/?ref=jina-ai-gmbh.ghost.io">expertsexchange.com</a>. It&apos;s a site I use every day, sometimes several times. To exchange information with other experts.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">If you don&apos;t see why I&apos;m bringing this up, re-read that URL</div></div><p>It&apos;s not just a lack of clear word spacing that can trip you up. Take keming (uh, kerning) for example.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text"> In typography, kerning is the process of adjusting the spacing between characters in a proportional font, usually to achieve a visually pleasing result.</div></div><p>Bad kerning can have interesting results:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn.mos.cms.futurecdn.net/9cdc770ee6d32d569f4dfa1d7e44029e-1200-80.jpg.webp" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy"><figcaption><span style="white-space: pre-wrap;">10 flickering lights</span></figcaption></figure><p>If these things are ambiguous to readers like you, just imagine how confused an AI model can get!</p><h2 id="how-does-scenexplain-fix-hallucination">How does SceneXplain fix hallucination?</h2><p>SceneXplain&#x2019;s new Jelly algorithm aims to eliminate these hallucinations, using the following methods:</p><ol><li>A simplified processing pipeline. Our prior algorithms had much more complex pipelines, meaning more places for things to go wrong. Jelly pares this back with a new end-to-end approach to image labeling that produces detailed image captions with much fewer hallucinations.</li><li>Self-consistency: We&#x2019;ve implemented an advanced prompt technique (self-consistency) to get the most consistent and accurate caption for any given image.</li><li>OCR improvements: We&apos;ve improved Jelly&#x2019;s text recognition capabilities which leads to fewer hallucinations and better captions.</li></ol><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Hallucination is seeing things that don&#x2019;t exist. Just like people &#x201C;see&#x201D; ghosts. Since Jelly removes hallucinations you could call it the ultimate &#x201C;Ghostbuster&#x201D;. Cue the theme music.</div></div><h2 id="before-and-after">Before and After</h2><p>As you can see from the examples below:</p><ul><li>Glide suffers from way more hallucinations (highlighted in bold) than Jelly</li><li>Jelly provides more readable captions, lacking the elaborate flowery language of Glide</li></ul><h3 id="les-choristes">Les Choristes</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--4-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="764" height="1024" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--4-.png 764w" sizes="(min-width: 720px) 720px"></figure><p><strong>Glide:</strong></p><blockquote>The image showcases a poster for a movie called &quot;Les Choristes.&quot; The poster features the title &quot;LES CHORISTES&quot; prominently displayed in the center of the image. The names of the actors and director, including &quot;GERARD JUGNOT,&quot; &quot;FRAN&#xC7;OIS BERLEAND,&quot; &quot;KAD,&quot; and &quot;CHRISTOPHE BARRATIER,&quot; are listed below the title.</blockquote><blockquote>The poster shows a group of people posing for a picture in various settings. On the left side of the image, a group of people in tuxedos can be seen standing in front of a tree. In the middle of the image, a group of children sits on a bench with a chalkboard. To the right of the children, <strong>another group of children is sitting in a classroom</strong>. They are separated by a group of boys who are sitting on a bench. Further to the right, a group of people is posing for another picture. <strong>Above all these scenes, there is a group of children sitting on a stage</strong>.</blockquote><blockquote><strong>The color scheme of the poster is predominantly white</strong>, with the title and texts in black. The actors&apos; names are in smaller font compared to the title. The overall appearance suggests a heartwarming and collaborative movie involving children and adults in different settings, possibly related to music or education.</blockquote><blockquote>Additional details, such as the ratings &quot;9.8&quot; and &quot;27.7.761,&quot; are included at the bottom right corner of the poster. The meaning of these numbers is unclear without further context.</blockquote><p><strong>Jelly:</strong></p><blockquote>The image is a movie poster for the French film &quot;Les Choristes&quot;, directed by Christophe Barratier. The movie&apos;s title is printed in large letters under an archway. Beneath this are young boys and men, some sitting on a bench, and others standing. They are dressed in classic school-like clothing and are positioned at different heights, perhaps indicating different ages. In the lower corners of the poster, the names of actors G&#xE9;rard Jugnot, Fran&#xE7;ois Berl&#xE9;and, and Kad are printed. The poster has a vintage feel, possibly due to its color scheme and the classic clothing of the characters.</blockquote><h3 id="liberty-leading-the-people">Liberty leading the people</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--5-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="464" height="367"></figure><p><strong>Glide:</strong></p><blockquote>The artwork depicts a scene with a strong patriotic theme. The central focus of the painting showcases a woman holding the French flag, her figure dominating the composition. Her proud stance and the vibrant colors of the flag capture the viewer&apos;s attention. Surrounding her, several other characters are depicted, each with their own distinct role. A man is shown holding a rifle, signifying the presence of armed conflict. In another section, <strong>a group of men is seen together on a boat</strong>, suggesting a collective effort or journey. Additionally, <strong>a man on a separate boat is featured</strong>, adding further depth to the narrative. <strong>A woman with a knife and a man with a gun are shown</strong>, reflecting a sense of danger and potential violence. The artwork overall conveys a sense of patriotism, conflict, and tension through its composition and subject matter.</blockquote><p><strong>Jelly:</strong></p><blockquote>This image is a painting titled &quot;Liberty Leading the People&quot;, created in 1830 by the French artist Eug&#xE8;ne Delacroix. The painting depicts a woman symbolizing France and the revolution, standing on the bodies of prostrate figures signifying defeated enemies. She is dressed in a <strong>blue</strong> dress, holding a French flag in her left hand, symbolizing the French republic and democracy. In her right hand, she is likely holding a weapon, possibly a sword. The painting represents a moment from the French Revolution, showcasing the spirit of the people resisting tyranny.</blockquote><h3 id="emojis">Emojis</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--6-.png" class="kg-image" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead" loading="lazy" width="1024" height="537" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Untitled--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Untitled--6-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Untitled--6-.png 1024w" sizes="(min-width: 720px) 720px"></figure><p><strong>Glide:</strong></p><blockquote>A playful scene unfolds on a vibrant green grassy expanse, as a group of colorful emoji balls gathers together. These enchanting characters fill the frame, showcasing a wide array of expressive emotions and witty designs. In the center, a striking <strong>red and yellow ball</strong> stands out with its angry face emblazoned upon its surface. Positioned nearby, lying comfortably on the grass, two yellow emoji balls exude a relaxed demeanor. To the right, a buoyant <strong>blue and yellow ball</strong> features a cheerful smiley face that adds a touch of delight and cheerfulness to the mix. On the left, <strong>two</strong> yellow balls adorned with sunglasses mirror each other&apos;s fashionable style while resting peacefully in the grass. Amidst the lively atmosphere, a <strong>yellow and blue plush toy makes its presence known with a sad expression</strong>, attracting sympathy and affection. Additionally, a yellow and red emoji doll captivates attention with a heartfelt kiss imprinted upon its face. Further accentuating this whimsical scene, a lone yellow face with big expressive eyes captures the viewer&apos;s gaze, radiating a sense of curiosity and intrigue. Lastly, <strong>a yellow teddy bear</strong> wearing sunglasses and a friendly smile completes the array of delightful characters, inviting joy and warmth into the playful world they inhabit.</blockquote><p><strong>Jelly:</strong></p><blockquote>The image shows a group of six stress balls with different emoji faces on them, situated on a green grassy surface. Each stress ball displays a unique emoji expression and they come in varied colors including yellow, red, and blue. The background of the image is blurred, ensuring the focus is on these stress balls.</blockquote><p>Want to see more examples of how SceneXplain algorithms stack up? Check them out here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/benchmark?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"></div></a></figure><h2 id="get-your-jelly-on">Get your Jelly on</h2><p>As you can see, Jelly gives you captions that are more concise, readable, and <strong>accurate</strong> than ever before. Wave goodbye to hallucinations and say hello to detailed, precise descriptions of your images.</p><p>To get started with Jelly (or any other SceneXplain algorithm), sign up for a free account at <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">scenex.jina.ai</a> and start captioning your images!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Are you ready for this Jelly? SceneXplain&#x2019;s new algo kills hallucinations dead"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina 3.22.3 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-22-3-update/</link><guid isPermaLink="false">65390517dcdd090001c7d210</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Wed, 25 Oct 2023 12:10:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-2.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3223">Release Note (<code>3.22.3</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-Jina-dark-2.jpg" alt="Jina 3.22.3 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.22.3?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.22.3 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.22.3) Release time: 2023-10-25 12:03:58 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Add and use POST endpoint for streaming (#6093)
HTTP streaming when using a GET endpoint only&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.22.3 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/bbf622e38936bd97688a794004636257a997cf3b42c4e080c2308a580498fffe/jina-ai/jina/releases/tag/v3.22.3" alt="Jina 3.22.3 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="add-and-use-post-endpoint-for-streaming-6093">Add and use POST endpoint for streaming (<a href="https://github.com/jina-ai/jina/pull/6093?ref=jina-ai-gmbh.ghost.io">#6093</a>)</h3><p>HTTP streaming when using a&#xA0;<code>GET</code>&#xA0;endpoint only allows Documents whose schemas contain only&#xA0;<code>str</code>,&#xA0;<code>int</code>, and&#xA0;<code>float</code>&#xA0;fields.</p><p>By adding a&#xA0;<code>POST</code>&#xA0;endpoint and modifying the Jina Client to use it, it is now possible to send Documents with complex nested document schemas over streaming HTTP.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Peter Willemsen (<a href="https://github.com/peterwilli?ref=jina-ai-gmbh.ghost.io">@peterwilli</a>&#xA0;)</li><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[Jina AI Launches World's First Open-Source 8K Text Embedding, Rivaling OpenAI]]></title><description><![CDATA[Jina AI introduces jina-embeddings-v2, the world's first open-source model boasting an 8K context length. Matching the prowess of OpenAI's proprietary models, this innovation is now publicly accessible on Huggingface, signaling a significant milestone in the landscape of text embeddings.]]></description><link>https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/</link><guid isPermaLink="false">6538999ddcdd090001c7d17f</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Wed, 25 Oct 2023 05:43:17 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Explore-image-storytelling-beyond-pixels--11-.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"><p><strong>Berlin, Germany - October 25, 2023</strong> &#x2013; Jina AI, the Berlin-based artificial intelligence company, is thrilled to announce the launch of its second-generation text embedding model: <code><strong>jina-embeddings-v2</strong></code>. This cutting-edge model is now the only open-source offering that supports an impressive 8K (8192 tokens) context length, putting it on par with OpenAI&apos;s proprietary model, <code>text-embedding-ada-002</code>, in terms of both capabilities and performance <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">on the Massive Text Embedding Benchmark (MTEB) leaderboard.</a></p><h2 id="benchmarking-against-the-best-8k-model-from-open-ai"><strong>Benchmarking Against the Best 8K Model from Open AI</strong></h2><p>When directly compared with OpenAI&apos;s 8K model <code>text-embedding-ada-002</code>, <code>jina-embeddings-v2</code> showcases its mettle. Below is a performance comparison table, highlighting areas where <code>jina-embeddings-v2</code> particularly excels:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Model Size (GB)</th>
<th>Embedding Dimensions</th>
<th>Sequence Length</th>
<th>Average (56 datasets)</th>
<th>Classification Average (12 datasets)</th>
<th>Reranking Average (4 datasets)</th>
<th>Retrieval Average (15 datasets)</th>
<th>Summarization Average (1 dataset)</th>
</tr>
</thead>
<tbody>
<tr>
<td>15</td>
<td>text-embedding-ada-002</td>
<td>Unknown</td>
<td>1536</td>
<td>8191</td>
<td>60.99</td>
<td>70.93</td>
<td>84.89</td>
<td>56.32</td>
<td>30.8</td>
</tr>
<tr>
<td>17</td>
<td>jina-embeddings-v2-base-en</td>
<td>0.27</td>
<td>768</td>
<td>8192</td>
<td>60.38</td>
<td>73.45</td>
<td>85.38</td>
<td>56.98</td>
<td>31.6</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>Notably, <code>jina-embedding-v2</code> outperforms its OpenAI counterpart in Classification Average, Reranking Average, Retrieval Average, and Summarization Average.</p><h2 id="features-and-benefits"><strong>Features and Benefits</strong></h2><p>Jina AI&#x2019;s dedication to innovation is evident in this latest offering:</p><ul><li><strong>From Scratch to Superiority</strong>: The <code>jina-embeddings-v2</code> was built from the ground up. Over<strong> </strong>the last three months, the team at Jina AI engaged in intensive R&amp;D, data collection, and tuning. The outcome is a model that marks a significant leap from its predecessor.</li><li><strong>Unlocking Extended Context Potential with 8K</strong>: <code>jina-embeddings-v2</code> isn&#x2019;t just a technical feat; its 8K context length opens doors to new industry applications:<ul><li><strong>Legal Document Analysis</strong>: Ensure every detail in extensive legal texts is captured and analyzed.</li><li><strong>Medical Research</strong>: Embed scientific papers holistically for advanced analytics and discovery.</li><li><strong>Literary Analysis</strong>: Dive deep into long-form content, capturing nuanced thematic elements.</li><li><strong>Financial Forecasting</strong>: Attain superior insights from detailed financial reports.</li><li><strong>Conversational AI</strong>: Improve chatbot responses to intricate user queries.</li></ul></li></ul><p>Benchmarking shows that in several datasets, this extended context enabled <code>jina-embeddings-v2</code> to outperform other leading base embedding models, emphasizing the practical advantages of longer context capabilities.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png" class="kg-image" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI" loading="lazy" width="2000" height="647" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png 2359w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Availability</strong>: Both models are freely available for download on Huggingface:<ul><li><strong>Base Model</strong> (0.27G) - Designed for heavy-duty tasks requiring higher accuracy, like academic research or business analytics.</li><li><strong>Small Model</strong> (0.07G) - Crafted for lightweight applications such as mobile apps or devices with limited computing resources.</li></ul></li><li><strong>Size Options for Different Needs</strong>: Understanding the diverse needs of the AI community, Jina AI offers two versions of the model:</li></ul><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-base-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-en.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-embeddings-v2-small-en &#xB7; Hugging Face</div><div class="kg-bookmark-description">We&#x2019;re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-small-en.png" alt="Jina AI Launches World&apos;s First Open-Source 8K Text Embedding, Rivaling OpenAI"></div></a></figure><p>In reflecting on the journey and significance of this launch, Dr. Han Xiao, CEO of Jina AI, shared his thoughts:</p><blockquote><em>&quot;In the ever-evolving world of AI, staying ahead and ensuring open access to breakthroughs is paramount. With <code>jina-embeddings-v2</code>, we&apos;ve achieved a significant milestone. Not only have we developed <strong>the world&apos;s first open-source 8K context length model</strong>, but we have also brought it to a performance level on par with industry giants like OpenAI. Our mission at Jina AI is clear: we aim to democratize AI and empower the community with tools that were once confined to proprietary ecosystems. Today, I am proud to say, we have taken a giant leap towards that vision.&quot;</em></blockquote><p>This pioneering spirit is evident in Jina AI&apos;s forward-looking plans.</p><h2 id="a-glimpse-into-the-future"><strong>A Glimpse into the Future</strong></h2><p>Jina AI is committed to leading the forefront of innovation in AI. Here&#x2019;s what&#x2019;s next on their roadmap:</p><ul><li><strong>Academic Insights</strong>: An academic paper detailing the technical intricacies and benchmarks of <code>jina-embeddings-v2</code> will soon be published, allowing the AI community to gain deeper insights.</li><li><strong>API Development</strong>: The team is in the advanced stages of developing an OpenAI-like embeddings API platform. This will provide users with the capability to effortlessly scale the embedding model according to their needs.</li><li><strong>Language Expansion</strong>: Venturing into multilingual embeddings, Jina AI is setting its sights on launching German-English models, further expanding its repertoire.</li></ul><hr><p>About Jina AI GmbH:<br>Located at Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at <a>contact@jina.ai</a>. </p>]]></content:encoded></item><item><title><![CDATA[DocArray 0.39.1 Update]]></title><description><![CDATA[DocArray is a library for representing, sending and storing multi-modal data, perfect for Machine Learning applications.]]></description><link>https://jina.ai/news/docarray-0-39-1-update/</link><guid isPermaLink="false">653639aa6930100001517b22</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 23 Oct 2023 09:19:10 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-0391">Release Note (<code>0.39.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Image-DocArray-dak-1.jpg" alt="DocArray 0.39.1 Update"><p>This release contains 2 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/docarray/docarray/releases/tag/v0.39.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v0.39.1 &#xB7; docarray/docarray</div><div class="kg-bookmark-description">Release Note (0.39.1) Release time: 2023-10-23 08:56:38 This release contains 2 bug fixes.
&#x1F41E; Bug Fixes
From_dataframe with numpy==1.26.1 (#1823)
A recent update to numpy has changed some of the v&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="DocArray 0.39.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">docarray</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/2cc6511b791fbb31b8bb33b69f7a6fb2948965f11790323434871815a729578d/docarray/docarray/releases/tag/v0.39.1" alt="DocArray 0.39.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fromdataframe-with-numpy1261-1823">From_dataframe with numpy==1.26.1 (<a href="https://github.com/docarray/docarray/pull/1823?ref=jina-ai-gmbh.ghost.io">#1823</a>)</h3><p>A recent update to numpy has changed some of the versioning semantics, breaking DocArray&apos;s&#xA0;<code>from_dataframe()</code>&#xA0;method in some cases where the dataframe contains a numpy array. This has now been now fixed.</p><pre><code class="language-Python">class MyDoc(BaseDoc):
    embedding: NdArray
    text: str

da = DocVec[MyDoc](
    [
        MyDoc(
            embedding=[1, 2, 3, 4],
            text=&apos;hello&apos;,
        ),
        MyDoc(
            embedding=[5, 6, 7, 8],
            text=&apos;world&apos;,
        ),
    ],
    tensor_type=NdArray,
)
df_da = da.to_dataframe()
# This broke before and is now fixed
da2 = DocVec[MyDoc].from_dataframe(df_da, tensor_type=NdArray)</code></pre><h3 id="type-handling-in-python-39-1823">Type handling in python 3.9 (<a href="https://github.com/docarray/docarray/pull/1823?ref=jina-ai-gmbh.ghost.io">#1823</a>)</h3><p>Starting with Python 3.9,&#xA0;<code>Optional.__args__</code>&#xA0;is not always available, leading to some compatibility problems. This has been fixed by using the&#xA0;<code>typing.get_args</code>&#xA0;helper.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Johannes Messner (<a href="https://github.com/JohannesMessner?ref=jina-ai-gmbh.ghost.io">@JohannesMessner</a>&#xA0;)</li></ul>]]></content:encoded></item></channel></rss>
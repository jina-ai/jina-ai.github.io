<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Jina AI]]></title><description><![CDATA[The official newsroom of Jina AI]]></description><link>https://jina.ai/news</link><image><url>https://jina.ai/favicon.ico</url><title>Jina AI</title><link>https://jina.ai/news</link></image><generator>Ghost 5.75</generator><lastBuildDate>Sat, 16 Dec 2023 07:14:49 GMT</lastBuildDate><atom:link href="https://jina.ai/feed.rss" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[A Tale of Two Worlds: EMNLP 2023 at Sentosa]]></title><description><![CDATA[Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas.]]></description><link>https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa/</link><guid isPermaLink="false">657c3d130bab3100012d2878</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Sat, 16 Dec 2023 07:03:15 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><p>The sun blazed down on the glistening sidewalks of Sentosa, a symphony of laughter and chatter filling the air. Tourists, decked in their holiday best, meandered through the vibrantmaze of Universal Studios, their faces alight with the joy of a day out in this fantasy land. The click of cameras capturing moments against the backdrop of thrilling rides and colorful parades was omnipresent. Nearby, the enticing aromas of international cuisines wafted through the air, luring guests to indulge in a culinary adventure.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1440" height="1080" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2911702641285_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2911702641285_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg 1440w" sizes="(min-width: 720px) 720px"></figure><p>In stark contrast, just a stone&apos;s throw away, nestled in the heart of this revelry, was the Resorts World Convention Centre. Here, the atmosphere was charged with a different kind of excitement. The halls buzzed not with the sound of holiday-making, but with the fervor of intellectual discourse. Young researchers and seasoned academics, their conference badges swaying gently with each step, engaged in animated discussions about the latest in NLP.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-3.png" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As I walked into this place, the difference was clear. On one side, there was the loud and happy noise of a holiday resort, full of life and excitement. On the other side, there was a serious and busy atmosphere where people talked about NLP , LLMs, ChatGPT, prompting, and Google&apos;s new Gemini model. It was like a movie scene &#x2013; where learning and fun came together in a surprising way.</p><p>In this blog, I&apos;ll share some observations of EMNLP 2023 and some of the most interesting papers and posters we discovered at the conference.</p><h2 id="emnlp-2022-to-2023-shifts">EMNLP: 2022 to 2023 Shifts</h2><p>Attending EMNLP 2022 in Abu Dhabi and now looking back at EMNLP 2023, I&apos;ve observed significant shifts in research focus and conference dynamics. These changes, driven by the swift advancements in AI, paint a vivid picture of our adaptive and forward-looking community. Below, I share a comparison of the two conferences, highlighting how our priorities and discussions have transformed in just a year.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>EMNLP</th>
<th>2022</th>
<th>2023</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Research Focus</td>
<td>Diverse range of NLP methods, with emphasis on traditional approaches.</td>
<td>Strong focus on Large Language Models (LLMs) and prompting techniques.</td>
</tr>
<tr>
<td>Research Trends</td>
<td>Interest in a wide array of topics, but no standout groundbreaking papers.</td>
<td>Shift towards LLM interpretability, ethics, agents, and multimodal reasoning.</td>
</tr>
<tr>
<td>Conference Atmosphere</td>
<td>A bit peculiar and pessimistic due to the release of ChatGPT and its implications on traditional NLP methods.</td>
<td>More confidence and adaptability among researchers in embracing new trends.</td>
</tr>
<tr>
<td>Research Diversity</td>
<td>Still exploring traditional methods like topic modeling, n-grams smoothing, and Bayesian methods (as seen in COLING 2022).</td>
<td>Rapid adaptation to newer approaches, moving away from older methods.</td>
</tr>
<tr>
<td>Relevance of Presented Work</td>
<td>Consistent with contemporary research trends at the time.</td>
<td>Fast-paced AI development made some empirical methods and results feel outdated by the time of the conference.</td>
</tr>
<tr>
<td>Conference Engagement</td>
<td>Enjoyment derived more from personal conversations and interactions than from paper presentations.</td>
<td>Increased focus on personal communication, with more time spent at poster sessions than listening to oral presentations.</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="paper-highlights-from-emnlp-2023">Paper Highlights from EMNLP 2023</h2><p>At EMNLP 2023, several intriguing papers caught my attention, each addressing different aspects of NLP and pushing the boundaries of what&apos;s possible in this field. Let me share some of the highlights from these papers and my thoughts on them.</p><h3 id="hybrid-inverted-index-is-a-robust-accelerator-for-dense-retrieval">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.05521?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</div><div class="kg-bookmark-description">Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI$^2$ to achieve lossless retrieval quality with competitive efficiency across various index settings. Our code and checkpoint are publicly available at https://github.com/namespace-Pt/Adon/tree/HI2.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Peitian Zhang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Text embeddings have become very popular for information retrieval tasks. However, performing an exact embedding vector search requires one to calculate similarities between the embedding representation of the query and the embedding of each document. This becomes very slow for large datasets and leads to latencies that are not acceptable for real-world search applications. Therefore, many applications use approximated nearest neighbor search techniques to speed up the search system, whereby many of these techniques rely on vector quantization algorithms that learn an index of clusters based on the data distribution.&#xA0;</p><p>In addition, <strong>hybrid search</strong> has become popular which combines embedding-based search with traditional BM25-based search techniques. Usually, BM25 and embedding search are performed completely independently in hybrid search settings and only the result sets are combined.&#xA0;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/hype-and-hybrids-multimodal-search-means-more-than-keywords-and-vectors-2/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hype and Hybrids: Search is more than Keywords and Vectors</div><div class="kg-bookmark-description">Twenty years ago, &#x201C;hybrid&#x201D; was a term used only by botanists and chemists. Today, hybrid is booming&#x2026; even in search. Many search systems are rolling out hybrid search schemes with the latest AI. But is &#x201C;hybrid search&#x201D; really more than a buzzword?</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/Jina-AI-Website-Banners-Templates--21-.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper proposed a method to train a joined index of two parts: a cluster selector and a term selector. The cluster selector performs a vector quantization to assign texts into buckets of nearby clusters and the term selector determines the most representative terms of a document BM25 can be used to assign it into buckets associated with those terms, however, this is not trainable and can therefore not adjust to the training data. Alternatively, one can determine the most representative terms in a document with a BERT model with an MLP hat which is applied on each token to determine a score. In this way the term selector becomes trainable. Then the cluster centroids and the BERT model are trained together by using the KL divergence loss with the embedding model as a teacher to obtain a distribution of similarity values. The results in the paper show that this method can retrieve more relevant documents in the same amount of time as standard ANN techniques like HNSW and IVF-PQ implementations.</p><h3 id="is-chatgpt-good-at-search-investigating-large-language-models-as-re-ranking-agents">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2304.09542?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</div><div class="kg-bookmark-description">Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model&#x2019;s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Weiwei Sun</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>This paper investigates techniques to utilize LLMs for re-ranking documents. Re-ranking is usually performed in a search system after a first retrieval step to re-order the retrieved documents, e.g., to select the most relevant ones among them. Commonly used models are finetuned transformer models which are called <strong>cross encoders</strong>. Those receive as input a pair composed of a query and a document candidate and return a relevance score. Besides, more traditional learning-to-rank models like LambdaMart are also popular, especially in cases where the ranking is not only done based on semantic relevance itself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Cross-Encoders &#x2014; Sentence-Transformers documentation</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.sbert.net/_static/favicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://www.sbert.net/_static/logo.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Observing the strong NLP capabilities of LLMs, the authors of this paper wanted to investigate whether Models like GPT4 can be used to rank documents better. However, the limitation of those closed API-based models is usually that probability outputs are not accessible. Accordingly, the paper investigates techniques that rely only on prompting and the generated output text for re-ranking. The technique they propose inserts the documents together with an ID in the prompt and instructs the LLM to output a sequence of IDs with respect to the relevancy of the documents. In cases where the whole set of documents is too long to fit into the prompt, they apply a sliding window approach, where re-ranking is first performed on the documents with the lowest retrieval score obtained from the first-stage retriever. Then the most relevant documents according to the output are presented together with the documents in the next window of retrieval candidates and so on.</p><p>Since GPT-4 is too expensive and too slow for using it in a real-world setting, the authors propose distilling its ranking capabilities into a transformer-based cross-encoder model. The results show that even a comparably small model (440M parameters) distilled with this technique can outperform much larger state-of-the-art re-ranking models.</p><h3 id="large-language-models-can-self-improve">Large Language Models Can Self-Improve</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.11610?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Large Language Models Can Self-Improve</div><div class="kg-bookmark-description">Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate &#x201C;high-confidence&#x201D; rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and 63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Jiaxin Huang</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/qRfOSz3zHzg1gctMzgRjYRjeTyg-1pMyv7JIZZFJlTyKOQPoHJYfuB0u_eGC8wVnCKvN471-8D-avjk7-0XBGfAAFL7q0jI_-7eMGnXvCpEK8L8Pe3VSu5-iV5QHpfnY-5YSUQqvgT-gUs2sOUoQw0w" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="236"></figure><p>For many tasks, LLMs achieve already good results in zero-shot settings. However, to improve their performance on specific tasks (beyond zero-shot) common techniques still utilize a lot of training data to fine-tune them. To reduce the amount of training data needed, this paper presents a technique that uses the LLM to &#x201C;self-improve&#x201D;. The main idea behind this approach is to augment existing training datasets by using data generated with the LLM itself.</p><p>This is achieved by using a dataset with only questions and without answers. First, a chain of thoughts (CoT) method is used to generate for each question a couple of different reasoning paths and answers by using a temperature greater than zero to make the generative text non-deterministic. By determining the answer with the highest frequency the probability of the answer being correct can be increased. The paper also shows that LLMs can be used to effectively estimate the confidence of an answer by investigating the consistency of the answer. If a high percentage of the answers is equal, this answer also has a high probability being correct. These most frequent answers and the corresponding reasoning paths can then be used to construct additional prompts for constructing additional training data. However, the authors propose not only to use those reasoning paths and answers but also to add examples in different formats, e.g., presenting the question without any reasoning path and using the question together with some generic instruction like &#x201C;Let&#x2019;s think step by step&#x201D;. Finally, this enhanced training dataset can be used to fine-tune the LLM on the specific task. In their evaluation the authors show this technique is effective to fine-tune LLMs with minimal training data but also that it generalizes well, as it improves the result on out-of-domain datasets.</p><h3 id="adapting-language-models-to-compress-contexts">Adapting Language Models to Compress Contexts</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2305.14788?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Adapting Language Models to Compress Contexts</div><div class="kg-bookmark-description">Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Alexis Chevalier</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><figure class="kg-card kg-image-card"><img src="https://lh7-us.googleusercontent.com/RDaQqjnEa45QEArhdPfuNKXsZReEQfT7Hg_zgr-DlkmlMeeXJAYP0kiKfpQ3Vj8iL_I2mK5JZ0evaoHZAdBDHSaep9Z49Qx1dt5JdCcMrfH8UF-nRy_ZO6GT7wLwR654p0_9CF4jD7soGg-zBXbSFNg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="624" height="496"></figure><p>Language models are usually constrained by limited context length. While there are various techniques like <strong>AliBi</strong> to construct language models that can handle larger context lengths, those techniques do not help in cases where you want to use an existing model that has only a very limited context length. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</div><div class="kg-bookmark-description">Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi&#x2019;s inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Ofir Press</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Instead, this paper addresses this problem by introducing a new method that allows one to fine-tune an exciting language model to apply it to longer context lengths. To achieve this, the model&apos;s vocabulary is extended by a fixed number of summary tokens, which are supposed to tell the model to summarize the content of the other tokens in the respective embeddings. During inference, the text input is segmented into shorter text sequences which are extended with the summary tokens and prepended with the output summary vectors of the previously processed segments. To fine-tune the model to handle long sequences, the model is trained on a next-token prediction task. Thereby, the model should make use of the information encoded in the summary embeddings of the previous segments. Notably, the segment length is varied during the training and the backpropagation is done for one document as a whole. In other words, the model is first applied to all sequences in the afore-described scheme before starting the backpropagation. The technique is evaluated by the authors on OPT models of various sizes and the 7-B-Llama-2 model. Besides standard language modeling tasks, the authors also show that the models can be effectively used to solve in-context-learning classification tasks with longer prompts or used for re-ranking. Here the re-ranking follows a language modeling approach in which the passages are re-ranked based on the language model&#x2019;s likelihood to generate the question from the given passage.&#xA0; </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.emnlp-main.249/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Improving Passage Retrieval with Zero-Shot Question Generation</div><div class="kg-bookmark-description">Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, Luke Zettlemoyer. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aclanthology.org/thumb/2022.emnlp-main.249.jpg" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><h2 id="poster-highlights-from-emnlp-2023">Poster Highlights from EMNLP 2023</h2><p>At EMNLP 2023, alongside the compelling paper presentations, the poster sessions were a hub of vibrant discussion and exchange. Here&apos;s a rundown of some standout posters I came across, each offering a unique glimpse into the ongoing research and development within the field of NLP.</p><h3 id="can-retriever-augmented-language-models-reason"><strong>Can Retriever-Augmented Language Models Reason?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2931702707518_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2931702707518_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A poster from McGill University examined whether Retriever-Augmented Language Models (RALMs) can effectively reason by balancing the capabilities of both the retriever and the language model. The research highlighted the potential shortcomings of retrievers in sourcing all necessary statements for reasoning, and how language models might falter in reasoning even when provided with the required statements. It was a deep dive into improving the interactive components of language models.</p><h3 id="contrastive-learning-based-sentence-encoders"><strong>Contrastive Learning-based Sentence Encoders</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2941702707520_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2941702707520_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Researchers from Tohoku University presented findings on how Contrastive Learning (CL) can induce sentence encoders to implicitly weight informative words, enhancing the model&apos;s understanding and processing of language. This approach could refine the way sentence encoders prioritize and process key elements in text, making them more efficient and effective.</p><h3 id="investigating-semantic-subspaces-of-transformer-sentence-embeddings"><strong>Investigating Semantic Subspaces of Transformer Sentence Embeddings</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2951702707522_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2951702707522_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A team from the University of Stuttgart showcased their work on probing the semantic subspaces of transformer sentence embeddings. By employing linear structural probing, they aimed to understand how different layers of a transformer contribute to semantic content processing, offering insights into the inner workings of sentence embeddings.</p><h3 id="can-pre-trained-vision-and-language-model-answer-visual-information-seeking-questions"><strong>Can Pre-trained Vision and Language Model Answer Visual Information-Seeking Questions?</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2961702707523_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2961702707523_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>An intriguing poster by researchers from the Georgia Institute of Technology, Google Research, and Google DeepMind introduced a benchmark for testing the world knowledge in multimodal Large Language Models (LLMs) through Visual Information-Seeking Questions. The research focused on the capabilities of retrieval-augmented models and GPT-4 in answering questions that require visual understanding, pushing the envelope on multimodal AI.</p><h3 id="to-split-or-not-to-split-composing-compounds-in-contextual-vector-spaces"><strong>To Split or Not to Split: Composing Compounds in Contextual Vector Spaces</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2971702707525_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2971702707525_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>From the University of Stuttgart, a study delved into whether splitting compounds in contextual vector spaces is beneficial for the model&apos;s performance. The research explored the impact of compounds on semantic representation and processing, contributing to our understanding of compositional semantics in language models.</p><h3 id="subspace-chronicles-how-linguistic-information-emerges-shifts-and-interacts-during-language-model-training"><strong>Subspace Chronicles: How Linguistic Information Emerges, Shifts, and Interacts during Language Model Training</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2981702707526_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2981702707526_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>This poster detailed an exploration into the dynamics of linguistic information as it emerges and evolves during the training of language models. It&apos;s a fascinating look at the underpinnings of language model training and the critical learning phases that define their capabilities.</p><h3 id="theory-of-mind-for-multi-agent-collaboration-via-large-language-models"><strong>Theory of Mind for Multi-Agent Collaboration via Large Language Models</strong></h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2991702707528_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2991702707528_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Lastly, a poster outlined research on the Theory of Mind in Large Language Models and their application in multi-agent collaboration tasks. It&apos;s an exciting foray into the cognitive capabilities of LMs and their potential in collaborative environments.</p><h2 id="embeddings-roundtable-a-birds-of-a-feather-at-emnlp-2023">Embeddings Roundtable: A Birds of a Feather at EMNLP 2023</h2><p>During EMNLP 2023, we hosted a Birds of a Feather (BoF) session on embeddings that turned into a rich tapestry of insights and discussions. With a crowd of over 80 attendees, the session was an electrifying blend of sharp minds and cutting-edge topics.</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691.mp4" poster="https://img.spacergif.org/v1/1280x720/0a/spacer.png" width="1280" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h3 id="lightning-talks-and-panel-discussion">Lightning Talks and Panel Discussion</h3><p>The BoF session featured lightning talks by renowned researchers like Huiqiang, Hassan, Hwiyeol, Mattia, and Yang Chen. Each speaker brought a unique perspective to the table, sharing their latest findings in embedding research within NLP. The talks sparked an energizing dialogue that transitioned into a thought-provoking panel discussion.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3041702708673_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3041702708673_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3041702708673_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3031702708669_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3031702708669_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3031702708669_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3061702708681_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3061702708681_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3061702708681_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3051702708676_.pic-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3051702708676_.pic-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3051702708676_.pic-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg" width="1702" height="1276" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3081702708695_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3081702708695_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3081702708695_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div></div></figure><p>The panel, graced by Sebastian Ruder, Nicola Cancedda, Chia Ying Lee, Michael G&#xFC;nther, and Han Xiao, delved deep into the intricacies of embedding technologies. They covered a breadth of topics, from the evolution of embeddings to their future in a world increasingly dominated by Generative AI and Large Language Models (LLMs).</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg" class="kg-image" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" loading="lazy" width="720" height="541" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3011702708364_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg 720w" sizes="(min-width: 720px) 720px"></figure><h3 id="key-takeaways-from-the-panel">Key Takeaways from the Panel</h3><ol><li><strong>Diverse Perspectives on Embeddings:</strong><br>The panelists introduced themselves and their work with various embeddings, discussing the common threads and divergences they&apos;ve observed. They emphasized the nuanced differences in how embeddings behave depending on their design and application contexts.</li><li><strong>The Relevance of Embeddings Amidst Generative AI:</strong><br>With 2023&apos;s spotlight on LLMs, the panelists reflected on the enduring importance of embeddings. They highlighted that despite the LLM trend, embeddings retain a crucial role in understanding and processing language at a more granular level.</li><li><strong>Context Length in Embeddings vs. LLMs:</strong><br>A curious observation was the disparity in context length expansion between LLMs and embedding models. The panelists shed light on the technical and practical constraints that currently limit the context window in embedding models.</li><li><strong>Search and Generation:</strong><br>Addressing the assertion that &apos;search is an overfitted generation, and generation is an underfitted search,&apos; the panelists shared mixed views, sparking a lively debate on the interplay between search functions and generative capabilities.</li><li><strong>Future of RAG and Agent Models:</strong><br>Looking towards EMNLP 2024, the conversation turned to the prospective challenges and developments in Retrieval Augmented Generation (RAG) and agent models. The panelists hinted at their vision for the future integration of embeddings within these applications, recognizing the pivotal role they will continue to play.</li></ol><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3101702709471_.pic_hd-1.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3101702709471_.pic_hd-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3101702709471_.pic_hd-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3101702709471_.pic_hd-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3101702709471_.pic_hd-1.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3091702709471_.pic_hd.jpg" width="2000" height="2664" loading="lazy" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3091702709471_.pic_hd.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3091702709471_.pic_hd.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3091702709471_.pic_hd.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3091702709471_.pic_hd.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="summary">Summary</h2><p>Wrapping up EMNLP 2023, I&apos;m buzzing with ideas and energized by the community&apos;s shared passion for pushing the boundaries of NLP. Our Embeddings BoF session was a hit &#x2013; the engagement and insights made it a highlight for me.</p><p>Looking to get hands-on with the future of embeddings? We are hiring! We&apos;re all about diving deep into long-context, multilingual, and multimodal embeddings. So, if you&apos;re up for the challenge, check out the open roles here and maybe I&apos;ll see you at our Berlin, Shenzhen, or Beijing office.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/internship/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Intern Program</div><div class="kg-bookmark-description">Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-internship.png" alt="A Tale of Two Worlds: EMNLP 2023 at Sentosa"></div></a></figure><p>Can&apos;t wait to see what we&apos;ll cook up by EMNLP 2024 in Miami. Until then, keep innovating, keep questioning, and let&apos;s keep the conversations going!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.2 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-2-update/</link><guid isPermaLink="false">657b24340bab3100012d27cb</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Thu, 14 Dec 2023 15:54:21 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3232">Release Note (<code>3.23.2</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark-1.jpg" alt="Jina 3.23.2 Update"><p>This release contains 1 dependency update and 2 bug fixes.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.2?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.2 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.2) Release time: 2023-12-14 15:28:24 This release contains 1 dependency update and 2 bug fixes.
&#x2699; Dependency update
Update GRPC version requirements (#6110)
The grpc version re&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.2 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/66a744fcace7275ba964cb292fb9e91fd04678aa02883dd2f72c0c8ef1b2bc4a/jina-ai/jina/releases/tag/v3.23.2" alt="Jina 3.23.2 Update"></div></a></figure><h2 id="%E2%9A%99-dependency-update">&#x2699; Dependency update</h2><h3 id="update-grpc-version-requirements-6110">Update GRPC version requirements (<a href="https://github.com/jina-ai/jina/pull/6110?ref=jina-ai-gmbh.ghost.io">#6110</a>)</h3><p>The&#xA0;<code>grpc</code>&#xA0;version requirements have been updated to allow&#xA0;<code>grpcio&lt;=1.57.0</code>.</p><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="better-handling-of-exceptions-in-dynamic-batching-6128">Better handling of Exceptions in dynamic batching (<a href="https://github.com/jina-ai/jina/pull/6128?ref=jina-ai-gmbh.ghost.io">#6128</a>)</h3><p>An issue was identified when a dynamic batch raises an Exception that could affect some unrelated requests.</p><h3 id="load-balancing-streaming-based-on-response-type-6122">Load-balancing streaming based on response type (<a href="https://github.com/jina-ai/jina/pull/6122?ref=jina-ai-gmbh.ghost.io">#6122</a>)</h3><p>When using&#xA0;<code>Deployment</code>&#xA0;locally with multiple replicas, a load-balancing process is added in front of the replicas. </p><p>The load balancer assumed all&#xA0;<code>GET</code>&#xA0;requests to be streaming, but this may not be true for user-added FastAPI endpoints. We have fixed this assumption and now use the response type to determine if a request is streaming.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Narek Amirbekian (<a href="https://github.com/NarekA?ref=jina-ai-gmbh.ghost.io">@NarekA</a>&#xA0;)</li><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>&#xA0;)</li></ul>]]></content:encoded></item><item><title><![CDATA[There's a Time and a Place: Unleashing Dynamic Variables in PromptPerfect]]></title><description><![CDATA[Incorporate users' time, date and location into your prompts. Plus contents of other websites, and outputs of other prompts themselves!]]></description><link>https://jina.ai/news/theres-a-time-and-a-place-unleashing-dynamic-variables-in-promptperfect/</link><guid isPermaLink="false">6571be77782cc90001042c6f</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 07 Dec 2023 14:59:42 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--13-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--13-.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><p>Back in the mists of time, PromptPerfect introduced its <a href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io">Prompt-as-a-Service</a> feature, allowing you to serve your prompts via a RESTful API and call them from Python, JavaScript or cURL.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/whats-next-for-prompt-engineering-prompts-as-a-service?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">What&#x2019;s Next for Prompt Engineering? PromptPerfect&#x2019;s Prompt as a Service!</div><div class="kg-bookmark-description">Deploy prompts and flexible template prompts as REST API services, and integrate them into your applications with just a few clicks</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-publisher">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/06/Pic.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure><p>Let&#x2019;s imagine we&#x2019;re building an AI-powered recipe website. We want users to be able to click a button that suggests a delicious recipe for them. To do that, we can create a prompt and deploy it as a service. We would then incorporate that into our website via Prompt-as-a-Service&#x2019;s <a href="https://promptperfect.jina.ai/api?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">RESTful API.</a></p><p>For example, we could have a prompt that suggested recipes to users:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="760" height="943" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123047_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123047_Chrome.png 760w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Incorporating a RESTful API into your website is <i><em class="italic" style="white-space: pre-wrap;">way</em></i> outside the scope of this post, so we&#x2019;ll just use cURL moving forwards. But rest assured, you can do this in any language or framework with an HTTP library.</div></div><p>By calling the prompt, a user could get a new recipe each time:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><span style="white-space: pre-wrap;">Certainly! Below is a recipe for a classic Italian dish, Spaghetti Carbonara. This dish is known for its creamy texture and rich flavors, achieved with minimal ingredients. Enjoy!</span></p><p><b><strong style="white-space: pre-wrap;">Spaghetti Carbonara</strong></b></p><p><b><strong style="white-space: pre-wrap;">Ingredients</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">400g spaghetti</span></li><li value="2"><span style="white-space: pre-wrap;">150g pancetta or guanciale, diced</span></li><li value="3"><span style="white-space: pre-wrap;">3 large eggs</span></li><li value="4"><span style="white-space: pre-wrap;">1 cup (100g) freshly grated Parmesan cheese, plus extra for serving</span></li><li value="5"><span style="white-space: pre-wrap;">2 cloves garlic, peeled and left whole</span></li><li value="6"><span style="white-space: pre-wrap;">2 tablespoons extra-virgin olive oil</span></li><li value="7"><span style="white-space: pre-wrap;">Salt and freshly ground black pepper</span></li><li value="8"><span style="white-space: pre-wrap;">A small handful of fresh parsley, chopped (optional)</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions</strong></b></p><p dir="ltr"><b><strong style="white-space: pre-wrap;">Cook the Pasta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Bring a large pot of salted water to a boil.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the spaghetti and cook according to the package instructions until al dente.</span></li><li value="3"><span style="white-space: pre-wrap;">Reserve 1 cup of pasta water before draining the spaghetti.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Prepare the Sauce</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a small bowl, whisk together the eggs and grated Parmesan cheese until well combined. Set aside.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Cook the Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">While the pasta is cooking, heat the olive oil in a large skillet over medium heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Add the pancetta or guanciale and the whole garlic cloves. Cook, stirring frequently, until the meat is crispy and golden, about 5 minutes.</span></li><li value="3"><span style="white-space: pre-wrap;">Remove and discard the garlic cloves.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Combine Pasta and Pancetta</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Add the drained spaghetti to the skillet with the pancetta.</span></li><li value="2"><span style="white-space: pre-wrap;">Toss well to coat the pasta in the pancetta&apos;s fat.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Make Carbonara</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Remove the skillet from the heat.</span></li><li value="2"><span style="white-space: pre-wrap;">Quickly pour the egg and cheese mixture into the pasta, stirring vigorously with tongs or a fork until the eggs thicken but do not scramble.</span></li><li value="3"><span style="white-space: pre-wrap;">Add a little reserved pasta water if the sauce is too thick. The heat from the pasta will cook the eggs and melt the cheese to create a creamy sauce.</span></li><li value="4"><span style="white-space: pre-wrap;">Season generously with freshly ground black pepper and add a pinch of salt if needed.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Garnish and Serve</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Serve immediately with extra grated Parmesan cheese and a sprinkle of chopped parsley, if desired.</span></li></ul><p><span style="white-space: pre-wrap;">Enjoy your delicious Spaghetti Carbonara!</span></p><p><span style="white-space: pre-wrap;">This recipe serves 4 people and is best enjoyed fresh.</span></p></div>
        </div><h2 id="template-prompts">Template prompts</h2><p>Now, static prompts are fine and all, but we also allow template prompts. This means a user could set options (like ingredients or dietary restrictions) each time they called the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="614" height="467" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_123701_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_123701_Chrome.png 614w"></figure><p>Which would give the user something like:</p><pre><code class="language-bash">curl &quot;https://api.promptperfect.jina.ai/RYNiqoT3txBjPoALnV46&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;parameters&quot;: {&quot;ingredients&quot;:&quot;apples, cinnamon, nutmeg&quot;}}&apos;</code></pre><div class="kg-card kg-toggle-card" data-kg-toggle-state="close">
            <div class="kg-toggle-heading">
                <h4 class="kg-toggle-heading-text"><span style="white-space: pre-wrap;">View output</span></h4>
                <button class="kg-toggle-card-icon" aria-label="Expand toggle to read content">
                    <svg id="Regular" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                        <path class="cls-1" d="M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311"/>
                    </svg>
                </button>
            </div>
            <div class="kg-toggle-content"><p><b><strong style="white-space: pre-wrap;">Classic Apple Crisp Recipe</strong></b></p><p><span style="white-space: pre-wrap;">Enjoy the sweet and spicy flavors of apples, cinnamon, and nutmeg in this deliciously easy-to-make apple crisp. Perfect for a cozy night in or a festive gathering!</span></p><p><b><strong style="white-space: pre-wrap;">Ingredients:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">6 medium-sized apples, peeled, cored, and thinly sliced</span></li><li value="2"><span style="white-space: pre-wrap;">1 teaspoon ground cinnamon</span></li><li value="3"><span style="white-space: pre-wrap;">1/4 teaspoon ground nutmeg</span></li><li value="4"><span style="white-space: pre-wrap;">3/4 cup granulated sugar</span></li><li value="5"><span style="white-space: pre-wrap;">2 tablespoons lemon juice</span></li></ul><p><b><strong style="white-space: pre-wrap;">For the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">3/4 cup all-purpose flour</span></li><li value="2"><span style="white-space: pre-wrap;">3/4 cup old-fashioned rolled oats</span></li><li value="3"><span style="white-space: pre-wrap;">1/2 cup light brown sugar, packed</span></li><li value="4"><span style="white-space: pre-wrap;">1/2 teaspoon baking powder</span></li><li value="5"><span style="white-space: pre-wrap;">1/4 teaspoon ground cinnamon</span></li><li value="6"><span style="white-space: pre-wrap;">1/4 teaspoon salt</span></li><li value="7"><span style="white-space: pre-wrap;">1/3 cup unsalted butter, melted</span></li></ul><p><b><strong style="white-space: pre-wrap;">Instructions:</strong></b></p><p dir="ltr"><b><strong style="white-space: pre-wrap;">Preheat Oven and Prepare Apples:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Preheat your oven to 350&#xB0;F (175&#xB0;C).</span></li><li value="2"><span style="white-space: pre-wrap;">In a large mixing bowl, combine the sliced apples with granulated sugar, 1 teaspoon cinnamon, nutmeg, and lemon juice. Toss to coat the apples evenly.</span></li><li value="3"><span style="white-space: pre-wrap;">Transfer the apple mixture to a greased 9-inch square baking dish, spreading them out evenly.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Make the Topping:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">In a separate bowl, mix the flour, oats, brown sugar, baking powder, 1/4 teaspoon cinnamon, and salt.</span></li><li value="2"><span style="white-space: pre-wrap;">Pour the melted butter over the dry ingredients and mix until the mixture is crumbly.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Assemble and Bake:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Sprinkle the crumbly oat topping evenly over the apples in the baking dish.</span></li><li value="2"><span style="white-space: pre-wrap;">Bake in the preheated oven for about 45 minutes, or until the topping is golden brown and the apples are bubbling around the edges.</span></li></ul><p dir="ltr"><b><strong style="white-space: pre-wrap;">Serve:</strong></b></p><ul><li value="1"><span style="white-space: pre-wrap;">Allow the apple crisp to cool slightly before serving.</span></li><li value="2"><span style="white-space: pre-wrap;">Serve warm, optionally with a scoop of vanilla ice cream or a dollop of whipped cream.</span></li></ul><p><b><strong style="white-space: pre-wrap;">Notes:</strong></b></p><ul><li value="1"><b><strong style="white-space: pre-wrap;">Choosing Apples:</strong></b><span style="white-space: pre-wrap;"> For the best texture and flavor, use a mix of tart and sweet apples like Granny Smith and Honeycrisp.</span></li><li value="2"><b><strong style="white-space: pre-wrap;">Storage:</strong></b><span style="white-space: pre-wrap;"> Leftover apple crisp can be stored in the refrigerator for up to 3 days. Reheat in the oven or microwave before serving.</span></li></ul></div>
        </div><h2 id="a-time-and-a-place-for-everything">A time and a place for everything</h2><p>Now we&apos;re taking it up a notch with environmental awareness. Rather than the end-user having to specify where (and when) they are, you can choose to have that information automatically inserted. That means you can create a prompt once, and end-users will get different results depending on their location, date, and time.</p><p>Since recipes don&#x2019;t usually need a specified time and place, let&#x2019;s consider a new example: Building a website to suggest activities for a user to take part in:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="759" height="944" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124457_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124457_Chrome.png 759w" sizes="(min-width: 720px) 720px"></figure><p>We also need to enable Environment Awareness:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="611" height="110" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_124710_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_124710_Chrome.png 611w"></figure><p>Now, I (in Berlin at noon) get something like this when I call the prompt:</p><pre><code>- Visit the Berlin Wall Memorial at Bernauer Stra&#xDF;e
- Explore the Museum Island, especially the Pergamon Museum
- Take a walk through the Tiergarten Park
- Enjoy panoramic views from the Berliner Fernsehturm (TV Tower)
- Discover German history at the German Historical Museum
- Stroll along the East Side Gallery
- Check out the Brandenburg Gate at dusk
- Experience contemporary art at the Hamburger Bahnhof museum
- Shop or window-shop at Kurf&#xFC;rstendamm
- Relax in the evening at a traditional German beer garden&quot;
</code></pre><p>Whereas if my colleague in Tokyo called the prompt, she&apos;d get:</p><pre><code>- Visit Tokyo Skytree for panoramic views of the city at night.
- Explore the vibrant streets of Shibuya and see the famous Shibuya Crossing.
- Enjoy the illuminated Tokyo Tower and consider an evening visit.
- Take a stroll through the historic Asakusa district and see Senso-ji Temple.
- Experience the nightlife in Roppongi, known for its clubs and bars.
- Relax at Odaiba Seaside Park and enjoy the night view of Rainbow Bridge.
- Try out some local izakayas (Japanese pubs) for food and drinks.
- Visit a themed caf&#xE9;, like a cat caf&#xE9; or an owl caf&#xE9;, for a unique experience.
- Check out the latest gadgets and tech in Akihabara, the electronics district.
- Attend a traditional Kabuki performance at Kabukiza Theatre in Ginza (if available).
- Indulge in a sushi dinner at one of Tokyo&apos;s renowned sushi restaurants.
</code></pre><p>These variables are great when you&apos;re crafting prompts that deliver relevant output based on the user&#x2019;s time, date, and location. This is ideal for building websites and apps with a global audience.</p><h2 id="fetching-remote-data">Fetching remote data</h2><p>We can go even further though. With the crawler feature, you can download the main body text of the URL you specify:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="736" height="371" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125131_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125131_Chrome.png 736w" sizes="(min-width: 720px) 720px"></figure><p>This makes it ideal for, say, a summarizer/translator prompt. Here we summarize an article and convert it to a sea shanty:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="608" height="402" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125339_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125339_Chrome.png 608w"></figure><p>If I pass the URL for this story on <a href="https://www.cnn.com/2023/11/18/world/elon-musk-spacex-starship-launch-scn/index.html?ref=jina-ai-gmbh.ghost.io">Elon Musk&apos;s spaceship exploding</a>, I get this (abridged) output:</p><pre><code class="language-text">(Verse 1)
Oh, gather &apos;round, me hearties, and a tale I&apos;ll tell to thee,
Of SpaceX&apos;s Starship, and its flight so brief at sea.
Launched on a morn in November, with power so fierce and grand,
She climbed into the heavens, but &apos;twas not as Musk had planned.

(Chorus)
Heave ho, lift high, to the stars we aim to fly,
But the rocket&apos;s dreams did shatter, and into the sea did die.
Heave ho, lift high, with a fiery tail in sky,
SpaceX&apos;s Starship faltered, and we&apos;re left to wonder why.

(Verse 2)
The booster and the spacecraft, they parted with a roar,
The engines blazed like comets, as they&apos;d never done before.
The booster met its ending, in a ball of flame it fell,
While Starship soared a moment more, then silence cast its spell.
</code></pre><h2 id="calling-other-prompt-services">Calling other prompt services</h2><p>The above example wasn&apos;t exactly modular. I had both the summarizer and shantyizer in one prompt. That isn&apos;t so useful if I want to shanty all my things in full detail. Luckily, you can also call one prompt from another, allowing for a lot more modularity:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="762" height="493" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot_20231207_125614_Chrome.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125614_Chrome.png 762w" sizes="(min-width: 720px) 720px"></figure><p>So, if I create a summarizer prompt (named <code>summarizer</code>, which has the sole function of summarizing a web page)&#x2026;</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_130155_Chrome.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="592" height="387"></figure><p>&#x2026;I can easily call it from a new shantyizer prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot_20231207_125751_Chrome-1.png" class="kg-image" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect" loading="lazy" width="591" height="393"></figure><p>Sending a URL to the shantyizer prompt in turn processes the URL through the summarizer to download and summarize its contents. And of course, the summarizer can be used in any other prompt you like, acting more like a traditional programming language function.</p><p>That opens up many more possibilities. Especially when it comes to more complex tasks like cooking a dish. If I&#x2019;m cooking, for any given recipe I may want <em>one or more</em> of the following steps, but perhaps not all steps for all recipes.</p><ul><li>Download the content of a recipe page</li><li>If it&apos;s not in English, translate it (if I&apos;m cooking an authentic Chinese recipe, I want it from a real Chinese website)</li><li>Make it vegetarian (if I&apos;m eating with vegetarian friends)</li><li>Convert it to metric (because <a href="https://www.boredpanda.com/imperial-measurement-system-twitter-rant-innesmck/?ref=jina-ai-gmbh.ghost.io">reasons</a>)</li><li>Change serving size (depending on how many people I&apos;m eating with)</li></ul><h2 id="exercise-for-the-reader">Exercise for the reader</h2><p>As you can imagine, if I&apos;m eating alone (and I&apos;m not a vegetarian), my needs are quite different from when I&apos;m eating with my buddies at the local <a href="https://www.snopes.com/fact-check/peta-rename-fish-sea-kittens/?ref=jina-ai-gmbh.ghost.io">sea kitten appreciation society</a>. Either way, it means combining several of these &quot;functions&quot;.</p><p>We can also go multi-modal, combining text and image prompts:</p><ul><li>Create an image generation prompt from the given recipe</li><li>Generate an image of how the food should look</li></ul><p>The exercise for you, dear reader, is to build a prompt that performs the above recipe steps (or similar). Share your results on our Discord!</p><h2 id="make-the-magic-happen">Make the magic happen</h2><p>To get started with magic variables, head to <a href="http://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io">promptperfect.jina.ai</a> and get started. Let us know what you cook up on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">Discord</a>!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://promptperfect.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">PromptPerfect - Elevate Your Prompts to Perfection. Prompt Engineering, Optimizing, Debugging and Hosting.</div><div class="kg-bookmark-description">Unlock advanced prompt engineering and prompt optimization for large models such as GPT-4, ChatGPT, Midjourney and Stable Diffusion. Seamlessly deploy your text and image prompts as dedicated services with our free prompt hosting plan. Enhance your large models with superior performance and efficiency.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://promptperfect.jina.ai/icons/apple-icon-180x180.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"><span class="kg-bookmark-author">PromptPerfect</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://promptperfect.jina.ai/banner.png" alt="There&apos;s a Time and a Place: Unleashing Dynamic Variables in PromptPerfect"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Dify.AI integrates Jina Embeddings for RAG]]></title><description><![CDATA[Dify.AI, a leading open-source platform specialized in creating generative AI applications, is now leveraging Jina Embeddings v2! ]]></description><link>https://jina.ai/news/dify-ai-integrates-jina-embeddings-for-rag/</link><guid isPermaLink="false">65703e6d782cc90001042bca</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 06 Dec 2023 15:00:31 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Difyco.png" alt="Dify.AI integrates Jina Embeddings for RAG"><p>Online LLM application development platform <a href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Dify.AI</a> has integrated the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2 API</a> in its innovative AI toolkit for instant access when building and hosting LLM applications. All you need to do is add your Jina Embeddings API key via their intuitive web interface to get the full power of Jina AI&#x2019;s industry-leading embedding models in your RAG (retrieval-augmented generation) applications.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI x Jina AI&#xFF1A;Dify now Integrates Jina Embedding Model - Dify Blog</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Dify Blog</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/2OftM3MmDVHkLcxaNn58ZNWaE.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><h2 id="integrating-embeddings-in-rag">Integrating Embeddings in RAG</h2><p>Current AI architectures today have no direct way to integrate outside information sources. The model itself encodes information from its training data with varying levels of accuracy, and it is impractical to retrain the model every time there is new, potentially useful data that could be incorporated into it.</p><p>For example, I asked <a href="https://chat.jina.ai/chat?ref=jina-ai-gmbh.ghost.io">JinaChat</a> a question about current events:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1408" height="1654" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.48.59.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.48.59.png 1408w" sizes="(min-width: 720px) 720px"></figure><p>The only way to ensure that an LLM has the information needed to answer a factual question is to provide it in the prompt:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="1316" height="1186" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/Screenshot-2023-12-01-at-11.53.33.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/Screenshot-2023-12-01-at-11.53.33.png 1316w" sizes="(min-width: 720px) 720px"></figure><p>Naturally, an LLM that only answers questions correctly if you include the answer in your question isn&#x2019;t very useful. This has led to a body of techniques called <strong>Retrieval-Augmented Generation</strong> or <strong>RAG</strong>. RAG is a framework installed around an LLM that searches external information sources for materials that might contain the information needed to answer a user&#x2019;s request and then presents them, with the user&#x2019;s prompt, to the LLM.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-2.png" class="kg-image" alt="Dify.AI integrates Jina Embeddings for RAG" loading="lazy" width="2000" height="938" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-2.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This strategy has the added benefit that LLMs hallucinate much less when they are expected to handle a given text rather than recall things they might partially remember from training.</p><h2 id="leveraging-jina-embeddings-superior-performance">Leveraging Jina Embeddings Superior Performance</h2><p><a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a> has integrated Jina Embeddings v2 to enhance retrieval quality for RAG prompting. Jina AI&#x2019;s models provide <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">state-of-the-art accuracy in RAG applications</a>, and <a href="https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io">with an input window of 8,192 tokens</a>, they can support much larger and more complex questions than most competing models at a much lower price.</p><p>You can now use Jina Embeddings in your LLM projects via <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s intuitive application builder, as shown in the video below or in the <a href="https://dify.ai/blog/integrating-jina-embeddings-v2-dify-enhancing-rag-applications?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">post on Dify.AI&apos;s blog</a>:</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg" data-kg-custom-thumbnail>
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396.mp4" poster="https://img.spacergif.org/v1/1244x720/0a/spacer.png" width="1244" height="720" playsinline preload="metadata" style="background: transparent url(&apos;https://jina-ai-gmbh.ghost.io/content/media/2023/12/ssstwitter.com_1701878449396_thumb.jpg&apos;) 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"/>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:38</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1&#xD7;</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"/>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"/>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h2 id="get-involved">Get Involved</h2><p>Check out <a href="http://dify.ai/?ref=jina-ai-gmbh.ghost.io">Dify.AI</a>&#x2019;s LLM application builder and hosting service for yourself. You can get a free tester token from the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina AI website to use Jina Embeddings</a> to try it out.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://dify.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dify.AI &#xB7; The Innovation Engine for Generative AI Applications</div><div class="kg-bookmark-description">The next-gen development platform - Easily build and operate generative AI applications. Create Assistants API and GPTs based on any LLMs.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://framerusercontent.com/images/KWDRAMQLGjoMFBAjNjoCFMP7XI.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://framerusercontent.com/images/wh4qGCAanwpqHs0Kot8VLBSty4.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure><p>For more information about Jina AI&#x2019;s offerings, check out the <a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io">Jina AI website</a> or join our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">community on Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3873 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Dify.AI integrates Jina Embeddings for RAG"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Dify.AI integrates Jina Embeddings for RAG"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina Embeddings v2 and MongoDB Atlas]]></title><description><![CDATA[Supercharge MongoDB Atlas multi-cloud vector search solutions with Jina AI’s industry-leading embeddings!]]></description><link>https://jina.ai/news/jina-embeddings-v2-and-mongodb-atlas/</link><guid isPermaLink="false">6570380d782cc90001042bac</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 06 Dec 2023 09:49:40 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--8-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Blog-images--8-.jpg" alt="Jina Embeddings v2 and MongoDB Atlas"><p>Jina AI and <a href="https://www.mongodb.com/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">MongoDB</a> have collaborated to bring you a tutorial on using <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings v2</a> to bring state-of-the-art semantic search to MongoDB Atlas. Follow the link below to MongoDB&apos;s website to see how you can supercharge MongoDB&#x2019;s multi-cloud vector search solutions with Jina AI&#x2019;s industry-leading embeddings!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.mongodb.com/developer/products/atlas/jina-ai-semantic-search/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Semantic search with Jina Embeddings v2 and MongoDB Atlas | MongoDB</div><div class="kg-bookmark-description">Follow along with this tutorial on using Jina Embeddings v2 with MongoDB Atlas for vector search.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.mongodb.com/developer/favicon.ico" alt="Jina Embeddings v2 and MongoDB Atlas"><span class="kg-bookmark-author">MongoDB</span><span class="kg-bookmark-publisher">Scott Martens, Saahil Ognawala</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt3d996d2aa8d9ece9/647a2f77f7b1280bb15174b7/Technical_SOFTWARE_Terminal(4)_Spot_BS_ForestGreen.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Jina Embeddings v2 and MongoDB Atlas"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session]]></title><description><![CDATA[Mark your calendars for an immersive BoF session on Embeddings at EMNLP 2023. Join us on December 9th from 11:00 AM to 12:30 PM, Singapore time, in the 'Aquarius 1' room for a deep dive into the latest embeddings.]]></description><link>https://jina.ai/news/discover-the-latest-in-embeddings-at-emnlp-2023-in-person-bof-session/</link><guid isPermaLink="false">6569dc2e66eb300001a28a4b</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Fri, 01 Dec 2023 14:25:49 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--25--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--25--1.png" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"><p>At the EMNLP2023 BoF (birds of a feather) session, we&apos;re set to explore the frontiers of embeddings. While recent developments like <code>jina-embeddings-v2</code> <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83?ref=jina-ai-gmbh.ghost.io">have made significant strides</a>, they also open the door to new questions and challenges in the field. This 1.5-hour session is a platform to tackle these emerging issues in embeddings. You can register via the Google Form below:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://forms.gle/z5E5BFvQqFf3fpW68?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">EMNLP 2023/BoF 6: Embeddings</div><div class="kg-bookmark-description">What: A BoF (Birds of a Feather) in-person session on Embeddings, co-organized by EMNLP PC and Jina AI. When &amp; Where: Saturday, December 9th, from 11:00 AM to 12:30 PM Singapore time at &#x2018;Aquarius 1&#x2019; room. Highlights: multimodal embeddings, long context embeddings, multi- and bilingual embeddings,&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ssl.gstatic.com/docs/forms/device_home/android_192.png" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"><span class="kg-bookmark-author">Google Docs</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://lh5.googleusercontent.com/osAO-g4ajeOU2CCB6rhbi9UfHIsCU8_oa8HaZjsx0e6UuWoulCngSugztdf3Yoe2a5FQVSymImg=w1200-h630-p" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session"></div></a></figure><h2 id="event-details">Event Details</h2><p><strong>Date &amp; Time:</strong> December 9th, 11:00 AM - 12:30 PM Singapore time, at &apos;Aquarius 1&apos; room.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4C6;</div><div class="kg-callout-text">Add Embeddings BoF at EMNLP to your calendar now [<a href="https://www.google.com/calendar/render?action=TEMPLATE&amp;text=Embeddings%3A+Birds+of+a+Feather+6&amp;dates=20231209T040000%2F20231209T053000&amp;ctz=UTC&amp;details=Birds+of+a+Feather+6&amp;location=&amp;sprop=&amp;sprop=name%3A&amp;ref=jina-ai-gmbh.ghost.io" target="_blank">Google</a>][<a href="https://outlook.office365.com/owa/?path=%2Fcalendar%2Faction%2Fcompose&amp;rru=addevent&amp;subject=Embeddings%3A+Birds+of+a+Feather+6&amp;startdt=2023-12-09T05%3A00%3A00+01%3A00&amp;enddt=2023-12-09T06%3A30%3A00+01%3A00&amp;body=Birds+of+a+Feather+6&amp;location=true&amp;ref=jina-ai-gmbh.ghost.io" target="_blank">Office365</a>][<a href="about:blank">Outlook</a>][<a href="about:blank">iCal</a>]</div></div><p><strong>Format:</strong> In-person, with an interactive and collaborative approach.<br><strong>Registration: </strong><a href="https://forms.gle/z5E5BFvQqFf3fpW68?ref=jina-ai-gmbh.ghost.io">Google Form</a></p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-1.png" class="kg-image" alt="Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session" loading="lazy" width="1288" height="962" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-1.png 1288w" sizes="(min-width: 720px) 720px"></figure><h2 id="themes-and-highlights">Themes and Highlights</h2><p>This BoF session features a set of thought-provoking presentations, panel discussions, and lightning talks, reflecting the ongoing evolution in the field:</p><h4 id="multimodal-embeddings">Multimodal Embeddings:</h4><ul><li>&quot;How can multimodal embeddings be advanced to handle the context and nuance in diverse data types?&quot;</li><li>&quot;What are the challenges in scaling multimodal embeddings for complex applications like personalized AI assistants?&quot;</li><li>&quot;In what ways can vector databases be integrated with multimodal embeddings to enhance their contextual understanding?&quot;</li></ul><h4 id="long-context-embeddings">Long Context Embeddings:</h4><ul><li>&quot;How can long context embeddings be optimized to reduce hallucination in LLMs?&quot;</li><li>&quot;What are the computational and practical challenges in scaling embeddings to handle longer contexts efficiently?&quot;</li><li>&quot;Beyond the 8K limit, what novel approaches can push the boundaries of context length in embeddings?&quot;</li></ul><h4 id="multiand-bilingual-embeddings">Multi- and Bilingual Embeddings:</h4><ul><li>&quot;What are the key hurdles in achieving high accuracy and cultural sensitivity in multi- and bilingual embeddings?&quot;</li><li>&quot;How can embeddings tackle the challenge of representing less popular factual knowledge accurately?&quot;</li><li>&quot;In the development of bilingual embeddings, how can we address the lack of in-house NLP expertise and high development costs?&quot;</li></ul><h4 id="embeddings-for-low-resource-languages">Embeddings for Low-Resource Languages:</h4><ul><li>&quot;What innovative strategies can be adopted to significantly boost support for low-resource languages in embeddings?&quot;</li><li>&quot;How can we incentivize the development of embeddings for less popular languages amidst high costs and complexity?&quot;</li><li>&quot;What role can transfer learning and fine-tuning play in enhancing embeddings for low-resource languages?&quot;</li></ul><h4 id="rag-retrieval-augmented-generation-applications">RAG (Retrieval Augmented Generation) Applications:</h4><ul><li>&quot;How can RAG applications overcome the limitations of vector databases in understanding complex, nuanced meanings?&quot;</li><li>&quot;What are the potentials and challenges of using RAG in AI assistants for deep personalization and context understanding?&quot;</li><li>&quot;Considering the current state of RAG, what are the ethical considerations and potential risks in its widespread adoption?&quot;</li></ul><p>For more information, visit <a href="https://2023.emnlp.org/program/bof/?ref=jina-ai-gmbh.ghost.io">EMNLP2023 Program</a>. </p><p>We are looking forward to seeing you in person in Singapore next week!</p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.1 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-1-update/</link><guid isPermaLink="false">6569a11066eb300001a2899e</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Fri, 01 Dec 2023 09:59:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3231">Release Note (<code>3.23.1</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Image-Jina-dark.jpg" alt="Jina 3.23.1 Update"><p>This release contains 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.1?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.1 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.1) Release time: 2023-12-01 09:24:22 This release contains 1 bug fix.
&#x1F41E; Bug Fixes
Fix dependency on OpenTelemetry Exporter Prometheus (#6118)
We fixed the dependency version wi&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.1 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/bbfee09be3e253af0418f345fc17cff01a66ed9575027c8c932d25cf7cb4fb6f/jina-ai/jina/releases/tag/v3.23.1" alt="Jina 3.23.1 Update"></div></a></figure><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="fix-dependency-on-opentelemetry-exporter-prometheus-6118">Fix dependency on OpenTelemetry Exporter Prometheus (<a href="https://github.com/jina-ai/jina/pull/6118?ref=jina-ai-gmbh.ghost.io">#6118</a>)</h3><p>We fixed the dependency version with&#xA0;<code>opentelemetry-exporter-prometheus</code>&#xA0;to avoid using deprecated versions.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[Give and Take in FDP's AI Geopolitics Panel]]></title><description><![CDATA[This blog post captures my personal reflections and contributions to the panel, highlighting key differences in AI strategies among global powers like the US, China, and the EU.]]></description><link>https://jina.ai/news/give-and-take-in-fdps-ai-geopolitics-panel/</link><guid isPermaLink="false">6567283bd64f01000187a058</guid><category><![CDATA[Events]]></category><dc:creator><![CDATA[Han Xiao]]></dc:creator><pubDate>Wed, 29 Nov 2023 13:04:01 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--23-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--23-.png" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"><p>The crisp air of Berlin in late November carried a unique vibrancy, especially in the bustling district of Prenzlauer Berg. As I weaved through the lively Christmas market outside the <a href="https://palais-kulturbrauerei.de/?ref=jina-ai-gmbh.ghost.io">Palais der Kulturbrauerei</a>, the scene was a microcosm of Berlin itself. Stalls adorned with twinkling lights offered an array of handmade trinkets and steaming treats, drawing in a crowd that was as diverse as the city&apos;s own character. Artists, both emerging and established, mingled with locals and tourists, their conversations a blend of German and a myriad of other languages, creating a tapestry of global influence and local charm. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/2831701259388_.pic.jpg" class="kg-image" alt="Give and Take in FDP&apos;s AI Geopolitics Panel" loading="lazy" width="1702" height="1276" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/2831701259388_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/2831701259388_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/2831701259388_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/2831701259388_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Xmas market at Kulturbrauerei, a symbol of Berlin&apos;s industrial past and its cultural renaissance, was more than just a festive gathering. It was a reflection of the city&apos;s unique ethos - not striving to compete with the grandeur of Paris or the pace of Shanghai, but instead carving out its own identity, one that embraced its history while boldly facing the future.</span></figcaption></figure><p>As I approached the Palais, the sounds of the market slowly gave way to a more subdued atmosphere. Inside, the venue buzzed with the energy of intellectuals and policymakers, all gathered to dissect and debate the future of EU&apos;s AI. </p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://crm.fdpbt.de/termin/ai-revolution-wie-kuenstliche-intelligenz-unsere-welt-veraendert?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Zustimmung zu Cookies | FDP-Fraktion im Bundestag</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://crm.fdpbt.de/themes/custom/uv_fdpbt_theme/favicon.ico" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"><span class="kg-bookmark-author">FDP-Fraktion im Bundestag</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://crm.fdpbt.de/termin/themes/custom/uv_fdpbt_theme/images/logo.svg" alt="Give and Take in FDP&apos;s AI Geopolitics Panel"></div></a><figcaption><p><span style="white-space: pre-wrap;">The full program of this FDP event is available </span><a href="https://crm.fdpbt.de/sites/default/files/2023-11/231128%20FDPBT%20KI%20Programm.pdf?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">here</span></a><span style="white-space: pre-wrap;">.</span></p></figcaption></figure><p>Despite my role as a panelist and my years of experience in AI, spanning from Tencent AI to my own AI startups, I couldn&apos;t shake a sense of skepticism. Was my presence here truly impactful? Would the discussions tonight resonate beyond these walls? </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/2851701259389_.pic.jpg" class="kg-image" alt="Give and Take in FDP&apos;s AI Geopolitics Panel" loading="lazy" width="1702" height="1276" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/2851701259389_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/2851701259389_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/2851701259389_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/2851701259389_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">As the conversation unfolded, my thoughts turned towards what I would share with the audience. My answers, crafted from years of navigating the complex landscapes of AI in two very different worlds, were ready.</span></figcaption></figure><p>The air was thick with expectations as the panelists, including myself, took our seats. I was curious to hear the perspectives of my fellow speakers and the FDP&apos;s stance on these pressing issues. While I harbored doubts about the overall impact of such events, a part of me remained hopeful. Perhaps, through these discussions, we could foster a deeper understanding between Europe and China, bridging gaps in perception and strategy. </p><p>Below, you&apos;ll find my responses to the questions posed during the panel. Some answers have been expanded and further refined during my subsequent reflections.</p><h2 id="strategies-and-positions-of-major-powers">Strategies and positions of major powers</h2><h3 id="q-how-do-the-strategies-and-positions-of-the-us-china-and-the-eu-differ-in-achieving-global-leadership-in-ai">Q: How do the strategies and positions of the US, China and the EU differ in achieving global leadership in AI?</h3><p><strong>Han:</strong> In discussing how the strategies and positions of the US, China, and the EU differ in achieving global leadership in AI, I would start by highlighting that the US primarily drives innovation through the private sector with substantial funding and the robust Silicon Valley ecosystem. In contrast, China adopts a <strong>state-driven </strong>approach, utilizing <strong>large-scale</strong> data and focusing on <strong>rapid implementation</strong> of AI technologies. The EU, on the other hand, places a strong emphasis on ethical AI, developing regulatory frameworks, and maintaining a focus on human-centric AI.</p><p>Diving deeper into China&apos;s approach, we see ambitious state-driven initiatives, with the goal of becoming an AI world leader by 2030. This is rooted in a legacy of central planning and a national capitalism ethos. There&apos;s a significant emphasis on indigenous innovation, mirroring European approaches, but with a distinct focus on technological sovereignty. Talent development is also key, as evidenced by China&apos;s &quot;Thousand Talents&quot; plan, which aims to bolster AI expertise.</p><p>Large-scale data usage in China is a critical factor, with vast data repositories enhancing algorithm accuracy. This is supported by multidimensional datasets, particularly rich user data derived from diverse smartphone activities. When it comes to implementation, China has developed 79 large language models, each with over 1 billion parameters, showing remarkable progress. The focus here is on practical application, transitioning from discovery to a strong execution in &quot;1 to n&quot; innovation.</p><p>In contrast to the US, which focuses more on discovery, China excels in the implementation and application of AI technologies. This difference highlights a unique approach where China leverages its strengths in rapid development and practical application of AI solutions.</p><h2 id="cooperation-versus-regulation">Cooperation versus regulation</h2><h3 id="q-should-countries-limit-or-expand-their-collaborations-in-ai-research-due-to-geopolitical-rivalries">Q: Should countries limit or expand their collaborations in AI research due to geopolitical rivalries?</h3><p><strong>Han: </strong>When considering whether countries should limit or expand their collaborations in AI research due to geopolitical rivalries, it&apos;s tempting to adopt an old cold-war mindset. It&apos;s easy to see why the US might be hesitant to collaborate: they don&apos;t want to share their technological leadership, have security concerns, face economic competition, and grapple with ideological differences. In German, there&apos;s a saying &quot;Wer oben ist, will oben bleiben,&quot; meaning &quot;Those who are on top want to stay on top.&quot;</p><p>However, collaboration is the only rational way to accelerate AI development. The United States is known for its advanced AI algorithms and strong research capabilities, while China boasts vast data resources essential for training AI models.</p><p>The ideal scenario would involve collaboration between the US and China, specializing in AI excelling in efficiency and scalability, but we must be mindful of potential issues with global trust and acceptance due to weaker ethical and privacy safeguards. Similarly, collaboration between the US and the EU could yield technologically sophisticated, ethically compliant AI, but might struggle with data diversity and comprehensiveness. China and the EU could focus on ethically-grounded AI with rich data integration but might lag in incorporating the latest AI innovations. The best outcome would be a balanced AI, leading in innovation, ethics, and data scope, overcoming individual limitations.</p><p>A balanced approach is necessary, considering geopolitical tensions and concerns about data privacy, security, and ethical use of AI.</p><p>However, the reason China might be reluctant to open its arms fully to US AI is due to a focus on cultural and political autonomy. The Chinese emphasis on indigenous innovation is partly driven by concerns over cultural influences and values being exported through U.S.-dominated AI, especially in terms of political influence.</p><p>Information sovereignty is crucial for ensuring that AI development aligns with national values and interests, avoiding external cultural or political influences.</p><p>A true story illustrates this point: Post-WWII, Japan was heavily influenced by American ideas, including the notion that a bread diet was superior to rice for health. This belief, supported by some Japanese scholars and the government, led to significant changes in dietary habits in Japan, decreasing the country&apos;s self-sufficiency in food and increasing reliance on imports from the United States. This example shows how foreign influence can profoundly impact national culture and practices.</p><h3 id="q-should-the-eu-work-even-more-closely-with-strategic-partners-like-the-us">Q: Should the EU work even more closely with strategic partners like the US?</h3><p><strong>Han: </strong>Understanding the EU&apos;s inclination to collaborate with the US in AI is straightforward, as the US is a leader in AI innovation, which can complement the EU&apos;s strengths in regulatory frameworks. Additionally, the EU and US often share common values such as democracy, human rights, and privacy, aligning closely in AI development.</p><p>However, working with China also presents significant advantages. China ranks in the top 3 globally for AI vibrancy, as per the Stanford AI Index. Its AI market is projected to exceed $26.7 billion by 2026, offering the largest real-world testing ground for AI innovations, invaluable for global scalability. The sector diversity in China, with strong AI adoption in finance, retail, and high tech, and leadership in consumer AI apps, is noteworthy. Additionally, the automotive AI potential in China, being the world&apos;s largest auto market, presents significant opportunities, especially in autonomous vehicles and fleet management, with an estimated $380 billion economic value by 2030.</p><h2 id="challenges-and-opportunities-for-the-eu">Challenges and opportunities for the EU</h2><h3 id="q-compared-to-individual-state-actors-such-as-the-us-and-china-how-can-the-eu-maintain-its-competitiveness">Q: Compared to individual state actors such as the US and China, how can the EU maintain its competitiveness?</h3><p><strong>Han: </strong>The EU faces significant challenges in maintaining competitiveness with individual technological powerhouses like the US and China. One of the main hurdles is technological lag, with European companies generally underperforming compared to their US counterparts, especially in terms of growth and R&amp;D investment. This is reflective of Europe&apos;s lag in the last technology revolution, notably in ICT and disruptive innovations.</p><p>Moreover, there are stark investment disparities in key future technologies such as quantum computing, 5G, and AI. For instance, in AI, Europe only captured about 12% of external funding between 2015 and 2020, compared to 40% by the US and 32% by Asia, including China. This gap in investment is a significant factor contributing to the EU&apos;s challenges in maintaining technological competitiveness.</p><h3 id="q-does-the-eu-offer-any-specific-advantages">Q: Does the EU offer any specific advantages?</h3><p><strong>Han: </strong>The EU indeed offers specific advantages in the realm of AI. Europe boasts a rich talent pool and a strong tradition in theoretical AI research and mathematics. Notable European AI researchers include Yoshua Bengio from France/Canada, a leader in deep learning, J&#xFC;rgen Schmidhuber from Germany, known for his work on neural networks, and Max Welling, a Dutch computer scientist recognized for his contributions in machine learning and AI. Esteemed institutions like the Max Planck Institutes in Germany and INRIA in France have been at the forefront of significant AI advancements.</p><p>Europe is often seen as the &apos;Switzerland of AI politics,&apos; maintaining a neutral position amid US-China tech tensions. This neutrality is strategic, enabling Europe to focus on ethical AI development and collaborate innovatively without getting entangled in geopolitical struggles.</p><p>The EU&apos;s approach is characterized by the motto, &quot;In Europe, we don&apos;t just have history in our museums; we also make history in our AI labs.&quot; This reflects Europe&apos;s unique position in observing and learning from the technological race between the &apos;eagles&apos; (US) and &apos;dragons&apos; (China), emphasizing the importance of foresight in addition to speed.</p><p>In conclusion, AI is a transformative force comparable to the Industrial Revolution. Europe&apos;s stance encourages teamwork, openness, and curiosity. If not leveraged, there&apos;s a risk of turning this revolutionary era into a missed opportunity, relegating Europe to a spectator&apos;s role in this global race.</p>]]></content:encoded></item><item><title><![CDATA[SceneXplain's JSON Schema Store: Automate Your Alt-Text, and More!]]></title><description><![CDATA[Take the hassle out of extracting data from images with SceneXplain's new JSON Schema Store. Discover and share reusable JSON schemas. Create, contribute, and access schemas easily through GUI or API]]></description><link>https://jina.ai/news/scenexplains-json-schema-store-automate-your-alt-text-and-more/</link><guid isPermaLink="false">655f48ed14230d0001313816</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 27 Nov 2023 15:00:14 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/cover2-1.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><p>We&apos;re starting to run low on phrases like &quot;we&apos;re happy to announce x&quot;, or &quot;announcing x&quot; every time we push out a new feature in SceneXplain. So let&apos;s just jump to the meat: SceneXplain has a new JSON Schema Store, where you can discover and share reusable JSON schemas. This builds on our recent &quot;extract JSON from image&quot; feature, which lets you specify a JSON Schema when uploading an image, and get information back in JSON that adheres to that schema.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain&#x2019;s Image-to-JSON: Extract Structured Data from Images with Precision</div><div class="kg-bookmark-description">Pushing the boundaries of visual AI, we&#x2019;re thrilled to unveil SceneXplain&#x2019;s Image-to-JSON feature. Dive into a world where images aren&#x2019;t just seen, but deeply understood, translating visuals into structured data with unparalleled precision.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/09/Explore-image-storytelling-beyond-pixels--7-.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure><h2 id="what-is-a-json-schema">What is a JSON Schema?</h2><p>According to the <a href="https://json-schema.org/learn/getting-started-step-by-step?ref=jina-ai-gmbh.ghost.io">official website</a>, JSON Schema is a vocabulary that you can use to annotate and validate JSON documents. With SceneXplain, you can upload a schema like:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;alt_tag&quot;: {
      &quot;type&quot;: &quot;string&quot;,
      &quot;description&quot;: &quot;the most concise description possible of the image&apos;s content and function. Do not describe elements that are purely decorative (e.g. part of the website&apos;s design, not content). Do not include text like &apos;this image contains&apos; or &apos;image depicts&apos;&quot;
    }
  }
}</code></pre><p>Along with an image...</p><figure class="kg-card kg-image-card"><img src="https://media.discordapp.net/attachments/1062299968955764777/1177229401763614794/generativejina_Close-up_view_of_a_tall_tree_covered_in_moss_wit_029426a1-96f1-49e9-903d-7772488af950.png?ex=6571bf51&amp;is=655f4a51&amp;hm=6f73c30f7a67b93fa5e5bcb615a047701a6b1ddd4cf3debf39aaf58e0a348033&amp;=&amp;width=625&amp;height=625" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><p></p><p>And get the following output:</p><pre><code class="language-json">{
  &quot;alt_text&quot;: &quot;Close-up view of a tall tree covered in moss with a clear blue sky in the background&quot;
}</code></pre><p>The JSON schema above is pretty straightforward, with only one field. But crafting more complicated JSON schemas can be a lot of effort, even assuming they work well. And sharing them can get messy, having to shuffle code blocks around Slack or other messaging clients (believe me, I&apos;ve been there!)</p><h2 id="how-does-the-schema-store-work">How does the Schema Store work?</h2><p>The Schema Store solves these headaches. Now you copy and use someone else&apos;s Schema without having to go through the bother of crafting your own. Or you can easily share your schemas with colleagues for more efficient image processing.</p><h3 id="use-community-schemas">Use community schemas</h3><p>To get started, create an account on SceneXplain and follow these steps:</p>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h3 id="create-your-own-schema">Create your own schema</h3>
<!--kg-card-begin: html-->
<iframe src="https://scribehow.com/embed/Creating_a_New_JSON_Schema__uQJgxkWQQ7mcXFwQTobYkQ?skipIntro=true" width="100%" height="640" allowfullscreen frameborder="0"></iframe>
<!--kg-card-end: html-->
<h2 id="examples">Examples</h2><p>Here are a few more examples of what you can do with the schemas from the Store:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/emoji.png" width="585" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/tinder.png" width="590" height="728" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/super.png" width="590" height="1231" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/land_use.png" width="584" height="827" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text.png" width="584" height="795" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/text_language.png" width="584" height="870" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/brand_sentiment.png" width="597" height="876" loading="lazy" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></div></div></figure><h2 id="bulk-generating-alt-text-via-scenexplains-api">Bulk-generating alt-text via SceneXplain&apos;s API</h2><p>Now let&apos;s get our hands dirty by calling <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a> to bulk process images using a schema from the Store. In our use case, we&apos;ll perform the simple task of generating alt-text.</p><blockquote>Alt text is used in HTML to describe the appearance and function of an image on a webpage. They are crucial for accessibility, as they provide a text alternative for screen readers used by visually impaired users, and also aid in search engine optimization (SEO) by allowing search engines to better understand the content of the images.</blockquote><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">You can play with the code in <a href="https://www.notion.so/SceneXplain-s-JSON-Schema-Store-Automate-Your-Alt-Text-and-More-418d90b1477f459eb0d517869dcc80cd?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">this notebook</a>. Be sure to specify your own <a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain API key</a>.</div></div><p>We won&apos;t go through each individual step in depth, since the notebook handles that. We&apos;ll just give an overview of each step. Please refer to the notebook for the real code.</p><h3 id="choose-the-schema">Choose the schema</h3><p>We&apos;ll use the alt-tagger schema that I created earlier. Be sure to note its ID! In our case that&apos;s <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-61.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="584" height="826"></figure><h3 id="test-the-api">Test the API</h3><p>We can quickly test the API with a code snippet by clicking the API tab:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/api.png" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy" width="582" height="221"></figure><p>You can access the API in Python, cURL, or JavaScript. Right now we&apos;ll use cURL since it&apos;s nice and short. We&apos;ll send the URL to the following <a href="https://www.pexels.com/photo/heron-by-the-sea-18822188/?ref=jina-ai-gmbh.ghost.io">image</a>, along with our SceneXplain key:</p><figure class="kg-card kg-image-card"><img src="https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=1260&amp;h=750&amp;dpr=1" class="kg-image" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!" loading="lazy"></figure><pre><code class="language-bash">curl &quot;https://api.scenex.jina.ai/v1/describe&quot; \
  -H &quot;x-api-key: token $YOUR_GENERATED_SECRET&quot; \
  -H &quot;content-type: application/json&quot; \
  --data &apos;{&quot;data&quot;:[
    {&quot;image&quot;: &quot;https://images.pexels.com/photos/18822188/pexels-photo-18822188/free-photo-of-heron-by-the-sea.jpeg&quot;,
    &quot;features&quot;: [&quot;json&quot;],
    &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;}
  ]}&apos;</code></pre><p>After a few seconds (and some prettification via <a href="https://github.com/jqlang/jq?ref=jina-ai-gmbh.ghost.io"><code>jq</code></a>), we get the following JSON, which includes our alt-tag:</p><pre><code class="language-json">{
  &quot;code&quot;: 200,
  &quot;status&quot;: 20000,
  &quot;result&quot;: [
    {
      &quot;id&quot;: &quot;BiU7me3ytaKn4KaF0v84&quot;,
      &quot;image&quot;: &quot;https://storage.googleapis.com/causal-diffusion.appspot.com/imagePrompts%2Fe4710bdc-73de-469c-9202-c2c0fe1073af%2Foriginal.png&quot;,
      &quot;features&quot;: [
        &quot;json&quot;
      ],
      &quot;json_schema_id&quot;: &quot;qTcJ1uVh5d7y3HLDCn0Q&quot;,
      &quot;algorithm&quot;: &quot;jelly&quot;,
      &quot;uid&quot;: &quot;NIDud1AA3NMTBFYZ4MEpNZy5om62&quot;,
      &quot;optOut&quot;: false,
      &quot;fullyOptOut&quot;: false,
      &quot;__developer_options&quot;: null,
      &quot;text&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;,
      &quot;i18n&quot;: {
        &quot;en&quot;: &quot;{\&quot;alt_tag\&quot;:\&quot;White-faced heron standing in shallow shoreline water\&quot;}&quot;
      },
      &quot;userId&quot;: &quot;foo&quot;,
      &quot;createdAt&quot;: 1700737281840,
      &quot;languages&quot;: []
    }
  ]
}</code></pre><h3 id="collect-your-data">Collect your data</h3><p>Moving forwards, we&apos;ll be using Python for our code. Assuming we have a folder of images, for each image we&apos;ll need to send:</p><ul><li>The image file converted to a <a href="https://en.wikipedia.org/wiki/Data_URI_scheme?ref=jina-ai-gmbh.ghost.io">base64-encoded datauri</a></li><li>The SceneXplain features we want to use. In our case that&apos;s just <code>[&apos;json&apos;]</code>.</li><li>The ID of the JSON Schema: <code>qTcJ1uVh5d7y3HLDCn0Q</code>.</li></ul><p>We throw all of these into a dict, then throw each image&apos;s dict into a list.</p><h3 id="send-the-data-to-scenexplain">Send the data to SceneXplain</h3><p>That&apos;s really just a case of making an HTTP request and sending over our data. We&apos;ve wrapped it into a function in the notebook.</p><h3 id="process-the-output-data">Process the output data</h3><p>This is just a case of extracting the alt-tag from the output JSON, and in our case, writing it to a CSV file then zipping it up along with all the other images. Our <code>alt-text.csv</code> looks like:</p><pre><code class="language-csv">filename,alt-tag
/tmp/tmpexs68in3/free-photo-of-leaves-on-the-branch.jpeg,&quot;Close-up of a branch with mix of green and yellow leaves, portraying the onset of autumn, set against the blurred background of a serene forest.&quot;
/tmp/tmpexs68in3/free-photo-of-holida-christmas-party-drinks-ornaments.jpeg,&quot;Vividly colored table setting with red tablecloth, two empty crystal glasses, Christmas decorations including candy canes, gold ornaments, a small Christmas tree, on a backdrop of a green curtain and pink walls.&quot;
/tmp/tmpexs68in3/free-photo-of-red.jpeg,Close-up of a red classic Malibu car&apos;s rear end.
/tmp/tmpexs68in3/free-photo-of-pose-woman-dress-in-the-desert-gold-light-curly-hair.jpeg,&quot;Curly-haired woman in brown coat standing on beach, with the sun beaming light onto her.&quot;
/tmp/tmpexs68in3/free-photo-of-a-bowl-of-granola-with-fruit-and-nuts-on-a-wooden-cutting-board.jpeg,Bowl of granola with strawberries and pomegranate seeds on a wooden board on a dark brown table
/tmp/tmpexs68in3/free-photo-of-alexandrine-parakeet-in-side-view.png,&quot;Close-up of a green parrot with a red beak and yellow eyes on a branch, looking to the right; with a blurry green background&quot;
/tmp/tmpexs68in3/pexels-photo-12015253.png,&quot;Central, old gas pump with a red and white color scheme labeled &apos;Benzin&apos; with an attached black hose against a street scene backdrop&quot;</code></pre><p>You can view the process in full in <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/a11y-alt-tags/alt-tagger.ipynb?ref=jina-ai-gmbh.ghost.io">the notebook</a>.</p><h2 id="get-started-with-scenexplain-and-the-schema-store">Get started with SceneXplain and the Schema Store</h2><p>Like what you see? Go to <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io">https://scenex.jina.ai</a> to sign up and get started, and head on over to <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io">our Discord</a> to join the conversation!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s JSON Schema Store: Automate Your Alt-Text, and More!"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Artificial General Intelligence is Cursed, And Science Fiction isn't Helping]]></title><description><![CDATA[AI is cursed by intellectual hubris, moving goalposts, bad incentives, and science fiction. Recent talk about Artificial General Intelligence only highlights those curses.]]></description><link>https://jina.ai/news/artificial-general-intelligence-is-cursed-and-science-fiction-isnt-helping/</link><guid isPermaLink="false">655b2361bb728c000101beaa</guid><category><![CDATA[Insights]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Mon, 20 Nov 2023 15:22:02 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--22-.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><p>Many, many, many, (too many <em>many</em>&#x2019;s) years ago, I was a student taking <em>Introduction to Artificial Intelligence</em> at Stanford. <a href="https://en.wikipedia.org/wiki/Daphne_Koller?ref=jina-ai-gmbh.ghost.io">Daphne Koller</a> was my teacher, and my textbook was a first edition <a href="https://aima.cs.berkeley.edu/?ref=jina-ai-gmbh.ghost.io"><em>Artificial Intelligence: A Modern Approach</em></a> by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Stuart Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Peter Norvig</a>, which still sits on the shelf in my office.</p><p>A lot of us working in AI today have that textbook on our shelves.</p><p>That&#x2019;s why a <a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io">recent article in <em>Noema Magazine</em></a> by <a href="https://en.wikipedia.org/wiki/Blaise_Ag%C3%BCera_y_Arcas?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Blaise Ag&#xFC;era y Arcas</a> of Google Research and Peter Norvig, provocatively titled <em>Artificial General Intelligence Is Already Here</em> caught so much attention.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Artificial General Intelligence Is Already Here | NOEMA</div><div class="kg-bookmark-description">Today&#x2019;s most advanced AI models have many flaws, but decades from now, they will be recognized as the first true examples of artificial general intelligence.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.noemamag.com/wp-content/uploads/2020/06/cropped-ms-icon-310x310-1-180x180.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">NOEMA</span><span class="kg-bookmark-publisher">Blaise Ag&#xFC;era y Arcas</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://noemamag.imgix.net/2023/10/Noema_Card-display-2000x1000-0-00-04-04.jpg?fit=crop&amp;fm=pjpg&amp;h=628&amp;ixlib=php-3.3.1&amp;w=1200&amp;wpsize=noema-social-facebook&amp;s=2e1793c1499718ac74749479e3989c10" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><p>To summarize their claim, the core elements of general intelligence are already met by recent large language models: They can perform with human competence at tasks outside of their explicit training over a wide range of domains. Although hardly without flaws, human performance is hardly flawless either. Therefore, we should acknowledge that these systems have achieved &#x201C;general intelligence.&#x201D;</p><p>The rest of the article is, frankly, mostly taking down strawmen and rehashing old fights from the 90s. Honestly, the people who graduated from university in the last twenty years never fought in the Great Symbolicist-Connectionist War. That conflict ended a long time ago, and any Symbolicist bitter-enders who&#x2019;ve survived into the age of GPT should be allowed to stay in their thatched jungle huts, collecting nuts and fruits and proclaiming their loyalties to Fodor, Pinker, and Chomsky somewhere far away where it doesn&#x2019;t have to bother anyone.</p><p>I fought in that conflict (for the Connectionists, thank you), and on behalf of myself and the other veterans, please give it a rest! (<a href="http://garymarcus.com/index.html?ref=jina-ai-gmbh.ghost.io">Gary Marcus</a>, I mean you too!)</p><p>&#x201C;Artificial General Intelligence&#x201D; (AGI) is, to put it mildly, a controversial concept in AI research. The word is of recent vintage, dating to about the turn of the 21st century, and comes from an attempt to make clearer an idea that has circulated in AI since, arguably, before the invention of computers. It&#x2019;s gone by a lot of names over the years: &#x201C;true AI,&#x201D; &#x201C;real AI&#x201D;, &#x201C;strong AI,&#x201D; &#x201C;human-equivalent AI,&#x201D; etc. The history of attempts to define it in some testable way is, to say the least, not encouraging, but it doesn&#x2019;t seem to stop anyone from trying.</p><h2 id="the-curses-of-ai">The Curses of AI</h2><p>Artificial intelligence is a cursed field.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="1050" height="1050" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--3--1.png 1050w" sizes="(min-width: 720px) 720px"></figure><p>The word was coined by <a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)?ref=jina-ai-gmbh.ghost.io">John McCarthy</a> in 1956, in large part to distinguish it from &#x201C;cybernetics,&#x201D; a label associated with <a href="https://en.wikipedia.org/wiki/Norbert_Wiener?ref=jina-ai-gmbh.ghost.io">Norbert Wiener</a>, who saw it as an interdisciplinary field with political implications that did not appeal to McCarthy and his colleagues. So, from the very beginning, AI has been associated with intellectual hubris and disinterest in social consequences, willfully distancing itself from the humanities &#x2014; the study of human works &#x2014; while purporting to build machines that do the work of humans.</p><p>That is one of AI&apos;s great curses: There is an expansive, argumentative, routinely acrimonious literature on the nature of intelligence and human intellectual abilities, dating back centuries and even millennia, and discussions of artificial intelligence reference it approximately never. How exactly are you going to discuss &#x201C;artificial intelligence&#x201D; if you can&#x2019;t say what it means for something to be intelligent, or how you would demonstrate that?</p><p>Ag&#xFC;era y Arcas of Google and Norvig are not especially clear on how they&#x2019;ve defined &#x201C;general intelligence&#x201D; despite having a section titled <em>&#x201C;What Is General Intelligence?&#x201D;</em></p><p>Broadly, their claim seems to be:</p><ol><li>General intelligences can do more than one kind of thing.</li><li>General intelligences can do things that their designers didn&#x2019;t specifically intend for them to be able to do.</li><li>General intelligences can do things that are specified in your instructions to them rather than their programming.</li></ol><p>This is my summary, and perhaps I have misunderstood them, but the section where they explain this goes on a number of tangents that make it hard to identify what they intend to say. They appear to be using the label &#x201C;general intelligence&#x201D; to contrast with &#x201C;narrow intelligence,&#x201D; another new term (I can&#x2019;t find reference to it before the 2010s) that seems to have the same relationship to what we used to call &#x201C;weak AI&#x201D; as &#x201C;general intelligence&#x201D; has to &#x201C;strong AI.&#x201D;</p><p>They define it as follows:</p><blockquote>Narrowly intelligent systems typically perform a single or predetermined set of tasks, for which they are explicitly trained. Even multitask learning yields only narrow intelligence because the models still operate within the confines of tasks envisioned by the engineers.</blockquote><p>They contrast this with &#x201C;general intelligence&#x201D;:</p><blockquote>By contrast, frontier language models can perform competently at pretty much any information task that can be done by humans, can be posed and answered using natural language, and has quantifiable performance.</blockquote><p>This approach poses some obvious problems. It defines artificial general intelligence to exist only when engineers have poor imaginations. That doesn&#x2019;t seem quite right.</p><p>The most frequently cited example of an AI model performing outside of its explicit training (a.k.a. <em>zero-shot learning</em>) is the &#x201C;zebra&#x201D; example: Let us say we have a multimodal AI model that has never had a picture of a zebra or mention of a zebra anywhere in its training data. So we tell it that a zebra is a horse with stripes, and then we present it with pictures, some of zebras, some not, and ask it if each one is a zebra.</p><p>The current generation of multimodal models is capable of doing a good job of this.</p><p>I don&#x2019;t know if this is exactly what Ag&#xFC;era y Arcas and Norvig have in mind as performing outside of &#x201C;the confines of tasks envisioned by the engineers&#x201D; because they give no examples, nor does the article they link to on this subject. (&#x201D;A general AI model can perform tasks the designers never&#xA0;<a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/?ref=jina-ai-gmbh.ghost.io">envisioned</a>.&#x201D;) But this kind of zero-shot learning does seem to be what they mean.</p><p>However, it&apos;s not clear that this really is an example of going outside of the tasks envisioned by the model&apos;s designers. Given that the model was explicitly trained to recognize horses and stripes and to connect the features it finds in images to natural language words and statements, it was explicitly trained to handle all parts of the problem. So is it right to say it&#x2019;s exceeded the bounds of &#x201C;narrow intelligence?&#x201D;</p><p>Other zero-shot learning examples suffer from the same problem.</p><p>Also, unexpected correct results from computers have been around for some time. As an example, <a href="https://www.jpl.nasa.gov/news/remote-agent-experiment-meets-all-objectives?ref=jina-ai-gmbh.ghost.io">this article from 1999</a> shows a logistic planner in a NASA space probe doing something that not only was correct and unexpected but that the engineers could never have had in mind when they designed it.</p><p>But there&#x2019;s an even larger problem with defining artificial general intelligence this way.</p><p>Let&#x2019;s consider the large language models, like ChatGPT. What were the tasks its engineers designed it to do? Only one task: Add the next word to a text.</p><p>The web interface to ChatGPT obscures what happens behind the scenes, but ChatGPT works like this:</p><blockquote>Iteration 1:<br>    User Input: <strong>Knock, knock.</strong> <br>    GPT Output: <strong>Who&#x2019;s</strong><br><br>Iteration 2:<br>    GPT Input: <strong>Knock, knock. Who&#x2019;s</strong> <br>    GPT Output: <strong>there</strong><br><br>Iteration 3: <br>    GPT Input: <strong>Knock, knock. Who&#x2019;s there</strong> <br>    GPT Output: <strong>?</strong><br><br>Return to user: <strong>&#x201C;Who&#x2019;s there?&#x201D;</strong></blockquote><p>That&#x2019;s all it does! You see it writing poetry, answering questions, or whatever else you like, but under the surface, it&#x2019;s a sequence of identical simple operations to append a word to a text.</p><p>We could say that ChatGPT does just one task, the one it was explicitly designed for.</p><p>So, is this a single-task ability or multi-task abilities? Is it doing more than the engineers designed it to do, or only exactly the one thing they designed it to do?</p><p>It depends on how you choose to look at it. This kind of ambiguity is not trivial to resolve.</p><p>The article highlights &#x201C;[a] healthy skepticism about metrics for AGI&#x201D; as a legitimate concern. However, the problem with &#x201C;general intelligence&#x201D; isn&#x2019;t metrics and thresholds. It&#x2019;s definitions. AI&#x2019;s lack of clear definitions of its terms is the direct cause of AI&#x2019;s second great curse: Moving goalposts.</p><p>Goalpost-moving is so central to AI as an enterprise that Professor Koller introduced me to it in her introductory class long ago, I think on, like, Week 3.</p><p>Every time researchers solve some problem in AI, it&#x2019;s redefined as not needing intelligence. As <a href="https://en.wikipedia.org/wiki/Pamela_McCorduck?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Pamela McCorduck</a> puts it in her influential history of AI:</p><blockquote>Practical AI successes, computational programs that actually achieved intelligent behavior, were soon assimilated into whatever application domain they were found to be useful, and became silent partners alongside other problem-solving approaches, which left AI researchers to deal only with the &#x201C;failures,&#x201D; the tough nuts that couldn&#x2019;t yet be cracked. Once in use, successful AI systems were simply considered valuable automatic helpers. [<a href="https://doi.org/10.1201/9780429258985?ref=jina-ai-gmbh.ghost.io">McCorduck 2004</a>]</blockquote><p>Or, in shorter words, attributed to <a href="https://en.wikipedia.org/wiki/Larry_Tesler?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Larry Tesler</a> (the inventor of copy-paste functionality):</p><blockquote>AI is whatever hasn&apos;t been done yet.</blockquote><p>AI is a <a href="https://en.wikipedia.org/wiki/Red_Queen%27s_race?ref=jina-ai-gmbh.ghost.io">Red Queen&#x2019;s Race</a> where no matter how fast you run, the finish line moves away even faster. The history of AI is a history of failure because we excise the successes from it. Never mind that AI research was already years ago bringing us decent spellcheckers, functional machine translations, and visual object recognition, among other accomplishments of modern technology.</p><p>Or that half the world today runs on logistic planners that started life in the 1960s as AI research in automated theorem proving. Yes, your &#x201C;supply chain&#x201D; is already AI from end to end. AI didn&#x2019;t get rid of the drudgery of trucking and manually loading and unloading goods. It got rid of the comfy office job of scheduling.</p><p>In the history of AI goalpost-moving, Ag&#xFC;era y Arcas and Norvig&#x2019;s contribution is somewhat novel: We&#x2019;re used to moving the goalposts of &#x201C;real&#x201D; AI to make them ever harder to reach. They&#x2019;ve moved them to say we&#x2019;ve already passed them.</p><p>The cause of the moving goalposts is more complex: AI is cursed with bad incentives. It lends itself to hype. We might blame this on the grants process and academic pressures to overpromise, or on late capitalism and the way the tech investor class likes to chase after the latest shiny thing in hopes of owning a &#x201C;disruptive&#x201D; new technology.</p><p>But I put a lot of the blame on AI&#x2019;s final curse: Science fiction.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--6-.png 700w"></figure><p>Now, understand me: This is not a criticism of the makers of science fiction; it&#x2019;s a criticism of its consumers. I have been watching and reading science fiction since I was five years old, and I love it. It&#x2019;s a popular genre, beloved by millions, if not billions, of people.</p><p>But the ideas people have in their heads about AI, including the people who do AI research, have been so thoroughly colonized by fictitious machines that it has become difficult to separate legitimate debates from the dramatic visions of novelists and movie makers.</p><p>Ideas about &#x201C;artificial general intelligence&#x201D; or &#x201C;real&#x201D; AI owe far more to science fiction than to science. It&#x2019;s not the responsibility of the Issac Asimovs, the Stanley Kubriks, or the James Camerons of the world to police their imaginations or ours. It is the responsibility of AI professionals &#x2014; researchers, marketers, investors, and corporate frontmen &#x2014; to understand and make clear that &#x201C;<a href="https://en.wikipedia.org/wiki/Positronic_brain?ref=jina-ai-gmbh.ghost.io">positronic brains</a>,&#x201D; <a href="https://en.wikipedia.org/wiki/HAL_9000?ref=jina-ai-gmbh.ghost.io">HAL 9000</a>, and <a href="https://en.wikipedia.org/wiki/The_Terminator?ref=jina-ai-gmbh.ghost.io">killer robots with inexplicable Austrian accents</a> have no place in engineering.</p><p>We move the goalposts in AI because every accomplishment ends up disappointing us when it doesn&#x2019;t live up to our vision of AI. Rational beings might interrogate their visions of AI rather than move around the goalposts and say: &#x201C;This time, it&#x2019;s different!&#x201D;</p><p>Humans have deep prejudices that they struggle to recognize. Ag&#xFC;era y Arcas and Norvig recognize this in their mention of the &#x201C;<a href="https://en.wikipedia.org/wiki/Being_There?ref=jina-ai-gmbh.ghost.io">Chauncey Gardiner effect</a>.&#x201D; For younger readers who don&#x2019;t know the works of Peter Sellers and Hal Ashby, we might also call this the &#x201C;Rita Leeds effect&#x201D; after Charlize Theron&#x2019;s role in <a href="https://en.wikipedia.org/wiki/Arrested_Development_(season_3)?ref=jina-ai-gmbh.ghost.io">season three of <em>Arrested Development</em></a>. It&#x2019;s the same schtick.</p><p>Until the 1970s, there were serious social scientists who believed that black people, at least American ones, were systematically intellectually stunted and pointed to the way they spoke English as proof. Even today, plenty of people view African American Vernacular English (usually abbreviated as AAVE because who has time to spell that out?) as &#x201C;broken&#x201D; or &#x201C;defective&#x201D; or indicative of poor education.</p><p>Among linguists, no one of any importance today thinks that. We recognize AAVE as a fully functional variant form of English, just as well-suited to accurate and effective communication as standard English or any of its other variants. It has no relationship to cognitive abilities whatsoever.</p><p>Yet, the prejudice against it remains.</p><p>When we evaluate ChatGPT as &#x201C;real&#x201D; AI, or at least on the path to &#x201C;real&#x201D; AI, we are demonstrating the same prejudice, turned around. That should give us some pause. It should lead us to ask if <em>anything</em> we think makes software intelligent is really intelligence or just manipulating our prejudices. Is there any way to objectively judge intelligence at all? We may never be able to fully dismiss our prejudices.</p><p>It does not take a lot of interaction with ChatGPT to see it fall short of our science-fiction-based visions for AI. It can do some neat tricks, maybe even some useful ones, but it&#x2019;s not <a href="https://en.wikipedia.org/wiki/WALL-E?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">WALL-E</a>, or <a href="https://en.wikipedia.org/wiki/C-3PO?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">C-3PO</a>, or <a href="https://en.wikipedia.org/wiki/Data_(Star_Trek)?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Commander Data</a>. It&#x2019;s not even Arnold Schwarzenegger&apos;s iconic but not terribly verbal T-800, although I don&#x2019;t doubt you could readily integrate ChatGPT with a text-to-speech model that has a thick Austrian accent and pretend.</p><p>Changing the labels on things doesn&#x2019;t change things, at least not in any simple way. We could move the goalposts for &#x201C;artificial general intelligence&#x201D; in such a way that we could say we&#x2019;ve already accomplished that, but that will not make today&#x2019;s AI models more satisfactory to people looking for HAL 9000. It would just render the label &#x201C;general intelligence&#x201D; meaningless.</p><p>People would still want something we can&#x2019;t give them. Something we might never be able to give them.</p><h2 id="are-humans-still-special">Are Humans Still Special?</h2><p>Among the strawmen Ag&#xFC;era y Arcas and Norvig take on in a whole section is <em>&#x201C;Human (Or Biological) Exceptionalism.&#x201D;</em> People believe all kinds of things, but unless you count religious notions of the &#x201C;soul&#x201D;, it is hard to believe this is a commonly held position among AI professionals. More likely, most of them don&#x2019;t think about it at all.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png" class="kg-image" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping" loading="lazy" width="700" height="700" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/LINES-ALBUM-COVER-LIGHT-POP-MUSIC--5-.png 700w"></figure><p>The lesson we should be learning from recent AI is that it is difficult to identify a human skill that is absolutely impossible for a machine to perform. We cannot make AI models do everything a human can do yet, but we can no longer rule anything out.</p><p>This realization should end some of the bad ideas we have inherited from the past. We know today that animals have languages much like our own and that computers can learn language with a shocking facility. Noam Chomsky may not be dead yet, but ChatGPT has put the final nails in the coffin of his life&#x2019;s work.</p><p>We have known since Galileo that humanity does not live in the center of the universe. We have known for roughly two centuries that the universe existed for billions of years before people. We have known for some time that humans are not made special by the use of reason and language, as Descartes thought, as we find both among animals and note their frequent absence in humans.</p><p>AI research is removing the idea that humans are special because of some trick we can do, but that does not remove the specialness of humanity. Consider that we celebrate people who can run a mile in under four minutes. Yet, my 20-year-old cheap French-made car can do a mile in about 30 seconds if I really put my foot on the pedal.</p><p>We care about human accomplishment in ways that differ from how we care about pure performance benchmarks. We&#x2019;ll still prefer our human performers over AI, for the same reason I don&#x2019;t win Olympic medals with my old Citro&#xEB;n.</p><p>We imagine AI taking away the drudgery of human life, but we do not yet have robots that can restock shelves at Walmart. On the other hand, we now have AI models that can sing like <a href="https://en.wikipedia.org/wiki/%C3%89dith_Piaf?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Edith Piaf</a> and, if someone really tried, could probably dance like <a href="https://en.wikipedia.org/wiki/Shakira?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Shakira </a>or play the guitar like <a href="https://en.wikipedia.org/wiki/Jimi_Hendrix?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jimi Hendrix</a>.</p><p>This is progress. It&#x2019;s an accomplishment of significant value. But it&#x2019;s not the AI we wanted. Unless we rethink the whole program of AI research, we&#x2019;ll need to move the goalposts again.</p><h2 id="real-ai-instead-of-%E2%80%9Creal%E2%80%9D-ai">Real AI instead of &#x201C;Real&#x201D; AI</h2><p>Jina AI is a business. We don&#x2019;t sell fantasy, we sell AI models. There&#x2019;s no place for &#x201C;artificial general intelligence&#x201D; in what we do. We may love science fiction, but we deal in science fact.</p><p>&#x201C;Past performance is not a guarantee of future returns.&#x201D; Maybe it&#x2019;ll be different this time. Maybe we won&#x2019;t dismiss our successes as not &#x201C;real&#x201D; intelligence. Maybe we&#x2019;ll get over our science fiction dreams for once. Maybe this hype cycle won&#x2019;t lead to another AI Winter.</p><p>Maybe, but I won&#x2019;t bet on it.</p><p>Someday, we might have something that looks more like our visions of &#x201C;real&#x201D; AI. Perhaps in 50 years? 100? History does not encourage optimism about it. But regardless, it won&#x2019;t happen in a timeframe that pays any bills.</p><p>Moving around the goalposts of AI won&#x2019;t change that.</p><p>Our interest is business cases. We want AI to add to your bottom line. At Jina AI, we want to hear about your business processes and discuss how really existing AI models can fit into them. We know the limits of this technology intimately, but we also know its genuine potential.</p><p>Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to talk about what <em>real </em>AI can do for your business.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3602 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Artificial General Intelligence is Cursed, And Science Fiction isn&apos;t Helping"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Jina AI’s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment]]></title><description><![CDATA[Jina AI launches 8K token length embedding v2 models on AWS marketplace, elevating enterprise AI deployments with EU-engineered innovation and a commitment to data sovereignty.]]></description><link>https://jina.ai/news/jina-ai-8k-embedding-models-hit-aws-marketplace-for-on-prem-deployment/</link><guid isPermaLink="false">65577ae1bb728c000101be6c</guid><category><![CDATA[Press]]></category><dc:creator><![CDATA[Jina AI]]></dc:creator><pubDate>Mon, 20 Nov 2023 14:32:57 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--17-.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><p><strong>Berlin, Germany - November 20, 2023 </strong>- Catering to enterprise customers, Jina AI has released <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io">Embeddings v2</a> on AWS SageMaker, a milestone in accessible, top-tier AI solutions. Enterprise users can now search for <code>jina-embeddings-v2-base/small</code> on the AWS Marketplace and deploy them directly to their own AWS accounts. As a part of the AWS Startups program, this release underscores the collaboration between Jina AI&apos;s innovation and AWS&apos;s commitment to supporting groundbreaking startups, marking a significant advancement in AI development.</p><figure class="kg-card kg-image-card kg-card-hascaption"><a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png" class="kg-image" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment" loading="lazy" width="2000" height="1556" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-60.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-60.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-60.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-60.png 2134w" sizes="(min-width: 720px) 720px"></a><figcaption><span style="white-space: pre-wrap;">Enterprise users can now search jina-embeddings-v2-base/small on the AWS marketplace and deploy them directly on their own AWS account.</span></figcaption></figure><h2 id="superior-models-on-a-robust-platform">Superior Models on a Robust Platform</h2><ul><li><em>SageMaker Integration</em>: With global availability on the AWS SageMaker Marketplace, Jina AI underscores its dedication to enterprise users, providing them with an effortless way to build applications using our advanced embedding models.</li><li><em>Seamless Deployment</em>: Enterprises can now easily deploy Jina Embedding v2 models as SageMaker endpoints, bypassing the complexity associated with custom infrastructure setups.</li><li><em>Cost-Effective Licensing</em>: The English <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">base</a> and <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">small</a> models are available <em>without licensing fees</em>. Clients incur costs only for their AWS instances, ensuring a privacy-first, cost-effective solution within their VPC.</li></ul><h3 id="tailored-solutions-for-varied-use-cases">Tailored Solutions for Varied Use Cases</h3><ul><li><em>Model Diversity</em>: With a <a href="https://aws.amazon.com/marketplace/pp/prodview-egsf72y46ursa?ref=jina-ai-gmbh.ghost.io">0.27GB base model</a> and a <a href="https://aws.amazon.com/marketplace/pp/prodview-jwbhofu3iesos?ref=jina-ai-gmbh.ghost.io">0.07GB small model</a>, Jina AI provides tailored solutions for various needs, from in-depth analytics to lightweight applications.</li><li><em>Use Cases</em>: The base model is designed for comprehensive semantic representation, ideal for enterprise search and content discovery, while the small model caters to mobile and edge devices, optimizing for speed and efficiency.</li></ul><p>Commenting on this significant milestone, Dr. Han Xiao, CEO of Jina AI, offered the following insights:</p><blockquote>Launching Jina AI&apos;s 8K Context Length v2 Embedding Models on AWS Marketplace, we advance industry standards for private AI solutions. Developed in Germany, this pivotal release emphasizes data sovereignty and customer-centric innovation, addressing today&apos;s needs and shaping future secure, private AI deployments.</blockquote><p>Jina AI aims to make continuous strides towards privacy-aware, state-of-the-art AI, as evident from its plans.  </p><h2 id="why-jina-embeddings-v2-a-leap-in-ai-capability">Why Jina Embeddings v2: A Leap in AI Capability</h2><ul><li><em>Extended Context Length</em>: Jina Embeddings v2 models support an <a href="https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io">unprecedented 8K</a> (8192 tokens) context length, allowing for a full understanding of longer documents.</li><li><em>Open Source Pioneer</em>: Jina AI takes pride in offering the only open-source model with a context length that matches OpenAI&#x2019;s proprietary models, broadening access to advanced AI.</li><li><em>Benchmark Leadership</em>: On the Massive Text Embedding Benchmark (MTEB) <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io">leaderboard</a>, our models boast performance on par with industry-leading models, attesting to our commitment to excellence.</li></ul><h3 id="performance-vs-openais-text-embedding-ada002">Performance vs. OpenAI&apos;s text-embedding-ada002</h3><p>Below is a comparative performance snapshot that showcases the robust capabilities of Jina Embeddings v2:</p>
<!--kg-card-begin: html-->
<table id="e6f57e0d-fe4c-414f-8c0f-5081d4ad007d" class="simple-table"><tbody><tr id="29fae3d0-ca66-462f-8966-b3c1b74c9a86"><td id="bT&lt;r" class>Model</td><td id="hcDn" class>text-embedding-ada-002</td><td id="gMgn" class>jina-embeddings-v2-base-en</td></tr><tr id="5cff5519-ba19-40a8-9900-4488689c0a84"><td id="bT&lt;r" class>Rank</td><td id="hcDn" class><strong>18</strong></td><td id="gMgn" class>21</td></tr><tr id="f01a5da8-92d0-4a76-8e1c-8fb77c9ac6c8"><td id="bT&lt;r" class>Model Size (GB)</td><td id="hcDn" class>Unknown</td><td id="gMgn" class>0.27</td></tr><tr id="56cc4ec4-a35e-44f4-8f2a-5243a8c4331b"><td id="bT&lt;r" class>Average (56 datasets)</td><td id="hcDn" class><strong>60.99</strong></td><td id="gMgn" class>60.38</td></tr><tr id="ca2d0ce9-9157-46c5-a9c7-9179edea43ee"><td id="bT&lt;r" class>Embedding Dimensions</td><td id="hcDn" class>1536</td><td id="gMgn" class><strong>768</strong></td></tr><tr id="8c7ecb47-1d30-4c32-8a23-89dadfbbfc38"><td id="bT&lt;r" class>Sequence Length</td><td id="hcDn" class>8191</td><td id="gMgn" class>8192</td></tr><tr id="a0398e1a-3144-4ec4-bb36-8768488b3d8a"><td id="bT&lt;r" class>Classification Average (12 datasets)</td><td id="hcDn" class>70.93</td><td id="gMgn" class><strong>73.45</strong></td></tr><tr id="d5a7ccfb-7769-4d58-bad4-a219481876ff"><td id="bT&lt;r" class>Pair Classification Average (3 datasets)</td><td id="hcDn" class>84.89</td><td id="gMgn" class><strong>85.38</strong></td></tr><tr id="461eb054-e39a-4ec5-8c33-cc880cb9f2ac"><td id="bT&lt;r" class>Summarization Average (1 dataset)</td><td id="hcDn" class>30.8</td><td id="gMgn" class><strong>31.6</strong></td></tr><tr id="5f55e5e5-a6dc-4a4a-b67d-4ffed0084989"><td id="bT&lt;r" class>Retrieval Average (15 datasets)</td><td id="hcDn" class><strong>49.25</strong></td><td id="gMgn" class>47.87</td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Jina AI&apos;s base model excels particularly in Classification and Pair Classification tasks, underscoring its value in diverse applications ranging from document analysis to recommendation systems.</p><h2 id="get-started-with-jina-embeddings-v2-on-aws">Get Started with Jina Embeddings v2 on AWS</h2><p>To begin using Jina Embeddings v2, visit the <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io">AWS Marketplace listings</a> and select the model that best fits your needs.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&amp;ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">AWS Marketplace: Jina AI</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></div><div class="kg-bookmark-thumbnail"><img src="https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a></figure><p>These sample notebooks can help users get started with Jina Embeddings v2 models:</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Real-time%20inference.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Real-time inference.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Real-time inference on SageMaker</span></p></figcaption></figure><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Batch%20transform.ipynb?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-sagemaker/notebooks/Batch transform.ipynb at main &#xB7; jina-ai/jina-sagemaker</div><div class="kg-bookmark-description">Jina Embedding Models on AWS SageMaker. Contribute to jina-ai/jina-sagemaker development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/91499715ca0bec59273499acfddecae62d0a3833146f7ceaf9968656007b16f0/jina-ai/jina-sagemaker" alt="Jina AI&#x2019;s 8K Embedding Models Hit AWS Marketplace for On-Prem Deployment"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Embedding multiple sentences in a batch with SageMaker</span></p></figcaption></figure><h2 id="coming-soon-multilingual-embeddings-and-more">Coming Soon: Multilingual Embeddings and More</h2><p>Looking ahead, Jina AI is already deep in developing multilingual embedding models, making them available to its enterprise clients for private deployment on various cloud service providers (CSPs). With the imminent launch of these models, Jina AI is set to bridge language barriers, unlocking global opportunities for its clients.</p><h2 id="about-jina-ai-gmbh">About Jina AI GmbH</h2><p>Located at <a href="https://maps.app.goo.gl/rAZ1QiqoKp49KZAg6?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany</a>, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at&#xA0;<a>contact@jina.ai</a>.<br></p>]]></content:encoded></item><item><title><![CDATA[Jina 3.23.0 Update]]></title><description><![CDATA[Jina is a MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud.]]></description><link>https://jina.ai/news/jina-3-23-0-update/</link><guid isPermaLink="false">655b2af0bb728c000101bee8</guid><category><![CDATA[Releases]]></category><dc:creator><![CDATA[Engineering Group]]></dc:creator><pubDate>Mon, 20 Nov 2023 10:20:23 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Image-Jina-dark.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="release-note-3230">Release Note (<code>3.23.0</code>)</h2><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Image-Jina-dark.jpg" alt="Jina 3.23.0 Update"><p>This release contains 1 new feature and 1 bug fix.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/jina-ai/jina/releases/tag/v3.23.0?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Release &#x1F4AB; Patch v3.23.0 &#xB7; jina-ai/jina</div><div class="kg-bookmark-description">Release Note (3.23.0) Release time: 2023-11-20 10:02:37 This release contains 1 new feature and 1 bug fix.
&#x1F195; Features
Call extend rest interface in worker (#6108)
When running a Deployment with H&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" alt="Jina 3.23.0 Update"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">jina-ai</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/1e01faf8fa1f5ed07a44184f9998e6cc26cea7da380da0ed5c663d6859928bca/jina-ai/jina/releases/tag/v3.23.0" alt="Jina 3.23.0 Update"></div></a></figure><h2 id="%F0%9F%86%95-features">&#x1F195; Features</h2><h3 id="call-extend-rest-interface-in-worker-6108">Call extend rest interface in worker (<a href="https://github.com/jina-ai/jina/pull/6108?ref=jina-ai-gmbh.ghost.io">#6108</a>)</h3><p>When running a&#xA0;<code>Deployment</code>&#xA0;with HTTP locally without&#xA0;<code>docker</code>&#xA0;you can now extend the API by using&#xA0;<code>extend_rest_interface</code>, the same as with&#xA0;<code>Flow</code>.</p><pre><code class="language-Python">    import jina
    from jina import Deployment
    def extend_rest_function(app):
        @app.get(&apos;/hello&apos;, tags=[&apos;My Extended APIs&apos;])
        async def foo():
            return {&apos;msg&apos;: &apos;hello world&apos;}

        return app

    jina.helper.extend_rest_interface = extend_rest_function
    d = Deployment(protocol=&apos;http&apos;, port=8080)

    with d:
        response = requests.get(f&apos;http://localhost:8080/hello&apos;)
        assert response.status_code == 200
        assert response.json() == {&apos;msg&apos;: &apos;hello world&apos;}</code></pre><h2 id="%F0%9F%90%9E-bug-fixes">&#x1F41E; Bug Fixes</h2><h3 id="read-csv-with-escaped-chars-6102">Read CSV with escaped chars (<a href="https://github.com/jina-ai/jina/pull/6102?ref=jina-ai-gmbh.ghost.io">#6102</a>)</h3><p>There was an issue when using Jina to do batch transformations on SageMaker. Parsing texts with escape characters was not working properly.</p><h2 id="%F0%9F%A4%9F-contributors">&#x1F91F; Contributors</h2><p>We would like to thank all contributors to this release:</p><ul><li>Joan Fontanals Martinez (<a href="https://github.com/JoanFM?ref=jina-ai-gmbh.ghost.io">@JoanFM</a>)</li><li>Deepankar Mahapatro (<a href="https://github.com/deepankarm?ref=jina-ai-gmbh.ghost.io">@deepankarm</a>)</li></ul>]]></content:encoded></item><item><title><![CDATA[SceneXplain's OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!]]></title><description><![CDATA[SceneXplain beats GPT-4V by going beyond English, offering multilingual OCR for Chinese, Japanese, Korean, Arabic, Hindi and more]]></description><link>https://jina.ai/news/scenexplains-ocr-beats-gpt-4v-hands-down-chinese-japanese-korean-arabic-hindi-and-more/</link><guid isPermaLink="false">65537bd2c2c1f30001ea4816</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Thu, 16 Nov 2023 15:00:20 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/sensex--3-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/sensex--3-.jpg" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><p>There&apos;s been a lot of noise recently about the release of GPT-4V - a model that can analyze the content of images and summarize that into human language. Sound familiar? It may, because SceneXplain has been doing just that for months already!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><p>But just because you take an early lead doesn&apos;t mean you win the race. We&apos;ve got to show how we&apos;re better, right? Well, we&apos;ve already done that in <a href="https://jina.ai/news/scenexplain-vs-gpt-4-vision-the-best-image-captioning-tool-in-2023/?ref=jina-ai-gmbh.ghost.io">this post</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/scenexplain-vs-gpt-4-vision-the-best-image-captioning-tool-in-2023/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023?</div><div class="kg-bookmark-description">Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-publisher">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/09/Explore-image-storytelling-beyond-pixels--9-.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><p>But in this post, we&apos;ll go one further and show you how we excel at OCR for multilingual texts. In short, <strong>SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V</strong>. Also, it doesn&#x2019;t have any pesky daily request limits.</p><p>You might think GPT-4V is pretty decent if you&#x2019;re a user of Latin-character languages. Since you&#x2019;re reading this (I assume in English), that means you!</p><p>Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies:</p><ul><li><a href="https://blog.roboflow.com/gpt-4-vision/?ref=jina-ai-gmbh.ghost.io">First Impressions with GPT-4V(ision)</a></li><li><a href="https://arxiv.org/abs/2310.16809?ref=jina-ai-gmbh.ghost.io">Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</a></li></ul><p>But, as mentioned in the study:</p><blockquote class="kg-blockquote-alt">There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V.</blockquote><p>Bad news - it&apos;s not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts.</p><p>That&apos;s where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.)</p><h2 id="tldr-how-does-gpt-4v-fail-compared-to-scenexplain">TL;DR: How does GPT-4V fail compared to SceneXplain?</h2><p>In short:</p><ul><li><strong>API limits:</strong> 100 requests a day is the new &#x201C;<a href="https://quoteinvestigator.com/2011/09/08/640k-enough/?ref=jina-ai-gmbh.ghost.io">640K ought to be enough for anybody</a>&#x201D;. SceneXplain has a <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing">range of plans</a> to suit your needs.</li><li><strong>Reliability:</strong> Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it can&#x2019;t do it. SceneXplain gets it done right every time.</li><li><strong>Accuracy:</strong> GPT-4V will often extract strings that are <strong>plausible</strong> for a given image, but not the <strong>actual</strong> strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. <a href="https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead/?utm_source=PromptPerfect+Newsletter&amp;utm_campaign=ce1beac0b3-EMAIL_CAMPAIGN_2023_09_29_10_25_COPY_02&amp;utm_medium=email&amp;utm_term=0_fada173e11-6efedfdbc3-%5BLIST_EMAIL_ID%5D">SceneXplain pretty much cured hallucinations in its Jelly algorithm</a>.</li><li><strong>Consistency:</strong> GPT-4V doesn&#x2019;t just hallucinate. It hallucinates different things each time. SceneXplain is generally consistent.</li><li><strong>Completeness:</strong> GPT-4V doesn&#x2019;t always extract <strong><em>all</em></strong> the strings in the image. SceneXplain is more comprehensive.</li><li><strong>Cost:</strong> SceneXplain is up to 50% more affordable.</li></ul><p>Let&#x2019;s dive into each point in more detail.</p><h3 id="api-limits">API limits</h3><p>Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would&apos;ve put this post out earlier if we hadn&apos;t easily overshot that multiple times.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-29.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1435" height="71" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-29.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-29.png 1435w" sizes="(min-width: 1200px) 1200px"><figcaption><span style="white-space: pre-wrap;">Hello </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RateLimitError</span></code><span style="white-space: pre-wrap;">, my old friend.</span></figcaption></figure><h3 id="reliability">Reliability</h3><p>If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/qt1pq58r827iyyvugjfr.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1200" height="675" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/qt1pq58r827iyyvugjfr.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/qt1pq58r827iyyvugjfr.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/qt1pq58r827iyyvugjfr.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>We can extract the strings as JSON by using the prompt:</p><pre><code class="language-text">Extract the text strings from this image and populate a JSON that follows this schema:

{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;text_strings&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Every text string contained in the image. Consider all languages&quot;
    }
  }
}

Return just the output JSON. Do not put it in a code block
</code></pre><p>Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like &#x201C;Here is your JSON&#x201D; and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content.</p><p>It returns this output:</p><pre><code class="language-json">{
&quot;text_strings&quot;: [
&quot;YOU CAN&apos;T RUN&quot;,
&quot;FROM US FOREVER&quot;,
&quot;m&quot;
]
}
</code></pre><p>I would&#x2019;ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns:</p><figure class="kg-card kg-code-card"><pre><code class="language-json">{
  &quot;text_strings&quot;: [
    &quot;YOU CAN&apos;T RUN&quot;,
    &quot;FROM US FOREVER&quot;,
    &quot;M&quot;
  ]
}</code></pre><figcaption><p><span style="white-space: pre-wrap;">Ah, those sweet sweet indents</span></p></figcaption></figure><p>Now let&#x2019;s try asking in natural language about a Chinese ad:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/McDonald-EN-CHINE.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1329" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/McDonald-EN-CHINE.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/McDonald-EN-CHINE.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/McDonald-EN-CHINE.jpeg 1329w" sizes="(min-width: 720px) 720px"></figure><p>If we use the prompt: <code>Extract the text strings from this image</code>, GPT-4V gives us:</p><pre><code class="language-text">It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted.

Given this issue, I&apos;m unable to extract the text from the image as requested. If there&apos;s anything else you would like me to assist you with, please let me know!
</code></pre><p>That&#x2019;s&#x2026;not useful. <strong><em>However</em></strong>, asking it to extract it as JSON (by using the above prompt with schema) works:</p><pre><code class="language-json">{
&quot;text_strings&quot;: [
&quot;&#x9EA6;&#x5F53;&#x52B3;&quot;,
&quot;&#x4E00;&#x5468;&#x4E03;&#x5929;&quot;,
&quot;&#x65E9;&#x6668;&#x56DB;&#x70B9;&quot;,
&quot;&#x81F3;&quot;,
&quot;17:00 &#x65F6;&#x6BB5;&quot;,
&quot;&#x5468;&#x672B;&#x65E9;&#x9910; &#x5EF6;&#x65F6;&quot;,
&quot;&#x66F4;&#x591A;&#x60CA;&#x559C;&quot;,
&quot;&#x5C3D;&#x5728;&#x9EA6;&#x5F53;&#x52B3;APP&quot;
]
}
</code></pre><p>Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows.</p><h3 id="accuracy">Accuracy</h3><p>Take another look at that JSON output. Then, take a look at the ad.</p><p>At first glance, the JSON looks decent. It&#x2019;s got the <code>17:00</code> and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that <em>could</em> occur in a Chinese McDonald&apos;s ad, but none of them are actually in that particular ad.</p><p>This can be even more dangerous than being wrong - enough people are already worried about <a href="https://futurism.com/artificial-intelligence-automated-mansplaining-machine?ref=jina-ai-gmbh.ghost.io">GPT mansplaining</a> - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too.</p><p>Also, why does it suddenly think it can extract the text when it stated before that it couldn&#x2019;t?</p><p>For reference, SceneXplain returns:</p><pre><code class="language-json">{
  &quot;text_strings&quot;: [
    &quot;&#x9EA6;&#x9EA6;&#x591C;&#x5E02;&quot;,
    &quot;&#x51FA;&#x795E;&#x5364;&#x5473;&#x9E21;&#x67B6;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x62CC;&#x9E21;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x9178;&#x8FA3;&#x9E21;&quot;,
    &quot;&#x9752;&#x82B1;&#x6912;&#x534A;&#x9E21;&quot;,
    &quot;&#x9EA6;&#x9EA6;&#x591C;&#x5E02;&#x5C0F;&#x5821;&quot;,
    &quot;&#x6697;&#x7CD6;&#x5E26;&#x52B2;&#x5821;&quot;,
    &quot;&#x552F;&#x9187;&#x5E26;&#x52B2;&#x5821;&quot;,
    &quot;&#x83B1;&#x83B1;&#x771F;&#x9999;&#x5821;&quot;,
    &quot;17:00&#x8425;&#x591C;&quot;,
    &quot;&#x54C7;&#x85D5;&#x5E26;&#x52B2;&#x5821;&quot;
  ]
}
</code></pre><p>If you compare the strings, you&#x2019;ll see SceneXplain is much more accurate and hallucinates less.</p><h3 id="consistency">Consistency</h3><p>Coming back to our old friend, the Chinese McDonald&#x2019;s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here&apos;s what we get:</p><pre><code>{
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;text_strings&quot;: [
&quot;&#x9EA6;&#x8FA3;&#x9E21;&#x7FC5;&quot;,
&quot;&#x5821;&#x5821;&#x53CC;&#x62FC; &#x7B49;&#x4F60;&#x6765;&#x64A9;&quot;,
&quot;&#x4E00;&#x4EBA;&#x98DF;&#x597D;&#x798F;&#x5229;&quot;,
&quot;17:00&#x524D;&#x6765;&quot;,
&quot;&#x9EA6;&#x5F53;&#x52B3;&quot;,
&quot;&#x5C0A;&#x4EAB;&#x8F6F;&#x6B27;&#x5305;&quot;,
&quot;&#x5468;&#x8FB9;&#x597D;&#x8D27; &#x7B49;&#x4F60;&quot;,
&quot;&#x5C0A;&#x4EAB;&#x8F6F;&#x6B27;&#x5305;&quot;
]
}
}
</code></pre><p>Well dang. Not only is the text entirely different, but it&#x2019;s also sent us JSON in a completely different format, something like the <strong>schema</strong> we sent, rather than the <em>output format</em> that the schema defines.</p><h3 id="completeness">Completeness</h3><p>If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="624" height="808" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/d46l5mu-22863ee5-ff85-4f18-ac95-2a76e16e9f77.jpg 624w"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x645;&#x639;&#x62A;&#x648;&#x642; MAATOUK&quot;,
&quot;1960&quot;,
&quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
&quot;&#x62D;&#x628;&#x64A;&#x628;&#x629; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;&quot;,
&quot;&#x62A;&#x642;&#x62F;&#x645;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
&quot;&#x623;&#x635;&#x627;&#x644;&#x629; &#x627;&#x645;&#x62A;&#x62F;&#x62A; &#x648;&#x633;&#x637;&#x648;&#x644;&#x629; &#x627;&#x644;&#x646;&#x636;&#x62C;&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x645;&#x639;&#x62A;&#x648;&#x642;&quot;,
    &quot;MAATOUK&quot;,
    &quot;1 9 6 0&quot;,
    &quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
    &quot;&#x62D;&#x633;&#x628; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;&quot;,
    &quot;&#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629;&quot;,
    &quot;&#x645;&#x639;&#x62A;&#x648;&#x642;&quot;,
    &quot;mAATOUK&quot;,
    &quot;&#x62A;&#x62D;&#x645;&#x64A;&#x635; &#x63A;&#x627;&#x645;&#x642;&quot;,
    &quot;&#x628;&#x646; &#x645;&#x637;&#x62D;&#x648;&#x646; &#x661;&#x660;&#x660;&#x66A; &#x623;&#x631;&#x627;&#x628;&#x64A;&#x643;&#x627;&quot;,
    &quot;&#x623;&#x635;&#x627;&#x644;&#x629; &#x627;&#x644;&#x645;&#x630;&#x627;&#x642; &#x648;&#x633;&#x647;&#x648;&#x644;&#x629; &#x627;&#x644;&#x62A;&#x62D;&#x636;&#x64A;&#x631;&quot;,
    &quot;&#x644;&#x637;&#x627;&#x644;&#x645;&#x627; &#x627;&#x634;&#x62A;&#x647;&#x631; &#x627;&#x644;&#x639;&#x631;&#x628; &#x639;&#x628;&#x631; &#x645;&#x627;&#x636;&#x64A; &#x627;&#x644;&#x632;&#x645;&#x627;&#x646; &#x628;&#x62A;&#x62D;&#x636;&#x64A;&#x631; &#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x641;&#x64A; &#x645;&#x646;&#x627;&#x632;&#x644;&#x647;&#x645;&#x60C; &#x648;&#x627;&#x644;&#x62A;&#x64A; &#x62A;&#x639;&#x62F; &#x645;&#x646; &#x623;&#x62D;&#x62F; &#x631;&#x645;&#x648;&#x632; &#x627;&#x644;&#x643;&#x631;&#x645; &#x648;&#x627;&#x644;&#x636;&#x64A;&#x627;&#x641;&#x629;&quot;,
    &quot;&#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629;. &#x645;&#x646; &#x647;&#x646;&#x627; &#x627;&#x646;&#x637;&#x644;&#x642;&#x62A; &#x645;&#x635;&#x627;&#x646;&#x639; &#x645;&#x639;&#x62A;&#x648;&#x642; &#x644;&#x62A;&#x642;&#x62F;&#x64A;&#x645; &#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x627;&#x644;&#x633;&#x647;&#x644;&#x629; &#x627;&#x644;&#x62A;&#x62D;&#x636;&#x64A;&#x631; &#x630;&#x627;&#x62A; &#x627;&#x644;&#x631;&#x627;&#x626;&#x62D;&#x629; &#x627;&#x644;&#x645;&#x645;&#x64A;&#x632;&#x629; &#x648;&#x627;&#x644;&#x637;&#x639;&#x645; &#x627;&#x644;&#x623;&#x635;&#x64A;&#x644;.&quot;,
    &quot;&#x627;&#x644;&#x642;&#x647;&#x648;&#x629; &#x627;&#x644;&#x639;&#x631;&#x628;&#x64A;&#x629; &#x645;&#x639;&#x62A;&#x648;&#x642; &#x661;&#x669;&#x666;&#x660;&#x60C; &#x642;&#x647;&#x648;&#x629; &#x639;&#x631;&#x628;&#x64A;&#x629; &#x62D;&#x633;&#x628; &#x627;&#x644;&#x623;&#x635;&#x648;&#x644;.&quot;
  ]
</code></pre><h3 id="cost">Cost</h3><p>At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplain&#x2019;s MAX plan you pay less than half of that. You can find out more on our <a href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing">pricing page</a>:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io#pricing"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure><h2 id="head-to-head-gpt-4v-vs-scenexplain">Head to head: GPT-4V vs SceneXplain</h2><p>Let&#x2019;s put both services to the test and see who comes out on top.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">See our <a href="https://colab.research.google.com/github/alexcg1/notebooks/blob/main/scenex/scenex-gpt4v?ref=jina-ai-gmbh.ghost.io">notebook</a> for the full range of testing and results.</div></div><h3 id="testing-methodology">Testing methodology</h3><p>We took a selection of images, some from <a href="http://pexels.com/?ref=jina-ai-gmbh.ghost.io">pexels.com</a> and some from searching Google Images for advertisements in the given languages:</p><ul><li>Chinese (we didn&apos;t differentiate between Simplified and Traditional)</li><li>Japanese</li><li>Korean</li><li>Arabic</li><li>Hindi</li></ul><p>We then used:</p><ul><li><a href="https://scenex.jina.ai/api?ref=jina-ai-gmbh.ghost.io">SceneXplain&apos;s API</a> (using the <a href="https://jina.ai/news/are-you-ready-for-this-jelly-scenexplains-new-algo-kills-hallucinations-dead?ref=jina-ai-gmbh.ghost.io">Jelly algorithm</a>)</li><li>A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit)</li></ul><p>We ran several rounds of tests:</p><ul><li><strong>Basic image description:</strong> For SceneXplain, we just uploaded the image. For GPT-4V, we asked: <code>What is in the image?</code></li><li><strong>Visual question answering:</strong> We uploaded the image to both services and asked: <code>What does the text say in this image?</code></li><li><strong>JSON output:</strong> For SceneXplain, we simply used the <a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision?ref=jina-ai-gmbh.ghost.io">&quot;Extract JSON from image&quot;</a> feature and a predefined <a href="https://json-schema.org/?ref=jina-ai-gmbh.ghost.io">JSON Schema</a>.</li></ul><p>The JSON Schema was as follows:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;text_strings&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Every text string contained in the image. Consider all languages&quot;
    }
  }
}</code></pre><p>Since GPT-4V doesn&apos;t directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text:</p><pre><code class="language-text">Extract the text strings from this image and populate a JSON that follows this schema:

&lt;JSON Schema from above&gt;

Return just the output JSON. Do not put it in a code block&quot;</code></pre><p>The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">To save space, we&#x2019;ve simplified the JSON output to include just the text strings, not the rest of the JSON structure.</div></div><h3 id="chinese">Chinese</h3><p>We&#x2019;ve already dived into the McDonald&#x2019;s ad above, so we&#x2019;ll just look at one more Chinese image:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-2670327.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="2000" height="3000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/pexels-photo-2670327.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/pexels-photo-2670327.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/pexels-photo-2670327.jpeg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/pexels-photo-2670327.jpeg 2400w" sizes="(min-width: 720px) 720px"></figure><ul><li>SceneXplain output:</li></ul><pre><code class="language-json">[
  &quot;&#x91D1;&#x94B1;&#x809A;20&#x5143;&quot;,
  &quot;&#x65FA;&#x89D2;&#x725B;&#x7B4B;&#x8169;20&#x5143;&quot;,
  &quot;&#x65FA;&#x89D2;&#x725B;&#x6742;18&#x5143;&quot;
]</code></pre><ul><li>GPT-4V output:</li></ul><pre><code class="language-json">[
  &quot;&#x7AF9;&#x7B74;&#x9B5A;&quot;,
  &quot;20&#x5143;&quot;, 
  &quot;&#x751C;&#x751C;&#x5708;&quot;,
  &quot;20&#x5143;&quot;,
  &quot;&#x751C;&#x751C;&#x4ED9;&#x8C9D;&quot;,
  &quot;18&#x5143;&quot;
]</code></pre><p>Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices.</p><h3 id="japanese">Japanese</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/21d10241ed74414d8263d56330cd5d02.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="400" height="300"></figure><ul><li>GPT-4V gets some of the characters correct in the cigarette ad. The string <code>&#x79C1;&#x306F;&#x305F;&#x3060;&#x5438;&#x3044;&#x6BBB;&#x306B;&#x306A;&#x308A;&#x307E;&#x3059;</code> has most of the same characters but is still a bit off. Other strings are just hallucinated.</li></ul><pre><code class="language-json">[
&quot;&#x79C1;&#x306F;&#x305F;&#x3060;&#x5438;&#x3044;&#x6BBB;&#x306B;&#x306A;&#x308A;&#x307E;&#x3059;&quot;,
&quot;PLAISIR&quot;,
&quot;&#x7D14;&#x6B63;&quot;,
&quot;MICRONITE&quot;,
&quot;&#x65B0;&#x5F0F;&quot;,
&quot;&#x3059;&#x3079;&#x3066;&#x306E;&#x5473;&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x79C1;&#x306F;&#x305F;&#x3070;&#x3053;&#x3092;&#x5438;&#x3044;&#x307E;&#x3059;&quot;,
    &quot;PLAISIR&quot;,
    &quot;&#x72EC;&#x5360;&quot;,
    &quot;MICRONITE&quot;,
    &quot;&#x65B0;&#x305F;&quot;,
    &quot;&#x30B7;&#x30AC;&#x30EC;&#x30C3;&#x30C8;&quot;,
    &quot;&#x559C;&#x3073;&#x306E;&#x5473;&quot;,
    &quot;KING SIZE&quot;
  ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/poster.png" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="897" height="1390" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/poster.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/poster.png 897w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V: After extracting (wrong) text from Chinese and Japanese images, we uploaded another image with the prompt <code>Now do this one</code> (which we&#x2019;d used successfully to repeat prior commands). GPT-4V seems to have forgotten how. We got the output:</li></ul><pre><code class="language-text">I&apos;m sorry, but I can&apos;t assist with identifying or making statements about text within images. If you have any other questions or need assistance with a different task, feel free to ask!
</code></pre><ul><li>SceneXplain did its duty as expected:</li></ul><pre><code class="language-json">[
    &quot;&#x7528;&#x670D;&#x3050;&#x76F4;&#x305A;&#x305B;&#x8E87;&#x9DF9;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3;&quot;,
    &quot;&#x8A55;&#x5224;&#x306E;&#x6551;&#x6025;&#x85AC;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#x30EB;&#x30F3;&#x30C0;&quot;,
    &quot;&#x80BA;&#x708E;&#x306B;&quot;,
    &quot;&#x65E5;&#x82F1;&#x7C73; &#x88FD;&#x6CD5;&#x7279;&#x8A31;)&quot;,
    &quot;ANTI-LUMGEN&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#x30EB;&#x30D0;&#x30B2;&#x30CE;&quot;,
    &quot;&#x6771;&#x4EAC;&#x9A8C;&#x85AC;&#x682A;&#x5F0F;&#x4F1A;&#x793E;&quot;,
    &quot;&#x5448;&#x8D08;&#x737B;&#x6587;&quot;,
    &quot;(&#x91D1;&#x79D1;&#x767E;&#x8CA8;&#x5E97;&#x53CA;&#x5404;&#x9EB5;&#x5E97; &#x6709;&#x4E09;)&quot;,
    &quot;&#x3042;&#x308A;&#x304B;&#x305B;&#x3093;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x3055;&#x304C;&#x5E74;&#x4EE5;&#x4E0A;&#x4FDD;&#x5B58;&#x3057;&#x3066;&#x3082;&#x52B9;&#x76EE;&#x306B;&#x8B8A;&#x5316;&#x306F;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x4F55;&#x7B49;&#x671F;&#x4F5C;&#x54C1;&#x306E;&#x306A;&#x3044;&#x5B89;&#x5168;&#x306A;&#x5185;&#x670D;&#x85AC;&#x3067;&#x306D;&quot;,
    &quot;&#x672C;&#x5264;&#x306F;&#x4ED6;&#x306E;&#x91AB;&#x85E5;&#x3068;&#x4F75;&#x7528;&#x3057;&#x3066;&#x5DEE;&#x652F;&#x3078;&#x3042;&#x308A;&#x307E;&#x305B;&#x3093;&quot;,
    &quot;&#x306E;&#x5FC5;&#x8981;&#x306F;&#x3042;&#x308A;&#x307E;&#x305B;&#x3093;&quot;,
    &quot;&#x672C;&#x5291;&#x306F;&#x300C;&#x53EA;&#x4E00;&#x56DE;&#x5206;&#x300D;&#x306E;&#x307F;&#x3066;&#x7D50;&#x69CB;&#x3067;&#x3059;&#x6218;&#x7248;&quot;,
    &quot;&#x3089;&#x5C02;&#x9580;&#x5BB6;&#x306B;&#x671F;&#x76F8;&#x8AC7;&#x4E0B;&#x3055;&#x3044;&quot;,
    &quot;&#x8C61;&#x306E;&#x8D85;&#x3089;&#x306A;&#x3044;&#x6642;&#x306F;&#x4E7E;&#x708E;&#x3067;&#x306A;&#x3044;&#x3068;&#x601D;&#x3072;&#x307E;&#x3059;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3; &#x3092;&#x670D;&#x7528;&#x3057;&#x307E;&#x3059;&#x3068;&#x5FC5;&#x305A;&#x53F3;&#x306E;&quot;,
    &quot;&#x672C;&#x5291; &#x306E; &#x7279; &#x9577;&quot;,
    &quot;&#x308B;&#x306E;&#x3067;&#x3042;&#x308A;&#x307E;&#x3057;&#x3066;&#x5B89;&#x7720;&#x3088;&#x308A;&#x89BA;&#x3081;&#x308B;&#x6642;&#x306B;&#x306F;&#x5FB3;&quot;,
    &quot;&#x6B64;&#x306E;&#x7279;&#x6B8A;&#x73FE;&#x8C61; &#x767C;&#x6C57;&#x5B89;&#x7720; &#x3053;&#x305D;&#x85AC;&#x52B9;&#x3067;&#x64B2;&#x6EC5;&quot;,
    &quot;&#x3068;&#x3082;&#x5FEB;&#x4E0A;&#x3052;&#x306B;&#x9577;&#x6642;&#x9593;&#x5B89;&#x7720;&#x81F4;&#x3057;&#x307E;&#x3059;&quot;,
    &quot;&#x767C;&#x6C57;&#x3057;&#x5176;&#x767C;&#x6C57;&#x304C;&#x76F8;&#x4E4B;&#x6559;&#x751F;&#x308B;&#x9803;&#x3088;&#x308A;&#x5FE0;&#x8005;&#x306F;&#x3044;&quot;,
    &quot;&#x30A2;&#x30F3;&#x30C1;&#xB7;&#x30EB;&#x30F3;&#x30B2;&#x30F3; &#x3092;&#x670D;&#x7528;&#x3059;&#x308B;&#x3068;&#x767D;&#x8840;&#x7403;&#x306E;&#x306F;&quot;,
    &quot;&#x6CBB; &#x7642; &#x306E; &#x9435; &#x5247;&quot;,
    &quot;&#x5982;&#x4E16;&#x69AE;79&#x767C;&#x97F3;&#x5143;&#x4E2D;&#x5C71;&quot;,
    &quot;&#x50F9; &#x5C0F;&#x4EBA;&#x76F8; &#x91D1; &#x570B;&quot;,
    &quot;&#x5927;&#x4EBA;&#x7FD4; &#x4E94;&quot;,
    &quot;&#x90E8;&#x5B78;&#x5316;&#x6240;&#x92FC;&#x88FD;&#x5C71;&#x4E2D; &#x793E;&#x6703;&#x5F0F;&#x683C; &#x5143;&#x8CE3;&#x767C;&quot;,
    &quot;&#x793E;&#x6703;&#x8005;&#x5408;&#x6728;&#x5927; &#xB7;&#x5E97;&#x5546;&#x7F6E;&#x7F50; &#x610F; &#x675F;)&quot;,
    &quot;&#x793E;&#x6703;&#x540D;&#x4EE4;&#x90E8;&#x592A;&#x9577;&#x5DDD;&#x9F8D; &#x570B;&#x53E4;&#x540D;(&quot;,
    &quot;&#x6703;&#x9762;&#x30A4;&#x30E9;&#x30D6;&#x30B5;&#x30EB;&#x30BF;&#x30D3;&#x30B9;&#x30DB; &#x8AAA; &#x5927;&#x30CE;&quot;,
    &quot;&#x5E97;&#x7406;&#x4EE3;&quot;,
    &quot;&#x793E;&#x6703;&#x5F0F;&#x682A;&#x85E5;&#x88FD;&#x4EAC;&#x6771; &#x5143;&#x9020;&#x88FD;&quot;,
    &quot;Image ID: W5DPKC&quot;,
    &quot;www.alamy.com&quot;
  ]
</code></pre><h3 id="korean">Korean</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-6314649.jpeg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="1125" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/pexels-photo-6314649.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/pexels-photo-6314649.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/pexels-photo-6314649.jpeg 1125w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V remembers how to extract text again. And it remembers how to get it wrong too! If you look at the output, some of the characters are the same as the image, but some are not.</li></ul><pre><code class="language-json">[
&quot;&#xC220;&#xC740; &#xB0A8;&#xC5D0; &#xB9D0;&#xB098;&quot;,
&quot;&#xC220;&#xC740; &#xB098;&#xB85C; &#xB9D0;&#xB098;&quot;
]
</code></pre><ul><li>SceneXplain gives accurate output, though I&#x2019;m not sure where the <code>50</code> is coming from:</li></ul><pre><code class="language-json">[
    &quot;|&#xC57C;&quot;,
    &quot;&#xC88B;&#xC740; &#xB0A0;&#xC5D0; &#xB9CC;&#xB098;&quot;,
    &quot;&#xB2E8;&#xC774; &#xBC29;&quot;,
    &quot;50&quot;,
    &quot;&#xC88B;&#xC740; &#xB098;&#xB97C; &#xB9CC;&#xB098;&quot;
 ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/230531_--_----------_--_--2.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="800" height="1028" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/230531_--_----------_--_--2.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/230531_--_----------_--_--2.jpg 800w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V actually does okay with the more complex image. It doesn&#x2019;t catch <strong><em>all</em></strong> the strings, but it&#x2019;s less wrong than usual from a quick comparison (note: I don&#x2019;t read Korean, so I&#x2019;m just eyeballing it):</li></ul><pre><code class="language-json">[
&quot;&#xC790;&#xC5F0;&#xACFC; &#xAD6D;&#xBBFC;&#xC744; &#xC5F0;&#xACB0;&#xD558;&#xB294;&quot;,
&quot;&#xC218;&#xBAA9;&#xB7;&#xC815;&#xC6D0; &#xD50C;&#xB7AB;&#xD3FC; &#xAE30;&#xAD00;&quot;,
&quot;&#xC218;&#xBAA9;&#xC6D0;&#xC758; &#xACE0;&#xAC1D;&#xAC00;&#xCE58;&#xB97C; &#xB118;&#xC5B4;&quot;,
&quot;&#xAD6D;&#xBBFC; &#xC0DD;&#xD65C; &#xC18D; &#xB179;&#xC0C9;&#xACF5;&#xAC04; &#xD655;&#xC0B0;&#xACFC;&quot;,
&quot;&#xC815;&#xC6D0;&#xC0B0;&#xC5C5; &#xC0DD;&#xD0DC;&#xACC4;&#xB97C; &#xD65C;&#xC131;&#xD654;&#xD558;&#xB294; &#xC5ED;&#xD560;&#xC744;&quot;,
&quot;&#xC801;&#xADF9;&#xC801;&#xC73C;&#xB85C; &#xC218;&#xD589;&#xD558;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;&quot;
]
</code></pre><ul><li>SceneXplain extracts a lot more:</li></ul><pre><code class="language-json">[
    &quot;&#xD55C;&#xAD6D;&#xC218;&#xBAA9;&#xC6D0;&#xC815;&#xC6D0;&#xAD00;&#xB9AC;&#xC6D0;&quot;,
    &quot;&#xC790;&#xC5F0;&#xACFC; &#xAD6D;&#xBBFC;&#xC744; &#xC5F0;&#xACB0;&#xD558;&#xB294;&quot;,
    &quot;&#xC218;&#xBAA9;&#xC6D0;&#xB7;&#xC815;&#xC6D0; &#xD50C;&#xB7AB;&#xD3FC; &#xAE30;&#xAD00;&quot;,
    &quot;&#xC218;&#xBAA9;&#xC6D0;&#xC758; &#xACF5;&#xAC04;&#xC801; &#xAC1C;&#xB150;&#xC744; &#xB118;&#xC5B4;&quot;,
    &quot;&#xAD6D;&#xBBFC; &#xC0DD;&#xD65C; &#xC18D; &#xB179;&#xC0C9;&#xBB38;&#xD654; &#xD655;&#xC0B0;&#xACFC;&quot;,
    &quot;&#xC815;&#xC6D0;&#xC0B0;&#xC5C5; &#xC0DD;&#xD0DC;&#xACC4;&#xB97C; &#xD65C;&#xC131;&#xD654;&#xD558;&#xB294; &#xC5ED;&#xD560;&#xC744;&quot;,
    &quot;&#xC801;&#xADF9;&#xC801;&#xC73C;&#xB85C; &#xC218;&#xD589;&#xD558;&#xACA0;&#xC2B5;&#xB2C8;&#xB2E4;&quot;,
    &quot;&#xC0B0;&#xB9BC;&#xC0DD;&#xBB3C;&#xC790;&#xC6D0;&quot;,
    &quot;&#xACE0;&#xD488;&#xC9C8; &#xC218;&#xBAA9;&#xC6D0;&quot;,
    &quot;ES&quot;,
    &quot;&#xBCF4;&#xC804; &#xB7; &#xBCF5;&#xC6D0;&quot;,
    &quot;&#xC11C;&#xBE44;&#xC2A4;&quot;,
    &quot;&#xC5ED;&#xB7C9; &#xAC15;&#xD654;&quot;,
    &quot;&#xC0B0;&#xB9BC;&#xC0DD;&#xBB3C;&#xC790;&#xC6D0;&quot;,
    &quot;&#xC5F0;&#xAD6C; &#xB7;&#xD65C;&#xC6A9;&quot;,
    &quot;&#xC815;&#xC6D0;&#xBB38;&#xD654;&quot;,
    &quot;&#xD65C;&#xC131;&#xD654;&quot;
  ]
</code></pre><h3 id="arabic">Arabic</h3><p>Since we already covered the Arabic coffee image above, we&apos;ll just leave one Arabic here:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/artworks-000043896167-z3x71u-t500x500.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="500" height="500"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x627;&#x644;&#x62D;&#x64A;&#x627;&#x629; &#x637;&#x631;&#x64A;&#x642; &#x623;&#x643;&#x62B;&#x631; &#x633;&#x631;&#x639;&#x629;&quot;
]
</code></pre><ul><li>SceneXplain gets a little confused about the 4G logo, seeing it as <code>EAG</code> but is otherwise sound:</li></ul><pre><code class="language-json">[
    &quot;&#x627;&#x644;&#x62D;&#x64A;&#x627;&#x629; &#x635;&#x627;&#x631;&#x62A; &#x623;&#x643;&#x62B;&#x631; &#x633;&#x631;&#x639;&#x629;&quot;,
    &quot;EAG&quot;,
    &quot;LTE&quot;
  ]
</code></pre><h3 id="hindi">Hindi</h3><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="955" height="1575" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/dabur-sarso-avala-hair-oil-ad-dainik-bhaskar-jaipur-12-12-2017.jpg 955w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V:</li></ul><pre><code class="language-json">[
&quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x914;&#x930; &#x906;&#x902;&#x935;&#x932;&#x93E;&quot;,
&quot;&#x915;&#x93E; &#x92A;&#x94B;&#x937;&#x923; &#x92C;&#x93F;&#x928;&#x93E; &#x91A;&#x93F;&#x92A;&#x91A;&#x93F;&#x92A;&#x93E;&#x939;&#x91F;&quot;,
&quot;&#x938;&#x93F;&#x930;&#x94D;&#x92B; &#x930;&#x941;&quot;,
&quot;9 &#x92E;&#x947;&#x902;&quot;,
&quot;40ml&quot;,
&quot;&#x921;&#x93E;&#x92C;&#x930;&quot;,
&quot;&#x91F;&#x94B;&#x932; &#x92B;&#x94D;&#x930;&#x940; 1800-103-1644&quot;
]
</code></pre><ul><li>SceneXplain:</li></ul><pre><code class="language-json">[
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;&#x915;&#x947;&#x936; &#x924;&#x947;&#x932;&quot;,
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x914;&#x930; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;&#x915;&#x93E; &#x92A;&#x94B;&#x937;&#x923; &#x92C;&#x93F;&#x928;&#x93E;&quot;,
    &quot;&#x91A;&#x93F;&#x92A;&#x91A;&#x93F;&#x92A;&#x93E;&#x939;&#x91F;&quot;,
    &quot;&#x92E;&#x947;&#x902;&quot;,
    &quot;&#x928;&#x92F;&#x93E;&quot;,
    &quot;&#x92A;&#x948;&#x915;&quot;,
    &quot;&#x20B9;9/-&quot;,
    &quot;&#x938;&#x930;&#x938;&#x94B;&#x902; &#x906;&#x901;&#x935;&#x932;&#x93E;&quot;,
    &quot;40ml&quot;,
    &quot;DABUR CARES: CALL OR WRITE&quot;,
    &quot;+ TOLL FREE 1800-103-1644&quot;
  ]
</code></pre><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg" class="kg-image" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!" loading="lazy" width="973" height="1576" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/mdh-masala-ki-ek-dhamkedhar-offer-ad-hindustan-hindi-delhi-21-10-2017.jpg 973w" sizes="(min-width: 720px) 720px"></figure><ul><li>GPT-4V gave us the error <code>Something went wrong. If this issue persists please contact us through our help center at help.openai.com</code>. After retrying, it gave us:</li></ul><pre><code class="language-json">[
&quot;MDH &#x915;&#x940; &#x90F;&#x915;&quot;,
&quot;&#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&quot;,
&quot;&#x938;&#x94D;&#x935;&#x93E;&#x926; &#x915;&#x947; &#x938;&#x93E;&#x925; - &#x938;&#x93E;&#x925; &#x916;&#x941;&#x936;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x940; &#x92C;&#x94C;&#x91B;&#x93E;&#x930; !!&quot;,
&quot;&#x92E;&#x938;&#x93E;&#x932;&#x93E; &#x90F;&#x915; &#x938;&#x94D;&#x935;&#x93E;&#x926;&#x93F;&#x937;&#x94D;&#x91F; &#x914;&#x930; &#x91C;&#x940;&#x935;&#x902;&#x924; &#x909;&#x92A;&#x939;&#x93E;&#x930; !!&quot;,
&quot;MDH&quot;,
&quot;&#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
&quot;&#x905;&#x938;&#x932;&#x940; &#x92E;&#x938;&#x93E;&#x932;&#x947; &#x938;&#x91A;-&#x938;&#x91A;&quot;,
&quot;MDH Ltd.&quot;,
&quot;E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com&quot;
]
</code></pre><ul><li>SceneXplain goes much further, even extracting product names and tiny details like when the company was established:</li></ul><pre><code class="language-json">[
    &quot;MDH&quot;,
    &quot;&#x915;&#x940; &#x90F;&#x915;&quot;,
    &quot;&#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&quot;,
    &quot;&#x938;&#x94D;&#x935;&#x93E;&#x926; &#x915;&#x947; &#x938;&#x93E;&#x925; -&#x938;&#x93E;&#x925; &#x916;&#x941;&#x936;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x940; &#x92C;&#x94C;&#x91B;&#x93E;&#x930; !&quot;,
    &quot;&#x92E;&#x938;&#x93E;&#x932;&#x93E; &#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x94B; &#x914;&#x930; &#x91C;&#x940;&#x924;&#x94B; &#x909;&#x92A;&#x939;&#x93E;&#x930; !!&quot;,
    &quot;&#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x915;&#x940; &#x913;&#x930; &#x938;&#x947; &#x905;&#x92A;&#x928;&#x947; &#x938;&#x92D;&#x940; &#x917;&#x94D;&#x930;&#x93E;&#x939;&#x915;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x90F;&#x915; &#x927;&#x92E;&#x93E;&#x915;&#x947;&#x926;&#x93E;&#x930; &#x911;&#x92B;&#x930;&#x964;&quot;,
    &quot;&#x915;&#x93F;&#x938;&#x940; &#x92D;&#x940; &#x928;&#x91C;&#x926;&#x940;&#x915;&#x940; &#x935;&#x93F;&#x915;&#x94D;&#x930;&#x947;&#x924;&#x93E; &#x938;&#x947; &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x92E;&#x938;&#x93E;&#x932;&#x94B;&#x902; &#x915;&#x947; &#x928;&#x940;&#x91A;&#x947; &#x91B;&#x92A;&#x947; &#x92A;&#x948;&#x915;&#x94D;&#x938; &#x92E;&#x947;&#x902; &#x938;&#x947;&quot;,
    &quot;&#x915;&#x94B;&#x908; &#x92D;&#x940; 100 &#x917;&#x94D;&#x930;&#x93E;&#x92E; &#x935;&#x93E;&#x932;&#x93E; &#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x947;&#x902; &#x914;&#x930; &#x92D;&#x93E;&#x917;&#x94D;&#x92F;&#x936;&#x93E;&#x932;&#x940; &#x935;&#x93F;&#x91C;&#x947;&#x924;&#x93E; &#x92C;&#x928;&#x928;&#x947; &#x915;&#x93E; &#x905;&#x935;&#x938;&#x930;&quot;,
    &quot;&#x92A;&#x94D;&#x930;&#x93E;&#x92A;&#x94D;&#x924; &#x915;&#x930;&#x947;&#x902;, &#x939;&#x94B; &#x938;&#x915;&#x924;&#x93E; &#x939;&#x948; &#x906;&#x92A; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x917;&#x90F; &#x92A;&#x948;&#x915; &#x915;&#x947; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x92E;&#x947;&#x902; &#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x93E; &#x928;&#x93E;&#x92E; &#x939;&#x94B;&#x964;&quot;,
    &quot;&#x905;&#x938;&#x932;&#x940; &#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
    &quot;&#x92E;&#x938;&#x93E;&#x932;&#x947;&quot;,
    &quot;&#x938;&#x91A; -&#x938;&#x91A;&quot;,
    &quot;MD H&quot;,
    &quot;SPICES&quot;,
    &quot;MD&quot;,
    &quot;MOH&quot;,
    &quot;Garam&quot;,
    &quot;Pav Bhaji&quot;,
    &quot;masala&quot;,
    &quot;Kitchen&quot;,
    &quot;Shahi Pancer&quot;,
    &quot;King&quot;,
    &quot;Chana&quot;,
    &quot;DEGGI&quot;,
    &quot;MIRCH&quot;,
    &quot;Chunky Chat&quot;,
    &quot;&#x928;&#x93F;&#x92F;&#x92E; &#x935; &#x936;&#x930;&#x94D;&#x924;&#x947;&#x902; :- &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x92E;&#x938;&#x93E;&#x932;&#x94B;&#x902; &#x915;&#x947; &#x90A;&#x92A;&#x930; &#x926;&#x930;&#x94D;&#x936;&#x93E;&#x92F;&#x947; &#x917;&#x90F; &#x91A;&#x941;&#x928;&#x93F;&#x928;&#x94D;&#x926;&#x93E; &#x92A;&#x948;&#x915;&#x94D;&#x938; &#x92E;&#x947;&#x902; &#x938;&#x947; &#x915;&#x94B;&#x908; &#x92D;&#x940; 100 &#x917;&#x94D;&#x930;&#x93E;&#x92E; &#x935;&#x93E;&#x932;&#x93E;&quot;,
    &quot;&#x92A;&#x948;&#x915; &#x916;&#x930;&#x940;&#x926;&#x947;&#x902;, &#x909;&#x938;&#x915;&#x93E; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x916;&#x94B;&#x932;&#x947;&#x902;, &#x939;&#x94B; &#x938;&#x915;&#x924;&#x93E; &#x939;&#x948; &#x915;&#x93F; &#x909;&#x938; &#x92B;&#x94D;&#x932;&#x947;&#x92A; &#x915;&#x947; &#x928;&#x940;&#x91A;&#x947; &#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x93E; &#x928;&#x93E;&#x92E; &#x91B;&#x92A;&#x93E; &#x939;&#x94B;&#x964; &#x909;&#x938;&quot;,
    &quot;&#x909;&#x92A;&#x939;&#x93E;&#x930; &#x915;&#x94B; &#x92A;&#x94D;&#x930;&#x93E;&#x92A;&#x94D;&#x924; &#x915;&#x930;&#x928;&#x947; &#x915;&#x947; &#x92D;&#x93E;&#x917;&#x94D;&#x92F;&#x936;&#x93E;&#x932;&#x940; &#x935;&#x93F;&#x91C;&#x947;&#x924;&#x93E; &#x92C;&#x928;&#x947;&#x902;&#x964; &#x2022; &#x907;&#x938; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x915;&#x93E; &#x928;&#x915;&#x926; &#x932;&#x93E;&#x92D; &#x915;&#x94B;&#x908; &#x92D;&#x940; &#x928;&#x939;&#x940;&#x902; &#x939;&#x948;&quot;,
    &quot;&#x932;&#x93F;&#x90F; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; &#x92E;&#x947;&#x902; &#x939;&#x940; &#x932;&#x93E;&#x917;&#x942; &#x939;&#x948; &#x2022; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x938;&#x94D;&#x91F;&#x949;&#x915; &#x930;&#x939;&#x928;&#x947; &#x924;&#x915; &#x938;&#x940;&#x92E;&#x93F;&#x924; &#x939;&#x948; &#x2022; &#x92F;&#x939; &#x938;&#x94D;&#x915;&#x940;&#x92E; &#x90F;&#x92E; &#x921;&#x940; &#x90F;&#x91A; &#x915;&#x947; &#x938;&#x941;&#x92A;&#x930;&quot;,
    &quot;&#x938;&#x94D;&#x91F;&#x949;&#x915;&#x93F;&#x938;&#x94D;&#x91F;, &#x938;&#x94D;&#x91F;&#x949;&#x915;&#x93F;&#x938;&#x94D;&#x91F; &#x914;&#x930; &#x915;&#x930;&#x94D;&#x92E;&#x91A;&#x93E;&#x930;&#x93F;&#x92F;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x928;&#x939;&#x940;&#x902; &#x939;&#x948; &#xB7; &#x915;&#x902;&#x92A;&#x928;&#x940; &#x915;&#x93E; &#x928;&#x93F;&#x930;&#x94D;&#x923;&#x92F; &#x905;&#x902;&#x924;&#x93F;&#x92E; &#x914;&#x930; &#x938;&#x930;&#x94D;&#x935;&#x92E;&#x93E;&#x928;&#x94D;&#x92F; &#x939;&#x94B;&#x917;&#x93E;&quot;,
    &quot;&#x914;&#x930; &#x938;&#x92D;&#x940; &#x935;&#x93F;&#x935;&#x93E;&#x926; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; &#x915;&#x947; &#x928;&#x94D;&#x92F;&#x93E;&#x92F; &#x915;&#x94D;&#x937;&#x947;&#x924;&#x94D;&#x930; &#x915;&#x947; &#x905;&#x927;&#x940;&#x928; &#x939;&#x94B;&#x902;&#x917;&#x947;&#x964; &#x2022; &#x905;&#x928;&#x94D;&#x92F; &#x936;&#x930;&#x94D;&#x924;&#x94B;&#x902; &#x915;&#x947; &#x932;&#x93F;&#x90F; &#x92A;&#x948;&#x915; &#x926;&#x947;&#x916;&#x947;&#x902;&#x964;&quot;,
    &quot;&#x92E;&#x939;&#x93E;&#x936;&#x93F;&#x92F;&#x93E;&#x901; &#x926;&#x940; &#x939;&#x91F;&#x94D;&#x91F;&#x940; (&#x92A;&#x94D;&#x930;&#x93E;&#x966;) &#x932;&#x93F;&#x92E;&#x93F;&#x91F;&#x947;&#x921;&quot;,
    &quot;9/44, &#x915;&#x940;&#x930;&#x94D;&#x924;&#x93F; &#x928;&#x917;&#x930;, &#x928;&#x908; &#x926;&#x93F;&#x932;&#x94D;&#x932;&#x940; - 110015 &#x92B;&#x94B;&#x928; &#x928;&#x902;&#x966; 011-41425106 - 07-08&quot;,
    &quot;ESTD. 1919&quot;,
    &quot;E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com&quot;
  ]
</code></pre><h2 id="conclusion">Conclusion</h2><p>At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap:</p><p><strong>API limits:</strong></p><ul><li>GPT-4V: 100 requests per day</li><li>SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month.</li></ul><p><strong>Reliability:</strong></p><ul><li>GPT-4V: Is often like &#x201C;I&#x2019;m sorry Dave, I&#x2019;m afraid I can&#x2019;t do that.&#x201D;</li><li>SceneXplain: Doggedly reliable. Gets the job done.</li></ul><p><strong>Accuracy:</strong></p><ul><li>GPT-4V: hallucinates strings it believes are plausible from a given image, rather than reading the actual text.</li><li>SceneXplain: Extracts the correct text.</li></ul><p><strong>Consistency:</strong></p><ul><li>GPT-4V doesn&#x2019;t just hallucinate. It hallucinates different things each time.</li><li>SceneXplain gives much more consistent output.</li></ul><p><strong>Completeness:</strong></p><ul><li>GPT-4V: Often doesn&#x2019;t always extract <strong><em>all</em></strong> the strings in the image.</li><li>SceneXplain: Extracts more strings.</li></ul><p><strong>Cost:</strong></p><ul><li>GPT-4V: $0.025 per image.</li><li>SceneXplain: As low as 0.02 USD per image on the MAX plan if you enable JSON output. As low as 0.01 USD without JSON output.</li></ul><p>When you take all those factors into account, it&apos;s clear that SceneXplain is the obvious choice.</p><h2 id="get-started-with-scenexplain">Get started with SceneXplain</h2><p>Ready to explore multilingual OCR? Sign up for a free account with SceneXplain and start converting images to text easily. It&apos;s a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make!</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="SceneXplain&apos;s OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Embeddings in Depth]]></title><description><![CDATA[This second article in our series on embedding technology is much more concrete. It explains where embeddings come from and how they are used.]]></description><link>https://jina.ai/news/embeddings-in-depth/</link><guid isPermaLink="false">65535d16173d1400018405a8</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Scott Martens]]></dc:creator><pubDate>Wed, 15 Nov 2023 15:00:38 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--18--1.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--18--1.png" alt="Embeddings in Depth"><p>This article is the second in our series on embedding technology. In the <a href="https://jina.ai/news/how-embeddings-drive-ai-a-guide/?ref=jina-ai-gmbh.ghost.io">previous article, we discussed</a> embeddings in very abstract terms, but this one will be much more concrete. It explains where embeddings come from and how they are used.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x26A0;&#xFE0F;</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Trigger Warning:</strong></b> This article contains some light algebra, and although it tries to avoid complicated math, there are some notational conventions from vector algebra.</div></div><p>An <em>embedding</em> is a <em>mapping</em> of objects to <em>vectors</em>. Those objects may be real things or digital objects like images and files.</p><p>For example, imagine a robot that harvests tomatoes:</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/JsZJWyMVBtY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen title="This Tomato-Picking Robot Is the Future of Farming &#x1F345; &#x1F916;"></iframe></figure><p>This robot selects tomatoes on the vine that are ready to harvest, and then snips their stalks and takes them away. Recognizing which tomatoes are ripe is a good application of image embeddings. Each tomato on the vine is assigned a representative vector based on digital images taken by the robot&#x2019;s cameras. It then uses these embeddings to decide whether or not to harvest a particular tomato.</p><p>Similarly, we might use embeddings to study and predict customer behavior. These embedding vectors stand in for a person &#x2014; a real human being who presumably exists somewhere &#x2014; but we construct their embedding vectors from digital information about that person. For a study of consumer buying behavior, this is likely to be some combination of demographic information and purchasing records.</p><p>In the previous article, we discussed how vectors are just ordered lists of numbers that we decide to interpret as points in a high-dimensional space. Data stored in a computer is also just an ordered list of numbers. Embeddings are, therefore, made by translating input vectors in one vector space &#x2014; the one that contains the source data &#x2014; into vectors in a different space called the <strong><em>embedding space</em></strong>.</p><p>But not just any mapping between vector spaces is an embedding. The translation into embeddings is purposeful. The placement of embedding vectors relative to each other should be informative in some way about the objects they represent.</p><h2 id="cosines-how-we-compare-embeddings">Cosines: How We Compare Embeddings</h2><p>When we use embeddings in applications, the thing we most often do is compare two embeddings and calculate how close or far apart they are. This is indicative of the similarity or dissimilarity of the objects those embeddings stand in for with respect to some set of properties.</p><p>To do this, we use a metric called the <strong>cosine</strong>. This section explains cosines and how we calculate them in high-dimensional vector spaces.</p><p>An embedding is a vector, and as we showed in the previous section, we can interpret a vector as a point in a multi-dimensional space. However, it also has an interpretation as a direction and magnitude. Traditionally, we visualize this interpretation with an arrow:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-19.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1350" height="1560" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-19.png 1350w" sizes="(min-width: 720px) 720px"></figure><p>The <em>direction</em> of this three-dimensional vector is specified by the $(x,y,z)$ coordinates, and the <em>magnitude</em> is just the length of the vector, i.e., the distance from the coordinate $(x,y,z)$ to the origin at $(0,0,0)$. The length is calculated using a multidimensional extension of <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem?ref=jina-ai-gmbh.ghost.io">Pythagoras&apos; theorem</a>:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-20.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1500" height="300" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-20.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-20.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-20.png 1500w" sizes="(min-width: 720px) 720px"></figure><p>The double bars around a vector $\|\vec{v}\|$ indicate the vector&apos;s <em>length</em> (or <em>magnitude</em>).</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">Although this example uses only three dimensions, all these formulas apply exactly the same to arbitrarily large vectors of any number of dimensions.</div></div><p>We can see that different vectors can have the same direction but different lengths. For example, if we have the vectors $(4,2)$ and $(2,1)$, they have different lengths ($\sqrt{20} \approx 4.47$ and $\sqrt{5} \approx 2.23$ respectively) but they point in the same direction.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-21.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="938" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-21.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-21.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-21.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>How do we know they point in the same direction? We measure the angle between the two vectors. If the angle is zero, they point in the same direction.</p><p>To show how this works, let&#x2019;s consider two vectors: $\vec{a}=(3,2,5)$ and $\vec{b}=(4,5,2)$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-52.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="1140" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-52.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-52.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-52.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-52.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>To calculate the angle, we have a formula that scales up to any number of dimensions: The cosine.</p><p>The cosine of the angle&#xA0;$\theta$&#xA0;between vectors&#xA0;$\vec{a}$&#xA0;and&#xA0;$\vec{b}$&#xA0;is:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-50.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="208" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-50.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-50.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-50.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-50.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>The numerator of this fraction, $a&#x22C5;b$,&#xA0;is the&#xA0;<em>dot product</em>&#xA0;of the two vectors, and it&#x2019;s easy to calculate:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-49.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="288" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-49.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-49.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-49.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-49.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>As for&#xA0;&#x2016;<em>a</em>&#x2016;&#xA0;and&#xA0;&#x2016;<em>b</em>&#x2016;, those are the lengths of the two vectors: $\|\vec{a}\| = \sqrt{38} \approx 6.164$ and $\vec{b} = \sqrt{45} \approx 6.708$. Therefore, to calculate&#xA0;$cos&#x3B8;$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-22.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="587" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-22.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>This cosine corresponds to an angle of approximately 39.3 degrees, but in machine learning, we typically stop once we&#x2019;ve calculated the cosine because if all the numbers in both vectors are greater than zero, then the cosine of the angle will be between 0 and 1. A cosine of 1 means the angle between two vectors is 0 degrees, i.e., the vectors have the same direction (although they may have different lengths). A cosine of 0 means they have an angle of 90 degrees. i.e., they are <em>perpendicular</em> or <em>orthogonal</em>.</p><p>So let&#x2019;s compare $\vec{a}=(4,2)$&#xA0;and&#xA0;$\vec{b}=(2,1)$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-23.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-23.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>When the cosine of two vectors is one, they point in the same direction, even though they may have different lengths.</p><p>Another way to do the same operation is to take each vector and <em>normalize</em> it. Normalizing a vector means creating a new vector that points in the same direction but has a length of exactly 1. If two vectors point in the same direction, they will normalize to identical vectors.</p><p>To normalize a vector, divide it in each dimension by the vector&#x2019;s length. If $\vec{v} = (3,2,5)$ and $\hat{v} = norm(\vec{v})$ then:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-24.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1550" height="1232" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-24.png 1550w" sizes="(min-width: 720px) 720px"></figure><p>If two vectors $\vec{a}$ and $\vec{b}$ have an angle between them $\theta$, then their normalized forms $\hat{a}$ and $\hat{b}$ will have the same angle $\theta$ between them. Cosine is a measure that is indifferent to the magnitudes of vectors.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">When we compare embeddings to decide how near or far they are from each other, we almost always use cosines.</div></div><p>The reason for this is that the magnitudes of embedding vectors are not usually informative about anything. Most neural network architectures perform normalizations between some or all of their layers, rendering the magnitudes of embedding vectors typically useless.</p><h2 id="neural-networks-as-vector-mappings">Neural Networks As Vector Mappings</h2><p>Embedding vectors work because they are placed so that things that have common properties have cosines that are closer to 1, and those that do not have cosines closer to 0. An embedding model takes input vectors and translates them into embedding vectors.</p><p>Although there are many techniques for performing such translations, today, embedding models are exclusively neural networks.</p><p>A neural network is a sequence of vector transformations. In formal mathematical terms, for each layer of the network, we multiply the vector output of the previous layer by a matrix, and then generally add a <em>bias.</em> A <em>bias</em> is a set of values that we add or subtract from the resulting vector. We take the result and make that the input to the next layer of the network.</p><p>More formally, the values of the $m$th layer of the network are $\vec{m}$ and are transformed into the values of the $n$th layer ($\vec{n}$) by:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-25.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="325" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-25.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-25.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-25.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-25.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>Where $\textbf{W}$ is a matrix and $\vec{bias}$ is the vector of bias values.</p><p>It&#x2019;s customary to think of it as a network of &#x201C;nodes&#x201D; bearing numerical values, connected by &#x201C;weights&#x201D;. This is truer to the roots of neural network theory but is exactly equivalent to multiplying a vector by a matrix.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-38.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="650" height="1100" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-38.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-38.png 650w"></figure><p>If we have a vector $\vec{m} = (m_1,m_2m_3)$, this one neural network layer transforms it into a vector $\vec{n} = (n_1,n_2,n_3,n_4,n_5)$. It does that by multiplying the values in $\vec{m}$ by a set of weights (also called <em>parameters</em>), then adding them up, and adding the bias vector $\vec{bias} = (bias_1,bias_2,bias_3,bias_4,bias_5)$</p><p>Each line between nodes in the diagram above is one weight. If we designate the weight from $m_1$ to $n_1$ as $w_{11}$, from $m_2$ to $n_3$ as $w_{23}$, etc., then:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-26.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="600" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-26.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-26.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-26.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-26.png 2000w" sizes="(min-width: 720px) 720px"></figure><p>In matrix algebra notation, this is the same as multiplying the 3-dimensional vector $\vec{n}$ by a $3x5$ matrix to get $\vec{m}$:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-27.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="472" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-27.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-27.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-27.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Neural networks have multiple layers, so the output of this matrix multiplication is multiplied by another matrix and then another, typically with some kind of thresholding or normalization between multiplications and sometimes other tricks. The most common way of doing this today is to apply the <strong>ReLU</strong> (<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)?ref=jina-ai-gmbh.ghost.io">rectified linear activation unit</a>) activation function. If the value of any node is below zero, it&#x2019;s replaced with a zero, and otherwise, left unchanged.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-28.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="520" height="198"></figure><p>Otherwise, a neural network is just a series of transformations like the one above from $\vec{m}$ to $\vec{n}$.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-40.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="261" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-40.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-40.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-40.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-40.png 2300w" sizes="(min-width: 720px) 720px"></figure><p>This structure has some important properties. The most important is that:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">In theory, any kind of vector transformation is possible with enough layers. Any problem that can be expressed as a mapping from a vector space $\vec{x} \in \textit{A}$ to vectors in another vector space $\vec{y} \in \textit{B}$ has some solution that looks just like a neural network.</div></div><p>It may be impractical to construct a large enough network for a given problem, and there may be no procedure for correctly calculating the weights. AI models are not guaranteed in advance to be able to learn or perform any task at all.</p><p>What happens in practice is that neural networks <strong><em>approximate</em></strong> a solution by learning from examples. How good a solution that approximation is depends on a lot of factors and can&#x2019;t typically be known in advance of training a model.</p><p>There is an aphorism attributed to the British statistician George Box:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">All models are wrong, but some are useful.</em></i></div></div><p>Neural networks can be very good approximations, but they are still approximations, and it&#x2019;s not possible to know for certain how good they are at something until you put them into production. The real measure of an AI model is its usefulness, and that&#x2019;s hard to know in advance.</p><h2 id="turning-problems-into-vector-mappings">Turning Problems Into Vector Mappings</h2><p>Defining neural networks as mappings from one vector space to another sounds very limiting. We don&#x2019;t naturally see the kinds of things we want AI models to do in those terms.</p><p>However, many of the most visible developments in recent AI are really just ways of using vector-to-vector mappings in clever ways. Large language models like ChatGPT translate input texts, rendered as vectors, into vectors that designate a single word. They write texts by adding the single-word output to the end of the input and then running again to get the next word.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-42.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="2000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-42.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-42.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-42.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/11/image-42.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>It may take some creativity and engineering to find a way to express some task as a vector mapping, but as recent developments show, it is surprisingly often possible.</p><h2 id="where-do-embeddings-fit-in">Where Do Embeddings Fit In?</h2><p>When you train an AI model, you do it with some goal in mind. In the typical scenario, you have training data that consists of inputs and matching correct outputs. For example, let&#x2019;s say you want to classify tweets by their emotional content. This is a common benchmarking task used to evaluate text AI models.</p><p>One widely used dataset (<a href="https://github.com/dair-ai/emotion_dataset?ref=jina-ai-gmbh.ghost.io">DAIR.AI Emotion</a>) sorts tweets into eight emotional categories:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-58.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="740" height="420" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-58.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-58.png 740w" sizes="(min-width: 720px) 720px"></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-55.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1500" height="1106" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-55.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-55.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-55.png 1500w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Number of tweets per emotion category in the </span><a href="http://dair.ai/?ref=jina-ai-gmbh.ghost.io"><span style="white-space: pre-wrap;">DAIR.AI</span></a><span style="white-space: pre-wrap;"> Emotion dataset</span></figcaption></figure><p>To train a model to do this task, you need to decide two things:</p><ol><li>How to map the output category labels to vectors.</li><li>How to map the input texts to vectors</li></ol><p>The easiest way to encode these emotion labels is to create six-dimensional vectors where each value corresponds to one category. For example, if we make a list of emotions, like the one above, &#x201C;anger&#x201D; is the third in the list. So if a tweet has angry content, we train the network to output a six-dimensional vector with zero values everywhere except the third item in it, which is set to one:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-30.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="589" height="81"></figure><p>This approach is called <em>zero-hot encoding</em> among AI engineers.</p><p>We also have to encode the tweets as vectors. There is more than one way to do this, but most text-processing models follow broadly the same pattern:</p><p>First, input texts are split into <em>tokens</em>, which you can understand as something like splitting it into words. The exact mechanisms vary somewhat, but the procedure broadly follows the following steps:</p><ol><li>Normalize the case, i.e., make everything lowercase.</li><li>Split a text on punctuation and spaces.</li><li>Look up each resulting subsegment of text in a dictionary (which is a part of the AI model bundle) and accept it if it is in the dictionary.</li><li>For segments not in the dictionary, split them into the smallest number of parts that are in the dictionary, and then accept those segments.</li></ol><p>So, for example, let&#x2019;s say we wanted an embedding for this famous line from <em>Alice in Wonderland</em>:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">Why, sometimes I&apos;ve believed as many as six impossible things before breakfast.</em></i></div></div><p>Jina Embeddings&#x2019; text tokenizer will split it up into 18 tokens like this:</p><pre><code>[CLS] why , sometimes i &apos; ve believed as many as six impossible 
things before breakfast . [SEP]
</code></pre><p>The <code>[CLS]</code> and <code>[SEP]</code> are special tokens added to the text to indicate the beginning and end, but otherwise, you can see that the splits are between words on spaces and between letters and punctuation.</p><p>If the tokenizer encounters a sentence with words that are not in the dictionary, it tries to match the parts to entries in its dictionary. For example:</p><div class="kg-card kg-callout-card kg-callout-card-white"><div class="kg-callout-text"><i><em class="italic" style="white-space: pre-wrap;">Klaatu barada nikto</em></i></div></div><p>This is rendered in the Jina tokenizer as:</p><pre><code>[CLS] k ##la ##at ##u bar ##ada nik ##to [SEP]
</code></pre><p>The <code>##</code> indicates a segment appearing elsewhere than the beginning of a word.</p><p>This approach guarantees that every possible string can be tokenized and can always be processed by the AI model, even if it&#x2019;s a new word or a nonsense string of letters.</p><p>Once the input text is transformed into a list of tokens, we replace each token with a unique vector from a lookup table. Then, we concatenate those vectors into one long vector.</p><p>Many text-processing AI models (including Jina Embeddings 2 models) use vectors of 512 dimensions for each token. All text-processing models are limited in the number of tokens they can accept as input. Jina Embeddings 2 models accept 8,192 tokens, but most models only take 512 or fewer.</p><p>If we use 512 for this example, then to do emotion classification for these tweets, we need to build a model that takes an input vector of $512 \times 512 = 262144$ dimensions, and outputs a vector of 6 dimensions:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-43.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1350" height="800" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-43.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-43.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-43.png 1350w" sizes="(min-width: 720px) 720px"></figure><p>Neural networks are often presented as a kind of black box in which the parts on the inside are not described in depth. This is not wrong, but it&#x2019;s not very informative. In practice, large, modern AI models tend to have three components, although this is not always true, and the boundaries between them are not always clear:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-44.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1400" height="800" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-44.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-44.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-44.png 1400w" sizes="(min-width: 720px) 720px"></figure><p>The vocabulary for some of this is not entirely codified, and this schema does not apply to all AI models, but in general, a trained model has three parts:</p><ol><li><strong>An input processor or encoder</strong><br><br>Especially for networks that take in very large input vectors, there is some part of the network devoted to correlating all parts of the input with each other. This section of the network can be very large, and it can be structurally complex. Many of the biggest breakthroughs in AI in the last two decades have involved innovative structures in this part of the model.<br></li><li><strong>A multi-layer perceptron</strong><br><br>There isn&#x2019;t a consistent term for this &#x201C;middle&#x201D; part of the model, but &#x201C;MLP&#x201D; or &#x201C;multi-layer perceptron&#x201D; appear to be the most common terms in the technical literature. The term itself is a bit old-fashioned, recalling the &#x201C;perceptron&#x201D;, a kind of small single-layer neural network invented in the 1950s that is ancestral to modern AI models.<br><br>The MLP typically consists of a number of connected layers of the same size. For example, the smallest of the ViT image processing models has twelve layers with 192 dimensions in each, while the largest has 24 layers, each with 768 dimensions.<br><br>The size and structure of the MLP are key design variables in an AI model.<br></li><li><strong>The &#x201C;Classifier&#x201D;</strong><br><br>This is usually the smallest part of the model, typically much smaller than the MLP. It may be as small as one layer but typically has more in modern models. This part of the model translates the output of the MLP into the exact form specified for the output, like the thousand-dimension one-hot vectors used for Imagenet-1k. It may not actually be a classifier. This final section of the model may be trained to do some other task.</li></ol><p>The size of the input encoder is mostly determined by your planned input sizes and the size of the first layer of the MLP. The size of the &#x201C;classifier&#x201D; is mostly determined by the training objective and the size of the last layer of the MLP. You have relatively few design choices in those areas. The MLP is the only part where you can make free decisions about your model.</p><p>The size of the MLP is a design choice balancing three factors:</p><ul><li>Smaller MLPs learn faster, run faster, and take less memory than large MLPs.</li><li>Large MLPs typically learn more and perform better, all else being equal.</li><li>Large MLPs are more likely to &#x201C;overfit&#x201D; than small MLPs. There is a risk that instead of generalizing from their training examples, they will memorize them and not learn effectively.</li></ul><p>The optimal balance depends on your use case and context, and it is very difficult to calculate the optimum in advance.</p><p>Most of the learning takes place in the MLP. The MLP learns to recognize those features of the inputs that are useful in calculating the correct output. The last layer of the MLP outputs vectors that are closer together the more they share those features. The &#x201C;classifier&#x201D; then has the relatively easy task of translating that into the correct output, like what class a particular input belongs to..</p><p>So, for example, if we train a text classifier with the six emotion labels from our dataset, we would expect the outputs of the MLP to be a high-dimensional analog of something like this:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-53.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1800" height="1250" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-53.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-53.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-53.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-53.png 1800w" sizes="(min-width: 720px) 720px"></figure><p>In short, the output of the last layer of the MLP has all the properties we want in an embedding: It puts similar things close together and dissimilar things further apart.</p><p>We typically construct embedding models by taking a model we&#x2019;ve trained for something &#x2014; often classification, but not necessarily &#x2014; and throwing away the &#x201C;classifier&#x201D; part of the model. We just stop at the last layer of the MLP and use its outputs as embeddings.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-47.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1200" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-47.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-47.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-47.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>There are four key points to take from this:</p><ol><li>Embeddings put things close to each other that are similar based on a set of features.</li><li>The set of features that embeddings encode is not usually specified in advance. It comes from training a neural network to do a task and consists of the features the model discovered were relevant to doing that task.</li><li>Embedding vectors are typically very high-dimensional but are usually much, much smaller than input vectors.</li><li>Most models have an embedding layer in them somewhere, even if they never expose it or use it as a source of embedding vectors.</li></ol><h2 id="text-embeddings">Text Embeddings</h2><p>Text embeddings work in very much the same way we&#x2019;ve described above, but while it&#x2019;s easy to understand how apples are like other apples and unlike oranges, it&#x2019;s not so easy to understand what makes texts similar to each other.</p><p>The first and most prototypical application of text embeddings is for search and information retrieval. In this kind of application, we start with a collection of texts:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-31.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="1778" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-31.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-31.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-31.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-31.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We then calculate and store their embeddings:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-32.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="391" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-32.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-32.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-32.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-32.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>When users want to search this document collection, they write a query in normal language:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-33.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="171" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-33.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-33.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-33.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-33.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We also convert this text into its corresponding embedding:</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-34.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="171" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-34.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-34.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-34.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-34.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>We perform search by taking the cosine between $\vec{q}$ and each of the document embeddings $\vec{d}_1,\vec{d}_2,\vec{d}_3$, identifying the one with the highest cosine (and thus is closest to the query embedding). We then return to the user the corresponding document.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-35.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="2000" height="164" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/11/image-35.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-35.png 2250w" sizes="(min-width: 720px) 720px"></figure><p>So, one training goal for text similarity is information retrieval: If we train a model to be a good search engine, it will give us embeddings that put two documents close to each other in proportion to how much the same queries will retrieve them both.</p><p>There are other possible goals we can use to train text embeddings, like classifying texts by genre, type, or author, which will lead to embeddings that preserve different features. There are a number of ways we can train text embedding models, but we will discuss that topic in the next article, which will go into detail about training and adapting AI models. What matters is what you&#x2019;ve trained the AI model to see as a text&apos;s most relevant features.</p><h2 id="jina-embeddings">Jina Embeddings</h2><p>As a concrete example, the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Jina Embeddings 2</a> models represent each token with a 512-dimension vector and can accept a maximum of 8192 tokens as input. This AI model, therefore, has an input vector size of 512x8192 or 4,194,304 dimensions. If the input text is less than 8192 tokens long, the rest of the input vector is set to zeros. If longer, it&#x2019;s truncated.</p><p>The model outputs a 512-dimension embedding vector to represent the input text.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-48.png" class="kg-image" alt="Embeddings in Depth" loading="lazy" width="1400" height="1000" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/image-48.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/image-48.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/image-48.png 1400w" sizes="(min-width: 720px) 720px"></figure><p>As a developer or user, you don&#x2019;t have to deal directly with any of these complexities. They are hidden from you by software libraries and web APIs that make using embeddings very straightforward. To transform a text into an embedding can involve as little as one line of code:</p><pre><code class="language-bash">curl &lt;https://api.jina.ai/v1/embeddings&gt; \\
  -H &quot;Content-Type: application/json&quot; \\
  -H &quot;Authorization: Bearer jina_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; \\
  -d &apos;{
    &quot;input&quot;: [&quot;Your text string goes here&quot;, &quot;You can send multiple texts&quot;],
    &quot;model&quot;: &quot;jina-embeddings-v2-base-en&quot;
  }&apos;
</code></pre><p>Go to the <a href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io">Jina Embeddings page</a> to get a token and see code snippets for how to get embeddings in your development environment and programming language.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Embedding API</div><div class="kg-bookmark-description">Top-performing, 8192-token length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Embeddings in Depth"></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner-embedding-api.png" alt="Embeddings in Depth"></div></a></figure><h2 id="embeddings-the-heart-of-ai">Embeddings: The Heart of AI</h2><p>Practically all models have an embedding layer in them, even if it&apos;s only implicit, and most AI projects can be built on top of embedding models. That&apos;s why good embeddings are so important to the effective use of AI technologies.</p><p>This series of articles is designed to demystify embeddings so that you can better understand and oversee the introduction of this technology in your business. Read the <a href="https://jina.ai/news/how-embeddings-drive-ai-a-guide/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">first installment here</a>. Next, we will dive further into this technology, and provide hands-on tutorials with code you can use in your own projects.</p><p>Jina AI is committed to providing you with tools and help for creating, optimizing, evaluating, and implementing embedding models for your enterprise. We&#x2019;re here to help you navigate the new world of business AI. Contact us via our&#xA0;<a href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">website</a>&#xA0;or join our&#xA0;<a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">community on Discord</a>&#xA0;to get started.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina AI - Your Portal to Multimodal AI</div><div class="kg-bookmark-description">Jina AI offers powerful multimodal AI solutions for everyday users, developers, and scalable enterprise solutions. We aim to democratize access to the limitless potential of AI-generated creativity and innovation, empowering individuals and businesses alike.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="Embeddings in Depth"><span class="kg-bookmark-author">Your Portal to Multimodal AI</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://jina.ai/banner.png" alt="Embeddings in Depth"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Join the Jina AI Discord Server!</div><div class="kg-bookmark-description">Check out the Jina AI community on Discord - hang out with 3602 other members and enjoy free voice and text chat.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://discord.jina.ai/assets/images/favicon.ico" alt="Embeddings in Depth"><span class="kg-bookmark-author">Discord</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512" alt="Embeddings in Depth"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data]]></title><description><![CDATA[Unlock the Secrets of the Satellites: Leverage SceneXplain's powerful 'Extract JSON from Image' feature for land use classification]]></description><link>https://jina.ai/news/look-up-in-the-sky-using-scenexplain-to-classify-land-use-from-satellite-data/</link><guid isPermaLink="false">6544c26c2305600001855879</guid><category><![CDATA[Tech Blog]]></category><dc:creator><![CDATA[Alex C-G]]></dc:creator><pubDate>Mon, 06 Nov 2023 15:00:55 GMT</pubDate><media:content url="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" medium="image"/><content:encoded><![CDATA[<img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Release-banner-DocArray.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><p>Is it a bird? Is it a plane? Is it a dense residential area? Or maybe even a chaparral? Worry not readers. SceneXplain is here to let you know. We don&apos;t even have to jump into a phone booth to change first.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F331;</div><div class="kg-callout-text">A <a href="https://en.wikipedia.org/wiki/Chaparral?ref=jina-ai-gmbh.ghost.io">chaparral</a> is a shrubland plant community found primarily in California, in southern Oregon and in the northern portion of the Baja California Peninsula in Mexico. <br><br>You&apos;ll see why this is relevant later.</div></div><p>In this blog post, we&apos;ll talk about our experiments using SceneXplain to look at satellite imagery and classify it based on what the land is used for. We&apos;ll go through three different datasets and explain how we did what we did.</p><h2 id="what-is-land-use-classification-and-what-is-it-used-for">What is land use classification and what is it used for?</h2><p>Using satellite data for classifying land use involves working out what a section of the Earth is used for, such as agriculture, city, forest, or body of water. The focus here is on using optical satellite imagery as the data source, which is then analyzed through AI image models to determine the land use. It&apos;s often used for:</p><ul><li>Resource Management: Knowing how land is being used aids in allocating and managing resources, such as water for irrigation.</li><li>Urban Planning: Good land use maps are invaluable to city planners so that they can know what types of land use exist in and around urban areas to make informed decisions on infrastructure development.</li><li>Environmental Protection: By identifying sensitive or critical ecological zones, conservationists can better protect and manage them.</li><li>Disaster Management: Land use classification can help to identify vulnerable areas that require priority during emergencies.</li><li>Climate Change Studies: Monitoring land use over time can offer insights into climate change impacts, such as the rate of deforestation.</li><li>Research and Education: Scientists and educators across disciplines use this data for various kinds of research, from ecology to social sciences.</li></ul><h2 id="up-up-and-away-how-we-classify-land-usage-in-scenexplain">Up, up, and away! How we classify land usage in SceneXplain</h2><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F40D;</div><div class="kg-callout-text">You can find a link to the notebook <a href="https://colab.research.google.com/github/alexcg1/scenex-geospatial/blob/main/notebook.ipynb?ref=jina-ai-gmbh.ghost.io">here</a>.</div></div><h3 id="datasets">Datasets</h3><p>We used three datasets for our testing:</p><ul><li><a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html?ref=jina-ai-gmbh.ghost.io">UC Merced land use classification</a></li><li><a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID (Aerial Image Dataset)</a></li><li><a href="https://www.tensorflow.org/datasets/catalog/resisc45?ref=jina-ai-gmbh.ghost.io">RESISC45 (Remote Sensing Image Scene Classification)</a></li></ul><p>Each of these consists of satellite and aerial images of different sections of Earth, labeled by what they are used for, e.g. <code>dense_residential</code>, <code>round_farm</code>, <code>intersection</code>, etc.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg" width="600" height="600" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/2023/11/airport_10.jpg 600w"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral_010.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><code spellcheck="false" style="white-space: pre-wrap;"><span>airport</span></code><span style="white-space: pre-wrap;"> from AID dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>denseresidential</span></code><span style="white-space: pre-wrap;"> from UC Merced dataset, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>chaparral</span></code><span style="white-space: pre-wrap;"> from RESISC45 dataset</span></p></figcaption></figure><h3 id="scenexplain">SceneXplain</h3><p>We used SceneXplain&apos;s <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">&quot;Extract JSON from image&quot; feature</a> with the Flash algorithm to extract one label per input image. After testing with several algorithms, we saw the Flash algorithm offered the fastest performance, and precision on par with more recent algorithms like <a href="https://www.notion.so/Look-Up-in-the-Sky-Using-SceneXplain-To-Classify-Land-Use-From-Satellite-Data-e46f7ee6e503425fba837e09fb03b26f?pvs=21&amp;ref=jina-ai-gmbh.ghost.io">Jelly</a>.</p><p>Rather than manually uploading each image via the web UI, we used SceneXplain&apos;s API to upload and analyze images one at a time.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A1;</div><div class="kg-callout-text">We could speed this up by (ab)using SceneXplain&apos;s cid feature when batching and giving each image a unique ID. The ID would contain the image&apos;s &quot;official&quot; category, so (after processing) we could extract that and compare it to the category assigned by SceneXplain.</div></div><h3 id="json-schema">JSON Schema</h3><p>Why even use a JSON schema? Why not just let SceneXplain interpret the images directly? Say, by looking at an aerial view of a baseball diamond from the UC Merced dataset?</p><p>Here&apos;s why:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--39-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="441" height="666"><figcaption><a href="https://scenex.jina.ai/share?thread=qjW5p1gZBYK2HONbNsOE&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>It&#x2019;s a clear description, but nowhere does it mention <code>baseballdiamond</code>, which (since we&#x2019;re doing classification) is the label we want.</p><p>We could also try asking it a question using visual question answering (VQA):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--40-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="444" height="798"><figcaption><a href="https://scenex.jina.ai/share?thread=VW4xcmLawvG8lWInwwAa&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>That answer is slightly better, but still throws in a lot more superfluous words. If we were to bulk classify images using this method we would have to search each answer for the category label, since there would be no consistent wording between them. And what would happen if an image were ambiguous and SceneXplain tried to assign it two categories or more?</p><p>For this reason, we used SceneXplain&#x2019;s &#x201C;<a href="https://jina.ai/news/scenexplains-image-json-extract-structured-data-images-precision/?ref=jina-ai-gmbh.ghost.io">Extract JSON from image</a>&#x201D; feature, where we specify a JSON Schema and can thus get highly structured, standardized output:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="612" height="1389" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--41-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--41-.png 612w"><figcaption><a href="https://scenex.jina.ai/share?thread=agnPj7H47wl87kDiEjM6&amp;ref=jina-ai-gmbh.ghost.io" rel="noreferrer"><span style="white-space: pre-wrap;">Link to this scene on SceneXplain</span></a></figcaption></figure><p>We used the following JSON schema:</p><pre><code class="language-json">{
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;category&quot;: {
      &quot;type&quot;: &quot;array&quot;,
      &quot;description&quot;: &quot;Which single main category of geospatial imagery does this image belong to?&quot;,
      &quot;enum&quot;: [&lt;categories from dataset&gt;],
      &quot;maxContains&quot;: 1
    }
  }
}
</code></pre><p>Since each dataset has different (albeit similar) categories, we generated the list dynamically for each dataset.</p><h2 id="does-it-work">Does it work?</h2><p>Sometimes! Like at least 60% of the time! In some randomized tests we got close to 80% (or occasionally as low as 50%).</p><p>Several factors can cause it to fail:</p><ul><li>Some categories can look very similar, e.g. <code>sparse_residential</code>, <code>medium_residential</code>, <code>dense_residential</code>. SceneXplain often picks the wrong one. This can also be seen in cases like <code>road</code> vs <code>runway</code>.</li><li>Occasionally it hallucinates a new category not specified in the <code>enum</code>, for example <code>residential</code> (as opposed to <code>denseresidential</code>). Occasionally it glitches and assigns a category like <code>A</code>.</li><li>Some category names like <code>chaparral</code> are uncommon words and/or concepts. It seems unlikely that many pictures of (or references to) chaparrals are in its training datasets. With so little data, the model barely knows what a chaparral is or what it looks like. (Before reading this post, did you?)</li><li>Aerial view geospatial images are under-represented in the training corpora relative to other images. This means that to a general-purpose tool like SceneXplain, an aerial view of a chaparral may look more like mold or dirt on a surface.</li></ul><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png" width="1200" height="1200" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--42-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--42-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--42-.png 1200w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/chaparral01.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">One of these is mold. The other is chaparral. Sorry, California.</span></p></figcaption></figure><h2 id="why-not-use-a-dedicated-model">Why not use a dedicated model?</h2><p>Most land-use classification is indeed done with dedicated models, specifically trained on aerial-view land use imagery. This makes them perfect for that particular task while not being so hot at general image classification.</p><p>So, why not just use one of those?</p><h3 id="self-hosting-is-a-drag">Self-hosting is a drag</h3><p>Even self-hosting something like Resnet (which has <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/torch-neuronx/resnet50-inference-on-trn1-tutorial.html?ref=jina-ai-gmbh.ghost.io">decent tutorials</a>) is a drag. You need to set up AWS, install dependencies, compile the model, and so on. It&#x2019;s not as easy as using an existing software-as-a-service like SceneXplain.</p><h3 id="academic-models-are-pure-kryptonite">Academic models are pure kryptonite</h3><p>Let&apos;s look at the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset</a> and see how we can replicate what they did in their paper:</p><p>First of all, we have to download the code from OneDrive or Baidu Pan. Dang, the file no longer exists on OneDrive, and Baidu Pan wants me to install a random RPM or DEB file just to download the dataset. Blech.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="1238" height="602" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--43-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--43-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--43-.png 1238w" sizes="(min-width: 720px) 720px"><figcaption><span style="white-space: pre-wrap;">Parents: Please check your children&apos;s Halloween candy. I just found a Baidu Pan installation RPM in my kid&apos;s bag. Disgusting.</span></figcaption></figure><p>Assuming we have a friend in China who downloads the code and sends it to us (thanks Kelly!), we can extract it and check the <code>readme.txt</code> (Did you assume the readme would be in a repo somewhere, you sweet innocent summer child?)</p><p>Checking the file dates with <code>ls -l</code>, we see the files were last modified in 2016. Great.</p><figure class="kg-card kg-image-card"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png" class="kg-image" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" loading="lazy" width="741" height="338" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--44-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--44-.png 741w" sizes="(min-width: 720px) 720px"></figure><p>Now we download the pre-trained model from the <a href="https://www.vlfeat.org/matconvnet/pretrained/?ref=jina-ai-gmbh.ghost.io">URL in the readme</a>. Good news! The link works! Bad news! There are lots of CNNs there and the readme doesn&apos;t say which one to download!</p><p>To quote the readme, the next step is to: <code>Compile vlfeat, gist, matconvnet and liblinear, and place them under &lt;libs&gt;</code>. We didn&apos;t actually try this, but I&apos;m <em>sure</em> compiling software from over five years ago will go without a hitch. I just need to use version...um. There&apos;s no version specified. It&apos;s sheer dumb luck I guess!</p><p>The final step: We&apos;ve put so much work into the other steps that now we can finally reap the fruits of our labor. We just need to install MatLab. Which is 860 EUR per year. Wonderful.</p><p>I&apos;m sure setting up the model was worth the effort for whoever wrote the paper. But if I&apos;m just trying things out, that&apos;s a lot of work and money.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">&#x1F4A2;</div><div class="kg-callout-text">On the verge of giving up, I checked the directory structure of the code. Reading the readme, I assumed I had to <i><em class="italic" style="white-space: pre-wrap;">download</em></i> the libs. But the libs folder already exists. Already (supposedly) populated with what I need. Turns out I can&apos;t even trust the readme (or perhaps myself, to be honest)</div></div><h3 id="i-got-ninety-nine-problems-but-a-chaparral-aint-one">I got ninety-nine problems, but a chaparral ain&apos;t one</h3><p>Even assuming pre-trained models were simple to use, the categories are baked in already. Since most of the training data was (I assume) taken from California, chaparrals are all over the place, but there aren&apos;t so many medieval castles. So if I wanted to apply the same model to Europe, I&apos;d be stuck with those categories.</p><p>On the other hand, SceneXplain is a general-purpose tool. This means it may not have extensive chaparral knowledge (seriously, outside of Californians, who among us does?), but it has enough general-purpose knowledge that it can more reliably classify what you&apos;re specifically looking for.</p><p>Let&apos;s just say that specialist data can be kryptonite to a general-purpose image classification/captioning model. AI models aren&#x2019;t (and can&#x2019;t be) all things to all people, after all.</p><p>When you think of the data used to train general-purpose image models, very little would be aerial views of the landscape taken from satellites and labels for such data (<code>road</code>, <code>residential</code>, etc.) would far more commonly be applied to pictures of those phenomena taken from a more human-centric angle.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png" width="1125" height="750" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/11/Untitled--45-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/11/Untitled--45-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/11/Untitled--45-.png 1125w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/denseresidential10-1.jpg" width="256" height="256" loading="lazy" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Dense residential (L: How it&apos;s represented in most models, R: how it&apos;s represented in UC Merced dataset)</span></p></figcaption></figure><h2 id="why-not-use-scenexplain">Why NOT use SceneXplain?</h2><p>While using dedicated models has its downsides, there are some advantages:</p><h3 id="you-only-care-about-chaparrals">You only care about chaparrals</h3><p>Good for you buddy. Good for you.</p><h3 id="scenexplain-isnt-as-accurate-as-pre-trained-models">SceneXplain isn&apos;t as accurate as pre-trained models</h3><p>While a pain to set up, pre-trained models offer superior accuracy when it comes to this very specific domain. Those models were trained specifically on aerial imagery and thus offer accuracy rates of 85-95% when using high-level methods (see final table in the <a href="https://captain-whu.github.io/AID/?ref=jina-ai-gmbh.ghost.io">AID dataset paper</a>), compared to SceneXplain&apos;s accuracy of about 60%.</p><h2 id="next-steps">Next steps</h2><p>Large language models and image-to-text models aren&#x2019;t specially trained to recognize landscape types from satellite imagery. There&#x2019;s enough in their training data to make a stab at it, and it&#x2019;s not totally useless, but zero-shot detection is just not good. Some categories are (by their nature) going to be tough to recognize, even for a human. If we can&#x2019;t tell from 30,000 feet how many people live on a city block, then how could we expect SceneXplain to tell <code>sparseresidential</code> from <code>mediumresidential</code>?</p><p>Specially trained and fine-tuned models perform well enough to be a hard bar to beat.</p><p>But often you hit upon a problem that <strong>doesn&#x2019;t</strong> have a specialized dataset that can be used for training a dedicated model. That means that a general-purpose tool, like SceneXplain, may be your best bet. Our next steps are trying to see how much better we can make SceneX with the least effort, so that our users can always get the best performance possible for the least added effort.</p><p>Even if no one ever uses SceneXplain to catalog land use, learning to make it better at this task teaches us how to make it better for something else that it&#x2019;s never specially learned to do.</p><p>To test out SceneXplain for our own use case, create a free account and start playing! Let us know how you&apos;re doing on our <a href="https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">Discord</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scenex.jina.ai/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SceneXplain - Leading AI Solution for Image Captions and Video Summaries</div><div class="kg-bookmark-description">Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scenex.jina.ai/icons/apple-icon-180x180.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"><span class="kg-bookmark-author">SceneXplain</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scenex.jina.ai/banner.png" alt="Look, Up in the Sky! Using SceneXplain To Classify Land Use From Satellite Data"></div></a></figure>]]></content:encoded></item></channel></rss>
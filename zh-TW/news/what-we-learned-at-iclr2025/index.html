<!DOCTYPE html><html translate="no" dir="ltr" lang="zh-TW"><head><title>ICLR2025 大會心得</title><meta charset="utf-8"><meta name="title" content="ICLR2025 大會心得"><meta name="description" content="我們整理了一些 ICLR 2025 中最有趣的論文，包括 TIPS、FlexPrefill、零樣本重排器 (Zero-Shot Rerankers)、SVD-LLM、Hymba 等。"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="og:title" content="ICLR2025 大會心得"><meta property="og:description" content="我們整理了一些 ICLR 2025 中最有趣的論文，包括 TIPS、FlexPrefill、零樣本重排器 (Zero-Shot Rerankers)、SVD-LLM、Hymba 等。"><meta property="og:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="twitter:title" content="ICLR2025 大會心得"><meta property="twitter:description" content="我們整理了一些 ICLR 2025 中最有趣的論文，包括 TIPS、FlexPrefill、零樣本重排器 (Zero-Shot Rerankers)、SVD-LLM、Hymba 等。"><meta property="twitter:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-DT0WMXkH.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-D1NNdesA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-BdmlIiiV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-4vAClytG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-Dh2VRU1i.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-bx0z6pB2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-Du-mcGhl.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/zh-tw-Dz4le2qD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BlFyrnmJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-Dg41fNXJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-C0CcRHgl.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-D2tyvQ_A.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-h_EOUvUJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-BXPHD_Rd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-BklE9DT8.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-CjwcgHlR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-BFOc4AyV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-B39lhuOo.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-BYjyM6EQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-CsmeSBHc.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-BsYGlil0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-BDJ7RyJA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-K7W-cY9U.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-peIhr9Jp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DBebhe92.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-D_M035NG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-YWuEdvZ9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-D3scaTVE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-LdmFp2FG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-DZS7MNsd.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-BfS3_prr.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-BwJwTn_-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-CYgB_clr.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-DO7dNblr.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-B-CcG8-m.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-0-2UlpUb.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-C1vDmaBY.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Jina AI"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Jina AI"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="21 mins read"><meta property="article:published_time" content="2025-05-26T00:06:37.000+02:00"><meta property="article:modified_time" content="2025-05-26T00:06:37.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "ICLR2025 大會心得",
  "description": "我們整理了一些 ICLR 2025 中最有趣的論文，包括 TIPS、FlexPrefill、零樣本重排器 (Zero-Shot Rerankers)、SVD-LLM、Hymba 等。",
  "image": [
    "https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"
  ],
  "datePublished": "2025-05-26T00:06:37.000+02:00",
  "dateModified": "2025-05-26T00:06:37.000+02:00",
  "author": [
    {
      "@type": "Organization",
      "name": "Jina AI",
      "url": "https://jina.ai"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">新聞</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">模型</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_8f69efec-b4dc-4251-a2a2-ac85b7cbf993" aria-label="展開&quot;產品&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">產品</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_8f69efec-b4dc-4251-a2a2-ac85b7cbf993" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="產品"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">讀取器</div><div class="q-item__label q-item__label--caption text-caption">讀取URL或搜索為大模型提供更好的依據。</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">向量模型</div><div class="q-item__label q-item__label--caption text-caption">世界一流的多模態多語言向量模型。</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">重排器</div><div class="q-item__label q-item__label--caption text-caption">世界一流的重排器，最大限度地提高搜索相關性。</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">深度搜索</div><div class="q-item__label q-item__label--caption text-caption">搜索、讀取並推理直到找到最佳答案。</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_1a1620c9-d47c-492f-8ae9-fbf6c85af96b" aria-label="展開&quot;更多的&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">更多的</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_1a1620c9-d47c-492f-8ae9-fbf6c85af96b" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">分類器</div><div class="q-item__label q-item__label--caption text-caption">圖片和文本的零樣本和少樣本分類。</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">切分器</div><div class="q-item__label q-item__label--caption text-caption">將長文本切分成塊或詞元。</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">API 文檔</div><div class="q-item__label q-item__label--caption text-caption">為您的AI 編程助手 IDE 或大模型自動生成代碼</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_2bc959c2-3cd9-4bb5-854d-06d68c449911" aria-label="展開&quot;公司&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">公司</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_2bc959c2-3cd9-4bb5-854d-06d68c449911" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">關於我們</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">聯繫銷售</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">實習生計劃</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">加入我們</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">下載Logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">條款及條件</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="登錄"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">登錄</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_b198481a-123a-4975-b86f-b745c666dbc0"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_b198481a-123a-4975-b86f-b745c666dbc0" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_b198481a-123a-4975-b86f-b745c666dbc0_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Mitigate the Gap：改善 CLIP 中的跨模態對齊</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">jina-clip-v2：用於文字和圖像的多語言多模態向量模型 (Embeddings)</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ReaderLM-V2：用於 HTML 轉 Markdown 和 JSON 的小模型 (Small Language Model, SLM)</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TIPS：具有空間感知能力的文本圖像預訓練</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Cut Cross-Entropy：用於大型詞彙表的記憶體高效損失計算</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">FlexPrefill：用於長序列的上下文感知稀疏注意力</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">通過溫度控制實現有效的訓練後向量模型 (Embeddings)壓縮</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">大型語言模型中的注意力機制產生高效的零樣本重排器 (Reranker)</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">在成對數據中橋接和建模相關性，以實現直接偏好優化</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TAID：用於高效知識轉移的時序自適應插值蒸餾</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">SVD-LLM：用於大型語言模型壓縮的截斷感知奇異值分解</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">看你被告知的內容：大型多模態模型中的視覺注意力接收器</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">邁向多模態 LLM 中詞元化 (Tokenization) 的語義等價</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Hymba：用於小型語言模型的混合頭架構</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">活動</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">五月 25, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">ICLR2025 大會心得</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">我們整理了一些 ICLR 2025 中最有趣的論文，包括 TIPS、FlexPrefill、零樣本重排器 (Zero-Shot Rerankers)、SVD-LLM、Hymba 等。</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp" style="object-fit: contain; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Jina AI"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Jina AI" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Jina AI • 21 分鐘的讀取量</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>ICLR 2025 是世界上規模最大、最具影響力的機器學習會議之一，與 NeurIPS 和 ICML 並列為三大頂尖的 AI 研究發表場所。今年是 ICLR 首次在亞洲舉辦，於 4 月 24 日至 28 日在新加坡 EXPO 舉行，這是一個歷史性的里程碑。時機再完美不過了——就在 2025 年 1 月下旬的「DeepSeek 時刻」之後幾個月，這件事震驚了矽谷，並展示了中國快速發展的 AI 研究。再加上 2024 年 2 月生效的中國-新加坡 30 天互免簽證協議，我們見證了中國參與者在此次會議上空前激增。</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png" class="kg-image" alt="" width="2000" height="1106" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png 2126w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>今年，我們的團隊很高興能前往新加坡，Sedigheh Eslami、Andreas Koukounas、Wang Feng 和 CEO Han Xiao 將發表三篇研究論文，展示我們在 <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a> 和 <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a> 上為改進搜尋所做的最新研究。當 AI 世界的其他地方似乎都陷入了對更大模型的軍備競賽時，我們決定反其道而行——證明當你把設計做好時，更小、更聰明的模型可以發揮更大的作用。</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png" class="kg-image" alt="" width="2000" height="1391" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png 2108w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>所以，拿起你的咖啡，舒適地坐下來，讓我們一起探索一些我們覺得有趣的 ICLR 研究——首先從我們自己關於為什麼小即是強大的觀點開始。</p><h2 id="mitigate-the-gap-improving-cross-modal-alignment-in-clip" style="position: relative;"><a href="#mitigate-the-gap-improving-cross-modal-alignment-in-clip" title="Mitigate the Gap：改善 CLIP 中的跨模態對齊" id="anchor-mitigate-the-gap-improving-cross-modal-alignment-in-clip"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mitigate the Gap：改善 CLIP 中的跨模態對齊</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.17639"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</div><div class="kg-bookmark-description">Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding space overly sparse and disconnected, with different modalities being densely distributed in distinct subregions of the hypersphere. In this work, we aim at answering three main questions: 1. Does sharing the parameter space between the multi-modal encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart the uni-modal embeddings via intra-modality separation? 3. How do these gap reduction approaches affect the downstream performance? We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings, and thereby, reduces the modality gap, while improving the performance across several zero-shot and fine-tuning downstream evaluations.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-21.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Sedigheh Eslami</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-17.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png" class="kg-image" alt="" width="2000" height="1279" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png 2120w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>CLIP 模型在圖像-文字任務中表現出色，但存在 <a href="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models">「模態差距 (modality gap)」</a>——圖像和文字的向量模型 (Embeddings)聚集在不同的區域，限制了效能。這項工作由我們的實習生 Sedigheh Eslami 在哈索·普拉特納研究所攻讀博士學位期間領導，旨在解決這個根本問題。</p><p>我們發現，簡單的向量轉換會破壞向量模型 (Embedding)的結構。相反地，<strong>AlignCLIP</strong> 使用具有語義正規化分離目標的共享編碼器參數。這種雙重方法成功地減少了模態差距 (modality gap)，同時提高了零樣本和微調任務的效能。</p><p><strong>重點：</strong></p><ul><li>模態差距 (Modality gap)是 CLIP 效能的關鍵瓶頸</li><li>參數共享 + 語義分離有效地彌合了模態差異</li><li>該方法在下游評估中提供了可衡量的增益</li></ul><h2 id="jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" style="position: relative;"><a href="#jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" title="jina-clip-v2：用於文字和圖像的多語言多模態向量模型 (Embeddings)" id="anchor-jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>jina-clip-v2：用於文字和圖像的多語言多模態向量模型 (Embeddings)</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2412.08802"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model’s performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-22.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-18.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png" class="kg-image" alt="" width="2000" height="1115" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png 2268w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>這是 <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a> 背後的論文，它是一個多語言多模態向量模型 (embedding)，使用多任務、多階段對比學習方法，支援純文字和跨模態任務。該模型結合了一個文字編碼器（Jina XLM-RoBERTa，561M 參數）和一個視覺編碼器（EVA02-L14，304M 參數），總共有 865M 參數。我們在來自 29 種非英語語言的多語言文本和視覺豐富的文檔上進行訓練，採用 Matryoshka Representation Learning 來實現靈活的向量模型 (embedding)維度。</p><p><strong>重點：</strong></p><ul><li>由於模態資訊不對稱，在具有共享溫度參數的單一批次中混合圖像-文字和文字-文字數據，比單獨訓練的效果更差。</li><li>跨模態對齊的訓練本質上會損害純文字向量模型 (embedding)的品質，這顯示了一種根本性的權衡。</li><li>將向量模型 (embedding)從 1,024 維度縮減到 256 維度造成的效能損失不到 1%，這揭示了高維表示中存在的大量效率低下。</li></ul><h2 id="readerlm-v2-small-language-model-for-html-to-markdown-and-json" style="position: relative;"><a href="#readerlm-v2-small-language-model-for-html-to-markdown-and-json" title="ReaderLM-V2：用於 HTML 轉 Markdown 和 JSON 的小模型 (Small Language Model, SLM)" id="anchor-readerlm-v2-small-language-model-for-html-to-markdown-and-json"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ReaderLM-V2：用於 HTML 轉 Markdown 和 JSON 的小模型 (Small Language Model, SLM)</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.01151"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</div><div class="kg-bookmark-description">We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model’s effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-23.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Feng Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-19.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png" class="kg-image" alt="" width="1196" height="912" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png 1196w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>這是 <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a> 背後的論文，這是一個緊湊的 15 億參數語言模型，專為高效的網路內容提取而設計。該模型處理高達 512K 個詞元 (Tokens) 的文檔，將混亂的 HTML 轉換為乾淨的 Markdown 或 JSON 格式。我們的方法結合了一個三階段的數據合成流程 (DRAFT-REFINE-CRITIQUE)，該流程透過迭代改進生成高品質的訓練數據，並結合統一的訓練框架，包括連續預訓練、監督微調、直接偏好優化和自我對弈迭代調整。ReaderLM-v2 在基準測試中優於 GPT-4o 和其他更大的模型 15-20%，尤其擅長處理超過 100K 個詞元 (Tokens) 的文檔，同時保持顯著較低的計算需求。</p><p><strong>重點：</strong></p><ul><li>一個 15 億參數的模型在 HTML 提取方面優於 GPT-4o 和 32B 模型 15-20%，證明了針對特定任務的微調勝過原始規模以獲得領域專業知識。</li><li>該模型在第 4 階段「自我對弈」中生成自己的訓練數據，創建比人工策劃的數據集更好的數據集，並透過遞迴回饋不斷提高效能。</li><li>該模型在訓練期間遭受了災難性的詞元 (Tokens) 重複，但添加對比損失以鼓勵區分性表示完全消除了這種退化問題。</li></ul><h2 id="tips-text-image-pretraining-with-spatial-awareness" style="position: relative;"><a href="#tips-text-image-pretraining-with-spatial-awareness" title="TIPS：具有空間感知能力的文本圖像預訓練" id="anchor-tips-text-image-pretraining-with-spatial-awareness"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TIPS：具有空間感知能力的文本圖像預訓練</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.16512"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TIPS: Text-Image Pretraining with Spatial awareness</div><div class="kg-bookmark-description">While image-text representation learning has become very popular in recent years, existing models tend to lack spatial awareness and have limited direct applicability for dense understanding tasks. For this reason, self-supervised image-only pretraining is still the go-to method for many dense vision applications (e.g. depth estimation, semantic segmentation), despite the lack of explicit supervisory signals. In this paper, we close this gap between image-text and self-supervised learning, by proposing a novel general-purpose image-text model, which can be effectively used off the shelf for dense and global vision tasks. Our method, which we refer to as Text-Image Pretraining with Spatial awareness (TIPS), leverages two simple and effective insights. First, on textual supervision: we reveal that replacing noisy web image captions by synthetically generated textual descriptions boosts dense understanding performance significantly, due to a much richer signal for learning spatially aware representations. We propose an adapted training method that combines noisy and synthetic captions, resulting in improvements across both dense and global understanding tasks. Second, on the learning technique: we propose to combine contrastive image-text learning with self-supervised masked image modeling, to encourage spatial coherence, unlocking substantial enhancements for downstream applications. Building on these two ideas, we scale our model using the transformer architecture, trained on a curated set of public images. Our experiments are conducted on 8 tasks involving 16 datasets in total, demonstrating strong off-the-shelf performance on both dense and global understanding, for several image-only and image-text tasks. Code and models are released at https://github.com/google-deepmind/tips.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-24.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Kevis-Kokitsi Maninis</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-20.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png" class="kg-image" alt="" width="2000" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png 2210w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>透過對比學習訓練的視覺語言模型擅長全局圖像文本對齊，但在密集空間理解任務中失敗。TIPS 將對比學習與遮罩圖像建模相結合，並使用編碼空間關係的合成生成標題，創建適用於密集和全局理解的 向量模型 (Embeddings)，而無需針對特定任務進行微調。該方法展示了如何將空間感知納入 向量模型 (Embedding)，以實現更好的文檔理解和多模式檢索應用。</p><p><strong>重點：</strong></p><ul><li>對於學習空間感知表示，具有空間描述的合成標題比嘈雜的網路標題提供更豐富的訓練訊號</li><li>將對比圖像文本學習與自我監督目標相結合，彌合了全局和密集理解之間的差距</li><li>在各種任務上的現成效能消除了跨不同視覺應用進行專門微調的需要</li></ul><h2 id="cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" style="position: relative;"><a href="#cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" title="Cut Cross-Entropy：用於大型詞彙表的記憶體高效損失計算" id="anchor-cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Cut Cross-Entropy：用於大型詞彙表的記憶體高效損失計算</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.09009"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Cut Your Losses in Large-Vocabulary Language Models</div><div class="kg-bookmark-description">As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-25.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Erik Wijmans</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-21.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png" class="kg-image" alt="" width="1904" height="1226" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png 1904w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>交叉熵計算在大型詞彙表語言模型中佔據了主要的記憶體使用量，需要具體化與 batch_size × vocabulary_size 成比例的 logit 矩陣。CCE 重新制定了計算方式，使用自定義 CUDA 核心即時計算僅必要的組件，從而將記憶體消耗從 GB 級減少到 MB 級，同時保持相同的訓練動態。這使得在有限的硬體上訓練具有更大詞彙表的 向量模型 (Embedding) 和 重排器 (Reranker) 模型成為可能，尤其有利於多語言和特定領域的應用。</p><p><strong>重點：</strong></p><ul><li>對於大型詞彙表模型，交叉熵損失計算可能消耗 90% 的訓練記憶體，成為主要的瓶頸</li><li>即時計算 log-sum-exp 項消除了具體化完整 logit 矩陣的需要，而無需進行數學近似</li><li>自定義核心實作可以在保持精確收斂特性的同時顯著減少記憶體</li></ul><h2 id="flexprefill-context-aware-sparse-attention-for-long-sequences" style="position: relative;"><a href="#flexprefill-context-aware-sparse-attention-for-long-sequences" title="FlexPrefill：用於長序列的上下文感知稀疏注意力" id="anchor-flexprefill-context-aware-sparse-attention-for-long-sequences"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>FlexPrefill：用於長序列的上下文感知稀疏注意力</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2502.20766"><div class="kg-bookmark-content"><div class="kg-bookmark-title">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</div><div class="kg-bookmark-description">大型語言模型 (LLM) 在長序列推理過程中面臨計算挑戰，尤其是在注意力預填充階段，其複雜性隨著提示詞 (Prompt) 長度呈平方級增長。先前為了解決這些挑戰所做的努力，依賴於固定的稀疏注意力模式，或基於有限案例識別稀疏注意力模式。然而，這些方法缺乏靈活性，無法有效地適應不同的輸入需求。在本文中，我們介紹了 FlexPrefill，一種靈活的稀疏預填充機制，可即時動態調整稀疏注意力模式和計算預算，以滿足每個輸入和注意力頭的特定需求。我們方法的靈活性通過兩個關鍵創新點來展示：1) 查詢感知稀疏模式確定：通過測量 Jensen-Shannon 散度，該組件自適應地在特定於查詢的多樣化注意力模式和預定義的注意力模式之間切換。2) 基於累積注意力的索引選擇：該組件基於不同的注意力模式動態選擇要計算的查詢-鍵索引，確保注意力分數的總和達到預定義的閾值。FlexPrefill 基於提示詞 (Prompt) 自適應地優化每個注意力頭的稀疏模式和稀疏比率，從而提高長序列推理任務的效率。實驗結果表明，與先前的方法相比，在速度和準確性方面都有顯著的改進，為 LLM 推理提供了一種更靈活和高效的解決方案。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-26.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xunhao Lai</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-22.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png" class="kg-image" alt="" width="1882" height="1254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png 1882w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>長序列 Transformer 推理面臨著平方級的注意力複雜度。FlexPrefill 使用 Jensen-Shannon 散度動態地確定每個頭部的稀疏注意力模式，並基於累積注意力分數自適應地分配計算預算，從而在不同的內容類型中實現顯著的加速，同時將準確性損失降到最低。該方法能夠有效地處理搜尋和檢索系統的長文檔，使較小的語言模型能夠處理擴展的上下文，從而更好地理解文檔。</p><p><strong>要點：</strong></p><ul><li>適應內容類型的動態稀疏注意力模式，優於不同輸入特徵的固定稀疏策略</li><li>基於注意力分數累積的每個頭部的自適應預算分配，可即時優化計算分佈</li><li>上下文感知的稀疏性實現了 13.7 倍的加速，且準確性損失僅為 0.1%，同時無需重新訓練模型</li></ul><h2 id="effective-post-training-embedding-compression-via-temperature-control" style="position: relative;"><a href="#effective-post-training-embedding-compression-via-temperature-control" title="通過溫度控制實現有效的訓練後向量模型 (Embeddings)壓縮" id="anchor-effective-post-training-embedding-compression-via-temperature-control"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>通過溫度控制實現有效的訓練後向量模型 (Embeddings)壓縮</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://openreview.net/forum?id=szRmEM8Kx5"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Effective post-training embedding compression via temperature...</div><div class="kg-bookmark-description">Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates…</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-37.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">OpenReview.net</span><span class="kg-bookmark-publisher">Georgiana Dinu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/pdf_icon_blue.svg" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png" class="kg-image" alt="" width="1230" height="906" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png 1230w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>對比學習中的溫度縮放顯著影響了學習到的向量模型 (Embeddings)的內在維度，較低的溫度會產生更易於壓縮的表示。該論文表明，溫度聚合方法可以將向量模型 (Embeddings)維度降低一個數量級，同時保持檢索性能，揭示了聚類有效性和檢索準確性之間的權衡。這使得密集檢索系統能夠高效部署，在這些系統中，記憶體約束對於生產應用至關重要。</p><p><strong>要點：</strong></p><ul><li>對比訓練中較低的溫度值會產生具有較低內在維度的向量模型 (Embeddings)，從而更有效地壓縮</li><li>溫度聚合技術在檢索任務中實現了 10 倍的壓縮率，且品質下降極小</li><li>在訓練期間系統地控制溫度，為優化壓縮-性能權衡提供了一種直接的機制</li></ul><h2 id="attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" style="position: relative;"><a href="#attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" title="大型語言模型中的注意力機制產生高效的零樣本重排器 (Reranker)" id="anchor-attention-in-large-language-models-yields-efficient-zero-shot-re-rankers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>大型語言模型中的注意力機制產生高效的零樣本重排器 (Reranker)</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.02642"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers</div><div class="kg-bookmark-description">Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more directly leverage such signals, we propose in-context re-ranking (ICR), a novel method that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>) forward passes to re-rank <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> documents, making it substantially more efficient than generative re-ranking methods that require at least <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mclose">)</span></span></span></span></span> forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR’s performance is specially strong on tasks that require more complex re-ranking signals. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-27.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shijie Chen</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-23.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd_nbgJG03yk0oOma7ozLQ6zutKGy3ngkVLKUwmp3ie6UPp2RR6qjwsWzwwtP0QzyAneCTD24nrPXpA085rkjtS_HmlrkHbrksxSjsaknx1lb9OtgtlACmwoOZkoRK9oPb4Haf?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>In-Context Re-ranking (ICR) 利用大型語言模型 (LLM) 中的注意力模式變化來執行文檔重排序，而無需生成文本，從而將計算複雜度從 O(N log N) 降低到 O(1)。該方法聚合跨層和頭部的注意力權重以計算相關性分數，並使用無內容的查詢校準來減輕 LLM 的偏差。這種方法可以使用開放權重的模型實現高效的重排序，而無需專門的微調或昂貴的生成過程。</p><p><strong>要點：</strong> </p><ul><li>大型語言模型 (LLM) 中的注意力模式包含足夠的信號，可以有效地進行文檔重排序，而無需生成文本</li><li>無內容的查詢校準成功地減輕了基於注意力的評分機制中的內在偏差</li><li>與生成方法相比，ICR 實現了卓越的性能和效率，尤其是在複雜的多跳檢索任務中</li></ul><h2 id="bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" style="position: relative;"><a href="#bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" title="在成對數據中橋接和建模相關性，以實現直接偏好優化" id="anchor-bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>在成對數據中橋接和建模相關性，以實現直接偏好優化</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2408.07471"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization</div><div class="kg-bookmark-description">直接偏好最佳化 (Direct preference optimization, DPO) 是一種廣泛採用的離線偏好最佳化演算法，旨在利用成對偏好資料，使大型語言模型 (LLM) 與人類期望的行為保持一致。然而，成對資料中獲勝回應和失敗回應的產生通常是孤立的，導致它們之間的關聯性較弱，以及次佳的對齊效能。為了應對這個問題，我們提出了一個有效的框架，用於橋接和建模成對資料中的相關性，名為 BMC。首先，我們透過有針對性的修改來提高成對偏好訊號的一致性和資訊量，透過以獲勝回應為參考來改進失敗回應，從而合成偽獲勝回應。其次，我們發現僅靠 DPO 不足以對這些相關性進行建模並捕捉細微的變化。因此，我們提出透過在訓練期間動態利用策略模型的置信度來學習詞元 (Tokens) 等級的相關性。在 QA、數學和指令遵循任務上的綜合實驗證明了我們方法的有效性，顯著超越了包括 DPO 在內的具有競爭力的基準。此外，我們深入的定量分析揭示了我們的方法優於 DPO 的原因，並展示了其對其他 DPO 變體的通用性。我們在 https://github.com/YJiangcm/BMC 發布了我們的儲存庫。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-28.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Yuxin Jiang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-24.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd94pEqUtljUS8gF3KonDFqs9umcVCfXEASHlAeCjl07YMviucHiIj1doZIe5_VHSVxthzhgA_ta0E90vQVcunSRj0UnHsubFzD75ow-EfNICcDadQvdtUx-WOZGt9v9rFB_4E?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>傳統的 DPO 受到偏好對中選擇的回應和拒絕的回應之間關聯性較弱的影響，從而限制了對齊效果。 BMC 透過合成在獲勝回應和失敗回應之間插值的偽首選回應來解決此問題，然後使用策略模型置信度應用詞元 (Tokens) 等級的相關性建模。這個兩階段的方法首先透過有針對性的修改來橋接偏好對，然後在訓練期間對細粒度相關性進行建模，以提高學習訊號品質。</p><p><strong>重點：</strong></p><ul><li>偏好資料中選擇的回應和拒絕的回應之間關聯性較弱，顯著限制了 DPO 在模型對齊方面的效果</li><li>合成作為偏好對之間插值的偽首選回應，為最佳化提供了更豐富的學習訊號</li><li>使用策略置信度的詞元 (Tokens) 等級相關性建模動態權衡訓練訊號，以捕捉偏好資料中的細微變化</li></ul><h2 id="taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" style="position: relative;"><a href="#taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" title="TAID：用於高效知識轉移的時序自適應插值蒸餾" id="anchor-taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TAID：用於高效知識轉移的時序自適應插值蒸餾</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2501.16937"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TAID：用於語言模型中高效知識轉移的時序自適應插值蒸餾</div><div class="kg-bookmark-description">因果語言模型已經展示了卓越的能力，但它們的規模對資源受限環境中的部署提出了重大挑戰。知識蒸餾是一種廣泛使用的技術，用於將知識從大型教師模型轉移到小型學生模型，為模型壓縮提供了一種有前景的方法。一個重要的遺留問題是教師模型和學生模型之間的主要差異，即巨大的容量差距、模式平均和模式崩潰，這些都在蒸餾過程中造成了障礙。為了解決這些問題，我們引入了<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">時序自適應插值蒸餾</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">(TAID)</mtext></mrow><annotation encoding="application/x-tex">\textit{時序自適應插值蒸餾 (TAID)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord textit cjk_fallback">時序自適應插值蒸餾</span><span class="mord textit">&nbsp;(TAID)</span></span></span></span></span></span>，這是一種新穎的知識蒸餾方法，它透過自適應中間分佈動態插值學生分佈和教師分佈，從學生初始分佈逐漸轉向教師分佈。我們提供了一個理論分析，證明了 TAID 能夠防止模式崩潰，並透過實驗證明了它在解決容量差距，同時平衡模式平均和模式崩潰方面的有效性。我們的綜合實驗證明了 TAID 在指令調整和預訓練場景中，在各種模型大小和架構上的卓越效能。此外，我們透過開發兩種最先進的緊湊型基礎模型展示了 TAID 的實際影響：用於語言任務的 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-LLM-1.5B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-LLM-1.5B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-LLM-1.5B</span></span></span></span></span></span> 和用於視覺語言任務的 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-VLM-2B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-VLM-2B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-VLM-2B</span></span></span></span></span></span>。這些結果證明了 TAID 在創建高效能和高效模型方面的有效性，從而推動了更易於訪問的 AI 技術的發展。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-29.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Makoto Shing</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-25.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe9EWtq20jDVieU8M2BPDP5kENd3oSJKwIKNnKq_9bB4mb9vtNvjK-RMx8ZB29EhnyjIST90b2HRNek6bSkPXFlOxzTPhUAjf86d6iBCphJtgjfcxrCdY__HcDW9ADgVla1mVWBpQ?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="312" style="cursor: help;"></figure><p>當在大型模型和小型模型之間傳輸知識時，知識蒸餾面臨著容量差距、模式平均和模式崩潰的挑戰。 TAID 引入了一種動態中間教師，它在學生分佈和教師分佈之間進行插值，根據訓練進度逐漸調整目標分佈。這種方法透過理論保證防止了模式崩潰，並在各種模型大小上實現了卓越的效能，從而能夠開發緊湊但功能強大的語言模型。</p><p><strong>重點： </strong></p><ul><li>與固定的教師蒸餾相比，在訓練期間進行調整的動態中間教師提供了更平滑的學習軌跡</li><li>TAID 透過自適應插值防止了模式崩潰，同時平衡了不同容量差距之間的知識轉移</li><li>該方法能夠訓練最先進的緊湊模型，而無需專門的架構或廣泛的超參數調整</li></ul><h2 id="svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" style="position: relative;"><a href="#svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" title="SVD-LLM：用於大型語言模型壓縮的截斷感知奇異值分解" id="anchor-svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>SVD-LLM：用於大型語言模型壓縮的截斷感知奇異值分解</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2403.07378"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SVD-LLM：用於大型語言模型壓縮的截斷感知奇異值分解</div><div class="kg-bookmark-description">大型語言模型 (LLM) 的發展受到其龐大規模的阻礙，這需要 LLM 壓縮方法才能進行實際部署。奇異值分解 (SVD) 為 LLM 壓縮提供了一種有前景的解決方案。然而，目前最先進的基於 SVD 的 LLM 壓縮方法有兩個主要限制：截斷較小的奇異值可能會導致更高的壓縮損失，並且在 SVD 截斷後，壓縮權重缺乏更新。在這項工作中，我們提出了 SVD-LLM，一種基於 SVD 的後訓練 LLM 壓縮方法，旨在解決現有方法的局限性。 SVD-LLM 結合了一種截斷感知資料美白技術，以確保奇異值和壓縮損失之間的直接映射。此外，SVD-LLM 採用具有序列低秩近似的參數更新，以補償 SVD 壓縮後的精度下降。我們在來自三個不同 LLM 系列的 10 個資料集和 7 個模型上，以三種不同的規模評估了 SVD-LLM。我們的結果證明了 SVD-LLM 優於最先進的方法，尤其是在高模型壓縮率下。我們的程式碼可在 https://github.com/AIoT-MLSys-Lab/SVD-LLM 取得</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-30.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-26.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXenhj46Ar7NKFevDTmA3FK2dnjd7nQxdhULJ1H3Je-2OKoQN6_Ov8km-AvIldpEriENz2Q465hq2yoOZ1lLAle7ijbMgSK0ME9UxNeIN3yqyRFtRO_3FFXEyXdI04wndPS17a-3?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>現有的基於 SVD 的壓縮方法未能考慮近似期間的輸入激活，並且缺乏截斷後的微調。 SVD-LLM 結合了考慮激活分佈的截斷感知資料美白，並在壓縮後應用基於 LoRA 的微調。該方法建立了奇異值和壓縮損失之間的理論聯繫，從而實現了比結構化剪枝和量化方法更合理的壓縮決策。</p><p><strong>重點：</strong> </p><ul><li>考慮輸入激活的截斷感知資料美白顯著提高了 SVD 壓縮效果，優於與激活無關的方法</li><li>壓縮後的 LoRA 微調彌補了精度下降，同時保持了低秩分解的優勢</li><li>將奇異值與壓縮損失聯繫起來的理論分析能夠做出合理的截斷決策，優於啟發式方法</li></ul><h2 id="see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" style="position: relative;"><a href="#see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" title="看你被告知的內容：大型多模態模型中的視覺注意力接收器" id="anchor-see-what-you-are-told-visual-attention-sink-in-large-multimodal-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>看你被告知的內容：大型多模態模型中的視覺注意力接收器</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.03321"><div class="kg-bookmark-content"><div class="kg-bookmark-title">看你被告知的內容：大型多模態模型中的視覺注意力接收器</div><div class="kg-bookmark-description">大型多模態模型 (Large multimodal models, LMMs) 透過轉換器解碼器中文字與視覺詞元 (Tokens) 之間的注意力機制來「看」圖像。理想情況下，這些模型應該專注於與文字詞元 (Token) 相關的關鍵視覺資訊。然而，最近的研究表明，LMMs 有一種非同尋常的趨勢，即始終如一地將高注意力權重分配給特定的視覺詞元 (Tokens)，即使這些詞元 (Tokens) 與相應的文字無關。在本研究中，我們研究了這些無關視覺詞元 (Tokens) 出現的特性，並檢查了它們的特徵。我們的研究結果表明，這種行為是由於某些隱藏狀態維度的大量激活所引起的，這類似於語言模型中發現的注意力沉沒 (attention sink)。因此，我們將這種現象稱為視覺注意力沉沒 (visual attention sink)。特別是，我們的分析表明，儘管接收到很高的注意力權重，但移除無關的視覺沉沒詞元 (Tokens) 並不影響模型的效能。因此，我們將對這些詞元 (Tokens) 的注意力回收為剩餘資源，重新分配注意力預算，以增強對圖像的關注。為了實現這一點，我們引入了視覺注意力重新分配 (Visual Attention Redistribution, VAR)，這是一種在以圖像為中心的注意力頭中重新分配注意力的方法，我們將其識別為本質上專注於視覺資訊。VAR 可以無縫地應用於不同的 LMMs，以提高在各種任務上的效能，包括通用視覺語言任務、視覺幻覺任務和以視覺為中心的任務，所有這些都不需要額外的訓練、模型或推論步驟。實驗結果表明，VAR 使 LMMs 能夠透過調整其內部注意力機制更有效地處理視覺資訊，為增強 LMMs 的多模態能力提供了一個新的方向。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-31.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Seil Kang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-27.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrHjLu7qX5TOjtDH10svBjs-6rihxNRpgS3Bq9r8qtY9UvOC4LqyBo-NDWeESuRrv-vj6btANt6doA4IneaENN1712o3kzHhQwx20PR62b8JKDA5jIjCNgKAhXoCp9bEcbadyfPA?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>大型多模態模型 (Large multimodal models) 表現出一種稱為「視覺注意力沉沒 (visual attention sink)」的現象，即它們始終如一地將高注意力權重分配給特定的視覺詞元 (Tokens)，而這些詞元 (Tokens) 與相應的文字詞元 (Tokens) 無關。這些無關的視覺詞元 (Tokens) 出現是由於特定隱藏狀態維度的大量激活，類似於語言模型中的注意力沉沒 (attention sinks)。視覺注意力重新分配 (Visual Attention Redistribution, VAR) 方法識別以圖像為中心的注意力頭，並將注意力預算從沉沒詞元 (Tokens) 重新分配到有意義的視覺內容，從而提高跨視覺語言任務的效能，而無需額外的訓練。</p><p><strong>要點：</strong></p><ul><li>視覺沉沒詞元 (Tokens) 可以透過從基礎語言模型繼承的固定維度中的極端激活幅度來識別</li><li>移除視覺沉沒詞元 (Tokens) 儘管接收到很高的注意力權重，但不會影響模型效能，這表明浪費了計算資源</li><li>VAR 將注意力從沉沒詞元 (Tokens) 重新分配到有意義的視覺內容，從而提高通用視覺語言、減少幻覺和以視覺為中心的任務的效能</li></ul><h2 id="towards-semantic-equivalence-of-tokenization-in-multimodal-llm" style="position: relative;"><a href="#towards-semantic-equivalence-of-tokenization-in-multimodal-llm" title="邁向多模態 LLM 中詞元化 (Tokenization) 的語義等價" id="anchor-towards-semantic-equivalence-of-tokenization-in-multimodal-llm"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>邁向多模態 LLM 中詞元化 (Tokenization) 的語義等價</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.05127"><div class="kg-bookmark-content"><div class="kg-bookmark-title">邁向多模態 LLM 中詞元化 (Tokenization) 的語義等價</div><div class="kg-bookmark-description">多模態大型語言模型 (Multimodal Large Language Models, MLLMs) 在處理視覺語言任務方面表現出卓越的能力。MLLMs 的關鍵之一在於視覺詞元化 (vision tokenization)，它涉及有效地將輸入視覺訊號轉換為對 LLM 最有利的特徵表示。然而，現有的視覺詞元器 (vision tokenizers) 對於視覺和語言之間的語義對齊至關重要，但仍然存在問題。現有的方法過度地分割視覺輸入，破壞了視覺語義的完整性。為了解决這個問題，本文提出了一種新穎的動態語義等價視覺詞元器 (Semantic-Equivalent Vision Tokenizer, SeTok)，它透過動態聚類演算法將視覺特徵分組成語義單元，根據圖像複雜性靈活地確定詞元 (Tokens) 的數量。由此產生的視覺詞元 (Tokens) 有效地保留了語義完整性，並捕捉了低頻和高頻視覺特徵。實驗結果證明，配備 SeTok 的所提出的 MLLM (Setokim) 在各種任務中顯著地表現出卓越的效能。專案頁面位於 https://chocowu.github.io/SeTok-web/。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-32.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shengqiong Wu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-28.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXtLgOhJPcWaPKTtGm7D3lK6tc7EhPQGXleGMHMqYG_KFVTxCSBGOd8z6xovad6UMgDjTWPBFfKqD4J2gSD6L6YXpSaTlGNNrLWiViAlfPkKinc9jNjsD2Ulnrh0tZQ74RR62tvQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>多模態 LLM 中傳統的視覺詞元化 (vision tokenization) 方法使用固定圖塊分割視覺輸入，破壞了語義完整性，並導致不良的視覺語言對齊。SeTok (Semantic-Equivalent Vision Tokenizer) 透過動態聚類來解决這個問題，該聚類將視覺特徵分組為連貫的語義單元，並且詞元 (Token) 計數會根據圖像複雜性進行調整。該系統使用雙重訓練目標：用於與語言進行語義對齊的對比損失和用於保留圖像重建的像素級細節的重建損失。</p><p><strong>主要要點：</strong></p><ul><li>固定圖塊詞元化 (tokenization) 透過跨任意圖塊邊界分割物件來破壞視覺語義的完整性</li><li>動態聚類演算法可以根據圖像語義複雜性（而不是固定網格結構）自適應地確定最佳詞元 (Token) 計數</li><li>雙重目標訓練平衡了與語言的語義對齊，同時保留了足夠的視覺細節以用於重建任務</li></ul><h2 id="hymba-a-hybrid-head-architecture-for-small-language-models" style="position: relative;"><a href="#hymba-a-hybrid-head-architecture-for-small-language-models" title="Hymba：用於小型語言模型的混合頭架構" id="anchor-hymba-a-hybrid-head-architecture-for-small-language-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Hymba：用於小型語言模型的混合頭架構</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.13676"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hymba：用於小型語言模型的混合頭架構</div><div class="kg-bookmark-description">我們提出了 Hymba，這是一個小型語言模型系列，其特點是混合頭平行架構，該架構將轉換器注意力機制與狀態空間模型 (state space models, SSMs) 整合在一起，以提高效率。注意力頭提供高解析度的回憶，而 SSM 頭則實現有效的上下文摘要。此外，我們引入了可學習的元詞元 (meta tokens)，這些元詞元 (meta tokens) 會添加到提示詞 (Prompts) 的前面，用於儲存關鍵資訊並減輕與注意力機制相關的「被迫關注」負擔。該模型透過整合跨層鍵值 (key-value, KV) 共享和部分滑動視窗注意力來進一步優化，從而形成緊湊的快取大小。在開發過程中，我們進行了一項受控研究，比較了相同設定下的各種架構，並觀察到我們提出的架構的顯著優勢。值得注意的是，Hymba 為小型 LM 取得了最先進的成果：我們的 Hymba-1.5B-Base 模型在效能上超越了所有 2B 以下的公開模型，甚至優於 Llama-3.2-3B，平均準確度提高了 1.32%，快取大小減少了 11.67 倍，吞吐量提高了 3.49 倍。</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-33.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Dong</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-29.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqTXEnWvnqhnbdUKsRw-mZ8hzuqwIbX_dnFwiY4BHa8DF4ViWiekeIRVlRBtQkJF8a2EPv5U_H5kvqxFQfCg0jGplWefzce1RHzHBd17D93k6DpE3vNurR0Ufg7kMEJ_C4IeDBZQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="552" height="573" style="cursor: help;"></figure><p>Hymba 引入了一種混合頭架構，該架構在每一層中並行地將轉換器注意力機制與狀態空間模型 (state space models, SSMs) 結合在一起，從而實現同時進行高解析度的回憶和有效的上下文摘要。該架構結合了可學習的元詞元 (meta tokens)、跨層鍵值共享和部分滑動視窗注意力，以實現緊湊的快取大小。Hymba-1.5B 超越了所有 2B 以下的模型，並且優於 Llama-3.2-3B，同時實現了 11.67 倍的快取減少和 3.49 倍的吞吐量改進。</p><p><strong>要點：</strong></p><ul><li>並行混合頭架構優於注意力和 SSM 組件的順序堆疊，因為它可以同時處理互補機制</li><li>可學習的元詞元 (meta tokens) 充當壓縮的世界知識，並減輕了 softmax 注意力機制的「被迫關注」負擔</li><li>跨層鍵值共享和滑動視窗注意力優化在不犧牲效能的情況下實現了顯著的快取大小減少</li></ul></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">類別:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">活動</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fzh-TW%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fzh-TW%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fzh-TW%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fzh-TW%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fzh-TW%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">辦公室</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">加利福尼亞州桑尼維爾</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, 桑尼維爾, CA 94085, 美國</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">德國柏林（總部）</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20，10969 柏林，德國</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">中國北京</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">中國北京市海淀區西大街48號6號樓5層</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">中國深圳</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">中國深圳市賦安科技大廈4樓402</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">搜索底座</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">讀取器</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">向量模型</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">重排器</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">深度搜索</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">分類器</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">切分器</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">API 文檔</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">獲取 Jina API 密鑰</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">速率限制</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">API 狀態</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">公司</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">關於我們</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">聯繫銷售</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">新聞</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">實習生計劃</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">加入我們</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">下載Logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">條款</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">安全</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">條款及條件</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">隱私</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">管理 Cookie</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
<!DOCTYPE html><html translate="no" dir="ltr" lang="en-US"><head><title>ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?</title><meta charset="utf-8"><meta name="title" content="ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?"><meta name="description" content="Jina AI의 Hugging Face 상의 ColBERT가 8192 토큰 처리 능력으로 Twitter를 뜨겁게 달구며 검색 분야에 새로운 시각을 제시했습니다. 이 글에서는 ColBERT와 ColBERTv2의 미묘한 차이점을 살펴보고, 혁신적인 설계와 후기 상호작용(late interaction) 기능이 검색을 혁신적으로 바꾸는 이유를 소개합니다."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"><meta property="og:title" content="ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?"><meta property="og:description" content="Jina AI의 Hugging Face 상의 ColBERT가 8192 토큰 처리 능력으로 Twitter를 뜨겁게 달구며 검색 분야에 새로운 시각을 제시했습니다. 이 글에서는 ColBERT와 ColBERTv2의 미묘한 차이점을 살펴보고, 혁신적인 설계와 후기 상호작용(late interaction) 기능이 검색을 혁신적으로 바꾸는 이유를 소개합니다."><meta property="og:image" content="https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"><meta property="twitter:title" content="ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?"><meta property="twitter:description" content="Jina AI의 Hugging Face 상의 ColBERT가 8192 토큰 처리 능력으로 Twitter를 뜨겁게 달구며 검색 분야에 새로운 시각을 제시했습니다. 이 글에서는 ColBERT와 ColBERTv2의 미묘한 차이점을 살펴보고, 혁신적인 설계와 후기 상호작용(late interaction) 기능이 검색을 혁신적으로 바꾸는 이유를 소개합니다."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-CFIL_oFr.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-Db0tgwFK.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-DaO5p_Fi.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-DOF-ZeYh.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-k1sPC4JF.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-CbdD4mkS.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-DkDSvH36.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-D1S8lOo9.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ko-kc2g2gNR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-CNmtTF9r.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-CsTTBvRe.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-D-mLV1DP.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-UhiIxGC3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/PurchaseSuccessDialog-O925KtQU.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-DvSmXvIx.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-DkuDAuFV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-ChkjZlPE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-Bq796LnA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-BzI5dnwO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-BvDACZE8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-D4_tDs7p.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-BJwF1I9Q.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-B7n1dc8c.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-D3_wwNMk.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-BbAoufWX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-BaZdpxk3.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-BBhkin09.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-CFvj_zty.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-Co7vaquz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-B2mnQm7z.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-DuQMcLZj.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-__S2BMiv.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-CXarHZEm.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-bYrO6D_i.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-Yw1XlP-q.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-F8MwYv0G.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-Czs8C5e3.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-BoiqfsFc.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-vSZoRHSY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Han Xiao"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Han Xiao"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-02-20T02:19:04.000+01:00"><meta property="article:modified_time" content="2024-08-30T23:11:22.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?",
  "description": "Jina AI의 Hugging Face 상의 ColBERT가 8192 토큰 처리 능력으로 Twitter를 뜨겁게 달구며 검색 분야에 새로운 시각을 제시했습니다. 이 글에서는 ColBERT와 ColBERTv2의 미묘한 차이점을 살펴보고, 혁신적인 설계와 후기 상호작용(late interaction) 기능이 검색을 혁신적으로 바꾸는 이유를 소개합니다.",
  "image": [
    "https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"
  ],
  "datePublished": "2024-02-20T02:19:04.000+01:00",
  "dateModified": "2024-08-30T23:11:22.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Han Xiao",
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script charset="utf-8" src="https://platform.twitter.com/js/tweet.d7aeb21a88e025d2ea5f5431a103f586.js"></script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-478b3f77="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-478b3f77="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-478b3f77="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-478b3f77="" class="q-space"></div><button data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-478b3f77="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-478b3f77="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-478b3f77="" class="q-list q-list--dark" role="list"><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">소식</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">모델</div></a><div data-v-478b3f77="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_131e25c4-e068-4e60-91f8-c2877879e70a" aria-label="Expand &quot;제품&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">제품</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_131e25c4-e068-4e60-91f8-c2877879e70a" style="display: none;"><div data-v-478b3f77="" class="q-list q-list--dark" role="list" label="제품"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">심층 검색</div><div class="q-item__label q-item__label--caption text-caption">검색하고, 읽고, 추론하여 가장 좋은 답을 찾으세요.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">리더</div><div class="q-item__label q-item__label--caption text-caption">URL을 읽거나 더 큰 모델에 대한 더 나은 통찰력을 검색하세요.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">벡터 모델</div><div class="q-item__label q-item__label--caption text-caption">세계적 수준의 다중 모드 다중 언어 임베딩.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">재배열자</div><div class="q-item__label q-item__label--caption text-caption">검색 관련성을 극대화하는 세계적 수준의 신경 검색기입니다.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_4adec5fd-684f-49a0-b8a0-28f97be5c397" aria-label="Expand &quot;더&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">더</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_4adec5fd-684f-49a0-b8a0-28f97be5c397" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">분류자</div><div class="q-item__label q-item__label--caption text-caption">이미지와 텍스트의 제로샷 및 퓨샷 분류.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">얇게 써는 기계</div><div class="q-item__label q-item__label--caption text-caption">긴 텍스트를 청크 또는 토큰으로 분할합니다.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">API 문서</div><div class="q-item__label q-item__label--caption text-caption">AI 프로그래밍 어시스턴트 IDE 또는 대형 모델에 대한 코드를 자동으로 생성합니다.</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-478b3f77="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_e0a8e5cb-f2ef-49d4-b818-384484d39e64" aria-label="Expand &quot;회사&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">회사</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_e0a8e5cb-f2ef-49d4-b818-384484d39e64" style="display: none;"><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">회사 소개</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">영업팀에 문의</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">인턴 프로그램</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">우리와 함께</div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">로고 다운로드</div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">이용약관</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="로그인"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">로그인</div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-478b3f77="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_87e61ee9-bc1a-46cc-beab-0ba5e023d14b"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_87e61ee9-bc1a-46cc-beab-0ba5e023d14b" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_87e61ee9-bc1a-46cc-beab-0ba5e023d14b_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-478b3f77="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT란 무엇인가?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT의 설계 이해하기</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT의 질의 및 문서 인코더</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT를 사용한 상위 K개 문서 찾기</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT의 인덱싱 전략</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ColBERT의 효과성과 효율성</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">jina-colbert-v1-en 사용하기: 8192 길이의 ColBERTv2 모델</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">결론</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><i class="q-icon notranslate material-symbols material-symbols-sharp q-chip__icon q-chip__icon--left" aria-hidden="true" role="presentation">star</i><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">선택</div></div></div><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">기술 기사</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">2월 20, 2024</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">ColBERT와 Late Interaction이란 무엇이며 검색에서 왜 중요한가?</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Jina AI의 Hugging Face 상의 ColBERT가 8192 토큰 처리 능력으로 Twitter를 뜨겁게 달구며 검색 분야에 새로운 시각을 제시했습니다. 이 글에서는 ColBERT와 ColBERTv2의 미묘한 차이점을 살펴보고, 혁신적인 설계와 후기 상호작용(late interaction) 기능이 검색을 혁신적으로 바꾸는 이유를 소개합니다.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img" aria-label="Neon theater or concert hall marquee letters lit up at night with city lights and faint &quot;Adobe Sto&quot; visible."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Neon theater or concert hall marquee letters lit up at night with city lights and faint &quot;Adobe Sto&quot; visible." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Han Xiao • 16 독서 시간</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content">{{{input start}}}
<figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina ColBERT v2: 임베딩과 재순위화를 위한 다국어 후기 상호작용 검색기</div><div class="kg-bookmark-description">Jina ColBERT v2는 89개 언어를 지원하며 우수한 검색 성능, 사용자 지정 출력 차원, 8192 토큰 길이를 제공합니다.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg" alt="" style="cursor: help;"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">업데이트: 2024년 8월 31일, 향상된 성능, 89개 언어에 대한 다국어 지원 및 유연한 출력 차원을 갖춘 Jina-ColBERT의 2번째 버전을 출시했습니다. 자세한 내용은 출시 게시물을 확인하세요.</span></p></figcaption></figure><p>지난 금요일, <a href="https://huggingface.co/jinaai/jina-colbert-v1-en">Hugging Face에서 Jina AI의 ColBERT 모델</a> 출시는 특히 Twitter/X에서 AI 커뮤니티 전반에 걸쳐 상당한 관심을 불러일으켰습니다. 많은 사람들이 혁신적인 BERT 모델에 익숙하지만, ColBERT에 대한 관심은 일부 사람들에게 의문을 남겼습니다: 정보 검색 기술 분야에서 ColBERT가 두드러지는 점은 무엇일까요? AI 커뮤니티가 8192 길이의 ColBERT에 대해 왜 흥분하고 있을까요? 이 글에서는 ColBERT와 ColBERTv2의 세부사항을 살펴보고, 그들의 설계, 개선사항, 그리고 ColBERT의 후기 상호작용의 놀라운 효과성을 조명합니다.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">검색 관련성과 RAG 정확도를 쉽게 극대화하세요</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-reranker-api.png" alt="" style="cursor: help;"></div></a></figure><figure class="kg-card kg-embed-card"><div class="twitter-tweet twitter-tweet-rendered" style="display: flex; max-width: 550px; width: 100%; margin-top: 10px; margin-bottom: 10px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="" style="position: static; visibility: visible; width: 550px; height: 671px; display: block; flex-grow: 1;" title="X Post" src="https://platform.twitter.com/embed/Tweet.html?creatorScreenName=JinaAI_&amp;dnt=false&amp;embedId=twitter-widget-0&amp;features=eyJ0ZndfdGltZWxpbmVfbGlzdCI6eyJidWNrZXQiOltdLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2ZvbGxvd2VyX2NvdW50X3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9iYWNrZW5kIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19yZWZzcmNfc2Vzc2lvbiI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfZm9zbnJfc29mdF9pbnRlcnZlbnRpb25zX2VuYWJsZWQiOnsiYnVja2V0Ijoib24iLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X21peGVkX21lZGlhXzE1ODk3Ijp7ImJ1Y2tldCI6InRyZWF0bWVudCIsInZlcnNpb24iOm51bGx9LCJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3Nob3dfYmlyZHdhdGNoX3Bpdm90c19lbmFibGVkIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19kdXBsaWNhdGVfc2NyaWJlc190b19zZXR0aW5ncyI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdXNlX3Byb2ZpbGVfaW1hZ2Vfc2hhcGVfZW5hYmxlZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdmlkZW9faGxzX2R5bmFtaWNfbWFuaWZlc3RzXzE1MDgyIjp7ImJ1Y2tldCI6InRydWVfYml0cmF0ZSIsInZlcnNpb24iOm51bGx9LCJ0ZndfbGVnYWN5X3RpbWVsaW5lX3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9mcm9udGVuZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9fQ%3D%3D&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1758503072999907825&amp;lang=en&amp;origin=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F&amp;sessionId=1e070cc90a126101a225f44a295d9d85a23e146d&amp;siteScreenName=JinaAI_&amp;theme=light&amp;widgetsVersion=2615f7e52b7e0%3A1702314776716&amp;width=550px" data-tweet-id="1758503072999907825"></iframe></div>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-is-colbert" style="position: relative;"><a href="#what-is-colbert" title="ColBERT란 무엇인가?" id="anchor-what-is-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT란 무엇인가?</h2><p>"ColBERT"는 <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>의 약자로, 스탠포드 대학교에서 시작된 모델로, BERT의 깊은 언어 이해력을 활용하면서 새로운 상호작용 메커니즘을 도입했습니다. "후기 상호작용"으로 알려진 이 메커니즘은 검색 프로세스의 최종 단계까지 쿼리와 문서를 별도로 처리함으로써 효율적이고 정확한 검색을 가능하게 합니다. 구체적으로 두 가지 버전의 모델이 있습니다:</p><ul><li><strong>ColBERT</strong>: 초기 모델은 <a href="https://x.com/lateinteraction?s=20"><strong>Omar Khattab</strong></a><strong>와 Matei Zaharia</strong>의 작품으로, "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT" 논문을 통해 정보 검색에 대한 새로운 접근 방식을 제시했습니다. 이 연구는 SIGIR 2020에서 발표되었습니다.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2004.12832"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class="kg-bookmark-description">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Omar Khattab</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="" style="cursor: help;"></div></a><figcaption><p><span style="white-space: pre-wrap;">"후기 상호작용"을 소개하는 원본 ColBERT 논문.</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: 기초 작업을 바탕으로, <strong>Omar Khattab</strong>는 <strong>Barlas Oguz, Matei Zaharia, Michael S. Bernstein</strong>과 협력하여 SIGIR 2021에서 "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"을 발표했습니다. ColBERT의 이 다음 반복 버전은 이전의 한계를 해결하고 <strong>노이즈 제거 감독</strong>과 <strong>잔차 압축</strong>과 같은 주요 개선사항을 도입하여 모델의 검색 효과성과 저장 효율성을 향상시켰습니다.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.01488"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class="kg-bookmark-description">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">×</span></span></span></span></span>.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Keshav Santhanam</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="" style="cursor: help;"></div></a><figcaption><p><span style="white-space: pre-wrap;">ColBERTv2는 훈련 데이터의 품질을 개선하고 공간 점유율을 줄이기 위해 노이즈 제거 감독과 잔차 압축을 추가했습니다.</span></p></figcaption></figure><h2 id="understand-colberts-design" style="position: relative;"><a href="#understand-colberts-design" title="ColBERT의 설계 이해하기" id="anchor-understand-colberts-design"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 설계 이해하기</h2><p>ColBERTv2의 아키텍처가 원래 ColBERT와 매우 유사하고, 주요 혁신이 훈련 기법과 압축 메커니즘을 중심으로 이루어졌기 때문에, 먼저 원래 ColBERT의 기초적인 측면을 살펴보겠습니다.</p><h3 id="what-is-late-interaction-in-colbert" style="position: relative;"><a href="#what-is-late-interaction-in-colbert" title="ColBERT의 후기 상호작용이란 무엇인가?" id="anchor-what-is-late-interaction-in-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 후기 상호작용이란 무엇인가?</h3><p>"상호작용"은 쿼리와 문서의 표현을 비교하여 관련성을 평가하는 프로세스를 의미합니다.</p><p>"<em>후기 상호작용</em>"은 ColBERT의 핵심입니다. 이 용어는 쿼리와 문서 표현 간의 상호작용이 둘 다 독립적으로 인코딩된 후, 프로세스의 후반부에서 발생하는 모델의 아키텍처와 처리 전략에서 유래했습니다. 이는 쿼리와 문서 임베딩이 더 이른 단계, 일반적으로 모델에 의한 인코딩 전이나 도중에 상호작용하는 "<em>조기 상호작용</em>" 모델과 대조됩니다.</p>

<table>
<thead>
<tr>
<th>Interaction Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early Interaction</td>
<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>
</tr>
<tr>
<td>Late Interaction</td>
<td>ColBERT, ColBERTv2</td>
</tr>
</tbody>
</table>

<p>조기 상호작용은 모든 가능한 쿼리-문서 쌍을 고려해야 하므로 계산 복잡성을 증가시킬 수 있어 대규모 애플리케이션에는 덜 효율적입니다.</p><p>ColBERT와 같은 후기 상호작용 모델은 문서 표현의 사전 계산을 허용하고 마지막에 더 가벼운 상호작용 단계를 사용함으로써 효율성과 확장성을 최적화합니다. 이 설계 선택은 더 빠른 검색 시간과 감소된 계산 요구사항을 가능하게 하여 대규모 문서 컬렉션 처리에 더 적합하게 만듭니다.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg" class="kg-image" alt="Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep" width="300" height="143" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">뉴럴 IR에서 질의-문서 상호작용 패러다임을 보여주는 도식도, 왼쪽에는 ColBERT의 후기 상호작용이 있음.</span></figcaption></figure><h3 id="no-interaction-cosine-similarity-of-document-and-query-embeddings" style="position: relative;"><a href="#no-interaction-cosine-similarity-of-document-and-query-embeddings" title="상호작용 없음: 문서와 질의 임베딩의 코사인 유사도" id="anchor-no-interaction-cosine-similarity-of-document-and-query-embeddings"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>상호작용 없음: 문서와 질의 임베딩의 코사인 유사도</h3><p>많은 실용적인 벡터 데이터베이스와 뉴럴 검색 솔루션들은 문서와 질의 임베딩 간의 빠른 코사인 유사도 매칭에 의존합니다. 단순성과 계산 효율성 측면에서 매력적이지만, "상호작용 없음" 또는 "상호작용 기반이 아님"이라고 불리는 이 방법은 질의와 문서 간의 일종의 상호작용을 포함하는 모델들과 비교했을 때 성능이 떨어지는 것으로 나타났습니다.</p><p>"상호작용 없음" 접근 방식의 핵심적인 한계는 질의와 문서 용어 사이의 복잡한 뉘앙스와 관계를 포착하지 못한다는 점입니다. 정보 검색은 본질적으로 질의 뒤의 의도와 문서 내의 내용을 이해하고 매칭하는 것입니다. 이 과정은 종종 관련된 용어들의 깊은 문맥적 이해를 필요로 하는데, 이는 문서와 질의에 대한 단일의 집계된 임베딩으로는 제공하기 어려운 것입니다.</p><h2 id="query-and-document-encoders-in-colbert" style="position: relative;"><a href="#query-and-document-encoders-in-colbert" title="ColBERT의 질의 및 문서 인코더" id="anchor-query-and-document-encoders-in-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 질의 및 문서 인코더</h2><p>ColBERT의 인코딩 전략은 언어의 깊은 문맥적 이해로 알려진 BERT 모델에 기반을 두고 있습니다. 이 모델은 질의나 문서의 각 토큰에 대해 밀집 벡터 표현을 생성하여, <strong>질의에 대한 문맥화된 임베딩 집합과 문서에 대한 임베딩 집합을 각각 생성합니다.</strong> 이는 후기 상호작용 단계에서 임베딩들의 세밀한 비교를 가능하게 합니다.</p><h3 id="query-encoder-of-colbert" style="position: relative;"><a href="#query-encoder-of-colbert" title="ColBERT의 질의 인코더" id="anchor-query-encoder-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 질의 인코더</h3><p>토큰 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>q</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">{q_1, q_2, ..., q_l}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span>를 가진 질의 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span>에 대해, 과정은 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span>를 BERT 기반 WordPiece 토큰으로 토큰화하고 특수 <code>[Q]</code> 토큰을 앞에 붙이는 것으로 시작됩니다. BERT의 <code>[CLS]</code> 토큰 바로 뒤에 위치하는 이 <code>[Q]</code> 토큰은 질의의 시작을 알립니다.</p><p>질의가 미리 정의된 토큰 수 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>보다 짧으면 <code>[mask]</code> 토큰으로 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>까지 패딩되고, 그렇지 않으면 처음 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 토큰으로 잘립니다. 패딩된 시퀀스는 BERT를 통과한 후 CNN(합성곱 신경망)과 정규화를 거칩니다. 출력은 아래와 같이 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9722em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>로 표시되는 임베딩 벡터 집합입니다:<br><span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>q</mi></msub><mo>:</mo><mo>=</mo><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">z</mi><mi mathvariant="normal">e</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">Q</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><msub><mi>q</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>q</mi><mi>l</mi></msub><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{E}_q := \mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[Q]},q_0,q_1,\ldots,q_l\mathtt{[mask]},\mathtt{[mask]},\ldots,\mathtt{[mask]}\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9722em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathrm">Normalize</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">BERT</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">Q</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span></span></p><h3 id="document-encoder-of-colbert" style="position: relative;"><a href="#document-encoder-of-colbert" title="ColBERT의 문서 인코더" id="anchor-document-encoder-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 문서 인코더</h3><p>마찬가지로, 토큰 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">{d_1, d_2, ..., d_n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span>을 가진 문서 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span></span></span></span></span>에 대해, 문서의 시작을 나타내는 <code>[D]</code> 토큰이 앞에 붙습니다. 패딩이 필요 없는 이 시퀀스는 동일한 과정을 거쳐 아래와 같이 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8361em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>로 표시되는 임베딩 벡터 집합이 됩니다:<br><span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>d</mi></msub><mo>:</mo><mo>=</mo><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">z</mi><mi mathvariant="normal">e</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">D</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><msub><mi>d</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{E}_d := \mathrm{Filter}\left(\mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[D]},d_0,d_1,...,d_n\right)\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8361em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathrm">Filter</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">Normalize</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">BERT</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">D</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span></span></p><p>질의 패딩에 <code>[mask]</code> 토큰을 사용하는 것(논문에서 <strong>질의 증강</strong>이라고 명명됨)은 모든 질의에 대해 균일한 길이를 보장하여 배치 처리를 용이하게 합니다. <code>[Q]</code>와 <code>[D]</code> 토큰은 각각 질의와 문서의 시작을 명시적으로 표시하여 모델이 두 종류의 입력을 구분하는 데 도움을 줍니다.</p><h3 id="comparing-colbert-to-cross-encoders" style="position: relative;"><a href="#comparing-colbert-to-cross-encoders" title="ColBERT와 교차 인코더 비교" id="anchor-comparing-colbert-to-cross-encoders"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT와 교차 인코더 비교</h3><p>교차 인코더는 질의와 문서 쌍을 함께 처리하여 매우 정확하지만, 가능한 모든 쌍을 평가하는 데 필요한 계산 비용으로 인해 대규모 작업에는 덜 효율적입니다. 그들은 의미적 유사도 작업이나 상세한 내용 비교와 같이 문장 쌍의 정확한 점수 매기기가 필요한 특정 시나리오에서 뛰어납니다. 하지만 이 설계는 사전 계산된 임베딩과 효율적인 유사도 계산이 중요한 대규모 데이터셋에서의 빠른 검색이 필요한 상황에서는 적용 가능성이 제한됩니다.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg" class="kg-image" alt="Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum" width="210" height="150" style="cursor: help;"></figure><p>반면에 ColBERT의 후기 상호작용 모델은 문서 임베딩의 사전 계산을 허용하여 의미 분석의 깊이를 손상시키지 않으면서 검색 과정을 크게 가속화합니다. 교차 인코더의 직접적인 접근 방식과 비교했을 때 직관적이지 않아 보일 수 있지만, 이 방법은 실시간 및 대규모 정보 검색 작업을 위한 확장 가능한 솔루션을 제공합니다. 이는 계산 효율성과 상호작용 모델링의 품질 사이의 전략적 타협을 나타냅니다.</p><h2 id="finding-the-top-k-documents-using-colbert" style="position: relative;"><a href="#finding-the-top-k-documents-using-colbert" title="ColBERT를 사용한 상위 K개 문서 찾기" id="anchor-finding-the-top-k-documents-using-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT를 사용한 상위 K개 문서 찾기</h2><p>질의와 문서에 대한 임베딩이 있으면 가장 관련성 높은 상위 K개 문서를 찾는 것은 간단해집니다(하지만 두 벡터의 코사인을 계산하는 것만큼 간단하지는 않습니다).</p><p>주요 작업에는 용어별 유사도를 계산하기 위한 배치 내적 연산, 질의 용어별 최고 유사도를 찾기 위한 문서 용어에 대한 최대 풀링, 총 문서 점수를 도출하기 위한 질의 용어에 대한 합산, 그리고 이러한 점수를 기반으로 문서를 정렬하는 것이 포함됩니다. PyTorch 의사 코드는 아래와 같습니다:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_relevance_scores</span>(<span class="hljs-params">query_embeddings, document_embeddings, k</span>):
    <span class="hljs-string">"""
    Compute relevance scores for top-k documents given a query.
    
    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]
    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]
    :param k: Number of top documents to re-rank
    :return: Sorted document indices based on their relevance scores
    """</span>
    
    <span class="hljs-comment"># Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]</span>
    <span class="hljs-comment"># Pad the k documents to their maximum length for batch operations</span>
    <span class="hljs-comment"># Note: Assuming document_embeddings is already padded and moved to GPU</span>
    
    <span class="hljs-comment"># Compute batch dot-product of Eq (query embeddings) and D (document embeddings)</span>
    <span class="hljs-comment"># Resulting shape: [k, num_query_terms, max_doc_length]</span>
    scores = torch.matmul(query_embeddings.unsqueeze(<span class="hljs-number">0</span>), document_embeddings.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
    
    <span class="hljs-comment"># Apply max-pooling across document terms (dim=2) to find the max similarity per query term</span>
    <span class="hljs-comment"># Shape after max-pool: [k, num_query_terms]</span>
    max_scores_per_query_term = scores.<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">2</span>).values
    
    <span class="hljs-comment"># Sum the scores across query terms to get the total score for each document</span>
    <span class="hljs-comment"># Shape after sum: [k]</span>
    total_scores = max_scores_per_query_term.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)
    
    <span class="hljs-comment"># Sort the documents based on their total scores</span>
    sorted_indices = total_scores.argsort(descending=<span class="hljs-literal">True</span>)
    
    <span class="hljs-keyword">return</span> sorted_indices
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>이 절차는 훈련과 추론 시의 재순위화 모두에서 사용됩니다. ColBERT 모델은 쌍별 랭킹 손실을 사용하여 훈련되며, 훈련 데이터는 질의를 나타내는 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span>, 질의와 관련된(긍정) 문서 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">d^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span>, 관련 없는(부정) 문서 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">d^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span></span>로 구성된 삼중항 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>+</mo></msup><mo separator="true">,</mo><msup><mi>d</mi><mo>−</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d^+, d^-)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.0213em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>으로 구성됩니다. 모델은 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span>와 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">d^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span> 사이의 유사도 점수가 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span>와 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">d^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span></span> 사이의 점수보다 높도록 표현을 학습하는 것을 목표로 합니다.</p><p>훈련 목적 함수는 수학적으로 다음과 같은 손실 함수를 최소화하는 것으로 표현할 수 있습니다: <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">L</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi></mrow><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>−</mo><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>+</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>−</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{Loss} = \max(0, 1 - S(q, d^+) + S(q, d^-))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord"><span class="mord mathrm">Loss</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0713em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8213em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0713em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8213em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></span></p><p>, 여기서 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S(q, d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span>는 질의 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span>와 문서 <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></span> 사이에 ColBERT가 계산한 유사도 점수를 나타냅니다. 이 점수는 모델 아키텍처에서 설명한 후기 상호작용 패턴에 따라 질의와 문서 사이의 가장 잘 매칭되는 임베딩의 최대 유사도 점수를 집계하여 얻습니다. 이 접근 방식은 모델이 주어진 질의에 대해 관련 있는 문서와 관련 없는 문서를 구분하도록 훈련되어, 긍정 및 부정 문서 쌍의 유사도 점수에서 더 큰 마진을 장려합니다.</p><h3 id="denoised-supervision-in-colbertv2" style="position: relative;"><a href="#denoised-supervision-in-colbertv2" title="ColBERTv2의 잡음 제거 감독" id="anchor-denoised-supervision-in-colbertv2"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERTv2의 잡음 제거 감독</h3><p>ColBERTv2의 잡음 제거 감독은 도전적인 부정 예제를 선택하고 교차 인코더를 활용하여 지식 증류를 수행함으로써 원래의 훈련 과정을 개선합니다. 훈련 데이터 품질을 향상시키는 이 정교한 방법은 다음과 같은 여러 단계를 포함합니다:</p><ol><li><strong>초기 훈련</strong>: 질의, 관련 문서, 관련 없는 문서로 구성된 MS MARCO 데이터셋의 공식 삼중항을 활용합니다.</li><li><strong>인덱싱 및 검색</strong>: ColBERTv2의 압축을 사용하여 훈련 패시지를 인덱싱하고, 각 질의에 대해 상위 k개 패시지를 검색합니다.</li><li><strong>교차 인코더 재순위화</strong>: MiniLM 교차 인코더를 통한 재순위화로 패시지 선택을 향상시키고, 그 점수를 ColBERTv2로 증류합니다.</li><li><strong>훈련 튜플 형성</strong>: 도전적인 예제를 만들기 위해 높은 순위와 낮은 순위의 패시지를 모두 포함하는 w-way 튜플을 생성합니다.</li><li><strong>반복적 개선</strong>: 어려운 부정 예제의 선택을 지속적으로 개선하여 모델 성능을 향상시키는 과정을 반복합니다.</li></ol><p>이 과정은 ColBERT 아키텍처의 근본적인 변화가 아닌 훈련 체계의 정교한 개선을 나타냅니다.</p><h3 id="hyperparameters-of-colbert" style="position: relative;"><a href="#hyperparameters-of-colbert" title="ColBERT의 하이퍼파라미터" id="anchor-hyperparameters-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 하이퍼파라미터</h3><p>ColBERT의 하이퍼파라미터는 다음과 같이 요약됩니다:</p>

<table>
<thead>
<tr>
<th>하이퍼파라미터</th>
<th>최적 선택</th>
<th>이유</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>3 x 10^{-6}</td>
<td>안정적이고 효과적인 모델 업데이트를 위해 미세조정용으로 선택.</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>연산 효율성과 업데이트당 충분한 정보 캡처 능력의 균형을 맞춤.</td>
</tr>
<tr>
<td>Number of Embeddings per Query (Nq)</td>
<td>32</td>
<td>쿼리 간 일관된 표현 크기를 보장하여 효율적인 처리를 지원하도록 고정됨.</td>
</tr>
<tr>
<td>Embedding Dimension (m)</td>
<td>128</td>
<td>표현력과 연산 효율성 사이의 좋은 균형을 제공하는 것으로 입증됨.</td>
</tr>
<tr>
<td>Training Iterations</td>
<td>200k (MS MARCO), 125k (TREC CAR)</td>
<td>과적합을 피하면서 충분한 학습을 보장하기 위해 선택되었으며, 데이터셋 특성에 따라 조정됨.</td>
</tr>
<tr>
<td>Bytes per Dimension in Embeddings</td>
<td>4 (re-ranking), 2 (end-to-end ranking)</td>
<td>적용 맥락(re-ranking vs. end-to-end)을 고려한 정밀도와 공간 효율성 간의 절충안.</td>
</tr>
<tr>
<td>Vector-Similarity Function</td>
<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>
<td>각각의 검색 맥락에서의 성능과 효율성을 기반으로 선택됨.</td>
</tr>
<tr>
<td>FAISS Index Partitions (P)</td>
<td>2000</td>
<td>검색 공간 분할의 세분성을 결정하여 검색 효율성에 영향을 미침.</td>
</tr>
<tr>
<td>Nearest Partitions Searched (p)</td>
<td>10</td>
<td>검색의 범위와 연산 효율성 간의 균형을 맞춤.</td>
</tr>
<tr>
<td>Sub-vectors per Embedding (s)</td>
<td>16</td>
<td>양자화의 세분성에 영향을 미치며, 검색 속도와 메모리 사용량에 영향을 줌.</td>
</tr>
<tr>
<td>Index Representation per Dimension</td>
<td>16-bit values</td>
<td>end-to-end 검색의 두 번째 단계에서 정확도와 공간의 절충을 위해 선택됨.</td>
</tr>
<tr>
<td>Number of Layers in Encoders</td>
<td>12-layer BERT</td>
<td>맥락 이해의 깊이와 연산 효율성 사이의 최적 균형.</td>
</tr>
<tr>
<td>Max Query Length</td>
<td>128</td>
<td>쿼리 인코더가 처리하는 최대 토큰 수. <b>Jina-ColBERT 모델에서는 확장됨.</b></td>
</tr>
<tr>
<td>Max Document Length</td>
<td>512</td>
<td>문서 인코더가 처리하는 최대 토큰 수. <b>Jina-ColBERT 모델에서는 8192로 확장됨.</b></td>
</tr>
</tbody>
</table>

<h2 id="the-indexing-strategy-of-colbert" style="position: relative;"><a href="#the-indexing-strategy-of-colbert" title="ColBERT의 인덱싱 전략" id="anchor-the-indexing-strategy-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 인덱싱 전략</h2><p>각 문서를 하나의 임베딩 벡터로 인코딩하는 표현 기반 접근 방식과 달리, <strong>ColBERT는 문서(및 쿼리)를 임베딩 집합으로 인코딩하며, 문서의 각 토큰마다 고유한 임베딩을 가집니다.</strong> 이 접근 방식은 본질적으로 더 긴 문서의 경우 더 많은 임베딩이 저장됨을 의미하며, <strong>이는 원래 ColBERT의 문제점이었고, 후에 ColBERTv2에서 해결되었습니다.</strong></p><p>이를 효율적으로 관리하는 핵심은 ColBERT가 인덱싱과 검색을 위해 벡터 데이터베이스(예: <a href="https://github.com/facebookresearch/faiss">FAISS</a>)를 사용하는 것과 대량의 데이터를 효율적으로 처리하도록 설계된 상세한 인덱싱 프로세스에 있습니다. 원래 ColBERT 논문은 인덱싱과 검색의 효율성을 높이기 위한 여러 전략을 언급합니다:</p><ul><li><strong>오프라인 인덱싱</strong>: 문서 표현은 오프라인에서 계산되어 문서 임베딩의 사전 계산과 저장이 가능합니다. 이 프로세스는 배치 처리와 GPU 가속을 활용하여 대규모 문서 컬렉션을 효율적으로 처리합니다.</li><li><strong>임베딩 저장</strong>: 문서 임베딩은 각 차원에 대해 32비트 또는 16비트 값을 사용하여 저장될 수 있으며, 정밀도와 저장 요구사항 사이의 절충안을 제공합니다. 이러한 유연성은 ColBERT가 효과성(검색 성능 측면)과 효율성(저장 및 계산 비용 측면) 사이의 균형을 유지할 수 있게 합니다.</li></ul><p>원래 ColBERT에는 없었던 ColBERTv2의 <strong>residual compression</strong> 도입은 품질을 유지하면서 모델의 공간 점유율을 6-10배 줄이는 데 핵심적인 역할을 합니다. 이 기술은 고정된 참조 중심점들로부터의 차이만을 효과적으로 캡처하고 저장함으로써 임베딩을 더욱 압축합니다.</p><h2 id="effectiveness-and-efficiency-of-colbert" style="position: relative;"><a href="#effectiveness-and-efficiency-of-colbert" title="ColBERT의 효과성과 효율성" id="anchor-effectiveness-and-efficiency-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ColBERT의 효과성과 효율성</h2><p>처음에는 검색에 BERT의 심층 맥락 이해를 통합하는 것이 본질적으로 상당한 컴퓨팅 리소스를 필요로 하여, 높은 지연 시간과 계산 비용으로 인해 실시간 애플리케이션에 적합하지 않을 것이라고 가정할 수 있습니다. 하지만 ColBERT는 후기 상호작용 메커니즘의 혁신적인 사용을 통해 이러한 가정에 도전하고 이를 뒤집습니다. 주목할 만한 점은 다음과 같습니다:</p><ol><li><strong>큰 폭의 효율성 향상</strong>: ColBERT는 전통적인 BERT 기반 랭킹 모델에 비해 계산 비용(FLOPs)과 지연 시간을 대폭 감소시킵니다. 특히, 주어진 모델 크기(예: 12-layer "base" transformer encoder)에서 ColBERT는 BERT 기반 모델의 성능을 매칭하거나 일부 경우에는 훨씬 적은 계산 요구사항으로 초과합니다. 예를 들어, 재랭킹 깊이 <em>k</em>=10에서 BERT는 ColBERT보다 거의 180배 더 많은 FLOPs를 필요로 합니다; 이 차이는 <em>k</em>가 증가함에 따라 더 벌어져서 <em>k</em>=1000에서는 13900배, <em>k</em>=2000에서는 23000배에 이릅니다.</li><li><strong>End-to-End 검색에서의 향상된 Recall과 MRR@10</strong>: 쿼리와 문서 표현 간의 더 깊은 상호작용(초기 상호작용 모델에서 볼 수 있는)이 높은 검색 성능에 필요할 것이라는 초기 직관과는 달리, ColBERT의 end-to-end 검색 설정은 우수한 효과성을 보여줍니다. 예를 들어, Recall@50은 공식 BM25의 Recall@1000과 거의 모든 다른 모델의 Recall@200을 초과하여, 각 쿼리-문서 쌍의 직접 비교 없이도 방대한 컬렉션에서 관련 문서를 검색하는 모델의 주목할 만한 능력을 강조합니다.</li><li><strong>실제 애플리케이션에 대한 실용성</strong>: 실험 결과는 ColBERT의 실제 시나리오 적용 가능성을 강조합니다. 인덱싱 처리량과 메모리 효율성으로 인해 MS MARCO와 같은 대규모 문서 컬렉션을 몇 시간 내에 인덱싱할 수 있으며, 관리 가능한 공간 점유율로 높은 효과성을 유지합니다. 이러한 특성은 성능과 계산 효율성이 모두 중요한 프로덕션 환경에서 ColBERT의 적합성을 강조합니다.</li><li><strong>문서 컬렉션 크기에 대한 확장성</strong>: 아마도 가장 놀라운 결론은 ColBERT의 대규모 문서 컬렉션 처리에 대한 확장성과 효율성일 것입니다. 이 아키텍처는 문서 임베딩의 사전 계산을 허용하고 쿼리-문서 상호작용을 위한 효율적인 배치 처리를 활용하여 시스템이 문서 컬렉션의 크기에 따라 효과적으로 확장될 수 있게 합니다. 이러한 확장성은 효과적인 문서 검색에 필요한 복잡성과 이해의 깊이를 고려할 때 직관적이지 않으며, 계산 효율성과 검색 효과성의 균형을 맞추는 ColBERT의 혁신적인 접근 방식을 보여줍니다.</li></ol><h2 id="using-jina-colbert-v1-en-a-8192-length-colbertv2-model" style="position: relative;"><a href="#using-jina-colbert-v1-en-a-8192-length-colbertv2-model" title="jina-colbert-v1-en 사용하기: 8192 길이의 ColBERTv2 모델" id="anchor-using-jina-colbert-v1-en-a-8192-length-colbertv2-model"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a><a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> 사용하기: 8192 길이의 ColBERTv2 모델</h2><p>Jina-ColBERT는 빠르고 정확한 검색을 위해 설계되었으며, 아키텍처 개선으로 더 긴 시퀀스 처리가 가능한 JinaBERT의 발전을 활용하여 <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/">최대 8192의 더 긴 컨텍스트 길이를 지원합니다</a>.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">엄밀히 말하면 Jina-ColBERT는 8190 토큰 길이를 지원합니다. ColBERT 문서 인코더에서 각 문서는 시작 부분에 <code spellcheck="false" style="white-space: pre-wrap;">[D],[CLS]</code>로 패딩됨을 기억하세요.</div></div><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-colbert-v1-en"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-colbert-v1-en · Hugging Face</div><div class="kg-bookmark-description">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png" alt="" style="cursor: help;"></div></a></figure><h3 id="jinas-improvement-over-original-colbert" style="position: relative;"><a href="#jinas-improvement-over-original-colbert" title="원래 ColBERT에 대한 Jina의 개선사항" id="anchor-jinas-improvement-over-original-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>원래 ColBERT에 대한 Jina의 개선사항</h3><p>Jina-ColBERT의 주요 발전은 그 백본인 <code>jina-bert-v2-base-en</code>으로, <code>bert-base-uncased</code>를 사용하는 원래 ColBERT와 비교하여 훨씬 더 긴 컨텍스트(최대 8192 토큰)를 처리할 수 있습니다. 이 기능은 광범위한 콘텐츠를 가진 문서를 처리하는 데 중요하며, 더 자세하고 맥락적인 검색 결과를 제공합니다.</p><h3 id="jina-colbert-v1-en-performance-comparison-vs-colbertv2" style="position: relative;"><a href="#jina-colbert-v1-en-performance-comparison-vs-colbertv2" title="jina-colbert-v1-en의 ColBERTv2 대비 성능 비교" id="anchor-jina-colbert-v1-en-performance-comparison-vs-colbertv2"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a><a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>의 ColBERTv2 대비 성능 비교</h3><p>우리는 BEIR 데이터셋과 긴 컨텍스트를 선호하는 새로운 LoCo 벤치마크에서 <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>을 평가하여 원래 ColBERTv2 구현과 비상호작용 기반<a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> 모델.</p>

<table>
<thead>
<tr>
<th>Dataset</th>
<th>ColBERTv2</th>
<th><a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a></th>
<th><a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Arguana</td>
<td>46.5</td>
<td><strong>49.4</strong></td>
<td>44.0</td>
</tr>
<tr>
<td>Climate-Fever</td>
<td>18.1</td>
<td>19.6</td>
<td><strong>23.5</strong></td>
</tr>
<tr>
<td>DBPedia</td>
<td><strong>45.2</strong></td>
<td>41.3</td>
<td>35.1</td>
</tr>
<tr>
<td>FEVER</td>
<td>78.8</td>
<td><strong>79.5</strong></td>
<td>72.3</td>
</tr>
<tr>
<td>FiQA</td>
<td>35.4</td>
<td>36.8</td>
<td><strong>41.6</strong></td>
</tr>
<tr>
<td>HotpotQA</td>
<td><strong>67.5</strong></td>
<td>65.9</td>
<td>61.4</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>33.7</td>
<td><strong>33.8</strong></td>
<td>32.5</td>
</tr>
<tr>
<td>NQ</td>
<td>56.1</td>
<td>54.9</td>
<td><strong>60.4</strong></td>
</tr>
<tr>
<td>Quora</td>
<td>85.5</td>
<td>82.3</td>
<td><strong>88.2</strong></td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>15.4</td>
<td>16.9</td>
<td><strong>19.9</strong></td>
</tr>
<tr>
<td>SciFact</td>
<td>68.9</td>
<td><strong>70.1</strong></td>
<td>66.7</td>
</tr>
<tr>
<td>TREC-COVID</td>
<td>72.6</td>
<td><strong>75.0</strong></td>
<td>65.9</td>
</tr>
<tr>
<td>Webis-touch2020</td>
<td>26.0</td>
<td><strong>27.0</strong></td>
<td>26.2</td>
</tr>
<tr>
<td>LoCo</td>
<td>74.3</td>
<td>83.7</td>
<td><strong>85.4</strong></td>
</tr>
<tr>
<td>Average</td>
<td>51.7</td>
<td><strong>52.6</strong></td>
<td>51.6</td>
</tr>
</tbody>
</table>

<p>이 표는 <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>의 우수한 성능을 보여주며, 특히 기존 ColBERTv2와 비교하여 긴 컨텍스트 길이가 필요한 시나리오에서 더욱 두드러집니다. <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>은 <a href="https://arxiv.org/abs/2310.19923">더 많은 학습 데이터를 사용</a>하는 반면, <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>은 MSMARCO만 사용하는데, 이는 일부 작업에서 <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>의 좋은 성능을 설명할 수 있습니다.</p><h3 id="example-usage-of-jina-colbert-v1-en" style="position: relative;"><a href="#example-usage-of-jina-colbert-v1-en" title="jina-colbert-v1-en 사용 예시" id="anchor-example-usage-of-jina-colbert-v1-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a><a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> 사용 예시</h3><p>이 코드 스니펫은 Jina-ColBERT의 인덱싱 프로세스를 보여주며, 긴 문서에 대한 지원을 보여줍니다.</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> colbert <span class="hljs-keyword">import</span> Indexer
<span class="hljs-keyword">from</span> colbert.infra <span class="hljs-keyword">import</span> Run, RunConfig, ColBERTConfig

n_gpu: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>  <span class="hljs-comment"># Set your number of available GPUs</span>
experiment: <span class="hljs-built_in">str</span> = <span class="hljs-string">""</span>  <span class="hljs-comment"># Name of the folder where the logs and created indices will be stored</span>
index_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">""</span>  <span class="hljs-comment"># The name of your index, i.e. the name of your vector database</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">with</span> Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):
        config = ColBERTConfig(
          doc_maxlen=<span class="hljs-number">8192</span>  <span class="hljs-comment"># Our model supports 8k context length for indexing long documents</span>
        )
        indexer = Indexer(
          checkpoint=<span class="hljs-string">"jinaai/jina-colbert-v1-en"</span>,
          config=config,
        )
        documents = [
          <span class="hljs-string">"ColBERT is an efficient and effective passage retrieval model."</span>,
          <span class="hljs-string">"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length."</span>,
          <span class="hljs-string">"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length."</span>,
          <span class="hljs-string">"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2."</span>,
          <span class="hljs-string">"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2."</span>,
          <span class="hljs-string">"Jina is an easier way to build neural search systems."</span>,
          <span class="hljs-string">"You can use Jina-ColBERT to build neural search systems with ease."</span>,
          <span class="hljs-comment"># Add more documents here to ensure the clustering work correctly</span>
        ]
        indexer.index(name=index_name, collection=documents)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><h3 id="use-jina-colbert-v1-en-in-ragatouille" style="position: relative;"><a href="#use-jina-colbert-v1-en-in-ragatouille" title="RAGatouille에서 jina-colbert-v1-en 사용하기" id="anchor-use-jina-colbert-v1-en-in-ragatouille"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>RAGatouille에서 <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> 사용하기</h3><p>RAGatouille는 RAG 파이프라인 내에서 고급 검색 방법의 사용을 용이하게 하는 새로운 Python 라이브러리입니다. 모듈성과 쉬운 통합을 위해 설계되었으며, 사용자가 최신 연구 결과를 원활하게 활용할 수 있게 해줍니다. RAGatouille의 주요 목표는 ColBERT와 같은 복잡한 모델을 RAG 파이프라인에 적용하는 것을 단순화하여, 개발자들이 기본 연구에 대한 깊은 전문 지식 없이도 이러한 방법을 활용할 수 있게 하는 것입니다. <a href="https://twitter.com/bclavie">Benjamin Clavié</a> 덕분에 이제 <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>을 쉽게 사용할 수 있습니다:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> ragatouille <span class="hljs-keyword">import</span> RAGPretrainedModel

<span class="hljs-comment"># Get your model &amp; collection of big documents ready</span>
RAG = RAGPretrainedModel.from_pretrained(<span class="hljs-string">"jinaai/jina-colbert-v1-en"</span>)
my_documents = [
    <span class="hljs-string">"very long document1"</span>,
    <span class="hljs-string">"very long document2"</span>,
    <span class="hljs-comment"># ... more documents</span>
]

<span class="hljs-comment"># And create an index with them at full length!</span>
RAG.index(collection=my_documents,
          index_name=<span class="hljs-string">"the_biggest_index"</span>,
          max_document_length=<span class="hljs-number">8190</span>,)

<span class="hljs-comment"># or encode them in-memory with no truncation, up to your model's max length</span>
RAG.encode(my_documents)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Jina-ColBERT에 대한 더 자세한 정보와 추가 탐색을 위해 <a href="https://huggingface.co/jinaai/jina-colbert-v1-en">Hugging Face 페이지</a>를 방문할 수 있습니다.</p><h2 id="conclusion" style="position: relative;"><a href="#conclusion" title="결론" id="anchor-conclusion"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>결론</h2><p>ColBERT는 정보 검색 분야에서 중요한 진전을 나타냅니다. Jina-ColBERT로 더 긴 컨텍스트 길이를 가능하게 하고 후기 상호작용에 대한 ColBERT 접근 방식과의 호환성을 유지함으로써, 최첨단 검색 기능을 구현하고자 하는 개발자들에게 강력한 대안을 제공합니다.</p><p>복잡한 검색 모델을 RAG 파이프라인에 통합하는 것을 단순화하는 RAGatouille 라이브러리와 결합하여, 개발자들은 이제 고급 검색 기능을 쉽게 활용할 수 있으며, 워크플로우를 간소화하고 애플리케이션을 향상시킬 수 있습니다. Jina-ColBERT와 RAGatouille의 시너지는 고급 AI 검색 모델을 실용적인 사용을 위해 접근 가능하고 효율적으로 만드는 데 있어 주목할 만한 진전을 보여줍니다.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">범주:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><i class="q-icon notranslate material-symbols material-symbols-sharp q-chip__icon q-chip__icon--left" aria-hidden="true" role="presentation">star</i><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">선택</div></div></div><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">기술 기사</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fko%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-478b3f77="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-478b3f77="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-478b3f77="" class="col-sm-12 col-md"><div data-v-478b3f77="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">사무실</div><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-478b3f77="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center"><div data-v-478b3f77="" class="q-item__label">캘리포니아주 서니베일</div><div data-v-478b3f77="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, 서니베일, CA 94085, 미국</div></div></div><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-478b3f77="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center"><div data-v-478b3f77="" class="q-item__label">독일 베를린(본사)</div><div data-v-478b3f77="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 베를린, 독일</div></div></div><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-478b3f77="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center"><div data-v-478b3f77="" class="q-item__label">중국 베이징</div><div data-v-478b3f77="" class="q-item__label q-item__label--caption text-caption text-dim">중국 베이징 하이뎬구 서가 48호 6호관 5층</div></div></div><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-478b3f77="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center"><div data-v-478b3f77="" class="q-item__label">중국 선전</div><div data-v-478b3f77="" class="q-item__label q-item__label--caption text-caption text-dim">중국 선전 푸안 테크놀로지 빌딩 4층 402호</div></div></div></div></div><div data-v-478b3f77="" class="col-sm-12 col-md row"><div data-v-478b3f77="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">검색 기반</div><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">심층 검색</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">리더</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">벡터 모델</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">재배열자</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">분류자</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">얇게 써는 기계</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">API 문서</div></a><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">Jina API 키 받기</div></div><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">비율 제한</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-478b3f77="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">API 상태</div></a></div><div data-v-478b3f77="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">회사</div><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">회사 소개</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">영업팀에 문의</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">소식</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">인턴 프로그램</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">우리와 함께</div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">로고 다운로드</div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-478b3f77="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-478b3f77="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">자귀</div><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">안전</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">이용약관</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">은둔</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--main justify-center">쿠키 관리</div></a><a data-v-478b3f77="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-478b3f77="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-478b3f77="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-478b3f77="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-478b3f77="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-478b3f77="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-478b3f77="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div><script src="https://platform.twitter.com/widgets.js"></script><iframe scrolling="no" frameborder="0" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.2f70fb173b9000da126c79afe2098f02.html?origin=http%3A%2F%2F127.0.0.1%3A3000" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe"></iframe></body></html>
const e={interests:"I'm interested in this configured commercial licensing (${_Price})",short_title:"Config License",title:"Enterprise License Configurator",subtitle:"Configure your enterprise license for Jina AI models",contractType:{title:"Contract Type",standard:"Standard Enterprise License",poc:"Proof of Concept (3-6 months)",department:"Department License"},poc:{duration:"POC Duration (months)",description:"POC includes success metrics tracking and upgrade path to full license"},department:{executionModel:"Execution Model",businessSponsor:"Business Unit Sponsor",sponsorDescription:"Dedicated executive sponsor who champions the implementation, provides strategic direction, and ensures resources are allocated",growthTitle:"Growth Trajectory",growth:{limited:"Limited to Department",limitedDesc:"Focus on single department needs with standard support",steady:"Potential for Other Departments",steadyDesc:"Planned expansion to 2-3 departments within 12 months",high:"Clear Enterprise-Wide Potential",highDesc:"Strategic initiative with planned company-wide adoption"}},usage:{title:"Usage Configuration",business:"B2B (Business Accounts)",consumer:"B2C (End Users)",businessCount:"Number of Business Accounts",consumerCount:"Monthly Active Users",businessDescription:"Number of distinct business accounts using our models",consumerDescription:"Number of monthly active end users across all applications"},models:{title:"Select Models",description:"Choose the models to include in your commercial package",embeddings:"jina-embeddings-v3",clip:"jina-clip-v2",colbert:"jina-colbert-v2",reranker:"jina-reranker-v2",lm:"reader-lm"},support:{title:"Support Tier",standard:"Lite",premium:"Standard",enterprise:"Premium",hoursQuarter:"{hours} hours/quarter"},features:{title:"Additional Features",priority:"Priority Response SLA (4 hours)",csm:"Dedicated Customer Success Manager",training:"Custom Model Training Support"},payment:{title:"Payment Terms",features:"Features Included",quarterly:"Quarterly Billing",annual:"Annual Billing (10% discount)"},pricing:{cta:"Talk to Our Sales",disclaimer:"This pricing calculator provides an estimate. Your final price may vary based on specific requirements, volume commitments, and custom configurations. Contact our sales team for a detailed quote.",title:"Estimated Price",pocTotal:"{months}-month POC price: ${price}",oneTime:"${price} one-time",frequency:"${price} / {frequency}",annual:"year",quarterly:"quarter"},messaging:{pocIncludes:"POC package includes:",pocMetrics:"Success metrics tracking dashboard",pocCheckins:"Bi-weekly check-ins with technical team",pocMigration:"Migration support to full license",pocTemplate:"POC results documentation template",deptIncludes:"Department license includes:",deptReviews:"Quarterly business review meetings",deptRoadmap:"Enterprise expansion roadmap planning",deptSponsor:"Executive sponsor alignment sessions",deptWorkshops:"Cross-department collaboration workshops",enterpriseAlert:"Your usage level suggests an enterprise-wide opportunity. Let's schedule a call to discuss a custom enterprise agreement.",selectedModels:"Selected Models:",noModelsSelected:"No additional models selected. Using base embeddings model.",usageTierBusiness:"Business Usage Tier: Up to {count} business accounts",usageTierConsumer:"Consumer Usage Tier: Up to {count} monthly active users",standardFeatures:"Standard License Features:",baseModelIncluded:"Base embeddings model (jina-embeddings-v3) included",supportTierIncluded:"{tier} support tier included {hours}",additionalFeatures:"Additional Features Included:"},descriptions:{contractType:{standard:"Full enterprise deployment with unrestricted model usage",poc:"Trial deployment to test models in your specific environment and use cases",department:"Single department deployment with flexibility to expand later"},poc:{duration:"Timeline for testing and validating model performance in your environment",metrics:"Track key performance indicators and model effectiveness"},department:{sponsorship:"Indicates if a revenue-generating department is backing the deployment",growth:"Plans for expanding model usage across other departments"},usage:{business:"How many distinct businesses will use applications powered by our models",consumer:"How many end users will interact with our models monthly"},models:{embeddings:"Text embedding model for semantic search and text similarity",clip:"Process both images and text for multimodal applications",colbert:"Specialized model for high-precision document retrieval",reranker:"Fine-tunes search results for better relevancy",reader:"Converts HTML content to clean markdown format"},support:{standard:"Basic technical support and implementation guidance",premium:"Additional consulting hours and faster response times",enterprise:"Full support coverage with highest priority"},features:{priority:"Guaranteed quick response for critical issues",csm:"Personal customer success manager for strategic guidance and support",training:"Help with pre-training or fine-tuning models for your specific data"},payment:{quarterly:"Regular payments every three months",annual:"Single yearly payment for simplified accounting"}},labels:{selectSupport:"Select Support Tier",included:"Included in base price",custom:"Contact sales for custom pricing",selectAll:"Select all models",learnMore:"Learn more",priceQuarterly:"Price per quarter",discountApplied:"Discount applied",basePrice:"Base price",totalPrice:"Total price",upTo:"Up to {count}"},tooltips:{businessSponsor:"Having a business unit sponsor can qualify for additional discounts",pocDuration:"Select the duration of your proof of concept period",supportTier:"Choose the level of support that best fits your needs",annualDiscount:"Save 10% by paying annually",usageLimit:"Contact us for custom pricing if you exceed these limits"}},t={caption:'Discover "Re·Search", our beautifully designed yearbook showcasing our best research articles and search foundation models in 2024.',order_now:"Order now"},n={share_key_expired_title:"Shared link expired",share_key_expired_message:"The shared link of the key has expired. Please ask the key owner to share it again.",share_key_expired_at:"The shared link will expired at {_time}!",top_up:"Top up",shared_from:"Key shared by {_user}",share_key_message:"{_user} has shared an API key with you. Add it to manage the key and its balance.",add_shared_key:"Add to my keys",shared_key:"Shared key",ignore:"Ignore",share_link_copied:"Share link copied",share:"Share key",share_key_confirm_title:"Share API key",share_key_confirm_message:`The recipient will be able to view, manage and top up this key's balance. You'll retain the same abilities. 
Please note that the link will expire in 24 hours.`,copy_share_link:"Copy link",confirm:"Confirm",login_required:"Please log in before adding the shared key.",no_key_title:"Need an API Key?",no_key_with_login:"You haven't created an API key yet. Generate one now and get free tokens to start.",no_key_without_login:"Already have an account? Sign in to access your API keys, or click '{_button}' to create a new one.",get_free_key:"Create API Key",advance_settings:"Open advanced settings",settings:"Settings",generate_new_key_tooltip:"Generate a new API key with empty balance. You can top up the balance later.",is_primary:"Your primary API key. You can change it after log in.",available_resources:"Available Tokens",title:"Jina Search Foundation API",description:"Manage API keys for all Jina AI services—Embeddings, Reader, Reranker, and more.",balance_primary_key:"Primary Key Balance",primary_key_set:"Successfully set {_apiKey} as your primary key.",primary_key_set_caption:"This key will be used across all demos, examples, and playgrounds on jina.ai.",login_explain:"Manage multiple API keys and track usage—all in one account.",login_explain_long:"Log in to securely store and manage your API keys. Track usage history, manage multiple keys, and never lose access to your credentials.",ok:"OK",cancel:"Cancel",save:"Save",last_used:"Last Used",login:"Log in",logout:"Log out",balance:"Available Tokens",add:"Add key",purchase:"Purchase Tokens",last_used_at:"Last Activity",subscribed_key:"Premium Key",free_key:"Free Key",usage_history:"Usage History",usage_summary:"Past 7 Days: {_usage} tokens",to_dashboard:"Manage Keys",remove:"Remove Key",remove_title:"Remove Key",revoke_title:"Revoke Key",revoke_explain:"Revoking a key will immediately disable it for all users who have stored it, and all remaining balance and associated properties will be permanently unusable. This action cannot be undone.",revoke_message:"Are you sure you want to revoke this key? Once revoked, this key will become permanently invalid for all users who have stored it. All remaining balance and associated properties will be permanently unusable. This action cannot be undone.",revoke_label:"Please confirm the revocation of this key by typing it below",revoke_error:"The key you entered does not match the key you are trying to revoke.",remove_explain:"Removing the key from your list will not affect the service, dependent operations, or other users who have stored it. The key remains functional and can be added back at any time.",remove_message:"Are you sure you want to remove this key? The key remains functional and can be added back at any time.",copy:"Copy Key",revoke:"Revoke Key",do_it_later:"Do it Later",invalid_key:"Invalid Key",existing_key:"Existing Key",logout_message:"Your API keys remain safely stored in your account. Sign in anytime to manage them.",logout_success:"Successfully signed out",auto_recharge_title:"Enable Auto-recharge?",recharge_threshold_confirm_title:"Change Auto-recharge Threshold",recharge_threshold_confirm_message:"Are you sure you want to change the auto-recharge threshold to {_threshold} tokens?",filter_by:"Filter by Key",remove_primary_key:"Please set another key as primary before removing the current primary key.",primary_key:"Set as Primary Key",total_keys:"Total Keys",add_key_explain:"Add another API key to your account. Added keys can be managed, topped up, or removed at any time.",generate_new_key:"Generate New Key",login_via:"logged in via {_provider}",email:"Email",invalid_email:"Invalid email",auto_reminder_threshold:"Remind if",auto_reminder:"Low Balance Reminder",auto_reminder_description:"Receive automated email alerts when your token balance drops below your set thresholds. You can configure up to three thresholds.",auto_reminder_email:"Email address for reminders",auto_reminder_cancel_title:"Cancel Auto-remind",auto_reminder_cancel_message:"Are you sure you want to cancel the auto-remind for this key?",auto_reminder_toggle:"Toggle Auto-remind, please note only premium key can enable this feature.",auto_reminder_threshold_error:"The threshold must be between 1 and 1T.",auto_reminder_info:"The notification will be sent to {_email} when the token balance falls below {_threshold} tokens.",transfer_title:"Transfer Tokens",transfer_explain:"Seamlessly transfer your remaining paid tokens to another account for greater flexibility and enhanced security in managing your resources.",transfer_label:"Transfer to",transfer_message:"Are you sure you want to transfer your remaining paid tokens {_tokens} from {_source} to {_target}?",transfer_before_revoke:"Transfer the remaining paid tokens before revoking the key.",advanced_feature:"Advanced feature for premium key only.",transfer_success:"Successfully transferred tokens from {_source} to {_target}.",revoke_success:"Successfully revoked the key {_key}.",remove_success:"Successfully removed the key {_key}.",add_success:"Successfully added a key {_key}.",generate_success:"Successfully generated a new key {_key}.",auto_recharge_enable_success:"Successfully enabled auto-recharge for the key {_key}.",no_transferable_keys:"There are no other keys available for transfer, please add a new key first."},i={how_does_it_work:"How it works?",download_success:"Avatar downloaded successfully",title:"Avatar Generator",description:"Generate unique avatars with customizable features",generate:"Generate Avatar",usage:"Avatar Generation",customize:"Customize Features",error_loading:"Failed to load avatar assets. Please try again.",upload_title:"Upload Image",upload_description:"Select an image to convert to base64 (256x256)",select_file:"Select a portrait image file",file_hint:"Supported formats: JPG, PNG, GIF, WebP",error_processing:"Error processing image",auth_required:"Authentication required to use avatar generation",noImageSelected:"Please select an image first",classificationError:"Error classifying image. Please try again.",clickToDownload:"Click to download SVG",downloadError:"Error downloading avatar",downloadSuccess:"Avatar downloaded successfully"},a={usage:"Grounding Usage",title:"Fact Checking",description:"Ground statements with web knowledge"},o={classifier:"Train a classifier using labeled examples",classifier_few_shot:"Classify inputs using a trained few-shot classifier",classifier_zero_shot:"Classify inputs using zero-shot classification",classifier_token_counting:"Tokens counted as: input_tokens × num_iters",classifier_zero_shot_token_counting:"Tokens counted as: input_tokens + label_tokens",classifier_few_shot_token_counting:"Tokens counted as: input_tokens",classifier_latency:"Response time varies with input size",explain:"Rate limits are tracked in two ways: <b>RPM</b> (requests per minute) and <b>TPM</b> (tokens per minute). Limits are enforced per IP and can be reached based on whichever threshold—RPM or TPM—is hit first.",batch_explain:"This API supports batch operations, allowing up to 512 documents per request, with each document containing up to 8192 tokens. Smartly leveraging batch operations can significantly reduce the number of requests and improve performance.",rjinaai:"Convert URL to LLM-friendly text",sjinaai:"Search the web and convert results to LLM-friendly text",gjinaai:"Grounding a statement with web knowledge",total_token_counting:"Count the total number of tokens in the whole process.",tokenizer:"Tokenize and segment long text",reranker:"Rank documents by query",embeddings:"Convert text/images to fixed-length vectors",premium_rate:"With potential for higher rate limits",product:"Product",understanding:"Understand the rate limit",understanding_description:"Rate limits are the maximum number of requests that can be made to an API within a minute per IP address (RPM). Find out more about the rate limits for each product and tier below.",title:"Rate Limit",description:"Description",requestType:"Allowed Request",endpoint:"API Endpoint",woAPIkey:"w/o API Key",wAPIkey:"w/ API Key",wPremium:"w/ Premium API Key",latency:"Average Latency",tokenCounting:"Token Usage Counting",tbd:"To be determined",output_token_counting:"Count the number of tokens in the output response.",input_token_counting:"Count the number of tokens in the input request.",no_token_counting:"Token is not counted as usage.",depends:"depends on the input size"},r={chars:"characters",chunk_all:"All Chunks",chunk:"Chunk",chunking_short:"Chunking",english:"English",chinese:"Chinese",japanese:"Japanese",korean:"Korean",chunking:"Chunking long documents, lightning fast!",chunking_explain:"You can also use Segmenter API to cut long documents into smaller chunks, making it easier to process them in embeddings or rerankers. We leverage common structural cues and build a set of rules and heuristics which perform well across diverse types of content, e.g. Markdown, HTML, LaTeX and CJK languages.",chunks_in_total:"{_numChunks} chunks in total",show_space:"Show leading/trailing spaces",what_is:"What is a Segmenter?",is_free:"Segmenter API is free!",is_free_description:"By providing your API key, you can access a higher rate limit, and your key won't be charged.",explain:"A segmenter is a crucial component that converts text into tokens or chunks, which are the basic units of data that an embedding/reranker model or LLM processes. Tokens can represent whole words, parts of words, or even individual characters.",description_long:"Our Segmenter API is crucial for helping LLMs manage input within context limits, and optimizing model performance. It allows developers to count tokens and extract relevant text segments, ensuring efficient data processing and cost management.",title:"Segmenter API",usage:"Usage",input_text:"Input text",description:"Cut long text into chunks and do tokenization.",description_long1:"Free API for segmenting long text into chunks and tokenization.",free_api:"Segmenter API is free to use. By providing your API key, you can access a higher rate limit, and you key won't be charged.",basic_usage:"Use GET request to count tokens",visualization:"Visualization",basic_usage_explain:"You can simply send a GET request to count the number of tokens in your text.",count_tokens_hint:"<b>{_numTokens}</b> tokens, {_numChars} characters.",advance_usage:"Use POST request for more features",change_content:"Change 'content' and see live result",token_index:"Token index: {_index}",remove_boundary_cues:"Remove line breaks",remove_boundary_cues_explain:"Remove all line breaks (the main boundary cues) from the input, this makes the problem more challenging and see how the response changes!",table:{td_1_0:"Tokenize texts, count and get first/last-N tokens.",td_1_1:"20 RPM",td_1_2:"200 RPM",td_1_3:"1000 RPM",td_1_4:"No charge",td_1_5:"800ms"},faq_v1:{title:"Segmenter-related common questions",question1:"How much does the Segmenter API cost?",answer1:"The Segmenter API is free to use. By providing your API key, you can access a higher rate limit, and your key won't be charged.",question2:"If I don't provide an API key, what is the rate limit?",answer2:"Without an API key, you can access the Segmenter API at a rate limit of 20 RPM.",question3:"If I provide an API key, what is the rate limit?",answer3:"With an API key, you can access the Segmenter API at a rate limit of 200 RPM. For premium paid users, the rate limit is 1000 RPM.",question4:"Will you charge the tokens from my API key?",answer4:"No, your API key is only used to access a higher rate limit.",question5:"Does the Segmenter API support multiple languages?",answer5:"Yes, the Segmenter API is multilingual and supports over 100 languages.",question6:"What is the difference between GET and POST requests?",answer6:"GET requests are solely used to count the number of tokens in a text, allows you easily integrate it as a counter in your application. POST requests supports more parameters and features, such as returning the first/last N tokens.",question7:"What is the maximum length I can tokenize per request?",answer7:"You can send up to 64k characters per request.",question8:"How does the chunking feature work? Is it semantic chunking?",answer8:"The chunking feature segments long documents into smaller chunks based on common structural cues, ensuring accurate segmentation of text into meaningful chunks. Essentially it is a (big!) regex pattern that segments text based on certain syntactical features that often align with semantic boundaries, such as sentence endings, paragraph breaks, punctuation, and certain conjunctions. It is not semantic chunking. This (big) regex is as powerful as it can be within the limitations of regular expressions. It balances complexity and performance. While true semantic understanding isn't possible with regex, it well-approximates context by common structural cues.",question9:"How do you handle special tokens such as 'endoftext' in the Segmenter API?",answer9:"If the input contains special tokens, our Segmenter API will put them in the field 'special_tokens'. This allows you to easily identify them and handle them accordingly for your downstream tasks, e.g. removing them before feeding the text into an LLM to prevent injection attacks.",question10:"Does chunking support other languages than English?",answer10:"Besides western languages, chunking also works well with Chinese, Japanese, and Korean."},parameters:{learn_more:"Learn more",type:"Segmenter",type_explain:"Choose the tokenizer to use.",used_by_models:"Used in {_usedBy}.",auth_token:"Add API Key for Higher Rate Limit",auth_token_explain:"@:tokenizer.free_api",head:"Return the first N tokens",tail:"Return the last N tokens",head_explain:"Return the first N tokens of the given content. Boundary exclusive. Can not be used with 'tail'.",tail_explain:"Return the last N tokens of the given content. Boundary exclusive. Can not be used with 'head'.",return_tokens:"Return the tokens",return_chunks:"Return the chunks",return_chunks_explain:"Chunking the input into semantically meaningful segments while handling a wide variety of text types and edge cases based on common structural cues.",return_tokens_explain:"Return the tokens and their corresponding ids in the response. Toggle to see the result visualization.",max_chunk_length:"Maximum length of each chunk",max_chunk_length_explain:"Maximum number of characters in each chunk. In practice the chunk length can be smaller than this value, if there is a good boundary in the text."}},s="Table of Content",l={standard_key:"Standard key",standard_key_description:"Access to all Jina Search Foundation API products with a standard rate limit.",premium_key:"Premium key with much higher rate limits",premium_key_description:"Get much higher rate limits and access to premium features, check the rate limit table for details.",premium_key_manager:"Advanced key management",premium_key_manager_description:"Basic plus advanced features like auto-remind, revoke, token transfer.",key_manager:"Basic key management",key_manager_description:"Manage multiple API keys in one account, track usage history, and top-up tokens.",secured_by_stripe:"Secure payment via Stripe",commercial_licence:{title:"Team License",download_title:"Download Commercial License",subtitle:"Every model you need for better search",chip_label:"For Growing Businesses",price_amount:"$1,000",price_period:"/ 3 months",feature_models:"Unlimited commercial use of our CC BY-NC models",feature_models_desc:"Use the Models for commercial purposes, including internal use or incorporation into customer-facing applications.",feature_consulting:"Two hours of consulting with our model experts",feature_consulting_desc:"Two (2) hours of technical consulting services per License Period.",feature_api_title:"Free API Testing Access",feature_api_desc:"Test before purchase",test_before_purchase:"Try Before You Buy",try_api:"Try API first",test_before_purchase_desc:"Get 1M free API tokens or use our Hugging Face model to validate performance",feature_future_support:"Access to future CC BY-NC models without permission",feature_future_support_desc:"Any new models released by Licensor under CC-BY-NC-4.0 during the License Period.",cta_button:"Get Started",company_size_note:"Exclusive for companies under 50 employees or $500K revenue",read_the_terms:"Review License Terms",read_the_terms_desc:"Review commercial license rights and limitations before purchase",read_the_terms_btn:"Terms"},on_prem:"With a commercial license for on-prem use",on_prem_explain:"Require 100% control and privacy? Purchase a commercial license to use our models on-premises.",via_api:"With Jina Search Foundation API",via_api_explain:"The easiest way to access all of our products. Top-up tokens as you go.",higher_limit:"Much higher rate limit",higher_limit_description:"Get up to 1000 RPM for r.jina.ai and 100 RPM for s.jina.ai; more details in the rate limit section.",full_commercial:"Unrestricted commercial use",no_commercial:"Non-commercial use only (CC-BY-NC)",no_commercial_description:"You can use the API for non-commercial purposes only. For commercial use, please top up your API key.",full_commercial_description:"You can use the API for commercial purposes without any restrictions.",priority_support:"Priority technical support",priority_support_description:"Guaranteed email response on technical issues & incidents within 24 hours.",free_hour_consult:"Free 1-hour consultation",free_hour_consult_description:"One hour of free consultation with our product and engineering teams to discuss the best practice for your use case"},d={api_key:"Topped up API key",success:"Thank you for your purchase!",success_caption:"Your order was completed at {_purchasedTime}. Your API key has been topped up and is ready to use!",generation:"Your API key is ready!",generation_caption:"Your API key was generated at {_purchasedTime} and is ready to use!",free_key:"Free API key"},c={cta:"Translate to {_lang} code",select_language:"Language"},u={get_started:"Get started with PromptPerfect",features:[{name:"Assistant",title:"Daily dose of productivity.",description:"Easily switch between content generation and prompt optimization, push your content quality to the next level."},{name:"Prompt optimization",title:"Better inputs, better outputs",description:"Don't know how to write an effective instruction? Just put your idea in, one click, get a better instruction."},{name:"Compare models",title:"Side-by-side model comparison.",description:"Understand the vibe of every AI model by comparing their output of the same prompt."},{name:"Deploy prompts",title:"No Ops, just deploy.",description:"Perhaps the simplest way to deploy your prompts as API for integration."},{name:"Multi-agent",title:"Explore how agents collaborate",description:"Customize your own LLM agents, and start a multi-agent simulation. See how they collaborate or compete in a virtual environment to reach the goal."}]},m={embedding:"Embeddings",reranker:"Reranker",which_to_go:"Which one to integrate with {_vendor}?",meta_prompt:"Use Meta-prompt for code generation",meta_prompt_description:"The Meta-prompt guides LLMs (like ChatGPT and Claude) through all of our Search Foundation APIs, making code generation easier and higher quality.",copy_full_prompt:"Copy full prompt",how_to_use_meta_prompt:"How to use"},h={results:"results",more_results:"{_numMore} more results",placeholder:"Type your question about this page",proposing_solution:"Crafting answer from the page content...",ask_on_current_page:"Ask the current page about...",find_solution:"Generate a solution for...",hotkey:"Use / key for quick questions",hotkey1:"Use",hotkey2:"to toggle",hint:"Search over products, news and your questions",hotkey_long1:"At any time, press",hotkey_long3:"to open search bar",required:"Please describe your question with more details."},p="Your Search Foundation, Supercharged.",g="Best-in-class embeddings, rerankers, LLM-reader, web scraper, classifiers. The best search AI for multilingual and multimodal data.",f='🎉 Our first book, "Neural Search — From Prototype to Production with Jina" is officially out today!',y="Purchase now",b="Copy",_="Copied to clipboard",v="Powered by",w={title:"Open Day",description:"An exclusive opportunity to gain an insider's view of Jina AI.",organization:"Organization",group_size:"Number of visitors",organization_website:"Organization website",organization_website_placeholder:"URL for your organization's homepage or LinkedIn profile",preferred_products:"Which products are you interested in?",preferred_date:"Preferred date",preferred_language:"Preferred language",subtitle:"A Glimpse into the Future of Multimodal AI",introduction:"Jina AI is delighted to open our doors to esteemed entities and organizations interested in the progress and future of Artificial Intelligence. We extend this exclusive opportunity for those in politics, NGOs, NPOs, and investment sectors to gain an insider's view of our operations and visions here at our Berlin headquarters.",vision_title:"Our Vision for the Future",vision:"Join us for a comprehensive overview of the AI landscape as we see it. Our discussion will focus on the potential of Large Language Models, multimodal AI, and the impact of open-source technology in shaping the future of global innovation.",experience:"We've arranged an immersive three-hour tour for our guests, available in German, English, French, Spanish, Chinese, and Russian. The tour covers an in-depth look into our advancements in multimodal AI, our perspective on the AI landscape, followed by a detailed examination of specific projects. We'll conclude with a group discussion to facilitate the exchange of ideas and insights. A lunch option is also available upon request.",experience_title:"An Insider's Journey",impact_title:"Impact and Influence",impact:"Understand how our contributions to the open-source community and our work in multimodal AI technology are establishing Jina AI as an influential player in AI innovation. We aim to play a significant role in decision-making processes, ensuring that the advancement of AI technology benefits all.",engage_title:"Engage with Us",engage:"We highly encourage an interactive dialogue throughout the day. The exchange of thoughts and perspectives is invaluable to us. Potential collaborations stemming from these discussions could significantly contribute to a more integrated and innovative future.",tutor_title:"An Exclusive Deep Dive into",tutor_subtitle:"A meticulously curated three-hour tour, bringing you closer to the heart of Jina AI's groundbreaking work in multimodal AI technology.",one_hour:"1 hour",motivation_to_attend_v2:"Why are you interested in our Open Day?",motivation_placeholder_v2:"Sharing your motivations will help us improve your experience.",motivation_min_length_v1:"Please provide a more detailed motivation."},k={question1:"Who can apply for the Jina AI internship program?",answer1:"Undergraduate, Masters, and Ph.D. students from all over the world, with interest in fields such as research, engineering, marketing, and sales, are encouraged to apply. We also welcome non-technical internships in marketing, sales, executive assistance, and more. We are seeking passionate individuals ready to pioneer multimodal AI with us.",question2:"Where will the internship take place?",answer2:"Internships must be carried out onsite at one of our offices, which are located in Sunnyvale (2025), Berlin, Beijing, and Shenzhen.",question3:"Does Jina AI assist with visa processes?",answer3:"Yes, Jina AI offers reasonable assistance in the visa process for successful applicants.",question4:"Does Jina AI provide any allowances or benefits for interns?",answer4:"Yes, Jina AI provides a reasonable amount of living cost coverage for interns during the internship period.",question5:"Can I work on my Master's thesis during the internship at Jina AI?",answer5:"Yes, it is possible to work on your Master's thesis during your internship at Jina AI, typically applicable to students at German universities. However, you must have prior communication and agreement from your university's supervisor. Note that we do not help students find advisors.",question6:"What does the application process involve?",answer6:"The application process includes submitting your application form, a resume, a cover letter expressing your interest and motivation, and any relevant professional links such as GitHub or LinkedIn. We evaluate candidates based on their performance during the interview and their performance in their university.",question7:"Does Jina AI provide any letter of recommendation post-internship?",answer7:"Yes, successful interns may receive a letter of recommendation at the end of their internship, signed by our CEO.",question8:"What is the duration of the internship?",answer8:"The duration of the internship varies based on the role and project. However, it typically ranges from three to six months.",question9:"Can I apply if I don't have prior experience in AI?",answer9:"Yes, we welcome applications from all academic backgrounds. We value your passion and commitment to learn as much as prior experience.",question10:"Is this a paid internship?",answer10:"Yes, our internship program offers competitive remuneration.",question11:"What opportunities will I have as a Jina AI intern?",answer11:"As a Jina AI intern, you'll get hands-on experience working on challenging projects, learn from industry experts, be part of a vibrant community, and have the opportunity to make real contributions to our pioneering work in multimodal AI."},x={question1:"What languages do you offer for the tour?",answer1:"We offer tours in German, English, French, Spanish, Chinese, and Russian.",question2:"What is the duration of the tour?",answer2:"The tour typically lasts for approximately three hours.",question3:"Is lunch provided?",answer3:"Lunch is optional and can be arranged upon request.",question4:"Can individuals register for the Open Day?",answer4:"Our Open Day is designed primarily for professional groups, such as politicians, NGOs, NPOs, and investors. However, we occasionally make exceptions based on the individual's profile.",question5:"How many people can a group consist of for the Open Day?",answer5:"We can accommodate a variety of group sizes. Please indicate the size of your group in the registration form, and we will confirm the details with you.",question6:"How can I specify areas of interest for the tour?",answer6:"There's a section in the registration form where you can specify your areas of interest or any special requests. We will do our best to tailor the tour according to your needs.",question7:"Are tours available at your Beijing or Shenzhen offices?",answer7:"At this time, we only offer tours at our Berlin headquarter located in Kreuzberg. Our Beijing and Shenzhen offices are not currently open for tours."},A={api_docs:"API Docs",api_docs_explain:"Auto codegen for your copilot IDE or LLM",maximize_btn:"Maximize",maximize:"⇧1",logos:"Download logo",open_in_full:"Show all enterprise products in a new window",products:"Products",news:"News",open_day:"Open day",for_power_users:"For Power Users",for_power_users_description:"Utilize our streamlined multimodal tools to enhance your productivity.",power_users_others:"More power user tools",for_developers:"For Developers",for_developers_description:"Experience a comprehensive open-source multimodal AI stack designed for developers.",developers_others:"More developer tools",for_enterprise:"For Enterprises",for_enterprise_description:"Discover scalable multimodal AI strategies tailored to meet business needs.",enterprise_others:"More enterprise tools",internship1:"Intern program",company:"Company",about_us:"About us",contact_us:"Contact sales",jobs:"Join us",join_discord:"Join our Discord community"},I={soc2:"We are SOC 2 Type 1 & 2 compliant with the American Institute of Certified Public Accountants (AICPA).",security:"Security",sefo:"Search Foundation",api_documentation:"API Documentation",get_api_key:"Get Jina API key",power_users:"Power Users",developers:"Developers",enterprise:"Enterprise",address_beijing:"Beijing, China",address_shenzhen:"Shenzhen, China",address_berlin:"Berlin, Germany (HQ)",address_sunnyvale:"Sunnyvale, CA",offices:"Offices",docs:"Docs",company:"Company",all_rights_reserved:"All rights reserved.",tc:"Terms & Conditions",tc1:"Terms",privacy:"Privacy",status:"API Status",status_short:"Status",privacy_policy:"Privacy Policy",privacy_settings:"Manage Cookies"},T={description:"Premier tool for prompt engineering",intro:"Premier tool for prompt engineering",intro1:"The premier tool for prompt engineering",original_title:"Original prompt",optimized_title:"Optimized prompt",original:"Your role is to be my brainstorming partner.",optimized:"Your task is to be my brainstorming partner and provide creative ideas and suggestions for a given topic or problem. Your response should include original, unique, and relevant ideas that could help solve the problem or further explore the topic in an interesting way. Please note that your response should also take into account any specific requirements or constraints of the task.",text_model:"Text models",image_model:"Image models"},P={description:"More modality, longer memory, less cost",example_1:"Who are you?",example_2:"I'm a LLM chat service made by Jina AI"},q={intro1:"Leading AI solution for image captions and video summaries",description:"Explore image storytelling beyond pixels",example1:"This video appears to be a nature footage featuring a charming white bunny and a butterfly in a grassy field. The bunny is seen interacting with the butterfly in different ways, showcasing their unique relationship. The natural surroundings provide a picturesque backdrop, enhancing the beauty of this simple yet captivating scene.",caption_image_title:"Caption Image",caption_image_desc:"Generate a textual description of the image.",json_image_title:"Extract JSON from Image",json_image_desc:"Generate a structured JSON format from the image using a predefined schema. This allows for specific data extraction from the image.",visual_q_a_title:"Visual Q&A",visual_q_a_desc:"Answer a query based on the image's content.",summarize_video_title:"Summarize Video",summarize_video_desc:"Generate a concise summary of the video, highlighting key events.",generate_story_title:"Generate Story",generate_story_desc:"Craft a story inspired by the image, often featuring dialogues or monologues of its characters."},S={description:"Ultimate AI decision-making tools",intro:"See two sides of the coin, make rational decisions",decision:"Decision"},R={description:"Blog to banner, without the prompts!",example_title:"Alice's Adventures in Wonderland - Chapter 1",example_description:"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?” So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her."},M={description:"The data structure for multimodal data"},L={description:"Build multimodal AI applications on the cloud"},C={description:"Fine-tune embeddings on domain specific data for better search quality",intro:"On-prem embedding-tuning for your company on your data"},z={description:"Share and discover building blocks for multimodal AI applications"},E={description:"Embed images and sentences into fixed-length vectors with CLIP"},B={description:"A human-in-the-Loop workflow for creating HD images from text"},j={description:"Create compelling Disco Diffusion artworks in one line of code"},W={description:"Agent techniques to augment your LLM and push it beyond its limits"},F={description:"Deploy a local project as a cloud service. Radically easy, no nasty surprises."},O={description:"Langchain apps on production with Jina & FastAPI"},D={description:"A Python vector database you just need - no more, no less"},G={description:"An open-source cloud-native of large multimodal models serving framework"},U={description:"An experimental finetuner for open-source LLMs"},J={description:"Empower your enterprise with on-premise finetuning solutions"},H={description:"State-of-the-art multimodal models available for inference"},N={description:"Cloud hosting platform for multimodal AI applications"},Y={description:"Bridging the semantic gap in your existing search infrastructure"},K={description:"Navigate, interact, refine: reimagine product discovery"},V={top_n:"Number of returned documents",top_n_explain:"The number of most relevant documents to return for the query.",example_input_query:"Example query",customize_urself:"Change it and see how the response changes!",customize_urself_pl:"Change them and see how the response changes!",example_input_document:"Example candidate documents to rank",v2_features:{title1:"Multilingual Retrieval",description1:"Reranker v2 enables document retrieval in over 100 languages, regardless of the query language.",title2:"Function-Calling & Code Search",description2:"Reranker v2 ranks code snippets and function signatures based on natural language queries, ideal for Agentic RAG applications.",title3:"Tabular and Structured Data Support",description3:"Reranker v2 ranks the most relevant tables based on natural language queries, helping to sort different table schemas and identify the most relevant one before generating an SQL query."},show_v2benchmark:"Show benchmark for v2 model (latest)",read_more_about_v2:"Jina Reranker v2 is the best-in-class reranker released on Jun 25th 2024; it is built for Agentic RAG. It features function-calling support, multilingual retrieval for over 100 languages, code search capabilities, and offers a 6x speedup over v1. Read more about v2 model.",v2benchmark:{titleMKQA:"MKQA (Multilingual Knowledge Questions and Answers)",descMKQA:"Recall 10 scores reported for different reranking models for MKQA dataset",titleBeir:"BEIR (Heterogeneous Benchmark on Diverse IR Tasks)",descBeir:"NDCG 10 scores reported for different reranking models for Beir dataset",titleToolBench:"ToolBench. The benchmark collects over 16 thousand public APIs and corresponding synthetically-generated instructions for using them in single and multi-API settings.",descToolBench:"Recall 3 scores reported for different reranking models for ToolBench dataset",titleNSText2SQL:"NSText2SQL",descNSText2SQL:"Recall 3 scores reported for different reranking models for NSText2SQL dataset",titleRTX4090:"Throughput of Jina Reranker v2 on RTX4090",descRTX4090:"Throughput (documents retrieved in 50ms) scores reported for different reranking models on an RTX 4090 GPU.",titleCodeSearchNet:"CodeSearchNet. The benchmark is a combination of queries in docstring and natural language formats, with labelled code-segments relevant to the queries.",descCodeSearchNet:"MRR 10 scores reported for different reranking models for CodeSearchNet dataset"},table:{title:"Below is the time cost of reranking one query and 100 documents in milliseconds:",number_token_document:"Number of tokens in each document",number_token_query:"Number of tokens in the query"},faq_v1:{title:"Reranker-related common questions",question1:"How much does the Reranker API cost?",answer1:"The pricing for the Reranker API is aligned with our Embedding API pricing structure. It begins with 1 million free tokens for each new API key. Beyond the free tokens, different packages are available for purchase. For more details, please visit our pricing section.",question3:"What is the difference between the two rerankers?",answer3:"<code>jina-reranker-v2-base-multilingual</code> excels in multilingual support, outperforming <code>bge-reranker-v2-m3</code> and offering 15x faster throughput than <code>jina-reranker-v1-base-en</code>. It also supports agentic tasks and code retrieval. <code>jina-colbert-v2</code> improves upon <code>ColBERTv2</code>, delivering 6.5% better retrieval performance and adding multilingual support for 89 languages. It features user-controlled embedding sizes for optimal efficiency and precision.",question4:"Are Jina Rerankers open source?",answer4:"Yes, both <code>jina-reranker-v2-base-multilingual</code> and <code>jina-colbert-v2</code> are open source and available under the CC-BY-NC 4.0 license. You are freely to use, share, and adapt the models for non-commercial purposes.",question5:"Do the rerankers support multiple languages?",answer5:"Yes, both <code>jina-reranker-v2-base-multilingual</code> and <code>jina-colbert-v2</code> support 100+ languages, including English, Chinese, and other major global languages. They are optimized for multilingual tasks and outperform previous models.",question6:"What is the maximum length for queries and documents?",answer6:"The maximum query token length is 512. There is no token limit for documents.",question7:"What is the maximum number of documents I can rerank per query?",answer7:"You can rerank up to 2048 documents per query.",question8:"What is the batch size and how many query-document tuples can I send in one request?",answer8:"There is no concept of batch size unlike our Embedding API. You can send only one query-document tuple per request, but the tuple can include up to 2048 candidate documents.",question9:"What latency can I expect when reranking 100 documents?",answer9:"Latency varies from 100 milliseconds to 7 seconds, depending largely on the length of the documents and the query. For instance, reranking 100 documents of 256 tokens each with a 64-token query takes about 150 milliseconds. Increasing the document length to 4096 tokens raises the time to 3.5 seconds. If the query length is increased to 512 tokens, the time further increases to 7 seconds.",question10:"Can your endpoints be hosted privately on AWS, Azure, or GCP?",answer10:"Yes, our services are available on AWS, Azure, and GCP marketplaces. If you have specific requirements, please contact us at sales AT jina.ai.",question11:"Do you offer a fine-tuned reranker on domain-specific data?",answer11:"If you are interested in a fine-tuned reranker tailored to specific domain data, please contact our sales team. Our team will respond to your inquiry promptly."},title:"Reranker API",read_more_about_benchmark:"Read more about the benchmark",read_more_about_turbo:"Read more about the turbo and tiny models",choose_turbo:"Get up to 5x speedup with reranker-turbo",choose_turbo_description:"We also offer two new open-source reranker models: jina-reranker-v1-turbo-en and jina-reranker-v1-tiny-en, the latter has only 30M parameters and four layers. These two new rerankers enjoy 5X faster inference speed than the base model at only a very small cost on the quality. They are perfect for applications that require real-time reranking. Read the benchmark below.",benchmark_title:"Performance Benchmark",try_embedding:"Try embedding API for free",try_reranker:"Try reranker API for free",benchmark_description:"For comparison, we included three other leading rerankers by BGE (BAAI), BCE (Netease Youdao), and Cohere in the benchmark. As shown by the results below, Jina Reranker holds the highest average score in all relevant categories for reranking, making it a clear leader among its peers.",benchmark:{title:"Benchmark",title0:"LlamaIndex",title1:"BEIR",title2:"LoCo",title3:"MTEB",description0:"LlamaIndex assessed various combinations of embeddings and rerankers for RAG, conducting a replication study that measured the Mean Reciprocal Rank. The findings highlight the Jina Reranker's significant enhancement of search quality, a benefit that is independent of the specific embeddings used.",description1:"BIER (Benchmarking IR) assesses a model's retrieval effectiveness, including relevance and NDCG. A higher BIER score correlates to more accurate matches and search result rankings.",description2:"Through the LoCo benchmark, we measured a model's understanding of local coherence and context, together with query-specific ranking. A LoCo higher score reflects a better ability to identify and prioritize relevant information.",description3:"The MTEB (Multilingual Text Embedding Benchmark), on the whole, tests a model’s abilities in text embeddings, including clustering, classification, retrieval, and other metrics. However, for our comparison, we only used the MTEB’s Reranking tasks."},vs_table:{title:"Comparison of Reranker, Vector Search, and BM25",subtitle:"The table below provides a comprehensive comparison of the Reranker, Vector/Embeddings Search, and BM25, highlighting their strengths and weaknesses across various categories.",col0:"Reranker",col1:"Vector Search",col2:"BM25",header0:"Best For",header1:"Granularity",header2:"Query Time Complexity",header3:"Indexing Time Complexity",header4:"Training Time Complexity",header5:"Search Quality",header6:"Strengths",header7:"Weaknesses",col0_2:"Initial, rapid filtering",col0_1:"Enhanced search precision and relevance",col0_3:"General text retrieval across wide-ranging queries",col1_1:"Detailed: Sub-document and query segment",col1_2:"Broad: Entire documents",col1_3:"Intermediate: Various text segments",col2_1:"High",col2_2:"Medium",col2_3:"Low",col3_1:"Not required",col3_2:"High",col3_3:"Low, utilizes pre-built index",col4_1:"High",col4_2:"High",col4_3:"Not required",col5_1:"Superior for nuanced queries",col5_2:"Balanced between efficiency and accuracy",col5_3:"Consistent and reliable for a broad set of queries",col6_1:"Highly accurate with deep contextual understanding",col6_2:"Quick and efficient, with moderate accuracy",col6_3:"Highly scalable, with established efficacy",col7_1:"Resource-intensive with complex implementation",col7_2:"May not capture deep query context or nuances",col7_3:"May underperform for highly specific or contextual searches"},feature_solid_description:"Developed from our cutting-edge academic research and rigorously tested against the SOTA rerankers to ensure unparalleled performance.",improve_performance:"+33% relevance over vector search",improve_performance_description:"Our evaluations show that search systems employing the Jina Reranker enjoy +8% in hit rate and +33% in mean reciprocal rank.",description_rich:"Maximize the search relevancy and RAG accuracy with our cutting-edge reranker API.",reranker_description:"Try our cutting-edge reranker API to maximize your search relevancy and RAG accuracy. Starting for free!",description:"World-class neural retriever for maximizing search relevancy.",learning1:"Learning about Reranker",what_is:"What is a Reranker?",how_it_works:"Here's how it works:",how_it_works_v1:{title1:"Initial Retrieval",description1:"A search system uses embeddings/BM25 to find a broad set of potentially relevant documents based on the user's query.",title2:"Reranking",description2:"The reranker then takes these results and analyzes them at a more granular level, considering the nuances of how the query terms interact with the document content.",title3:"Improved Results",description3:"It reorders the search results, placing the ones it deems most relevant at the top, based on this deeper analysis."},what_is_answer_long:`The goal of a search system is to find the most relevant results quickly and efficiently. Traditionally, methods like BM25 or tf-idf have been used to rank search results based on keyword matching. Recent methods, such as embedding-based cosine similarity, have been implemented in many vector databases. These methods are straightforward but can sometimes miss the subtleties of language, and most importantly, the interaction between documents and a query's intent.

This is where the "reranker" shines. A reranker is an advanced AI model that takes the initial set of results from a search—often provided by an embeddings/token-based search—and reevaluates them to ensure they align more closely with the user's intent. It looks beyond the surface-level matching of terms to consider the deeper interaction between the search query and the content of the documents.`,what_is_answer_long_ending:"The reranker can significantly improve the search quality because it operates at a sub-document and sub-query level, meaning it looks at the individual words and phrases, their meanings, and how they relate to each other within the query and the documents. This results in a more precise and contextually relevant set of search results.",what_is_desc:"A reranker is an AI model that refines the search results from a vector search or a dense retrieval model. Read more.",learning1_description:"What is a reranker? Why is vector search or cosine similarity not enough? Learn about rerankers from the ground up with our comprehensive guide.",feature_on_premises_description2:"Deploy Jina Reranker on AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers.",feature_on_premises_description3:"Deploy Jina Reranker on AWS Sagemaker and Microsoft Azure and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."},Q={lm_v2_title:"ReaderLM v2: Small Language Model for HTML to Markdown and JSON",lm_v2_description:"ReaderLM-v2 is a 1.5B parameter language model specialized in HTML-to-Markdown conversion and HTML-to-JSON extraction. It supports documents up to 512K tokens across 29 languages and offers 20% higher accuracy compared to its predecessor.",check_price_table:"Check the price table",want_higher_rate_limit:"Are you already a paid API user but still want a higher rate limit of up to 1000 RPM? We can support you!",rate_limit:"Rate limit",table:{th0:"Endpoint",th1:"Description",th2:"Rate limit w/o API key",th3:"Rate limit with API key",th4:"Rate limit with API key and premium plan",th5:"Token counting scheme",th6:"Average latency",td_1_0:"Read a URL return its content, useful for check grounding",td_1_1:"20 RPM",td_1_2:"200 RPM",td_1_3:"1000 RPM",td_1_4:"Based on the output tokens",td_1_5:"3 seconds",td_2_0:"Search on the web return top-5 results, useful for search grounding",td_2_1:"5 RPM",td_2_2:"40 RPM",td_2_3:"100 RPM",td_2_4:"Based on the output tokens for all 5 search results",td_2_5:"10 seconds"},reader_do_search:"Reader for web search",read_grounding_release_note:"Read release note",reader_do_grounding:"Reader for fact-checking",reader_do_grounding_explain:"The new grounding endpoint offers an end-to-end, near real-time fact-checking experience. It takes a given statement, grounds it using real-time web search results, and returns a factuality score and the exact references used. You can easily ground statements to reduce LLM hallucinations or improve the integrity of human-written content.",reader_do_pdf_explain:"Yes, Reader natively supports PDF reading. It's compatible with most PDFs, including those with many images, and it's lightning fast! Combined with an LLM, you can easily build a ChatPDF or document analysis AI in no time.",reader_do_search_explain:"Reader allows you to feed your LLM with the latest information from the web. Simply prepend https://s.jina.ai/ to your query, and Reader will search the web and return the top five results with their URLs and contents, each in clean, LLM-friendly text. This way, you can always keep your LLM up-to-date, improve its factuality, and reduce hallucinations.",is_free:"The best part? It's free!",is_free_description:"Reader API is available for free and offers flexible rate limit and pricing. Built on a scalable infrastructure, it offers high accessibility, concurrency, and reliability. We strive to be your preferred grounding solution for your LLMs.",dont_panic_api_key_is_free:"Don't panic! Every new API key contains one million free tokens!",reader_reads_images:"Reader also reads images!",reader_reads_pdf:"Reader also reads PDFs!",original_pdf:"Original PDF",reader_result:"Reader Result",reader_also_read_images:"Images on the webpage are automatically captioned using a vision language model in the reader and formatted as image alt tags in the output. This gives your downstream LLM just enough hints to incorporate those images into its reasoning and summarizing processes. This means you can ask questions about the images, select specific ones, or even forward their URLs to a more powerful VLM for deeper analysis!",description:"Convert any URL to Markdown for better grounding LLMs.",reader_description:"Convert a URL to LLM-friendly input, by simply adding <code>r.jina.ai</code> in front.",what_is1:"What is Reader?",what_is_desc:"A proxy that accesses any URL and transforms the main content into plain text optimized for LLMs.",open:"Open in new tab",copy:"Copy",title:"Reader API",usage:"Usage",better_input:"Enhance input quality right from the start",fast_stream:"Immediate data streaming",free:"Free forever",fast:"Fast",faq_v1:{title:"Reader-related common questions",question1:"What are the costs associated with using the Reader API?",answer1:"The Reader API is free of charge and does not require an API key. Simply prepend 'https://r.jina.ai/' to your URL.",question2:"How does the Reader API function?",answer2:"The Reader API uses a proxy to fetch any URL, rendering its content in a browser to extract high-quality main content.",question3:"Is the Reader API open source?",answer3:"Yes, the Reader API is open source and available on the Jina AI GitHub repository.",question4:"What is the typical latency for the Reader API?",answer4:"The Reader API generally processes URLs and returns content within 2 seconds, although complex or dynamic pages might require more time.",question5:"Why should I use the Reader API instead of scraping the page myself?",answer5:"Scraping can be complicated and unreliable, particularly with complex or dynamic pages. The Reader API provides a streamlined, reliable output of clean, LLM-ready text.",question6:"Does the Reader API support multiple languages?",answer6:"The Reader API returns content in the original language of the URL. It does not provide translation services.",question7:"What should I do if a website blocks the Reader API?",answer7:"If you experience blocking issues, please contact our support team for assistance and resolution.",question8:"Can the Reader API extract content from PDF files?",answer8:"Yes, the Reader API can natively extract content from PDF files.",question9:"Can the Reader API process media content from web pages?",answer9:"Currently, the Reader API does not process media content, but future enhancements will include image captioning and video summarization.",question10:"Is it possible to use the Reader API on local HTML files?",answer10:"No, the Reader API can only process content from publicly accessible URLs.",question11:"Does Reader API cache the content?",answer11:"If you request the same URL within 5 minutes, the Reader API will return the cached content.",question12:"Can I use the Reader API to access content behind a login?",answer12:"Unfortunately not.",question13:"Can I use the Reader API to access PDF on arXiv?",answer13:"Yes, you can either use the native PDF support from the Reader (https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4) or use the HTML version from the arXiv (https://r.jina.ai/https://arxiv.org/html/2310.19923v4)",question14:"How does image caption work in Reader?",answer14:"Reader captions all images at the specified URL and adds `Image [idx]: [caption]` as an alt tag (if they initially lack one). This enables downstream LLMs to interact with the images in reasoning, summarizing etc.",question15:"What is the scalability of the Reader? Can I use it in production?",answer15:"The Reader API is designed to be highly scalable. It is auto-scaled based on the real-time traffic and the maximum concurrency requests is now around 4000. We are maintaining it actively as one of the core products of Jina AI. So feel free to use it in production.",question16:"What is the rate limit of the Reader API?",answer16:"Please find the latest rate limit information in the table below. Note that we are actively working on improving the rate limit and performance of the Reader API, the table will be updated accordingly.",question17:"What is Reader-LM? How can I use it?",answer17:"Reader-LM is a novel small language model (SLM) designed for data extraction and cleaning from the open web. It converts raw, noisy HTML into clean markdown, drawing inspiration from Jina Reader. With a focus on cost-efficiency and small model size, Reader-LM is both practical and powerful. It is currently available on AWS, Azure, and GCP marketplaces. If you have specific requirements, please contact us at sales AT jina.ai."},beta:"Experimental",demo:{grounding_result_true:"This statement is true.",grounding_result_false:"This statement is false.",warn_grounding_title:"High latency and token usage!",warn_grounding_message:"Grounding API is experimental! This process may take up to 20 seconds and consume up to 100K tokens per grounding request. Some browsers may terminate the request due to the long latency, so we recommend copying the code and running it from your terminal.",key_required:"API key required to use this endpoint",slow_warning:"This may take up to 20 seconds and costs up to 100K tokens per request.",select_mode:"Select Mode",search_params:"Search Parameters/Headers",search_query_rewrite:"Please note that unlike the demo shown above, in practice you do not search the original question on the web for grounding. What people often do is rewrite the original question or use multi-hop questions. They read the retrieved results and then generate additional queries to gather more information as needed before arriving at a final answer.",params_classification:"Parameters",common_parameters:"Common",common_parameter_explain:"Common parameters that can be used for {_product1}, {_product2} and {_product3}.",advanced_parameters:"Specific",advanced_parameter_explain:"Specific parameters that are only used for {_product}.",raw_html:"Raw HTML",waiting_for_reader:"Waiting for the Reader API result first...",basic_usage:"Basic Usage",show_read_demo:"See how Reader reads a URL",show_search_demo:"See how Reader searches the web",basic_usage1:"Use <code>r.jina.ai</code> to read a URL",basic_usage2:"Use <code>s.jina.ai</code> to search a query",basic_usage3:"Use <code>g.jina.ai</code> for grounding",reader_output:"Reader Output",try_demo:"Demo",ask_llm:"Ask LLM w/o & w/ Search Grounding",use_headers:"The behavior of the Reader API can be controlled with request headers. Here is a complete list of supported headers.",standard_usage:"Standard Usage",stream_mode:"Stream Mode",stream_mode_explain1:"Streaming mode is useful when you find that the standard mode provides an incomplete result. This is because streaming mode will wait a bit longer until the page is fully rendered. Use the accept-header to toggle the streaming mode:",stream_mode_explain:"Stream mode is useful when the target page is large to render. If you find standard mode gives you incomplete content, try stream mode.",how_to_stream:"To process content as it becomes available, set the request header to stream mode. This minimizes the time until the first byte is received. Example in curl:",how_to_use1:"This will return the main content of the page in clean, LLM-friendly text.",how_to_use2:"This will search the web and returns URLs and contents, each in clean, LLM-friendly text.",how_to_use3:"This will call our grounding engine do fact-checking.",tagline:"Try the demo",your_url:"Enter your URL",advanced_usage:"Advanced Usage",your_query:"Enter your query",your_statement:"Your fact-checking statement",your_query_hint:"Type a question that requires latest information or world knowledge.",reader_search_hint:"If you use this URL in code, dont forget to encode the URL.",ask_llm_directly:"Ask LLM directly",ask_llm_with_search_grounding:"Ask LLM with search grounding",reader_response:"Reader's response",reader_url:"Reader URL",fetch:"Fetch Content",copy:"Copy",ask_question:"Pose a Question",your_url_hint:"Click below to fetch the source code of the page directly",reader_url_hint:"Click below to obtain the content through our Reader API",ask_question_hint:"Input a question and combine it with the fetched content for LLM to generate an answer",get_response:"Get response",headers:{base:"Redirect Resolution",base_explain:"Choose whether to resolve to the final destination URL after following all redirects. Enable to follow the full redirect chain.",engine:"Read Engine",engine_explain:"Choose the engine to use for parsing the content for the given URL. Affect the quality, speed, compatibility of the result.",auto:"Auto (default)",auto_explain:"Our server automatically selects the best engine for the given URL.",browser:"Default",browser_explain:"Most compatible engine offering good balance between quality and speed.",direct:"Speed First",direct_explain:"Fastest engine, but may struggle with few JavaScript-heavy websites.",vlm:"VLM",vlm_explain:"Ideal for short pages featuring rich media and complex layouts.",respond_with:"Use ReaderLM-v2",respond_with_explain:"Uses ReaderLM-v2 for HTML to Markdown conversion, to deliver high-quality results for websites with complex structures and contents. Costs 3x tokens!",site_selector:"In-site Search",site_selector_explain:"Returns the search results only from the specified website or domain. By default it searches the entire web.",post_with_url:"Use POST Method",post_with_url_explain:"Use POST instead of GET method with a URL passed in the body. Useful for building SPAs with hash-based routing.",links_summary:"Gather All Links At the End",images_summary:"Gather All Images At the End",token_budget:"Token Budget",token_budget_explain:"Limits the maximum number of tokens used for this request. Exceeding this limit will cause the request to fail.",links_summary_explain:'A "Buttons & Links" section will be created at the end. This helps the downstream LLMs or web agents navigating the page or take further actions.',images_summary_explain:'An "Images" section will be created at the end. This gives the downstream LLMs an overview of all visuals on the page, which may improve reasoning.',mode:"Read or Search Mode",mode_explain:"Read mode is for accessing the content of a URL, while Search mode allows you to search a query on the web, applying Read mode to each search result URL.",auth_token:"Add API Key for Higher Rate Limit",auth_token_explain:"Enter your Jina API key to access a higher rate limit. For latest rate limit information, please refer to the table below.",image_caption:"Image Caption",image_caption_explain:"Captions all images at the specified URL, adding 'Image [idx]: [caption]' as an alt tag for those without one. This allows downstream LLMs to interact with the images in activities such as reasoning and summarizing.",json_response:"JSON Response",json_response_explain:"The response will be in JSON format, containing the URL, title, content, and timestamp (if available). In Search mode, it returns a list of five entries, each following the described JSON structure.",set_cookie:"Forward Cookie",set_cookie_explain:"Our API server can forward your custom cookie settings when accessing the URL, which is useful for pages requiring extra authentication. Note that requests with cookies will not be cached.",proxy_server:"Use a Proxy Server",proxy_server_explain:"Our API server can utilize your proxy to access URLs, which is helpful for pages accessible only through specific proxies.",no_cache:"Bypass the Cache",no_cache_explain:"Our API server caches both Read and Search mode contents for a certain amount of time. To bypass this cache, set this header to true.",stream_mode:"Stream Mode",stream_mode_explain:"Stream mode is beneficial for large target pages, allowing more time for the page to fully render. If standard mode results in incomplete content, consider using Stream mode.",browser_locale:"Browser Locale",browser_locale_explain:"Control the browser locale to render the page. Lots of websites serve different content based on the locale.",deepdive:"Deep source analysis",deepdive_explain:"Searches more sources and reads full documents for thorough fact-checking. Slightly slower but more accurate and more references.",return_format:"Content Format",return_format_explain:"You can control the level of detail in the response to prevent over-filtering. The default pipeline is optimized for most websites and LLM input.",target_selector:"Target Selector",target_selector_explain:"Provide a list of CSS selector to focus on more specific parts of the page. Useful when your desired content doesn't show under the default settings.",x_timeout:"Timeout",x_timeout_explain:"Maximum time to wait for the webpage to load. Note that this is NOT the total time for the whole end-to-end request.",wait_for_selector:"Wait For Selector",wait_for_selector_explain:"Provide a list of CSS selector to wait for specific elements to appear before returning. Useful when your desired content doesn't show under the default settings.",remove_selector:"Excluded Selector",remove_selector_explain:"Provide a list of CSS selector to remove the specified elements of the page. Useful when you want to exclude specific parts of the page like headers, footers, etc.",with_iframe:"Enable iframe Extraction",with_iframe_explain:"Extracts and processes content from all embedded iframes within the DOM tree",with_shadow_dom:"Enable Shadow DOM Extraction",with_shadow_dom_explain:"Traverses and extracts content from all Shadow DOM roots in the document",custom_script:"Pre-Execute Custom JavaScript",custom_script_explain:"Executes pre-processing JavaScript code, accepting either inline code string or remote script URL endpoint",viewport:"Viewport Configuration",viewport_explain:"Configure browser viewport dimensions for responsive rendering",default:"Default",default_explain:"The default pipeline optimized for most websites and LLM input.",markdown:"Markdown",markdown_explain:"Returns the markdown directly from the HTML, bypassing the readability filtering.",with_gfm:"Enabled",with_gfm_explain:"GFM (Github Flavored Markdown) features enabled.",no_gfm:"Disabled",no_gfm_explain:"GFM (Github Flavored Markdown) features disabled.",opt_out_gfm:"Github Flavored Markdown",opt_out_gfm_explain:"Opt in/out features from GFM (Github Flavored Markdown).",no_gfm_table:"No GFM Table",no_gfm_table_explain:"Opt out GFM Table but keep the table HTML elements in response.",file:"Local PDF/HTML file",file_explain:"Use Reader on your local PDF and HTML file by uploading them. Only support pdf and html files. For HTML, please also specify a reference URL for better parsing related CSS/JS scripts.",html:"HTML",html_explain:"Returns documentElement.outerHTML.",references:"Reference URLs",references_explain:"A comma-separated list of URLs to be used as grounding references.",text:"Text",text_explain:"Returns document.body.innerText.",remove_all_images:"Remove All Images",remove_all_images_explain:"Remove all images from the response.",screenshot:"Screenshot",pageshot:"Pageshot",pageshot_explain:"Returns the image URL of full page screenshot (with best effort).",screenshot_explain:"Returns the image URL of the first screen.",result_count:"Result Limit",result_count_explain:"The number of search results to return."},learn_more:"Learn more",open:"Open in a new tab",requires_post_method:"This function requires POST method. By uploading your local file, POST method will be turned on automatically."},usage_details_null:"Show basic and advanced usages",usage_details_true:"Show advanced usages only",usage_details_false:"Show basic usages only",what_is_answer_long:"Feeding web information into LLMs is an important step of grounding, yet it can be challenging. The simplest method is to scrape the webpage and feed the raw HTML. However, scraping can be complex and often blocked, and raw HTML is cluttered with extraneous elements like markups and scripts. The Reader API addresses these issues by extracting the core content from a URL and converting it into clean, LLM-friendly text, ensuring high-quality input for your agent and RAG systems.",free_description:"Reader API is free! It requires no credit card or API secret. It will not consume your token quota.",fast_stream_description:"Need data quickly? Our Reader API can stream data to minimize latency.",better_input_description:"Experiencing issues with your agent or RAG system output? It might be due to poor input quality."},X={browse_catalog:"Browse catalog",search_for:"Search it on our site",find_on_hf:"Find it on HuggingFace",api:"Jina AI API",use_it_via:"Use it via",deploy_it_on:"Deploy it on",contact_sales_about_it:"Contact sales about it",search_models:"Filter by model name",title:"Our Search Foundation Models",description:"We've been moving the needle in search models since day one. Take a look at our model evolution below—hover or click to discover each milestone."},$={embedding_provider:"Select a base embedding model",temporarily_unavailable:"Temporarily unavailable. We are upgrading our auto fine-tune system to serve you better. Please check back later.",base_model:"Base model for fine-tuning",does_it_work_tho:"But does it work though?",does_it_work_tho_explain:"Auto fine-tuning holds an auto-magical promise to deliver fine-tuned embeddings for any domain you want. But does it really work? This is a fairly reasonable doubt. We've tested it on a variety of domains and base models to find out. Check out the cherry-picked and lemon-picked results below.",domain_instruction:"Domain instruction",eval_performance_before_after:"Performance on synthetic validation set before and after fine-tuning",eval_ndcg:"NDCG",total_improve:"Avg. improvement",eval_mrr:"MRR",eval_map:"MAP",test_performance_before_after:"Performance on held-out test set before and after fine-tuning",test_on:"Tested on {_dataSize} random samples from {_dataName}",data_size:"Synthetic data generated",eval_syntheticDataSize:"Total",eval_training:"Training",eval_evaluation:"Validation",eval_test:"Real data for testing",check_data:"Download synthetic data",check_model:"Download fine-tuned model",usage:"Usage",find_on_hf:"List fine-tuned models",title:"Auto Fine-Tuning API",what_is:"What is Auto Fine-Tuning?",description:"Get fine-tuned embeddings for any domain you want.",description_long:"Just tell us which domain you want your embeddings to excel in, and we automatically deliver a ready-to-use, fine-tuned embedding model for that domain.",what_is_answer_long:"Fine-tuning allows you to take a pre-trained model and adapt it to a specific task or domain by training it on a new dataset. In practice, finding effective training data is not straightforward for many users. Effective training requires more than just throwing raw PDFs, HTMLs into the model; and it is hard to get it right. Auto fine-tuning solves this problem by automatically generating effective training data using an advanced LLM agent pipeline; and fine-tuning the model within a ML workflow. You can think it as a combination of synthetic data generation and AutoML, so all you need to do is describe your target domain in natural language and let our system do the rest.",faq_v1:{title:"Auto Fine-Tuning-related common questions",question1:"How much does the Fine-tuning API cost?",answer1:"The feature is currently in beta and costs 1M tokens per fine-tuned model. You can use your existing API key from the Embedding/Reranker API if it has sufficient tokens, or you can create a new API key, which includes 1M free tokens.",question2:"What do I need to input? Do I need to provide training data?",answer2:"You don't need to provide any training data. Simply describe your target domain (the domain for which you want the fine-tuned embeddings to be optimized) in natural language, or use a URL as a reference, and our system will generate synthetic data to train the model.",question3:"How long does it take to fine-tune a model?",answer3:"About 30 minutes.",question4:"Where are the fine-tuned models stored?",answer4:"The fine-tuned models and synthetic data are stored publicly in the Hugging Face model hub.",question5:"If I provide a reference URL, how does the system use it?",answer5:"The system uses the Reader API to fetch the content from the URL. It then analyzes the content to summarize the tone and domain, which it uses as guidelines for generating synthetic data. Therefore, the URL should be publicly accessible and representative of the target domain.",question6:"Can I fine-tune a model for a specific language?",answer6:"Yes, you can fine-tune a model for a non-English language. The system automatically detects the language of your domain instructions and generates synthetic data accordingly. We also recommend choosing the appropriate base model for the target language. For example, if targeting a German domain, you should select the 'jina-embeddings-v2-base-de' as the base model.",question7:"Can I fine-tune non-Jina embeddings, e.g., bge-M3?",answer7:"No, our fine-tuning API only supports Jina v2 models.",question8:"How do you ensure the quality of the fine-tuned models?",answer8:"At the end of the fine-tuning process, the system evaluates the model using a held-out test set and reports performance metrics. You will receive an email detailing the before/after performance on this test set. You are also encouraged to evaluate the model on your own test set to ensure its quality.",question9:"How do you generate synthetic data?",answer9:"The system generates synthetic data by integrating the target domain instruction you provide with LLM agents' reasoning. It produces hard negative triplets, which are essential for training high-quality embedding models. For more details, please refer to our upcoming research paper on Arxiv.",question10:"Can I keep my fine-tuned models and synthetic data private?",answer10:"Currently, no. Note that this feature is still in beta. Storing the fine-tuned models and synthetic data publicly in the Hugging Face model hub helps us and the community evaluate the quality of the training. In the future, we plan to offer a private storage option.",question11:"How can I use the fine-tuned model?",answer11:"Since all fine-tuned models are uploaded to Hugging Face, you can access them via SentenceTransformers by simply specifying the model name.",question12:"I never received the email with the evaluation results. What should I do?",answer12:"Please check your spam folder. If you still can't find it, please contact our support team using the email address you provided."}},Z={more:"more",read_notes:"Read release notes",faq_v1:{title:"Classifier-related common questions",question1:"What's different about labels in zero-shot vs few-shot?",answer1:"Zero-shot requires semantic labels during classification and none during training, while few-shot requires labels during training but not classification. This means zero-shot is better for flexible, immediate classification needs, while few-shot is better for fixed, domain-specific categories that can evolve over time.",question2:"What's num_iters for and how should I use it?",answer2:"<code>num_iters</code> controls training intensity - higher values reinforce important examples while lower values minimize impact of less reliable data. It can be used to implement time-aware learning by giving recent examples higher iteration counts, making it valuable for evolving data patterns.",question3:"How does public classifier sharing work?",answer3:"Public classifiers can be used by anyone with the <code>classifier_id</code>, consuming their own token quota. Users can't access training data or configuration, and can't see others' classification requests, enabling safe classifier sharing.",question4:"How much data do I need for few-shot to work well?",answer4:"Few-shot requires 200-400 training examples to outperform zero-shot classification. While it ultimately achieves higher accuracy, it needs this warm-up period to become effective. Zero-shot provides consistent performance immediately without training data.",question5:"Can it handle multiple languages and both text/images?",answer5:"Yes - the API supports multilingual queries using <code>jina-embeddings-v3</code> and multimodal (text/image) classification using <code>jina-clip-v1</code>, with support for URL or base64 encoded images in the same request.",question6:"What are the hard limits I should know about?",answer6:"Zero-shot supports 256 classes with no classifier limit, while few-shot is limited to 16 classes and 16 classifiers. Both support 1,024 inputs per request and 8,192 tokens per input.",question7:"How do I handle data changes over time?",answer7:"Few-shot mode allows continuous updating through the <code>/train</code> endpoint for adapting to changing data patterns. You can incrementally add new examples or classes when data distribution changes, without rebuilding the entire classifier.",question8:"What happens to my training data after I send it?",answer8:"The API uses one-pass online learning - training examples update classifier weights but aren't stored afterward. This means you can't retrieve historical training data, but it ensures privacy and resource efficiency.",question9:"Zero-shot vs few-shot - when to use which?",answer9:"Start with zero-shot for immediate results and when you need flexible classification with semantic labels. Switch to few-shot when you have 200-400 examples, need higher accuracy, or need to handle domain-specific/time-sensitive data.",question10:"Can I use different models for different languages/tasks?",answer10:"Yes, you can choose between <code>jina-embeddings-v3</code> for text classification (especially good for multilingual) and <code>jina-clip-v1</code> for multimodal classification. New models like <code>jina-clip-v2</code> will be automatically available through the API when released."},access_public:"Public",access_private:"Private",title:"Classifier API",description_long:"Try out our API playground to see how our classifier works.",description_long1:"High performance zero-shot and few-shot classifier for multimodal and multilingual data.",what_is:"What is Classifier?",explain:"The Classifier is an API service that categorizes text and images using embedding models (<code>jina-embeddings-v3</code> and <code>jina-clip-v1</code>), supporting both zero-shot classification without training data and few-shot learning with minimal examples.",when_to_use_what:"When to use zero-shot or few-shot?",task_select:"Select a task",task_classify:"Classify",task_classify_explain:"Use zero-shot or a few-shot classifier to categorize text or images into defined classes.",task_train:"Train",task_train_explain:"Create or update a few-shot classifier with labeled examples.",task_manage:"Manage",task_manage_explain:"List, or delete your few-shot classifiers.",classify_inputs:"Inputs to classify",classify_labels:"Candidate labels",create_classifier:"New few-shot classifier",access:"Public access",train_label:"Label",train_inputs:"Training data",api_list:"List classifiers",api_list_explain:"List all classifiers you have created.",api_delete:"Delete classifier",classifier_id:"Classifier ID",api_delete_explain:"Delete a classifier by its ID.",train_inputs_explain:"Text or image examples with labels for training. You can incrementally update the classifier with new examples and labels over time.",num_iters:"Training iterations",num_iters_explain:"Controls training intensity - higher values improve accuracy on current examples but increase token cost. Default of 10 typically works well.",access_explain:"Public classifiers can be used by anyone with the <code>classifier_id</code>, and their usage will consume the caller's token quota rather than yours. Private classifiers are only accessible by you.",create_classifier_explain:"Create a new few-shot classifier and train it with labeled examples.",classify_labels_explain:"Inputs will be categorized into these labels. It can be up to 256 classes. Use semantic labels for better performance.",select_classifier_or_model:"Select a classifier or an embedding model",classify_inputs_explain:"For text, it can be a sentence up to 8192 tokens. For images, it can be a URL or a base64-encoded image.",when_to_use_what_explain:"Use zero-shot classification as your default solution for immediate results on general classification tasks with up to 256 classes, while few-shot learning is better suited when dealing with domain-specific data outside the embedding models' knowledge or when you need to handle time-sensitive data that requires continuous model updates.",description:"Zero-shot and few-shot classification for image and text.",compare_table:{image_multi_lingual_support:"Multimodal & Multilingual Support",feature:"Feature",zero_shot:"Zero-shot",few_shot:"Few-shot",primary_use_case:"Primary Use Case",default_solution:"Default solution for general classification",out_of_domain_solution:"For data outside v3/clip-v1's domain or time-sensitive data",training_data_required:"Training Data Required",labels_required_train:"Labels Required in /train",labels_required_classify:"Labels Required in /classify",classifier_id_required:"Classifier ID Required",semantic_labels_required:"Semantic Labels Required",state_management:"State Management",continuous_updates:"Continuous Model Updates",access_control:"Access Control",max_classes:"Maximum Classes",max_classifiers:"Maximum Classifiers",max_inputs_request:"Maximum Inputs per Request",max_token_length:"Maximum Token Length per Input",yes:"Yes",no:"No",na:"N/A",stateless:"Stateless",stateful:"Stateful",token_count:"{count} tokens"}},ee={for_better_search:"For Better Search",key_manager:"Manage API Key",avatar:"Avatar Generator",grounding:"Grounding",embedding:"Embeddings",models:"Models",ccbync:"This model is licensed under CC BY-NC 4.0. Use it via API or our official AWS/Azure image; or contact sales for on-premises deployment.",classifier:"Classifier",num_publications:"{_total} publications in total.",go_to_product_homepage:"Go to product homepage",our_customer:"Our Customers",our_customer_explain:"Businesses of all sizes trust Jina AI’s Search Foundation to power their tools and products—so can you.",more:"More",tokenizer:"Segmenter",for_enterprise:"For Enterprise",for_power_users:"For Power Users",for_developers:"For Developers",badge:{v2:"v2 release!",v3:"v3 release!","clip-v2":"clip-v2 release!","readerlm-v2":"ReaderLM-v2 release!"},autotune:"Auto Fine-Tuning",get_started:"Get started",source_code:"Source code",reader:"Reader",podcast:"Podcast",get_api_now:"API",pricing:"Pricing",your_search_foundation1:"Your Search Foundation",search_foundation:"Search Foundation",supercharged1:"Supercharged.",learn_more_embeddings:"Learn more about embeddings",learn_more_reranker:"Learn more about reranker",learn_more_reader:"Learn more about reader",try_it_for_free:"Start instantly—no credit card or registration needed!",reranker:"Reranker",finding_faq:"Generating answer based on the FAQ knowledge below",embeddings:"Embeddings",new:"New","on-premises":"On-premises","on-prem-deploy":"On-premises deployment",also_available_on1:"Available on the marketplaces of your enterprise cloud",coming_soon:"Coming soon",try_our_saas:"Try our hosted solution, a drop-in replacement for OpenAI's embedding API.",also_available_on:"Available on the marketplaces",our_publications:"Our Publications",embedding_desc1:"Top-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.",enterprise_desc_v2:"Try our world-class embedding models to improve your search and RAG systems. Start with a free trial!",enterprise_desc_v3:"Our frontier models form the search foundation for high-quality enterprise search and RAG systems.",researcher_desc:"Understand how our frontier search models were trained from scratch, check out our latest publications. Meet our team at EMNLP, SIGIR, ICLR, NeurIPS, and ICML!",include_experiment:"Includes our experimental and archived projects in the solution.",your_portal_to:"Your Portal to",copy:"Copy",require_full_question:"Please describe your problem with more details.",proposing_solution:"Proposing a solution based on Jina AI products...",mentioned_products:"Mentioned products:",copied_to_clipboard:"Copied to clipboard",checkout_our_solution_for_you:"Find out our solution tailored for you",ask_how_your_question:"Please describe your problem",how_to:"How to",powered_by_promptperfect:`Powered by PromptPerfect's "Prompt optimization" and "Prompt as a service" feature`,error:"There was a network issue: {message}",find_your_portal:"Find Your Portal",opensource:"Open Source",mmstack:"Multimodal Stack",mmstack_desc:"Our open-source tools help developers build GenAI and search apps faster.",llm:"LLM embedding models",llm_desc:"We provide a collection of high-performance sentence embedding models, boasting between 35 million to 6 billion parameters. They're excellent for enhancing neural search, reranking, sentence similarity, recommendations, etc. Get ready to elevate your AI experience!",parameters:"Parameters",download_pdf:"Download PDF",multimodal_ai:"Multimodal AI",multimodal:"Multimodal",contact_sales:"Contact",join_community:"Community",trusted_by:"TRUSTED BY",newsroom:"Newsroom",read_more:"Read more",for:"For",power_users:"Power Users",researchers:"Researchers",power_users_desc:"Auto prompt engineering for your daily productivity.",developers:"Developers",developers_desc:"Unleash the full power of multimodal AI with cutting-edge cloud-native technologies and open-source infrastructure.",sdk:"SDK",starter_kit:"Starter Kit",sdk_desc:"Want to build high-level AIGC applications using PromptPerfect, SceneXplain, BestBanner, JinaChat, Rationale APIs? We've got you covered! Try our easy-to-use SDK and get started in minutes.",sdk_docs:"Read docs",sdk_example:"Example",build_python:"Build with Python",build_js:"Build with JavaScript",enterprise:"Enterprise",enterprise_desc:"Boost your business with scalable, secure, and bespoke multimodal AI solutions.",embedding_paper_title:"Jina Embeddings: A Novel Set of High-P`erformance Sentence Embedding Models",embedding_paper_desc:"Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB)."},te={sectors:{technology_short:"Tech",media_short:"Media",financeConsulting_short:"Fin & Consult",ecommerceRetail_short:"E-comm & Retail",misc_short:"Others",technology:"Trailblazing Tech Innovators",technology_description:"Leaders in software, cloud, AI, and data rely on Jina AI's neural search solutions to power their LLM-based search, RAG, and AI agent systems. Our advanced reader, embeddings, rerankers, small LMs help them move from POC to enterprise-ready faster than ever.",media:"Captivating Content Creators",media_description:"Media organizations use Jina AI to transform vast multimedia assets into searchable knowledge, streamlining internal research and enriching user experiences. Our specialized Reader and reranking services ensure accurate, context-aware discovery across articles, videos, and archives.",financeConsulting:"Visionary Advisors & Analysts",financeConsulting_description:"Financial firms and consultancies tap into Jina AI’s large-scale data cleaning and domain-specific model training to gain real-time insights. Our enterprise licensing and secure on-prem deployments allow them to maintain confidentiality while benefiting from advanced retrieval and analytics.",ecommerceRetail:"Next-Gen Marketplaces",ecommerceRetail_description:"E-commerce and retail leaders partner with Jina AI to deliver precise product recommendations and in-depth search experiences. Our multilingual embeddings and AI-driven rerankers help optimize discovery, boost conversion, and reduce time-to-insight for global product catalogs.",misc:"Forward-Thinking Pioneers",misc_description:"Organizations spanning education, agriculture, real estate, and beyond leverage Jina AI’s flexible solutions to clean, extract, and transform data at scale. By harnessing our cutting-edge neural retrieval stack, they unlock new possibilities and stay ahead in their respective fields."},media:{video:"Video interview"},show_position:"What's search foundation's position in the ecosystem?",many:"Many",sues_und_sauer:"Süẞ & Sauer",sues_und_sauer_tooltip:"Süß-Sauer, a popular (yet stereotype) flavor in German-Chinese cuisine, means sweet and sour. It’s a metaphor for the highs and lows of startup life.",mit_techreview:"MIT Technology Review",mit_report_title:"Multimodal: AI’s new frontier",sefo:{layer0:"End-user applications",layer1:"RAG / orchestration",layer3:"GPU / mobile / edge / local computing"},title3:"Starts Here",download_logo:"Download logos",download_jina_logo:"Download the Jina AI Logo",download_jina_logo_desc:"Get the Jina AI logo in both light and dark modes, available in PNG and SVG formats. This logo is a registered trademark with the European Union Intellectual Property Office (EUIPO).",download_docarray_logo:"Download the DocArray Logo",download_docarray_logo_desc:"Access the DocArray logo, an open-source project initiated by Jina AI and contributed to the Linux Foundation in December 2022. Available in light and dark modes, in PNG and SVG formats.",brochure_info:"Your Guide to Our Company Awaits",download_brochure1:"Download brochure",employees:"Employees Today",approach:"Our Approach",approach_content1:"In the rapidly evolving world of AI, strategies need to be both nimble and forward-thinking. While our core offering remains centered on enterprises, the AI landscape has shifted in ways that necessitate a rethinking of our approach to customer acquisition. Here's why introducing power users as the entry point of our funnel isn't just innovative, but crucial for our sustained growth in the enterprise sector.",approach_content2:"At Jina AI, our strategy is to be proactive rather than reactive. The inclusion of power users as the funnel's entry point ensures we're not only capturing current market trends but are also strategically poised for future enterprise growth. Our commitment to enterprises remains unwavering; however, our approach to reaching them is innovative, robust, and, above all, forward-thinking.",approach_new_paradigm:"Prompt-based Technology: A New Paradigm",approach_miss_mark:"Why Traditional MLOps Miss the Mark",approach_connect_dots:"Connecting the Dots: Power Users to Enterprises",approach_new_paradigm_description:`2023 heralded a significant change: the rise of prompt-based technology. By simplifying the AI development process, it has democratized access to AI tools. Now, those without extensive programming experience—termed as 'power users'—can engage in AI development without the steep learning curves associated with tools like Pytorch, Docker, or Kubernetes.

Drawing a parallel, this is akin to the evolution of personal computing. Initially, only tech experts operated computers. But with the advent of user-friendly interfaces, a broader audience could participate. Today, with prompt-based technology, we're witnessing a similar democratization in AI.`,approach_miss_mark_description:"While the influx of power users is significant, traditional MLOps tools are ill-equipped to cater to their needs. These tools are reminiscent of using a tractor to navigate city streets—they're heavy and often excessive. The new-gen developers demand agile, intuitive tools that complement their rapid development pace.",approach_connect_dots_description:"So, why is a power user focus essential for our enterprise-centric model? Because it’s about establishing early relationships. By catering to power users now, we're building bridges to the enterprises they'll influence in the future. It’s a strategic play—a long-term investment to ensure our enterprise offering remains top-of-mind when these power users ascend to decision-making roles within organizations.",title:"About Jina AI",title0:"The Future",title1:"Starts",title2:"Here",description:"The future starts here.",subtitle:"Revolutionizing content creation through AI-generated solutions to unlock infinite possibilities. Shaping the future of AI-generated content and enhancing human creativity.",stats_v1:"Our Mission: Search/acc",founded:"Founded",founded_in:"Founded in",empower_developers:"Developers Empowered",technologies:"Technologies",users:"Users Registered",value:"Our Awards",team_size:"These photos include our former colleagues and interns—we appreciate every one of them.",value_content1:"We don't settle. We don't compromise. We push for excellence.",stats_1:"Founded in February 2020, Jina AI has swiftly emerged as a global pioneer in multimodal AI technology. Within an impressive timeframe of 20 months, we have successfully raised $37.5M, marking our strong position in the AI industry. Our ground-breaking technology, open-sourced on GitHub, has empowered over 40,000 developers around the globe to seamlessly build and deploy sophisticated multimodal applications.",stats_2:"In 2023, we've made significant strides in advancing AI generation tools grounded on multimodal technology. This innovation has benefited over 250,000 users worldwide, catering to a plethora of unique business requirements. From facilitating business growth and enhancing operational efficiency to optimizing costs, Jina AI is dedicated to empowering businesses to excel in the multimodal era.",stats_4:'Founded in 2020, Jina AI is a leading search AI company. Our <span class="text-primary text-bold">Search Foundation</span> platform combines Embeddings, Rerankers, and Small Language Models to help businesses build reliable and high-quality GenAI and multimodal search applications.',investors:"Our Investors",vision:"Our Mission",vision_content1:"Inspired by Yann LeCun's insight that '",vision_content3:`The future of AI is <span class="text-primary text-bold">multimodal</span>, and we are part of it. We realize that businesses face challenges in leveraging multimodal data. In response, we're committed to the <span class="text-primary text-bold">Search Foundation</span> to help businesses and developers search better, and utilize multimodal data for business growths.`,yannlecun_quote:"An artificial intelligence system trained on words and sentences alone will never approximate human understanding.",our_answer:"Absolutely, Yann. We're on it, building bridges to a multimodal AI future!",mission:"Our Mission",mission_content1:"We  Our key technologies, including prompt-tuning, prompt-serving, model-tuning, and model-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains.",mission_content2:"At the heart of Jina AI lies our mission to be the portal to multimodal AI for a diverse clientele, from power users and developers to enterprises. We deeply believe in the power of open-source and are dedicated to building advanced, accessible tools for the AI community. Our key technologies, including prompt-tuning, prompt-serving, embedding-tuning, and embedding-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains.",mission_content3:"At Jina AI, our mission is to lead the advancement of multimodal AI through innovative embedding and prompt-based technologies, focusing specifically on areas like natural language processing, image and video analysis, and cross-modal data interaction. This specialization allows us to provide unique solutions that turn complex, multi-source data into actionable insights and groundbreaking applications.",approach_content4:'Everyone wants a better search. At Jina AI, we enable better search by providing the <span class="text-primary text-bold">Search Foundation</span>, which consists of Embeddings, Rerankers, Reader, and Prompt Ops. These components work in concert to revolutionize how we search and understand data.',team:"Inside the Portal of Jina AI",team_content1:"From diverse corners of the globe, we're building the future of AI. Our distinct perspectives enrich our work, sparking innovations. Within this portal, we embrace our individuality, and passionately pursue our dreams. Welcome to the portal of the AI future.",team_join:"Join us",office:"Our Offices",berlin:"Berlin, Germany (HQ)",berlin_address:"Prinzessinnenstraße 19-20, 10969 Berlin, Germany",sunnyvale_address:"710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA",berlin_address2:"Geschäftsanschrift: Leipzigerstr. 96, 10117 Berlin, Germany",bj:"Beijing, China",bj_address:"Level 5, Building 6, No.48 Haidian West St. Beijing, China",sz:"Shenzhen, China",sz_address:"402 Floor 4, Fu'an Technology Building, Shenzhen, China",awards:"Awards & Recognition",fastApiCaption:"Contributed over $20,000 since 2021.",linuxFoundationCaption:"Makes an annual contribution of $10,000 starting from 2022.",pythonSoftwareFoundationCaption:"Provided a one-time donation of $10,000 and sponsored multiple PyCon events including those in Germany, Italy, China, and the US.",numfocusCaption:"Regularly donates each month starting from 2022.",segmentFaultCaption:"Contributed a one-time donation of $6,000.",otherProjectsCaption:"Donated over $3,000 via Github Sponsorship.",understand_our_strength:"Understand Our Strength",understand_our_view2:"Understand the Search Foundation"},ne={title:"Intern Program",description:"Worldwide call for students: Internship in research, engineering, product management.",subtitle:"Our full-time internship programme provides hands-on work experience through well-designed internship projects in a wide range of scope.",subtitle1:"Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together.",alumni:"ALUMNI",about_internship_program:"About Internship Program",about_internship_program_desc1:"We are excited to offer this unique opportunity for talented individuals to join our dynamic team and contribute to groundbreaking projects in the field of Artificial Intelligence. This internship is designed to provide you with valuable hands-on experience, mentorship, and exposure to cutting-edge technologies that are shaping the future of AI.",about_internship_program_desc2:"At Jina AI, we understand the significance of nurturing and harnessing young talent. We recognize that interns bring fresh perspectives, enthusiasm, and creativity to the table, invigorating our team with new ideas and approaches. By providing internships, we aim to foster the growth of future leaders in the AI industry while offering them real-world experience in a supportive and challenging environment.",who_do_we_look_for:"Who do we look for?",who_do_we_look_for_desc:"We value diversity and encourage applicants from diverse profiles and backgrounds to join our Internship Program. The internship opportunities are offered in multiple departments, including Engineering, Design, Product Management, Sales and Account Management, Marketing and Community Management.",enthusiastic:"ENTHUSIASTIC",self_motivated:"SELF-MOTIVATED",innovative:"INNOVATIVE",explore_stories_from_our_interns:"Discover our interns' stories",explore_stories_from_our_interns1:"Get inspired by our interns' journeys",software_engineer_intern:"Software Engineer Intern",researcher_intern:"Researcher Intern",recruiting_and_administrative_intern:"Recruiting and Administrative Intern",dev_rel_intern:"Developer Relations Intern",alumni_network:"Our intern alumni",intern_work1:"Fine-tuned LLM models for better embeddings",intern_work2:"Explored the potential of Retrieval Augmented Generation",intern_work3:"Published a paper on the topic of sentence embeddings",intern_work4:"Injecting continuous youthful vitality into the team",intern_work5:"Benchmarked quantization techniques to compress LLM",intern_work6:"Creating and promoting compelling campaign for PromptPerfect",intern_work7:"Rapidly developed and improved JinaColBERT V2",application:"Application",submit_application:"Apply an internship",application_desc:" The minimum duration is three months, with the goal of publishing a paper or work on a search product used by thousands of developers. The starting date is as soon as possible. We welcome PhD, Master's, and Bachelor's students!",summer:"Summer",autumn:"Autumn",winter:"Winter",spring:"Spring",apply:"Apply now"},ie={request_audit:"Request audit report",download_type1:"Download SOC 2 Type 1 Attestation",download_type2:"Download SOC 2 Type 2 Attestation",title:"Legal Information",description:"Legal information, terms of service, privacy policy, and other important documents about Jina AI's products and services."},ae={description:"World-class multimodal multilingual embeddings."},oe={preprocessing:"Preprocessing",representation:"Representation",contextualization:"Contextualization",personalization:"Personalization",grounding:"Grounding",preprocessing_desc:"Preprocessing involves cleaning, normalizing, and transforming raw data into a format that is digestible by the search system.",representation_desc:"Embeddings transform multimodal data into a uniform, vectorized format. This enables the search system to understand and categorize content beyond simple keywords.",contextualization_desc:"Rerankers adjust initial search results based on deep contextual relevance wrt. query. This refines the ranking to better match what users are likely to find useful.",grounding_desc:"Reader refining inputs and results through LLMs. They improve the quality, readability and factuality of the final answer.",personalization_desc:"Using synthetic data guided by the user instruction to automatically train a domain-specific embedding and reranker model.",click_to_learn_more:"Click to learn more",embeddings:"Embeddings",embeddings_desc:"Embeddings are the cornerstones of modern search system, representing multimodal data into vectors of numbers. This process enables a more nuanced and contextual understanding of content, far beyond simple keyword matching.",rerankers:"Reranker",rerankers_desc:"Rerankers take the initial results from the embeddings and refine them, ensuring that the most relevant results are presented to the user. This is crucial for delivering high-quality search results that meet the user's intent.",promptOps:"PromptOps",promptOps_desc:"Prompt Ops improve the input and output of the search system, including those used in queries expansion, LLM-input and results rewriting. This ensures that the the search understands better and results better.",coreInfra:"Core Infra",coreInfra_desc:"Core infra provides a cloud-native layer for developing, deploying and orchestration search foundation models both in the public cloud and on-premises, enabling services to scale up and down effortlessly.",prompt_tech:"Prompt & agent engineering",embedding_tech:"Embeddings",for_power_users:"For Power Users",for_developers:"For Developers",for_enterprise:"For Enterprises",prompt_serving:"Prompt Serving",prompt_tuning:"Prompt Tuning",model_serving:"Model Serving",model_tuning:"Model Tuning",embedding_serving:"Embedding Serving",embedding_tuning:"Embedding Tuning",prompt_tech_description:`At Jina AI, we recognize prompt engineering as vital for interacting with large language models (LLMs). As these models advance, the complexity of prompts escalates, encompassing intricate reasoning and logic. This advancement underscores the intertwined growth of LLMs and prompt sophistication.

We foresee a future where LLMs act as compilers, with prompts becoming the new programming language. This shift suggests that future technological proficiency may focus more on prompt mastery than traditional coding. Our commitment at Jina AI is to lead in this transformative area, making advanced AI accessible and practical for everyday use by mastering this emerging 'language'.`,embedding_tech_description:`At Jina AI, we harness the power of embedding technology to revolutionize diverse AI applications. This technology serves as a unified method to efficiently represent and compress various data types, ensuring no loss of critical information. Our focus is on transforming complex datasets into a universally understandable embedding format, which is essential for precise and insightful AI analysis.

Embeddings are fundamental, especially in applications like precise image and voice recognition, where they help discern fine-grained details and nuances. In natural language processing, embeddings enhance understanding of context and sentiment, leading to more accurate conversational AI and language translation tools. They are also crucial in developing sophisticated recommendation systems that require a deep understanding of user preferences across different content forms, such as text, audio, and video.`,prompt_serving_description:"Wrapping and serving prompts through an API, without hosting heavy models. The API calls a public large language model service and handles the orchestration of inputs and outputs in a chain of operations.",prompt_tuning_description:"The process of crafting and refining the input prompts in order to guide its output towards specific, desired responses.",embedding_serving_description:"Delivering embeddings through a robust, scalable microservice using cloud-native technologies.",embedding_tuning_description:"Optimizing high-quality embeddings by integrating domain expertise for enhanced task-specific performance.",model_serving_description:"The deployment of fine-tuned models in a production environment, usually requiring substantial resources such as GPU hosting. MLOps, emphasizing the serving of mid-size to large models in a scalable, efficient, and reliable manner.",model_tuning_description:"Also known as fine-tuning, involves adjusting the parameters of a pre-trained model on a new, often task-specific dataset to improve its performance and adapt it to a specific application."},re={sentence_similarity:"Sentence embedding",updated_about:"Updated about"},se={project1:"Enable high-accuracy search within 3D mesh data using point cloud information.",project2:"Design a content-based search engine for short animation films.",project3:"Enhance e-commerce conversion rates by fine-tuning embedding models.",project4:"Execute prompt tuning to boost efficiency for a business consulting company.",project5:"Pioneer game scene understanding and automatic annotation for a leading gaming enterprise.",project6:"Implement real-time input expansion for a chatbot company, enhancing user experience.",project7:"Revolutionize legal tech by enabling efficient search within lengthy legal documents.",project8:"Support a high-throughput generative art service for large-scale operations.",project9:"Carry out process mining and modeling using advanced language models.",project10:"Leverage computer vision to improve digital accessibility of government websites.",project11:"Fine-tune LLM for a consulting firm to optimize finance data analysis.",project12:"Advance marketing strategies by fine-tuning text-to-image models for style transferring."},le={observability:"Observability",graduated:"Graduated",incubating:"Incubating",sandbox:"Sandbox",archived:"Archived",kubernetes:"Kubernetes",cloud_native:"Cloud Native",prompt_tuning:"Prompt Tuning",model_serving:"Model Serving",model_tuning:"Model Tuning",embedding_serving:"Embedding Serving",embedding_tuning:"Embedding Tuning",prompt_serving:"Prompt Serving",core:"Core",small_size_model:"Small Size Model",large_size_model:"Large Size Model",mid_size_model:"Mid Size Model",orchestration:"Orchestration",linux_foundation:"Linux Foundation",vector_database:"Vector Database",data_structure:"Data Structure",vector_store:"Vector Store",llm1:"LLMOps",rag1:"RAG"},de={pricing:"Pricing?",drop_area_for_image:"Drop your images here",image_upload:"Attach images",image_validate:"You can attach up to {_num} images. Only JPG, JPEG, PNG, WEBP.",sending_feedback:"Sending...",feedback_sent:"Submitted! We will get back to you shortly.",priority:"Priority support for paid users",shortcut:"Shortcut",self_check:"Self check",turn_on_volume:"Turn up the volume",get_api_key:"How to get my API key?",rate_limit:"What's the rate limit?",nc_check:"Do I need a commercial license?",other_questions:"Other questions",title:"Contact sales",description:"Grow your business with Jina AI.",impact_snapshots:"Impact Snapshots",subtitle:"Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise.",subtitle1:"Jina AI, a leader in multimodal AI, excels in embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. Leveraging cloud-native technologies like Kubernetes and serverless architectures, we deliver robust, scalable, and production-ready solutions. With expertise in large language models, text, image, video, audio understanding, neural search, and generative AI, we provide innovative, future-proof strategies to elevate your business.",subtitle2:"Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise.",trusted_by:"Trusted by",name:"Name",work_email:"Work email",country:"Country",company:"Organization",company_size:"Organization size",company_website:"Organization website",cc_by_nc:"Request commercial use of CC BY-NC models",cc_by_nc_description:"Our latest models are CC BY-NC licensed. For commercial use, access via our API, Azure Marketplace, or AWS SageMaker - no permission needed. <b>Check this box only for on-prem commercial use outside these channels.</b>",company_website_placeholder:"URL for your company's homepage or LinkedIn profile",invalid_url:"URL is invalid",invalid_email:"Email is invalid",preferred_products:"Which products are you interested in?",preferred_models:"Which models are you interested in?",department:"Department",role:"Job role",field_required:"Field is required",invalid_date_format:"Invalid date format. Please use DD-MM-YYYY format.",invalid_number:"Invalid number. Please input again",anything_else:"Tell us about your problem, idea or drop some screenshots.",agreement:"By submitting, you confirm that you agree to the processing of your personal data by Jina AI as described in the",private_statement:"Privacy Statement",submit:"Submit",submit_success:"Thank you for your submission. We will get back to you shortly.",submit_failed:"Submission failed. Please try again later.",faq:"FAQ"},ce={featured:"Featured","tech-blog":"Tech blog",all:"All",latest:"Latest",press:"Press release",events:"Event",insights:"Opinion","knowledge-base":"Knowledge base",releases:"Software update"},ue={api_paid_or_free:"Are you using a paid API key or free trial key?",csp_user:"Are you using our official model images on AWS and Azure?",api_paid:"Paid API key",api_free_trial:"Free API key",trial_key_restrictions:"Free trial key can be only used for non-commercial purposes. Please purchase a paid package for commercial use.",title:"CC BY-NC License Self-Check",using_api_or_cloud:"Are you using our official API or official images on Azure or AWS?",yes:"Yes",no:"No",yes1:"Yes",no1:"No",yes2:"Yes",no2:"No",yes3:"Yes",no3:"No",no_restrictions:"No restrictions. Use as per your current agreement.",using_cc_by_nc_models:"Are you using these models?",no_restrictions_apply:"No restrictions apply.",is_use_commercial:"Is your use commercial?",not_sure:"Not sure",are_you:"Are you:",personal_hobby_projects:"Using it for personal or hobby projects?",non_commercial_free_use:"This is non-commercial. You can use the models freely.",for_profit_internal_use:"A for-profit company using it internally?",commercial_contact_sales:"This is commercial. Contact our sales team.",educational_teaching:"An educational institution using it for teaching?",typically_non_commercial_free_use:"This is typically non-commercial. You can use the models freely.",non_profit_ngo_mission:"A non-profit or NGO using it for your mission?",typically_non_commercial_check:"This is typically non-commercial, but check with us if unsure.",product_service_sale:"Using it in a product or service you sell?",government_public_services:"A government entity using it for public services?",may_be_commercial_contact:"This may be commercial. Please contact us for clarification.",free_use:"You can use the models freely.",contact_sales_for_licensing:"Contact our sales team for licensing."},me={metadata:{input_type:"Input",output_type:"Output",license:"License",license_link:"Commercial License","reader-api_link":"Jina API",api_link:"Jina API",aws_link:"AWS SageMaker",azure_link:"Microsoft Azure",gcp_link:"Google Cloud",huggingface_link:"Hugging Face",arxiv:"ArXiv Paper",release_blog:"Release Post",deprecated_by:"Deprecated by",related_models:"Related Models",release_date:"Release Date"},input_type:{text:"Text",ranking:"Rankings","text (json)":"Text (JSON)","text (code)":"Text (Code)","text (html)":"Text (HTML)","text (query)":"Text (Query)","text (document)":"Text (Document)","text (markdown)":"Text (Markdown)",vector:"Vector","multi-vector":"Multi-Vector",image:"Image",audio:"Audio",video:"Video",code:"Code",document:"Document","3d":"3D",tabular:"Tabular",timeseries:"Time Series",graph:"Graph",other:"Other"},comparison:{select_models:"Choose models to compare",btn:"Compare"},sections:{model_io_graph:"I/O graph {_number}",model_name:"Name",model_comparison:"Model Comparison",blogs:"Blogs that mention this model",publications:"Publications",image_size:"Input Image Size",parameter_size:"Parameters",token_length:"Input Token Length",output_dimension:"Output Dimension",model_details:"Model Details",language:"Language Support",performance_metrics:"Performance Metrics",usage_requirements:"Usage & Requirements",external_links:"External Links & Resources",availability:"Availability",using_model:"Available via",tags:"Tags",overview:{title:"Overview","jina-clip-v1":"Jina CLIP v1 revolutionizes multimodal AI by being the first model to excel equally in both text-to-text and text-to-image retrieval tasks. Unlike traditional CLIP models that struggle with text-only scenarios, this model achieves state-of-the-art performance across all retrieval combinations while maintaining a remarkably compact 223M parameter size. The model addresses a critical industry challenge by eliminating the need for separate models for text and image processing, reducing system complexity and computational overhead. For teams building search systems, recommendation engines, or content analysis tools, Jina CLIP v1 offers a single, efficient solution that handles both text and visual content with exceptional accuracy.","jina-clip-v2":"Jina CLIP v2 revolutionizes multimodal AI by bridging the gap between visual and textual understanding across 89 languages. This model solves critical challenges in global e-commerce, content management, and cross-cultural communication by enabling accurate image-text matching regardless of language barriers. For businesses expanding internationally or managing multilingual content, it eliminates the need for separate models per language or complex translation pipelines. The model particularly shines in scenarios requiring precise visual search across language boundaries, such as global marketplace product discovery or multilingual digital asset management.","jina-colbert-v1-en":"Jina-ColBERT-v1-en revolutionizes text search by solving a critical challenge in information retrieval: achieving high accuracy without sacrificing computational efficiency. Unlike traditional models that compress entire documents into single vectors, this model maintains precise token-level understanding while requiring only 137M parameters. For teams building search applications, recommendation systems, or content discovery platforms, Jina-ColBERT-v1-en eliminates the traditional trade-off between search quality and system performance. The model particularly shines in scenarios where nuanced text understanding is crucial, such as technical documentation search, academic paper retrieval, or any application where capturing subtle semantic relationships can make the difference between finding the right information and missing critical content.","jina-colbert-v2":"Jina-ColBERT-v2 is a groundbreaking multilingual information retrieval model that solves the critical challenge of efficient, high-quality search across multiple languages. As the first multilingual ColBERT-like model to generate compact embeddings, it addresses the growing need for scalable, cost-effective multilingual search solutions in global applications. Organizations dealing with multilingual content, from e-commerce platforms to content management systems, can leverage this model to provide accurate search results across 89 languages while significantly reducing storage and computational costs through its innovative dimension reduction capabilities.","jina-embedding-b-en-v1":"Jina Embedding B v1 is a specialized text embedding model designed to transform English text into high-dimensional numerical representations while maintaining semantic meaning. The model addresses the critical need for efficient and accurate text embeddings in production environments, particularly valuable for organizations requiring a balance between computational efficiency and embedding quality. With its 110M parameters generating 768-dimensional embeddings, it serves as a practical solution for teams implementing semantic search, document clustering, or content recommendation systems without requiring extensive computational resources.","jina-embeddings-v2-base-code":"Jina Embeddings v2 Base Code tackles a critical challenge in modern software development: efficiently navigating and understanding large codebases. For development teams struggling with code discovery and documentation, this model transforms how developers interact with code by enabling natural language search across 30 programming languages. Unlike traditional code search tools that rely on exact pattern matching, this model understands the semantic meaning behind code, allowing developers to find relevant code snippets using plain English descriptions. This capability is particularly valuable for teams maintaining large legacy codebases, developers onboarding to new projects, or organizations looking to improve code reuse and documentation practices.","jina-embeddings-v2-base-de":"Jina Embeddings v2 Base German addresses a critical challenge in international business: bridging the language gap between German and English markets. For German companies expanding into English-speaking territories, where a third of businesses generate over 20% of their global sales, accurate bilingual understanding is essential. This model transforms how organizations handle cross-language content by enabling seamless text understanding and retrieval in both German and English, making it invaluable for companies implementing international documentation systems, customer support platforms, or content management solutions. Unlike traditional translation-based approaches, this model directly maps equivalent meanings in both languages to the same embedding space, enabling more accurate and efficient bilingual operations.","jina-embeddings-v2-base-en":"Jina Embeddings v2 Base English is a groundbreaking open-source text embedding model that solves the critical challenge of processing long documents while maintaining high accuracy. Organizations struggling with analyzing extensive legal documents, research papers, or financial reports will find this model particularly valuable. It stands out by handling documents up to 8,192 tokens in length—16 times longer than traditional models—while matching the performance of OpenAI's proprietary solutions. With a compact size of 0.27GB and efficient resource usage, it offers an accessible solution for teams seeking to implement advanced document analysis without excessive computational overhead.","jina-embeddings-v2-base-es":"Jina Embeddings v2 Base Spanish is a groundbreaking bilingual text embedding model that addresses the critical challenge of cross-lingual information retrieval and analysis between Spanish and English content. Unlike traditional multilingual models that often show bias towards specific languages, this model delivers truly balanced performance across both Spanish and English, making it indispensable for organizations operating in Spanish-speaking markets or handling bilingual content. The model's most remarkable feature is its ability to generate geometrically aligned embeddings - when texts in Spanish and English express the same meaning, their vector representations naturally cluster together in the embedding space, enabling seamless cross-language search and analysis.","jina-embeddings-v2-base-zh":"Jina Embeddings v2 Base Chinese breaks new ground as the first open-source model to seamlessly handle both Chinese and English text with an unprecedented 8,192 token context length. This bilingual powerhouse addresses a critical challenge in global business: the need for accurate, long-form document processing across Chinese and English content. Unlike traditional models that struggle with cross-lingual understanding or require separate models for each language, this model maps equivalent meanings in both languages to the same embedding space, making it invaluable for organizations expanding globally or managing multilingual content.","jina-embeddings-v3":"Jina Embeddings v3 is a groundbreaking multilingual text embedding model that transforms how organizations handle text understanding and retrieval across languages. At its core, it solves the critical challenge of maintaining high performance across multiple languages and tasks while keeping computational requirements manageable. The model particularly shines in production environments where efficiency matters - it achieves state-of-the-art performance with just 570M parameters, making it accessible for teams that can't afford the computational overhead of larger models. Organizations needing to build scalable, multilingual search systems or analyze content across language barriers will find this model especially valuable.","jina-reranker-v1-base-en":"Jina Reranker v1 Base English revolutionizes search result refinement by addressing a critical limitation in traditional vector search systems: the inability to capture nuanced relationships between queries and documents. While vector search with cosine similarity provides fast initial results, it often misses subtle relevance signals that human users intuitively understand. This reranker bridges that gap by performing sophisticated token-level analysis of both queries and documents, delivering a remarkable 20% improvement in search accuracy. For organizations struggling with search precision or implementing RAG systems, this model offers a powerful solution that significantly improves result quality without requiring a complete overhaul of existing search infrastructure.","jina-reranker-v1-tiny-en":"Jina Reranker v1 Tiny English represents a breakthrough in efficient search refinement, designed specifically for organizations requiring high-performance reranking in resource-constrained environments. This model addresses the critical challenge of maintaining search quality while significantly reducing computational overhead and deployment costs. With just 33M parameters - a fraction of typical reranker sizes - it delivers remarkably competitive performance through innovative knowledge distillation techniques. The model's most surprising feature is its ability to process documents nearly five times faster than base models while maintaining over 92% of their accuracy, making enterprise-grade search refinement accessible to applications where computational resources are at a premium.","jina-reranker-v1-turbo-en":"Jina Reranker v1 Turbo English addresses a critical challenge in production search systems: the trade-off between result quality and computational efficiency. While traditional rerankers offer improved search accuracy, their computational demands often make them impractical for real-time applications. This model breaks that barrier by delivering 95% of the base model's accuracy while processing documents three times faster and using 75% less memory. For organizations struggling with search latency or computational costs, this model offers a compelling solution that maintains high-quality search refinement while significantly reducing infrastructure requirements and operational costs.","jina-reranker-v2-base-multilingual":"Jina Reranker v2 Base Multilingual is a cross-encoder model designed to enhance search accuracy across language barriers and data types. This reranker addresses the critical challenge of precise information retrieval in multilingual environments, particularly valuable for global enterprises needing to refine search results across different languages and content types. With support for over 100 languages and unique capabilities in function calling and code search, it serves as a unified solution for teams requiring accurate search refinement across international content, API documentation, and multilingual codebases. The model's compact 278M parameter design makes it particularly appealing for organizations seeking to balance high performance with resource efficiency.","reader-lm-05b":"Reader LM 0.5B is a specialized language model designed to solve the complex challenge of converting HTML documents into clean, structured markdown text. This model addresses a critical need in modern data processing pipelines: efficiently transforming messy web content into a format that's ideal for LLMs and documentation systems. Unlike general-purpose language models that require massive computational resources, Reader LM 0.5B achieves professional-grade HTML processing with just 494M parameters, making it accessible to teams with limited computing resources. Organizations dealing with web content processing, documentation automation, or building LLM-powered applications will find this model particularly valuable for streamlining their content preparation workflows.","reader-lm-15b":"Reader LM 1.5B represents a breakthrough in efficient document processing, addressing the critical challenge of converting complex web content into clean, structured formats. This specialized language model tackles a fundamental problem in modern AI pipelines: the need to efficiently process and clean HTML content for downstream tasks without relying on brittle rule-based systems or resource-intensive large language models. What makes this model truly remarkable is its ability to outperform models 50 times its size while maintaining a surprisingly compact 1.54B parameter footprint. Organizations dealing with large-scale web content processing, documentation automation, or content management systems will find this model particularly valuable for its ability to handle extremely long documents while delivering superior accuracy in HTML-to-markdown conversion.","ReaderLM-v2":"ReaderLM-v2 is a 1.5B parameter language model that converts raw HTML into markdown or JSON, handling up to 512K tokens combined input/output length with support for 29 languages. Unlike its predecessor that treated HTML-to-markdown as a 'selective-copy' task, v2 approaches it as a translation process, enabling superior handling of complex elements like code fences, nested lists, tables, and LaTeX equations. The model maintains consistent performance across varying context lengths and introduces direct HTML-to-JSON generation capabilities with predefined schemas."},methods:{title:"Methods","ReaderLM-v2":"Built on Qwen2.5-1.5B-Instruction, ReaderLM-v2's training involved a html-markdown-1m dataset of one million HTML documents, averaging 56,000 tokens each. The training process included: 1) long-context pretraining using ring-zag attention and RoPE to expand context from 32K to 256K tokens, 2) supervised fine-tuning with refined datasets, 3) direct preference optimization for output alignment, and 4) self-play reinforcement tuning. Data preparation followed a three-step pipeline (Draft-Refine-Critique) powered by Qwen2.5-32B-Instruction, with specialized models trained for specific tasks before merging via linear parameter interpolation.","jina-clip-v1":"The model's architecture represents a significant innovation in multimodal AI design, combining an adapted Jina BERT v2 text encoder with the cutting-edge EVA-02 image encoder from the Beijing Academy for Artificial Intelligence. The text encoder supports sequences up to 12,288 tokens - over 100 times longer than the original CLIP's 77-token limit - while the image encoder efficiently processes 16 patch tokens. The training process follows a novel three-step approach: first, aligning image-caption pairs while maintaining text understanding through interleaved text-pair training; second, incorporating AI-generated longer text descriptions of images; and finally, using hard negative text triplets to enhance semantic distinction capabilities. This unique training methodology enables the model to maintain high performance across both short captions and detailed textual descriptions while preserving strong visual understanding.","jina-clip-v2":"At its core, Jina CLIP v2 employs a sophisticated dual-encoder architecture that combines a Jina XLM-RoBERTa text encoder (561M parameters) with an EVA02-L14 vision encoder (304M parameters). The text encoder processes content in 89 languages with a massive context window of 696,320 tokens, while the vision encoder handles high-resolution images up to 512x512 pixels. The model introduces innovative Matryoshka representation learning, which enables dynamic embedding dimension adjustment from 1024 down to 64 dimensions while preserving performance. This architecture processes both text and images through their respective encoders, projecting them into a shared semantic space where similar concepts align regardless of their original modality or language.","jina-colbert-v1-en":"The model employs an innovative late interaction architecture that fundamentally changes how document retrieval works. Instead of comparing entire documents at once, it processes queries and documents independently until the final matching stage, using an adapted version of the ColBERT approach. The architecture combines two key components: a document encoder that processes text up to 8,192 tokens (over 16 times longer than standard transformers) and a query encoder that creates precise token-level representations. Each token in both query and document gets its own 128-dimensional embedding vector, preserving fine-grained semantic information that would be lost in single-vector models. The late interaction mechanism then enables efficient token-by-token matching between queries and documents, using max-pooling and summation operations to compute final relevance scores without requiring expensive all-to-all comparisons.","jina-colbert-v2":"The model builds upon the ColBERT architecture, introducing a sophisticated late interaction mechanism that fundamentally changes how queries and documents are matched. At its core, it uses a modified XLM-RoBERTa backbone with 560M parameters, enhanced by rotary position embeddings and optimized with flash attention. The training process involves two key stages: initial pretraining with diverse weakly-supervised data from various languages, followed by fine-tuning with labeled triplet data and supervised distillation. What makes this approach unique is the implementation of Matryoshka representation learning, which enables the model to produce embeddings in multiple dimensions (128, 96, or 64) from a single training process, allowing for dynamic storage optimization without retraining.","jina-embedding-b-en-v1":"The model employs a T5 encoder-based architecture enhanced with mean pooling to generate fixed-length representations. Trained on the carefully curated Linnaeus-Clean dataset, which contains 385 million high-quality sentence pairs filtered down from an initial 1.6 billion pairs, the model underwent a two-phase training process. The first phase utilized contrastive learning with InfoNCE loss on text pairs, while the second phase incorporated triplet training to refine the model's ability to distinguish between similar and dissimilar content. This innovative training approach, combined with rigorous data filtering including language detection and consistency checking, enables the model to capture nuanced semantic relationships effectively.","jina-embeddings-v2-base-code":"The model achieves its impressive performance through a specialized architecture designed specifically for code understanding. At its core, it uses a transformer-based neural network with 161 million parameters, trained on diverse programming language datasets with emphasis on six major languages: Python, JavaScript, Java, PHP, Go, and Ruby. What makes this architecture unique is its extended context window of 8,192 tokens, allowing it to process entire functions or multiple files at once while maintaining semantic understanding. The model generates dense 768-dimensional embeddings that capture both the syntactic structure and semantic meaning of code, enabling it to understand relationships between different code segments even when they use different programming patterns or syntax to achieve the same goal.","jina-embeddings-v2-base-de":"The model achieves its impressive bilingual capabilities through an innovative architecture that processes both German and English text within a unified 768-dimensional embedding space. At its core, it employs a transformer-based neural network with 161 million parameters, carefully trained to understand semantic relationships across both languages. What makes this architecture particularly effective is its bias minimization approach, specifically designed to avoid the common pitfall of favoring English grammatical structures - a problem identified in recent research with multilingual models. The model's extended context window of 8,192 tokens allows it to process entire documents or multiple pages of text in a single pass, maintaining semantic coherence across long-form content in both languages.","jina-embeddings-v2-base-en":"The model's architecture combines a BERT Small backbone with an innovative symmetric bidirectional ALiBi (Attention with Linear Biases) mechanism, eliminating the need for traditional positional embeddings. This architectural choice enables the model to extrapolate far beyond its training length of 512 tokens, handling sequences up to 8,192 tokens without performance degradation. The training process involved two key phases: initial pretraining on the C4 dataset, followed by refinement on Jina AI's curated collection of over 40 specialized datasets. This diverse training data, including challenging negative examples and varied sentence pairs, ensures robust performance across different domains and use cases. The model produces 768-dimensional dense vectors that capture nuanced semantic relationships, achieved with a relatively modest 137M parameters.","jina-embeddings-v2-base-es":"At the heart of this model lies an innovative architecture based on symmetric bidirectional ALiBi (Attention with Linear Biases), a sophisticated approach that enables processing of sequences up to 8,192 tokens without traditional positional embeddings. The model utilizes a modified BERT architecture with 161M parameters, incorporating Gated Linear Units (GLU) and specialized layer normalization techniques. Training follows a three-stage process: initial pre-training on a massive text corpus, followed by fine-tuning with carefully curated text pairs, and finally, hard-negative training to enhance discrimination between similar but semantically distinct content. This approach, combined with 768-dimensional embeddings, allows the model to capture nuanced semantic relationships while maintaining computational efficiency.","jina-embeddings-v2-base-zh":"The model's architecture combines a BERT-based backbone with symmetric bidirectional ALiBi (Attention with Linear Biases), enabling efficient processing of long sequences without the traditional 512-token limitation. The training process follows a carefully orchestrated three-phase approach: initial pre-training on high-quality bilingual data, followed by primary and secondary fine-tuning stages. This methodical training strategy, coupled with the model's 161M parameters and 768-dimensional output, achieves remarkable efficiency while maintaining balanced performance across both languages. The symmetric bidirectional ALiBi mechanism represents a significant innovation, allowing the model to handle documents up to 8,192 tokens in length—a capability previously limited to proprietary solutions.","jina-embeddings-v3":"The model's architecture represents a significant innovation in embedding technology, built on a foundation of jina-XLM-RoBERTa with 24 layers and enhanced with task-specific Low-Rank Adaptation (LoRA) adapters. LoRA adapters are specialized neural network components that optimize the model for different tasks like retrieval, classification, or clustering without increasing the parameter count significantly - they add less than 3% to the total parameters. The model incorporates Matryoshka Representation Learning (MRL), allowing embeddings to be flexibly reduced from 1024 to as low as 32 dimensions while preserving performance. Training involved a three-stage process: initial pre-training on multilingual text from 89 languages, fine-tuning on paired texts for embedding quality, and specialized adapter training for task optimization. The model supports context lengths up to 8,192 tokens through Rotary Position Embeddings (RoPE), with an innovative base frequency adjustment technique that improves performance on both short and long texts.","jina-reranker-v1-base-en":"The model employs a BERT-based cross-attention architecture that fundamentally differs from traditional embedding-based approaches. Instead of comparing pre-computed document embeddings, it performs dynamic token-level interactions between queries and documents, enabling it to capture contextual nuances that simple similarity metrics miss. The architecture's 137M parameters are carefully structured to enable deep semantic understanding while maintaining computational efficiency. A standout innovation is its ability to handle sequences up to 262,144 tokens—far beyond typical model limitations—achieved through sophisticated optimization techniques that maintain fast inference speeds despite the increased context window.","jina-reranker-v1-tiny-en":"The model employs a streamlined four-layer architecture based on JinaBERT with symmetric bidirectional ALiBi (Attention with Linear Biases), enabling efficient processing of long sequences. Its development leverages an advanced knowledge distillation approach where a larger, high-performance teacher model (jina-reranker-v1-base-en) guides the training process, allowing the smaller model to learn optimal ranking behaviors without requiring extensive real-world training data. This innovative training methodology, combined with architectural optimizations like reduced hidden layers and efficient attention mechanisms, enables the model to maintain high-quality rankings while significantly reducing computational requirements. The result is a model that achieves remarkable efficiency without compromising its ability to understand complex document relationships.","jina-reranker-v1-turbo-en":"The model achieves its efficiency through an innovative six-layer architecture that compresses the sophisticated reranking capabilities of its larger counterpart into just 37.8 million parameters—a dramatic reduction from the base model's 137 million. This streamlined design employs knowledge distillation, where the larger base model acts as a teacher, training the turbo variant to match its behavior while using fewer resources. The architecture maintains the core BERT-based cross-attention mechanism for token-level interactions between queries and documents, but optimizes it for speed through reduced layer count and efficient parameter allocation. The model supports sequences up to 8,192 tokens, enabling comprehensive document analysis while maintaining fast inference speeds through sophisticated optimization techniques.","jina-reranker-v2-base-multilingual":"The model employs a cross-encoder architecture enhanced with Flash Attention 2, enabling direct comparison between queries and documents for more accurate relevance assessment. Trained through a four-stage process, the model first establishes English language capabilities, then progressively incorporates cross-lingual and multilingual data, before final refinement with hard-negative examples. This innovative training approach, combined with the Flash Attention 2 implementation, allows the model to process sequences up to 524,288 tokens while maintaining exceptional speed. The architecture's efficiency enables it to handle complex reranking tasks across multiple languages with 6x higher throughput compared to its predecessor, while ensuring accurate relevance assessment through direct query-document interaction.","reader-lm-05b":'The model employs an innovative "shallow-but-wide" architecture specifically optimized for selective-copy operations rather than creative text generation. Built on a decoder-only foundation with 24 layers and 896 hidden dimensions, the model uses specialized attention mechanisms with 14 query heads and 2 key-value heads to efficiently process input sequences. The training process involved two distinct stages: first with shorter, simpler HTML (32K tokens) to learn basic conversion patterns, then with complex, real-world HTML (128K tokens) to handle challenging cases. The model incorporates contrastive search during training and implements a repetition detection mechanism to prevent degeneration issues like token loops. A unique aspect of its architecture is the zigzag-ring-attention mechanism, which enables the model to handle extremely long sequences up to 256K tokens while maintaining stable performance.',"reader-lm-15b":'The model employs an innovative "shallow-but-wide" architecture that challenges traditional scaling approaches in language model design. At its core are 28 transformer layers configured with 12 query heads and 2 key-value heads, creating a unique balance that optimizes for selective-copy operations while maintaining deep semantic understanding. The architecture features a hidden size of 1536 and an intermediate size of 8960, carefully tuned to handle sequences up to 256K tokens. The training process involved two distinct stages: first focusing on short-and-simple HTML with 32K token sequences, then advancing to long-and-hard HTML with 128K tokens, implementing zigzag-ring-attention for efficient processing. This approach, combined with contrastive search and specialized repetition detection mechanisms, enables the model to avoid common issues like degeneration and dull loops that typically plague smaller language models handling complex document processing tasks.'},performance:{title:"Performance","ReaderLM-v2":"In comprehensive benchmarks, ReaderLM-v2 outperforms larger models like Qwen2.5-32B-Instruct and Gemini2-flash-expr on HTML-to-Markdown tasks. For main content extraction, it achieves ROUGE-L of 0.84, Jaro-Winkler of 0.82, and significantly lower Levenshtein distance (0.22) compared to competitors. In HTML-to-JSON tasks, it maintains competitive performance with F1 scores of 0.81 and 98% pass rate. The model processes at 67 tokens/s input and 36 tokens/s output on a T4 GPU, with significantly reduced degeneration issues through contrastive loss training.","jina-clip-v1":"Jina CLIP v1 demonstrates remarkable improvements over OpenAI's original CLIP across all benchmarks. In text-only retrieval, it achieves a 165% performance increase with a score of 0.429 compared to CLIP's 0.162. For image-related tasks, it shows consistent improvements: 2% better in text-to-image retrieval (0.899), 6% in image-to-text retrieval (0.803), and 12% in image-to-image retrieval (0.916). The model particularly shines in zero-shot visual classification tasks, successfully categorizing images without prior training on specific domains. When evaluated on standard benchmarks like MTEB for text retrieval, CIFAR-100 for image tasks, and Flickr8k/30k and MSCOCO Captions for cross-modal performance, it consistently outperforms specialized single-modality models while maintaining competitive performance in cross-modal tasks.","jina-clip-v2":"The model achieves state-of-the-art performance with 98.0% accuracy on Flickr30k image-to-text retrieval tasks, surpassing both its predecessor and NLLB-CLIP-SigLIP. In multilingual scenarios, it demonstrates up to 4% improvement over NLLB-CLIP-SigLIP in cross-lingual image retrieval tasks, despite having fewer parameters than its largest competitor. The model maintains strong performance even when embeddings are compressed - reducing dimensions by 75% still preserves over 99% of performance across text, image, and cross-modal tasks. On the comprehensive Multilingual MTEB benchmarks, it achieves 69.86% on retrieval and 67.77% on semantic similarity tasks, performing competitively with specialized text embedding models.","jina-colbert-v1-en":"Jina-ColBERT-v1-en demonstrates remarkable improvements over baseline models across various benchmarks. On the BEIR dataset collection, it achieves superior performance in multiple categories: 49.4% on Arguana (vs. 46.5% for ColBERTv2), 79.5% on FEVER (vs. 78.8%), and 75.0% on TREC-COVID (vs. 72.6%). Most impressively, it shows a dramatic improvement on the LoCo benchmark for long-context understanding, scoring 83.7% compared to ColBERTv2's 74.3%. The model particularly excels in scenarios requiring detailed semantic understanding, outperforming traditional embedding models while maintaining computational efficiency through its innovative late interaction approach. These improvements are achieved while keeping the model's parameter count at a modest 137M, making it both powerful and practical for production deployments.","jina-colbert-v2":"In real-world testing, Jina-ColBERT-v2 demonstrates exceptional capabilities across multiple benchmarks. It achieves a 6.5% improvement over the original ColBERT-v2 on English tasks, with an average score of 0.521 across 14 BEIR benchmarks. More impressively, it outperforms traditional BM25-based retrieval methods across all tested languages on MIRACL benchmarks, showing particular strength in cross-lingual scenarios. The model maintains this high performance even when using reduced embedding dimensions - dropping from 128 to 64 dimensions results in only a 1.5% performance decrease while halving storage requirements. This translates to significant cost savings in production: for example, storing 100 million documents with 64-dimension vectors costs $659.62 per month on AWS, compared to $1,319.24 for 128 dimensions.","jina-embedding-b-en-v1":"In real-world evaluations, Jina Embedding B v1 demonstrates impressive capabilities, particularly in semantic textual similarity tasks. The model achieves state-of-the-art performance on STS12 with a score of 0.751, surpassing established models like all-mpnet-base-v2 and all-minilm-l6-v2. It shows strong performance across various benchmarks while maintaining efficient inference times. However, users should note that the model is specifically optimized for English language content and may not perform optimally on multilingual or code-specific tasks. The model has since been superseded by jina-embeddings-v2-base-en and jina-embeddings-v3, which offer enhanced performance across a broader range of use cases.","jina-embeddings-v2-base-code":"In real-world testing, Jina Embeddings v2 Base Code demonstrates exceptional capabilities, leading the field in nine out of fifteen crucial CodeNetSearch benchmarks. When compared to models from industry giants like Microsoft and Salesforce, it achieves superior performance while maintaining a more efficient footprint. The model particularly excels in cross-language code understanding, successfully matching functionally equivalent code snippets across different programming languages. Its 8,192 token context window proves particularly valuable for large functions and complex code files, significantly outperforming traditional models that typically handle only a few hundred tokens. The model's efficiency is evident in its compact size of 307MB (unquantized), enabling fast inference while maintaining high accuracy in code similarity and search tasks.","jina-embeddings-v2-base-de":"In real-world testing, Jina Embeddings v2 Base German demonstrates exceptional efficiency and accuracy, particularly in cross-language retrieval tasks. The model outperforms Microsoft's E5 base model while being less than a third of its size, and matches the performance of E5 large despite being seven times smaller. Across key benchmarks, including WikiCLIR for English-to-German retrieval, STS17 and STS22 for bidirectional language understanding, and BUCC for precise bilingual text alignment, the model consistently demonstrates superior capabilities. Its compact size of 322MB enables deployment on standard hardware while maintaining state-of-the-art performance, making it particularly efficient for production environments where computational resources are a consideration.","jina-embeddings-v2-base-en":"In real-world testing, Jina Embeddings v2 Base English demonstrates exceptional capabilities across multiple benchmarks. It outperforms OpenAI's text-embedding-ada-002 in several key metrics: classification (73.45% vs 70.93%), reranking (85.38% vs 84.89%), retrieval (56.98% vs 56.32%), and summarization (31.6% vs 30.8%). These numbers translate to practical advantages in tasks like document classification, where the model shows superior ability to categorize complex texts, and in search applications, where it better understands user queries and finds relevant documents. However, users should note that performance may vary when dealing with highly specialized domain-specific content not represented in the training data.","jina-embeddings-v2-base-es":"In comprehensive benchmark evaluations, the model demonstrates exceptional capabilities, particularly in cross-language retrieval tasks where it outperforms significantly larger multilingual models like E5 and BGE-M3 despite being only 15-30% of their size. The model achieves superior performance in retrieval and clustering tasks, showing particular strength in matching semantically equivalent content across languages. When tested on the MTEB benchmark, it exhibits robust performance across various tasks including classification, clustering, and semantic similarity. The extended context window of 8,192 tokens proves especially valuable for long-document processing, showing consistent performance even with documents spanning multiple pages - a capability most competing models lack.","jina-embeddings-v2-base-zh":"In benchmarks on the Chinese MTEB (C-MTEB) leaderboard, the model demonstrates exceptional performance among models under 0.5GB, particularly excelling in Chinese language tasks. It significantly outperforms OpenAI's text-embedding-ada-002 in Chinese-specific applications while maintaining competitive performance in English tasks. A notable improvement in this release is the refined similarity score distribution, addressing the score inflation issues present in the preview version. The model now provides more distinct and logical similarity scores, ensuring more accurate representation of semantic relationships between texts. This enhancement is particularly evident in comparative tests, where the model shows superior discrimination between related and unrelated content in both languages.","jina-embeddings-v3":"The model demonstrates exceptional efficiency-to-performance ratio in real-world testing, outperforming both open-source alternatives and proprietary solutions from OpenAI and Cohere on English tasks while excelling in multilingual scenarios. Most surprisingly, it achieves better results than e5-mistral-7b-instruct, which has 12 times more parameters, highlighting its remarkable efficiency. In MTEB benchmark evaluations, it achieves an average score of 65.52 across all tasks, with particularly strong performance in Classification Accuracy (82.58) and Sentence Similarity (85.80). The model maintains consistent performance across languages, scoring 64.44 on multilingual tasks. When using MRL for dimension reduction, it retains strong performance even at lower dimensions - for example, maintaining 92% of its retrieval performance at 64 dimensions compared to the full 1024 dimensions.","jina-reranker-v1-base-en":"In comprehensive benchmarks, the model demonstrates exceptional improvements across key metrics, achieving an 8% increase in hit rate and a 33% boost in mean reciprocal rank compared to baseline vector search. On the BEIR benchmark, it achieves an average score of 0.5588, outperforming other rerankers from BGE (0.5032), BCE (0.4969), and Cohere (0.5141). Particularly impressive is its performance on the LoCo benchmark, where it scores 0.873 on average, significantly ahead of competitors in understanding local coherence and context-aware ranking. The model shows particular strength in technical content evaluation, achieving scores of 0.996 on qasper_abstract tasks and 0.962 on government report analysis, though it shows relatively lower performance (0.466) on meeting summarization tasks.","jina-reranker-v1-tiny-en":"In comprehensive benchmark evaluations, the model demonstrates exceptional capabilities that challenge the conventional trade-off between size and performance. On the BEIR benchmark, it achieves an NDCG-10 score of 48.54, retaining 92.5% of the base model's performance while being just a quarter of its size. Even more impressively, in LlamaIndex RAG benchmarks, it maintains an 83.16% hit rate, nearly matching larger models while processing documents significantly faster. The model particularly excels in throughput, processing documents almost five times faster than the base model while using 13% less memory than even the turbo variant. These metrics translate to real-world performance that rivals or exceeds much larger models like mxbai-rerank-base-v1 (184M parameters) and bge-reranker-base (278M parameters).","jina-reranker-v1-turbo-en":"In comprehensive benchmarks, the turbo variant demonstrates remarkable efficiency without significant accuracy trade-offs. On the BEIR benchmark, it achieves an NDCC-10 score of 49.60, retaining 95% of the base model's performance (52.45) while outperforming many larger competitors like bge-reranker-base (47.89, 278M parameters). In RAG applications, it maintains an impressive 83.51% hit rate and 0.6498 MRR, showing particular strength in practical retrieval tasks. The model's speed improvements are even more striking—it processes documents three times faster than the base model, with throughput scaling nearly linearly with reduced parameter count. However, users should note slightly lower performance on extremely nuanced ranking tasks where the full parameter count of larger models provides marginal advantages.","jina-reranker-v2-base-multilingual":"In real-world evaluations, the model demonstrates exceptional capabilities across diverse benchmarks. It achieves state-of-the-art performance on the AirBench leaderboard for RAG systems and shows strong results in multilingual tasks, including the MKQA dataset covering 26 languages. The model excels particularly in structured data tasks, achieving high recall scores in both function calling (ToolBench benchmark) and SQL schema matching (NSText2SQL benchmark). Most impressively, it delivers these results while processing documents 15 times faster than comparable models like bge-reranker-v2-m3, making it practical for real-time applications. However, users should note that optimal performance requires a CUDA-capable GPU for inference.","reader-lm-05b":"In real-world testing, Reader LM 0.5B demonstrates impressive efficiency-to-performance ratios across multiple metrics. The model achieves a ROUGE-L score of 0.56, indicating strong content preservation, and maintains a low token error rate of 0.34, showing minimal hallucination. In qualitative evaluations across 22 diverse HTML sources including news articles, blog posts, and e-commerce pages in multiple languages, it shows particular strength in structure preservation and markdown syntax usage. The model excels at handling complex modern web pages where inline CSS and scripts can expand to hundreds of thousands of tokens - a scenario where traditional rule-based approaches often fail. However, it's important to note that while the model performs exceptionally well on straightforward HTML-to-markdown conversion tasks, it may require additional processing for highly dynamic or JavaScript-heavy pages.","reader-lm-15b":"In comprehensive benchmark evaluations, Reader LM 1.5B demonstrates exceptional capabilities that challenge industry standards. The model achieves a ROUGE-L score of 0.72 and a Token Error Rate of 0.19, significantly outperforming larger models like GPT-4 (0.43 ROUGE-L, 0.50 TER) and Gemini-1.5-Pro (0.42 ROUGE-L, 0.48 TER) in HTML-to-markdown conversion tasks. Its performance particularly shines in qualitative evaluations across four key dimensions: header extraction, main content extraction, rich structure preservation, and markdown syntax usage. The model consistently maintains high accuracy across diverse document types, from news articles and blog posts to landing pages and forum posts, in multiple languages including English, German, Japanese, and Chinese. This performance is achieved while processing documents up to 256K tokens in length, eliminating the need for expensive chunking operations that are typically required with larger models."},guidance:{title:"Best Practice","ReaderLM-v2":"The model is accessible through a Google Colab notebook demonstrating HTML-to-markdown conversion, JSON extraction, and instruction-following. For HTML-to-Markdown tasks, users can input raw HTML without prefix instructions, while JSON extraction requires specific schema formatting. The create_prompt helper function facilitates easy prompt creation for both tasks. While the model works on Colab's free T4 GPU tier (requiring vllm and triton), it has limitations without bfloat16 or flash attention 2 support. RTX 3090/4090 is recommended for production use. The model will be available on AWS SageMaker, Azure, and GCP marketplace, licensed under CC BY-NC 4.0 for non-commercial use.","jina-clip-v1":"To effectively deploy Jina CLIP v1, teams should consider both its capabilities and resource requirements. The model processes images in 224x224 pixel tiles, with each tile consuming 1,000 tokens of processing capacity. For optimal performance, implement efficient image preprocessing to match these dimensions. While the model excels at both short and long text processing, it currently only supports English language input. Teams should carefully consider token usage: text requires approximately 1.1 tokens per word, while images are processed in tiles (e.g., a 750x500 pixel image requires 12 tiles, consuming 12,000 tokens). The model is available through both the Jina Embeddings API and as an open-source release on Hugging Face under the Apache 2.0 license, offering flexibility in deployment options. For production environments, consider using the AWS Marketplace or Azure deployment options, which provide optimized infrastructure setups.","jina-clip-v2":"For optimal deployment, users should consider several key factors. The model requires CUDA-capable hardware for efficient processing, with memory requirements scaling based on batch size and image resolution. To optimize API costs and performance, resize images to 512x512 pixels before processing - larger images are automatically tiled, increasing token usage and processing time. The model excels at matching images with descriptive text across languages but may struggle with abstract concepts or highly specialized domain-specific content. It's particularly effective for e-commerce product search, content recommendation systems, and visual search applications, but may not be suitable for tasks requiring fine-grained visual detail analysis or highly specialized domain expertise. When using the Matryoshka representation feature, consider the trade-off between dimension reduction and performance - while 64-dimension embeddings maintain strong performance, critical applications may benefit from higher dimensions.","jina-colbert-v1-en":"To effectively deploy Jina-ColBERT-v1-en, teams should consider several practical aspects. The model requires a CUDA-capable GPU for optimal performance, though CPU inference is possible for development. For document processing, the 8,192 token limit translates to approximately 6,000 words, making it suitable for most document types including academic papers, technical documentation, and long-form content. Teams should implement efficient document preprocessing to handle token limits and consider batch processing for large-scale indexing. While the model excels at English language content, it's not designed for multilingual applications or cross-language retrieval. For production deployments, implement proper document chunking strategies and consider using vector similarity indexes (like FAISS) for efficient retrieval. The model is particularly effective when integrated into RAG pipelines using frameworks like RAGatouille, which simplifies the implementation of complex retrieval patterns.","jina-colbert-v2":"To effectively deploy Jina-ColBERT-v2, teams should consider several practical aspects. The model requires CUDA-capable hardware for optimal performance and supports document lengths up to 8,192 tokens (extendable to 12,288) while limiting queries to 32 tokens. For production deployment, the model is available through the Jina Search Foundation API, AWS marketplace, and Azure, with a non-commercial version accessible via Hugging Face. When implementing, teams should specify whether they're embedding queries or documents, as the model uses asymmetric encoding. The model isn't designed for real-time processing of extremely large document collections without proper indexing, and while it excels at multilingual retrieval, it may show slightly lower performance on specialized domain-specific tasks compared to models fine-tuned for those specific domains.","jina-embedding-b-en-v1":"For optimal deployment, the model requires a CUDA-capable GPU, though its moderate size allows for efficient inference on standard hardware. The model accepts input sequences up to 512 tokens in length and is particularly well-suited for production environments where consistent, reliable embedding generation is crucial. It performs best on English language content and is ideal for applications like semantic search, document similarity comparison, and content recommendation systems. Teams should consider using the newer v2 or v3 versions for new projects, as they offer improved performance and broader language support. The model is not recommended for tasks requiring multilingual understanding or specialized domain knowledge outside of general English text.","jina-embeddings-v2-base-code":"To effectively deploy Jina Embeddings v2 Base Code, teams should consider several practical aspects. The model integrates seamlessly with popular vector databases like MongoDB, Qdrant, and Weaviate, making it easy to build scalable code search systems. For optimal performance, implement proper code preprocessing to handle the 8,192 token limit, which typically accommodates most function and class definitions. While the model supports 30 programming languages, it shows strongest performance in the six core languages: Python, JavaScript, Java, PHP, Go, and Ruby. Teams should consider using batch processing for large-scale code indexing to optimize performance. The model's RAG compatibility makes it particularly effective for automated documentation generation and code understanding tasks, though teams should implement appropriate chunking strategies for very large codebases. For production deployments, consider using the AWS SageMaker endpoint for managed inference, and implement appropriate caching strategies to optimize query performance.","jina-embeddings-v2-base-de":"To effectively deploy Jina Embeddings v2 Base German, organizations should consider several practical aspects. The model integrates seamlessly with popular vector databases like MongoDB, Qdrant, and Weaviate, making it straightforward to build scalable bilingual search systems. For optimal performance, implement proper text preprocessing to handle the 8,192 token limit effectively - this typically accommodates about 15-20 pages of text. While the model excels at both German and English content, it's particularly effective when used for cross-language retrieval tasks where query and document languages may differ. Organizations should consider implementing caching strategies for frequently accessed content and use batch processing for large-scale document indexing. The model's AWS SageMaker integration provides a reliable path to production deployment, though teams should monitor token usage and implement appropriate rate limiting for high-traffic applications. When using the model for RAG applications, consider implementing language detection to optimize prompt construction based on the input language.","jina-embeddings-v2-base-en":"To effectively deploy Jina Embeddings v2 Base English, teams should consider several practical aspects. The model requires CUDA-capable hardware for optimal performance, though its efficient architecture means it can run on consumer-grade GPUs. It's available through multiple channels: direct download from Hugging Face, AWS Marketplace deployment, or the Jina AI API with 1M free tokens. For production deployments, AWS SageMaker in the us-east-1 region offers the most scalable solution. The model excels at general-purpose text analysis but may not be the best choice for highly specialized scientific terminology or domain-specific jargon without fine-tuning. When processing long documents, consider breaking them into meaningful semantic chunks rather than arbitrary splits to maintain context integrity. For optimal results, implement proper text preprocessing and ensure clean, well-formatted input data.","jina-embeddings-v2-base-es":"To effectively utilize this model, organizations should ensure access to CUDA-capable GPU infrastructure for optimal performance. The model integrates seamlessly with major vector databases and RAG frameworks including MongoDB, Qdrant, Weaviate, and Haystack, making it readily deployable in production environments. It excels in applications such as bilingual document search, content recommendation systems, and cross-language document analysis. While the model shows impressive versatility, it's particularly optimized for Spanish-English bilingual scenarios and may not be the best choice for monolingual applications or scenarios involving other language pairs. For optimal results, input texts should be properly formatted in either Spanish or English, though the model handles mixed-language content effectively. The model supports fine-tuning for domain-specific applications, but this should be approached with careful consideration of the training data quality and distribution.","jina-embeddings-v2-base-zh":"The model requires 322MB of storage and can be deployed through multiple channels including AWS SageMaker (us-east-1 region) and the Jina AI API. While GPU acceleration isn't mandatory, it can significantly improve processing speed for production workloads. The model excels in various applications including document analysis, multilingual search, and cross-lingual information retrieval, but users should note that it's specifically optimized for Chinese-English bilingual scenarios. For optimal results, input text should be properly segmented, and while the model can handle up to 8,192 tokens, breaking extremely long documents into semantically meaningful chunks is recommended for better performance. The model may not be suitable for tasks requiring real-time processing of very short texts where lower-latency, specialized models might be more appropriate.","jina-embeddings-v3":"To effectively deploy Jina Embeddings v3, teams should consider their specific use case to select the appropriate task adapter: retrieval.query and retrieval.passage for search applications, separation for clustering tasks, classification for categorization, and text-matching for semantic similarity. The model requires CUDA-capable hardware for optimal performance, though its efficient architecture means it needs significantly less GPU memory than larger alternatives. For production deployment, AWS SageMaker integration provides a streamlined path to scalability. The model excels in multilingual applications but may require additional evaluation for low-resource languages. While it supports long documents up to 8,192 tokens, optimal performance is achieved with the late chunking feature for very long texts. Teams should avoid using the model for tasks requiring real-time generation or complex reasoning - it's designed for embedding and retrieval, not text generation or direct question answering.","jina-reranker-v1-base-en":"The model requires CUDA-capable hardware for optimal performance and is accessible through both API endpoints and AWS SageMaker deployment options. While it can process extremely long sequences, users should consider the trade-off between context length and processing time—the model's latency increases notably with longer documents, from 156ms for 256 tokens to 7068ms for 4096 tokens with a 512-token query. For production deployments, it's recommended to implement a two-stage pipeline where vector search provides initial candidates for reranking. The model is specifically optimized for English content and may not perform optimally on multilingual or code-heavy documents. When integrating with RAG systems, users should carefully tune the number of documents sent for reranking based on their latency requirements, with 100-200 documents typically providing a good balance between quality and performance.","jina-reranker-v1-tiny-en":"To effectively deploy this model, organizations should prioritize scenarios where processing speed and resource efficiency are critical considerations. The model is particularly well-suited for edge computing deployments, mobile applications, and high-throughput search systems where latency requirements are strict. While it performs exceptionally well across most reranking tasks, it's important to note that for applications requiring the absolute highest level of ranking precision, the base model might still be preferable. The model requires CUDA-capable GPU infrastructure for optimal performance, though its efficient architecture means it can run effectively on less powerful hardware than its larger counterparts. For deployment, the model integrates seamlessly with major vector databases and RAG frameworks, and it's available through both the Reranker API and AWS SageMaker. When fine-tuning for specific domains, users should carefully balance the training data quality with the model's compact architecture to maintain its performance characteristics.","jina-reranker-v1-turbo-en":"The model requires CUDA-capable hardware for optimal performance and can be deployed through AWS SageMaker or accessed via API endpoints. For production deployments, organizations should implement a two-stage pipeline where vector search provides initial candidates for reranking. While the model supports 8,192 tokens, users should consider the latency impact of longer sequences—processing time increases with document length. The sweet spot for most applications is reranking 100-200 candidates per query, which balances quality and speed. The model is specifically optimized for English content and may not perform optimally on multilingual documents. Memory requirements are significantly lower than the base model, typically requiring only 150MB of GPU memory compared to 550MB, making it suitable for deployment on smaller instances and enabling significant cost savings in cloud environments.","jina-reranker-v2-base-multilingual":"For optimal deployment, the model requires a CUDA-capable GPU and can be accessed through multiple channels including the Reranker API, major RAG frameworks like Haystack and LangChain, or deployed privately via cloud marketplaces. The model excels in scenarios requiring precise understanding across language barriers and data types, making it ideal for global enterprises working with multilingual content, API documentation, or code repositories. Its extensive context window of 524,288 tokens enables processing of large documents or entire codebases in a single pass. Teams should consider using this model when they need to enhance search accuracy across languages, require function calling capabilities for agentic RAG systems, or want to improve code search functionality across multilingual codebases. The model is particularly effective when used in conjunction with vector search systems, where it can significantly improve the final ranking of retrieved documents.","reader-lm-05b":"To effectively deploy Reader LM 0.5B, organizations should ensure their infrastructure can handle the model's CUDA requirements, though its efficient architecture means it can run on consumer-grade GPUs. The model works best with raw HTML input and doesn't require special prefixes or instructions. For optimal performance, implement the provided repetition detection mechanism to prevent potential token loops in output generation. While the model supports multiple languages and various HTML structures, it's specifically designed for content extraction and markdown conversion - it shouldn't be used for tasks like text generation, summarization, or direct question answering. The model is available through AWS SageMaker for production deployment, and a Google Colab notebook is provided for testing and experimentation. Teams should be aware that while the model can handle extremely long documents up to 256K tokens, processing such large inputs may require additional memory management strategies.","reader-lm-15b":"To effectively deploy Reader LM 1.5B, organizations should focus on scenarios involving complex HTML document processing where accuracy and efficiency are paramount. The model requires CUDA-capable GPU infrastructure for optimal performance, though its efficient architecture means it can run effectively on more modest hardware compared to larger alternatives. For production deployments, the model is available through both AWS SageMaker and Azure Marketplace, offering flexible integration options. While the model excels at HTML-to-markdown conversion, it's important to note that it's specifically optimized for this task and may not be suitable for general-purpose text generation or other NLP tasks. When processing extremely long documents (approaching 512K tokens), users should be aware that performance might degrade as this exceeds the model's training parameters. For optimal results, implement the provided repetition detection mechanisms and consider using contrastive search during inference to maintain output quality."}},warnings:{deprecated:"This model is deprecated by newer models."},search:{placeholder:"Search by name, tags, or type...",no_results:'No models found matching "{query}"'},sort:{label:"Sort",name:"Name",release_date:"Date",parameter_size:"Size",direction:{name:"Direction",asc:"Ascending",desc:"Descending"}},title:"{_modelName} - Search Foundation Models",loading:"Loading model details...",error:"Failed to load model",select_model:"Select a model from the list to view details",back_to_models:"Back to Models"},he={academic:"Academic",academic_research:"Academic Publications",title:"Newsroom",top_stories:"Top stories",description:"Read the latest news and updates from Jina AI.",description1:"Accelerate search AI, one word at a time.",tech_blog:"Tech blog",news_title:"Search All the Things: We're Running a MEME Contest for Jina 2.0",news_description:"For Jina 2.0, we listened to the community. Truly, deeply listened. “What are your pain points?” we asked, eagerly anticipating valuable feedback",engineering_group:"Engineering Group",engineering_group_date:"31 May, 2021",author:"Filter by author",product:"Filter by product",photos:"Photos",most_recent_articles:"Most recent articles",minutes_read:"minutes read",search:"Search by title"},pe={copy_link:"Copy the link to this section",news_not_found:"Article not found",redirect_to_news:"Redirecting to newsroom in 5 seconds...",back_to_newsroom:"Back to Newsroom",categories:"Categories",learn_more:"Learn more",in_this_article:"In this article"},ge={stars:"Stars"},fe={share_btn:"Share","Hacker News":"Hacker News",LinkedIn:"LinkedIn",reddit:"Reddit",twitter:"X (Twitter)",facebook:"Facebook",rss:"RSS feed"},ye="FAQ",be={question1:"What does Jina AI specialize in?",answer1:"Jina AI specializes in multimodal AI technologies, including embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. We leverage advanced tools like Kubernetes and serverless architectures to create robust, scalable, and production-ready solutions.",question2:"What types of AI does Jina AI work with?",answer2:"Our expertise spans a broad spectrum, encompassing large language models, text, image, video, audio understanding, neural search, and generative art.",question3:"Are your solutions scalable and production-ready?",answer3:"Yes, our solutions are designed to be scalable and ready for production. We build our solutions using cloud-native technologies that allow for efficient scaling and reliable performance in production environments.",question4:"What industries can benefit from Jina AI's solutions?",answer4:"Our services are versatile and adaptable, making them suitable for a wide range of industries, including e-commerce, legal tech, digital marketing, gaming, healthcare, finance, and many more.",question5:"How do we start a project with Jina AI?",answer5:"You can get in touch with our sales team through the contact form on this page. We would love to discuss your project requirements and how our solutions can help your business.",question6:"What support do you provide after implementing a solution?",answer6:"We provide continuous support to ensure the smooth operation of our solutions. This includes troubleshooting, regular updates, and improvements based on your feedback and needs.",question7:"What is the typical duration for a project?",answer7:"Project duration varies depending on the complexity and scope of the project. After understanding your requirements, we can provide a more accurate estimate.",question8:"How does Jina AI protect my data?",answer8:"Data security is our top priority. We adhere to strict data protection policies and regulations to ensure your data is secure and confidential.",question9:"What is the pricing structure for your services?",answer9:"Pricing depends on the project's complexity and requirements. We offer both project-based and retainer pricing models. Please contact our sales team for more information.",question10:"What are the licensing terms for your solutions?",answer10:"We provide different licensing options based on the nature of the project and the client's needs. Detailed terms can be discussed with our sales team.",question11:"What is your service area?",answer11:"We provide services globally, with our headquarters based in Berlin, Europe, and additional offices in Sunnyvale, Beijing and Shenzhen.",question12:"Do you offer onsite support?",answer12:"Yes, we offer onsite support, especially for clients located near our offices in Berlin, Beijing, and Shenzhen. For other locations, we strive to provide the best possible remote support and can arrange for onsite support if necessary."},_e="Beta",ve="Print",we="Download SOC 2 Type 1 Attestation",ke="Get your API key",xe={used_product_required:"Select the model you are using or you are interested in",used_product:"Which model are you using?",domain_required:"Tell us your work domain helps us to provide better service",contactTitle_required:"Your job title is required",contactName_required:"How should we address you?",company_url_required:"Tell us your company's website helps us to provide better service",care_most_required:"When choosing a service, what do you care most about?",company_size_required:"Tell us your company's size helps us to provide better service",usage_type_required:"Tell us your usage type helps us to provide better service",get_new_key:"Get your API key",usage_type_options:{research:"Research",poc:"Proof of Concept",production:"Production",other:"Other"},care_most_options:{accuracy:"Accuracy",speed:"Speed",cost:"Cost",scalability:"Scalability",other:"Other"},contactTitle:"What is your job title?",full_survey:"Take the full survey and get faster response from our team",usage_type:"What type of usage best describes you?",care_most:"What do you care most about?",company_url:"What is your company's website?",company_size:"What is your company's size?",contactName:"Your name",email:"Email",email_contact:"Your contact Email",subscribe:"Subscribe",send:"Send",contact_us:"Contact us",sign_up:"Sign up",tell_domain:"What domain do you work in?",get_update_embeddings:"Get the latest updates for the embeddings",get_update_blog_posts:"Get the latest updates for the blog posts",email_required:"Email is required",email_invalid:"Email is invalid",fine_tuned_embedding:"Interested in fine-tuned embeddings tailored to your data and use case? Let's discuss!",fine_tuned_reranker:"Interested in fine-tuned rerankers tailored to your data and use case? Let's discuss!"},Ae={explain:"Discover hidden features on our website",GlobalQA:{title:"On-page RAG",description:"Press the '/' key on any page to open the question box. Type your query and hit 'Enter' to receive answers directly related to the page content. This feature is powered by PromptPerfect."},SceneXplainTooltip:{title:"Image Captioning",description:"Hover your cursor over any image on news pages or in our newsroom catalog to reveal the description of that image. Descriptions are precomputed by SceneXplain and embedded in the image's ALT attribute for accessibility."},Recommender:{title:"Related Article",description:"Open the recommendation box on any news page with 'Shift+2'. Select the reranker model to discover the top-5 articles related to that news page. Enjoy this real-time feature, powered by our Reranker API."}},Ie={recommended_articles:"Top-5 similar articles",recommend:"Get top-5",out_of_quota:"This API key has run out of tokens. Please recharge your account or use a different API key.",confirm_title:"Warning: High Token Usage",confirm_message:"Your API key has {_leftTokens} tokens remaining. Sending the full text of {_numArticles} articles to the Reranker API, utilizing the {_selectedReranker} model to discover related articles for the current page, will significantly reduce the token count of your API key {_APIKey}. Do you want to proceed?"},Te={api_key:"Enter your API key.",new_key:"Get new key",continue:"Continue",preview:"Preview",use_url:"Use URL instead. Toggle it on means we will base on the page content of that URL to generate synthetic data for fine-tuning.",job_acknowledged:"Your fine-tuning job has been queued. You will receive an email when the job starts. The full process often takes 20 minutes to complete.",find_on_huggingface:"Find results on Hugging Face",not_enough_token:"Not enough tokens in this API key. Please top up your balance or use a different API key.",wait_for_processing:"Please wait while we process your request...",back:"Back",failed_job:"The fine-tuning request failed. See the reason below.",reset:"Start over",domain_hint:"Describe the domain you wish to fine-tune for.",query_doc:"Query-document description",query_explain:"Describe what a query looks like.",doc_explain:"Describe what a matched document should look like.",url_explain:"Public URL of a webpage that contains the content you want to fine-tune on.",query_doc_caption:"Describe what the query looks like and what the matched document looks like in your domain.",url:"Or, webpage URL",url_caption:"Refer to the content from a URL for fine-tuning.",general_instruction:"Or, general instruction",general_instruction_explain:'Describe your domain in free-form text. You can imagine it as a "prompt" like in ChatGPT.',general_instruction_caption:"Provide a detailed description of how the fine-tuned embeddings will be used.",domain_explain:"Provide a detailed description of how the fine-tuned embeddings will be used. This is essential for generating high-quality synthetic data that will improve the performance of your embeddings.",domain_explain2:"There are three ways to specify your requirement: a general instruction, a URL, or a query-document description. Choose one.",select_base_model:"Choose a base embedding model for fine-tuning.",base_model_selected:"Base model selected",select_base_model_explain:"Select a base model as the starting point for fine-tuning. Typically, base-en is a good choice, but for tasks in other languages, consider using a bilingual model.",write_email_explain:"Fine-tuning takes time. We'll communicate via email about the start, progress, completion, and any issues of your fine-tuning job, along with details on the fine-tuned model and training dataset.",cost_1m_token:"Each fine-tuning job consumes 1M tokens. Ensure you have sufficient tokens or top-up your balance. You can also generate a new API key. Every API key comes with 1M free tokens.",placeholder:"Car insurance claims",which_domain:"Fine-tuning domain",start_tuning:"Start fine-tuning",click_start:"Agree to the terms and begin fine-tuning.",how_it_works:"Learn about the fine-tuning process.",email_not_match:"Email addresses do not match. Please verify.",consent0:"I agree that synthetic data for model fine-tuning will be generated based on my instructions.",consent1:"I acknowledge that the final model and synthetic data will be publicly accessible on Hugging Face.",consent2:"I understand that this feature is in beta and Jina AI offers no warranties. The pricing and UX may change.",confirm_title:"Confirm fine-tuning job",confirm_your_email:"Re-enter your email address to confirm the fine-tuning job. Updates and the download link will be sent to this email."},Pe={"ReaderLM-v2_description":"Frontier small language model for converting raw HTML into markdown or JSON","jina-embedding-b-en-v1_description":"The first version of the Jina Embedding model, the OG.","reader-lm-05b_description":"A small language model for converting raw HTML into markdown","reader-lm-15b_description":"A small language model for converting raw HTML into markdown",mminput_placeholder:"Text, image URL, image base64 string",clip_v2_title:"clip-v2: Multilingual Multimodal Embeddings",clip_v2_description:"jina-clip-v2 is a 0.9B CLIP-style model that brings three major advances: multilingual support for 89 languages, high image resolution at 512x512, and Matryoshka representation learning for truncated embeddings.",three_ways:"Three Ways to Purchase",three_ways_desc:"Subscribe to our API, purchase through cloud providers, or obtain a commercial license for your organization.",more_models:"{_numMore} more models",request_path:"Request endpoint",request_number:"Request times",read_release_note:"Read Release Note",v3_title:"v3: Frontier Multilingual Embeddings",v3_description:"<code>jina-embeddings-v3</code> is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB. Read our blog post and research paper below.",late_chunking:"Late chunking",late_chunking_explain:"Apply the late chunking technique to leverage the model's long-context capabilities for generating contextual chunk embeddings.",task_type:"Downstream task",task_type_explain:"Select the downstream task for which the embeddings will be used. The model will return the optimized embeddings for that task.",task_type_retrieval_query:"Retrieval Query",task_type_retrieval_query_explain:"Embedding queries in a query-document retrieval task.",task_type_retrieval_passage:"Retrieval Passage",task_type_retrieval_passage_explain:"Embedding documents in a query-document retrieval task.",task_type_classification:"Classification",task_type_classification_explain:"Text classification.","task_type_text-matching":"Text Matching","task_type_text-matching_explain":"Semantic text similarity, general symentric retrieval, recommendation, find alike, deduplication.",task_type_separation:"Separation",task_type_separation_explain:"Clustering documents, visualizing corpus.",task_type_none_explain:"No adapter will be used. A generic embedding will be returned, useful for debugging or hacking.",dimensions:"Output dimensions",dimensions_explain:"Smaller dimensions enable efficient storage and retrieval, with minimal impact thanks to Matryoshka representation.",dimensions_warning:"We recommend keeping the dimension size above {_minDimension} for performance.",dimensions_error:"The dimension size must be between 1 and 1024.",colbert_dimensions_explain:"The dimension size of the per-token embedding.",api_key_update_title:"Replacing API key",api_key_update_message:"By replacing your old API key, the new key will appear in the UI whenever you visit jina.ai. Future top-ups will apply to this new key. Your old key remains valid, so if you plan to use it again, please store it securely.",add_time_explain:"The date when this model was added to the Search Foundation.",tools:"Tools",raise_issue:"Raise issue",turnstile_error:"We cannot generate an API key because we couldn't verify if you are human.",turnstile_unsupported:"We cannot generate an API key because your browser isn't supported.",mistake_contact:"If you believe this is an error, please contact us.",auto_request:"Auto preview",auto_request_tooltip:"Automatically preview the API response when changing the model, using hundreds of tokens from your API key. Turn off to manually send a request by clicking 'Get response'.",public_cloud_integration:"With <b>{_numPartners}</b> cloud service providers",public_cloud_integration_desc:"Using AWS or Azure? You can deploy our models directly on your company's cloud platform and handle billing through the CSP account.","3p_integration":"With <b>{_numPartners}</b> third-party services","3p_integration_desc":"Integrate our search foundation with your existing services. Our partners have built connectors to our API, making it easy to use our models in your applications.",input_type:"Embed as document/query",input_type_explain:"The same input can serve as either a query or a document embedding, depending on its search role.",return_format_title:"Returning data type",return_format_explain:"Besides the float, you can ask it to return as binary for faster vector retrieval, or as base64 encoding for faster transmission.",example_inputs:"Example inputs",remaining_left:"You have <b>{_leftTokens}</b> tokens left in the API key below.",tax_may_apply:"Depending on your location, you may be charged in USD, EUR, or other currencies. Taxes may apply.",auto_recharge:"Auto-Recharge for Low Token Balance",auto_recharge_description:"Recommended for uninterrupted service in production. When your token balance drops below the set threshold, we will automatically recharge your saved payment method for the last purchased package, until the threshold is met.",auto_recharge_enable:"You have enabled auto-recharge on low tokens",auto_recharge_confirm_title:"Disable auto-recharge",auto_recharge_confirm_message:"Are you sure you want to disable auto-recharge? This will stop automatic top-ups when your token balance is low and may interrupt your service or application.",auto_recharge_enable_title:"Enable auto-recharge",auto_recharge_enable_message:"To enable auto-recharge, please purchase a pack with auto-recharge set to true.",auto_recharge_enable_message2:"Please select a package to be purchased when auto-recharge is triggered.",recharge_threshold:"Recharge if",includes:"Tokens valid for:",return_float:"Default (as float)",return_binary:"Binary (packed as int8)",return_ubinary:"Binary (packed as uint8)",return_base64:"Base64 (as string)",return_format:"Embeddings format",float_description:"The embeddings are returned as a list of floating-point numbers. Most common and easy to use.",binary_description:"The embeddings are packed as int8. Much more efficient for storage, search and transmission.",ubinary_description:"The embeddings are packed as uint8. Much more efficient for storage, search and transmission.",base64_description:"The embeddings are returned as a base64-encoded string. More efficient for transmission.",tuning:"Fine-Tune",onprem:"On-prem",oncsp:"On CSP",running:"Active",sleeping:"Inactive",status_explain:"Our serverless architecture may offload certain models during periods of low usage. For active models, responses are immediate. Inactive models require a few seconds to load upon the initial request. After activation, subsequent requests are processed more swiftly.",score:"Score",index_and_search1:"Index & search",index_and_search:"Index & search",none:"None",results_fed_to_reranker:"#docs fed to reranker",results_as_final_result:"#docs as result",please_select_model:"Please select an Embedding model or a Reranker model",embedding_none_description:"Do not use any embedding model",rank_none_description:"Do not use any reranker model",get_new_key_survey:"Fill in the survey, help us understand your usage, and get a new API key for free!",pricing:"API Pricing",right_api_key_to_charge:"Please input the right API key to top up",pricing_desc:"API pricing is based on token usage - input tokens for standard APIs and output tokens for Reader API. One API key gives you access to all search foundation products.",select_embedding_model:"Select embeddings",select_rerank_model:"Select reranker",select_classify_model:"Select classifier",fill_example:"Fill in an example","bge-small-en-v1_5_description":"A streamlined English model delivering efficient and high-quality embeddings.","bge-base-en-v1_5_description":"A robust English model balancing performance and efficiency for versatile use.","bge-large-en-v1_5_description":"A powerhouse English model offering top-tier embeddings with exceptional quality.","bge-small-zh-v1_5_description":"A compact Chinese model providing nimble and precise embeddings.","bge-base-zh-v1_5_description":"A well-rounded Chinese model balancing capability and efficiency.","bge-large-zh-v1_5_description":"A high-capacity Chinese model delivering superior and detailed embeddings.","bge-small-en_description":"An efficient English model for streamlined and accurate embeddings.","bge-base-en_description":"A balanced English model designed for solid and reliable performance.","bge-large-en_description":"A top-performing English model crafted for premium quality embeddings.","bge-small-zh_description":"An agile Chinese model for efficient and precise embeddings.","bge-base-zh_description":"A versatile Chinese model combining efficiency and robust performance.","bge-large-zh_description":"A high-performance Chinese model optimized for top-tier embeddings.","bge-m3_description":"A versatile multilingual model offering expansive capabilities and high-quality embeddings.",multimodal:"Multimodal",multimodal_explain:"This model can encode both text and image inputs, making it ideal for multimodal search tasks.","jina-clip-v1_description":"Multimodal embedding models for images and English text","jina-reranker-v1-base-en_description":"Our first reranker model maximizing search and RAG relevance","jina-reranker-v1-turbo-en_description":"The best combination of fast inference speed and accurate relevance scores","jina-reranker-v1-tiny-en_description":"The fastest reranker model, best suited for ranking a large number of documents reliably","jina-reranker-v2-base-multilingual_description":"The latest and best reranker model with multilingual, function calling and code search support.","jina-colbert-v1-en_description":"Improved ColBERT with 8K-token length for embedding and reranking tasks","jina-colbert-v2_description":"The best multilingual ColBERT with top performance on embedding and reranking","jina-clip-v2_description":"Multilingual Multimodal embeddings for texts and images",multi_embedding:"Multi-vector",multi_embedding_explain:"This model will return a bag of contextualized embeddings for a given input. Each token in the input is mapped to a vector in the output.",read_api_docs:"API Spec",input:"Request",output:"Response",usage_rerank:"Usage",input_length:"Input length",output_dimension:"Output dimensions",token_length_explain:"The maximum length of the input token sequence is {_tokenLength} for this model.",output_dim_explain:"The output dimension of an embedding vector from this model is {_outputDim}.",size_explain:"The number of parameters in the model is {_size}, note that this is not the size of the model file.",language_explain:"This model best supports {_language} language.",opensource:"OSS",opensource_explain:"This model is open source and available on Hugging Face. Click this button to view the model on Hugging Face.",wait_for_processing:"Your request is being processed.",you_can_leave:"You can leave this page and we will send you the download link upon completion.",click_upload_btn_above:"Click the upload button above to start.",start_batch:"Start batch embedding",visualization_example_you_can:"Use our API below, you can do it too!",start_embedding:"Index",maximize_tooltip:"Maximize this panel with Shift+1",show_api_key:"Show API Key",batch_job:"Batch Job",bulk_embedding_failed:"Fail to create batch embedding job",search:"Search",visualize:"Visualize",bulk:"Batch embed",what_are_embedding:"What are Embeddings?",what_are_embedding_answer:`Imagine teaching a computer to grasp the nuanced meanings of words and phrases. Traditional methods, which relied on rigid, rule-based systems, fell short because language is too complex and fluid. Enter text embeddings: a powerful solution that translates text into a language of numbers—specifically, into vectors in a high-dimensional space.

Consider the phrases "sunny weather" and "clear skies." To us, they paint a similar picture. Through the lens of embeddings, these phrases are transformed into numerical vectors that reside close to each other in this multi-dimensional space, capturing their semantic kinship. This closeness in the vector space is not just about words or phrases being similar; it's about understanding context, sentiment, and even subtle nuances in meaning.

Why is this breakthrough important? For starters, it bridges the gap between the richness of human language and the computational efficiency of algorithms. Algorithms excel at crunching numbers, not interpreting texts. By converting text into vectors, embeddings make it possible for these algorithms to 'understand' and process language in a way that was previously out of reach.

The practical applications are vast and varied. Whether it's recommending content that resonates with your interests, powering conversational AI that feels surprisingly human, or even detecting subtle patterns in large volumes of text, embeddings are the key. They enable machines to perform tasks like sentiment analysis, language translation, and much more, with an understanding of language that is increasingly nuanced and refined.`,open_tensorboard:"Open visualizer",visualization_example:"Embedding all sentences from this section to a 3D vector space",visualize_done:"Visualization is done, you can now click the top button to open the visualizer.",more_than_two2:"Please enter more than two documents, i.e. more than two lines.",generating_visualization:"Generating visualization...",query:"Query",document:"Document",pairwise_test:"Pairwise",search_hint:"Type to search within the sentences listed below",original_documents:"Sentences to embed",total_documents:"Embedding progress: {_Processed}/{_Count} sentences.",embedding_done:"Successfully embedded {_Count} sentences.",please_fill_docs_first:"Please first enter some sentences below before search.",original_documents_hint:"Enter your sentences here. Each new line will be considered a separate sentence/document.",upload_file:"Click here to upload a file",max_file_size:"Maximum size allowed: {_maxSize}.",write_email_here:"Please enter the email where you want to receive the download link upon completion.",upload:"Upload",download:"Download",batch_upload_hint:"We will use the API key and the model below to process the documents.",autostart:"Embedding will automatically start after a brief delay",learn_more:"Learn more",why_do_you_need:"Choosing the Right Embeddings",why_do_you_need_before:"Our embedding models are designed to cover diverse search and GenAI applications.",why_do_you_need_after:"Leveraging deep neural networks and LLMs, our embedding models represent multimodal data into a streamlined format, improving machine comprehension, efficient storage and enabling advanced AI applications. These embeddings play a crucial role in understanding the data, enhancing user engagement, overcoming language barriers, and optimizing development processes.",why_need_1_title:"General-Purpose Embeddings",why_need_4_title:"Multimodal Embeddings",why_need_4_description:"Jina CLIP is our latest multimodal embedding model for image and text. A big improvement over OpenAI CLIP is that this single model can be used for text-text retrieval, as well as text-image, image-text, and image-image retrieval tasks! So one model, two modalities, four search directions!",why_need_1_description:"Our core embedding model, powered by JinaBERT, is built for a broad spectrum of applications. It excels in understanding detailed text, making it ideal for semantic search, content classification, and intricate language analysis. Its versatility is unmatched, supporting the creation of advanced sentiment analysis tools, text summarization, and personalized recommendation systems.",why_need_2_title:"Bilingual Embeddings",why_need_2_description:"Our bilingual models facilitate communication across languages, enhancing multilingual platforms, global customer support, and cross-lingual content discovery. Designed to master German-English and Chinese-English translations, these models simplify interactions and foster understanding among diverse linguistic groups.",why_need_3_title:"Code Embeddings",why_need_3_description:"Tailored for developers, our code embedding model optimizes coding tasks like summarization, code generation, and automatic reviews. It boosts productivity by offering deeper insights into code structures and suggesting improvements, making it essential for developing advanced IDE plugins, automatic documentation, and cutting-edge debugging tools.",top_up_warning_message1:"The current API key has {_remainedTokens} tokens remaining and will be replaced by a new key with {_freeTokens} tokens. You may continue to use or top up the old key if you have stored it securely. How do you want to proceed?",top_up_warning_title:"Replace Old API Key",top_up_button:"Top Up Old Key",top_up_button_explain:"Integrating this API key offers a more professional solution, eliminating the need for frequent key changes. Usage data is retained and accessible at any time.",get_new_key_button_explain:"Opting for a new key will result in the loss of usage history associated with the old key.",get_new_key_button:"Get New Key",cancel_button:"Cancel","1M_free":"1M free tokens",free:"Free",Free1M:"1M tokens","1M_free_description":"Enjoy your new API key with free tokens, no credit card required.",protectData1:"Request data and documents are not used for training models.",protectData2:"Data encryption in transit (TLS 1.2+) and at rest (AES-GCM 256).",protectData3:"SOC 2 and GDPR compliant.",multilingual:"Multilingual support",protect_data:"Protect Your Data",feature_multilingual:"Offering bilingual models for German-English, Chinese-English, Spanish-English among others, ideal for cross-lingual applications.",add_pair:"New",poster:"The Evolution of Embeddings Poster",poster_description:"Discover the ideal poster for your space, featuring captivating infographics or breathtaking visuals tracing the evolution of text embedding models since 1950.",buy_poster:"Buy a hard copy",learn_poster:"Learn how we made it",delete_pair:"Delete",length:"Token length",debugging:"Test",text1:"Left",text2:"Right",new:"New model",edit_text1_text:"Edit left text",edit_text2_text:"Edit right text",no_data1:"Add a pair of sentences to calculate the similarity",cosine_similarity:"Cosine similarity","jina-embeddings-v2-base-es_description":"Spanish-English bilingual embeddings with SOTA performance","jina-embeddings-v2-base-code_description":"Optimized for code and docstring search","jina-embeddings-v2-small-en_description":"Optimized for low latency and memory footprint","jina-embeddings-v2-base-en_description":"On par with OpenAI's text-embedding-ada002","jina-embeddings-v2-base-zh_description":"Chinese-English bilingual embeddings with SOTA performance","jina-embeddings-v2-base-de_description":"German-English bilingual embeddings with SOTA performance","jina-embeddings-v3_description":"Frontier multilingual embedding model with SOTA performance",learning1:"Learning about Embeddings",learning1_description:"Where to start with embeddings? We've got you covered. Learn about embeddings from the ground up with our comprehensive guide.",feature_solid:"Best-in-class",feature_solid_description1:"Developed from our cutting-edge academic research and rigorously tested against the SOTA models to ensure unparalleled performance.",feature_8k1:"8192 token-length",feature_8k_description1:"Pioneering the first open-source embedding model with an 8192-token length, enabling the representation of an entire chapter in a single vector.",feature_top_perform1:"Seamless integration",feature_top_perform_description1:"Fully compatible with OpenAI's API. Effortlessly integrates with over 10 vector databases and RAG systems for a smooth user experience.",feature_cheap:"50x cheaper",feature_cheap_v1:"5x more affordable",feature_cheap_v1_description1:"Start with free trials and enjoy a straightforward pricing structure. Get access to powerful embeddings for just 20% of OpenAI's cost.",feature_on_premises:"Privacy first",feature_on_premises_description1:"Seamlessly deploy our embedding models directly within your Virtual Private Cloud (VPC). Currently supported on AWS Sagemaker, with forthcoming integrations for Microsoft Azure and Google Cloud Platform. For tailored Kubernetes deployments, reach out to our sales team for specialized assistance.",feature_on_premises_description2:"Deploy Jina Embeddings models in AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers.",feature_on_premises_description3:"Deploy Jina Embeddings models in AWS Sagemaker and Microsoft Azure, and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers.",feature_on_premises_description4:"Deploy Jina Embedding and Reranker models on-premises using AWS SageMaker, Microsoft Azure, or Google Cloud Services, ensuring your data remains securely in your control.",vector_database_integration1:"Integrations",integrate:"Integrate",vector_database_integration_description:"Seamlessly and easily integrate the Jina Embeddings API with any of these databases, frameworks and applications below. Our tutorials will show you how.",vector_database_integration2:"Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start.",vector_database_integration3:"Our Embedding & Reranker API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start.",api_integration_short:"Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks.",title:"Embedding API",description:"@:landing_page.embedding_desc1",key_enter_placeholder_to_topup:"Enter the API key you wish to recharge",key_to_top_up:"Have a different API key to top up? Paste above and click 'Save'.",key_enter_placeholder:"Please enter your API key",key_warn_v2:"This is your unique key. Store it securely!",last_7_days:"Usage",key_warn:"Make sure to store your API key at a safe place. Otherwise you will need to generate a new key",refresh_key_tooltip1:"Get a new API key for free",regenerate:"Regenerate",retry:"Retry",refresh:"Refresh",generate_api_key_error:"Fail to generate an API key",key:"API key",code:"code",manage_quota1:"API Key & Billing",manage_billing:"Manage invoice",manage_billing_tip:"Manage your billing information, get invoices, and set up auto-recharge.",size:"Parameters",output_dim:"Dimensions",remaining:"Available tokens",usage:"Usage",api_integrations:"API Integrations",usage_history:"Usage in last 7 days",view_details:"View Details",input_api_key_error1:"Your API key is not valid!",usage_time:"Date time",usage_amount:"Tokens",usage_reason:"Description",usage_reason_trial:"Trial",usage_reason_consume:"Used",usage_reason_purchase:"Purchased",usage_reason_transfer_out:"Transfer out",usage_reason_transfer_in:"Transfer in",usage_history_explain:"Data is not in real-time and can be few minutes delayed.",wait_stripe:"Opening Stripe payment, please wait",refresh_token_count1:"Refresh to get available tokens of current API key",what_is_a_token:'A token in text processing is a unit, often a word. For example, "Jina AI is great!" becomes five tokens, including the punctuation.',token_example:`A tweet is about 20 tokens, a news article is about 1000 tokens, and Charles Dickens' novel "A Tale of Two Cities" has over a million tokens.`,buy_more_quota:"Top up this API key with more tokens",tokens:"Tokens","500M tokens":"500M tokens","1M tokens":"1 Million","1M tokens_targetUser":"Toy Experiment","1B tokens_targetUser":"Prototype Development","11B tokens_targetUser":"Production Deployment","1M tokens_intuition1":'Equivalent to reading the entire text of "The Hobbit" and "The Great Gatsby".',"500M tokens_intuition1":'Similar to watching every episode of "The Simpsons" from season 1 to season 30.',"1B tokens_intuition1":'About the same as reading the complete works of Shakespeare and the entire "Harry Potter" series.',"2_5B tokens_intuition1":`Comparable to transcribing every word spoken in the movie "The Lord of the Rings" trilogy 1,000 times.
`,"5_5B tokens_intuition1":"Equivalent to reading the entire text of the Encyclopaedia Britannica.","11B tokens_intuition1":"Similar to reading all the English language articles on Wikipedia.","59B tokens_intuition1":"Equal to all tweets posted worldwide over a two-day period.","1B tokens":"1 Billion","2_5B tokens":"2.5B tokens","5_5B tokens":"5.5B tokens","11B tokens":"11 Billion","59B tokens":"59B tokens",per_k:"/ 1K tokens",per_m:"/ 1M tokens",faq:"@:contact_us_page.faq",file_type_not_supported:"File type not supported",file_size_exceed:"Exceed max file size {_size}",model_required:"Model is required",file_required:"File is required",faqs_v2:{title:"Embeddings-related common questions",question1:"What are the jina-clip models, and can I use them for text and image search?",answer1:"Jina CLIP <code>jina-clip-v2</code> is an advanced multimodal embedding model that supports text-text, text-image, image-image, and image-text retrieval tasks. Unlike the original OpenAI CLIP, which struggles with text-text search, Jina CLIP excels as a text retriever. <code>jina-clip-v2</code> offers a 3% performance improvement over <code>jina-clip-v1</code> in both text-image and text-text retrieval tasks, supports 89 languages for multilingual image retrieval, processes higher resolution images (512x512), and reduces storage requirements with Matryoshka representations. You can read more about it in our tech report.",question0:"How were the jina-embeddings-v3 models trained?",answer0:"For detailed information on our training processes, data sources, and evaluations, please refer to our technical report available on arXiv.",question3:"Which languages do your models support?",answer3:"As of its release on September 18, 2024, <code>jina-embeddings-v3</code> is the best multilingual model and ranks 2nd on the MTEB English leaderboard for models with fewer than 1 billion parameters. v3 supports a total of 89 languages, including the top 30 with the best performance: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Latvian, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, Swedish, Thai, Turkish, Ukrainian, Urdu, and Vietnamese. For more details, please refer to the <code>jina-embeddings-v3</code> tech report.",question4:"What is the maximum length for a single sentence input?",answer4:"Our models allow for an input length of up to 8192 tokens, which is significantly higher than most other models. A token can range from a single character, like 'a', to an entire word, such as 'apple'. The total number of characters that can be input depends on the length and complexity of the words used. This extended input capability enables our <code>jina-embeddings-v3</code> and <code>jina-clip</code> models to perform more comprehensive text analysis and achieve higher accuracy in context understanding, especially for extensive textual data.",question5:"What is the maximum number of sentences I can include in a single request?",question6:"How do I send images to the jina-clip models?",answer6:"You can use either <code>url</code> or <code>bytes</code> in the <code>input</code> field of the API request. For <code>url</code>, provide the URL of the image you want to process. For <code>bytes</code>, encode the image in base64 format and include it in the request. The model will return the embeddings of the image in the response.",answer5:"A single API call can process up to 2048 sentences or texts, facilitating extensive text analysis in one request.",question7:"How do Jina Embeddings models compare to OpenAI's and Cohere's latest embeddings?",answer7:"In evaluations on the MTEB English, Multilingual, and LongEmbed benchmarks, <code>jina-embeddings-v3</code> outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, and surpasses <code>multilingual-e5-large-instruct</code> across all multilingual tasks. With a default output dimension of 1024, users can truncate the embedding dimensions down to 32 without sacrificing performance, thanks to the integration of Matryoshka Representation Learning (MRL).",question8:"How seamless is the transition from OpenAI's text-embedding-3-large to your solution?",answer8:"The transition is streamlined, as <a class='text-primary' href='https://api.jina.ai/v1/embeddings'>our API endpoint</a>, matches the input and output JSON schemas of OpenAI’s <code>text-embedding-3-large</code> model. This compatibility ensures users can easily replace the OpenAI model with ours when using OpenAI’s endpoint.",question9:"How tokens are calculated when using jina-clip models?",answer9:`Tokens are calculated based on the text length and image size. For text in the request, tokens are counted in the standard way. For images, the following steps are conducted:

1. Tile Size: Each image is divided into tiles. For <code>jina-clip-v2</code>, tiles are 512x512 pixels, while for <code>jina-clip-v1</code>, tiles are 224x224 pixels.
2. Coverage: The number of tiles required to cover the input image is calculated. Even if the image dimensions are not perfectly divisible by the tile size, partial tiles are counted as full tiles.
3. Total Tiles: The total number of tiles covering the image determines the cost. For example, a 600x600 pixel image would be covered by 2x2 tiles (4 tiles) in v2 and 3x3 tiles (9 tiles) in v1.
4. Cost Calculation: For <code>jina-clip-v2</code>, each tile costs 4000 tokens, while for <code>jina-clip-v1</code>, each tile costs 1000 tokens.

Example:
For an image with dimensions 600x600 pixels:

• With <code>jina-clip-v2</code>
	• The image is divided into 512x512 pixel tiles.
	• The total number of tiles required is 2 (horizontal) x 2 (vertical) = 4 tiles.
	• The cost for <code>jina-clip-v2</code> will be 4*4000 = 16000 tokens.

• With <code>jina-clip-v1</code>
	• The image is divided into 224x224 pixel tiles.
	• The total number of tiles required is 3 (horizontal) x 3 (vertical) = 9 tiles.
	• The cost for jina-clip-v1 will be 9*1000 = 9000 tokens.`,question17:"Do you provide models for embedding images or audio?",answer17:"Yes, <code>jina-clip-v2</code> and <code>jina-clip-v1</code> can embed both images and texts. Embedding models on more modalities will be announced soon!",question18:"Can Jina Embedding models be fine-tuned with private or company data?",answer18:"For inquiries about fine-tuning our models with specific data, please contact us to discuss your requirements. We are open to exploring how our models can be adapted to meet your needs.",question19:"Can your endpoints be hosted privately on AWS, Azure, or GCP?",answer19:"Yes, our services are available on AWS, Azure, and GCP marketplaces. If you have specific requirements, please contact us at sales AT jina.ai."},normalized:"L2 normalization",normalized_explain:"Scales the embedding so its Euclidean (L2) norm becomes 1, preserving direction. Useful when downstream involves dot-product, classification, visualization.",compatible:"Compatible mode",compatible_explain:"Follows the same request format as our text embedding models. This allows you to switch between models without changing the request. Note, image input is not supported in this mode."},qe={title:"Billing-related common questions",question9:"Is billing based on the number of sentences or requests?",answer9:"Our pricing model is based on the total number of tokens processed, allowing users the flexibility to allocate these tokens across any number of sentences, offering a cost-effective solution for diverse text analysis requirements.",question10:"Is there a free trial available for new users?",answer10:"We offer a welcoming free trial to new users, which includes one million tokens for use with any of our models, facilitated by an auto-generated API key. Once the free token limit is reached, users can easily purchase additional tokens for their API keys via the 'Buy tokens' tab.",question13:"Are tokens charged for failed requests?",answer13:"No, tokens are not deducted for failed requests.",question14:"What payment methods are accepted?",answer14:"Payments are processed through Stripe, supporting a variety of payment methods including credit cards, Google Pay, and PayPal for your convenience.",question15:"Is invoicing available for token purchases?",answer15:"Yes, an invoice will be issued to the email address associated with your Stripe account upon the purchase of tokens."},Se={title:"API-related common questions",question1:"Can I use the same API key for reader, embedding, reranking, classifying and fine-tuning APIs?",answer1:"Yes, the same API key is valid for all search foundation products from Jina AI. This includes the reader, embedding, reranking, classifying and fine-tuning APIs, with tokens shared between the all services.",question5:"Do API keys expire?",answer5:"No, our API keys do not have an expiration date. However, if you suspect your key has been compromised and wish to retire it, please contact our support team for assistance. You can also revoke your key in <a class='text-primary' href='https://jina.ai/api-dashboard'>the API Key Management dashboard</a>.",question3:"Can I monitor the token usage of my API key?",answer3:"Yes, token usage can be monitored in the 'API Key & Billing' tab by entering your API key, allowing you to view the recent usage history and remaining tokens. If you have logged in to the API dashboard, these details can also be viewed in the 'Manage API Key' tab.",question4:"What should I do if I forget my API key?",answer4:"If you have misplaced a topped-up key and wish to retrieve it, please contact support AT jina.ai with your registered email for assistance. It's recommended to log in to keep your API key securely stored and easily accessible.",question6:"Can I transfer tokens between API keys?",answer6:"Yes, you can transfer tokens from a premium key to another. After logging into your account on <a class='text-primary' href='https://jina.ai/api-dashboard'>the API Key Management dashboard</a>, use the settings of the key you want to transfer out to move all remaining paid tokens.",question7:"Can I revoke my API key?",answer7:"Yes, you can revoke your API key if you believe it has been compromised. Revoking a key will immediately disable it for all users who have stored it, and all remaining balance and associated properties will be permanently unusable. If the key is a premium key, you have the option to transfer the remaining paid balance to another key before revocation. Notice that this action cannot be undone. To revoke a key, go to the key settings in <a class='text-primary' href='https://jina.ai/api-dashboard'>the API Key Management dashboard</a>.",question10:"Why is the first request for some models slow?",answer10:"This is because our serverless architecture offloads certain models during periods of low usage. The initial request activates or 'warms up' the model, which may take a few seconds. After this initial activation, subsequent requests process much more quickly.",question12:"Is user input data used for training your models?",answer12:"We adhere to a strict privacy policy and do not use user input data for training our models. We are also SOC 2 Type I and Type II compliant, ensuring high standards of security and privacy."},Re={toggle_btn:"Keep this panel open on your next visit",warning_title:"Show at startup",warning_message:"This panel will automatically open when you visit jina.ai. You'll need to close it to see the website content. Enable this setting?",text:"Farewell."},Me={estimator:e,book2024:t,key_manager:n,avatar:i,grounding:a,rate_limit:o,tokenizer:r,toc:s,paywall:l,purchase:d,translator:c,promptperfect:u,integrations:m,searchbar:h,SEO_TAG_LINE:p,PRODUCT_DESCRIPTION:g,notice:f,purchase_now:y,copy:b,copy_to_clipboard_success:_,powered_by:v,open_day:w,internship_faq:k,open_day_faq:x,header:A,footer:I,prompt_perfect:T,jina_chat:P,scenex:q,rationale:S,best_banner:R,doc_array:M,jina:L,finetuner:C,hub:z,clip_as_service:E,dalle_flow:B,disco_art:j,think_gpt:W,jcloud:F,"dev-gpt":{description:"Your virtual development team"},langchain_serve:O,vectordb:D,open_gpt:G,jerboa:U,finetuner_plus:J,inference:H,cloud:N,semantic:Y,searchscape:K,reranker:V,reader:Q,model_graph:X,autotune:$,classifier:Z,landing_page:ee,about_us_page:te,internship_page:ne,legal_page:ie,embeddings:ae,spectrum:oe,huggingface:re,impact_snapshots:se,project_status:le,contact_us_page:de,blog_tags:ce,cclicence:ue,models:me,newsroom_page:he,news_page:pe,github:ge,share:fe,faq_button:ye,faq:be,beta:_e,print:ve,download:we,get_new_key:ke,subscribe_system:xe,lab_dialog:Ae,recommender:Ie,finetuning:Te,embedding:Pe,billing_general_faq:qe,api_general_faq:Se,farewell:Re};export{g as PRODUCT_DESCRIPTION,p as SEO_TAG_LINE,te as about_us_page,Se as api_general_faq,$ as autotune,i as avatar,R as best_banner,_e as beta,qe as billing_general_faq,ce as blog_tags,t as book2024,ue as cclicence,Z as classifier,E as clip_as_service,N as cloud,de as contact_us_page,b as copy,_ as copy_to_clipboard_success,B as dalle_flow,Me as default,j as disco_art,M as doc_array,we as download,Pe as embedding,ae as embeddings,e as estimator,be as faq,ye as faq_button,Re as farewell,C as finetuner,J as finetuner_plus,Te as finetuning,I as footer,ke as get_new_key,ge as github,a as grounding,A as header,z as hub,re as huggingface,se as impact_snapshots,H as inference,m as integrations,k as internship_faq,ne as internship_page,F as jcloud,U as jerboa,L as jina,P as jina_chat,n as key_manager,Ae as lab_dialog,ee as landing_page,O as langchain_serve,ie as legal_page,X as model_graph,me as models,pe as news_page,he as newsroom_page,f as notice,w as open_day,x as open_day_faq,G as open_gpt,l as paywall,v as powered_by,ve as print,le as project_status,T as prompt_perfect,u as promptperfect,d as purchase,y as purchase_now,o as rate_limit,S as rationale,Q as reader,Ie as recommender,V as reranker,q as scenex,h as searchbar,K as searchscape,Y as semantic,fe as share,oe as spectrum,xe as subscribe_system,W as think_gpt,s as toc,r as tokenizer,c as translator,D as vectordb};

const e="Forniamo incorporamenti, riclassificatori, lettori LLM e ottimizzatori tempestivi di altissimo livello, un'intelligenza artificiale di ricerca pionieristica per dati multimodali.",i="La tua base di ricerca, potenziata.",a={approach:"Il nostro approccio",approach_connect_dots:"Collegare i punti: Power Users alle imprese",approach_connect_dots_description:"Quindi, perché l'attenzione per gli utenti esperti è essenziale per il nostro modello incentrato sull'azienda? Perché si tratta di stabilire relazioni precoci. Rivolgendoci ora agli utenti esperti, stiamo costruendo ponti verso le imprese che influenzeranno in futuro. È un gioco strategico: un investimento a lungo termine per garantire che la nostra offerta aziendale rimanga al primo posto quando questi utenti esperti salgono a ruoli decisionali all'interno delle organizzazioni.",approach_content1:"Nel mondo in rapida evoluzione dell'IA, le strategie devono essere agili e lungimiranti. Sebbene la nostra offerta principale rimanga incentrata sulle aziende, il panorama dell'IA è cambiato in modi che richiedono un ripensamento del nostro approccio all'acquisizione dei clienti. Ecco perché introdurre gli utenti esperti come punto di ingresso della nostra canalizzazione non è solo innovativo, ma cruciale per la nostra crescita sostenuta nel settore aziendale.",approach_content2:"In Jina AI, la nostra strategia è quella di essere proattivi piuttosto che reattivi. L'inclusione degli utenti esperti come punto di ingresso della canalizzazione garantisce che non solo catturiamo le attuali tendenze del mercato, ma siamo anche strategicamente pronti per la futura crescita aziendale. Il nostro impegno nei confronti delle imprese rimane incrollabile; tuttavia, il nostro approccio per raggiungerli è innovativo, robusto e, soprattutto, lungimirante.",approach_content4:'Tutti vogliono una ricerca migliore. In Jina AI, rendiamo possibile una ricerca migliore fornendo la <span class="text-primary text-bold">Search Foundation</span>, che consiste in Embeddings, Rerankers, Reader e Prompt Ops. Questi componenti lavorano di concerto per rivoluzionare il modo in cui cerchiamo e comprendiamo i dati.',approach_miss_mark:"Perché i MLOps tradizionali mancano il bersaglio",approach_miss_mark_description:"Sebbene l'afflusso di utenti esperti sia significativo, gli strumenti MLOps tradizionali non sono attrezzati per soddisfare le loro esigenze. Questi strumenti ricordano l'uso di un trattore per spostarsi nelle strade della città: sono pesanti e spesso eccessivi. Gli sviluppatori di nuova generazione richiedono strumenti agili e intuitivi che completino il loro rapido ritmo di sviluppo.",approach_new_paradigm:"Tecnologia basata su prompt: un nuovo paradigma",approach_new_paradigm_description:`Il 2023 ha annunciato un cambiamento significativo: l'ascesa della tecnologia basata sul prompt. Semplificando il processo di sviluppo dell'IA, ha democratizzato l'accesso agli strumenti di intelligenza artificiale. Ora, coloro che non hanno una vasta esperienza di programmazione, definiti "utenti esperti", possono dedicarsi allo sviluppo dell'IA senza le ripide curve di apprendimento associate a strumenti come Pytorch, Docker o Kubernetes.

Facendo un parallelo, questo è simile all'evoluzione del personal computer. Inizialmente, solo gli esperti di tecnologia gestivano i computer. Ma con l'avvento delle interfacce user-friendly, potrebbe partecipare un pubblico più ampio. Oggi, con la tecnologia basata sul prompt, stiamo assistendo a una democratizzazione simile nell'IA.`,awards:"Premi e riconoscimenti",berlin:"Berlino, Germania (sede centrale)",berlin_address:"Prinzessinnenstraße 19-20, 10969 Berlino, Germania",berlin_address2:"Geschäftsanschrift: Leipzigerstr. 96, 10117 Berlino, Germania",bj:"Pechino, Cina",bj_address:"Livello 5, Edificio 6, No.48 Haidian West St. Pechino, Cina",brochure_info:"La tua guida alla nostra azienda ti aspetta",description:"Il futuro inizia qui.",download_brochure1:"Scarica l'opuscolo",download_docarray_logo:"Scarica il logo DocArray",download_docarray_logo_desc:"Accedi al logo DocArray, un progetto open source avviato da Jina AI e contribuito alla Linux Foundation nel dicembre 2022. Disponibile in modalità chiara e scura, nei formati PNG e SVG.",download_jina_logo:"Scarica il logo Jina AI",download_jina_logo_desc:"Ottieni il logo Jina AI sia in modalità chiara che scura, disponibile nei formati PNG e SVG. Questo logo è un marchio registrato presso l'Ufficio dell'Unione europea per la proprietà intellettuale (EUIPO).",download_logo:"Scarica loghi",employees:"Dipendenti oggi",empower_developers:"Sviluppatori potenziati",fastApiCaption:"Ha contribuito con oltre $ 20.000 dal 2021.",founded:"Fondato",founded_in:"Fondato nel",investors:"I nostri investitori",linuxFoundationCaption:"Versa un contributo annuo di $ 10.000 a partire dal 2022.",many:"Molti",media:{video:"Intervista video"},mission:"La nostra missione",mission_content1:"Le nostre tecnologie chiave, tra cui prompt-tuning, prompt-serving, model-tuning e model-serving, incarnano il nostro impegno a democratizzare l'accesso all'intelligenza artificiale. Attraverso la nostra iniziativa open source, ci impegniamo a promuovere l'innovazione, la collaborazione e la trasparenza, garantendo soluzioni scalabili, efficienti e robuste. Jina AI è più di una semplice azienda; è una comunità dedicata a consentire alle aziende di affrontare le sfide dinamiche dell'era digitale e prosperare nei loro settori.",mission_content2:"Al centro di Jina AI c'è la nostra missione di essere il portale dell'intelligenza artificiale multimodale per una clientela diversificata, dagli utenti esperti e dagli sviluppatori alle imprese. Crediamo profondamente nella potenza dell'open source e ci dedichiamo alla creazione di strumenti avanzati e accessibili per la comunità AI. Le nostre tecnologie chiave, tra cui prompt-tuning, prompt-serving, embedding-tuning e embedding-serving, incarnano il nostro impegno per la democratizzazione dell'accesso all'intelligenza artificiale. Attraverso la nostra iniziativa open source, ci impegniamo a promuovere l'innovazione, la collaborazione e la trasparenza, garantendo soluzioni scalabili, efficienti e robuste. Jina AI è più di una semplice azienda; è una comunità dedicata a consentire alle aziende di affrontare le sfide dinamiche dell'era digitale e prosperare nei loro settori.",mission_content3:"In Jina AI, la nostra missione è guidare il progresso dell'intelligenza artificiale multimodale attraverso tecnologie innovative di incorporamento e basate su prompt, concentrandoci specificamente su aree come l'elaborazione del linguaggio naturale, l'analisi di immagini e video e l'interazione intermodale dei dati. Questa specializzazione ci consente di fornire soluzioni uniche che trasformano dati complessi provenienti da più fonti in informazioni fruibili e applicazioni rivoluzionarie.",mit_report_title:"Multimodale: la nuova frontiera dell’IA",mit_techreview:"Revisione della tecnologia del MIT",numfocusCaption:"Dona regolarmente ogni mese a partire dal 2022.",office:"I nostri uffici",otherProjectsCaption:"Donati oltre $ 3.000 tramite Github Sponsorship.",our_answer:"Assolutamente, Yann. Ci stiamo lavorando, costruendo ponti verso un futuro di intelligenza artificiale multimodale!",pythonSoftwareFoundationCaption:"Ha fornito una donazione una tantum di $ 10.000 e ha sponsorizzato numerosi eventi PyCon, inclusi quelli in Germania, Italia, Cina e Stati Uniti.",sefo:{layer0:"Applicazioni per l'utente finale",layer1:"RAG/orchestrazione",layer3:"GPU/mobile/edge/elaborazione locale"},segmentFaultCaption:"Ha contribuito con una donazione una tantum di $ 6.000.",show_position:"Come si posizionano le fondamenta della ricerca nell'ecosistema?",stats_1:"Fondata nel febbraio 2020, Jina AI è rapidamente emersa come pioniere globale nella tecnologia AI multimodale. Nell'impressionante lasso di tempo di 20 mesi, abbiamo raccolto con successo 37,5 milioni di dollari, segnando la nostra posizione di forza nel settore dell'IA. La nostra rivoluzionaria tecnologia, open-source su GitHub, ha consentito a oltre 40.000 sviluppatori in tutto il mondo di creare e distribuire senza problemi sofisticate applicazioni multimodali.",stats_2:"Nel 2023, abbiamo fatto passi da gigante nel far progredire gli strumenti di generazione dell'IA basati sulla tecnologia multimodale. Questa innovazione ha beneficiato di oltre 250.000 utenti in tutto il mondo, soddisfacendo una pletora di requisiti aziendali unici. Dalla facilitazione della crescita aziendale e dal miglioramento dell'efficienza operativa all'ottimizzazione dei costi, Jina AI è dedicata a consentire alle aziende di eccellere nell'era multimodale.",stats_4:`Fondata nel 2020, Jina AI è un'azienda leader nel campo dell'intelligenza artificiale per la ricerca. La nostra piattaforma <span class="text-primary text-bold">Search Foundation</span> combina Embeddings, Rerankers e Small Language Models per aiutare le aziende a creare applicazioni di ricerca GenAI e multimodali affidabili e di alta qualità.`,stats_v1:"Cerca/acc",subtitle:"Rivoluzionando la creazione di contenuti attraverso soluzioni generate dall'intelligenza artificiale per sbloccare infinite possibilità. Plasmare il futuro dei contenuti generati dall'intelligenza artificiale e migliorare la creatività umana.",sues_und_sauer:"Suẞ & Sauer",sues_und_sauer_tooltip:"Süß-Sauer, un sapore popolare (ma stereotipato) nella cucina tedesco-cinese, significa agrodolce. È una metafora degli alti e bassi della vita da startup.",sunnyvale_address:"710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, Stati Uniti",sz:"Shenzen, Cina",sz_address:"402 Piano 4, Fu'an Technology Building, Shenzhen, Cina",team:"All'interno del Portale di Jina AI",team_content1:"Da diversi angoli del globo, stiamo costruendo il futuro dell'intelligenza artificiale. Le nostre prospettive distinte arricchiscono il nostro lavoro, innescando innovazioni. All'interno di questo portale, abbracciamo la nostra individualità e perseguiamo con passione i nostri sogni. Benvenuti nel portale del futuro dell'IA.",team_join:"Unisciti a noi",team_size:"Queste foto includono i nostri ex colleghi e stagisti: li apprezziamo tutti.",technologies:"Tecnologie",title:"A proposito di Jina AI",title0:"Il futuro",title1:"Inizia",title2:"Qui",title3:"Inizia qui",understand_our_strength:"Comprendi la nostra forza",understand_our_view2:"Comprendere il fondamento della ricerca",users:"Utenti registrati",value:"I nostri premi",value_content1:"Non ci accontentiamo. Non scendiamo a compromessi. Ci impegniamo per l'eccellenza.",vision:"La nostra missione",vision_content1:"Ispirato dall'intuizione di Yann LeCun che '",vision_content3:`Il futuro dell'intelligenza artificiale è <span class="text-primary text-bold">multimodale</span> e noi ne facciamo parte. Ci rendiamo conto che le aziende devono affrontare sfide nello sfruttamento dei dati multimodali. In risposta, ci impegniamo a favore della <span class="text-primary text-bold">Search Foundation</span> per aiutare le aziende e gli sviluppatori a effettuare ricerche migliori e a utilizzare dati multimodali per la crescita aziendale.`,yannlecun_quote:"Un sistema di intelligenza artificiale addestrato solo su parole e frasi non si avvicinerà mai alla comprensione umana."},o={answer1:"Sì, la stessa chiave API è valida per tutti i prodotti di base di ricerca di Jina AI. Ciò include l'incorporamento, il riclassificazione, il lettore e l'ottimizzazione delle API, con token condivisi tra tutti i servizi.",answer12:"Aderiamo a una rigorosa politica sulla privacy e non utilizziamo i dati di input degli utenti per addestrare i nostri modelli.",answer3:`Sì, l'utilizzo dei token può essere monitorato nella scheda "Acquista token" inserendo la chiave API, consentendoti di visualizzare la cronologia di utilizzo e i token rimanenti.`,answer4:"Se hai smarrito una chiave ricaricata e desideri recuperarla, contatta il supporto AT jina.ai con la tua e-mail registrata per ricevere assistenza.",answer5:"No, le nostre chiavi API non hanno una data di scadenza. Tuttavia, se sospetti che la tua chiave sia stata compromessa e desideri ritirarla o trasferire i suoi token su una nuova chiave, contatta il nostro team di supporto per assistenza.",answer6:'Questo perché la nostra architettura serverless scarica determinati modelli durante i periodi di scarso utilizzo. La richiesta iniziale attiva o "riscalda" il modello, operazione che potrebbe richiedere alcuni secondi. Dopo questa attivazione iniziale, le richieste successive vengono elaborate molto più rapidamente.',question1:"Posso utilizzare la stessa chiave API per incorporare, riclassificare, leggere e ottimizzare le API?",question12:"I dati di input dell'utente vengono utilizzati per addestrare i tuoi modelli?",question3:"Posso monitorare l'utilizzo del token della mia chiave API?",question4:"Cosa devo fare se dimentico la mia chiave API?",question5:"Le chiavi API scadono?",question6:"Perché la prima richiesta per alcuni modelli è lenta?",title:"Domande comuni relative all'API"},n={base_model:"Modello base per la messa a punto",check_data:"Scarica i dati sintetici",check_model:"Scarica il modello ottimizzato",data_size:"Dati sintetici generati",description:"Ottieni incorporamenti ottimizzati per qualsiasi dominio desideri.",description_long:"Dicci semplicemente in quale dominio desideri che i tuoi incorporamenti eccellano e noi forniremo automaticamente un modello di incorporamento pronto all'uso e ottimizzato per quel dominio.",does_it_work_tho:"Ma funziona comunque?",does_it_work_tho_explain:"La regolazione fine automatica mantiene la promessa auto-magica di fornire incorporamenti ottimizzati per qualsiasi dominio tu voglia. Ma funziona veramente? Questo è un dubbio abbastanza ragionevole. Lo abbiamo testato su una varietà di domini e modelli base per scoprirlo. Dai un'occhiata ai risultati selezionati con la ciliegia e con il limone qui sotto.",domain_instruction:"Istruzioni sul dominio",embedding_provider:"Seleziona un modello di incorporamento di base",eval_evaluation:"Validazione",eval_map:"CARTA GEOGRAFICA",eval_mrr:"MRR",eval_ndcg:"NDCG",eval_performance_before_after:"Prestazioni sul set di validazione sintetica prima e dopo la messa a punto",eval_syntheticDataSize:"Totale",eval_test:"Dati reali per i test",eval_training:"Formazione",faq_v1:{answer1:"La funzionalità è attualmente in versione beta e costa 1 milione di token per modello ottimizzato. Puoi utilizzare la chiave API esistente dall'API Embedding/Reranker se dispone di token sufficienti oppure puoi creare una nuova chiave API, che include 1 milione di token gratuiti.",answer10:"Attualmente no. Tieni presente che questa funzionalità è ancora in versione beta. L'archiviazione pubblica dei modelli ottimizzati e dei dati sintetici nell'hub del modello Hugging Face aiuta noi e la comunità a valutare la qualità della formazione. In futuro, prevediamo di offrire un'opzione di archiviazione privata.",answer11:"Poiché tutti i modelli ottimizzati vengono caricati su Hugging Face, puoi accedervi tramite SentenceTransformers semplicemente specificando il nome del modello.",answer12:"Per favore controlla la tua cartella spam. Se ancora non riesci a trovarlo, contatta il nostro team di supporto utilizzando l'indirizzo email che hai fornito.",answer2:"Non è necessario fornire alcun dato di addestramento. Descrivi semplicemente il tuo dominio di destinazione (il dominio per il quale desideri ottimizzare gli incorporamenti) in linguaggio naturale o utilizza un URL come riferimento e il nostro sistema genererà dati sintetici per addestrare il modello.",answer3:"Circa 30 minuti.",answer4:"I modelli ottimizzati e i dati sintetici vengono archiviati pubblicamente nell'hub del modello Hugging Face.",answer5:"Il sistema utilizza l'API Reader per recuperare il contenuto dall'URL. Quindi analizza il contenuto per riassumere il tono e il dominio, che utilizza come linee guida per la generazione di dati sintetici. Pertanto, l'URL dovrebbe essere accessibile al pubblico e rappresentativo del dominio di destinazione.",answer6:`Sì, puoi mettere a punto un modello per una lingua diversa dall'inglese. Il sistema rileva automaticamente la lingua delle istruzioni del tuo dominio e genera di conseguenza dati sintetici. Raccomandiamo inoltre di scegliere il modello base appropriato per la lingua di destinazione. Ad esempio, se scegli come target un dominio tedesco, dovresti selezionare "jina-embeddings-v2-base-de" come modello base.`,answer7:"No, la nostra API di ottimizzazione supporta solo i modelli Jina v2.",answer8:"Al termine del processo di messa a punto, il sistema valuta il modello utilizzando un set di test e segnala le metriche delle prestazioni. Riceverai un'e-mail con i dettagli delle prestazioni prima/dopo su questo set di test. Ti invitiamo inoltre a valutare il modello sul tuo set di test per garantirne la qualità.",answer9:"Il sistema genera dati sintetici integrando le istruzioni del dominio di destinazione fornite con il ragionamento degli agenti LLM. Produce triplette negative rigide, essenziali per l'addestramento di modelli di incorporamento di alta qualità. Per maggiori dettagli, fare riferimento al nostro prossimo documento di ricerca su Arxiv.",question1:"Quanto costa l'API di fine-tuning?",question10:"Posso mantenere privati ​​i miei modelli ottimizzati e i dati sintetici?",question11:"Come posso utilizzare il modello ottimizzato?",question12:"Non ho mai ricevuto l'e-mail con i risultati della valutazione. Cosa dovrei fare?",question2:"Cosa devo inserire? Devo fornire i dati di allenamento?",question3:"Quanto tempo ci vuole per mettere a punto un modello?",question4:"Dove vengono archiviati i modelli ottimizzati?",question5:"Se fornisco un URL di riferimento, come lo utilizza il sistema?",question6:"Posso mettere a punto un modello per una lingua specifica?",question7:"Posso ottimizzare gli incorporamenti non Jina, ad esempio bge-M3?",question8:"Come garantite la qualità dei modelli perfezionati?",question9:"Come si generano dati sintetici?",title:"Domande comuni relative alla regolazione fine automatica"},find_on_hf:"Elenca i modelli ottimizzati",temporarily_unavailable:"Temporaneamente non disponibile. Stiamo aggiornando il nostro sistema di regolazione automatica per offrirti un servizio migliore. Per favore controllare più tardi.",test_on:"Testato su campioni casuali {_dataSize} da {_dataName}",test_performance_before_after:"Prestazioni sul test impostato prima e dopo la messa a punto",title:"API di regolazione fine automatica",total_improve:"Media miglioramento",usage:"Utilizzo",what_is:"Che cos'è la sintonizzazione fine automatica?",what_is_answer_long:"L'ottimizzazione consente di prendere un modello pre-addestrato e adattarlo a un'attività o dominio specifico addestrandolo su un nuovo set di dati. In pratica, trovare dati di addestramento efficaci non è semplice per molti utenti. Una formazione efficace richiede molto più che il semplice inserimento di PDF grezzi e HTML nel modello; ed è difficile farlo bene. La regolazione fine automatica risolve questo problema generando automaticamente dati di addestramento efficaci utilizzando una pipeline di agenti LLM avanzata; e mettere a punto il modello all'interno di un flusso di lavoro ML. Puoi pensarlo come una combinazione di generazione di dati sintetici e AutoML, quindi tutto ciò che devi fare è descrivere il tuo dominio di destinazione in linguaggio naturale e lasciare che il nostro sistema faccia il resto."},t={auth_required:"Autenticazione richiesta per utilizzare la generazione di avatar",classificationError:"Errore nella classificazione dell'immagine. Riprova.",clickToDownload:"Clicca per scaricare SVG",customize:"Personalizza le funzionalità",description:"Genera avatar unici con funzionalità personalizzabili",downloadError:"Errore durante il download dell'avatar",downloadSuccess:"Avatar scaricato con successo",download_success:"Avatar scaricato con successo",error_loading:"Impossibile caricare le risorse dell'avatar. Riprova.",error_processing:"Errore durante l'elaborazione dell'immagine",file_hint:"Formati supportati: JPG, PNG, GIF, WebP",generate:"Genera Avatar",how_does_it_work:"Come funziona?",noImageSelected:"Seleziona prima un'immagine",select_file:"Seleziona un file immagine ritratto",title:"Generatore di avatar",upload_description:"Seleziona un'immagine da convertire in base64 (256x256)",upload_title:"Carica Immagine",usage:"Generazione Avatar"},r={description:"Dal blog al banner, senza i prompt!",example_description:`Alice cominciava a stancarsi molto di stare seduta accanto a sua sorella sulla riva e di non avere niente da fare: una o due volte aveva sbirciato nel libro che sua sorella stava leggendo, ma non conteneva immagini o conversazioni, "e a che serve un libro", pensò Alice "senza immagini o conversazioni?" Quindi stava valutando tra sé (come meglio poteva, perché la giornata calda la faceva sentire molto assonnata e stupida), se il piacere di fare una catena di margherite sarebbe valsa la pena di alzarsi e raccogliere le margherite, quando all'improvviso un Bianconiglio con gli occhi rosa le corse vicino.`,example_title:"Le avventure di Alice nel paese delle meraviglie - Capitolo 1"},l="Beta",s={answer10:`Offriamo un'accogliente prova gratuita ai nuovi utenti, che include un milione di token da utilizzare con qualsiasi dei nostri modelli, facilitata da una chiave API generata automaticamente. Una volta raggiunto il limite di token gratuiti, gli utenti possono facilmente acquistare token aggiuntivi per le proprie chiavi API tramite la scheda "Acquista token".`,answer13:"No, i token non vengono detratti per le richieste non riuscite.",answer14:"I pagamenti vengono elaborati tramite Stripe, che supporta una varietà di metodi di pagamento tra cui carte di credito, Google Pay e PayPal per la tua comodità.",answer15:"Sì, al momento dell'acquisto dei token verrà emessa una fattura all'indirizzo e-mail associato al tuo account Stripe.",answer9:"Il nostro modello di prezzo si basa sul numero totale di token elaborati, consentendo agli utenti la flessibilità di allocare questi token su qualsiasi numero di frasi, offrendo una soluzione economicamente vantaggiosa per diversi requisiti di analisi del testo.",question10:"È disponibile una prova gratuita per i nuovi utenti?",question13:"Vengono addebitati i token per le richieste non riuscite?",question14:"Quali metodi di pagamento sono accettati?",question15:"È disponibile la fatturazione per gli acquisti di token?",question9:"La fatturazione è basata sul numero di frasi o richieste?",title:"Domande comuni relative alla fatturazione"},c={all:"Tutto",events:"Evento",featured:"In primo piano",insights:"Opinione","knowledge-base":"Base di conoscenza",latest:"Ultimo",press:"comunicato stampa",releases:"Aggiornamento del software","tech-blog":"Blog tecnico"},d={caption:'Scopri "Re·Search", il nostro annuario dal design accattivante che presenta i nostri migliori articoli di ricerca e modelli di fondazioni di ricerca del 2024.',order_now:"Ordina ora"},u={api_free_trial:"Chiave API gratuita",api_paid:"Chiave API a pagamento",api_paid_or_free:"Stai utilizzando una chiave API a pagamento o una chiave di prova gratuita?",are_you:"Sei:",commercial_contact_sales:"Questo è commerciale. Contatta il nostro team di vendita.",contact_sales_for_licensing:"Contatta il nostro team commerciale per le licenze.",csp_user:"Stai utilizzando le nostre immagini modello ufficiali su AWS e Azure?",educational_teaching:"Un istituto scolastico che lo utilizza per l'insegnamento?",for_profit_internal_use:"Un'azienda a scopo di lucro che lo utilizza internamente?",free_use:"Puoi utilizzare i modelli liberamente.",government_public_services:"Un ente governativo che lo utilizza per servizi pubblici?",is_use_commercial:"Il tuo utilizzo è commerciale?",may_be_commercial_contact:"Potrebbe essere commerciale. Vi preghiamo di contattarci per chiarimenti.",no:"NO",no1:"NO",no2:"NO",no3:"NO",no_restrictions:"Nessuna restrizione. Utilizza secondo il tuo attuale accordo.",no_restrictions_apply:"Non si applicano restrizioni.",non_commercial_free_use:"Questo non è commerciale. Puoi usare i modelli liberamente.",non_profit_ngo_mission:"Un'organizzazione non-profit o una ONG che lo utilizza per la propria missione?",not_sure:"Non è sicuro",personal_hobby_projects:"Lo usi per progetti personali o hobbistici?",product_service_sale:"Lo stai utilizzando in un prodotto o servizio che vendi?",title:"Autoverifica della licenza CC BY-NC",trial_key_restrictions:"La chiave di prova gratuita può essere utilizzata solo per scopi non commerciali. Acquista un pacchetto a pagamento per uso commerciale.",typically_non_commercial_check:"In genere non si tratta di un'attività commerciale, ma in caso di dubbi contattateci.",typically_non_commercial_free_use:"Questo è tipicamente non commerciale. Puoi usare i modelli liberamente.",using_api_or_cloud:"Stai utilizzando la nostra API ufficiale o le immagini ufficiali su Azure o AWS?",using_cc_by_nc_models:"Stai utilizzando questi modelli?",yes:"SÌ",yes1:"SÌ",yes2:"SÌ",yes3:"SÌ"},m={access:"Accesso pubblico",access_explain:"I classificatori pubblici possono essere utilizzati da chiunque abbia <code>classifier_id</code> e il loro utilizzo consumerà la quota di token del chiamante anziché la tua. I classificatori privati sono accessibili solo a te.",access_private:"Privato",access_public:"Pubblico",api_delete:"Elimina classificatore",api_delete_explain:"Elimina un classificatore in base al suo ID.",api_list:"Elenca classificatori",api_list_explain:"Elenca tutti i classificatori che hai creato.",classifier_id:"ID classificatore",classify_inputs:"Input per classificare",classify_inputs_explain:"Per il testo, può essere una frase fino a 8192 token. Per le immagini, può essere un URL o un'immagine codificata in base64.",classify_labels:"Etichette dei candidati",classify_labels_explain:"Gli input saranno categorizzati in queste etichette. Possono essere fino a 256 classi. Utilizza etichette semantiche per prestazioni migliori.",compare_table:{access_control:"Controllo degli accessi",classifier_id_required:"ID classificatore obbligatorio",continuous_updates:"Aggiornamenti continui del modello",default_solution:"Soluzione predefinita per la classificazione generale",feature:"Caratteristica",few_shot:"Pochi colpi",image_multi_lingual_support:"Supporto multimodale e multilingue",labels_required_classify:"Etichette richieste in /classify",labels_required_train:"Etichette richieste in /train",max_classes:"Classi massime",max_classifiers:"Classificatori massimi",max_inputs_request:"Input massimi per richiesta",max_token_length:"Lunghezza massima del token per input",na:"N / A",no:"NO",out_of_domain_solution:"Per dati esterni al dominio v3/clip-v1 o dati sensibili al tempo",primary_use_case:"Caso d'uso primario",semantic_labels_required:"Etichette semantiche richieste",state_management:"Gestione dello Stato",stateful:"Con stato",stateless:"Apolide",token_count:"{count} token",training_data_required:"Dati di formazione richiesti",yes:"SÌ",zero_shot:"Colpo zero"},create_classifier:"Nuovo classificatore a pochi colpi",create_classifier_explain:"Crea un nuovo classificatore a pochi scatti e addestralo con esempi etichettati.",description:"Classificazione zero-shot e few-shot per immagini e testo.",description_long:"Prova il nostro API playground per vedere come funziona il nostro classificatore.",description_long1:"Classificatore zero-shot e few-shot ad alte prestazioni per dati multimodali e multilingue.",explain:"Classifier è un servizio API che categorizza testo e immagini utilizzando modelli di incorporamento (<code>jina-embeddings-v3</code> e <code>jina-clip-v1</code>), supportando sia la classificazione zero-shot senza dati di addestramento sia l'apprendimento few-shot con esempi minimi.",faq_v1:{answer1:"Zero-shot richiede etichette semantiche durante la classificazione e nessuna durante l'addestramento, mentre few-shot richiede etichette durante l'addestramento ma non la classificazione. Ciò significa che zero-shot è migliore per esigenze di classificazione flessibili e immediate, mentre few-shot è migliore per categorie fisse e specifiche del dominio che possono evolversi nel tempo.",answer10:"Sì, puoi scegliere tra <code>jina-embeddings-v3</code> per la classificazione del testo (particolarmente adatta per il multilingua) e <code>jina-clip-v1</code> per la classificazione multimodale. Nuovi modelli come <code>jina-clip-v2</code> saranno automaticamente disponibili tramite l'API quando saranno rilasciati.",answer2:"<code>num_iters</code> controlla l'intensità dell'allenamento: valori più alti rafforzano esempi importanti mentre valori più bassi riducono al minimo l'impatto di dati meno affidabili. Può essere utilizzato per implementare l'apprendimento basato sul tempo assegnando agli esempi recenti conteggi di iterazioni più elevati, rendendolo prezioso per l'evoluzione dei modelli di dati.",answer3:"I classificatori pubblici possono essere utilizzati da chiunque abbia <code>classifier_id</code>, consumando la propria quota di token. Gli utenti non possono accedere ai dati di training o alla configurazione e non possono vedere le richieste di classificazione degli altri, consentendo una condivisione sicura dei classificatori.",answer4:"Few-shot richiede 200-400 esempi di training per superare la classificazione zero-shot. Sebbene alla fine raggiunga una maggiore accuratezza, ha bisogno di questo periodo di riscaldamento per diventare efficace. Zero-shot fornisce prestazioni costanti immediatamente senza dati di training.",answer5:"Sì, l'API supporta query multilingue utilizzando <code>jina-embeddings-v3</code> e classificazione multimodale (testo/immagine) utilizzando <code>jina-clip-v1</code>, con supporto per URL o immagini codificate in base64 nella stessa richiesta.",answer6:"Zero-shot supporta 256 classi senza limiti di classificatori, mentre few-shot è limitato a 16 classi e 16 classificatori. Entrambi supportano 1.024 input per richiesta e 8.192 token per input.",answer7:"La modalità Few-shot consente l'aggiornamento continuo tramite l'endpoint <code>/train</code> per adattarsi ai modelli di dati in evoluzione. È possibile aggiungere gradualmente nuovi esempi o classi quando cambia la distribuzione dei dati, senza ricostruire l'intero classificatore.",answer8:"L'API utilizza l'apprendimento online one-pass: gli esempi di training aggiornano i pesi del classificatore ma non vengono archiviati in seguito. Ciò significa che non è possibile recuperare i dati di training storici, ma garantisce privacy ed efficienza delle risorse.",answer9:"Inizia con zero-shot per risultati immediati e quando hai bisogno di una classificazione flessibile con etichette semantiche. Passa a few-shot quando hai 200-400 esempi, hai bisogno di una maggiore accuratezza o devi gestire dati specifici del dominio/sensibili al tempo.",question1:"Qual è la differenza tra le etichette zero-shot e few-shot?",question10:"Posso usare modelli diversi per lingue/attività diverse?",question2:"A cosa serve num_iters e come dovrei utilizzarlo?",question3:"Come funziona la condivisione del classificatore pubblico?",question4:"Di quanti dati ho bisogno affinché few-shot funzioni bene?",question5:"Può gestire più lingue e sia testo che immagini?",question6:"Quali sono i limiti rigorosi che dovrei conoscere?",question7:"Come posso gestire le modifiche dei dati nel tempo?",question8:"Cosa succede ai miei dati di allenamento dopo averli inviati?",question9:"Zero-shot vs few-shot: quando usare quale?",title:"Domande frequenti relative al classificatore"},more:"Di più",num_iters:"Iterazioni di formazione",num_iters_explain:"Controlla l'intensità dell'allenamento: valori più alti migliorano la precisione negli esempi correnti ma aumentano il costo dei token. Il valore predefinito di 10 in genere funziona bene.",read_notes:"Leggi le note di rilascio",select_classifier_or_model:"Seleziona un classificatore o un modello di incorporamento",task_classify:"Classificare",task_classify_explain:"Utilizzare un classificatore zero-shot o few-shot per categorizzare testo o immagini in classi definite.",task_manage:"Maneggio",task_manage_explain:"Elenca o elimina i tuoi classificatori di pochi scatti.",task_select:"Seleziona un'attività",task_train:"Treno",task_train_explain:"Crea o aggiorna un classificatore di pochi scatti con esempi etichettati.",title:"API del classificatore",train_inputs:"Dati di formazione",train_inputs_explain:"Esempi di testo o immagini con etichette per l'addestramento. È possibile aggiornare gradualmente il classificatore con nuovi esempi ed etichette nel tempo.",train_label:"Etichetta",what_is:"Che cos'è il classificatore?",when_to_use_what:"Quando usare il metodo zero-shot o few-shot?",when_to_use_what_explain:"Utilizza la classificazione zero-shot come soluzione predefinita per risultati immediati su attività di classificazione generali con un massimo di 256 classi, mentre l'apprendimento few-shot è più adatto quando si gestiscono dati specifici di dominio al di fuori delle conoscenze dei modelli di incorporamento o quando è necessario gestire dati sensibili al fattore tempo che richiedono continui aggiornamenti del modello."},p={description:"Incorpora immagini e frasi in vettori di lunghezza fissa con CLIP"},g={description:"Piattaforma di cloud hosting per applicazioni AI multimodali"},z={agreement:"Inviando, confermi di accettare il trattamento dei tuoi dati personali da parte di Jina AI come descritto nell'",anything_else:"Raccontaci di più sulla tua idea",cc_by_nc:"Richiedi l'uso commerciale dei modelli CC BY-NC",cc_by_nc_description:"I nostri ultimi modelli sono in genere con licenza CC BY-NC. Per uso commerciale, accedi tramite la nostra API, Azure Marketplace o AWS SageMaker. Seleziona questa casella per l'uso in locale al di fuori di questi canali.",company:"Organizzazione",company_size:"Dimensioni dell'organizzazione",company_website:"Sito web dell'organizzazione",company_website_placeholder:"URL della home page o del profilo LinkedIn della tua azienda",country:"Paese",department:"Dipartimento",description:"Fai crescere la tua attività con Jina AI.",drop_area_for_image:"Trascina qui le tue immagini",faq:"FAQ",feedback_sent:"Inviato! Ti contatteremo al più presto.",field_required:"Il campo è obbligatiorio",get_api_key:"Come posso ottenere la mia chiave API?",image_upload:"Allega immagini",image_validate:"Puoi allegare fino a {_num} immagini. Solo JPG, JPEG, PNG, WEBP.",impact_snapshots:"Istantanee di impatto",invalid_date_format:"Formato data non valido. Utilizza il formato GG-MM-AAAA.",invalid_email:"L'email non è valida",invalid_number:"Numero non valido. Si prega di inserire di nuovo",invalid_url:"L'URL non è valido",name:"Nome",nc_check:"Ho bisogno di una licenza commerciale?",other_questions:"Altre domande",preferred_models:"A quali modelli sei interessato?",preferred_products:"A quali prodotti sei interessato?",pricing:"Prezzi?",priority:"Supporto prioritario per gli utenti paganti",private_statement:"Informativa sulla Privacy",rate_limit:"Qual è il limite di tariffa?",role:"Ruolo",self_check:"Autocontrollo",sending_feedback:"Invio in corso...",shortcut:"Scorciatoia",submit:"Invia",submit_failed:"Invio non riuscito. Per favore riprova più tardi.",submit_success:"Grazie per la vostra presentazione. Vi risponderemo al più presto.",subtitle:"Jina AI, leader nell'IA multimodale, eccelle nell'ottimizzazione dei modelli, nel servizio dei modelli, nella messa a punto rapida e nel servizio rapido. Sfruttando le tecnologie native del cloud come Kubernetes e le architetture serverless, forniamo soluzioni robuste, scalabili e pronte per la produzione. Con esperienza in modelli linguistici di grandi dimensioni, testo, immagini, video, comprensione audio, ricerca neurale e arte generativa, forniamo strategie innovative e a prova di futuro per elevare la tua attività.",subtitle1:"Jina AI, leader nell'intelligenza artificiale multimodale, eccelle nell'ottimizzazione dell'incorporamento, dell'incorporamento, dell'ottimizzazione e del servizio tempestivo. Sfruttando tecnologie native del cloud come Kubernetes e architetture serverless, forniamo soluzioni robuste, scalabili e pronte per la produzione. Con esperienza in modelli linguistici di grandi dimensioni, testo, immagini, video, comprensione dell'audio, ricerca neurale e intelligenza artificiale generativa, forniamo strategie innovative e a prova di futuro per far crescere il tuo business.",subtitle2:"Esplora Jina AI, l'avanguardia dell'IA multimodale. Eccelliamo nell'incorporamento e nell'immediatezza delle tecnologie, utilizzando soluzioni native del cloud come Kubernetes per sistemi robusti e scalabili. Specializzati in modelli linguistici di grandi dimensioni e nell'elaborazione dei media, offriamo strategie aziendali innovative e pronte per il futuro con la nostra competenza avanzata in materia di intelligenza artificiale.",title:"Contatta le vendite",trusted_by:"Scelto da",turn_on_volume:"Alza il volume",work_email:"E-mail di lavoro"},v="copia",f="Copiato negli appunti",_={description:"Un flusso di lavoro human-in-the-Loop per la creazione di immagini HD dal testo"},b={description:"Crea avvincenti opere d'arte Disco Diffusion in una riga di codice"},h={description:"La struttura dei dati per i dati multimodali"},I="Scarica l'attestazione SOC 2 Tipo 1",k={"11B tokens":"11 miliardi","11B tokens_intuition1":"Simile alla lettura di tutti gli articoli in lingua inglese su Wikipedia.","11B tokens_targetUser":"Distribuzione della produzione","1B tokens":"1 miliardo","1B tokens_intuition1":`Più o meno come leggere l'opera completa di Shakespeare e l'intera serie "Harry Potter".`,"1B tokens_targetUser":"Sviluppo del prototipo","1M tokens":"1 milione","1M tokens_intuition1":`Equivale a leggere l'intero testo de "Lo Hobbit" e "Il Grande Gatsby".`,"1M tokens_targetUser":"Esperimento di giocattoli","1M_free":"1 milione di token gratuiti","1M_free_description":"Goditi la tua nuova chiave API con token gratuiti, senza bisogno di carta di credito.","2_5B tokens":"Gettoni da 2,5 miliardi","2_5B tokens_intuition1":`Paragonabile a trascrivere 1.000 volte ogni parola pronunciata nella trilogia del film "Il Signore degli Anelli".
`,"3p_integration":"Con <b>{_numPartners}</b> servizi di terze parti","3p_integration_desc":"Integra la nostra base di ricerca con i tuoi servizi esistenti. I nostri partner hanno creato connettori per la nostra API, semplificando l'utilizzo dei nostri modelli nelle tue applicazioni.","500M tokens":"500 milioni di gettoni","500M tokens_intuition1":'È simile a guardare ogni episodio di "I Simpson" dalla stagione 1 alla stagione 30.',"59B tokens":"59 miliardi di gettoni","59B tokens_intuition1":"Uguale a tutti i tweet pubblicati in tutto il mondo in un periodo di due giorni.","5_5B tokens":"Gettoni da 5,5 miliardi","5_5B tokens_intuition1":"Equivale a leggere l'intero testo dell'Enciclopedia Britannica.",Free1M:"Gettoni da 1 milione",add_pair:"Nuovo",add_time_explain:"Data in cui questo modello è stato aggiunto alla Search Foundation.",api_integration_short:"La nostra API di incorporamento è integrata nativamente con vari database rinomati, archivi di vettori, framework RAG e LLMOps.",api_integrations:"Integrazioni API",api_key_update_message:"Sostituendo la tua vecchia chiave API, la nuova chiave apparirà nell'interfaccia utente ogni volta che visiti jina.ai. Le ricariche future si applicheranno a questa nuova chiave. La tua vecchia chiave rimane valida, quindi se hai intenzione di usarla di nuovo, conservala in modo sicuro.",api_key_update_title:"Sostituzione della chiave API",auto_recharge:"Ricarica automatica quando i gettoni sono bassi",auto_recharge_confirm_message:"Vuoi davvero disattivare la ricarica automatica? Questo impedirà le ricariche automatiche quando il tuo saldo token è basso.",auto_recharge_confirm_title:"Disattiva la ricarica automatica",auto_recharge_description:"Consigliato per un servizio ininterrotto in produzione. Quando il saldo del tuo token è inferiore alla soglia impostata, ricaricheremo automaticamente la tua carta di credito per lo stesso importo dell'ultima ricarica. Se hai acquistato più pacchetti nell'ultima ricarica, ricaricheremo solo un pacchetto.",auto_recharge_enable:"Hai abilitato la ricarica automatica sui token bassi",auto_recharge_enable_message:"Per abilitare la ricarica automatica, acquista un pacchetto con la ricarica automatica impostata su vero.",auto_recharge_enable_title:"Abilita la ricarica automatica",auto_request:"Anteprima automatica",auto_request_tooltip:'Visualizza in anteprima automaticamente la risposta API quando cambi il modello, utilizzando centinaia di token dalla tua chiave API. Disattiva per inviare manualmente una richiesta facendo clic su "Ricevi risposta".',autostart:"L'incorporamento verrà avviato automaticamente dopo un breve ritardo",base64_description:"Gli incorporamenti vengono restituiti come stringa con codifica base64. Più efficiente per la trasmissione.",batch_job:"Lavoro in batch",batch_upload_hint:"Utilizzeremo la chiave API e il modello seguente per elaborare i documenti.","bge-base-en-v1_5_description":"Un robusto modello inglese che bilancia prestazioni ed efficienza per un utilizzo versatile.","bge-base-en_description":"Un modello inglese equilibrato progettato per prestazioni solide e affidabili.","bge-base-zh-v1_5_description":"Un modello cinese a tutto tondo che bilancia capacità ed efficienza.","bge-base-zh_description":"Un modello cinese versatile che unisce efficienza e prestazioni robuste.","bge-large-en-v1_5_description":"Un potente modello inglese che offre incastri di alto livello con una qualità eccezionale.","bge-large-en_description":"Un modello inglese dalle prestazioni elevate, realizzato per incorporamenti di alta qualità.","bge-large-zh-v1_5_description":"Un modello cinese ad alta capacità che offre incorporamenti superiori e dettagliati.","bge-large-zh_description":"Un modello cinese ad alte prestazioni ottimizzato per incorporamenti di alto livello.","bge-m3_description":"Un modello multilingue versatile che offre funzionalità estese e incorporamenti di alta qualità.","bge-small-en-v1_5_description":"Un modello inglese semplificato che offre incorporamenti efficienti e di alta qualità.","bge-small-en_description":"Un modello inglese efficiente per incorporamenti snelli e accurati.","bge-small-zh-v1_5_description":"Un modello cinese compatto che fornisce incorporamenti agili e precisi.","bge-small-zh_description":"Un modello cinese agile per incorporamenti efficienti e precisi.",binary_description:"Gli incorporamenti sono impacchettati come int8. Molto più efficiente per l'archiviazione, la ricerca e la trasmissione.",bulk:"Incorporamento batch",bulk_embedding_failed:"Impossibile creare il processo di incorporamento batch",buy_more_quota:"Ricarica questa chiave API con più token",buy_poster:"Acquista una copia cartacea",cancel_button:"Annulla",click_upload_btn_above:"Fai clic sul pulsante di caricamento in alto per iniziare.",clip_v2_description:"jina-clip-v2 è un modello in stile CLIP da 0,9B che apporta tre importanti innovazioni: supporto multilingue per 89 lingue, elevata risoluzione delle immagini a 512x512 e apprendimento della rappresentazione Matryoshka per incorporamenti troncati.",clip_v2_title:"clip-v2: Incorporamenti multimodali multilingue",code:"codice",colbert_dimensions_explain:"La dimensione dell'incorporamento per token.",compatible:"Modalità compatibile",compatible_explain:"Segue lo stesso formato di richiesta dei nostri modelli di incorporamento del testo. Ciò consente di passare da un modello all'altro senza modificare la richiesta. Nota, l'input di immagini non è supportato in questa modalità.",cosine_similarity:"Somiglianza del coseno",debugging:"Test",delete_pair:"Eliminare",description:"@:landing_page.embedding_desc1",dimensions:"Dimensioni di uscita",dimensions_error:"La dimensione deve essere compresa tra 1 e 1024.",dimensions_explain:"Le dimensioni ridotte consentono un'archiviazione e un recupero efficienti, con un impatto minimo grazie alla rappresentazione a matrioska.",dimensions_warning:"Per migliorare le prestazioni, consigliamo di mantenere la dimensione superiore a {_minDimension}.",document:"Documento",download:"Scaricamento",edit_text1_text:"Modifica il testo a sinistra",edit_text2_text:"Modifica il testo corretto",embedding_done:"{_Count} frasi incorporate correttamente.",embedding_none_description:"Non utilizzare alcun modello di incorporamento",example_inputs:"Ingressi di esempio",faq:"@:contattaci_pagina.faq",faqs_v2:{answer0:"Per informazioni dettagliate sui nostri processi di formazione, fonti di dati e valutazioni, fare riferimento al nostro rapporto tecnico disponibile su arXiv.",answer1:"Ogni utente può effettuare fino a 100 richieste al secondo, pari a 204.800 frasi di input al secondo.",answer17:"Attualmente stiamo sviluppando incorporamenti multimodali che elaboreranno congiuntamente testo, immagini e audio. Gli aggiornamenti saranno annunciati presto!",answer18:"Per domande sulla messa a punto dei nostri modelli con dati specifici, contattaci per discutere le tue esigenze. Siamo aperti a esplorare come i nostri modelli possono essere adattati per soddisfare le vostre esigenze.",answer19:"Sì, i nostri servizi sono disponibili sul marketplace AWS e siamo in fase di espansione sui marketplace Azure e GCP. Se hai esigenze particolari, contattaci all'ufficio vendite AT jina.ai.",answer3:"I nostri modelli supportano inglese, tedesco, spagnolo, cinese e vari linguaggi di programmazione. Per maggiori dettagli si rimanda alla nostra pubblicazione sui modelli bilingui.",answer4:`I nostri modelli consentono una lunghezza di input fino a 8192 token, che è significativamente più alta rispetto alla maggior parte degli altri modelli. Un token può variare da un singolo carattere, come "a", a un'intera parola, come "mela". Il numero totale di caratteri che possono essere immessi dipende dalla lunghezza e dalla complessità delle parole utilizzate. Questa funzionalità di input estesa consente ai nostri modelli jina-embeddings-v2 di eseguire analisi del testo più complete e ottenere una maggiore precisione nella comprensione del contesto, in particolare per dati testuali estesi.`,answer5:"Una singola chiamata API può elaborare fino a 2048 frasi o testi, facilitando un'analisi approfondita del testo in un'unica richiesta.",answer6:"Puoi utilizzare <code>url</code> o <code>bytes</code> nel campo <code>input</code> della richiesta API. Per <code>url</code>, fornisci l'URL dell'immagine che desideri elaborare. Per <code>bytes</code>, codifica l'immagine in formato base64 e includila nella richiesta. Il modello restituirà gli incorporamenti dell'immagine nella risposta.",answer7:"Secondo la classifica MTEB, il nostro modello Base compete strettamente con il text-embedding-ada-002 di OpenAI, mostrando in media prestazioni comparabili. Inoltre, il nostro modello Base eccelle in diversi compiti, tra cui classificazione, classificazione di coppie, riclassificazione e riepilogo, superando il modello di OpenAI.",answer8:"La transizione è semplificata, poiché il nostro endpoint API, https://api.jina.ai/v1/embeddings, corrisponde agli schemi JSON di input e output del modello text-embeddings-ada-002 di OpenAI. Questa compatibilità garantisce che gli utenti possano facilmente sostituire il modello OpenAI con il nostro quando utilizzano l'endpoint OpenAI.",answer9:`I token vengono calcolati in base alla lunghezza del testo e alla dimensione dell'immagine. Per il testo nella richiesta, i token vengono conteggiati in modo standard. Per l'immagine nella richiesta, vengono eseguiti i seguenti passaggi:
1. Dimensione tessera: ogni immagine è divisa in tessere di dimensione 224x224 pixel.
	2. Copertura: viene calcolato il numero di tessere necessarie per coprire completamente l'immagine in ingresso. Anche se le dimensioni dell'immagine non sono perfettamente divisibili per 224, conteremo le tessere parziali come tessere intere.
	3. Tessere totali: il numero totale di tessere che coprono l'immagine determina il costo. Ad esempio, se un'immagine è 500x500 pixel, sarebbe coperta da riquadri 3x3, risultando in 9 riquadri.
	4. Calcolo del costo: Ogni tessera contribuisce al costo finale dell'elaborazione dell'immagine. Il costo per tessera è di 1000 gettoni.

Esempio:
Per un'immagine con dimensioni 500x500 pixel:

	• L'immagine è divisa in riquadri da 224x224 pixel.
	• Il numero totale di tessere richieste è 3 (orizzontale) x 3 (verticale) = 9 tessere.
	• Il costo sarà 9*1000 = 9000 token`,question0:"Come sono stati addestrati i modelli jina-embeddings-v2?",question1:"Quante richieste API posso effettuare al secondo?",question17:"Fornite modelli per incorporare immagini o audio?",question18:"I modelli Jina Embedding possono essere ottimizzati con dati privati ​​o aziendali?",question19:"I tuoi endpoint possono essere ospitati privatamente su AWS, Azure o GCP?",question3:"Quali lingue supportano i vostri modelli?",question4:"Qual è la lunghezza massima per una singola frase inserita?",question5:"Qual è il numero massimo di frasi che posso includere in una singola richiesta?",question6:"Come posso inviare immagini al modello jina-clip-v1?",question7:"Come si confrontano i modelli Jina Embeddings con il modello text-embedding-ada-002 di OpenAI?",question8:"Quanto è fluida la transizione da text-embedding-ada-002 di OpenAI alla tua soluzione?",question9:"Come vengono calcolati i token quando si utilizza jina-clip-v1?",title:"Domande comuni relative agli incorporamenti"},feature_8k1:"8192 lunghezza del token",feature_8k_description1:"Pioniere del primo modello di incorporamento open source con una lunghezza di 8192 token, che consente la rappresentazione di un intero capitolo in un unico vettore.",feature_cheap:"20 volte più economico",feature_cheap_v1:"5 volte più economico",feature_cheap_v1_description1:"Inizia con prove gratuite e goditi una struttura dei prezzi semplice. Ottieni l'accesso a potenti incorporamenti a solo il 20% del costo di OpenAI.",feature_multilingual:"Offre modelli bilingui tedesco-inglese, cinese-inglese, tra gli altri, ideali per applicazioni multilingue.",feature_on_premises:"La privacy prima di tutto",feature_on_premises_description1:"Distribuisci senza problemi i nostri modelli di incorporamento direttamente nel tuo Virtual Private Cloud (VPC). Attualmente supportato su AWS Sagemaker, con prossime integrazioni per Microsoft Azure e Google Cloud Platform. Per implementazioni Kubernetes personalizzate, contatta il nostro team di vendita per assistenza specializzata.",feature_on_premises_description2:"Distribuisci i modelli Jina Embeddings in AWS Sagemaker e presto anche in Microsoft Azure e nei servizi cloud di Google, oppure contatta il nostro team di vendita per ottenere distribuzioni Kubernetes personalizzate per il tuo cloud privato virtuale e i server locali.",feature_on_premises_description3:"Distribuisci i modelli Jina Embeddings in AWS Sagemaker e Microsoft Azure, e presto anche nei servizi Google Cloud, oppure contatta il nostro team di vendita per ottenere distribuzioni Kubernetes personalizzate per il tuo cloud privato virtuale e i server locali.",feature_on_premises_description4:"Distribuisci modelli Jina Embedding e Reranker in locale utilizzando AWS SageMaker, Microsoft Azure o Google Cloud Services, garantendo che i tuoi dati rimangano saldamente sotto il tuo controllo.",feature_solid:"Migliore della classe",feature_solid_description1:"Sviluppato dalla nostra ricerca accademica all'avanguardia e rigorosamente testato rispetto ai modelli SOTA per garantire prestazioni senza pari.",feature_top_perform1:"Integrazione senza problemi",feature_top_perform_description1:"Pienamente compatibile con l'API di OpenAI. Si integra facilmente con oltre 10 database vettoriali e sistemi RAG per un'esperienza utente fluida.",file_required:"Il file è obbligatorio",file_size_exceed:"Supera la dimensione massima del file {_size}",file_type_not_supported:"Tipo di file non supportato",fill_example:"Compila un esempio",float_description:"Gli incorporamenti vengono restituiti come un elenco di numeri a virgola mobile. Il più comune e facile da usare.",free:"Gratuito",generate_api_key_error:"La generazione della chiave API non è riuscita.",generating_visualization:"Generazione della visualizzazione...",get_new_key_button:"Ottieni una nuova chiave",get_new_key_button_explain:"La scelta di una nuova chiave comporterà la perdita della cronologia di utilizzo associata alla vecchia chiave.",get_new_key_survey:"Compila il sondaggio, aiutaci a comprendere il tuo utilizzo e ottieni una nuova chiave API gratuitamente!",includes:"Gettoni validi per:",index_and_search:"Indicizza e cerca",index_and_search1:"Indicizza e cerca",input:"Richiesta",input_api_key_error1:"La tua chiave API non è valida!",input_length:"Lunghezza immessa",input_type:"Incorpora come documento/query",input_type_explain:"Lo stesso input può fungere da query o da incorporamento di un documento, a seconda del suo ruolo di ricerca.",integrate:"Integrare","jina-clip-v1_description":"Modelli di incorporamento multimodale per immagini e testo inglese","jina-clip-v2_description":"Incorporamenti multimodali multilingue per testi e immagini","jina-colbert-v1-en_description":"ColBERT migliorato con token di lunghezza 8K per attività di incorporamento e riclassificazione","jina-colbert-v2_description":"Il miglior ColBERT multilingue con le massime prestazioni nell'incorporamento e nella riclassificazione","jina-embedding-b-en-v1_description":"La prima versione del modello Jina Embedding, la OG.","jina-embeddings-v2-base-code_description":"Ottimizzato per la ricerca di codici e stringhe di documenti","jina-embeddings-v2-base-de_description":"Incorporamenti bilingue tedesco-inglese con prestazioni SOTA","jina-embeddings-v2-base-en_description":"Alla pari con text-embedding-ada002 di OpenAI","jina-embeddings-v2-base-es_description":"Incorporamenti bilingue spagnolo-inglese con prestazioni SOTA","jina-embeddings-v2-base-zh_description":"Incorporamenti bilingue cinese-inglese con prestazioni SOTA","jina-embeddings-v2-small-en_description":"Ottimizzato per bassa latenza e ingombro di memoria","jina-embeddings-v3_description":"Modello di incorporamento multilingue di frontiera con prestazioni SOTA","jina-reranker-v1-base-en_description":"Il nostro primo modello di riclassificazione che massimizza la ricerca e la pertinenza RAG","jina-reranker-v1-tiny-en_description":"Il modello di riclassificazione più veloce, più adatto per classificare in modo affidabile un gran numero di documenti","jina-reranker-v1-turbo-en_description":"La migliore combinazione di elevata velocità di inferenza e punteggi di pertinenza accurati","jina-reranker-v2-base-multilingual_description":"L'ultimo e migliore modello di riclassificazione con supporto multilingue, chiamata di funzioni e ricerca di codice.",key:"Chiave API",key_enter_placeholder:"Inserisci la tua chiave API",key_enter_placeholder_to_topup:"Inserisci la chiave API che desideri ricaricare",key_to_top_up:'Hai una chiave API diversa da ricaricare? Incollala qui sopra e clicca su "Salva".',key_warn:"Assicurati di conservare la chiave API in un luogo sicuro. Altrimenti dovrai generare una nuova chiave",key_warn_v2:"Questa è la tua chiave unica. Conservala in modo sicuro!",language_explain:"Questo modello supporta al meglio la lingua {_lingual}.",last_7_days:"Utilizzo",late_chunking:"Chunking tardivo",late_chunking_explain:"Applicare la tecnica del late chunking per sfruttare le capacità del modello in contesti lunghi per generare incorporamenti di chunk contestuali.",learn_more:"Saperne di più",learn_poster:"Scopri come l'abbiamo realizzato",learning1:"Conoscere gli incorporamenti",learning1_description:"Da dove cominciare con gli incorporamenti? Ti abbiamo coperto. Scopri di più sugli incorporamenti con la nostra guida completa.",length:"Lunghezza del token",manage_billing:"Gestisci la fattura",manage_billing_tip:"Gestisci i tuoi dati di fatturazione, ricevi fatture e imposta la ricarica automatica.",manage_quota1:"Chiave API e fatturazione",max_file_size:"Dimensione massima consentita: {_maxSize}.",maximize_tooltip:"Massimizza questo pannello con Maiusc+1",mistake_contact:"Se ritieni che si tratti di un errore, contattaci.",mminput_placeholder:"Testo, URL immagine, stringa base64 immagine",model_required:"Il modello è obbligatorio",more_models:"{_numMore} altri modelli",more_than_two2:"Inserisci più di due documenti, ovvero più di due righe.",multi_embedding:"Multivettore",multi_embedding_explain:"Questo modello restituirà un pacchetto di incorporamenti contestualizzati per un dato input. Ogni token nell'input viene mappato su un vettore nell'output.",multilingual:"Supporto multilingue",multimodal:"Multimodale",multimodal_explain:"Questo modello può codificare input sia di testo che di immagini, rendendolo ideale per attività di ricerca multimodale.",new:"Nuovo modello",no_data1:"Aggiungi un paio di frasi per calcolare la somiglianza",none:"Nessuno",normalized:"Normalizzazione L2",normalized_explain:"Scala l'incorporamento in modo che la sua norma euclidea (L2) diventi 1, preservando la direzione. Utile quando downstream coinvolge prodotto scalare, classificazione, visualizzazione.",oncsp:"Su CSP",onprem:"In sede",open_tensorboard:"Apri visualizzatore",opensource:"sistema operativo",opensource_explain:"Questo modello è open source e disponibile su Hugging Face. Fare clic su questo pulsante per visualizzare il modello su Hugging Face.",original_documents:"Frasi da incorporare",original_documents_hint:"Inserisci qui le tue frasi. Ogni nuova riga sarà considerata una frase/documento separato.",output:"Risposta",output_dim:"Dimensioni",output_dim_explain:"La dimensione di output di un vettore di incorporamento da questo modello è {_outputDim}.",output_dimension:"Dimensioni di uscita",pairwise_test:"A coppie",per_k:"/ Gettoni da 1K",per_m:"/1 milione di gettoni",please_fill_docs_first:"Per favore inserisci alcune frasi qui sotto prima della ricerca.",please_select_model:"Seleziona un modello di incorporamento o un modello di riclassificazione",poster:"Poster L'evoluzione degli incorporamenti",poster_description:"Scopri il poster ideale per il tuo spazio, con infografiche accattivanti o immagini mozzafiato che tracciano l'evoluzione dei modelli di incorporamento del testo dal 1950.",pricing:"Prezzi dell'API",pricing_desc:"I nostri prezzi API sono strutturati in base alla quantità di token inviati nelle richieste. Per l'API Reader, è la quantità di token nelle risposte. Questo modello di prezzo è applicabile a tutti i prodotti nella base di ricerca di Jina AI: API di incorporamento, riclassificazione, lettore e ottimizzazione automatica. Con la stessa chiave API hai accesso a tutti i servizi API.",protectData1:"I dati e i documenti della richiesta non vengono utilizzati per i modelli di training.",protectData2:"Crittografia dei dati in transito (TLS 1.2+) e a riposo (AES-GCM 256).",protectData3:"Conforme a SOC 2 e GDPR.",protect_data:"Proteggi i tuoi dati",public_cloud_integration:"Con <b>{_numPartners}</b> fornitori di servizi cloud",public_cloud_integration_desc:"La tua azienda utilizza AWS o Azure? Quindi distribuisci direttamente i nostri modelli di base di ricerca su queste piattaforme nella tua azienda, in modo che i tuoi dati rimangano sicuri e conformi.",query:"Domanda",raise_issue:"Sollevare la questione",rank_none_description:"Non utilizzare alcun modello di riclassificazione",read_api_docs:"Specifiche API",read_release_note:"Leggi la nota di rilascio","reader-lm-05b_description":"Un piccolo modello linguistico per convertire HTML grezzo in markdown","reader-lm-15b_description":"Un piccolo modello linguistico per convertire HTML grezzo in markdown",recharge_threshold:"Soglia di ricarica",refresh:"ricaricare",refresh_key_tooltip1:"Ottieni una nuova chiave API gratuitamente",refresh_token_count1:"Aggiorna per ottenere i token disponibili della chiave API corrente",regenerate:"Rigenerare",remaining:"Gettoni disponibili",remaining_left:"Hai <b>{_leftTokens}</b> token rimasti nella chiave API di seguito.",request_number:"Tempi di richiesta",request_path:"Richiedi endpoint",results_as_final_result:"#docs come risultato finale",results_fed_to_reranker:"#documenti alimentati a Reranker",retry:"Riprova",return_base64:"Base64 (come stringa)",return_binary:"Binario (compresso come int8)",return_float:"Predefinito (come float)",return_format:"Formato degli incorporamenti",return_format_explain:"Oltre al float, puoi chiedergli di restituire come binario per un recupero vettoriale più veloce o come codifica base64 per una trasmissione più veloce.",return_format_title:"Tipo di dati restituito",return_ubinary:"Binario (compresso come uint8)",right_api_key_to_charge:"Inserisci la chiave API corretta per ricaricare",running:"Attivo",score:"Punto",search:"Ricerca",search_hint:"Digita per cercare all'interno delle frasi elencate di seguito",select_classify_model:"Seleziona classificatore",select_embedding_model:"Seleziona incorporamenti",select_rerank_model:"Seleziona riclassificazione",show_api_key:"Mostra chiave API",size:"Parametri",size_explain:"Il numero di parametri nel modello è {_size}, tieni presente che questa non è la dimensione del file del modello.",sleeping:"Inattivo",start_batch:"Avvia l'incorporamento batch",start_embedding:"Indice",status_explain:"La nostra architettura serverless potrebbe scaricare alcuni modelli durante i periodi di scarso utilizzo. Per i modelli attivi, le risposte sono immediate. I modelli inattivi richiedono alcuni secondi per essere caricati alla richiesta iniziale. Dopo l'attivazione, le richieste successive vengono elaborate più rapidamente.",task_type:"Attività a valle",task_type_classification:"Classificazione",task_type_classification_explain:"Classificazione del testo.",task_type_explain:"Seleziona l'attività downstream per cui verranno utilizzati gli embedding. Il modello restituirà gli embedding ottimizzati per tale attività.",task_type_none_explain:"Non verrà utilizzato alcun adattatore. Verrà restituito un embedding generico, utile per il debug o l'hacking.",task_type_retrieval_passage:"Passaggio di recupero",task_type_retrieval_passage_explain:"Incorporamento di documenti in un'attività di recupero di query-documenti.",task_type_retrieval_query:"Query di recupero",task_type_retrieval_query_explain:"Incorporamento di query in un'attività di recupero di documenti di query.",task_type_separation:"Separazione",task_type_separation_explain:"Raggruppamento di documenti, visualizzazione del corpus.","task_type_text-matching":"Corrispondenza del testo","task_type_text-matching_explain":"Somiglianza semantica del testo, recupero simbolico generale, raccomandazione, ricerca di elementi simili, deduplicazione.",tax_may_apply:"A seconda della tua posizione, l'addebito potrebbe essere effettuato in USD, EUR o altre valute. Potrebbero essere applicate tasse.",text1:"Sinistra",text2:"Giusto",three_ways:"Tre modi per acquistare",three_ways_desc:"Iscriviti alla nostra API, acquista tramite provider cloud o ottieni una licenza commerciale per la tua organizzazione.",title:"Incorporamento dell'API",token_example:'Un tweet è di circa 20 token, un articolo di notizie è di circa 1000 token e il romanzo di Charles Dickens "A Tale of Two Cities" ha oltre un milione di token.',token_length_explain:"La lunghezza massima della sequenza del token di input è {_tokenLength} per questo modello.",tokens:"Gettoni",tools:"Utensili",top_up_button:"Ricarica la vecchia chiave",top_up_button_explain:"L'integrazione di questa chiave API offre una soluzione più professionale, eliminando la necessità di frequenti modifiche della chiave. I dati di utilizzo vengono conservati e accessibili in qualsiasi momento.",top_up_warning_message1:"Alla chiave API corrente sono rimasti token {_remainedTokens} e verrà sostituita da una nuova chiave con token {_freeTokens}. Puoi continuare a utilizzare o ricaricare la vecchia chiave se l'hai conservata in modo sicuro. Come vuoi procedere?",top_up_warning_title:"Sostituisci la vecchia chiave API",total_documents:"Avanzamento incorporamento: {_Processed}/{_Count} frasi.",tuning:"Sintonizzare",turnstile_error:"Non possiamo generare una chiave API perché non siamo riusciti a verificare se sei un essere umano.",turnstile_unsupported:"Non possiamo generare una chiave API perché il tuo browser non è supportato.",ubinary_description:"Gli incorporamenti sono impacchettati come uint8. Molto più efficiente per l'archiviazione, la ricerca e la trasmissione.",upload:"Caricamento",upload_file:"Fare clic qui per caricare un file",usage:"Utilizzo",usage_amount:"Gettoni",usage_history:"Utilizzo negli ultimi 7 giorni",usage_history_explain:"I dati non sono in tempo reale e possono subire ritardi di alcuni minuti.",usage_reason:"Descrizione",usage_reason_consume:"Usato",usage_reason_purchase:"Acquistato",usage_reason_transfer_in:"Trasferimento in",usage_reason_transfer_out:"Trasferisci fuori",usage_reason_trial:"Prova",usage_rerank:"Utilizzo",usage_time:"Appuntamento",v3_description:"<code>jina-embeddings-v3</code> è un modello di incorporamento di testo multilingue di frontiera con 570M di parametri e 8192 token-length, che supera gli ultimi incorporamenti proprietari di OpenAI e Cohere su MTEB. Leggi il nostro post sul blog e il documento di ricerca qui sotto.",v3_title:"v3: Incorporamenti multilingue di Frontier",vector_database_integration1:"Integrazioni",vector_database_integration2:"La nostra API di incorporamento è integrata nativamente con vari database rinomati, archivi di vettori, framework RAG e LLMOps. Per iniziare, copia e incolla la tua chiave API in una qualsiasi delle integrazioni elencate per un avvio rapido e senza intoppi.",vector_database_integration3:"La nostra API Embedding & Reranker è integrata nativamente con vari database rinomati, archivi di vettori, framework RAG e LLMOps. Per iniziare, copia e incolla la tua chiave API in una qualsiasi delle integrazioni elencate per un avvio rapido e senza intoppi.",vector_database_integration_description:"Integra in modo semplice e senza soluzione di continuità l'API Jina Embeddings con qualsiasi database vettoriale, framework di orchestrazione LLM e applicazioni RAG riportati di seguito. I nostri tutorial ti mostreranno come.",view_details:"Visualizza dettagli",visualization_example:"Mappare tutte le frasi di questa sezione in uno spazio vettoriale 3D",visualization_example_you_can:"Utilizza la nostra API qui sotto, puoi farlo anche tu!",visualize:"Visualizzare",visualize_done:"La visualizzazione è terminata, ora puoi fare clic sul pulsante in alto per aprire il visualizzatore.",wait_for_processing:"La tua richiesta sta per essere eseguita.",wait_stripe:"Apertura pagamento Stripe, attendere prego",what_are_embedding:"Cosa sono gli incorporamenti?",what_are_embedding_answer:`Immagina di insegnare a un computer a cogliere le sfumature dei significati di parole e frasi. I metodi tradizionali, che si basavano su sistemi rigidi e basati su regole, non erano all’altezza perché il linguaggio è troppo complesso e fluido. Inserisci gli incorporamenti di testo: una soluzione potente che traduce il testo in un linguaggio di numeri, in particolare in vettori in uno spazio ad alta dimensione.

Considera le frasi "tempo soleggiato" e "cielo sereno". Per noi dipingono un quadro simile. Attraverso la lente degli incorporamenti, queste frasi vengono trasformate in vettori numerici che risiedono vicini gli uni agli altri in questo spazio multidimensionale, catturandone la parentela semantica. Questa vicinanza nello spazio vettoriale non riguarda solo la somiglianza di parole o frasi; si tratta di comprendere il contesto, il sentimento e persino le sottili sfumature di significato.

Perché è importante questa svolta? Per cominciare, colma il divario tra la ricchezza del linguaggio umano e l’efficienza computazionale degli algoritmi. Gli algoritmi eccellono nell’elaborazione dei numeri, non nell’interpretazione dei testi. Convertendo il testo in vettori, gli incorporamenti consentono a questi algoritmi di "comprendere" ed elaborare il linguaggio in un modo che prima era fuori portata.

Le applicazioni pratiche sono vaste e variegate. Che si tratti di consigliare contenuti che siano in sintonia con i tuoi interessi, di potenziare un'intelligenza artificiale conversazionale che sembri sorprendentemente umana o addirittura di rilevare modelli sottili in grandi volumi di testo, gli incorporamenti sono la chiave. Consentono alle macchine di eseguire attività come l'analisi del sentiment, la traduzione linguistica e molto altro, con una comprensione del linguaggio sempre più sfumata e raffinata.`,what_is_a_token:`Un token nell'elaborazione del testo è un'unità, spesso una parola. Ad esempio, "Jina AI è fantastica!" diventa cinque gettoni, compresa la punteggiatura.`,why_do_you_need:"Scegliere gli incorporamenti giusti",why_do_you_need_after:"Sfruttando reti neurali profonde e LLM, i nostri modelli di incorporamento rappresentano dati multimodali in un formato semplificato, migliorando la comprensione automatica, l'archiviazione efficiente e consentendo applicazioni IA avanzate. Questi incorporamenti svolgono un ruolo cruciale nella comprensione dei dati, nel miglioramento del coinvolgimento degli utenti, nel superamento delle barriere linguistiche e nell'ottimizzazione dei processi di sviluppo.",why_do_you_need_before:"I nostri modelli di incorporamento sono progettati per coprire diverse applicazioni di ricerca e GenAI.",why_need_1_description:"Il nostro modello di incorporamento del core, basato su JinaBERT, è costruito per un ampio spettro di applicazioni. Eccelle nella comprensione di testi dettagliati, rendendolo ideale per la ricerca semantica, la classificazione dei contenuti e l'analisi linguistica complessa. La sua versatilità non ha eguali e supporta la creazione di strumenti avanzati di analisi del sentiment, riepilogo del testo e sistemi di consigli personalizzati.",why_need_1_title:"Incorporamenti per uso generale",why_need_2_description:"I nostri modelli bilingui facilitano la comunicazione tra le lingue, migliorando le piattaforme multilingue, l'assistenza clienti globale e la scoperta di contenuti multilinguistici. Progettati per padroneggiare le traduzioni tedesco-inglese e cinese-inglese, questi modelli semplificano le interazioni e favoriscono la comprensione tra diversi gruppi linguistici.",why_need_2_title:"Incorporamenti bilingue",why_need_3_description:"Progettato su misura per gli sviluppatori, il nostro modello di incorporamento del codice ottimizza le attività di codifica come il riepilogo, la generazione di codice e le revisioni automatiche. Aumenta la produttività offrendo approfondimenti sulle strutture del codice e suggerendo miglioramenti, rendendolo essenziale per lo sviluppo di plug-in IDE avanzati, documentazione automatica e strumenti di debug all'avanguardia.",why_need_3_title:"Incorporamenti di codice",why_need_4_description:"Jina CLIP è il nostro ultimo modello di incorporamento multimodale per immagini e testo. Un grande miglioramento rispetto a OpenAI CLIP è che questo singolo modello può essere utilizzato per il recupero di testo-testo, nonché per attività di recupero di testo-immagine, immagine-testo e immagine-immagine! Quindi un modello, due modalità, quattro direzioni di ricerca!",why_need_4_title:"Incorporamenti multimodali",write_email_here:"Inserisci l'e-mail in cui desideri ricevere il collegamento per il download al termine.",you_can_leave:"Puoi lasciare questa pagina e al termine ti invieremo il collegamento per il download."},A={description:"Incorporamenti multilingue multimodali di livello mondiale."},P={contractType:{department:"Licenza di dipartimento",poc:"Prova di concetto (3-6 mesi)",standard:"Licenza Standard Enterprise",title:"Tipo di contratto"},department:{businessSponsor:"Sponsor dell'unità aziendale",executionModel:"Modello di esecuzione",growth:{high:"Chiarire il potenziale aziendale",highDesc:"Iniziativa strategica con adozione pianificata in tutta l'azienda",limited:"Limitato al Dipartimento",limitedDesc:"Concentrati sulle esigenze dei singoli reparti con supporto standard",steady:"Potenziale per altri dipartimenti",steadyDesc:"Espansione pianificata a 2-3 dipartimenti entro 12 mesi"},growthTitle:"Traiettoria di crescita",sponsorDescription:"Sponsor esecutivo dedicato che sostiene l'implementazione, fornisce una direzione strategica e garantisce l'assegnazione delle risorse"},descriptions:{contractType:{department:"Distribuzione di un singolo reparto con flessibilità di espansione futura",poc:"Distribuzione di prova per testare i modelli nel tuo ambiente specifico e nei casi d'uso",standard:"Distribuzione aziendale completa con utilizzo del modello illimitato"},department:{growth:"Piani per espandere l'utilizzo del modello in altri dipartimenti",sponsorship:"Indica se un reparto generatore di entrate sta supportando la distribuzione"},features:{csm:"Responsabile personale del successo del cliente per consulenza e supporto strategico",priority:"Risposta rapida garantita per problemi critici",training:"Aiuto con la pre-formazione o la messa a punto dei modelli per i tuoi dati specifici"},models:{clip:"Elaborare sia immagini che testo per applicazioni multimodali",colbert:"Modello specializzato per il recupero di documenti ad alta precisione",embeddings:"Modello di incorporamento del testo per la ricerca semantica e la similarità del testo",reader:"Converte il contenuto HTML in un formato markdown pulito",reranker:"Ottimizza i risultati della ricerca per una migliore pertinenza"},payment:{annual:"Pagamento unico annuale per contabilità semplificata",quarterly:"Pagamenti regolari ogni tre mesi"},poc:{duration:"Cronologia per testare e convalidare le prestazioni del modello nel tuo ambiente",metrics:"Monitorare gli indicatori chiave delle prestazioni e l'efficacia del modello"},support:{enterprise:"Copertura completa del supporto con la massima priorità",premium:"Ore di consulenza aggiuntive e tempi di risposta più rapidi",standard:"Supporto tecnico di base e guida all'implementazione"},usage:{business:"Quante aziende distinte utilizzeranno applicazioni basate sui nostri modelli?",consumer:"Quanti utenti finali interagiranno mensilmente con i nostri modelli?"}},features:{csm:"Responsabile dedicato al successo del cliente",priority:"SLA di risposta prioritaria (4 ore)",title:"Caratteristiche aggiuntive",training:"Supporto per la formazione di modelli personalizzati"},interests:"Sono interessato a questa licenza commerciale configurata (${_Price})",labels:{basePrice:"Prezzo base",custom:"Contatta il reparto vendite per prezzi personalizzati",discountApplied:"Sconto applicato",included:"Incluso nel prezzo base",learnMore:"Saperne di più",priceQuarterly:"Prezzo al trimestre",selectAll:"Seleziona tutti i modelli",selectSupport:"Seleziona il livello di supporto",totalPrice:"Prezzo totale",upTo:"Fino a {count}"},messaging:{additionalFeatures:"Funzionalità aggiuntive incluse:",baseModelIncluded:"Modello di incorporamento di base (jina-embeddings-v3) incluso",deptIncludes:"La licenza del dipartimento include:",deptReviews:"Riunioni trimestrali di revisione aziendale",deptRoadmap:"Pianificazione della roadmap di espansione aziendale",deptSponsor:"Sessioni di allineamento degli sponsor esecutivi",deptWorkshops:"Workshop di collaborazione interdipartimentale",enterpriseAlert:"Il tuo livello di utilizzo suggerisce un'opportunità per tutta l'azienda. Fissiamo una chiamata per discutere di un accordo aziendale personalizzato.",noModelsSelected:"Nessun modello aggiuntivo selezionato. Utilizzo del modello di incorporamento di base.",pocCheckins:"Check-in bisettimanali con il team tecnico",pocIncludes:"Il pacchetto POC include:",pocMetrics:"Dashboard di monitoraggio delle metriche di successo",pocMigration:"Supporto per la migrazione alla licenza completa",pocTemplate:"Modello di documentazione dei risultati POC",selectedModels:"Modelli selezionati:",standardFeatures:"Caratteristiche della licenza standard:",supportTierIncluded:"{tier} livello di supporto incluso {hours}",usageTierBusiness:"Livello di utilizzo aziendale: fino a {count} account aziendali",usageTierConsumer:"Livello di utilizzo del consumatore: fino a {count} utenti attivi mensili"},models:{clip:"clip-jina-v2",colbert:"jina-colbert-v2",description:"Scegli i modelli da includere nel tuo pacchetto commerciale",embeddings:"incorporamenti jina-v3",lm:"lettore-lm",reranker:"jina-reranker-v2",title:"Seleziona modelli"},payment:{annual:"Fatturazione annuale (sconto del 10%)",features:"Caratteristiche incluse",quarterly:"Fatturazione trimestrale",title:"Termini di pagamento"},poc:{description:"Il POC include il monitoraggio delle metriche di successo e il percorso di aggiornamento alla licenza completa",duration:"Durata POC (mesi)"},pricing:{annual:"anno",cta:"Parla con i nostri venditori",disclaimer:"Questo calcolatore di prezzi fornisce una stima. Il prezzo finale può variare in base a requisiti specifici, impegni di volume e configurazioni personalizzate. Contatta il nostro team di vendita per un preventivo dettagliato.",frequency:"${price} / {frequency}",oneTime:"${price} una tantum",pocTotal:"Prezzo POC di {mesi} mesi: ${price}",quarterly:"trimestre",title:"Prezzo stimato"},short_title:"Licenza di configurazione",subtitle:"Configura la tua licenza aziendale per i modelli Jina AI",support:{enterprise:"Premio",hoursQuarter:"{ore} ore/trimestre",premium:"Standard",standard:"Leggero",title:"Livello di supporto"},title:"Configuratore di licenze aziendali",tooltips:{annualDiscount:"Risparmia il 10% pagando annualmente",businessSponsor:"Avere uno sponsor di unità aziendale può dare diritto a sconti aggiuntivi",pocDuration:"Seleziona la durata del periodo di prova del concetto",supportTier:"Scegli il livello di supporto più adatto alle tue esigenze",usageLimit:"Contattaci per prezzi personalizzati se superi questi limiti"},usage:{business:"B2B (Conti aziendali)",businessCount:"Numero di conti aziendali",businessDescription:"Numero di account aziendali distinti che utilizzano i nostri modelli",consumer:"B2C (utenti finali)",consumerCount:"Utenti attivi mensili",consumerDescription:"Numero di utenti finali attivi mensili su tutte le applicazioni",title:"Configurazione di utilizzo"}},q={answer1:"Jina AI è specializzata in tecnologie AI multimodali, tra cui ottimizzazione dei modelli, servizio dei modelli, messa a punto rapida e servizio rapido. Sfruttiamo strumenti avanzati come Kubernetes e architetture serverless per creare soluzioni robuste, scalabili e pronte per la produzione.",answer10:"Forniamo diverse opzioni di licenza in base alla natura del progetto e alle esigenze del cliente. I termini dettagliati possono essere discussi con il nostro team di vendita.",answer11:"Forniamo servizi a livello globale, con la nostra sede centrale a Berlino, in Europa, e uffici aggiuntivi a Pechino e Shenzhen.",answer12:"Sì, offriamo assistenza in loco, in particolare per i clienti che si trovano vicino ai nostri uffici di Berlino, Pechino e Shenzhen. Per altre località, ci sforziamo di fornire il miglior supporto remoto possibile e, se necessario, possiamo organizzare il supporto in loco.",answer2:"La nostra esperienza copre un ampio spettro, comprendendo modelli linguistici di grandi dimensioni, testo, immagini, video, comprensione audio, ricerca neurale e arte generativa.",answer3:"Sì, le nostre soluzioni sono progettate per essere scalabili e pronte per la produzione. Costruiamo le nostre soluzioni utilizzando tecnologie cloud-native che consentono una scalabilità efficiente e prestazioni affidabili negli ambienti di produzione.",answer4:"I nostri servizi sono versatili e adattabili, il che li rende adatti a un'ampia gamma di settori, tra cui e-commerce, tecnologia legale, marketing digitale, giochi, assistenza sanitaria, finanza e molti altri.",answer5:"Puoi metterti in contatto con il nostro team di vendita tramite il modulo di contatto in questa pagina. Ci piacerebbe discutere i requisiti del tuo progetto e come le nostre soluzioni possono aiutare la tua azienda.",answer6:"Forniamo supporto continuo per garantire il buon funzionamento delle nostre soluzioni. Ciò include la risoluzione dei problemi, aggiornamenti regolari e miglioramenti in base al tuo feedback e alle tue esigenze.",answer7:"La durata del progetto varia a seconda della complessità e della portata del progetto. Dopo aver compreso le vostre esigenze, possiamo fornire una stima più accurata.",answer8:"La sicurezza dei dati è la nostra massima priorità. Aderiamo a rigide politiche e normative sulla protezione dei dati per garantire che i tuoi dati siano sicuri e riservati.",answer9:"Il prezzo dipende dalla complessità e dai requisiti del progetto. Offriamo sia modelli di prezzi basati su progetto che di trattenuta. Si prega di contattare il nostro team di vendita per ulteriori informazioni.",question1:"In cosa è specializzata Jina AI?",question10:"Quali sono i termini di licenza per le vostre soluzioni?",question11:"Qual è la tua area di servizio?",question12:"Offrite supporto in loco?",question2:"Con quali tipi di IA funziona Jina AI?",question3:"Le tue soluzioni sono scalabili e pronte per la produzione?",question4:"Quali settori possono trarre vantaggio dalle soluzioni di Jina AI?",question5:"Come si avvia un progetto con Jina AI?",question6:"Quale supporto fornite dopo aver implementato una soluzione?",question7:"Qual è la durata tipica di un progetto?",question8:"In che modo Jina AI protegge i miei dati?",question9:"Qual è la struttura dei prezzi per i vostri servizi?"},L="FAQ",S={text:"Addio.",toggle_btn:"Tieni aperto questo pannello alla tua prossima visita",warning_message:"Questo pannello si aprirà automaticamente quando visiti jina.ai. Dovrai chiuderlo per vedere il contenuto del sito web. Abilitare questa impostazione?",warning_title:"Mostra all'avvio"},C={description:"Ottimizza gli incorporamenti sui dati specifici del dominio per una migliore qualità della ricerca",intro:"La tua azienda. I tuoi dati. Il tuo modello"},y={description:"Potenzia la tua azienda con soluzioni di fine tuning on-premise"},w={api_key:"Inserisci la tua chiave API.",back:"Indietro",base_model_selected:"Modello base selezionato",click_start:"Accetta i termini e inizia la messa a punto.",confirm_title:"Conferma il lavoro di regolazione fine",confirm_your_email:"Inserisci nuovamente il tuo indirizzo email per confermare il lavoro di perfezionamento. Gli aggiornamenti e il collegamento per il download verranno inviati a questa email.",consent0:"Accetto che i dati sintetici per la messa a punto del modello vengano generati in base alle mie istruzioni.",consent1:"Riconosco che il modello finale e i dati sintetici saranno accessibili pubblicamente su Hugging Face.",consent2:"Comprendo che questa funzionalità è in versione beta e Jina AI non offre garanzie. Il prezzo e l'UX potrebbero cambiare.",continue:"Continua",cost_1m_token:"Ogni lavoro di perfezionamento consuma 1 milione di token. Assicurati di avere gettoni sufficienti o ricarica il tuo saldo. Puoi anche generare una nuova chiave API. Ogni chiave API viene fornita con 1 milione di token gratuiti.",doc_explain:"Descrivi come dovrebbe apparire un documento abbinato.",domain_explain:"Fornire una descrizione dettagliata di come verranno utilizzati gli incorporamenti ottimizzati. Ciò è essenziale per generare dati sintetici di alta qualità che miglioreranno le prestazioni dei tuoi incorporamenti.",domain_explain2:"Esistono tre modi per specificare le tue esigenze: un'istruzione generale, un URL o una descrizione del documento di query. Scegline uno.",domain_hint:"Descrivi il dominio per il quale desideri ottimizzare.",email_not_match:"Gli indirizzi email non combaciano. Si prega di verificare.",failed_job:"La richiesta di perfezionamento non è riuscita. Vedi il motivo qui sotto.",find_on_huggingface:"Trova risultati su Abbracciare il viso",general_instruction:"Oppure istruzioni generali",general_instruction_caption:"Fornire una descrizione dettagliata di come verranno utilizzati gli incorporamenti ottimizzati.",general_instruction_explain:'Descrivi il tuo dominio in testo in formato libero. Puoi immaginarlo come un "prompt" come in ChatGPT.',how_it_works:"Scopri il processo di perfezionamento.",job_acknowledged:"Il tuo lavoro di perfezionamento è stato messo in coda. Riceverai un'e-mail all'inizio del lavoro. Il completamento dell'intero processo richiede spesso 20 minuti.",new_key:"Ottieni una nuova chiave",not_enough_token:"Token insufficienti in questa chiave API. Ricarica il tuo saldo o utilizza una chiave API diversa.",placeholder:"Sinistri di assicurazione auto",preview:"Anteprima",query_doc:"Descrizione del documento di query",query_doc_caption:"Descrivi come appare la query e come appare il documento corrispondente nel tuo dominio.",query_explain:"Descrivi come appare una query.",reset:"Ricominciare",select_base_model:"Scegli un modello di incorporamento di base per la messa a punto.",select_base_model_explain:"Selezionare un modello base come punto di partenza per la messa a punto. In genere, base-en è una buona scelta, ma per attività in altre lingue, considera l'utilizzo di un modello bilingue.",start_tuning:"Inizia la messa a punto",url:"Oppure l'URL della pagina web",url_caption:"Fare riferimento al contenuto di un URL per la messa a punto.",url_explain:"URL pubblico di una pagina Web che contiene il contenuto che desideri ottimizzare.",use_url:"Utilizza invece l'URL. Attivarlo significa che ci baseremo sul contenuto della pagina di quell'URL per generare dati sintetici per la messa a punto.",wait_for_processing:"Si prega di attendere l'elaborazione della tua richiesta...",which_domain:"Dominio di messa a punto",write_email_explain:"La messa a punto richiede tempo. Comunicheremo via e-mail l'inizio, l'avanzamento, il completamento e tutti i problemi del lavoro di ottimizzazione, insieme ai dettagli sul modello ottimizzato e sul set di dati di addestramento."},R={address_beijing:"Pechino, Cina",address_berlin:"Berlino, Germania (sede centrale)",address_shenzhen:"Shenzen, Cina",address_sunnyvale:"Sunnyvale, California",all_rights_reserved:"Tutti i diritti riservati.",company:"Azienda",developers:"Sviluppatori",docs:"Documenti",enterprise:"Impresa",get_api_key:"Ottieni la chiave API Jina AI",offices:"Uffici",power_users:"Utenti esperti",privacy:"Privacy",privacy_policy:"politica sulla riservatezza",privacy_settings:"Gestisci i cookie",security:"Sicurezza",sefo:"Fondazione di ricerca",soc2:"Siamo conformi agli standard SOC 2 Tipo 1 e 2 dell'American Institute of Certified Public Accountants (AICPA).",status:"Stato dell'API",status_short:"Stato",tc:"Termini & Condizioni",tc1:"Termini"},M="Ottieni la tua chiave API",T={stars:"Stelle"},x={description:"Dichiarazioni di base con conoscenza del web",title:"Verifica dei fatti",usage:"Utilizzo della messa a terra"},U={about_us:"Chi siamo",company:"Azienda",contact_us:"Contatta le vendite",developers_others:"Altri strumenti per sviluppatori",enterprise_others:"Altri strumenti aziendali",for_developers:"Per gli sviluppatori",for_developers_description:"Sperimenta uno stack IA multimodale open source completo progettato per gli sviluppatori.",for_enterprise:"Per le Imprese",for_enterprise_description:"Scopri strategie AI multimodali scalabili su misura per soddisfare le esigenze aziendali.",for_power_users:"Per utenti esperti",for_power_users_description:"Utilizza i nostri strumenti multimodali ottimizzati per migliorare la tua produttività.",internship1:"Programma di stagista",jobs:"Unisciti a noi",join_discord:"Unisciti alla nostra comunità Discord",logos:"Scarica il logo",maximize:"⇧1",maximize_btn:"Massimizzare",news:"Notizia",open_day:"Giornata aperta",open_in_full:"Mostra tutti i prodotti aziendali in una nuova finestra",power_users_others:"Più strumenti per utenti esperti",products:"Prodotti"},E={description:"Condividi e scopri elementi costitutivi per applicazioni IA multimodali"},G={sentence_similarity:"Incorporamento di frasi",updated_about:"Aggiornato circa"},D={project1:"Ricerca ad alta precisione abilitata all'interno dei dati mesh 3D utilizzando le informazioni sulla nuvola di punti.",project10:"Sfruttata la computer vision per migliorare l'accessibilità digitale dei siti web governativi.",project11:"LLM perfezionato per una società di consulenza per ottimizzare l'analisi dei dati finanziari.",project12:"Strategie di marketing avanzate perfezionando i modelli di testo in immagine per il trasferimento dello stile.",project2:"Progettato un motore di ricerca basato sui contenuti per cortometraggi di animazione.",project3:"Miglioramento dei tassi di conversione dell'e-commerce perfezionando i modelli di incorporamento.",project4:"Messa a punto rapida eseguita per aumentare l'efficienza per una società di consulenza aziendale.",project5:"Precursore della comprensione delle scene di gioco e dell'annotazione automatica per un'azienda leader nel settore dei giochi.",project6:"Implementata l'espansione dell'input in tempo reale per un'azienda di chatbot, migliorando l'esperienza dell'utente.",project7:"Tecnologia legale rivoluzionata consentendo una ricerca efficiente all'interno di lunghi documenti legali.",project8:"Supporto di un servizio di arte generativa ad alto rendimento per operazioni su larga scala.",project9:"Eseguito il process mining e la modellazione utilizzando modelli linguistici avanzati."},B={description:"Modelli multimodali all'avanguardia disponibili per l'inferenza"},j={copy_full_prompt:"Copia il prompt completo",embedding:"Incorporamenti",how_to_use_meta_prompt:"Come usare",meta_prompt:"Utilizzare Meta-prompt per la generazione del codice",meta_prompt_description:"Il Meta-prompt guida gli LLM (come ChatGPT e Claude) attraverso tutte le nostre API Search Foundation, semplificando e migliorando la qualità della generazione del codice.",reranker:"Riclassificazione",which_to_go:"Quale integrare con {_vendor}?"},N={answer1:"Laurea, master e dottorato di ricerca. studenti provenienti da tutto il mondo, con interesse in campi come la ricerca, l'ingegneria, il marketing e le vendite, sono incoraggiati ad applicare. Accogliamo con favore anche stage non tecnici in marketing, vendite, assistenza esecutiva e altro ancora. Cerchiamo persone appassionate pronte a fare da pionieri nell'IA multimodale con noi.",answer10:"Sì, il nostro programma di tirocinio offre una remunerazione competitiva.",answer11:"In qualità di stagista Jina AI, acquisirai esperienza pratica lavorando su progetti stimolanti, imparerai da esperti del settore, entrerai a far parte di una vivace comunità e avrai l'opportunità di dare un contributo reale al nostro lavoro pionieristico nell'IA multimodale.",answer2:"Gli stage devono essere svolti in loco presso uno dei nostri uffici, che si trovano a Berlino, Pechino e Shenzhen.",answer3:"Sì, Jina AI offre un'assistenza ragionevole nel processo di visto per i richiedenti selezionati.",answer4:"Sì, Jina AI fornisce una ragionevole copertura del costo della vita per gli stagisti durante il periodo di tirocinio.",answer5:"Sì, è possibile lavorare alla tua tesi di Master durante il tirocinio presso Jina AI, tipicamente applicabile agli studenti delle università tedesche. Tuttavia, è necessario disporre di una comunicazione preventiva e del consenso del supervisore della propria università. Tieni presente che non aiutiamo gli studenti a trovare consulenti.",answer6:"Il processo di candidatura include l'invio del modulo di candidatura, un curriculum, una lettera di presentazione che esprima il tuo interesse e la tua motivazione e qualsiasi link professionale pertinente come GitHub o LinkedIn. Valutiamo i candidati in base alle loro prestazioni durante il colloquio e alle loro prestazioni nella loro università.",answer7:"Sì, gli stagisti di successo possono ricevere una lettera di raccomandazione alla fine del loro tirocinio, firmata dal nostro CEO.",answer8:"La durata dello stage varia in base al ruolo e al progetto. Tuttavia, in genere varia da tre a sei mesi.",answer9:"Sì, accogliamo candidature di ogni estrazione accademica. Apprezziamo la tua passione e il tuo impegno per imparare tanto quanto l'esperienza precedente.",question1:"Chi può fare domanda per il programma di tirocinio Jina AI?",question10:"È uno stage retribuito?",question11:"Quali opportunità avrò come stagista Jina AI?",question2:"Dove si svolgerà il tirocinio?",question3:"Jina AI assiste con le procedure di visto?",question4:"Jina AI fornisce indennità o benefici per gli stagisti?",question5:"Posso lavorare alla mia tesi di Master durante lo stage presso Jina AI?",question6:"Cosa prevede il processo di candidatura?",question7:"Jina AI fornisce una lettera di raccomandazione dopo lo stage?",question8:"Qual è la durata del tirocinio?",question9:"Posso candidarmi se non ho precedenti esperienze in IA?"},O={about_internship_program:"Informazioni sul programma di tirocinio",about_internship_program_desc1:"Siamo entusiasti di offrire questa opportunità unica a persone di talento per entrare a far parte del nostro team dinamico e contribuire a progetti innovativi nel campo dell'Intelligenza Artificiale. Questo stage è progettato per fornirti preziosa esperienza pratica, tutoraggio ed esposizione a tecnologie all'avanguardia che stanno plasmando il futuro dell'IA.",about_internship_program_desc2:"In Jina AI, comprendiamo l'importanza di coltivare e sfruttare i giovani talenti. Riconosciamo che i tirocinanti portano sul tavolo nuove prospettive, entusiasmo e creatività, rinvigorendo il nostro team con nuove idee e approcci. Fornendo stage, miriamo a favorire la crescita dei futuri leader nel settore dell'intelligenza artificiale, offrendo loro un'esperienza del mondo reale in un ambiente stimolante e stimolante.",alumni:"ALUNNI",alumni_network:"La nostra fiorente rete di ex studenti",application:"Applicazione",application_desc:"Intraprendi un viaggio di trasformazione con Jina AI. Il nostro programma di tirocinio completo invita tutte le menti appassionate che aspirano a plasmare il futuro dell'intelligenza artificiale. Unisciti a noi per fare esperienza nel mondo reale, lavorare su progetti stimolanti e collaborare con alcune delle menti più brillanti del settore dell'IA.",apply:"Applica ora",autumn:"Autunno",description:"Bando mondiale per studenti: stage in ricerca, ingegneria, marketing, vendite e altro ancora.",dev_rel_intern:"Stagista nelle relazioni con gli sviluppatori",enthusiastic:"ENTUSIASTA",explore_stories_from_our_interns:"Esplora le storie dei nostri stagisti",explore_stories_from_our_interns1:"Lasciati ispirare dai viaggi dei nostri stagisti",innovative:"INNOVATIVO",intern_work1:"Modelli LLM ottimizzati per incorporamenti migliori",intern_work2:"Esplorato il potenziale di Retrieval Augmented Generation",intern_work3:"Ha pubblicato un articolo sul tema dell'incorporamento di frasi",intern_work4:"Infondere continua vitalità giovanile nella squadra",intern_work5:"Tecniche di quantizzazione benchmark per comprimere LLM",intern_work6:"Creazione e promozione di una campagna avvincente per PromptPerfect",intern_work7:"JinaColBERT V2 sviluppato e migliorato rapidamente",recruiting_and_administrative_intern:"Stagista reclutamento e amministrazione",researcher_intern:"Ricercatore tirocinante",self_motivated:"AUTOMOTIVATO",software_engineer_intern:"Tirocinante Ingegnere informatico",spring:"Primavera",submit_application:"Dai il via alla tua avventura con Jina AI",subtitle:"Il nostro programma di tirocinio a tempo pieno offre un'esperienza lavorativa pratica attraverso progetti di tirocinio ben progettati in una vasta gamma di ambiti.",subtitle1:"Bando mondiale per studenti: stagista in ricerca, ingegneria, marketing, vendite e altro ancora per aprire la strada all'IA multimodale insieme.",summer:"Estate",title:"Programma di stagista",who_do_we_look_for:"Chi cerchiamo?",who_do_we_look_for_desc:"Apprezziamo la diversità e incoraggiamo i candidati con profili e background diversi a partecipare al nostro programma di tirocinio. Le opportunità di tirocinio sono offerte in più dipartimenti, tra cui ingegneria, design, gestione del prodotto, gestione delle vendite e dell'account, marketing e gestione della comunità.",winter:"Inverno"},Q={description:"Distribuisci un progetto locale come servizio cloud. Radicalmente facile, senza brutte sorprese."},J={description:"Un finetuner sperimentale per LLM open source"},F={description:"Crea applicazioni IA multimodali nel cloud"},H={description:"Più modalità, memoria più lunga, meno costi",example_1:"Chi sei?",example_2:"Sono un servizio di chat LLM creato da Jina AI"},W={add:"Aggiungi chiave",add_key_explain:"Aggiungi un'altra chiave API al tuo account. Le chiavi aggiunte possono essere gestite, ricaricate o rimosse in qualsiasi momento.",add_success:"Aggiunta con successo una chiave {_key}.",advance_settings:"Apri impostazioni avanzate",advanced_feature:"Funzionalità avanzata disponibile solo con la chiave premium.",auto_recharge_title:"Abilitare la ricarica automatica?",auto_reminder:"Promemoria automatico quando i token sono bassi",auto_reminder_cancel_message:"Vuoi davvero annullare il promemoria automatico per questa chiave?",auto_reminder_cancel_title:"Annulla promemoria automatico",auto_reminder_description:"Rimani informato con avvisi e-mail automatici quando il saldo del tuo token scende sotto le soglie personalizzate. È possibile impostare fino a tre soglie.",auto_reminder_email:"Indirizzo email per promemoria",auto_reminder_info:"La notifica verrà inviata a {_email} quando il saldo dei token scenderà al di sotto di {_threshold} token.",auto_reminder_threshold:"Ricorda se",auto_reminder_threshold_error:"La soglia deve essere compresa tra 1 e 1T.",auto_reminder_toggle:"Attiva il promemoria automatico. Tieni presente che solo la chiave premium può abilitare questa funzione.",available_resources:"Token disponibili",balance:"Token disponibili",balance_primary_key:"Equilibrio chiave primaria",cancel:"Cancellare",copy:"Copia chiave",description:"Gestisci le chiavi API per tutti i servizi Jina AI: Embeddings, Reader, Reranker e altro ancora.",do_it_later:"Fallo dopo",email:"E-mail",existing_key:"Chiave esistente",filter_by:"Filtra per chiave",free_key:"Chiave gratuita",generate_new_key:"Genera nuova chiave",generate_new_key_tooltip:"Genera una nuova chiave API con saldo vuoto. Puoi ricaricare il saldo in seguito.",generate_success:"Generata correttamente una nuova chiave {_key}.",invalid_email:"Email non valida",invalid_key:"Chiave non valida",is_primary:"La tua chiave API primaria. Puoi modificarla dopo aver effettuato l'accesso.",last_used:"Ultimo utilizzo",last_used_at:"Ultima attività",login:"Login",login_explain:"Gestisci più chiavi API e monitora l'utilizzo, tutto in un unico account.",login_explain_long:"Accedi per archiviare e gestire in modo sicuro le tue chiavi API. Tieni traccia della cronologia di utilizzo, gestisci più chiavi e non perdere mai l'accesso alle tue credenziali.",login_via:"effettuato l'accesso tramite {_provider}",logout:"Esci",logout_message:"Le tue chiavi API rimangono archiviate in modo sicuro nel tuo account. Accedi in qualsiasi momento per gestirle.",logout_success:"Disconnessione avvenuta con successo",ok:"OK",primary_key:"Imposta come chiave primaria",primary_key_set:"Impostazione corretta di {_apiKey} come chiave primaria.",primary_key_set_caption:"Questa chiave verrà utilizzata in tutte le demo, gli esempi e i playground su jina.ai.",purchase:"Acquista token",remove:"Elimina chiave",remove_explain:"L'eliminazione rimuoverà la chiave dalla visualizzazione di gestione mantenendola funzionale",remove_message:"Vuoi davvero eliminare questa chiave? La chiave rimarrà valida e potrà essere aggiunta di nuovo in seguito.",remove_primary_key:"Impostare un'altra chiave come primaria prima di rimuovere la chiave primaria corrente.",remove_success:"Eliminazione della chiave {_key} riuscita.",remove_title:"Elimina chiave",revoke:"Revoca chiave",revoke_error:"La chiave inserita non corrisponde a quella che si sta tentando di revocare.",revoke_explain:"La revoca invaliderà immediatamente la chiave e tutti i token rimanenti diventeranno inutilizzabili",revoke_label:"Si prega di confermare la revoca di questa chiave digitandola qui sotto",revoke_message:"Vuoi davvero revocare questa chiave? Una volta revocata, questa chiave diventerà definitivamente non valida.",revoke_success:"La chiave {_key} è stata revocata correttamente.",revoke_title:"Revoca chiave",save:"Salva",settings:"Impostazioni",subscribed_key:"Chiave Premium",title:"API della Fondazione di ricerca Jina",to_dashboard:"Gestisci le chiavi",total_keys:"Chiavi totali",transfer_before_revoke:"Trasferisci i token pagati rimasti prima di revocare la chiave.",transfer_explain:"Trasferisci senza sforzo i tuoi token pagati su un altro account, aumentando la flessibilità e migliorando la sicurezza nella gestione delle tue risorse.",transfer_label:"Trasferisci a",transfer_message:"Vuoi davvero trasferire i token pagati {_tokens} da {_source} a {_target}?",transfer_success:"I token sono stati trasferiti correttamente da {_source} a {_target}.",transfer_title:"Trasferisci token",usage_history:"Cronologia di utilizzo",usage_summary:"Ultimi 7 giorni: {_usage} token"},V={GlobalQA:{description:'Premi il tasto "/" su qualsiasi pagina per aprire la casella delle domande. Digita la tua query e premi "Invio" per ricevere risposte direttamente correlate al contenuto della pagina. Questa funzionalità è fornita da PromptPerfect.',title:"RAG a pagina"},Recommender:{description:'Apri la casella dei consigli su qualsiasi pagina di notizie con "Shift+2". Seleziona il modello di riclassificazione per scoprire i primi 5 articoli relativi a quella pagina di notizie. Goditi questa funzionalità in tempo reale, basata sulla nostra API Reranker.',title:"Articolo correlato"},SceneXplainTooltip:{description:"Passa il cursore su qualsiasi immagine nelle pagine delle notizie o nel nostro catalogo della redazione per rivelare la descrizione di quell'immagine. Le descrizioni sono precalcolate da SceneXplain e incorporate nell'attributo ALT dell'immagine per l'accessibilità.",title:"Didascalie delle immagini"},explain:"Scopri le funzionalità nascoste sul nostro sito web"},K={also_available_on:"Disponibile anche sui marketplace",also_available_on1:"Disponibile sui marketplace del tuo cloud aziendale",ask_how_your_question:"Descrivi il tuo problema",autotune:"Sintonia automatica",avatar:"Generatore di avatar",badge:{"clip-v2":"uscita clip-v2!",v2:"Versione v2!",v3:"versione v3!"},build_js:"Costruisci con JavaScript",build_python:"Costruisci con Python",ccbync:"Questo modello è concesso in licenza con licenza CC BY-NC 4.0. Utilizzalo tramite API o la nostra immagine ufficiale AWS/Azure; oppure contatta il reparto vendite per la distribuzione in locale.",checkout_our_solution_for_you:"Scopri la nostra soluzione su misura per te",classifier:"Classificatore",coming_soon:"Prossimamente",contact_sales:"Contatto",copied_to_clipboard:"Copiato negli appunti",copy:"copia",developers:"Sviluppatori",developers_desc:"Libera tutta la potenza dell'intelligenza artificiale multimodale con tecnologie cloud-native all'avanguardia e un'infrastruttura open source.",download_pdf:"Scarica il pdf",embedding:"Incorporamenti",embedding_desc1:"Incorporamenti multimodali multilingue a contesto lungo ad alte prestazioni per applicazioni di ricerca, RAG e agenti.",embedding_paper_desc:"Jina Embeddings costituisce un insieme di modelli di incorporamento di frasi ad alte prestazioni abili nel tradurre vari input testuali in rappresentazioni numeriche, catturando così l'essenza semantica del testo. Sebbene questi modelli non siano progettati esclusivamente per la generazione di testo, eccellono in applicazioni come il recupero denso e la somiglianza testuale semantica. Questo documento descrive in dettaglio lo sviluppo di Jina Embeddings, a partire dalla creazione di un set di dati pairwise e triplet di alta qualità. Sottolinea il ruolo cruciale della pulizia dei dati nella preparazione del set di dati, fornisce approfondimenti sul processo di addestramento del modello e si conclude con una valutazione completa delle prestazioni utilizzando il Massive Textual Embedding Benchmark (MTEB).",embedding_paper_title:"Jina Embedding: un nuovo set di modelli di incorporamento di frasi ad alte prestazioni",embeddings:"Incorporamenti",enterprise:"Impresa",enterprise_desc:"Potenzia il tuo business con soluzioni AI multimodali scalabili, sicure e su misura.",enterprise_desc_v2:"Prova i nostri modelli di incorporamento di livello mondiale per migliorare i tuoi sistemi di ricerca e RAG. Inizia con una prova gratuita!",enterprise_desc_v3:"I nostri modelli di frontiera costituiscono la base di ricerca per sistemi di ricerca aziendale e RAG di alta qualità.",error:"Si è verificato un problema con l'operazione di recupero: {messaggio}",find_your_portal:"Trova il tuo portale",finding_faq:"Generazione di una risposta in base alla conoscenza delle domande frequenti riportate di seguito",for:"Per",for_better_search:"Per una ricerca migliore",for_developers:"Per gli sviluppatori",for_enterprise:"Per le imprese",for_power_users:"Per utenti esperti",get_api_now:"API",get_started:"Iniziare",go_to_product_homepage:"Vai alla home page del prodotto",grounding:"Messa a terra",how_to:"Come",include_experiment:"Include i nostri progetti sperimentali e archiviati nella soluzione.",join_community:"Comunità",key_manager:"Gestisci la chiave API",learn_more_embeddings:"Ulteriori informazioni sugli incorporamenti",learn_more_reader:"Scopri di più sul lettore",learn_more_reranker:"Scopri di più sul riclassificazione",llm:"Modelli di incorporamento LLM",llm_desc:"Forniamo una raccolta di modelli di incorporamento di frasi ad alte prestazioni, che vantano tra 35 milioni e 6 miliardi di parametri. Sono eccellenti per migliorare la ricerca neurale, la riclassificazione, la somiglianza delle frasi, i consigli, ecc. Preparati a migliorare la tua esperienza AI!",mentioned_products:"Prodotti menzionati:",mmstack:"Pila multimodale",mmstack_desc:"Nel corso degli anni, abbiamo sviluppato una varietà di software open source per aiutare gli sviluppatori a creare una migliore GenAI e a cercare le applicazioni più velocemente.",models:"Modelli",more:"Di più",multimodal:"Multimodale",multimodal_ai:"IA multimodale",new:"Nuovo",newsroom:"Sala stampa",num_publications:"{_total} pubblicazioni in totale.","on-prem-deploy":"Distribuzione locale","on-premises":"In sede",opensource:"Fonte aperta",our_customer:"I nostri clienti",our_customer_explain:"Le aziende di tutte le dimensioni si affidano alla Search Foundation di Jina AI per potenziare i propri strumenti e prodotti: puoi farlo anche tu.",our_publications:"Le nostre pubblicazioni",parameters:"Parametri",podcast:"Podcast",power_users:"Utenti esperti",power_users_desc:"Ingegneria automatica per la tua produttività quotidiana.",powered_by_promptperfect:'Alimentato dalla funzione "Prompt optimization" e "Prompt as a service" di PromptPerfect',pricing:"Prezzi",proposing_solution:"Proporre una soluzione basata sui prodotti Jina AI...",read_more:"Per saperne di più",reader:"Lettore",require_full_question:"Descrivi il tuo problema con maggiori dettagli.",reranker:"Riclassificazione",researcher_desc:"Scopri come i nostri modelli di ricerca di frontiera sono stati addestrati da zero, dai un'occhiata alle nostre ultime pubblicazioni. Incontra il nostro team presso EMNLP, SIGIR, ICLR, NeurIPS e ICML!",researchers:"Ricercatori",sdk:"SDK",sdk_desc:"Vuoi creare applicazioni AIGC di alto livello utilizzando le API PromptPerfect, SceneXplain, BestBanner, JinaChat, Rationale? Ti abbiamo coperto! Prova il nostro SDK facile da usare e inizia in pochi minuti.",sdk_docs:"Leggi i documenti",sdk_example:"Esempio",search_foundation:"Cerca Fondazione",source_code:"Codice sorgente",starter_kit:"Kit di partenza",supercharged1:"Sovralimentato.",tokenizer:"Segmentatore",trusted_by:"AFFIDATO DA",try_it_for_free:"Inizia subito: non serve alcuna carta di credito o registrazione!",try_our_saas:"Prova la nostra soluzione ospitata, un sostituto immediato dell'API di incorporamento di OpenAI.",your_portal_to:"Il tuo portale per",your_search_foundation1:"La tua base di ricerca"},X={description:"App Langchain in produzione con Jina e FastAPI"},$={description:"Informazioni legali, termini di servizio, informativa sulla privacy e altri documenti importanti sui prodotti e servizi di Jina AI.",download_type1:"Scarica l'attestazione SOC 2 Tipo 1",download_type2:"Scarica l'attestazione SOC 2 Tipo 2",request_audit:"Richiedi un rapporto di audit",title:"Informazioni legali"},Y={api:"API di intelligenza artificiale Jina",browse_catalog:"Sfoglia il catalogo",contact_sales_about_it:"Contatta il reparto vendite a riguardo",deploy_it_on:"Distribuiscilo su",description:"Abbiamo spostato l'ago nei modelli di ricerca fin dal primo giorno. Dai un'occhiata all'evoluzione del nostro modello qui sotto: passa il mouse o fai clic per scoprire ogni milestone.",find_on_hf:"Trovalo su HuggingFace",search_for:"Cercalo sul nostro sito",search_models:"Filtra per nome modello",title:"I nostri modelli di fondazione di ricerca",use_it_via:"Usalo tramite"},Z={back_to_models:"Torna ai modelli",comparison:{btn:"Confrontare",select_models:"Scegli i modelli da confrontare"},error:"Impossibile caricare il modello",input_type:{"3d":"3D",audio:"Audio",code:"Codice",document:"Documento",graph:"Grafico",image:"Immagine","multi-vector":"Multi-vettore",other:"Altro",ranking:"Classifiche",tabular:"Tabulare",text:"Testo","text (code)":"Testo (codice)","text (document)":"Testo (Documento)","text (html)":"Testo (HTML)","text (markdown)":"Testo (Markdown)","text (query)":"Testo (richiesta)",timeseries:"Serie temporali",vector:"Vettore",video:"Video"},loading:"Caricamento dettagli modello...",metadata:{api_link:"API di Jina",arxiv:"Articolo ArXiv",aws_link:"AWS SageMaker",azure_link:"Microsoft Azure",deprecated_by:"Obsoleto da",gcp_link:"Google Cloud",huggingface_link:"Faccia abbracciata",input_type:"Ingresso",license:"Licenza",license_link:"Licenza commerciale",output_type:"Produzione",related_models:"Modelli correlati",release_blog:"Pubblicazione post",release_date:"Data di rilascio"},search:{no_results:'Nessun modello trovato corrispondente a "{query}"',placeholder:"Cerca per nome, tag o tipo..."},sections:{availability:"Disponibilità",blogs:"Blog che menzionano questo modello",external_links:"Link e risorse esterne",guidance:{"jina-clip-v1":"Per distribuire efficacemente Jina CLIP v1, i team devono considerare sia le sue capacità che i requisiti di risorse. Il modello elabora le immagini in tile da 224x224 pixel, con ogni tile che consuma 1.000 token di capacità di elaborazione. Per prestazioni ottimali, implementa un'efficiente pre-elaborazione delle immagini per adattarla a queste dimensioni. Sebbene il modello eccella sia nell'elaborazione di testo breve che lungo, al momento supporta solo l'input in lingua inglese. I team devono considerare attentamente l'utilizzo dei token: il testo richiede circa 1,1 token per parola, mentre le immagini vengono elaborate in tile (ad esempio, un'immagine da 750x500 pixel richiede 12 tile, consumando 12.000 token). Il modello è disponibile sia tramite l'API Jina Embeddings sia come release open source su Hugging Face con licenza Apache 2.0, offrendo flessibilità nelle opzioni di distribuzione. Per gli ambienti di produzione, prendi in considerazione l'utilizzo delle opzioni di distribuzione AWS Marketplace o Azure, che forniscono configurazioni di infrastruttura ottimizzate.","jina-clip-v2":"Per un'implementazione ottimale, gli utenti devono considerare diversi fattori chiave. Il modello richiede hardware compatibile con CUDA per un'elaborazione efficiente, con requisiti di memoria in base alle dimensioni del batch e alla risoluzione dell'immagine. Per ottimizzare i costi e le prestazioni dell'API, ridimensionare le immagini a 512x512 pixel prima dell'elaborazione: le immagini più grandi vengono automaticamente affiancate, aumentando l'utilizzo del token e il tempo di elaborazione. Il modello eccelle nell'abbinamento di immagini con testo descrittivo in tutte le lingue, ma può avere difficoltà con concetti astratti o contenuti specifici di dominio altamente specializzati. È particolarmente efficace per la ricerca di prodotti di e-commerce, sistemi di raccomandazione di contenuti e applicazioni di ricerca visiva, ma potrebbe non essere adatto per attività che richiedono un'analisi dettagliata dei dettagli visivi o competenze di dominio altamente specializzate. Quando si utilizza la funzionalità di rappresentazione Matryoshka, considerare il compromesso tra riduzione delle dimensioni e prestazioni: mentre gli incorporamenti a 64 dimensioni mantengono prestazioni elevate, le applicazioni critiche possono trarre vantaggio da dimensioni più elevate.","jina-colbert-v1-en":"Per distribuire efficacemente Jina-ColBERT-v1-en, i team devono considerare diversi aspetti pratici. Il modello richiede una GPU compatibile con CUDA per prestazioni ottimali, sebbene sia possibile l'inferenza della CPU per lo sviluppo. Per l'elaborazione dei documenti, il limite di 8.192 token si traduce in circa 6.000 parole, rendendolo adatto alla maggior parte dei tipi di documenti, inclusi documenti accademici, documentazione tecnica e contenuti di formato lungo. I team devono implementare un'efficiente preelaborazione dei documenti per gestire i limiti dei token e considerare l'elaborazione batch per l'indicizzazione su larga scala. Sebbene il modello eccella nei contenuti in lingua inglese, non è progettato per applicazioni multilingue o recupero multilingua. Per le distribuzioni di produzione, implementare strategie di suddivisione in blocchi dei documenti appropriate e considerare l'utilizzo di indici di similarità vettoriale (come FAISS) per un recupero efficiente. Il modello è particolarmente efficace quando integrato in pipeline RAG utilizzando framework come RAGatouille, che semplifica l'implementazione di modelli di recupero complessi.","jina-colbert-v2":"Per distribuire efficacemente Jina-ColBERT-v2, i team devono considerare diversi aspetti pratici. Il modello richiede hardware compatibile con CUDA per prestazioni ottimali e supporta lunghezze di documenti fino a 8.192 token (estendibili a 12.288) limitando le query a 32 token. Per la distribuzione in produzione, il modello è disponibile tramite l'API Jina Search Foundation, AWS Marketplace e Azure, con una versione non commerciale accessibile tramite Hugging Face. Durante l'implementazione, i team devono specificare se stanno incorporando query o documenti, poiché il modello utilizza la codifica asimmetrica. Il modello non è progettato per l'elaborazione in tempo reale di raccolte di documenti estremamente grandi senza un'indicizzazione adeguata e, sebbene eccella nel recupero multilingue, potrebbe mostrare prestazioni leggermente inferiori su attività specifiche di dominio specializzate rispetto ai modelli ottimizzati per quei domini specifici.","jina-embedding-b-en-v1":"Per un'implementazione ottimale, il modello richiede una GPU compatibile con CUDA, sebbene le sue dimensioni moderate consentano un'inferenza efficiente su hardware standard. Il modello accetta sequenze di input lunghe fino a 512 token ed è particolarmente adatto per ambienti di produzione in cui è fondamentale una generazione di incorporamenti coerente e affidabile. Offre le massime prestazioni su contenuti in lingua inglese ed è ideale per applicazioni come la ricerca semantica, il confronto di similarità di documenti e i sistemi di raccomandazione di contenuti. I team dovrebbero prendere in considerazione l'utilizzo delle versioni v2 o v3 più recenti per i nuovi progetti, in quanto offrono prestazioni migliorate e un supporto linguistico più ampio. Il modello non è consigliato per attività che richiedono una comprensione multilingue o una conoscenza di dominio specializzata al di fuori del testo inglese generale.","jina-embeddings-v2-base-code":"Per distribuire in modo efficace Jina Embeddings v2 Base Code, i team dovrebbero considerare diversi aspetti pratici. Il modello si integra perfettamente con i database vettoriali più diffusi come MongoDB, Qdrant e Weaviate, semplificando la creazione di sistemi di ricerca del codice scalabili. Per prestazioni ottimali, implementare un'adeguata preelaborazione del codice per gestire il limite di 8.192 token, che in genere si adatta alla maggior parte delle definizioni di funzioni e classi. Sebbene il modello supporti 30 linguaggi di programmazione, mostra le prestazioni più elevate nei sei linguaggi principali: Python, JavaScript, Java, PHP, Go e Ruby. I team dovrebbero prendere in considerazione l'utilizzo dell'elaborazione batch per l'indicizzazione del codice su larga scala per ottimizzare le prestazioni. La compatibilità RAG del modello lo rende particolarmente efficace per la generazione automatizzata di documentazione e le attività di comprensione del codice, sebbene i team dovrebbero implementare strategie di suddivisione in blocchi appropriate per basi di codice molto grandi. Per le distribuzioni di produzione, prendere in considerazione l'utilizzo dell'endpoint AWS SageMaker per l'inferenza gestita e implementare strategie di memorizzazione nella cache appropriate per ottimizzare le prestazioni delle query.","jina-embeddings-v2-base-de":"Per distribuire efficacemente Jina Embeddings v2 Base German, le organizzazioni dovrebbero considerare diversi aspetti pratici. Il modello si integra perfettamente con i database vettoriali più diffusi come MongoDB, Qdrant e Weaviate, semplificando la creazione di sistemi di ricerca bilingue scalabili. Per prestazioni ottimali, implementare un'adeguata preelaborazione del testo per gestire efficacemente il limite di 8.192 token, che in genere si adatta a circa 15-20 pagine di testo. Sebbene il modello eccella sia nei contenuti in tedesco che in inglese, è particolarmente efficace quando utilizzato per attività di recupero multilingua in cui le lingue di query e documento possono differire. Le organizzazioni dovrebbero prendere in considerazione l'implementazione di strategie di memorizzazione nella cache per i contenuti a cui si accede di frequente e utilizzare l'elaborazione batch per l'indicizzazione di documenti su larga scala. L'integrazione del modello con AWS SageMaker fornisce un percorso affidabile per la distribuzione in produzione, sebbene i team debbano monitorare l'utilizzo dei token e implementare un'adeguata limitazione della velocità per le applicazioni ad alto traffico. Quando si utilizza il modello per le applicazioni RAG, prendere in considerazione l'implementazione del rilevamento della lingua per ottimizzare la costruzione dei prompt in base alla lingua di input.","jina-embeddings-v2-base-en":"Per distribuire in modo efficace Jina Embeddings v2 Base English, i team devono considerare diversi aspetti pratici. Il modello richiede hardware compatibile con CUDA per prestazioni ottimali, sebbene la sua architettura efficiente consenta di eseguirlo su GPU di livello consumer. È disponibile tramite più canali: download diretto da Hugging Face, distribuzione AWS Marketplace o API Jina AI con 1 milione di token gratuiti. Per le distribuzioni di produzione, AWS SageMaker nella regione us-east-1 offre la soluzione più scalabile. Il modello eccelle nell'analisi di testo di uso generale, ma potrebbe non essere la scelta migliore per terminologia scientifica altamente specializzata o gergo specifico di dominio senza una messa a punto precisa. Quando si elaborano documenti lunghi, prendere in considerazione la possibilità di suddividerli in blocchi semantici significativi anziché in suddivisioni arbitrarie per mantenere l'integrità del contesto. Per risultati ottimali, implementare una corretta preelaborazione del testo e garantire dati di input puliti e ben formattati.","jina-embeddings-v2-base-es":"Per utilizzare efficacemente questo modello, le organizzazioni dovrebbero garantire l'accesso a un'infrastruttura GPU compatibile con CUDA per prestazioni ottimali. Il modello si integra perfettamente con i principali database vettoriali e framework RAG, tra cui MongoDB, Qdrant, Weaviate e Haystack, rendendolo facilmente implementabile in ambienti di produzione. Eccelle in applicazioni come la ricerca di documenti bilingue, sistemi di raccomandazione di contenuti e analisi di documenti multilingua. Sebbene il modello mostri una versatilità impressionante, è particolarmente ottimizzato per scenari bilingue spagnolo-inglese e potrebbe non essere la scelta migliore per applicazioni monolingue o scenari che coinvolgono altre coppie di lingue. Per risultati ottimali, i testi di input dovrebbero essere formattati correttamente in spagnolo o inglese, sebbene il modello gestisca efficacemente i contenuti in lingue miste. Il modello supporta la messa a punto per applicazioni specifiche del dominio, ma questo dovrebbe essere affrontato con un'attenta considerazione della qualità e della distribuzione dei dati di training.","jina-embeddings-v2-base-zh":"Il modello richiede 322 MB di storage e può essere distribuito tramite più canali, tra cui AWS SageMaker (regione us-east-1) e l'API Jina AI. Sebbene l'accelerazione GPU non sia obbligatoria, può migliorare significativamente la velocità di elaborazione per i carichi di lavoro di produzione. Il modello eccelle in varie applicazioni, tra cui analisi di documenti, ricerca multilingue e recupero di informazioni multilinguistiche, ma gli utenti devono notare che è specificamente ottimizzato per scenari bilingue cinese-inglese. Per risultati ottimali, il testo di input deve essere segmentato correttamente e, sebbene il modello possa gestire fino a 8.192 token, si consiglia di suddividere documenti estremamente lunghi in blocchi semanticamente significativi per prestazioni migliori. Il modello potrebbe non essere adatto per attività che richiedono l'elaborazione in tempo reale di testi molto brevi, in cui potrebbero essere più appropriati modelli specializzati a bassa latenza.","jina-embeddings-v3":"Per distribuire in modo efficace Jina Embeddings v3, i team devono considerare il loro caso d'uso specifico per selezionare l'adattatore di attività appropriato: retrieval.query e retrieval.passage per le applicazioni di ricerca, separazione per le attività di clustering, classificazione per la categorizzazione e corrispondenza di testo per la similarità semantica. Il modello richiede hardware compatibile con CUDA per prestazioni ottimali, sebbene la sua architettura efficiente implichi che necessiti di una memoria GPU notevolmente inferiore rispetto ad alternative più grandi. Per la distribuzione in produzione, l'integrazione di AWS SageMaker fornisce un percorso semplificato verso la scalabilità. Il modello eccelle nelle applicazioni multilingue ma potrebbe richiedere una valutazione aggiuntiva per le lingue a basse risorse. Sebbene supporti documenti lunghi fino a 8.192 token, le prestazioni ottimali si ottengono con la funzionalità di chunking tardivo per testi molto lunghi. I team devono evitare di utilizzare il modello per attività che richiedono generazione in tempo reale o ragionamento complesso: è progettato per l'incorporamento e il recupero, non per la generazione di testo o la risposta diretta alle domande.","jina-reranker-v1-base-en":"Il modello richiede hardware compatibile con CUDA per prestazioni ottimali ed è accessibile tramite endpoint API e opzioni di distribuzione AWS SageMaker. Sebbene possa elaborare sequenze estremamente lunghe, gli utenti dovrebbero considerare il compromesso tra lunghezza del contesto e tempo di elaborazione: la latenza del modello aumenta notevolmente con documenti più lunghi, da 156 ms per 256 token a 7068 ms per 4096 token con una query di 512 token. Per le distribuzioni di produzione, si consiglia di implementare una pipeline a due fasi in cui la ricerca vettoriale fornisce candidati iniziali per la riclassificazione. Il modello è specificamente ottimizzato per contenuti in inglese e potrebbe non funzionare in modo ottimale su documenti multilingue o con codice pesante. Quando si integra con sistemi RAG, gli utenti dovrebbero regolare attentamente il numero di documenti inviati per la riclassificazione in base ai requisiti di latenza, con 100-200 documenti che in genere forniscono un buon equilibrio tra qualità e prestazioni.","jina-reranker-v1-tiny-en":"Per distribuire efficacemente questo modello, le organizzazioni dovrebbero dare priorità agli scenari in cui la velocità di elaborazione e l'efficienza delle risorse sono considerazioni critiche. Il modello è particolarmente adatto per distribuzioni di edge computing, applicazioni mobili e sistemi di ricerca ad alta produttività in cui i requisiti di latenza sono rigorosi. Sebbene funzioni eccezionalmente bene nella maggior parte delle attività di riclassificazione, è importante notare che per le applicazioni che richiedono il livello più alto di precisione di classificazione, il modello di base potrebbe comunque essere preferibile. Il modello richiede un'infrastruttura GPU compatibile con CUDA per prestazioni ottimali, sebbene la sua architettura efficiente significhi che può essere eseguito efficacemente su hardware meno potente rispetto alle sue controparti più grandi. Per la distribuzione, il modello si integra perfettamente con i principali database vettoriali e framework RAG ed è disponibile sia tramite l'API Reranker che tramite AWS SageMaker. Quando si esegue la messa a punto per domini specifici, gli utenti devono bilanciare attentamente la qualità dei dati di training con l'architettura compatta del modello per mantenerne le caratteristiche prestazionali.","jina-reranker-v1-turbo-en":"Il modello richiede hardware compatibile con CUDA per prestazioni ottimali e può essere distribuito tramite AWS SageMaker o accessibile tramite endpoint API. Per le distribuzioni di produzione, le organizzazioni dovrebbero implementare una pipeline in due fasi in cui la ricerca vettoriale fornisce candidati iniziali per la riclassificazione. Mentre il modello supporta 8.192 token, gli utenti dovrebbero considerare l'impatto sulla latenza di sequenze più lunghe: il tempo di elaborazione aumenta con la lunghezza del documento. Il punto debole per la maggior parte delle applicazioni è la riclassificazione di 100-200 candidati per query, che bilancia qualità e velocità. Il modello è specificamente ottimizzato per i contenuti in inglese e potrebbe non funzionare in modo ottimale su documenti multilingue. I requisiti di memoria sono significativamente inferiori rispetto al modello base, in genere richiedono solo 150 MB di memoria GPU rispetto a 550 MB, rendendolo adatto per la distribuzione su istanze più piccole e consentendo notevoli risparmi sui costi negli ambienti cloud.","jina-reranker-v2-base-multilingual":"Per un'implementazione ottimale, il modello richiede una GPU compatibile con CUDA ed è accessibile tramite più canali, tra cui l'API Reranker, i principali framework RAG come Haystack e LangChain, oppure implementato privatamente tramite cloud marketplace. Il modello eccelle in scenari che richiedono una comprensione precisa attraverso barriere linguistiche e tipi di dati, rendendolo ideale per le aziende globali che lavorano con contenuti multilingue, documentazione API o repository di codice. La sua ampia finestra di contesto di 524.288 token consente l'elaborazione di documenti di grandi dimensioni o intere basi di codice in un unico passaggio. I team dovrebbero prendere in considerazione l'utilizzo di questo modello quando hanno bisogno di migliorare la precisione della ricerca tra le lingue, richiedono capacità di chiamata di funzione per sistemi RAG agentici o desiderano migliorare la funzionalità di ricerca del codice tra basi di codice multilingue. Il modello è particolarmente efficace se utilizzato insieme a sistemi di ricerca vettoriale, dove può migliorare significativamente la classificazione finale dei documenti recuperati.","reader-lm-05b":"Per distribuire efficacemente Reader LM 0.5B, le organizzazioni devono assicurarsi che la propria infrastruttura possa gestire i requisiti CUDA del modello, sebbene la sua architettura efficiente implichi che possa essere eseguito su GPU di livello consumer. Il modello funziona meglio con input HTML raw e non richiede prefissi o istruzioni speciali. Per prestazioni ottimali, implementare il meccanismo di rilevamento delle ripetizioni fornito per prevenire potenziali loop di token nella generazione di output. Sebbene il modello supporti più linguaggi e varie strutture HTML, è specificamente progettato per l'estrazione di contenuti e la conversione di markdown: non deve essere utilizzato per attività come la generazione di testo, la sintesi o la risposta diretta alle domande. Il modello è disponibile tramite AWS SageMaker per la distribuzione in produzione e viene fornito un notebook Google Colab per test e sperimentazione. I team devono essere consapevoli che, sebbene il modello possa gestire documenti estremamente lunghi fino a 256K token, l'elaborazione di input così grandi potrebbe richiedere strategie di gestione della memoria aggiuntive.","reader-lm-15b":"Per distribuire efficacemente Reader LM 1.5B, le organizzazioni dovrebbero concentrarsi su scenari che coinvolgono l'elaborazione di documenti HTML complessi in cui accuratezza ed efficienza sono fondamentali. Il modello richiede un'infrastruttura GPU compatibile con CUDA per prestazioni ottimali, sebbene la sua architettura efficiente significhi che può essere eseguito efficacemente su hardware più modesto rispetto ad alternative più grandi. Per le distribuzioni di produzione, il modello è disponibile tramite AWS SageMaker e Azure Marketplace, offrendo opzioni di integrazione flessibili. Sebbene il modello eccella nella conversione da HTML a markdown, è importante notare che è specificamente ottimizzato per questa attività e potrebbe non essere adatto per la generazione di testo generica o altre attività NLP. Quando si elaborano documenti estremamente lunghi (che si avvicinano a 512K token), gli utenti devono essere consapevoli che le prestazioni potrebbero peggiorare poiché ciò supera i parametri di addestramento del modello. Per risultati ottimali, implementare i meccanismi di rilevamento delle ripetizioni forniti e prendere in considerazione l'utilizzo della ricerca contrastiva durante l'inferenza per mantenere la qualità dell'output.",title:"Orientamento"},image_size:"Dimensione immagine di input",language:"Supporto linguistico",methods:{"jina-clip-v1":"L'architettura del modello rappresenta un'innovazione significativa nella progettazione di AI multimodale, combinando un codificatore di testo Jina BERT v2 adattato con il codificatore di immagini EVA-02 all'avanguardia della Beijing Academy for Artificial Intelligence. Il codificatore di testo supporta sequenze fino a 12.288 token, oltre 100 volte più lunghe del limite di 77 token del CLIP originale, mentre il codificatore di immagini elabora in modo efficiente 16 token patch. Il processo di addestramento segue un nuovo approccio in tre fasi: in primo luogo, allineare le coppie immagine-didascalia mantenendo la comprensione del testo tramite l'addestramento di coppie di testo interlacciate; in secondo luogo, incorporare descrizioni di testo più lunghe generate dall'AI delle immagini; e infine, utilizzare triplette di testo negative rigide per migliorare le capacità di distinzione semantica. Questa metodologia di addestramento unica consente al modello di mantenere prestazioni elevate sia nelle didascalie brevi che nelle descrizioni testuali dettagliate, preservando al contempo una solida comprensione visiva.","jina-clip-v2":"Al suo interno, Jina CLIP v2 impiega una sofisticata architettura a doppio codificatore che combina un codificatore di testo Jina XLM-RoBERTa (561M parametri) con un codificatore di visione EVA02-L14 (304M parametri). Il codificatore di testo elabora contenuti in 89 lingue con una finestra di contesto massiccia di 696.320 token, mentre il codificatore di visione gestisce immagini ad alta risoluzione fino a 512x512 pixel. Il modello introduce un innovativo apprendimento della rappresentazione Matryoshka, che consente la regolazione dinamica delle dimensioni di incorporamento da 1024 a 64 dimensioni, preservando al contempo le prestazioni. Questa architettura elabora sia il testo che le immagini attraverso i rispettivi codificatori, proiettandoli in uno spazio semantico condiviso in cui concetti simili si allineano indipendentemente dalla loro modalità o lingua originale.","jina-colbert-v1-en":"Il modello impiega un'innovativa architettura di interazione tardiva che cambia radicalmente il modo in cui funziona il recupero dei documenti. Invece di confrontare interi documenti in una volta, elabora query e documenti in modo indipendente fino alla fase di corrispondenza finale, utilizzando una versione adattata dell'approccio ColBERT. L'architettura combina due componenti chiave: un codificatore di documenti che elabora il testo fino a 8.192 token (oltre 16 volte in più rispetto ai trasformatori standard) e un codificatore di query che crea rappresentazioni precise a livello di token. Ogni token sia nella query che nel documento ottiene il proprio vettore di incorporamento a 128 dimensioni, preservando le informazioni semantiche a grana fine che andrebbero perse nei modelli a vettore singolo. Il meccanismo di interazione tardiva consente quindi un efficiente confronto token per token tra query e documenti, utilizzando operazioni di max-pooling e sommatoria per calcolare i punteggi di pertinenza finali senza richiedere costosi confronti all-to-all.","jina-colbert-v2":"Il modello si basa sull'architettura ColBERT, introducendo un sofisticato meccanismo di interazione tardiva che cambia radicalmente il modo in cui vengono abbinate query e documenti. Al suo interno, utilizza un backbone XLM-RoBERTa modificato con 560M di parametri, potenziato da incorporamenti di posizione rotativa e ottimizzato con attenzione flash. Il processo di addestramento prevede due fasi chiave: pre-addestramento iniziale con diversi dati debolmente supervisionati da varie lingue, seguito da una messa a punto con dati di triplette etichettati e distillazione supervisionata. Ciò che rende unico questo approccio è l'implementazione dell'apprendimento della rappresentazione Matryoshka, che consente al modello di produrre incorporamenti in più dimensioni (128, 96 o 64) da un singolo processo di addestramento, consentendo l'ottimizzazione dinamica dell'archiviazione senza riaddestramento.","jina-embedding-b-en-v1":"Il modello impiega un'architettura basata su codificatore T5 potenziata con pooling medio per generare rappresentazioni di lunghezza fissa. Addestrato sul dataset Linnaeus-Clean attentamente curato, che contiene 385 milioni di coppie di frasi di alta qualità filtrate da 1,6 miliardi di coppie iniziali, il modello è stato sottoposto a un processo di addestramento in due fasi. La prima fase ha utilizzato l'apprendimento contrastivo con perdita di InfoNCE su coppie di testo, mentre la seconda fase ha incorporato l'addestramento di triplette per perfezionare la capacità del modello di distinguere tra contenuti simili e dissimili. Questo approccio di addestramento innovativo, combinato con un rigoroso filtraggio dei dati che include il rilevamento della lingua e il controllo della coerenza, consente al modello di catturare efficacemente relazioni semantiche sfumate.","jina-embeddings-v2-base-code":"Il modello raggiunge le sue prestazioni impressionanti attraverso un'architettura specializzata progettata specificamente per la comprensione del codice. Al suo interno, utilizza una rete neurale basata su trasformatore con 161 milioni di parametri, addestrata su diversi set di dati di linguaggi di programmazione con enfasi su sei linguaggi principali: Python, JavaScript, Java, PHP, Go e Ruby. Ciò che rende unica questa architettura è la sua finestra di contesto estesa di 8.192 token, che gli consente di elaborare intere funzioni o più file contemporaneamente mantenendo la comprensione semantica. Il modello genera densi embedding a 768 dimensioni che catturano sia la struttura sintattica che il significato semantico del codice, consentendogli di comprendere le relazioni tra diversi segmenti di codice anche quando utilizzano diversi modelli di programmazione o sintassi per raggiungere lo stesso obiettivo.","jina-embeddings-v2-base-de":"Il modello raggiunge le sue impressionanti capacità bilingue attraverso un'architettura innovativa che elabora sia il testo tedesco che quello inglese all'interno di uno spazio di incorporamento unificato a 768 dimensioni. Al suo interno, impiega una rete neurale basata su trasformatore con 161 milioni di parametri, attentamente addestrata per comprendere le relazioni semantiche in entrambe le lingue. Ciò che rende questa architettura particolarmente efficace è il suo approccio di minimizzazione dei bias, specificamente progettato per evitare la comune trappola di favorire le strutture grammaticali inglesi, un problema identificato in recenti ricerche con modelli multilingue. La finestra di contesto estesa del modello di 8.192 token gli consente di elaborare interi documenti o più pagine di testo in un unico passaggio, mantenendo la coerenza semantica nei contenuti di formato lungo in entrambe le lingue.","jina-embeddings-v2-base-en":"L'architettura del modello combina un backbone BERT Small con un innovativo meccanismo ALiBi (Attention with Linear Biases) bidirezionale simmetrico, eliminando la necessità di tradizionali incorporamenti posizionali. Questa scelta architettonica consente al modello di estrapolare ben oltre la sua lunghezza di addestramento di 512 token, gestendo sequenze fino a 8.192 token senza degradazione delle prestazioni. Il processo di addestramento ha coinvolto due fasi chiave: pre-addestramento iniziale sul set di dati C4, seguito da un perfezionamento sulla raccolta curata di Jina AI di oltre 40 set di dati specializzati. Questi dati di addestramento diversificati, inclusi esempi negativi impegnativi e coppie di frasi varie, garantiscono prestazioni robuste in diversi domini e casi d'uso. Il modello produce vettori densi a 768 dimensioni che catturano relazioni semantiche sfumate, ottenute con parametri relativamente modesti di 137 milioni.","jina-embeddings-v2-base-es":"Al centro di questo modello c'è un'architettura innovativa basata su ALiBi (Attention with Linear Biases) bidirezionale simmetrica, un approccio sofisticato che consente l'elaborazione di sequenze fino a 8.192 token senza i tradizionali embedding posizionali. Il modello utilizza un'architettura BERT modificata con 161M parametri, che incorpora Gated Linear Units (GLU) e tecniche specializzate di normalizzazione dei livelli. L'addestramento segue un processo in tre fasi: pre-addestramento iniziale su un corpus di testo massiccio, seguito da una messa a punto con coppie di testo attentamente curate e, infine, addestramento hard-negative per migliorare la discriminazione tra contenuti simili ma semanticamente distinti. Questo approccio, combinato con embedding a 768 dimensioni, consente al modello di catturare relazioni semantiche sfumate mantenendo l'efficienza computazionale.","jina-embeddings-v2-base-zh":"L'architettura del modello combina un backbone basato su BERT con ALiBi bidirezionale simmetrico (Attention with Linear Biases), consentendo un'elaborazione efficiente di lunghe sequenze senza la tradizionale limitazione di 512 token. Il processo di training segue un approccio in tre fasi attentamente orchestrato: pre-training iniziale su dati bilingue di alta qualità, seguito da fasi di fine-tuning primarie e secondarie. Questa strategia di training metodica, abbinata ai 161M parametri del modello e all'output a 768 dimensioni, raggiunge un'efficienza notevole mantenendo al contempo prestazioni equilibrate in entrambe le lingue. Il meccanismo ALiBi bidirezionale simmetrico rappresenta un'innovazione significativa, consentendo al modello di gestire documenti lunghi fino a 8.192 token, una capacità precedentemente limitata a soluzioni proprietarie.","jina-embeddings-v3":"L'architettura del modello rappresenta un'innovazione significativa nella tecnologia di incorporamento, costruita su una base di jina-XLM-RoBERTa con 24 livelli e potenziata con adattatori Low-Rank Adaptation (LoRA) specifici per attività. Gli adattatori LoRA sono componenti di rete neurale specializzati che ottimizzano il modello per diverse attività come recupero, classificazione o clustering senza aumentare significativamente il conteggio dei parametri: aggiungono meno del 3% ai parametri totali. Il modello incorpora Matryoshka Representation Learning (MRL), consentendo di ridurre in modo flessibile gli incorporamenti da 1024 a un minimo di 32 dimensioni, preservando al contempo le prestazioni. L'addestramento ha coinvolto un processo in tre fasi: pre-addestramento iniziale su testo multilingue da 89 lingue, messa a punto su testi accoppiati per la qualità dell'incorporamento e addestramento dell'adattatore specializzato per l'ottimizzazione delle attività. Il modello supporta lunghezze di contesto fino a 8.192 token tramite Rotary Position Embeddings (RoPE), con un'innovativa tecnica di regolazione della frequenza di base che migliora le prestazioni su testi sia brevi che lunghi.","jina-reranker-v1-base-en":"Il modello impiega un'architettura di cross-attention basata su BERT che differisce fondamentalmente dagli approcci tradizionali basati sull'incorporamento. Invece di confrontare gli embedding di documenti pre-calcolati, esegue interazioni dinamiche a livello di token tra query e documenti, consentendogli di catturare sfumature contestuali che le semplici metriche di similarità non rilevano. I 137 milioni di parametri dell'architettura sono attentamente strutturati per consentire una profonda comprensione semantica mantenendo al contempo l'efficienza computazionale. Un'innovazione di spicco è la sua capacità di gestire sequenze fino a 262.144 token, ben oltre le tipiche limitazioni del modello, ottenute tramite sofisticate tecniche di ottimizzazione che mantengono elevate velocità di inferenza nonostante l'aumento della finestra di contesto.","jina-reranker-v1-tiny-en":"Il modello impiega un'architettura semplificata a quattro livelli basata su JinaBERT con ALiBi bidirezionale simmetrico (attenzione con bias lineari), consentendo un'elaborazione efficiente di lunghe sequenze. Il suo sviluppo sfrutta un approccio avanzato di distillazione della conoscenza in cui un modello insegnante più grande e ad alte prestazioni (jina-reranker-v1-base-en) guida il processo di formazione, consentendo al modello più piccolo di apprendere comportamenti di classificazione ottimali senza richiedere dati di formazione estesi nel mondo reale. Questa metodologia di formazione innovativa, combinata con ottimizzazioni architettoniche come livelli nascosti ridotti ed efficienti meccanismi di attenzione, consente al modello di mantenere classifiche di alta qualità riducendo significativamente i requisiti computazionali. Il risultato è un modello che raggiunge un'efficienza notevole senza compromettere la sua capacità di comprendere relazioni complesse tra documenti.","jina-reranker-v1-turbo-en":"Il modello raggiunge la sua efficienza attraverso un'innovativa architettura a sei livelli che comprime le sofisticate capacità di riclassificazione della sua controparte più grande in soli 37,8 milioni di parametri, una drastica riduzione rispetto ai 137 milioni del modello base. Questo design semplificato impiega la distillazione della conoscenza, in cui il modello base più grande funge da insegnante, addestrando la variante turbo a corrispondere al suo comportamento utilizzando meno risorse. L'architettura mantiene il meccanismo di attenzione incrociata basato su BERT di base per le interazioni a livello di token tra query e documenti, ma lo ottimizza per la velocità attraverso un conteggio di livelli ridotto e un'allocazione efficiente dei parametri. Il modello supporta sequenze fino a 8.192 token, consentendo un'analisi completa dei documenti mantenendo velocità di inferenza elevate attraverso sofisticate tecniche di ottimizzazione.","jina-reranker-v2-base-multilingual":"Il modello impiega un'architettura cross-encoder potenziata con Flash Attention 2, che consente un confronto diretto tra query e documenti per una valutazione della pertinenza più accurata. Addestrato tramite un processo in quattro fasi, il modello stabilisce prima le capacità della lingua inglese, quindi incorpora progressivamente dati multilinguistici e multilingue, prima del perfezionamento finale con esempi hard-negativi. Questo approccio di addestramento innovativo, combinato con l'implementazione di Flash Attention 2, consente al modello di elaborare sequenze fino a 524.288 token mantenendo una velocità eccezionale. L'efficienza dell'architettura consente di gestire complesse attività di riclassificazione in più lingue con una produttività 6 volte superiore rispetto al suo predecessore, garantendo al contempo una valutazione della pertinenza accurata tramite interazione diretta query-documento.","reader-lm-05b":`Il modello impiega un'innovativa architettura "shallow-but-wide" specificamente ottimizzata per operazioni di copia selettiva piuttosto che per la generazione di testo creativo. Costruito su una base di decoder-only con 24 livelli e 896 dimensioni nascoste, il modello utilizza meccanismi di attenzione specializzati con 14 query head e 2 key-value head per elaborare in modo efficiente le sequenze di input. Il processo di addestramento ha coinvolto due fasi distinte: prima con HTML più breve e semplice (token da 32K) per apprendere modelli di conversione di base, poi con HTML complesso e reale (token da 128K) per gestire casi difficili. Il modello incorpora la ricerca contrastiva durante l'addestramento e implementa un meccanismo di rilevamento della ripetizione per prevenire problemi di degenerazione come i token loop. Un aspetto unico della sua architettura è il meccanismo di attenzione a zigzag-ring, che consente al modello di gestire sequenze estremamente lunghe fino a 256K token mantenendo prestazioni stabili.`,"reader-lm-15b":`Il modello impiega un'innovativa architettura "shallow-but-wide" che sfida gli approcci di ridimensionamento tradizionali nella progettazione di modelli linguistici. Al suo interno ci sono 28 livelli di trasformazione configurati con 12 query head e 2 key-value head, creando un equilibrio unico che ottimizza le operazioni di copia selettiva mantenendo al contempo una profonda comprensione semantica. L'architettura presenta una dimensione nascosta di 1536 e una dimensione intermedia di 8960, attentamente sintonizzate per gestire sequenze fino a 256K token. Il processo di formazione ha coinvolto due fasi distinte: prima focalizzandosi su HTML breve e semplice con sequenze di token da 32K, quindi avanzando a HTML lungo e difficile con token da 128K, implementando zigzag-ring-attention per un'elaborazione efficiente. Questo approccio, combinato con la ricerca contrastiva e meccanismi specializzati di rilevamento delle ripetizioni, consente al modello di evitare problemi comuni come degenerazione e loop noiosi che in genere affliggono modelli linguistici più piccoli che gestiscono attività di elaborazione di documenti complesse.`,title:"Metodi"},model_comparison:"Confronto dei modelli",model_details:"Dettagli del modello",model_name:"Nome",output_dimension:"Dimensione di uscita",overview:{"jina-clip-v1":"Jina CLIP v1 rivoluziona l'intelligenza artificiale multimodale, essendo il primo modello a eccellere in egual misura sia nelle attività di recupero testo-testo che testo-immagine. A differenza dei tradizionali modelli CLIP che hanno difficoltà con scenari solo testo, questo modello raggiunge prestazioni all'avanguardia in tutte le combinazioni di recupero, mantenendo al contempo una dimensione dei parametri notevolmente compatta di 223 M. Il modello affronta una sfida critica del settore eliminando la necessità di modelli separati per l'elaborazione di testo e immagini, riducendo la complessità del sistema e il sovraccarico computazionale. Per i team che creano sistemi di ricerca, motori di raccomandazione o strumenti di analisi dei contenuti, Jina CLIP v1 offre un'unica soluzione efficiente che gestisce sia il testo che i contenuti visivi con eccezionale accuratezza.","jina-clip-v2":"Jina CLIP v2 rivoluziona l'intelligenza artificiale multimodale colmando il divario tra comprensione visiva e testuale in 89 lingue. Questo modello risolve sfide critiche nell'e-commerce globale, nella gestione dei contenuti e nella comunicazione interculturale consentendo un'accurata corrispondenza immagine-testo indipendentemente dalle barriere linguistiche. Per le aziende che si espandono a livello internazionale o gestiscono contenuti multilingue, elimina la necessità di modelli separati per lingua o complesse pipeline di traduzione. Il modello brilla in particolare in scenari che richiedono una ricerca visiva precisa oltre i confini linguistici, come la scoperta di prodotti sul mercato globale o la gestione multilingue delle risorse digitali.","jina-colbert-v1-en":"Jina-ColBERT-v1-en rivoluziona la ricerca di testo risolvendo una sfida critica nel recupero delle informazioni: ottenere un'elevata accuratezza senza sacrificare l'efficienza computazionale. A differenza dei modelli tradizionali che comprimono interi documenti in singoli vettori, questo modello mantiene una comprensione precisa a livello di token richiedendo solo 137 milioni di parametri. Per i team che creano applicazioni di ricerca, sistemi di raccomandazione o piattaforme di scoperta di contenuti, Jina-ColBERT-v1-en elimina il tradizionale compromesso tra qualità della ricerca e prestazioni del sistema. Il modello brilla in particolare in scenari in cui la comprensione sfumata del testo è fondamentale, come la ricerca di documentazione tecnica, il recupero di articoli accademici o qualsiasi applicazione in cui l'acquisizione di sottili relazioni semantiche può fare la differenza tra trovare le informazioni giuste e perdere contenuti critici.","jina-colbert-v2":"Jina-ColBERT-v2 è un modello di recupero di informazioni multilingue rivoluzionario che risolve la sfida critica di una ricerca efficiente e di alta qualità in più lingue. Come primo modello multilingue simile a ColBERT a generare incorporamenti compatti, risponde alla crescente necessità di soluzioni di ricerca multilingue scalabili e convenienti nelle applicazioni globali. Le organizzazioni che si occupano di contenuti multilingue, dalle piattaforme di e-commerce ai sistemi di gestione dei contenuti, possono sfruttare questo modello per fornire risultati di ricerca accurati in 89 lingue, riducendo significativamente i costi di archiviazione e di elaborazione attraverso le sue innovative capacità di riduzione delle dimensioni.","jina-embedding-b-en-v1":"Jina Embedding B v1 è un modello di embedding di testo specializzato progettato per trasformare il testo inglese in rappresentazioni numeriche ad alta dimensionalità mantenendo il significato semantico. Il modello risponde all'esigenza critica di embedding di testo efficienti e precisi negli ambienti di produzione, particolarmente prezioso per le organizzazioni che richiedono un equilibrio tra efficienza computazionale e qualità dell'embedding. Con i suoi 110M di parametri che generano embedding a 768 dimensioni, funge da soluzione pratica per i team che implementano sistemi di ricerca semantica, clustering di documenti o raccomandazione di contenuti senza richiedere ampie risorse computazionali.","jina-embeddings-v2-base-code":"Jina Embeddings v2 Base Code affronta una sfida critica nello sviluppo software moderno: navigare e comprendere in modo efficiente grandi basi di codice. Per i team di sviluppo che hanno difficoltà con la scoperta e la documentazione del codice, questo modello trasforma il modo in cui gli sviluppatori interagiscono con il codice abilitando la ricerca in linguaggio naturale in 30 linguaggi di programmazione. A differenza dei tradizionali strumenti di ricerca del codice che si basano sulla corrispondenza esatta dei pattern, questo modello comprende il significato semantico dietro il codice, consentendo agli sviluppatori di trovare frammenti di codice pertinenti utilizzando semplici descrizioni in inglese. Questa capacità è particolarmente preziosa per i team che gestiscono grandi basi di codice legacy, gli sviluppatori che si iscrivono a nuovi progetti o le organizzazioni che cercano di migliorare le pratiche di riutilizzo e documentazione del codice.","jina-embeddings-v2-base-de":"Jina Embeddings v2 Base German affronta una sfida critica nel business internazionale: colmare il divario linguistico tra i mercati tedesco e inglese. Per le aziende tedesche che si espandono in territori di lingua inglese, dove un terzo delle aziende genera oltre il 20% delle proprie vendite globali, una comprensione bilingue accurata è essenziale. Questo modello trasforma il modo in cui le organizzazioni gestiscono i contenuti multilingua consentendo una comprensione e un recupero del testo senza soluzione di continuità sia in tedesco che in inglese, rendendolo prezioso per le aziende che implementano sistemi di documentazione internazionali, piattaforme di assistenza clienti o soluzioni di gestione dei contenuti. A differenza dei tradizionali approcci basati sulla traduzione, questo modello mappa direttamente significati equivalenti in entrambe le lingue nello stesso spazio di incorporamento, consentendo operazioni bilingue più accurate ed efficienti.","jina-embeddings-v2-base-en":"Jina Embeddings v2 Base English è un modello di incorporamento di testo open source rivoluzionario che risolve la sfida critica dell'elaborazione di documenti lunghi mantenendo un'elevata accuratezza. Le organizzazioni che hanno difficoltà ad analizzare documenti legali, relazioni di ricerca o report finanziari estesi troveranno questo modello particolarmente prezioso. Si distingue per la gestione di documenti lunghi fino a 8.192 token, 16 volte di più rispetto ai modelli tradizionali, eguagliando le prestazioni delle soluzioni proprietarie di OpenAI. Con una dimensione compatta di 0,27 GB e un utilizzo efficiente delle risorse, offre una soluzione accessibile per i team che cercano di implementare un'analisi avanzata dei documenti senza un sovraccarico computazionale eccessivo.","jina-embeddings-v2-base-es":"Jina Embeddings v2 Base Spanish è un modello di incorporamento di testo bilingue rivoluzionario che affronta la sfida critica del recupero e dell'analisi di informazioni interlinguistiche tra contenuti in spagnolo e inglese. A differenza dei tradizionali modelli multilingue che spesso mostrano una parzialità verso lingue specifiche, questo modello offre prestazioni veramente bilanciate sia in spagnolo che in inglese, rendendolo indispensabile per le organizzazioni che operano nei mercati di lingua spagnola o che gestiscono contenuti bilingue. La caratteristica più notevole del modello è la sua capacità di generare incorporamenti geometricamente allineati: quando i testi in spagnolo e inglese esprimono lo stesso significato, le loro rappresentazioni vettoriali si raggruppano naturalmente nello spazio di incorporamento, consentendo una ricerca e un'analisi interlinguistica senza soluzione di continuità.","jina-embeddings-v2-base-zh":"Jina Embeddings v2 Base Chinese apre nuove strade come primo modello open source a gestire senza problemi sia il testo cinese che quello inglese con una lunghezza di contesto di token senza precedenti di 8.192. Questa potenza bilingue affronta una sfida critica nel business globale: la necessità di un'elaborazione accurata e di lunga durata dei documenti nei contenuti in cinese e inglese. A differenza dei modelli tradizionali che hanno difficoltà con la comprensione interlinguistica o richiedono modelli separati per ogni lingua, questo modello mappa significati equivalenti in entrambe le lingue nello stesso spazio di incorporamento, rendendolo inestimabile per le organizzazioni che si espandono a livello globale o gestiscono contenuti multilingue.","jina-embeddings-v3":"Jina Embeddings v3 è un modello di incorporamento di testo multilingue rivoluzionario che trasforma il modo in cui le organizzazioni gestiscono la comprensione e il recupero del testo tra le lingue. In sostanza, risolve la sfida critica di mantenere prestazioni elevate su più lingue e attività, mantenendo al contempo i requisiti computazionali gestibili. Il modello brilla in modo particolare negli ambienti di produzione in cui l'efficienza è importante: raggiunge prestazioni all'avanguardia con soli 570 milioni di parametri, rendendolo accessibile ai team che non possono permettersi il sovraccarico computazionale di modelli più grandi. Le organizzazioni che hanno bisogno di creare sistemi di ricerca multilingue scalabili o di analizzare contenuti oltre le barriere linguistiche troveranno questo modello particolarmente prezioso.","jina-reranker-v1-base-en":"Jina Reranker v1 Base English rivoluziona il perfezionamento dei risultati di ricerca affrontando una limitazione critica nei tradizionali sistemi di ricerca vettoriale: l'incapacità di catturare relazioni sfumate tra query e documenti. Mentre la ricerca vettoriale con similarità del coseno fornisce risultati iniziali rapidi, spesso perde segnali di pertinenza sottili che gli utenti umani comprendono intuitivamente. Questo reranker colma questa lacuna eseguendo un'analisi sofisticata a livello di token sia di query che di documenti, offrendo un notevole miglioramento del 20% nella precisione della ricerca. Per le organizzazioni che hanno difficoltà con la precisione della ricerca o che implementano sistemi RAG, questo modello offre una soluzione potente che migliora significativamente la qualità dei risultati senza richiedere una revisione completa dell'infrastruttura di ricerca esistente.","jina-reranker-v1-tiny-en":"Jina Reranker v1 Tiny English rappresenta una svolta nell'affinamento efficiente della ricerca, progettato specificamente per le organizzazioni che richiedono un riclassificazione ad alte prestazioni in ambienti con risorse limitate. Questo modello affronta la sfida critica di mantenere la qualità della ricerca riducendo significativamente i costi di overhead computazionale e di distribuzione. Con soli 33 milioni di parametri, una frazione delle dimensioni tipiche dei reranker, offre prestazioni notevolmente competitive tramite tecniche innovative di distillazione della conoscenza. La caratteristica più sorprendente del modello è la sua capacità di elaborare documenti quasi cinque volte più velocemente rispetto ai modelli base, mantenendo oltre il 92% della loro accuratezza, rendendo l'affinamento della ricerca di livello aziendale accessibile alle applicazioni in cui le risorse computazionali sono un bene prezioso.","jina-reranker-v1-turbo-en":"Jina Reranker v1 Turbo English affronta una sfida critica nei sistemi di ricerca di produzione: il compromesso tra qualità dei risultati ed efficienza computazionale. Mentre i reranker tradizionali offrono una migliore accuratezza di ricerca, le loro esigenze computazionali spesso li rendono poco pratici per le applicazioni in tempo reale. Questo modello rompe questa barriera offrendo il 95% dell'accuratezza del modello base, elaborando i documenti tre volte più velocemente e utilizzando il 75% di memoria in meno. Per le organizzazioni che hanno difficoltà con la latenza di ricerca o i costi computazionali, questo modello offre una soluzione convincente che mantiene un raffinamento della ricerca di alta qualità riducendo significativamente i requisiti infrastrutturali e i costi operativi.","jina-reranker-v2-base-multilingual":"Jina Reranker v2 Base Multilingual è un modello cross-encoder progettato per migliorare l'accuratezza della ricerca attraverso barriere linguistiche e tipi di dati. Questo reranker affronta la sfida critica del recupero preciso delle informazioni in ambienti multilingue, particolarmente prezioso per le aziende globali che hanno bisogno di perfezionare i risultati della ricerca in diverse lingue e tipi di contenuto. Con il supporto per oltre 100 lingue e capacità uniche nella chiamata di funzioni e nella ricerca di codice, funge da soluzione unificata per i team che richiedono un perfezionamento della ricerca accurato in contenuti internazionali, documentazione API e basi di codice multilingue. Il design compatto dei parametri 278M del modello lo rende particolarmente interessante per le organizzazioni che cercano di bilanciare alte prestazioni con l'efficienza delle risorse.","reader-lm-05b":"Reader LM 0.5B è un modello di linguaggio specializzato progettato per risolvere la complessa sfida di convertire documenti HTML in testo markdown pulito e strutturato. Questo modello risponde a un'esigenza critica nelle moderne pipeline di elaborazione dati: trasformare in modo efficiente contenuti Web disordinati in un formato ideale per LLM e sistemi di documentazione. A differenza dei modelli di linguaggio generici che richiedono enormi risorse di calcolo, Reader LM 0.5B raggiunge un'elaborazione HTML di livello professionale con soli 494 milioni di parametri, rendendolo accessibile a team con risorse di calcolo limitate. Le organizzazioni che si occupano di elaborazione di contenuti Web, automazione della documentazione o creazione di applicazioni basate su LLM troveranno questo modello particolarmente prezioso per semplificare i flussi di lavoro di preparazione dei contenuti.","reader-lm-15b":"Reader LM 1.5B rappresenta una svolta nell'elaborazione efficiente dei documenti, affrontando la sfida critica di convertire contenuti Web complessi in formati puliti e strutturati. Questo modello di linguaggio specializzato affronta un problema fondamentale nelle moderne pipeline di intelligenza artificiale: la necessità di elaborare e pulire in modo efficiente i contenuti HTML per le attività downstream senza affidarsi a fragili sistemi basati su regole o a modelli di linguaggio di grandi dimensioni ad alta intensità di risorse. Ciò che rende questo modello davvero notevole è la sua capacità di superare modelli 50 volte più grandi mantenendo un'impronta di parametri sorprendentemente compatta di 1,54 B. Le organizzazioni che si occupano di elaborazione di contenuti Web su larga scala, automazione della documentazione o sistemi di gestione dei contenuti troveranno questo modello particolarmente prezioso per la sua capacità di gestire documenti estremamente lunghi offrendo al contempo una precisione superiore nella conversione da HTML a markdown.",title:"Panoramica"},parameter_size:"Parametri",performance:{"jina-clip-v1":"Jina CLIP v1 dimostra notevoli miglioramenti rispetto al CLIP originale di OpenAI in tutti i benchmark. Nel recupero solo testo, ottiene un aumento delle prestazioni del 165% con un punteggio di 0,429 rispetto a 0,162 di CLIP. Per le attività correlate alle immagini, mostra miglioramenti costanti: 2% in più nel recupero testo-immagine (0,899), 6% nel recupero immagine-testo (0,803) e 12% nel recupero immagine-immagine (0,916). Il modello brilla in particolare nelle attività di classificazione visiva zero-shot, categorizzando con successo le immagini senza formazione precedente su domini specifici. Quando valutato su benchmark standard come MTEB per il recupero testo, CIFAR-100 per le attività immagine e Flickr8k/30k e MSCOCO Captions per le prestazioni cross-modali, supera costantemente i modelli specializzati a modalità singola mantenendo prestazioni competitive nelle attività cross-modali.","jina-clip-v2":"Il modello raggiunge prestazioni all'avanguardia con una precisione del 98,0% nelle attività di recupero immagine-testo di Flickr30k, superando sia il suo predecessore che NLLB-CLIP-SigLIP. In scenari multilingue, dimostra un miglioramento fino al 4% rispetto a NLLB-CLIP-SigLIP nelle attività di recupero immagini multi-lingue, nonostante abbia meno parametri rispetto al suo principale concorrente. Il modello mantiene prestazioni elevate anche quando gli incorporamenti sono compressi: la riduzione delle dimensioni del 75% preserva comunque oltre il 99% delle prestazioni nelle attività di testo, immagine e multi-modali. Nei benchmark completi Multilingual MTEB, raggiunge il 69,86% nel recupero e il 67,77% nelle attività di similarità semantica, con prestazioni competitive rispetto ai modelli di incorporamento di testo specializzati.","jina-colbert-v1-en":"Jina-ColBERT-v1-en dimostra notevoli miglioramenti rispetto ai modelli di base in vari benchmark. Sulla raccolta di dataset BEIR, ottiene prestazioni superiori in più categorie: 49,4% su Arguana (rispetto al 46,5% per ColBERTv2), 79,5% su FEVER (rispetto al 78,8%) e 75,0% su TREC-COVID (rispetto al 72,6%). Ancora più impressionante, mostra un netto miglioramento sul benchmark LoCo per la comprensione del contesto lungo, con un punteggio dell'83,7% rispetto al 74,3% di ColBERTv2. Il modello eccelle in particolare negli scenari che richiedono una comprensione semantica dettagliata, superando i modelli di incorporamento tradizionali e mantenendo l'efficienza computazionale attraverso il suo innovativo approccio di interazione tardiva. Questi miglioramenti vengono ottenuti mantenendo il conteggio dei parametri del modello a un modesto 137M, rendendolo potente e pratico per le distribuzioni di produzione.","jina-colbert-v2":"Nei test nel mondo reale, Jina-ColBERT-v2 dimostra capacità eccezionali in più benchmark. Raggiunge un miglioramento del 6,5% rispetto al ColBERT-v2 originale nelle attività in inglese, con un punteggio medio di 0,521 in 14 benchmark BEIR. Ancora più impressionante, supera i tradizionali metodi di recupero basati su BM25 in tutte le lingue testate nei benchmark MIRACL, dimostrando una particolare forza negli scenari multilinguistici. Il modello mantiene queste elevate prestazioni anche quando si utilizzano dimensioni di incorporamento ridotte: il passaggio da 128 a 64 dimensioni comporta solo una riduzione delle prestazioni dell'1,5%, dimezzando al contempo i requisiti di archiviazione. Ciò si traduce in significativi risparmi sui costi di produzione: ad esempio, l'archiviazione di 100 milioni di documenti con vettori a 64 dimensioni costa $ 659,62 al mese su AWS, rispetto a $ 1.319,24 per 128 dimensioni.","jina-embedding-b-en-v1":"Nelle valutazioni del mondo reale, Jina Embedding B v1 dimostra capacità impressionanti, in particolare in attività di similarità testuale semantica. Il modello raggiunge prestazioni all'avanguardia su STS12 con un punteggio di 0,751, superando modelli consolidati come all-mpnet-base-v2 e all-minilm-l6-v2. Mostra prestazioni elevate in vari benchmark mantenendo tempi di inferenza efficienti. Tuttavia, gli utenti devono notare che il modello è specificamente ottimizzato per contenuti in lingua inglese e potrebbe non funzionare in modo ottimale su attività multilingue o specifiche del codice. Il modello è stato da allora sostituito da jina-embeddings-v2-base-en e jina-embeddings-v3, che offrono prestazioni migliorate in una gamma più ampia di casi d'uso.","jina-embeddings-v2-base-code":"Nei test nel mondo reale, Jina Embeddings v2 Base Code dimostra capacità eccezionali, guidando il campo in nove su quindici benchmark CodeNetSearch cruciali. Se confrontato con modelli di giganti del settore come Microsoft e Salesforce, raggiunge prestazioni superiori mantenendo un'impronta più efficiente. Il modello eccelle in particolare nella comprensione del codice multilinguaggio, abbinando con successo frammenti di codice funzionalmente equivalenti in diversi linguaggi di programmazione. La sua finestra di contesto di 8.192 token si dimostra particolarmente preziosa per funzioni di grandi dimensioni e file di codice complessi, superando significativamente i modelli tradizionali che in genere gestiscono solo poche centinaia di token. L'efficienza del modello è evidente nelle sue dimensioni compatte di 307 MB (non quantizzate), consentendo un'inferenza rapida mantenendo un'elevata accuratezza nella somiglianza del codice e nelle attività di ricerca.","jina-embeddings-v2-base-de":"Nei test nel mondo reale, Jina Embeddings v2 Base German dimostra un'efficienza e un'accuratezza eccezionali, in particolare nelle attività di recupero multilingua. Il modello supera il modello base E5 di Microsoft pur essendo meno di un terzo delle sue dimensioni e corrisponde alle prestazioni di E5 large nonostante sia sette volte più piccolo. In tutti i benchmark chiave, tra cui WikiCLIR per il recupero dall'inglese al tedesco, STS17 e STS22 per la comprensione bidirezionale della lingua e BUCC per l'allineamento preciso del testo bilingue, il modello dimostra costantemente capacità superiori. Le sue dimensioni compatte di 322 MB consentono l'implementazione su hardware standard mantenendo prestazioni all'avanguardia, rendendolo particolarmente efficiente per gli ambienti di produzione in cui le risorse di calcolo sono una considerazione.","jina-embeddings-v2-base-en":"Nei test nel mondo reale, Jina Embeddings v2 Base English dimostra capacità eccezionali in più benchmark. Supera text-embedding-ada-002 di OpenAI in diverse metriche chiave: classificazione (73,45% contro 70,93%), riclassificazione (85,38% contro 84,89%), recupero (56,98% contro 56,32%) e riepilogo (31,6% contro 30,8%). Questi numeri si traducono in vantaggi pratici in attività come la classificazione dei documenti, in cui il modello mostra una capacità superiore di categorizzare testi complessi, e nelle applicazioni di ricerca, in cui comprende meglio le query degli utenti e trova documenti pertinenti. Tuttavia, gli utenti devono tenere presente che le prestazioni possono variare quando si ha a che fare con contenuti altamente specializzati specifici del dominio non rappresentati nei dati di training.","jina-embeddings-v2-base-es":"Nelle valutazioni di benchmark complete, il modello dimostra capacità eccezionali, in particolare nelle attività di recupero multilingua, dove supera modelli multilingue significativamente più grandi come E5 e BGE-M3, nonostante sia solo il 15-30% delle loro dimensioni. Il modello raggiunge prestazioni superiori nelle attività di recupero e clustering, mostrando una particolare forza nell'abbinamento di contenuti semanticamente equivalenti tra le lingue. Quando testato sul benchmark MTEB, mostra prestazioni robuste in varie attività, tra cui classificazione, clustering e similarità semantica. La finestra di contesto estesa di 8.192 token si dimostra particolarmente preziosa per l'elaborazione di documenti lunghi, mostrando prestazioni costanti anche con documenti che si estendono su più pagine, una capacità di cui sono privi la maggior parte dei modelli concorrenti.","jina-embeddings-v2-base-zh":"Nei benchmark della classifica cinese MTEB (C-MTEB), il modello dimostra prestazioni eccezionali tra i modelli inferiori a 0,5 GB, eccellendo in particolare nelle attività in lingua cinese. Supera significativamente il text-embedding-ada-002 di OpenAI nelle applicazioni specifiche per il cinese, mantenendo al contempo prestazioni competitive nelle attività in inglese. Un notevole miglioramento in questa versione è la distribuzione raffinata del punteggio di similarità, che affronta i problemi di inflazione del punteggio presenti nella versione di anteprima. Il modello ora fornisce punteggi di similarità più distinti e logici, garantendo una rappresentazione più accurata delle relazioni semantiche tra i testi. Questo miglioramento è particolarmente evidente nei test comparativi, in cui il modello mostra una discriminazione superiore tra contenuti correlati e non correlati in entrambe le lingue.","jina-embeddings-v3":"Il modello dimostra un eccezionale rapporto efficienza-prestazioni nei test del mondo reale, superando sia le alternative open source che le soluzioni proprietarie di OpenAI e Cohere nelle attività in inglese, eccellendo negli scenari multilingue. La cosa più sorprendente è che ottiene risultati migliori di e5-mistral-7b-instruct, che ha 12 volte più parametri, evidenziando la sua notevole efficienza. Nelle valutazioni di benchmark MTEB, ottiene un punteggio medio di 65,52 in tutte le attività, con prestazioni particolarmente elevate in Accuratezza della classificazione (82,58) e Somiglianza delle frasi (85,80). Il modello mantiene prestazioni costanti in tutte le lingue, ottenendo un punteggio di 64,44 nelle attività multilingue. Quando si utilizza MRL per la riduzione delle dimensioni, mantiene prestazioni elevate anche a dimensioni inferiori, ad esempio, mantenendo il 92% delle sue prestazioni di recupero a 64 dimensioni rispetto alle 1024 dimensioni complete.","jina-reranker-v1-base-en":"Nei benchmark completi, il modello dimostra miglioramenti eccezionali in tutte le metriche chiave, ottenendo un aumento dell'8% nel tasso di successo e un incremento del 33% nel grado reciproco medio rispetto alla ricerca vettoriale di base. Nel benchmark BEIR, ottiene un punteggio medio di 0,5588, superando altri reranker di BGE (0,5032), BCE (0,4969) e Cohere (0,5141). Particolarmente impressionante è la sua prestazione nel benchmark LoCo, dove ottiene un punteggio medio di 0,873, significativamente più avanti dei concorrenti nella comprensione della coerenza locale e del ranking contestuale. Il modello mostra una particolare forza nella valutazione dei contenuti tecnici, ottenendo punteggi di 0,996 nelle attività qasper_abstract e 0,962 nell'analisi dei report governativi, sebbene mostri prestazioni relativamente inferiori (0,466) nelle attività di riepilogo delle riunioni.","jina-reranker-v1-tiny-en":"Nelle valutazioni di benchmark complete, il modello dimostra capacità eccezionali che sfidano il compromesso convenzionale tra dimensioni e prestazioni. Nel benchmark BEIR, ottiene un punteggio NDCG@10 di 48,54, mantenendo il 92,5% delle prestazioni del modello base pur essendo solo un quarto delle sue dimensioni. Ancora più impressionante, nei benchmark LlamaIndex RAG, mantiene un tasso di successo dell'83,16%, quasi eguagliando modelli più grandi mentre elabora i documenti in modo significativamente più rapido. Il modello eccelle in particolare nella produttività, elaborando i documenti quasi cinque volte più velocemente del modello base mentre utilizza il 13% di memoria in meno rispetto persino alla variante turbo. Queste metriche si traducono in prestazioni reali che rivaleggiano o superano modelli molto più grandi come mxbai-rerank-base-v1 (184 milioni di parametri) e bge-reranker-base (278 milioni di parametri).","jina-reranker-v1-turbo-en":"Nei benchmark completi, la variante turbo dimostra un'efficienza notevole senza compromessi significativi in termini di accuratezza. Nel benchmark BEIR, ottiene un punteggio NDCC@10 di 49,60, mantenendo il 95% delle prestazioni del modello base (52,45) e superando molti concorrenti più grandi come bge-reranker-base (47,89, 278 milioni di parametri). Nelle applicazioni RAG, mantiene un impressionante tasso di successo dell'83,51% e 0,6498 MRR, dimostrando una particolare forza nelle attività di recupero pratiche. I miglioramenti della velocità del modello sono ancora più sorprendenti: elabora i documenti tre volte più velocemente del modello base, con un throughput che scala quasi linearmente con un conteggio dei parametri ridotto. Tuttavia, gli utenti dovrebbero notare prestazioni leggermente inferiori su attività di classificazione estremamente sfumate in cui il conteggio completo dei parametri dei modelli più grandi fornisce vantaggi marginali.","jina-reranker-v2-base-multilingual":"Nelle valutazioni del mondo reale, il modello dimostra capacità eccezionali in diversi benchmark. Raggiunge prestazioni all'avanguardia nella classifica AirBench per i sistemi RAG e mostra risultati eccellenti in attività multilingue, tra cui il set di dati MKQA che copre 26 lingue. Il modello eccelle in particolare nelle attività di dati strutturati, ottenendo punteggi di richiamo elevati sia nella chiamata di funzione (benchmark ToolBench) che nella corrispondenza dello schema SQL (benchmark NSText2SQL). Ancora più impressionante, fornisce questi risultati elaborando i documenti 15 volte più velocemente rispetto a modelli comparabili come bge-reranker-v2-m3, rendendolo pratico per applicazioni in tempo reale. Tuttavia, gli utenti devono notare che le prestazioni ottimali richiedono una GPU compatibile con CUDA per l'inferenza.","reader-lm-05b":"Nei test nel mondo reale, Reader LM 0.5B dimostra impressionanti rapporti efficienza-prestazioni su più parametri. Il modello raggiunge un punteggio ROUGE-L di 0,56, che indica una forte conservazione dei contenuti, e mantiene un basso tasso di errore token di 0,34, mostrando un'allucinazione minima. Nelle valutazioni qualitative su 22 diverse fonti HTML, tra cui articoli di notizie, post di blog e pagine di e-commerce in più lingue, mostra una particolare forza nella conservazione della struttura e nell'uso della sintassi markdown. Il modello eccelle nella gestione di complesse pagine web moderne in cui CSS e script in linea possono espandersi fino a centinaia di migliaia di token, uno scenario in cui gli approcci tradizionali basati su regole spesso falliscono. Tuttavia, è importante notare che mentre il modello funziona eccezionalmente bene su semplici attività di conversione da HTML a markdown, potrebbe richiedere un'elaborazione aggiuntiva per pagine altamente dinamiche o con JavaScript pesante.","reader-lm-15b":"Nelle valutazioni di benchmark complete, Reader LM 1.5B dimostra capacità eccezionali che sfidano gli standard del settore. Il modello raggiunge un punteggio ROUGE-L di 0,72 e un Token Error Rate di 0,19, superando significativamente modelli più grandi come GPT-4 (0,43 ROUGE-L, 0,50 TER) e Gemini-1.5-Pro (0,42 ROUGE-L, 0,48 TER) nelle attività di conversione da HTML a markdown. Le sue prestazioni brillano in particolare nelle valutazioni qualitative su quattro dimensioni chiave: estrazione dell'intestazione, estrazione del contenuto principale, conservazione della struttura avanzata e utilizzo della sintassi markdown. Il modello mantiene costantemente un'elevata accuratezza su diversi tipi di documenti, dagli articoli di notizie e post di blog alle landing page e post di forum, in più lingue tra cui inglese, tedesco, giapponese e cinese. Queste prestazioni vengono raggiunte durante l'elaborazione di documenti fino a 256K token di lunghezza, eliminando la necessità di costose operazioni di chunking che sono in genere richieste con modelli più grandi.",title:"Prestazione"},performance_metrics:"Misure di prestazione",publications:"Pubblicazioni",tags:"Etichette",token_length:"Lunghezza del token di input",usage_requirements:"Utilizzo e requisiti",using_model:"Disponibile tramite"},select_model:"Seleziona un modello dall'elenco per visualizzarne i dettagli",sort:{direction:{asc:"Ascendente",desc:"Discendente",name:"Direzione"},label:"Ordinare",name:"Nome",parameter_size:"Misurare",release_date:"Data"},title:"{_modelName} - Cerca modelli di fondazione",warnings:{deprecated:"Questo modello è obsoleto nei modelli più recenti."}},ee={back_to_newsroom:"Torniamo alla redazione",categories:"Categorie",copy_link:"Copia il collegamento a questa sezione",in_this_article:"In questo articolo",learn_more:"Saperne di più",news_not_found:"Articolo non trovato",redirect_to_news:"Reindirizzamento alla redazione tra 5 secondi..."},ie={academic:"Accademico",academic_research:"Pubblicazioni accademiche",author:"Filtra per autore",description:"Leggi le ultime notizie e gli aggiornamenti da Jina AI.",description1:"Realizzare innovazioni legate all'intelligenza artificiale, una parola alla volta.",engineering_group:"Gruppo Ingegneria",engineering_group_date:"31 maggio 2021",minutes_read:"minuti letti",most_recent_articles:"Articoli più recenti",news_description:'Per Jina 2.0, abbiamo ascoltato la community. Veramente, profondamente ascoltato. "Quali sono i tuoi punti dolenti?" abbiamo chiesto, aspettando con impazienza un prezioso feedback',news_title:"Cerca tutte le cose: stiamo organizzando un concorso MEME per Jina 2.0",photos:"Fotografie",product:"Filtra per prodotto",search:"Cerca per titolo",tech_blog:"Blog tecnico",title:"Sala stampa",top_stories:"Le migliori storie"},ae='🎉 Il nostro primo libro, "Neural Search — From Prototype to Production with Jina" è ufficialmente uscito oggi!',oe={description:"Un'opportunità esclusiva per ottenere una visione dall'interno di Jina AI.",engage:"Incoraggiamo vivamente un dialogo interattivo durante tutta la giornata. Lo scambio di pensieri e prospettive è inestimabile per noi. Le potenziali collaborazioni derivanti da queste discussioni potrebbero contribuire in modo significativo a un futuro più integrato e innovativo.",engage_title:"Interagisci con noi",experience:"Abbiamo organizzato un coinvolgente tour di tre ore per i nostri ospiti, disponibile in tedesco, inglese, francese, spagnolo, cinese e russo. Il tour copre uno sguardo approfondito ai nostri progressi nell'IA multimodale, la nostra prospettiva sul panorama dell'IA, seguito da un esame dettagliato di progetti specifici. Concluderemo con una discussione di gruppo per facilitare lo scambio di idee e approfondimenti. Su richiesta è disponibile anche un'opzione per il pranzo.",experience_title:"Il viaggio di un insider",group_size:"Numero stimato di visitatori",impact:"Scopri come i nostri contributi alla comunità open source e il nostro lavoro nella tecnologia IA multimodale stanno facendo di Jina AI un attore influente nell'innovazione IA. Miriamo a svolgere un ruolo significativo nei processi decisionali, assicurandoci che il progresso della tecnologia AI sia vantaggioso per tutti.",impact_title:"Impatto e influenza",introduction:"Jina AI è lieta di aprire le nostre porte a entità e organizzazioni stimate interessate al progresso e al futuro dell'Intelligenza Artificiale. Estendiamo questa opportunità esclusiva per coloro che operano in politica, ONG, NPO e settori di investimento per ottenere una visione dall'interno delle nostre operazioni e visioni qui presso la nostra sede di Berlino.",motivation_min_length_v1:"Si prega di fornire una motivazione più dettagliata.",motivation_placeholder_v2:"Condividere le tue motivazioni ci aiuterà a migliorare la tua esperienza.",motivation_to_attend_v2:"Perché sei interessato al nostro Open Day?",one_hour:"1 ora",organization:"Organizzazione",organization_website:"Sito web dell'organizzazione",organization_website_placeholder:"URL della home page o del profilo LinkedIn della tua organizzazione",preferred_date:"Data preferita",preferred_language:"Lingua preferita del tour",preferred_products:"A quali prodotti sei interessato?",subtitle:"Uno sguardo al futuro dell'IA multimodale",title:"Giornata delle porte aperte",tutor_subtitle:"Un tour di tre ore meticolosamente curato, che ti avvicina al cuore del lavoro rivoluzionario di Jina AI nella tecnologia AI multimodale.",tutor_title:"Un'esclusiva immersione profonda in",vision:"Unisciti a noi per una panoramica completa del panorama dell'IA come lo vediamo noi. La nostra discussione si concentrerà sul potenziale dei modelli linguistici di grandi dimensioni, dell'IA multimodale e dell'impatto della tecnologia open source nel plasmare il futuro dell'innovazione globale.",vision_title:"La nostra visione per il futuro"},ne={answer1:"Offriamo tour in tedesco, inglese, francese, spagnolo, cinese e russo.",answer2:"Il tour dura in genere circa tre ore.",answer3:"Il pranzo è facoltativo e può essere organizzato su richiesta.",answer4:"Il nostro Open Day è progettato principalmente per gruppi professionali, come politici, ONG, NPO e investitori. Tuttavia, occasionalmente facciamo delle eccezioni in base al profilo dell'individuo.",answer5:"Siamo in grado di ospitare una varietà di dimensioni di gruppo. Indica la dimensione del tuo gruppo nel modulo di registrazione e confermeremo i dettagli con te.",answer6:"C'è una sezione nel modulo di registrazione dove puoi specificare le tue aree di interesse o eventuali richieste particolari. Faremo del nostro meglio per personalizzare il tour in base alle vostre esigenze.",answer7:"Al momento, offriamo tour solo presso la nostra sede di Berlino situata a Kreuzberg. I nostri uffici di Pechino e Shenzhen non sono attualmente aperti per i tour.",question1:"Quali lingue offrite per il tour?",question2:"Qual è la durata del tour?",question3:"Il pranzo è previsto?",question4:"Le persone possono iscriversi all'Open Day?",question5:"Da quante persone può essere composto un gruppo per l'Open Day?",question6:"Come posso specificare le aree di interesse per il tour?",question7:"I tour sono disponibili presso i vostri uffici di Pechino o Shenzhen?"},te={description:"Un cloud-native open source di grandi modelli multimodali al servizio del framework"},re={commercial_licence:{chip_label:"Esclusivo per le piccole aziende",company_size_note:"Esclusivo per aziende con meno di 50 dipendenti o un fatturato di $ 500K",cta_button:"Iniziare",download_title:"Scarica la licenza commerciale",feature_api_desc:"Prova prima dell'acquisto",feature_api_title:"Accesso gratuito ai test API",feature_consulting:"Due ore di consulenza con i nostri esperti di modelli",feature_consulting_desc:"Due (2) ore di servizi di consulenza tecnica per periodo di licenza.",feature_future_support:"Accesso ai futuri modelli CC BY-NC senza autorizzazione",feature_future_support_desc:"Tutti i nuovi modelli rilasciati dal Licenziante con licenza CC-BY-NC-4.0 durante il Periodo di Licenza.",feature_models:"Utilizzo commerciale illimitato dei nostri modelli CC BY-NC",feature_models_desc:"Utilizzare i Modelli per scopi commerciali, incluso l'uso interno o l'integrazione in applicazioni rivolte al cliente.",price_amount:"1.000 dollari",price_period:"/ quarto",read_the_terms:"Rivedi i termini della licenza",read_the_terms_btn:"Termini",read_the_terms_desc:"Esaminare i diritti e le limitazioni della licenza commerciale prima dell'acquisto",subtitle:"Ogni modello di cui hai bisogno per una ricerca migliore",test_before_purchase:"Prova prima di acquistare",test_before_purchase_desc:"Ottieni 1 milione di token API gratuiti o usa il nostro modello Hugging Face per convalidare le prestazioni",title:"Licenza di squadra",try_api:"Prova prima l'API"},free_hour_consult:"Consulenza gratuita di 1 ora",free_hour_consult_description:"Un'ora di consulenza gratuita con i nostri team di prodotto e ingegneria per discutere le best practice per il tuo caso d'uso",full_commercial:"Uso commerciale senza restrizioni",full_commercial_description:"È possibile utilizzare l'API per scopi commerciali senza alcuna restrizione.",higher_limit:"Limite di velocità molto più alto",higher_limit_description:"Ottieni fino a 1000 RPM per r.jina.ai e 100 RPM per s.jina.ai; maggiori dettagli nella sezione sui limiti di velocità.",key_manager:"Gestione delle chiavi di base",key_manager_description:"Gestisci più chiavi API in un unico account, monitora la cronologia di utilizzo e ricarica i token.",no_commercial:"Solo per uso non commerciale (CC-BY-NC)",no_commercial_description:"Puoi usare l'API solo per scopi non commerciali. Per uso commerciale, ricarica la tua chiave API.",on_prem:"Con una licenza commerciale per l'uso in sede",on_prem_explain:"Acquista una licenza commerciale per utilizzare i nostri modelli in sede.",premium_key:"Chiave premium con limiti di velocità molto più elevati",premium_key_description:"Ottieni limiti di tariffa molto più elevati e accedi alle funzionalità premium, controlla la tabella dei limiti di tariffa per i dettagli.",premium_key_manager:"Gestione avanzata delle chiavi",premium_key_manager_description:"Funzionalità di base e avanzate come promemoria automatico, revoca, trasferimento di token.",priority_support:"Supporto tecnico prioritario",priority_support_description:"Risposta via email garantita per problemi tecnici e incidenti entro 24 ore.",secured_by_stripe:"Pagamento sicuro tramite Stripe",standard_key:"Chiave standard",standard_key_description:"Accesso a tutti i prodotti API della Jina Search Foundation con un limite di tariffa standard.",via_api:"Con l'API Jina Search Foundation",via_api_explain:"Il modo più semplice per accedere a tutti i nostri prodotti. Ricarica i token man mano che procedi."},le="Offerto da",se="Stampa",ce={archived:"Archiviato",cloud_native:"Nativo del cloud",core:"Nucleo",data_structure:"Struttura dati",embedding_serving:"Incorporamento della pubblicazione",embedding_tuning:"Incorporamento dell'ottimizzazione",graduated:"Laureato",incubating:"Incubazione",kubernetes:"Kubernetes",large_size_model:"Modello di grandi dimensioni",linux_foundation:"Fondazione Linux",llm1:"LLMOps",mid_size_model:"Modello di taglia media",model_serving:"Modello che serve",model_tuning:"Messa a punto del modello",observability:"Osservabilità",orchestration:"Orchestrazione",prompt_serving:"Servizio rapido",prompt_tuning:"Sintonizzazione rapida",rag1:"STRACCIO",sandbox:"Sabbiera",small_size_model:"Modello di piccole dimensioni",vector_database:"Banca dati vettoriale",vector_store:"Negozio di vettore"},de={description:"Strumento principale per l'ingegneria rapida",image_model:"Modelli di immagine",intro:"Strumento principale per l'ingegneria rapida",intro1:"Lo strumento principale per un'ingegneria tempestiva",optimized:"Il tuo compito è essere il mio compagno di brainstorming e fornire idee e suggerimenti creativi per un determinato argomento o problema. La tua risposta dovrebbe includere idee originali, uniche e pertinenti che potrebbero aiutare a risolvere il problema o esplorare ulteriormente l'argomento in modo interessante. Tieni presente che la tua risposta dovrebbe anche tenere conto di eventuali requisiti o vincoli specifici dell'attività.",optimized_title:"Prompt ottimizzato",original:"Il tuo ruolo è quello di essere il mio compagno di brainstorming.",original_title:"Richiesta originale",text_model:"Modelli testuali"},ue={features:[{description:"Passa facilmente dalla generazione di contenuti all'ottimizzazione rapida e porta la qualità dei tuoi contenuti a un livello superiore.",name:"Assistente",title:"Dose giornaliera di produttività."},{description:"Non sai come scrivere un'istruzione efficace? Inserisci semplicemente la tua idea, con un clic ottieni istruzioni migliori.",name:"Ottimizzazione immediata",title:"Migliori input, migliori risultati"},{description:"Comprendi l'atmosfera di ogni modello di intelligenza artificiale confrontando l'output dello stesso prompt.",name:"Confronta i modelli",title:"Confronto dei modelli affiancati."},{description:"Forse il modo più semplice per distribuire le tue richieste come API per l'integrazione.",name:"Distribuire i prompt",title:"Nessuna operazione, basta schierarsi."},{description:"Personalizza i tuoi agenti LLM e avvia una simulazione multi-agente. Guarda come collaborano o competono in un ambiente virtuale per raggiungere l'obiettivo.",name:"Multiagente",title:"Scopri come collaborano gli agenti"}],get_started:"Inizia con PromptPerfect"},me={api_key:"Chiave API ricaricata",success:"Grazie per il vostro acquisto!",success_caption:"Abbiamo completato il tuo ordine alle {_purchasedTime}. La tua chiave API è pronta per l'uso!"},pe="Acquista adesso",ge={batch_explain:"Questa API supporta operazioni batch, consentendo fino a 512 documenti per richiesta, con ogni documento contenente fino a 8192 token. Sfruttare in modo intelligente le operazioni batch può ridurre significativamente il numero di richieste e migliorare le prestazioni.",classifier:"Addestrare un classificatore utilizzando esempi etichettati",classifier_few_shot:"Classificare gli input utilizzando un classificatore addestrato a pochi scatti",classifier_few_shot_token_counting:"I token sono conteggiati come: input_tokens",classifier_latency:"Il tempo di risposta varia in base alla dimensione dell'input",classifier_token_counting:"I token vengono conteggiati come: input_tokens × num_iters",classifier_zero_shot:"Classificare gli input utilizzando la classificazione zero-shot",classifier_zero_shot_token_counting:"I token vengono conteggiati come: input_tokens + label_tokens",depends:"dipende dalla dimensione dell'input",description:"Descrizione",embeddings:"Convertire testo/immagini in vettori di lunghezza fissa",endpoint:"Punto finale API",explain:"I limiti di velocità vengono monitorati in due modi: <b>RPM</b> (richieste al minuto) e <b>TPM</b> (token al minuto). I limiti vengono applicati per IP e possono essere raggiunti in base alla soglia (RPM o TPM) raggiunta per prima.",gjinaai:"Basare un'affermazione sulla conoscenza del web",input_token_counting:"Conta il numero di token nella richiesta di input.",latency:"Latenza media",no_token_counting:"Il token non viene conteggiato come utilizzo.",output_token_counting:"Contare il numero di token nella risposta di output.",premium_rate:"Con possibilità di limiti di velocità più elevati",product:"Prodotto",requestType:"Richiesta consentita",reranker:"Classifica i documenti per query",rjinaai:"Convertire l'URL in testo compatibile con LLM",sjinaai:"Cerca sul web e converti i risultati in testo compatibile con LLM",tbd:"Da determinare",title:"Limite di velocità",tokenCounting:"Conteggio dell'utilizzo del token",tokenizer:"Tokenizzare e segmentare il testo lungo",total_token_counting:"Contare il numero totale di token nell'intero processo.",understanding:"Comprendere il limite di velocità",understanding_description:"I limiti di velocità sono il numero massimo di richieste che possono essere effettuate a un'API in un minuto per indirizzo IP (RPM). Scopri di più sui limiti di velocità per ogni prodotto e livello qui sotto.",wAPIkey:"con chiave API",wPremium:"con chiave API Premium",woAPIkey:"senza chiave API"},ze={decision:"Decisione",description:"Strumenti decisionali IA all'avanguardia",intro:"Guarda i due lati della medaglia, prendi decisioni razionali"},ve={beta:"Sperimentale",better_input:"Migliora la qualità dell'input fin dall'inizio",better_input_description:"Riscontri problemi con l'output del tuo agente o del sistema RAG? Potrebbe essere dovuto alla scarsa qualità dell'input.",check_price_table:"Controlla la tabella dei prezzi",copy:"copia",demo:{advanced_parameter_explain:"Parametri specifici utilizzati solo per {_product}.",advanced_parameters:"Specifico",advanced_usage:"Utilizzo avanzato",ask_llm:"Chiedi a LLM senza e con ricerca messa a terra",ask_llm_directly:"Chiedi direttamente a LLM",ask_llm_with_search_grounding:"Chiedi a LLM con la messa a terra della ricerca",ask_question:"Poni una domanda",ask_question_hint:"Inserisci una domanda e combinala con il contenuto recuperato affinché LLM generi una risposta",basic_usage:"Utilizzo di base",basic_usage1:"Leggi un URL",basic_usage2:"Cerca una query",basic_usage3:"Messa a terra",common_parameter_explain:"Parametri comuni che possono essere utilizzati per {_product1}, {_product2} e {_product3}.",common_parameters:"Comune",copy:"copia",fetch:"Recupera contenuto",get_response:"Ottieni risposta",grounding_result_false:"Questa affermazione è falsa.",grounding_result_true:"Questa affermazione è vera.",headers:{auth_token:"Aggiungi chiave API per un limite di velocità più elevato",auth_token_explain:"Inserisci la tua chiave API Jina per accedere a un limite di velocità più elevato. Per le informazioni più recenti sui limiti di tariffa, fare riferimento alla tabella seguente.",browser_locale:"Impostazioni locali del browser",browser_locale_explain:"Controlla le impostazioni locali del browser per il rendering della pagina. Molti siti web offrono contenuti diversi in base alle impostazioni locali.",custom_script:"Pre-esegui JavaScript personalizzato",custom_script_explain:"Esegue il codice JavaScript di pre-elaborazione, accettando sia la stringa di codice in linea che l'endpoint URL dello script remoto",deepdive:"Analisi approfondita della fonte",deepdive_explain:"Cerca più fonti e legge documenti completi per un fact-checking approfondito. Leggermente più lento ma più accurato e con più riferimenti.",default:"Predefinito",default_explain:"Pipeline predefinita ottimizzata per la maggior parte dei siti web e input LLM.",file:"File PDF/HTML locale",file_explain:"Utilizza Reader sul tuo file PDF e HTML locale caricandoli. Supporta solo file PDF e HTML.",html:"HTML",html_explain:"Restituisce documentElement.outerHTML.",image_caption:"Didascalia immagine",image_caption_explain:`Sottotitola tutte le immagini all'URL specificato, aggiungendo "Immagine [idx]: [didascalia]" come tag alt per quelle senza. Ciò consente ai LLM a valle di interagire con le immagini in attività come il ragionamento e il riepilogo.`,images_summary:"Raccogli tutte le immagini alla fine",images_summary_explain:'Alla fine verrà creata una sezione "Immagini". Ciò fornisce ai LLM a valle una panoramica di tutti gli elementi visivi sulla pagina, il che può migliorare il ragionamento.',json_response:"Risposta JSON",json_response_explain:"La risposta sarà in formato JSON, contenente l'URL, il titolo, il contenuto e il timestamp (se disponibile). Nella modalità di ricerca, restituisce un elenco di cinque voci, ciascuna seguendo la struttura JSON descritta.",links_summary:"Raccogli tutti i collegamenti alla fine",links_summary_explain:'Alla fine verrà creata una sezione "Pulsanti e collegamenti". Ciò aiuta i LLM downstream o gli agenti web a navigare nella pagina o a intraprendere ulteriori azioni.',markdown:"Ribasso",markdown_explain:"Restituisce il markdown direttamente dall'HTML, aggirando il filtro di leggibilità.",mode:"Modalità di lettura o ricerca",mode_explain:"La modalità di lettura consente di accedere al contenuto di un URL, mentre la modalità di ricerca consente di cercare una query sul Web, applicando la modalità di lettura a ciascun URL dei risultati di ricerca.",no_cache:"Bypassa la cache",no_cache_explain:"Il nostro server API memorizza nella cache sia i contenuti in modalità Lettura che quelli in modalità Ricerca per un certo periodo di tempo. Per ignorare questa cache, imposta questa intestazione su true.",no_gfm:"Disabilitato",no_gfm_explain:"Funzionalità GFM (Github Flavored Markdown) disattivate.",no_gfm_table:"Nessuna tabella GFM",no_gfm_table_explain:"Disattiva la tabella GFM ma mantieni gli elementi HTML della tabella nella risposta.",opt_out_gfm:"Markdown aromatizzato su Github",opt_out_gfm_explain:"Attiva/disattiva le funzionalità di GFM (Github Flavored Markdown).",pageshot:"Pagina scattata",pageshot_explain:"Restituisce l'URL dell'immagine dello screenshot a pagina intera (con il massimo impegno).",post_with_url:"Utilizza il metodo POST",post_with_url_explain:"Utilizza il metodo POST anziché GET con un URL passato nel corpo. Utile per creare SPA con routing basato su hash.",proxy_server:"Utilizza un server proxy",proxy_server_explain:"Il nostro server API può utilizzare il tuo proxy per accedere agli URL, il che è utile per le pagine accessibili solo tramite proxy specifici.",references:"Riferimenti",references_explain:"Elenco separato da virgole dei riferimenti forniti dall'utente (URL)",remove_all_images:"Rimuovi tutte le immagini",remove_all_images_explain:"Rimuovi tutte le immagini dalla risposta.",remove_selector:"Selettore escluso",remove_selector_explain:"Fornisci un elenco di selettori CSS per rimuovere gli elementi specificati della pagina. Utile quando vuoi escludere parti specifiche della pagina come intestazioni, piè di pagina, ecc.",result_count:"Limite del risultato",result_count_explain:"Numero di risultati di ricerca da restituire.",return_format:"Formato del contenuto",return_format_explain:"Puoi controllare il livello di dettaglio nella risposta per evitare un filtro eccessivo. La pipeline predefinita è ottimizzata per la maggior parte dei siti Web e per l'input LLM.",screenshot:"Immagine dello schermo",screenshot_explain:"Restituisce l'URL dell'immagine della prima schermata.",set_cookie:"Cookie in avanti",set_cookie_explain:"Il nostro server API può inoltrare le tue impostazioni personalizzate dei cookie quando accedi all'URL, il che è utile per le pagine che richiedono un'autenticazione aggiuntiva. Tieni presente che le richieste con cookie non verranno memorizzate nella cache.",site_selector:"Ricerca nel sito",site_selector_explain:"Restituisce i risultati della ricerca solo dal sito Web o dal dominio specificato. Per impostazione predefinita esegue la ricerca in tutto il Web.",stream_mode:"Modalità flusso",stream_mode_explain:"La modalità streaming è vantaggiosa per le pagine di destinazione di grandi dimensioni, poiché consente più tempo per il rendering completo della pagina. Se la modalità standard genera contenuti incompleti, prendi in considerazione l'utilizzo della modalità Stream.",target_selector:"Selettore di destinazione",target_selector_explain:"Fornisci un elenco di selettori CSS per concentrarti su parti più specifiche della pagina. Utile quando il contenuto desiderato non viene visualizzato nelle impostazioni predefinite.",text:"Testo",text_explain:"Restituisce document.body.innerText.",wait_for_selector:"Attendi il selettore",wait_for_selector_explain:"Fornisci un elenco di selettori CSS per attendere che elementi specifici appaiano prima di tornare. Utile quando il contenuto desiderato non viene visualizzato nelle impostazioni predefinite.",with_gfm:"Abilitato",with_gfm_explain:"Funzionalità GFM (Github Flavored Markdown) abilitate.",with_iframe:"Abilita l'estrazione iframe",with_iframe_explain:"Estrae ed elabora il contenuto da tutti gli iframe incorporati nell'albero DOM",with_shadow_dom:"Abilita l'estrazione Shadow DOM",with_shadow_dom_explain:"Attraversa ed estrae il contenuto da tutte le radici Shadow DOM nel documento",x_timeout:"Tempo scaduto",x_timeout_explain:"Tempo massimo di attesa per il caricamento della pagina web. Nota che questo NON è il tempo totale per l'intera richiesta end-to-end."},how_to_stream:"Per elaborare il contenuto non appena diventa disponibile, imposta l'intestazione della richiesta sulla modalità streaming. Ciò riduce al minimo il tempo necessario alla ricezione del primo byte. Esempio nell'arricciatura:",how_to_use1:"Aggiungi https://r.jina.ai/ a qualsiasi URL nel tuo codice o strumento in cui è previsto l'accesso LLM. Ciò restituirà il contenuto principale della pagina in un testo pulito e compatibile con LLM.",how_to_use2:"Aggiungi https://s.jina.ai/ alla tua query. Questo chiamerà il motore di ricerca e restituirà i primi 5 risultati con i relativi URL e contenuti, ciascuno in testo pulito e compatibile con LLM.",how_to_use3:"Aggiungi https://g.jina.ai/ alla tua affermazione. Questo chiamerà il motore di giudizio e restituirà la percentuale di veridicità, un valore booleano che indica se l'affermazione è vera o falsa, un riepilogo della motivazione e un elenco di riferimenti.",key_required:"Chiave API richiesta per utilizzare questo endpoint",learn_more:"Saperne di più",open:"Apri in una nuova scheda",params_classification:"Parametri",raw_html:"HTML grezzo",reader_output:"Uscita del lettore",reader_response:"La risposta del lettore",reader_search_hint:"Se utilizzi questo URL nel codice, non dimenticare di codificare l'URL.",reader_url:"URL del lettore",reader_url_hint:"Fai clic di seguito per ottenere il contenuto tramite la nostra API Reader",requires_post_method:"Questa funzione richiede il metodo POST. Caricando il tuo file locale, il metodo POST verrà attivato automaticamente.",search_params:"Parametri di ricerca/intestazioni",search_query_rewrite:"Tieni presente che, a differenza della demo mostrata sopra, in pratica non cerchi la domanda originale sul web per il grounding. Ciò che le persone fanno spesso è riscrivere la domanda originale o utilizzare domande multi-hop. Leggono i risultati recuperati e quindi generano ulteriori query per raccogliere più informazioni necessarie prima di arrivare a una risposta finale.",select_mode:"Seleziona modalità",show_read_demo:"Scopri come Reader legge un URL",show_search_demo:"Scopri come Reader effettua ricerche sul Web",slow_warning:"Questa operazione potrebbe richiedere fino a 30 secondi e costare fino a 300.000 token per richiesta.",standard_usage:"Utilizzo standard",stream_mode:"Modalità flusso",stream_mode_explain:"La modalità stream è utile quando la pagina di destinazione è grande da visualizzare. Se ritieni che la modalità standard ti fornisca contenuti incompleti, prova la modalità streaming.",stream_mode_explain1:"La modalità streaming è utile quando si riscontra che la modalità standard fornisce un risultato incompleto. Questo perché la modalità streaming attenderà un po' più a lungo finché la pagina non sarà completamente renderizzata. Utilizza l'intestazione di accettazione per attivare/disattivare la modalità di streaming:",tagline:"Prova la demo",try_demo:"Dimostrazione",use_headers:"Il comportamento dell'API Reader può essere controllato con le intestazioni della richiesta. Ecco un elenco completo delle intestazioni supportate.",waiting_for_reader:"In attesa prima del risultato dell'API Reader...",warn_grounding_message:"Questo processo potrebbe richiedere fino a 30 secondi e consumare fino a 300K token per richiesta di grounding. Alcuni browser potrebbero terminare la richiesta a causa della lunga latenza, quindi ti consigliamo di copiare il codice ed eseguirlo dal tuo terminale.",warn_grounding_title:"Elevata latenza e utilizzo di token",your_query:"Inserisci la tua domanda",your_query_hint:"Digita una domanda che richiede le informazioni più recenti o la conoscenza del mondo.",your_statement:"La tua dichiarazione di fact-checking",your_url:"Inserisci il tuo URL",your_url_hint:"Fai clic di seguito per recuperare direttamente il codice sorgente della pagina"},description:"Leggi gli URL e cerca sul web per ottenere LLM più approfonditi.",dont_panic_api_key_is_free:"Niente panico! Ogni nuova chiave API contiene un milione di token gratuiti!",faq_v1:{answer1:`L'API Reader è gratuita e non richiede una chiave API. Basta anteporre "https://r.jina.ai/" al tuo URL.`,answer10:"No, l'API Reader può elaborare solo contenuti provenienti da URL accessibili pubblicamente.",answer11:"Se richiedi lo stesso URL entro 5 minuti, l'API Reader restituirà il contenuto memorizzato nella cache.",answer12:"Sfortunatamente no.",answer13:"Sì, puoi utilizzare il supporto PDF nativo dal Reader (https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4) o utilizzare la versione HTML da arXiv (https:// r.jina.ai/https://arxiv.org/html/2310.19923v4)",answer14:`Reader sottotitola tutte le immagini all'URL specificato e aggiunge "Immagine [idx]: [didascalia]" come tag alt (se inizialmente ne manca uno). Ciò consente ai LLM a valle di interagire con le immagini nel ragionamento, nel riepilogo, ecc.`,answer15:"L'API Reader è progettata per essere altamente scalabile. Viene ridimensionato automaticamente in base al traffico in tempo reale e il numero massimo di richieste simultanee è ora di circa 4000. Lo manteniamo attivamente come uno dei prodotti principali di Jina AI. Quindi sentitevi liberi di usarlo in produzione.",answer16:"Puoi trovare le informazioni più recenti sui limiti di tariffa nella tabella seguente. Tieni presente che stiamo lavorando attivamente per migliorare il limite di velocità e le prestazioni dell'API Reader, la tabella verrà aggiornata di conseguenza.",answer2:"L'API Reader utilizza un proxy per recuperare qualsiasi URL, visualizzandone il contenuto in un browser per estrarre contenuti principali di alta qualità.",answer3:"Sì, l'API Reader è open source e disponibile nel repository Jina AI GitHub.",answer4:"L'API Reader generalmente elabora gli URL e restituisce il contenuto entro 2 secondi, sebbene le pagine complesse o dinamiche potrebbero richiedere più tempo.",answer5:"Lo scraping può essere complicato e inaffidabile, in particolare con pagine complesse o dinamiche. L'API Reader fornisce un output ottimizzato e affidabile di testo pulito e pronto per LLM.",answer6:"L'API Reader restituisce il contenuto nella lingua originale dell'URL. Non fornisce servizi di traduzione.",answer7:"Se riscontri problemi di blocco, contatta il nostro team di supporto per assistenza e risoluzione.",answer8:"Sebbene progettata principalmente per le pagine Web, l'API Reader può estrarre contenuto dai PDF visualizzati in formato HTML su siti Web come arXiv, ma non è ottimizzata per l'estrazione PDF generale.",answer9:"Attualmente, l'API Reader non elabora i contenuti multimediali, ma i miglioramenti futuri includeranno i sottotitoli delle immagini e il riepilogo dei video.",question1:"Quali sono i costi associati all'utilizzo dell'API Reader?",question10:"È possibile utilizzare l'API Reader su file HTML locali?",question11:"L'API Reader memorizza nella cache il contenuto?",question12:"Posso utilizzare l'API Reader per accedere ai contenuti dietro un login?",question13:"Posso utilizzare l'API Reader per accedere ai PDF su arXiv?",question14:"Come funziona la didascalia delle immagini in Reader?",question15:"Qual è la scalabilità del Reader? Posso usarlo in produzione?",question16:"Qual è il limite di velocità dell'API Reader?",question2:"Come funziona l'API Reader?",question3:"L'API Reader è open source?",question4:"Qual è la latenza tipica per l'API Reader?",question5:"Perché dovrei utilizzare l'API Reader invece di raschiare la pagina da solo?",question6:"L'API Reader supporta più lingue?",question7:"Cosa devo fare se un sito web blocca l'API Reader?",question8:"L'API Reader può estrarre il contenuto dai file PDF?",question9:"L'API Reader può elaborare i contenuti multimediali dalle pagine Web?",title:"Domande comuni relative ai lettori"},fast:"Veloce",fast_stream:"Streaming immediato dei dati",fast_stream_description:"Hai bisogno di dati velocemente? La nostra API Reader può eseguire lo streaming dei dati per ridurre al minimo la latenza.",free:"Libero per sempre",free_description:"L'API Reader è gratuita! Non richiede carta di credito o segreto API. Non consumerà la tua quota di token.",is_free:"La parte migliore? È gratis!",is_free_description:"L'API Reader è disponibile gratuitamente e offre limiti di tariffa e prezzi flessibili. Costruito su un'infrastruttura scalabile, offre elevata accessibilità, concorrenza e affidabilità. Ci sforziamo di essere la soluzione di messa a terra preferita per i tuoi LLM.",open:"Apri in una nuova scheda",original_pdf:"PDF originale",rate_limit:"Limite di tariffa",read_grounding_release_note:"Leggi la nota di rilascio",reader_also_read_images:"Le immagini sulla pagina Web vengono automaticamente sottotitolate utilizzando un modello di linguaggio visivo nel lettore e formattate come tag alt dell'immagine nell'output. Ciò fornisce al tuo LLM a valle suggerimenti sufficienti per incorporare tali immagini nei suoi processi di ragionamento e riepilogo. Ciò significa che puoi porre domande sulle immagini, selezionarne di specifiche o persino inoltrare i loro URL a un VLM più potente per un'analisi più approfondita!",reader_description:"Converti un URL in un input compatibile con LLM, semplicemente aggiungendo <code>r.jina.ai</code> davanti.",reader_do_grounding:"Lettore per la verifica dei fatti",reader_do_grounding_explain:"Il nuovo endpoint di grounding offre un'esperienza di fact-checking end-to-end, quasi in tempo reale. Prende una data affermazione, la fonda utilizzando risultati di ricerca web in tempo reale e restituisce un punteggio di fattualità e i riferimenti esatti utilizzati. Puoi facilmente fondare le affermazioni per ridurre le allucinazioni LLM o migliorare l'integrità dei contenuti scritti da esseri umani.",reader_do_pdf_explain:"Sì, Reader supporta nativamente la lettura di PDF. È compatibile con la maggior parte dei PDF, compresi quelli con molte immagini, ed è velocissimo! In combinazione con un LLM, puoi facilmente creare un ChatPDF o un'intelligenza artificiale per l'analisi dei documenti in pochissimo tempo.",reader_do_search:"Lettore per la ricerca di messa a terra",reader_do_search_explain:"Gli LLM hanno un limite di conoscenza, il che significa che non possono accedere alle ultime conoscenze del mondo. Ciò porta a problemi come disinformazione, risposte obsolete, allucinazioni e altri problemi di fattualità. La messa a terra è assolutamente essenziale per le applicazioni GenAI. Reader ti consente di radicare il tuo LLM con le informazioni più recenti dal web. Basta anteporre https://s.jina.ai/ alla tua query e Reader effettuerà una ricerca sul Web e restituirà i primi cinque risultati con i relativi URL e contenuti, ciascuno in testo pulito e compatibile con LLM. In questo modo, puoi sempre mantenere aggiornato il tuo LLM, migliorarne la fattualità e ridurre le allucinazioni.",reader_reads_images:"Il lettore legge anche le immagini!",reader_reads_pdf:"Reader legge anche i PDF!",reader_result:"Risultato del lettore",table:{td_1_0:"Leggere un URL e restituirne il contenuto, utile per verificare la messa a terra",td_1_1:"20 giri al minuto",td_1_2:"200 giri al minuto",td_1_3:"In base ai token di output",td_1_4:"3 secondi",td_1_5:"3 secondi",td_2_0:"La ricerca sul Web restituisce i primi 5 risultati, utili per radicare la ricerca",td_2_1:"5 giri al minuto",td_2_2:"40 giri al minuto",td_2_3:"In base ai token di output per tutti e 5 i risultati della ricerca",td_2_4:"10 secondi",td_2_5:"10 secondi",th0:"Punto finale",th1:"Descrizione",th2:"Limite di velocità senza chiave API",th3:"Limite di velocità con chiave API",th4:"Schema di conteggio dei token",th5:"Latenza media",th6:"Latenza media"},title:"API del lettore",usage:"Utilizzo",usage_details_false:"Mostra solo gli utilizzi di base",usage_details_null:"Mostra gli usi di base e avanzati",usage_details_true:"Mostra solo gli usi avanzati",want_higher_rate_limit:"Desideri un limite di velocità più elevato fino a 1000 RPM? Possiamo supportarti!",what_is1:"Cos'è Reader?",what_is_answer_long:"L'inserimento delle informazioni web nei LLM è un passo importante per radicarsi, ma può essere impegnativo. Il metodo più semplice è raschiare la pagina web e alimentare l'HTML grezzo. Tuttavia, lo scraping può essere complesso e spesso bloccato e l'HTML grezzo è ingombro di elementi estranei come markup e script. L'API Reader risolve questi problemi estraendo il contenuto principale da un URL e convertendolo in testo pulito e compatibile con LLM, garantendo input di alta qualità per il tuo agente e i sistemi RAG.",what_is_desc:"Un proxy che accede a qualsiasi URL e trasforma il contenuto principale in testo semplice ottimizzato per LLM."},fe={confirm_message:"Nella tua chiave API restano {_leftTokens} token. L'invio del testo completo degli articoli {_numArticles} all'API Reranker, utilizzando il modello {_selectedReranker} per scoprire articoli correlati per la pagina corrente, ridurrà in modo significativo il conteggio dei token della tua chiave API {_APIKey}. Vuoi procedere?",confirm_title:"Avvertenza: utilizzo elevato di token",out_of_quota:"Questa chiave API ha esaurito i token. Ricarica il tuo account o utilizza una chiave API diversa.",recommend:"Ottieni i primi 5",recommended_articles:"Primi 5 articoli simili"},_e={benchmark:{description0:"LlamaIndex ha valutato varie combinazioni di incorporamenti e reranker per RAG, conducendo uno studio di replica che ha misurato il rango reciproco medio. I risultati evidenziano il significativo miglioramento della qualità della ricerca apportato da Jina Reranker, un vantaggio indipendente dagli specifici incorporamenti utilizzati.",description1:"BIER (Benchmarking IR) valuta l'efficacia del recupero di un modello, inclusi la pertinenza e l'NDCG. Un punteggio BIER più elevato è correlato a corrispondenze e classifiche dei risultati di ricerca più accurate.",description2:"Attraverso il benchmark LoCo, abbiamo misurato la comprensione di un modello della coerenza e del contesto locale, insieme alla classificazione specifica della query. Un punteggio LoCo più alto riflette una migliore capacità di identificare e dare priorità alle informazioni rilevanti.",description3:"L'MTEB (Multilingual Text Embedding Benchmark), nel complesso, mette alla prova le capacità di un modello negli incorporamenti di testo, inclusi clustering, classificazione, recupero e altri parametri. Tuttavia, per il nostro confronto, abbiamo utilizzato solo le attività di riclassificazione di MTEB.",title:"Segno di riferimento",title0:"LlamaIndex",title1:"BEIR",title2:"LoCo",title3:"MTEB"},benchmark_description:"Per fare un confronto, abbiamo incluso nel benchmark altri tre principali reranker di BGE (BAAI), BCE (Netease Youdao) e Cohere. Come mostrato dai risultati di seguito, Jina Reranker detiene il punteggio medio più alto in tutte le categorie rilevanti per il riranking, rendendolo un chiaro leader tra i suoi pari.",benchmark_title:"Benchmark delle prestazioni",choose_turbo:"Ottieni una velocità fino a 5 volte superiore con reranker-turbo",choose_turbo_description:"Offriamo anche due nuovi modelli di reranker open source: jina-reranker-v1-turbo-en e jina-reranker-v1-tiny-en, quest'ultimo ha solo 30 milioni di parametri e quattro livelli. Questi due nuovi reranker godono di una velocità di inferenza 5 volte più veloce rispetto al modello base con un costo molto basso in termini di qualità. Sono perfetti per le applicazioni che richiedono il riclassificazione in tempo reale. Leggi il benchmark qui sotto.",customize_urself:"Cambialo e guarda come cambia la risposta!",customize_urself_pl:"Cambiali e guarda come cambia la risposta!",description:"Recupero neurale di livello mondiale per massimizzare la pertinenza della ricerca.",description_rich:"Massimizza la pertinenza della ricerca e la precisione del RAG con la nostra API di riclassificazione all'avanguardia.",example_input_document:"Esempi di documenti candidati da classificare",example_input_query:"Interrogazione di esempio",faq_v1:{answer1:"I prezzi per l'API Reranker sono in linea con la nostra struttura dei prezzi dell'API Embedding. Si inizia con 1 milione di token gratuiti per ogni nuova chiave API. Oltre ai token gratuiti, sono disponibili per l'acquisto diversi pacchetti. Per maggiori dettagli, visita la nostra sezione prezzi.",answer10:"Sì, Jina Reranker può essere distribuito su AWS. Se hai bisogno di una distribuzione locale in un ambiente aziendale, puoi farlo facilmente tramite la nostra offerta AWS Marketplace.",answer11:"Se sei interessato a un reranking ottimizzato su misura per dati di dominio specifici, contatta il nostro team di vendita. Il nostro team risponderà tempestivamente alla tua richiesta.",answer3:"La differenza principale sta nella loro architettura. Per quanto riguarda le prestazioni, consigliamo jina-reranker-v1, che è stato ampiamente testato e confrontato con la concorrenza. Jina-reranker-v1 utilizza un'architettura cross-encoder, mentre Jina-colbert-v1 si basa sull'architettura ColBERTv2 ma estende la lunghezza del contesto sia della query che del documento a 8192, ottenendo prestazioni ancora migliori rispetto al modello ColBERTv2 originale.",answer4:"Sì, jina-colbert-v1 è open source ed è possibile accedervi tramite Huggingface. Tuttavia, jina-reranker-v1 non è open source.",answer5:"Attualmente supporta solo l'inglese. Tuttavia, alcuni utenti hanno segnalato che funziona bene anche con il cinese. Ciò potrebbe essere in parte dovuto al fatto che jina-reranker-v1-base-en condivide alcuni pesi con il nostro modello di incorporamento jina-embeddings-v2-base-zh.",answer6:"La lunghezza massima del token di query è 512. Non esiste alcun limite di token per i documenti.",answer7:"Puoi riclassificare fino a 2048 documenti per query.",answer8:"Non esiste il concetto di dimensione batch a differenza della nostra API di incorporamento. È possibile inviare solo una tupla di documento di query per richiesta, ma la tupla può includere fino a 2048 documenti candidati.",answer9:"La latenza varia da 100 millisecondi a 7 secondi, a seconda in gran parte della lunghezza dei documenti e della query. Ad esempio, la riclassificazione di 100 documenti di 256 token ciascuno con una query da 64 token richiede circa 150 millisecondi. Aumentando la lunghezza del documento a 4096 token, il tempo aumenta a 3,5 secondi. Se la lunghezza della query viene aumentata a 512 token, il tempo aumenta ulteriormente a 7 secondi.",question1:"Quanto costa l'API Reranker?",question10:"Posso distribuire Jina Reranker su AWS?",question11:"Offrite un reranking ottimizzato sui dati specifici del dominio?",question3:"Qual è la differenza tra i due reranker?",question4:"Jina Reranker è open source?",question5:"Il reranker supporta più lingue?",question6:"Qual è la lunghezza massima per query e documenti?",question7:"Qual è il numero massimo di documenti che posso riclassificare per query?",question8:"Qual è la dimensione del batch e quante tuple di documenti di query posso inviare in una richiesta?",question9:"Quale latenza posso aspettarmi quando riclassifico 100 documenti?",title:"Domande comuni relative al riclassificazione"},feature_on_premises_description2:"Distribuisci Jina Reranker su AWS Sagemaker e presto anche su Microsoft Azure e sui servizi cloud di Google oppure contatta il nostro team di vendita per ottenere distribuzioni Kubernetes personalizzate per il tuo cloud privato virtuale e i server locali.",feature_on_premises_description3:"Distribuisci Jina Reranker su AWS Sagemaker e Microsoft Azure e presto nei servizi cloud di Google, oppure contatta il nostro team di vendita per ottenere distribuzioni Kubernetes personalizzate per il tuo cloud privato virtuale e i server locali.",feature_solid_description:"Sviluppato dalla nostra ricerca accademica all'avanguardia e rigorosamente testato rispetto ai reranker SOTA per garantire prestazioni senza precedenti.",how_it_works:"Ecco come funziona:",how_it_works_v1:{description1:"Un sistema di ricerca utilizza embeddings/BM25 per trovare un'ampia serie di documenti potenzialmente rilevanti in base alla query dell'utente.",description2:"Il reranker prende quindi questi risultati e li analizza a un livello più granulare, considerando le sfumature di come i termini della query interagiscono con il contenuto del documento.",description3:"Riordina i risultati della ricerca, posizionando quelli che ritiene più rilevanti in alto, sulla base di questa analisi più approfondita.",title1:"Recupero iniziale",title2:"Riclassificazione",title3:"Risultati migliorati"},improve_performance:"Miglioramento garantito rispetto alla ricerca vettoriale",improve_performance_description:"Le nostre valutazioni hanno dimostrato miglioramenti per i sistemi di ricerca che utilizzano Jina Reranker con +8% nel tasso di successo e +33% nel ranking reciproco medio.",learning1:"Imparare a conoscere Reranker",learning1_description:"Cos'è un reranker? Perché la ricerca vettoriale o la somiglianza del coseno non sono sufficienti? Scopri i reranker da zero con la nostra guida completa.",read_more_about_benchmark:"Ulteriori informazioni sul benchmark",read_more_about_turbo:"Maggiori informazioni sui modelli turbo e tiny",read_more_about_v2:"Jina Reranker v2 è il miglior reranker della categoria rilasciato il 25 giugno 2024; è costruito per Agentic RAG. È dotato di supporto per chiamate di funzioni, recupero multilingue per oltre 100 lingue, funzionalità di ricerca di codice e offre una velocità 6 volte superiore rispetto alla versione v1. Ulteriori informazioni sul modello v2.",reranker_description:"Prova la nostra API di riclassificazione all'avanguardia per massimizzare la pertinenza della ricerca e la precisione del RAG. A partire gratis!",show_v2benchmark:"Mostra benchmark per il modello v2 (più recente)",table:{number_token_document:"Numero di token in ciascun documento",number_token_query:"Numero di token nella query",title:"Di seguito è riportato il tempo impiegato per riclassificare una query e 100 documenti in millisecondi:"},title:"API di riclassificazione",top_n:"Numero di documenti restituiti",top_n_explain:"Il numero di documenti più rilevanti da restituire per la query.",try_embedding:"Prova a incorporare l'API gratuitamente",try_reranker:"Prova gratuitamente l'API reranker",v2_features:{description1:"Reranker v2 consente il recupero di documenti in oltre 100 lingue, indipendentemente dalla lingua della query.",description2:"Reranker v2 classifica i frammenti di codice e le firme delle funzioni in base a query in linguaggio naturale, ideali per le applicazioni Agentic RAG.",description3:"Reranker v2 classifica le tabelle più rilevanti in base a query in linguaggio naturale, aiutando a ordinare diversi schemi di tabelle e a identificare quello più rilevante prima di generare una query SQL.",title1:"Recupero multilingue",title2:"Chiamata di funzioni e ricerca di codici",title3:"Supporto dati tabulari e strutturati"},v2benchmark:{descBeir:"Punteggi NDCG 10 riportati per diversi modelli di riclassificazione per il set di dati Beir",descCodeSearchNet:"Punteggi MRR 10 riportati per diversi modelli di riclassificazione per il set di dati CodeSearchNet",descMKQA:"Richiama 10 punteggi riportati per diversi modelli di riclassificazione per il set di dati MKQA",descNSText2SQL:"Richiama 3 punteggi riportati per diversi modelli di riclassificazione per il set di dati NSText2SQL",descRTX4090:"Punteggi di throughput (documenti recuperati in 50 ms) riportati per diversi modelli di riclassificazione su una GPU RTX 4090.",descToolBench:"Richiama 3 punteggi riportati per diversi modelli di riclassificazione per il set di dati ToolBench",titleBeir:"BEIR (Benchmark eterogeneo su diversi compiti IR)",titleCodeSearchNet:"CodeSearchNet. Il benchmark è una combinazione di query in formato docstring e linguaggio naturale, con segmenti di codice etichettati pertinenti alle query.",titleMKQA:"MKQA (Domande e risposte sulla conoscenza multilingue)",titleNSText2SQL:"NSText2SQL",titleRTX4090:"Velocità effettiva di Jina Reranker v2 su RTX4090",titleToolBench:"Banco degli attrezzi. Il benchmark raccoglie oltre 16mila API pubbliche e le corrispondenti istruzioni generate sinteticamente per utilizzarle in impostazioni API singole e multi-API."},vs_table:{col0:"Riclassificazione",col0_1:"Maggiore precisione e pertinenza della ricerca",col0_2:"Filtraggio iniziale rapido",col0_3:"Recupero generale del testo attraverso query ad ampio raggio",col1:"Ricerca vettoriale",col1_1:"Dettagliato: documento secondario e segmento di query",col1_2:"Ampio: interi documenti",col1_3:"Intermedio: vari segmenti di testo",col2:"BM25",col2_1:"Alto",col2_2:"medio",col2_3:"Basso",col3_1:"Non richiesto",col3_2:"Alto",col3_3:"Basso, utilizza un indice predefinito",col4_1:"Alto",col4_2:"Alto",col4_3:"Non richiesto",col5_1:"Superiore per query sfumate",col5_2:"In equilibrio tra efficienza e precisione",col5_3:"Coerente e affidabile per un'ampia gamma di query",col6_1:"Altamente accurato con una profonda comprensione del contesto",col6_2:"Veloce ed efficiente, con una precisione moderata",col6_3:"Altamente scalabile, con efficacia consolidata",col7_1:"Ad alta intensità di risorse con implementazione complessa",col7_2:"Potrebbe non acquisire il contesto o le sfumature approfondite della query",col7_3:"Potrebbe avere prestazioni inferiori per ricerche altamente specifiche o contestuali",header0:"Ideale per",header1:"Granularità",header2:"Complessità temporale delle query",header3:"Complessità temporale dell'indicizzazione",header4:"Complessità del tempo di allenamento",header5:"Ricerca di qualità",header6:"Punti di forza",header7:"Punti deboli",subtitle:"La tabella seguente fornisce un confronto completo tra Reranker, ricerca di vettori/incorporamenti e BM25, evidenziandone i punti di forza e di debolezza nelle varie categorie.",title:"Confronto tra Reranker, Ricerca vettoriale e BM25"},what_is:"Cos'è un reranker?",what_is_answer_long:`L'obiettivo di un sistema di ricerca è trovare i risultati più pertinenti in modo rapido ed efficiente. Tradizionalmente, metodi come BM25 o tf-idf sono stati utilizzati per classificare i risultati di ricerca in base alla corrispondenza delle parole chiave. Metodi recenti, come la somiglianza del coseno basata sull'incorporamento, sono stati implementati in molti database vettoriali. Questi metodi sono semplici ma a volte possono non cogliere le sottigliezze del linguaggio e, soprattutto, l'interazione tra i documenti e l'intento di una query.

È qui che brilla il "reranker". Un reranker è un modello di intelligenza artificiale avanzato che prende il set iniziale di risultati da una ricerca, spesso forniti da una ricerca basata su incorporamenti/token, e li rivaluta per garantire che siano più allineati con l'intento dell'utente. Si guarda oltre la corrispondenza superficiale dei termini per considerare l'interazione più profonda tra la query di ricerca e il contenuto dei documenti.`,what_is_answer_long_ending:"Il reranker può migliorare significativamente la qualità della ricerca perché opera a livello di sottodocumento e sottoquery, il che significa che esamina le singole parole e frasi, i loro significati e il modo in cui si relazionano tra loro all'interno della query e dei documenti. Ciò si traduce in un insieme di risultati di ricerca più precisi e contestualmente pertinenti.",what_is_desc:"Un reranker è un modello di intelligenza artificiale che affina i risultati della ricerca da una ricerca vettoriale o da un modello di recupero denso. Per saperne di più."},be={caption_image_desc:"Genera una descrizione testuale dell'immagine.",caption_image_title:"Immagine della didascalia",description:"Esplora la narrazione di immagini oltre i pixel",example1:"Questo video sembra essere un filmato naturalistico con un affascinante coniglietto bianco e una farfalla in un campo erboso. Il coniglietto viene visto interagire con la farfalla in diversi modi, mostrando la loro relazione unica. L'ambiente naturale offre uno sfondo pittoresco, esaltando la bellezza di questa scena semplice ma accattivante.",generate_story_desc:"Crea una storia ispirata all'immagine, spesso con dialoghi o monologhi dei suoi personaggi.",generate_story_title:"Genera storia",intro1:"Soluzione AI leader per didascalie di immagini e riepiloghi video",json_image_desc:"Genera un formato JSON strutturato dall'immagine utilizzando uno schema predefinito. Ciò consente l'estrazione di dati specifici dall'immagine.",json_image_title:"Estrai JSON dall'immagine",summarize_video_desc:"Genera un riepilogo conciso del video, evidenziando gli eventi chiave.",summarize_video_title:"Riepiloga il video",visual_q_a_desc:"Rispondi a una domanda in base al contenuto dell'immagine.",visual_q_a_title:"Domande e risposte visive"},he={ask_on_current_page:"Chiedi alla pagina corrente informazioni su...",find_solution:"Genera una soluzione per...",hint:"Cerca tra prodotti, novità e le tue domande",hotkey:"Premere il tasto / per effettuare la ricerca in questa pagina",hotkey1:"Premere",hotkey2:"per attivare",hotkey_long1:"In qualsiasi momento, premere",hotkey_long3:"per aprire la barra di ricerca",more_results:"{_numMore} altri risultati",placeholder:"Fai qualsiasi domanda in questa pagina",proposing_solution:"Generazione della risposta in base al contenuto della pagina...",required:"Descrivi la tua domanda con maggiori dettagli.",results:"risultati"},Ie={description:"Naviga, interagisci, perfeziona: reinventa la scoperta del prodotto"},ke={description:"Colmare il divario semantico nella tua infrastruttura di ricerca esistente"},Ae={"Hacker News":"Notizie sugli hacker",LinkedIn:"LinkedIn",facebook:"Facebook",reddit:"Reddit",rss:"RSS Feed",share_btn:"Condividere",twitter:"X (Twitter)"},Pe={click_to_learn_more:"Clicca per saperne di più",contextualization:"Contestualizzazione",contextualization_desc:"I reranker regolano i risultati della ricerca iniziale in base alla profonda pertinenza contestuale. domanda. Ciò perfeziona la classifica per corrispondere meglio a ciò che gli utenti potrebbero trovare utile.",coreInfra:"Core Infra",coreInfra_desc:"Core Infra fornisce un livello cloud-native per lo sviluppo, l'implementazione e l'orchestrazione di modelli di base della ricerca sia nel cloud pubblico che on-premise, consentendo ai servizi di scalare verso l'alto e verso il basso senza sforzo.",embedding_serving:"Incorporamento della pubblicazione",embedding_serving_description:"Fornire incorporamenti tramite un microservizio robusto e scalabile utilizzando tecnologie native del cloud.",embedding_tech:"Incorporamenti",embedding_tech_description:`In Jina AI, sfruttiamo la potenza dell'integrazione della tecnologia per rivoluzionare diverse applicazioni di intelligenza artificiale. Questa tecnologia funge da metodo unificato per rappresentare e comprimere in modo efficiente vari tipi di dati, garantendo l'assenza di perdita di informazioni critiche. Il nostro obiettivo è trasformare set di dati complessi in un formato di incorporamento universalmente comprensibile, essenziale per un'analisi AI precisa e approfondita.

Gli incorporamenti sono fondamentali, soprattutto in applicazioni come il riconoscimento preciso di immagini e voce, dove aiutano a distinguere dettagli e sfumature a grana fine. Nell'elaborazione del linguaggio naturale, gli incorporamenti migliorano la comprensione del contesto e del sentimento, portando a strumenti di intelligenza artificiale conversazionale e di traduzione linguistica più accurati. Sono inoltre cruciali nello sviluppo di sofisticati sistemi di raccomandazione che richiedono una profonda comprensione delle preferenze degli utenti attraverso diverse forme di contenuto, come testo, audio e video.`,embedding_tuning:"Incorporamento dell'ottimizzazione",embedding_tuning_description:"Ottimizzazione degli incorporamenti di alta qualità integrando le competenze del settore per migliorare le prestazioni specifiche delle attività.",embeddings:"Incorporamenti",embeddings_desc:"Gli incorporamenti sono i pilastri del moderno sistema di ricerca, rappresentando dati multimodali in vettori di numeri. Questo processo consente una comprensione più sfumata e contestuale dei contenuti, ben oltre la semplice corrispondenza delle parole chiave.",for_developers:"Per gli sviluppatori",for_enterprise:"Per le Imprese",for_power_users:"Per utenti esperti",grounding:"Messa a terra",grounding_desc:"Lettore che perfeziona input e risultati attraverso LLM. Migliorano la qualità, la leggibilità e la fattualità della risposta finale.",model_serving:"Modello che serve",model_serving_description:"La distribuzione di modelli ottimizzati in un ambiente di produzione, che in genere richiede risorse sostanziali come l'hosting GPU. MLOps, enfatizzando la fornitura di modelli di medie e grandi dimensioni in modo scalabile, efficiente e affidabile.",model_tuning:"Messa a punto del modello",model_tuning_description:"Conosciuto anche come fine tuning, comporta la regolazione dei parametri di un modello preaddestrato su un nuovo set di dati, spesso specifico per attività, per migliorarne le prestazioni e adattarlo a un'applicazione specifica.",personalization:"Personalizzazione",personalization_desc:"Utilizzo di dati sintetici guidati dalle istruzioni dell'utente per addestrare automaticamente un modello di incorporamento e riclassificazione specifico del dominio.",preprocessing:"Pre-elaborazione",preprocessing_desc:"La pre-elaborazione comporta la pulizia, la normalizzazione e la trasformazione dei dati grezzi in un formato digeribile dal sistema di ricerca.",promptOps:"PromptOps",promptOps_desc:"Prompt Ops migliora l'input e l'output del sistema di ricerca, compresi quelli utilizzati nell'espansione delle query, nell'input LLM e nella riscrittura dei risultati. Ciò garantisce che la ricerca sia compresa meglio e dia risultati migliori.",prompt_serving:"Servizio rapido",prompt_serving_description:"Wrapping e fornitura di prompt tramite un'API, senza ospitare modelli pesanti. L'API chiama un servizio di modello di linguaggio di grandi dimensioni pubblico e gestisce l'orchestrazione di input e output in una catena di operazioni.",prompt_tech:"Ingegneria dei prompt e degli agenti",prompt_tech_description:`In Jina AI, riconosciamo che il prompt engineering è vitale per interagire con modelli linguistici di grandi dimensioni (LLM). Man mano che questi modelli avanzano, la complessità dei suggerimenti aumenta, comprendendo ragionamenti e logiche intricati. Questo progresso sottolinea la crescita intrecciata degli LLM e la rapida sofisticazione.

Prevediamo un futuro in cui gli LLM fungeranno da compilatori, con i prompt che diventeranno il nuovo linguaggio di programmazione. Questo cambiamento suggerisce che la futura competenza tecnologica potrebbe concentrarsi maggiormente sulla padronanza tempestiva rispetto alla codifica tradizionale. Il nostro impegno in Jina AI è quello di guidare in quest'area di trasformazione, rendendo l'intelligenza artificiale avanzata accessibile e pratica per l'uso quotidiano attraverso la padronanza di questo "linguaggio" emergente.`,prompt_tuning:"Sintonizzazione rapida",prompt_tuning_description:"Il processo di creazione e raffinamento dell'input richiede al fine di guidare il suo output verso risposte specifiche e desiderate.",representation:"Rappresentazione",representation_desc:"Gli incorporamenti trasformano i dati multimodali in un formato uniforme e vettoriale. Ciò consente al sistema di ricerca di comprendere e classificare i contenuti oltre le semplici parole chiave.",rerankers:"Riclassificazione",rerankers_desc:"I reranker prendono i risultati iniziali dagli incorporamenti e li perfezionano, garantendo che all'utente vengano presentati i risultati più rilevanti. Questo è fondamentale per fornire risultati di ricerca di alta qualità che soddisfino le intenzioni dell'utente."},qe={care_most:"Cosa ti interessa di più?",care_most_options:{accuracy:"Precisione",cost:"Costo",other:"Altro",scalability:"Scalabilità",speed:"Velocità"},care_most_required:"Quando scegli un servizio, cosa ti interessa di più?",company_size:"Qual è la dimensione della tua azienda?",company_size_required:"Raccontaci che le dimensioni della tua azienda ci aiutano a fornire un servizio migliore",company_url:"Qual è il sito web della tua azienda?",company_url_required:"Raccontaci che il sito web della tua azienda ci aiuta a fornire un servizio migliore",contactName:"Il tuo nome",contactName_required:"Come dovremmo rivolgerci a te?",contactTitle:"Qual è la tua mansione?",contactTitle_required:"Il tuo titolo professionale è obbligatorio",contact_us:"Contattaci",domain_required:"Raccontaci che il tuo dominio di lavoro ci aiuta a fornire un servizio migliore",email:"E-mail",email_contact:"La tua e-mail di contatto",email_invalid:"L'email non è valida",email_required:"L'e-mail è obbligatoria",fine_tuned_embedding:"Sei interessato a incorporamenti ottimizzati su misura per i tuoi dati e il tuo caso d'uso? Discutiamone!",fine_tuned_reranker:"Ti interessano riranker ottimizzati su misura per i tuoi dati e il tuo caso d'uso? Discutiamone!",full_survey:"Partecipa al sondaggio completo e ottieni una risposta più rapida dal nostro team",get_new_key:"Ottieni la tua chiave API",get_update_blog_posts:"Ricevi gli ultimi aggiornamenti per i post del blog",get_update_embeddings:"Ottieni gli ultimi aggiornamenti per gli incorporamenti",send:"Inviare",sign_up:"Iscrizione",subscribe:"sottoscrivi",tell_domain:"Raccontaci il tuo dominio",usage_type:"Quale tipo di utilizzo ti descrive meglio?",usage_type_options:{other:"Altro",poc:"Verifica teorica",production:"Produzione",research:"Ricerca"},usage_type_required:"Comunicaci che il tuo tipo di utilizzo ci aiuta a fornire un servizio migliore",used_product:"Quale modello stai utilizzando?",used_product_required:"Seleziona il modello che stai utilizzando o a cui sei interessato"},Le={description:"Tecniche di agente per aumentare il tuo LLM e spingerlo oltre i suoi limiti"},Se="Indice dei contenuti",Ce={advance_usage:"Utilizza la richiesta POST per altre funzionalità",basic_usage:"Utilizzare la richiesta GET per contare i token",basic_usage_explain:"Puoi semplicemente inviare una richiesta GET per contare il numero di token nel tuo testo.",change_content:"Cambia 'contenuto' e guarda il risultato in tempo reale",chars:"caratteri",chinese:"cinese",chunk:"Pezzo",chunk_all:"Tutti i pezzi",chunking:"Suddividere documenti lunghi in frammenti, alla velocità della luce!",chunking_explain:"Puoi anche usare Segmenter API per tagliare documenti lunghi in blocchi più piccoli, rendendone più facile l'elaborazione in incorporamenti o reranker. Sfruttiamo spunti strutturali comuni e creiamo un set di regole ed euristiche che funzionano bene su diversi tipi di contenuto, ad esempio Markdown, HTML, LaTeX e linguaggi CJK.",chunking_short:"suddivisione in blocchi",chunks_in_total:"{_numChunks} blocchi in totale",count_tokens_hint:"<b>{_numTokens}</b> token, {_numChars} caratteri.",description:"Tagliare il testo lungo in blocchi ed effettuare la tokenizzazione.",description_long:"La nostra API Segmenter è fondamentale per aiutare gli LLM a gestire l'input entro i limiti del contesto e ottimizzare le prestazioni del modello. Consente agli sviluppatori di contare i token ed estrarre segmenti di testo rilevanti, garantendo un'elaborazione efficiente dei dati e una gestione dei costi.",description_long1:"API gratuita per la segmentazione di testi lunghi in blocchi e tokenizzazione.",english:"Inglese",explain:"Un segmentatore è un componente cruciale che converte il testo in token o blocchi, che sono le unità di dati di base che un modello di incorporamento/reranker o LLM elabora. I token possono rappresentare parole intere, parti di parole o persino singoli caratteri.",faq_v1:{answer1:"L'API Segmenter è gratuita. Fornendo la tua chiave API, puoi accedere a un limite di tariffa più elevato e la tua chiave non verrà addebitata.",answer10:"Oltre alle lingue occidentali, la suddivisione in blocchi funziona bene anche con il cinese, il giapponese e il coreano.",answer2:"Senza una chiave API, è possibile accedere all'API Segmenter a una velocità massima di 20 RPM.",answer3:"Con una chiave API, puoi accedere alla Segmenter API a un limite di velocità di 200 RPM. Per gli utenti premium a pagamento, il limite di velocità è di 1000 RPM.",answer4:"No, la tua chiave API viene utilizzata solo per accedere a un limite di velocità più elevato.",answer5:"Sì, l'API Segmenter è multilingue e supporta oltre 100 lingue.",answer6:"Le richieste GET sono utilizzate esclusivamente per contare il numero di token in un testo, consentendoti di integrarlo facilmente come contatore nella tua applicazione. Le richieste POST supportano più parametri e funzionalità, come la restituzione dei primi/ultimi N token.",answer7:"È possibile inviare fino a 64.000 caratteri per richiesta.",answer8:"La funzione di chunking segmenta i documenti lunghi in blocchi più piccoli in base a comuni indizi strutturali, assicurando una segmentazione accurata del testo in blocchi significativi. In sostanza, si tratta di un (grande!) modello regex che segmenta il testo in base a determinate caratteristiche sintattiche che spesso si allineano con i confini semantici, come terminazioni di frase, interruzioni di paragrafo, punteggiatura e determinate congiunzioni. Non è un chunking semantico. Questa (grande) regex è potente quanto può esserlo entro i limiti delle espressioni regolari. Bilancia complessità e prestazioni. Mentre una vera comprensione semantica non è possibile con regex, approssima bene il contesto tramite comuni indizi strutturali.",answer9:"Se l'input contiene token speciali, la nostra API Segmenter li inserirà nel campo 'special_tokens'. Ciò ti consente di identificarli facilmente e gestirli di conseguenza per le tue attività downstream, ad esempio rimuovendoli prima di immettere il testo in un LLM per prevenire attacchi di iniezione.",question1:"Quanto costa l'API Segmenter?",question10:"La suddivisione in blocchi supporta anche altre lingue oltre all'inglese?",question2:"Se non fornisco una chiave API, qual è il limite di velocità?",question3:"Se fornisco una chiave API, qual è il limite di velocità?",question4:"Addebiterete i token dalla mia chiave API?",question5:"L'API Segmenter supporta più lingue?",question6:"Qual è la differenza tra le richieste GET e POST?",question7:"Qual è la lunghezza massima che posso tokenizzare per richiesta?",question8:"Come funziona la funzione di chunking? È un chunking semantico?",question9:"Come si gestiscono i token speciali come 'endoftext' nella Segmenter API?",title:"Domande frequenti relative al segmentatore"},free_api:"L'API Segmenter è gratuita. Fornendo la tua chiave API, puoi accedere a un limite di tariffa più elevato e la tua chiave non verrà addebitata.",input_text:"Testo di input",is_free:"L'API Segmenter è gratuita!",is_free_description:"Fornendo la tua chiave API, potrai accedere a un limite di tariffa più elevato e la tua chiave non verrà addebitata.",japanese:"giapponese",korean:"coreano",parameters:{auth_token:"Aggiungi la chiave API per un limite di velocità più elevato",auth_token_explain:"Inserisci la tua chiave API Jina per accedere a un limite di velocità più elevato. Per le informazioni più recenti sul limite di velocità, fai riferimento alla tabella sottostante.",head:"Restituisci i primi N token",head_explain:"Restituisce i primi N token del contenuto specificato. Esclusivo di confine. Non può essere utilizzato con 'tail'.",learn_more:"Saperne di più",max_chunk_length:"Lunghezza massima di ogni blocco",max_chunk_length_explain:"Numero massimo di caratteri in ogni blocco. In pratica la lunghezza del blocco può essere inferiore a questo valore, se c'è un confine naturale nel testo.",return_chunks:"Restituisci i pezzi",return_chunks_explain:"Suddividere l'input in segmenti semanticamente significativi, gestendo al contempo un'ampia gamma di tipologie di testo e casi limite in base a spunti strutturali comuni.",return_tokens:"Restituisci i token",return_tokens_explain:"Restituisci i token e i loro ID corrispondenti nella risposta. Attiva/disattiva per vedere la visualizzazione del risultato.",tail:"Restituisce gli ultimi N token",tail_explain:"Restituisce gli ultimi N token del contenuto specificato. Esclusivo di confine. Non può essere utilizzato con 'head'.",type:"Segmentatore",type_explain:"Scegli il tokenizzatore da utilizzare.",used_by_models:"Utilizzato in {_usedBy}."},remove_boundary_cues:"Rimuovi le interruzioni di riga",remove_boundary_cues_explain:"Rimuovi tutte le interruzioni di riga (i principali segnali di confine) dall'input: questo rende il problema più impegnativo e osserva come cambia la risposta!",show_space:"Mostra spazi iniziali/finali",table:{td_1_0:"Tokenizza i testi, conta e ottieni i primi/ultimi N token.",td_1_1:"20 giri al minuto",td_1_2:"200 giri al minuto",td_1_3:"1000 giri al minuto",td_1_4:"Nessun costo",td_1_5:"800 ms"},title:"API del segmentatore",token_index:"Indice token: {_index}",usage:"Utilizzo",visualization:"Visualizzazione",what_is:"Che cosa è un Segmenter?"},ye={cta:"Traduci in codice {_lang}",select_language:"Lingua"},we={description:"Un database vettoriale Python di cui hai solo bisogno, né più né meno"},Re="zzz",Me={PRODUCT_DESCRIPTION:e,SEO_TAG_LINE:i,about_us_page:a,api_general_faq:o,autotune:n,avatar:t,best_banner:r,beta:l,billing_general_faq:s,blog_tags:c,book2024:d,cclicence:u,classifier:m,clip_as_service:p,cloud:g,contact_us_page:z,copy:v,copy_to_clipboard_success:f,dalle_flow:_,"dev-gpt":{description:"Il tuo team di sviluppo virtuale"},disco_art:b,doc_array:h,download:I,embedding:k,embeddings:A,estimator:P,faq:q,faq_button:L,farewell:S,finetuner:C,finetuner_plus:y,finetuning:w,footer:R,get_new_key:M,github:T,grounding:x,header:U,hub:E,huggingface:G,impact_snapshots:D,inference:B,integrations:j,internship_faq:N,internship_page:O,jcloud:Q,jerboa:J,jina:F,jina_chat:H,key_manager:W,lab_dialog:V,landing_page:K,langchain_serve:X,legal_page:$,model_graph:Y,models:Z,news_page:ee,newsroom_page:ie,notice:ae,open_day:oe,open_day_faq:ne,open_gpt:te,paywall:re,powered_by:le,print:se,project_status:ce,prompt_perfect:de,promptperfect:ue,purchase:me,purchase_now:pe,rate_limit:ge,rationale:ze,reader:ve,recommender:fe,reranker:_e,scenex:be,searchbar:he,searchscape:Ie,semantic:ke,share:Ae,spectrum:Pe,subscribe_system:qe,think_gpt:Le,toc:Se,tokenizer:Ce,translator:ye,vectordb:we,zzz:Re};export{e as PRODUCT_DESCRIPTION,i as SEO_TAG_LINE,a as about_us_page,o as api_general_faq,n as autotune,t as avatar,r as best_banner,l as beta,s as billing_general_faq,c as blog_tags,d as book2024,u as cclicence,m as classifier,p as clip_as_service,g as cloud,z as contact_us_page,v as copy,f as copy_to_clipboard_success,_ as dalle_flow,Me as default,b as disco_art,h as doc_array,I as download,k as embedding,A as embeddings,P as estimator,q as faq,L as faq_button,S as farewell,C as finetuner,y as finetuner_plus,w as finetuning,R as footer,M as get_new_key,T as github,x as grounding,U as header,E as hub,G as huggingface,D as impact_snapshots,B as inference,j as integrations,N as internship_faq,O as internship_page,Q as jcloud,J as jerboa,F as jina,H as jina_chat,W as key_manager,V as lab_dialog,K as landing_page,X as langchain_serve,$ as legal_page,Y as model_graph,Z as models,ee as news_page,ie as newsroom_page,ae as notice,oe as open_day,ne as open_day_faq,te as open_gpt,re as paywall,le as powered_by,se as print,ce as project_status,de as prompt_perfect,ue as promptperfect,me as purchase,pe as purchase_now,ge as rate_limit,ze as rationale,ve as reader,fe as recommender,_e as reranker,be as scenex,he as searchbar,Ie as searchscape,ke as semantic,Ae as share,Pe as spectrum,qe as subscribe_system,Le as think_gpt,Se as toc,Ce as tokenizer,ye as translator,we as vectordb,Re as zzz};

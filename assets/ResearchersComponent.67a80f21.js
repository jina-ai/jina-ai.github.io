import{e as o,ak as l,al as t,am as a,an as d,ao as m,aq as n,ap as g,av as c}from"./index.f5605212.js";import{Q as p}from"./QChip.dae2739c.js";import{P as h}from"./PublicationCardComponent.1f00838f.js";var u="/assets/paper_1.8ed04d7d.png",f="/assets/paper_2.59e1399a.png",v="/assets/paper_3.b50bdc91.png",k="/assets/paper_4.e1ffeaf5.png",b="/assets/paper_5.328264e2.png";const E=o({__name:"ResearchersComponent",props:{cardInLight:{type:Boolean,default:!0,required:!1}},setup(i){const s=[{title:"Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models",summary:"Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness.",paperImage:b,link:"https://arxiv.org/abs/2406.14848",date:"2024/06/21"},{title:"Jina CLIP: Your CLIP Model Is Also Your Text Retriever",summary:"Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.",paperImage:k,link:"https://arxiv.org/abs/2405.20204",date:"2024/05/30",conference:"ICML 2024"},{title:"Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings",summary:"We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations. By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embedding models. This integration aims to stimulate further research and advancement in text embedding technologies for these languages.",paperImage:v,link:"https://arxiv.org/abs/2402.17016",date:"2024/02/26"},{title:"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents",summary:"Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.",paperImage:u,link:"https://arxiv.org/abs/2310.19923",date:"2023/10/30"},{title:"Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models",summary:"Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB).",paperImage:f,link:"https://arxiv.org/abs/2307.11224",date:"2023/07/20",conference:"EMNLP 2023"}];return(y,x)=>(a(),l(p,{class:"row no-wrap scroll-x q-py-md"},{default:t(()=>[(a(),d(c,null,m(s,(e,r)=>n(g,{key:r,href:e.link,target:"_blank",clickable:"",class:"q-pa-none q-mx-sm"},{default:t(()=>[n(h,{title:e.title,style:{width:"300px"},conference:e.conference,date:e.date,"card-in-light":i.cardInLight,paperImage:e.paperImage},null,8,["title","conference","date","card-in-light","paperImage"])]),_:2},1032,["href"])),64))]),_:1}))}});export{E as _};

const e="Nous fournissons les meilleurs intégrations, reclassements, lecteurs LLM et optimiseurs d'invites, ainsi qu'une IA de recherche pionnière pour les données multimodales.",s="Votre base de recherche, survoltée!",n={approach:"Notre approche",approach_connect_dots:"Relier les points : les utilisateurs expérimentés aux entreprises",approach_connect_dots_description:"Alors, pourquoi l'accent mis sur les utilisateurs expérimentés est-il essentiel pour notre modèle centré sur l'entreprise ? Parce qu'il s'agit d'établir des relations précoces. En s'adressant aux utilisateurs expérimentés dès maintenant, nous construisons des ponts vers les entreprises qu'ils influenceront à l'avenir. Il s'agit d'un jeu stratégique - un investissement à long terme pour garantir que notre offre d'entreprise reste prioritaire lorsque ces utilisateurs expérimentés accèdent à des rôles décisionnels au sein des organisations.",approach_content1:"Dans le monde en évolution rapide de l'IA, les stratégies doivent être à la fois agiles et avant-gardistes. Bien que notre offre principale reste centrée sur les entreprises, le paysage de l'IA a évolué d'une manière qui nécessite de repenser notre approche de l'acquisition de clients. Voici pourquoi l'introduction des utilisateurs expérimentés comme point d'entrée de notre entonnoir n'est pas seulement innovante, mais cruciale pour notre croissance soutenue dans le secteur des entreprises.",approach_content2:"Chez Jina AI, notre stratégie est d'être proactif plutôt que réactif. L'inclusion des utilisateurs expérimentés comme point d'entrée de l'entonnoir garantit que nous capturons non seulement les tendances actuelles du marché, mais que nous sommes également stratégiquement prêts pour la croissance future de l'entreprise. Notre engagement envers les entreprises reste inébranlable ; cependant, notre approche pour les atteindre est innovante, robuste et, surtout, avant-gardiste.",approach_content4:`Tout le monde souhaite une meilleure recherche. Chez Jina AI, nous permettons une meilleure recherche en fournissant la <span class="text-primary text-bold">Search Foundation</span>, qui se compose d'Embeddings, de Rerankers, de Reader et de Prompt Ops. Ces composants fonctionnent de concert pour révolutionner la façon dont nous recherchons et comprenons les données.`,approach_miss_mark:"Pourquoi les MLOps traditionnels passent à côté de la cible",approach_miss_mark_description:"Alors que l'afflux d'utilisateurs expérimentés est important, les outils MLOps traditionnels sont mal équipés pour répondre à leurs besoins. Ces outils rappellent l'utilisation d'un tracteur pour naviguer dans les rues de la ville - ils sont lourds et souvent excessifs. Les développeurs de la nouvelle génération exigent des outils agiles et intuitifs qui complètent leur rythme de développement rapide.",approach_new_paradigm:"Technologie basée sur les invites : un nouveau paradigme",approach_new_paradigm_description:`2023 a annoncé un changement important : l'essor de la technologie basée sur les invites. En simplifiant le processus de développement de l'IA, il a démocratisé l'accès aux outils d'IA. Désormais, ceux qui n'ont pas une expérience approfondie de la programmation, appelés "utilisateurs expérimentés", peuvent s'engager dans le développement de l'IA sans les courbes d'apprentissage abruptes associées à des outils tels que Pytorch, Docker ou Kubernetes.

En établissant un parallèle, cela s'apparente à l'évolution de l'informatique personnelle. Au départ, seuls les experts en technologie utilisaient des ordinateurs. Mais avec l'avènement d'interfaces conviviales, un public plus large pourrait participer. Aujourd'hui, avec la technologie basée sur les invites, nous assistons à une démocratisation similaire de l'IA.`,awards:"Récompenses et reconnaissance",berlin:"Berlin, Allemagne (siège social)",berlin_address:"Prinzessinnenstraße 19-20, 10969 Berlin, Allemagne",berlin_address2:"Société d'expédition : Leipzigerstr. 96, 10117 Berlin, Allemagne",bj:"Pékin, Chine",bj_address:"Niveau 5, bâtiment 6, n° 48, rue Haidian Ouest, Pékin, Chine",brochure_info:"Votre guide de notre entreprise vous attend",description:"L'avenir commence ici.",download_brochure1:"Télécharger la brochure",download_docarray_logo:"Téléchargez le logo DocArray",download_docarray_logo_desc:"Accédez au logo DocArray, un projet open source initié par Jina AI et contribué à la Linux Foundation en décembre 2022. Disponible en modes clair et sombre, aux formats PNG et SVG.",download_jina_logo:"Téléchargez le logo Jina AI",download_jina_logo_desc:"Obtenez le logo Jina AI en modes clair et sombre, disponible aux formats PNG et SVG. Ce logo est une marque déposée auprès de l'Office de l'Union européenne pour la propriété intellectuelle (EUIPO).",download_logo:"Télécharger des logos",employees:"Les employés aujourd'hui",empower_developers:"Développeurs habilités",fastApiCaption:"Plus de 20 000 $ versés depuis 2021.",founded:"Fondé",founded_in:"Fondé en",investors:"Nos investisseurs",linuxFoundationCaption:"Verse une contribution annuelle de 10 000 $ à compter de 2022.",many:"Beaucoup",media:{video:"Entretien vidéo"},mission:"Notre mission",mission_content1:"Nos technologies clés, notamment le réglage rapide, le service rapide, le réglage de modèles et le service de modèles, incarnent notre engagement à démocratiser l'accès à l'IA. Grâce à notre initiative open source, nous nous efforçons de favoriser l'innovation, la collaboration et la transparence, garantissant des solutions évolutives, efficaces et robustes. Jina AI est plus qu'une simple entreprise ; il s'agit d'une communauté vouée à donner aux entreprises les moyens de relever les défis dynamiques de l'ère numérique et de prospérer dans leurs domaines.",mission_content2:"Au cœur de Jina AI se trouve notre mission : être le portail vers l’IA multimodale pour une clientèle diversifiée, des utilisateurs expérimentés et développeurs aux entreprises. Nous croyons profondément au pouvoir de l'open source et nous nous engageons à créer des outils avancés et accessibles pour la communauté de l'IA. Nos technologies clés, notamment le réglage rapide, le service rapide, le réglage intégré et le service intégré, incarnent notre engagement à démocratiser l'accès à l'IA. Grâce à notre initiative open source, nous nous efforçons de favoriser l'innovation, la collaboration et la transparence, garantissant des solutions évolutives, efficaces et robustes. Jina AI est plus qu'une simple entreprise ; il s'agit d'une communauté vouée à donner aux entreprises les moyens de relever les défis dynamiques de l'ère numérique et de prospérer dans leurs domaines.",mission_content3:"Chez Jina AI, notre mission est de diriger l'avancement de l'IA multimodale grâce à des technologies innovantes d'intégration et basées sur des invites, en nous concentrant spécifiquement sur des domaines tels que le traitement du langage naturel, l'analyse d'images et de vidéos et l'interaction de données multimodales. Cette spécialisation nous permet de fournir des solutions uniques qui transforment des données complexes et multi-sources en informations exploitables et en applications révolutionnaires.",mit_report_title:"Multimodal : la nouvelle frontière de l’IA",mit_techreview:"Revue technologique du MIT",numfocusCaption:"Donne régulièrement chaque mois à partir de 2022.",office:"Nos bureaux",otherProjectsCaption:"A fait don de plus de 3 000 $ via le parrainage Github.",our_answer:"Tout à fait Yann. Nous y sommes, construisant des ponts vers un avenir d'IA multimodal !",pythonSoftwareFoundationCaption:"A fourni un don unique de 10 000 $ et parrainé plusieurs événements PyCon, notamment ceux en Allemagne, en Italie, en Chine et aux États-Unis.",sectors:{ecommerceRetail:"Marchés de nouvelle génération",ecommerceRetail_description:"Les leaders du commerce électronique et de la vente au détail s'associent à Jina AI pour proposer des recommandations de produits précises et des expériences de recherche approfondies. Nos intégrations multilingues et nos rerankers pilotés par l'IA permettent d'optimiser la découverte, de stimuler la conversion et de réduire le temps d'obtention d'informations pour les catalogues de produits mondiaux.",ecommerceRetail_short:"Commerce électronique et vente au détail",financeConsulting:"Conseillers et analystes visionnaires",financeConsulting_description:"Les sociétés financières et les cabinets de conseil exploitent le nettoyage de données à grande échelle et la formation de modèles spécifiques à un domaine de Jina AI pour obtenir des informations en temps réel. Nos licences d'entreprise et nos déploiements sécurisés sur site leur permettent de préserver la confidentialité tout en bénéficiant d'une récupération et d'une analyse avancées.",financeConsulting_short:"Financement et conseil",media:"Des créateurs de contenu captivants",media_description:"Les organisations médiatiques utilisent Jina AI pour transformer de vastes ressources multimédias en connaissances consultables, rationalisant ainsi la recherche interne et enrichissant l'expérience utilisateur. Nos services spécialisés de lecture et de reclassement garantissent une découverte précise et contextuelle des articles, des vidéos et des archives.",media_short:"Médias",misc:"Des pionniers avant-gardistes",misc_description:"Les organisations des secteurs de l’éducation, de l’agriculture, de l’immobilier et bien plus encore exploitent les solutions flexibles de Jina AI pour nettoyer, extraire et transformer les données à grande échelle. En exploitant notre pile de récupération neuronale de pointe, elles ouvrent de nouvelles possibilités et restent en tête dans leurs domaines respectifs.",misc_short:"Autres",technology:"Des innovateurs technologiques avant-gardistes",technology_description:"Les leaders du secteur des logiciels, du cloud, de l'IA et des données s'appuient sur les solutions de recherche neuronale de Jina AI pour alimenter leurs systèmes de recherche basés sur LLM, RAG et agents IA. Notre lecteur avancé, nos intégrations, nos rerankers et nos petits LM les aident à passer du POC à l'entreprise plus rapidement que jamais.",technology_short:"Technologie"},sefo:{layer0:"Applications utilisateur final",layer1:"RAG / orchestration",layer3:"GPU/mobile/edge/informatique locale"},segmentFaultCaption:"A fait un don unique de 6 000 $.",show_position:"Comment rechercher des positions de fondation dans l'écosystème ?",stats_1:"Fondée en février 2020, Jina AI s'est rapidement imposée comme un pionnier mondial de la technologie d'IA multimodale. Dans un délai impressionnant de 20 mois, nous avons réussi à lever 37,5 millions de dollars, marquant notre position forte dans l'industrie de l'IA. Notre technologie révolutionnaire, open source sur GitHub, a permis à plus de 40 000 développeurs dans le monde de créer et de déployer de manière transparente des applications multimodales sophistiquées.",stats_2:"En 2023, nous avons fait des progrès significatifs dans l'avancement des outils de génération d'IA basés sur la technologie multimodale. Cette innovation a profité à plus de 250 000 utilisateurs dans le monde, répondant à une pléthore d'exigences commerciales uniques. Qu'il s'agisse de faciliter la croissance des entreprises, d'améliorer l'efficacité opérationnelle ou d'optimiser les coûts, Jina AI se consacre à donner aux entreprises les moyens d'exceller à l'ère multimodale.",stats_4:`Fondée en 2020, Jina AI est une entreprise leader dans le domaine de l'IA de recherche. Notre plateforme <span class="text-primary text-bold">Search Foundation</span> combine des intégrations, des rerankers et des modèles de langage réduits pour aider les entreprises à créer des applications de recherche GenAI et multimodales fiables et de haute qualité.`,stats_v1:"Rechercher/accéder",subtitle:"Révolutionner la création de contenu grâce à des solutions générées par l'IA pour débloquer des possibilités infinies. Façonner l'avenir du contenu généré par l'IA et améliorer la créativité humaine.",sues_und_sauer:"Suẞ & Sauer",sues_und_sauer_tooltip:"Süß-Sauer, une saveur populaire (mais stéréotypée) de la cuisine germano-chinoise, signifie sucré-salé. C'est une métaphore des hauts et des bas de la vie d'une start-up.",sunnyvale_address:"710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, États-Unis",sz:"Shenzhen, en Chine",sz_address:"402 étage 4, bâtiment technologique Fu'an, Shenzhen, Chine",team:"À l'intérieur du portail de Jina AI",team_content1:"Aux quatre coins du monde, nous construisons l’avenir de l’IA. Nos perspectives distinctes enrichissent notre travail et suscitent des innovations. Au sein de ce portail, nous embrassons notre individualité et poursuivons avec passion nos rêves. Bienvenue sur le portail du futur de l'IA.",team_join:"Rejoignez-nous",team_size:"Ces photos incluent nos anciens collègues et stagiaires. Nous apprécions chacun d’entre eux.",technologies:"Les technologies",title:"À propos de Jina AI",title0:"L'avenir",title1:"Départs",title2:"Ici",title3:"Commence ici",understand_our_strength:"Comprendre notre force",understand_our_view2:"Comprendre la Fondation Search",users:"Utilisateurs enregistrés",value:"Nos récompenses",value_content1:"Nous ne nous contentons pas de ce qui est demandé. Nous ne faisons aucun compromis. Nous visons l'excellence.",vision:"Notre mission",vision_content1:"Inspiré par la perspicacité de Yann LeCun selon laquelle '",vision_content3:`L'avenir de l'IA est <span class="text-primary text-bold">multimodal</span>, et nous en faisons partie. Nous sommes conscients que les entreprises sont confrontées à des difficultés pour exploiter les données multimodales. En réponse, nous nous engageons auprès de la <span class="text-primary text-bold">Search Foundation</span> pour aider les entreprises et les développeurs à mieux rechercher et à utiliser les données multimodales pour la croissance de leur entreprise.`,yannlecun_quote:"Un système d'intelligence artificielle entraîné uniquement sur des mots et des phrases ne se rapprochera jamais de la compréhension humaine."},t={answer1:"Oui, la même clé API est valable pour tous les produits de recherche de Jina AI. Cela inclut les API de lecture, d'intégration, de reclassement, de classification et de réglage fin, avec des jetons partagés entre tous les services.",answer10:"Cela est dû au fait que notre architecture sans serveur décharge certains modèles pendant les périodes de faible utilisation. La requête initiale active ou « réchauffe » le modèle, ce qui peut prendre quelques secondes. Après cette activation initiale, les requêtes suivantes sont traitées beaucoup plus rapidement.",answer12:"Nous adhérons à une politique de confidentialité stricte et n'utilisons pas les données saisies par les utilisateurs pour former nos modèles. Nous sommes également conformes aux normes SOC 2 Type I et Type II, garantissant des normes élevées de sécurité et de confidentialité.",answer3:"Oui, l'utilisation des jetons peut être surveillée dans l'onglet « Clé API et facturation » en saisissant votre clé API, ce qui vous permet d'afficher l'historique d'utilisation récent et les jetons restants. Si vous êtes connecté au tableau de bord de l'API, ces détails peuvent également être consultés dans l'onglet « Gérer la clé API ».",answer4:"Si vous avez égaré une clé rechargée et souhaitez la récupérer, veuillez contacter le support AT jina.ai avec votre adresse e-mail enregistrée pour obtenir de l'aide. Il est recommandé de vous connecter pour conserver votre clé API en toute sécurité et facilement accessible.",answer5:"Non, nos clés API n'ont pas de date d'expiration. Cependant, si vous pensez que votre clé a été compromise et souhaitez la retirer, veuillez contacter notre équipe d'assistance pour obtenir de l'aide. Vous pouvez également révoquer votre clé dans le <a class='text-primary' href='https://jina.ai/api-dashboard'>tableau de bord de gestion des clés API</a>.",answer6:"Oui, vous pouvez transférer des jetons d'une clé premium vers une autre. Après vous être connecté à votre compte sur <a class='text-primary' href='https://jina.ai/api-dashboard'>le tableau de bord de gestion des clés API</a>, utilisez les paramètres de la clé que vous souhaitez transférer pour déplacer tous les jetons payants restants.",answer7:"Oui, vous pouvez révoquer votre clé API si vous pensez qu'elle a été compromise. La révocation d'une clé la désactivera immédiatement pour tous les utilisateurs qui l'ont stockée, et tout le solde restant et les propriétés associées seront définitivement inutilisables. Si la clé est une clé premium, vous avez la possibilité de transférer le solde restant payé vers une autre clé avant la révocation. Notez que cette action ne peut pas être annulée. Pour révoquer une clé, accédez aux paramètres de clé dans <a class='text-primary' href='https://jina.ai/api-dashboard'>le tableau de bord de gestion des clés API</a>.",question1:"Puis-je utiliser la même clé API pour les API de lecture, d'intégration, de reclassement, de classification et de réglage fin ?",question10:"Pourquoi la première demande de certains modèles est-elle lente ?",question12:"Les données saisies par l'utilisateur sont-elles utilisées pour entraîner vos modèles ?",question3:"Puis-je surveiller l’utilisation des jetons de ma clé API ?",question4:"Que dois-je faire si j'oublie ma clé API ?",question5:"Les clés API expirent-elles ?",question6:"Puis-je transférer des jetons entre des clés API ?",question7:"Puis-je révoquer ma clé API ?",title:"Questions courantes liées à l'API"},i={base_model:"Modèle de base pour un réglage fin",check_data:"Télécharger des données synthétiques",check_model:"Télécharger le modèle affiné",data_size:"Données synthétiques générées",description:"Obtenez des intégrations affinées pour le domaine de votre choix.",description_long:"Dites-nous simplement dans quel domaine vous souhaitez que vos intégrations excellent, et nous vous fournirons automatiquement un modèle d'intégration prêt à l'emploi et affiné pour ce domaine.",does_it_work_tho:"Mais est-ce que ça marche ?",does_it_work_tho_explain:"Le réglage automatique contient la promesse auto-magique de fournir des intégrations affinées pour n'importe quel domaine de votre choix. Mais cela fonctionne-t-il vraiment? C'est un doute assez raisonnable. Nous l'avons testé sur une variété de domaines et de modèles de base pour le savoir. Découvrez les résultats cueillis aux cerises et au citron ci-dessous.",domain_instruction:"Instruction de domaine",embedding_provider:"Sélectionnez un modèle d'intégration de base",eval_evaluation:"Validation",eval_map:"CARTE",eval_mrr:"MRR",eval_ndcg:"NDCG",eval_performance_before_after:"Performances sur la validation synthétique définie avant et après le réglage fin",eval_syntheticDataSize:"Total",eval_test:"Données réelles pour les tests",eval_training:"Entraînement",faq_v1:{answer1:"La fonctionnalité est actuellement en version bêta et coûte 1 million de jetons par modèle affiné. Vous pouvez utiliser votre clé API existante à partir de l'API Embedding/Reranker si elle contient suffisamment de jetons, ou vous pouvez créer une nouvelle clé API, qui comprend 1 million de jetons gratuits.",answer10:"Actuellement non. Notez que cette fonctionnalité est toujours en version bêta. Le stockage public des modèles affinés et des données synthétiques dans le hub de modèles Hugging Face nous aide, ainsi que la communauté, à évaluer la qualité de la formation. À l’avenir, nous prévoyons de proposer une option de stockage privé.",answer11:"Étant donné que tous les modèles affinés sont téléchargés sur Hugging Face, vous pouvez y accéder via SentenceTransformers en spécifiant simplement le nom du modèle.",answer12:"Veuillez vérifier votre dossier spam. Si vous ne le trouvez toujours pas, veuillez contacter notre équipe d'assistance en utilisant l'adresse e-mail que vous avez fournie.",answer2:"Vous n'avez pas besoin de fournir de données d'entraînement. Décrivez simplement votre domaine cible (le domaine pour lequel vous souhaitez que les intégrations affinées soient optimisées) en langage naturel, ou utilisez une URL comme référence, et notre système générera des données synthétiques pour entraîner le modèle.",answer3:"Environ 30 minutes.",answer4:"Les modèles affinés et les données synthétiques sont stockés publiquement dans le hub de modèles Hugging Face.",answer5:"Le système utilise l'API Reader pour récupérer le contenu de l'URL. Il analyse ensuite le contenu pour résumer le ton et le domaine, qu'il utilise comme lignes directrices pour générer des données synthétiques. Par conséquent, l’URL doit être accessible au public et représentative du domaine cible.",answer6:"Oui, vous pouvez affiner un modèle pour une langue autre que l'anglais. Le système détecte automatiquement la langue des instructions de votre domaine et génère des données synthétiques en conséquence. Nous vous recommandons également de choisir le modèle de base approprié pour la langue cible. Par exemple, si vous ciblez un domaine allemand, vous devez sélectionner « jina-embeddings-v2-base-de » comme modèle de base.",answer7:"Non, notre API de réglage fin ne prend en charge que les modèles Jina v2.",answer8:"À la fin du processus de réglage fin, le système évalue le modèle à l'aide d'un ensemble de tests retenu et rapporte les mesures de performances. Vous recevrez un e-mail détaillant les performances avant/après sur cet ensemble de tests. Vous êtes également encouragé à évaluer le modèle sur votre propre ensemble de tests pour garantir sa qualité.",answer9:"Le système génère des données synthétiques en intégrant l'instruction du domaine cible que vous fournissez au raisonnement des agents LLM. Il produit des triplets négatifs durs, essentiels à la formation de modèles d’intégration de haute qualité. Pour plus de détails, veuillez vous référer à notre prochain document de recherche sur Arxiv.",question1:"Combien coûte l’API de réglage fin ?",question10:"Puis-je garder mes modèles affinés et mes données synthétiques privés ?",question11:"Comment puis-je utiliser le modèle affiné ?",question12:"Je n'ai jamais reçu l'e-mail avec les résultats de l'évaluation. Que dois-je faire?",question2:"Que dois-je saisir ? Dois-je fournir des données de formation ?",question3:"Combien de temps faut-il pour peaufiner un modèle ?",question4:"Où sont stockés les modèles peaufinés ?",question5:"Si je fournis une URL de référence, comment le système l'utilise-t-il ?",question6:"Puis-je affiner un modèle pour une langue spécifique ?",question7:"Puis-je affiner les intégrations non-Jina, par exemple bge-M3 ?",question8:"Comment garantissez-vous la qualité des modèles peaufinés ?",question9:"Comment générer des données synthétiques ?",title:"Questions courantes liées au réglage automatique"},find_on_hf:"Liste des modèles affinés",temporarily_unavailable:"Temporairement indisponible. Nous améliorons notre système de réglage automatique pour mieux vous servir. Veuillez revenir plus tard.",test_on:"Testé sur {_dataSize} échantillons aléatoires de {_dataName}",test_performance_before_after:"Performances sur un ensemble de tests retenu avant et après le réglage fin",title:"API de réglage automatique",total_improve:"Moy. amélioration",usage:"Usage",what_is:"Qu’est-ce que le réglage automatique ?",what_is_answer_long:"Le réglage fin vous permet de prendre un modèle pré-entraîné et de l'adapter à une tâche ou un domaine spécifique en l'entraînant sur un nouvel ensemble de données. En pratique, trouver des données d’entraînement efficaces n’est pas simple pour de nombreux utilisateurs. Une formation efficace nécessite plus que la simple introduction de fichiers PDF bruts et HTML dans le modèle ; et il est difficile de bien faire les choses. Le réglage automatique résout ce problème en générant automatiquement des données de formation efficaces à l'aide d'un pipeline d'agents LLM avancé ; et affiner le modèle dans un flux de travail ML. Vous pouvez le considérer comme une combinaison de génération de données synthétiques et d'AutoML, il vous suffit donc de décrire votre domaine cible en langage naturel et de laisser notre système faire le reste."},r={auth_required:"Authentification requise pour utiliser la génération d'avatar",classificationError:"Erreur lors de la classification de l'image. Veuillez réessayer.",clickToDownload:"Cliquez pour télécharger SVG",customize:"Personnaliser les fonctionnalités",description:"Générez des avatars uniques avec des fonctionnalités personnalisables",downloadError:"Erreur lors du téléchargement de l'avatar",downloadSuccess:"Avatar téléchargé avec succès",download_success:"Avatar téléchargé avec succès",error_loading:"Impossible de charger les éléments de l'avatar. Veuillez réessayer.",error_processing:"Erreur lors du traitement de l'image",file_hint:"Formats pris en charge : JPG, PNG, GIF, WebP",generate:"Générer un avatar",how_does_it_work:"Comment ça marche ?",noImageSelected:"Veuillez d'abord sélectionner une image",select_file:"Sélectionnez un fichier d'image de portrait",title:"Générateur d'avatars",upload_description:"Sélectionnez une image à convertir en base64 (256x256)",upload_title:"Télécharger l'image",usage:"Génération d'avatars"},a={description:"Du blog à la bannière, sans les invites !",example_description:`Alice commençait à être très fatiguée d'être assise à côté de sa sœur sur la berge et de n'avoir rien à faire : une ou deux fois, elle avait jeté un coup d'œil dans le livre que sa sœur lisait, mais il ne contenait ni images ni conversations, "et à quoi sert un livre", pensa Alice, "sans images ni conversations ?" Alors elle réfléchissait (du mieux qu'elle pouvait, car la chaleur de la journée la rendait somnolente et stupide), si le plaisir de faire une guirlande valait la peine de se lever et de cueillir les marguerites, quand soudain un lapin blanc aux yeux roses courut près d'elle.`,example_title:"Les aventures d'Alice au pays des merveilles - Chapitre 1"},o="Bêta",l={answer10:"Nous proposons un essai gratuit de bienvenue aux nouveaux utilisateurs, qui comprend un million de jetons à utiliser avec n'importe lequel de nos modèles, facilité par une clé API générée automatiquement. Une fois la limite de jetons gratuits atteinte, les utilisateurs peuvent facilement acheter des jetons supplémentaires pour leurs clés API via l'onglet « Acheter des jetons ».",answer13:"Non, les jetons ne sont pas déduits pour les demandes ayant échoué.",answer14:"Les paiements sont traités via Stripe, prenant en charge diverses méthodes de paiement, notamment les cartes de crédit, Google Pay et PayPal, pour votre commodité.",answer15:"Oui, une facture sera émise à l'adresse e-mail associée à votre compte Stripe lors de l'achat de tokens.",answer9:"Notre modèle de tarification est basé sur le nombre total de jetons traités, ce qui donne aux utilisateurs la possibilité d'attribuer ces jetons à un nombre illimité de phrases, offrant ainsi une solution rentable pour diverses exigences d'analyse de texte.",question10:"Existe-t-il un essai gratuit disponible pour les nouveaux utilisateurs ?",question13:"Les jetons sont-ils facturés pour les demandes ayant échoué ?",question14:"Quels moyens de paiement sont acceptés ?",question15:"La facturation est-elle disponible pour les achats de jetons ?",question9:"La facturation est-elle basée sur le nombre de phrases ou de demandes ?",title:"Questions courantes liées à la facturation"},u={all:"Tous",events:"Événement",featured:"Mis en exergue",insights:"Avis","knowledge-base":"Base de connaissances",latest:"Dernier",press:"communiqué de presse",releases:"Mise à jour logicielle","tech-blog":"Blog technique"},d={caption:"Découvrez « Re·Search », notre annuaire magnifiquement conçu présentant nos meilleurs articles de recherche et modèles de base de recherche en 2024.",order_now:"Commandez maintenant"},c={api_free_trial:"Clé API gratuite",api_paid:"Clé API payante",api_paid_or_free:"Utilisez-vous une clé API payante ou une clé d’essai gratuite ?",are_you:"Es-tu:",commercial_contact_sales:"Ceci est commercial. Contactez notre équipe commerciale.",contact_sales_for_licensing:"Contactez notre équipe commerciale pour les licences.",csp_user:"Utilisez-vous nos images de modèles officielles sur AWS et Azure ?",educational_teaching:"Un établissement d’enseignement qui l’utilise pour l’enseignement ?",for_profit_internal_use:"Une entreprise à but lucratif qui l'utilise en interne ?",free_use:"Vous pouvez utiliser les modèles librement.",government_public_services:"Une entité gouvernementale l’utilise pour des services publics ?",is_use_commercial:"Votre utilisation est-elle commerciale ?",may_be_commercial_contact:"Il peut s'agir d'une information commerciale. Veuillez nous contacter pour plus de précisions.",no:"Non",no1:"Non",no2:"Non",no3:"Non",no_restrictions:"Aucune restriction. Utilisez-le conformément à votre contrat actuel.",no_restrictions_apply:"Aucune restriction ne s'applique.",non_commercial_free_use:"Ceci n'est pas commercial. Vous pouvez utiliser les modèles librement.",non_profit_ngo_mission:"Une association à but non lucratif ou une ONG l'utilise pour votre mission ?",not_sure:"Pas sûr",personal_hobby_projects:"Vous l'utilisez pour des projets personnels ou de loisirs ?",product_service_sale:"Vous l’utilisez dans un produit ou un service que vous vendez ?",title:"Auto-vérification de la licence CC BY-NC",trial_key_restrictions:"La clé d'essai gratuite ne peut être utilisée qu'à des fins non commerciales. Veuillez acheter un forfait payant pour une utilisation commerciale.",typically_non_commercial_check:"Il s'agit généralement d'un service non commercial, mais vérifiez auprès de nous en cas de doute.",typically_non_commercial_free_use:"Il s'agit généralement d'un usage non commercial. Vous pouvez utiliser les modèles librement.",using_api_or_cloud:"Utilisez-vous notre API officielle ou nos images officielles sur Azure ou AWS ?",using_cc_by_nc_models:"Utilisez-vous ces modèles ?",yes:"Oui",yes1:"Oui",yes2:"Oui",yes3:"Oui"},p={access:"Accès public",access_explain:"Les classificateurs publics peuvent être utilisés par toute personne disposant du <code>classifier_id</code>, et leur utilisation consommera le quota de jetons de l'appelant plutôt que le vôtre. Les classificateurs privés ne sont accessibles que par vous.",access_private:"Privé",access_public:"Publique",api_delete:"Supprimer le classificateur",api_delete_explain:"Supprimer un classificateur par son ID.",api_list:"Classificateurs de listes",api_list_explain:"Listez tous les classificateurs que vous avez créés.",classifier_id:"ID du classificateur",classify_inputs:"Entrées à classer",classify_inputs_explain:"Pour le texte, il peut s'agir d'une phrase contenant jusqu'à 8192 jetons. Pour les images, il peut s'agir d'une URL ou d'une image codée en base64.",classify_labels:"Étiquettes des candidats",classify_labels_explain:"Les entrées seront classées selon ces étiquettes. Il peut y avoir jusqu'à 256 classes. Utilisez des étiquettes sémantiques pour de meilleures performances.",compare_table:{access_control:"Contrôle d'accès",classifier_id_required:"ID du classificateur requis",continuous_updates:"Mises à jour continues du modèle",default_solution:"Solution par défaut pour la classification générale",feature:"Fonctionnalité",few_shot:"Quelques coups",image_multi_lingual_support:"Support multimodal et multilingue",labels_required_classify:"Étiquettes requises dans /classify",labels_required_train:"Étiquettes requises dans /train",max_classes:"Classes maximales",max_classifiers:"Classificateurs maximum",max_inputs_request:"Nombre maximal d'entrées par requête",max_token_length:"Longueur maximale du jeton par entrée",na:"N / A",no:"Non",out_of_domain_solution:"Pour les données en dehors du domaine de v3/clip-v1 ou les données sensibles au temps",primary_use_case:"Cas d'utilisation principal",semantic_labels_required:"Étiquettes sémantiques requises",state_management:"Gestion de l'État",stateful:"Avec état",stateless:"Apatride",token_count:"{count} jetons",training_data_required:"Données de formation requises",yes:"Oui",zero_shot:"Coup zéro"},create_classifier:"Nouveau classificateur à quelques coups",create_classifier_explain:"Créez un nouveau classificateur à quelques coups et entraînez-le avec des exemples étiquetés.",description:"Classification à zéro plan et à quelques plans pour l'image et le texte.",description_long:"Essayez notre terrain de jeu API pour voir comment fonctionne notre classificateur.",description_long1:"Classificateur haute performance à zéro coup et à quelques coups pour données multimodales et multilingues.",explain:"Le classificateur est un service API qui catégorise le texte et les images à l'aide de modèles d'intégration (<code>jina-embeddings-v3</code> et <code>jina-clip-v1</code>), prenant en charge à la fois la classification à zéro coup sans données de formation et l'apprentissage à quelques coups avec un minimum d'exemples.",faq_v1:{answer1:"Le zero-shot nécessite des étiquettes sémantiques pendant la classification et aucune pendant l'apprentissage, tandis que le few-shot nécessite des étiquettes pendant l'apprentissage mais pas la classification. Cela signifie que le zero-shot est plus adapté aux besoins de classification flexibles et immédiats, tandis que le few-shot est plus adapté aux catégories fixes et spécifiques à un domaine qui peuvent évoluer au fil du temps.",answer10:"Oui, vous pouvez choisir entre <code>jina-embeddings-v3</code> pour la classification de texte (particulièrement utile pour le multilingue) et <code>jina-clip-v1</code> pour la classification multimodale. De nouveaux modèles comme <code>jina-clip-v2</code> seront automatiquement disponibles via l'API dès leur sortie.",answer2:"<code>num_iters</code> contrôle l'intensité de l'entraînement : les valeurs plus élevées renforcent les exemples importants tandis que les valeurs plus faibles minimisent l'impact des données moins fiables. Il peut être utilisé pour mettre en œuvre l'apprentissage en fonction du temps en donnant aux exemples récents un nombre d'itérations plus élevé, ce qui le rend utile pour faire évoluer les modèles de données.",answer3:"Les classificateurs publics peuvent être utilisés par toute personne disposant du <code>classifier_id</code>, en utilisant son propre quota de jetons. Les utilisateurs ne peuvent pas accéder aux données de formation ou à la configuration, et ne peuvent pas voir les demandes de classification des autres, ce qui permet un partage sécurisé des classificateurs.",answer4:"La méthode Few-shot nécessite 200 à 400 exemples d'entraînement pour surpasser la classification Zero-shot. Bien qu'elle atteigne finalement une plus grande précision, elle a besoin de cette période d'échauffement pour devenir efficace. Zero-shot fournit des performances cohérentes immédiatement sans données d'entraînement.",answer5:"Oui, l'API prend en charge les requêtes multilingues à l'aide de <code>jina-embeddings-v3</code> et la classification multimodale (texte/image) à l'aide de <code>jina-clip-v1</code>, avec prise en charge des images codées en URL ou en base64 dans la même requête.",answer6:"Zero-shot prend en charge 256 classes sans limite de classificateur, tandis que few-shot est limité à 16 classes et 16 classificateurs. Les deux prennent en charge 1 024 entrées par requête et 8 192 jetons par entrée.",answer7:"Le mode Few-shot permet une mise à jour continue via le point de terminaison <code>/train</code> pour s'adapter aux modèles de données changeants. Vous pouvez ajouter progressivement de nouveaux exemples ou classes lorsque la distribution des données change, sans reconstruire l'intégralité du classificateur.",answer8:"L'API utilise l'apprentissage en ligne en un seul passage : les exemples de formation mettent à jour les pondérations du classificateur mais ne sont pas stockés par la suite. Cela signifie que vous ne pouvez pas récupérer les données de formation historiques, mais cela garantit la confidentialité et l'efficacité des ressources.",answer9:"Commencez par un modèle à zéro coup pour obtenir des résultats immédiats et lorsque vous avez besoin d'une classification flexible avec des étiquettes sémantiques. Passez à un modèle à quelques coups lorsque vous avez 200 à 400 exemples, que vous avez besoin d'une plus grande précision ou que vous devez gérer des données spécifiques à un domaine/sensibles au temps.",question1:"Quelle est la différence entre les étiquettes en mode zero-shot et en mode few-shot ?",question10:"Puis-je utiliser différents modèles pour différentes langues/tâches ?",question2:"À quoi sert num_iters et comment dois-je l'utiliser ?",question3:"Comment fonctionne le partage de classificateurs publics ?",question4:"De combien de données ai-je besoin pour que le mode FPS fonctionne bien ?",question5:"Peut-il gérer plusieurs langues et à la fois du texte et des images ?",question6:"Quelles sont les limites strictes que je devrais connaître ?",question7:"Comment gérer les changements de données au fil du temps ?",question8:"Qu'advient-il de mes données d'entraînement après les avoir envoyées ?",question9:"Zero-shot ou few-shot : quand utiliser lequel ?",title:"Questions courantes liées au classificateur"},more:"plus",num_iters:"Itérations de formation",num_iters_explain:"Contrôle l'intensité de l'entraînement : des valeurs plus élevées améliorent la précision des exemples actuels mais augmentent le coût du jeton. La valeur par défaut de 10 fonctionne généralement bien.",read_notes:"Lire les notes de publication",select_classifier_or_model:"Sélectionnez un classificateur ou un modèle d'intégration",task_classify:"Classer",task_classify_explain:"Utilisez un classificateur à zéro coup ou à quelques coups pour classer du texte ou des images dans des classes définies.",task_manage:"Gérer",task_manage_explain:"Répertoriez ou supprimez vos classificateurs à quelques clichés.",task_select:"Sélectionnez une tâche",task_train:"Former",task_train_explain:"Créez ou mettez à jour un classificateur à quelques clichés avec des exemples étiquetés.",title:"API de classificateur",train_inputs:"Données de formation",train_inputs_explain:"Exemples de texte ou d'images avec des étiquettes pour la formation. Vous pouvez mettre à jour progressivement le classificateur avec de nouveaux exemples et étiquettes au fil du temps.",train_label:"Étiquette",what_is:"Qu'est-ce qu'un classificateur ?",when_to_use_what:"Quand utiliser le zero-shot ou le few-shot ?",when_to_use_what_explain:"Utilisez la classification zero-shot comme solution par défaut pour des résultats immédiats sur des tâches de classification générales avec jusqu'à 256 classes, tandis que l'apprentissage en quelques coups est mieux adapté lorsque vous traitez des données spécifiques à un domaine en dehors des connaissances des modèles d'intégration ou lorsque vous devez gérer des données sensibles au temps qui nécessitent des mises à jour continues du modèle."},m={description:"Intégrez des images et des phrases dans des vecteurs de longueur fixe avec CLIP"},g={description:"Plate-forme d'hébergement cloud pour les applications d'IA multimodales"},v={agreement:"En soumettant, vous confirmez que vous acceptez le traitement de vos données personnelles par Jina AI comme décrit dans le",anything_else:"Parlez-nous davantage de votre idée",cc_by_nc:"Demande d'utilisation commerciale des modèles CC BY-NC",cc_by_nc_description:"Nos derniers modèles sont généralement sous licence CC BY-NC. Pour une utilisation commerciale, accédez-y via notre API, Azure Marketplace ou AWS SageMaker. Cochez cette case pour une utilisation sur site en dehors de ces canaux.",company:"Organisation",company_size:"Taille de l'organisation",company_website:"Site Web de l'organisation",company_website_placeholder:"URL de la page d'accueil ou du profil LinkedIn de votre entreprise",country:"Pays",department:"Département",description:"Développez votre entreprise avec Jina AI.",drop_area_for_image:"Déposez vos images ici",faq:"FAQ",feedback_sent:"Soumis! Nous vous répondrons sous peu.",field_required:"Champ requis",get_api_key:"Comment obtenir ma clé API ?",image_upload:"Joindre des images",image_validate:"Vous pouvez joindre jusqu'à {_num} images. Uniquement JPG, JPEG, PNG, WEBP.",impact_snapshots:"Instantanés d'impact",invalid_date_format:"Format de date invalide. Veuillez utiliser le format JJ-MM-AAAA.",invalid_email:"Le courriel est invalide",invalid_number:"Numéro invalide. Veuillez saisir à nouveau",invalid_url:"L'URL n'est pas valide",name:"Nom",nc_check:"Ai-je besoin d’une licence commerciale ?",other_questions:"Autres questions",preferred_models:"Quels modèles vous intéressent ?",preferred_products:"Quels produits vous intéressent ?",pricing:"Tarifs ?",priority:"Assistance prioritaire pour les utilisateurs payants",private_statement:"Déclaration de confidentialité",rate_limit:"Quelle est la limite de débit ?",role:"Rôle de l'emploi",self_check:"Auto-vérification",sending_feedback:"Envoi...",shortcut:"Raccourci",submit:"Soumettre",submit_failed:"La soumission a échoué. Veuillez réessayer plus tard.",submit_success:"Merci pour votre soumission. Nous vous répondrons sous peu.",subtitle:"Jina AI, un leader de l'IA multimodale, excelle dans le réglage de modèle, le service de modèle, le réglage rapide et le service rapide. En tirant parti des technologies cloud natives telles que Kubernetes et des architectures sans serveur, nous proposons des solutions robustes, évolutives et prêtes pour la production. Grâce à notre expertise dans les grands modèles de langage, le texte, l'image, la vidéo, la compréhension audio, la recherche neuronale et l'art génératif, nous proposons des stratégies innovantes et pérennes pour faire progresser votre entreprise.",subtitle1:"Jina AI, leader de l'IA multimodale, excelle dans le réglage-intégration, le service-intégration, le réglage rapide et le service rapide. En tirant parti des technologies cloud natives telles que Kubernetes et des architectures sans serveur, nous proposons des solutions robustes, évolutives et prêtes pour la production. Grâce à notre expertise dans les grands modèles linguistiques, la compréhension du texte, des images, de la vidéo, de l'audio, la recherche neuronale et l'IA générative, nous proposons des stratégies innovantes et évolutives pour développer votre entreprise.",subtitle2:"Explorez Jina AI, la pointe de l'IA multimodale. Nous excellons dans les technologies d'intégration et d'invite, en utilisant des solutions cloud natives comme Kubernetes pour des systèmes robustes et évolutifs. Spécialisés dans les grands modèles de langage et le traitement multimédia, nous proposons des stratégies commerciales innovantes et tournées vers l’avenir grâce à notre expertise avancée en IA.",title:"Contacter le service commercial",trusted_by:"Approuvé par",turn_on_volume:"Augmentez le volume",work_email:"Email de travail"},f="Copie",_="Copié dans le presse-papier",h={description:"Un flux de travail humain dans la boucle pour créer des images HD à partir de texte"},q={api_endpoint:"Point de terminaison de l'API",api_key:"Clé API",api_tagline:"Entièrement compatible avec le schéma de l'API Chat d'OpenAI, échangez simplement <code>api.openai.com</code> avec <code>deepsearch.jina.ai</code> pour commencer.",api_title:"API DeepSearch",assistant_message:"Assistant",chat_ui:{clear_context_message:"Etes-vous sûr de vouloir effacer le contexte ? Cela réinitialisera la conversation.",clear_context_title:"Nouveau chat ?",description:"Vérifiez les vibrations avec une interface de chat simple. DeepSearch est idéal pour les questions complexes qui nécessitent un raisonnement itératif, une connaissance du monde ou des informations à jour.",example_q1:"Quel est le dernier article de blog d'OpenAI ?",example_q2:"quelle est l'idée derrière le projet node-deepresearch ?",example_q3:"qu'est-ce que jina-colbert-v2 améliore exactement par rapport à jina-colbert-v1 ?",input_cant_be_empty:"L'entrée ne peut pas être vide",input_placeholder:"Tapez votre question ici",keyman:"Gestionnaire de clés",move_to_new:"Nous venons de lancer une nouvelle interface utilisateur DeepSearch ultra-rapide, minimaliste et GRATUITE. Découvrez-la sur https://search.jina.ai ou cliquez sur le bouton ci-dessous pour l'essayer !",new_chat:"Effacer la conversation en cours et démarrer une nouvelle conversation",new_url:"Visitez la nouvelle interface utilisateur",payment_required:"Il ne vous reste plus assez de jetons dans votre clé API. Si vous avez plusieurs clés API, vous pouvez passer à celle qui contient suffisamment de jetons dans le gestionnaire de clés. Sinon, vous pouvez recharger votre clé API pour continuer.",purchase:"Recharger la clé API",rate_limit_exceeded:"Vous avez dépassé la limite de débit. Veuillez réessayer ultérieurement ou utiliser votre clé API pour obtenir une limite de débit plus élevée.",stay:"Restez avec l'interface de démonstration classique",thinking:"Pensée...",thinking_done:"Chaîne de pensées",title:"Chat DeepSearch",use_user_key:"Utilisez votre clé API pour obtenir une limite de débit plus élevée."},client_3p:"Clients de chat",client_3p_explain:"Pour une expérience optimale, nous vous recommandons d'utiliser des clients de chat professionnels. DeepSearch est entièrement compatible avec le schéma de l'API de chat d'OpenAI, ce qui le rend facile à utiliser avec n'importe quel client compatible OpenAI.",comparison:{group1:{bestFor:"Réponses rapides aux questions de culture générale",feature1:"Les réponses sont générées uniquement à partir de connaissances pré-entraînées avec une date limite fixe",limitations:"Impossible d'accéder aux informations en temps réel ou post-formation",timeCost:"environ 1s",title:"LLM standard",tokenCost:"environ 1000 jetons"},group2:{bestFor:"Questions nécessitant des informations actuelles ou spécifiques au domaine",feature1:"Réponses générées en résumant les résultats d'une recherche en un seul passage",feature2:"Peut accéder aux informations actuelles au-delà de la limite de formation",limitations:"A du mal à résoudre des questions complexes nécessitant un raisonnement à plusieurs sauts",timeCost:"environ 3s",title:"RAG et LLMs ancrés",tokenCost:"environ 10 000 jetons"},group3:{bestFor:"Questions complexes nécessitant une recherche et un raisonnement approfondis",feature1:"Agent autonome qui recherche, lit et raisonne de manière itérative",feature2:"Décide dynamiquement des prochaines étapes en fonction des résultats actuels",feature3:"Auto-évalue la qualité des réponses avant de renvoyer les résultats",feature4:"Peut effectuer des analyses approfondies sur des sujets grâce à plusieurs cycles de recherche et de raisonnement",limitations:"Prend plus de temps que les approches simples LLM ou RAG",timeCost:"environ 50 ans",title:"Recherche profonde",tokenCost:"environ 500 000 jetons"}},demo:"Discuter avec DeepSearch",demo_description:"Vérifiez les vibrations avec une interface de chat simple. DeepSearch est idéal pour les questions complexes qui nécessitent un raisonnement itératif, une connaissance du monde ou des informations à jour.",description:"Recherchez, lisez et raisonnez jusqu'à trouver la meilleure réponse.",explain:"DeepSearch combine la recherche sur le Web, la lecture et le raisonnement pour une enquête complète. Considérez-le comme un agent à qui vous confiez une tâche de recherche : il effectue une recherche approfondie et effectue plusieurs itérations avant de fournir une réponse. Ce processus implique une recherche continue, un raisonnement et une approche du problème sous différents angles. Cela diffère fondamentalement des LLM standard qui génèrent des réponses directement à partir de données pré-entraînées, et des systèmes RAG traditionnels qui s'appuient sur des recherches ponctuelles et superficielles.",faq:{answer1:"DeepSearch est une API LLM qui effectue une recherche, une lecture et un raisonnement itératifs jusqu'à ce qu'elle trouve une réponse précise à une requête ou atteigne sa limite de budget de jetons.",answer10:"Les limites de débit varient selon le niveau de clé API, allant de 10 à 30 tr/min. Il est important d'en tenir compte pour les applications avec des volumes de requêtes élevés.",answer11:"DeepSearch encapsule les étapes de réflexion dans des balises XML <think>...</think> et fournit ensuite la réponse finale, en suivant le format de streaming OpenAI mais avec ces marqueurs spéciaux pour la chaîne de pensées.",answer12:"Oui. Jina Reader est utilisé pour la recherche et la lecture sur le Web, offrant au système la possibilité d'accéder et de traiter efficacement le contenu Web.",answer14:"Oui, l'utilisation des tokens de DeepSearch sur les requêtes complexes est sans doute élevée : en moyenne 70 000 tokens contre 500 pour les réponses LLM de base. Cela montre la profondeur de la recherche, mais a également des implications financières.",answer18:"Le système est principalement contrôlé par le budget de jetons plutôt que par le nombre d'étapes. Une fois le budget de jetons dépassé, il entre en mode Beast pour la génération de la réponse finale. Consultez <code>reasoning_effort</code> pour plus de détails.",answer19:"Les références sont considérées comme si importantes que si une réponse est jugée définitive mais manque de références, le système continue la recherche plutôt que d'accepter la réponse.",answer2:"Contrairement à OpenAI et Gemini, DeepSearch se concentre spécifiquement sur la fourniture de réponses précises par itération plutôt que sur la génération d'articles longs. Il est optimisé pour des réponses rapides et précises issues de la recherche sur le Web profond plutôt que pour la création de rapports complets.",answer20:"Oui, mais avec des étapes de recherche approfondies. L'exemple de « qui sera président en 2028 » montre que l'on peut répondre à des questions spéculatives grâce à de multiples itérations de recherche, même si l'exactitude de telles prédictions n'est pas garantie.",answer3:"Vous avez besoin d'une clé API Jina. Nous offrons 1 M de jetons gratuits pour les nouvelles clés API.",answer5:"Il génère une réponse finale basée sur toutes les connaissances accumulées, plutôt que de simplement abandonner ou de renvoyer une réponse incomplète.",answer6:"Non. Bien qu'il utilise un processus de recherche itératif pour améliorer la précision, l'évaluation montre qu'il atteint un taux de réussite de 75 % aux questions du test, ce qui est nettement meilleur que la référence de 0 % (gemini-2.0-flash) mais pas parfait.",answer7:"Cela varie considérablement : les requêtes peuvent prendre entre 1 et 42 étapes, avec une moyenne de 4 étapes basées sur les données d'évaluation. Cela représente 20 secondes. Les requêtes simples peuvent être résolues rapidement, tandis que les questions de recherche complexes peuvent impliquer de nombreuses itérations et jusqu'à 120 secondes.",answer9:"Oui, l'API officielle DeepSearch sur deepsearch.jina.ai/v1/chat/completions est entièrement compatible avec le schéma API OpenAI, en utilisant « jina-deepsearch-v1 » comme nom de modèle. Il est donc très facile de passer d'OpenAI à DeepSearch et de l'utiliser avec des clients locaux ou tout client compatible OpenAI. Nous recommandons vivement Chatwise pour une expérience fluide.",question1:"Qu'est-ce que DeepSearch ?",question10:"Quelles sont les limites de débit pour l'API ?",question11:"Quel est le contenu à l'intérieur de la balise <think> ?",question12:"DeepSearch utilise-t-il Jina Reader pour la recherche et la lecture sur le Web ?",question14:"Pourquoi DeepSearch utilise autant de jetons pour mes requêtes ?",question18:"Existe-t-il un moyen de contrôler ou de limiter le nombre d’étapes ?",question19:"Dans quelle mesure les références dans les réponses sont-elles fiables ?",question2:"En quoi DeepSearch est-il différent des capacités de recherche approfondie d’OpenAI et de Gemini ?",question20:"DeepSearch peut-il gérer des questions sur des événements futurs ?",question3:"De quelle clé API ai-je besoin pour utiliser DeepSearch ?",question5:"Que se passe-t-il lorsque DeepSearch atteint son budget de jetons ? Renvoie-t-il une réponse incomplète ?",question6:"DeepSearch garantit-il des réponses précises ?",question7:"Combien de temps dure une requête DeepSearch typique ?",question9:"DeepSearch peut-il fonctionner avec n’importe quel client compatible OpenAI comme Chatwise, CherryStudio ou ChatBox ?",title:"Questions courantes liées à DeepSearch"},high_explain:"Raisonnement maximal et recherche de requêtes complexes (2M tokens/req)",low_explain:"Raisonnement de base et recherche de requêtes simples (max 500 000 jetons/req)",medium_explain:"Raisonnement modéré et profondeur de recherche (1 M de jetons/req)",message:"Message",messages:"Messages",messages_explain:"Une liste de messages entre l'utilisateur et l'assistant comprenant la conversation jusqu'à présent.",model:"Modèle",model_explain:"ID du modèle à utiliser.",model_name:"Nom du modèle",open:"Ouvrir",reasoning_effort:"Effort de raisonnement",reasoning_effort_explain:"Limite l'effort de raisonnement pour les modèles de raisonnement. Les valeurs actuellement prises en charge sont faible, moyenne et élevée. La réduction de l'effort de raisonnement peut entraîner des réponses plus rapides et moins de jetons utilisés pour le raisonnement dans une réponse.",stream:"Streaming",stream_explain:"Si la valeur est true, renvoie un flux d'événements qui se produisent pendant l'exécution en tant qu'événements envoyés par le serveur, se terminant lorsque l'exécution entre dans un état terminal avec un message de données : [DONE].",tagline:"@:deepsearch.description",title:"Recherche profonde",user_message:"Utilisateur",what_is:"Qu'est-ce que DeepSearch ?"},b={description:"Créez des œuvres d'art Disco Diffusion convaincantes en une seule ligne de code"},x={description:"La structure de données pour les données multimodales"},A="Télécharger l'attestation SOC 2 Type 1",L={"11B tokens":"11 milliards","11B tokens_intuition1":"C’est similaire à la lecture de tous les articles en anglais sur Wikipédia.","11B tokens_targetUser":"Déploiement de production","1B tokens":"1 milliard","1B tokens_intuition1":`C'est à peu près la même chose que de lire les œuvres complètes de Shakespeare et toute la série "Harry Potter".`,"1B tokens_targetUser":"Développement de prototypes","1M tokens":"1 million","1M tokens_intuition1":`Équivalent à la lecture de l'intégralité du texte de "Le Hobbit" et "The Great Gatsby".`,"1M tokens_targetUser":"Expérience de jouet","1M_free":"1 million de jetons gratuits","1M_free_description":"Profitez de votre nouvelle clé API avec des jetons gratuits, aucune carte de crédit requise.","2_5B tokens":"2,5 milliards de jetons","2_5B tokens_intuition1":`Comparable à la transcription 1 000 fois de chaque mot prononcé dans la trilogie du film « Le Seigneur des Anneaux ».
`,"3p_integration":"Avec <b>{_numPartners}</b> services tiers","3p_integration_desc":"Intégrez notre base de recherche à vos services existants. Nos partenaires ont construit des connecteurs vers notre API, facilitant l'utilisation de nos modèles dans vos applications.","500M tokens":"500 millions de jetons","500M tokens_intuition1":"C’est comme regarder chaque épisode des « Simpsons » de la saison 1 à la saison 30.","59B tokens":"59 milliards de jetons","59B tokens_intuition1":"Égal à tous les tweets publiés dans le monde sur une période de deux jours.","5_5B tokens":"5,5 milliards de jetons","5_5B tokens_intuition1":"Équivalent à la lecture de l’intégralité du texte de l’Encyclopaedia Britannica.",Free1M:"1 million de jetons","ReaderLM-v2_description":"Un petit modèle de langage pour convertir du HTML brut en Markdown ou JSON",add_pair:"Nouveau",add_time_explain:"La date à laquelle ce modèle a été ajouté à la Search Foundation.",api_integration_short:"Notre API d'intégration est nativement intégrée à diverses bases de données renommées, magasins de vecteurs, frameworks RAG et LLMOps.",api_integrations:"Intégrations d'API",api_key_update_message:"En remplaçant votre ancienne clé API, la nouvelle clé apparaîtra dans l'interface utilisateur à chaque fois que vous visiterez jina.ai. Les recharges futures s'appliqueront à cette nouvelle clé. Votre ancienne clé reste valide, donc si vous prévoyez de l'utiliser à nouveau, veuillez la conserver en toute sécurité.",api_key_update_title:"Remplacement de la clé API",auto_recharge:"Recharge automatique pour un solde de jetons faible",auto_recharge_confirm_message:"Êtes-vous sûr de vouloir désactiver la recharge automatique ? Cela arrêtera les recharges automatiques lorsque le solde de votre jeton est faible et peut interrompre votre service ou votre application.",auto_recharge_confirm_title:"Désactiver la recharge automatique",auto_recharge_description:"Recommandé pour un service ininterrompu en production. Lorsque le solde de votre token descend en dessous du seuil défini, nous rechargerons automatiquement votre mode de paiement enregistré pour le dernier forfait acheté, jusqu'à ce que le seuil soit atteint.",auto_recharge_enable:"Vous avez activé la recharge automatique sur les jetons faibles",auto_recharge_enable_message:"Pour activer la recharge automatique, veuillez acheter un pack avec la recharge automatique définie sur vrai.",auto_recharge_enable_message2:"Veuillez sélectionner un forfait à acheter lorsque la recharge automatique est déclenchée.",auto_recharge_enable_title:"Activer la recharge automatique",auto_request:"Aperçu automatique",auto_request_tooltip:`Prévisualisez automatiquement la réponse de l'API lors de la modification du modèle, en utilisant des centaines de jetons de votre clé API. Désactivez l'option d'envoi manuel d'une demande en cliquant sur "Obtenir une réponse".`,autostart:"L'intégration démarrera automatiquement après un bref délai",base64_description:"Les intégrations sont renvoyées sous forme de chaîne codée en base64. Plus efficace pour la transmission.",batch_job:"Travail par lots",batch_upload_hint:"Nous utiliserons la clé API et le modèle ci-dessous pour traiter les documents.","bge-base-en-v1_5_description":"Un modèle anglais robuste équilibrant performances et efficacité pour une utilisation polyvalente.","bge-base-en_description":"Un modèle anglais équilibré conçu pour des performances solides et fiables.","bge-base-zh-v1_5_description":"Un modèle chinois complet, équilibrant capacité et efficacité.","bge-base-zh_description":"Un modèle chinois polyvalent alliant efficacité et performances robustes.","bge-large-en-v1_5_description":"Un modèle anglais puissant offrant des intégrations de haut niveau avec une qualité exceptionnelle.","bge-large-en_description":"Un modèle anglais très performant conçu pour des intégrations de première qualité.","bge-large-zh-v1_5_description":"Un modèle chinois de grande capacité offrant des intégrations supérieures et détaillées.","bge-large-zh_description":"Un modèle chinois haute performance optimisé pour les intégrations de haut niveau.","bge-m3_description":"Un modèle multilingue polyvalent offrant des capacités étendues et des intégrations de haute qualité.","bge-small-en-v1_5_description":"Un modèle anglais simplifié offrant des intégrations efficaces et de haute qualité.","bge-small-en_description":"Un modèle anglais efficace pour des intégrations rationalisées et précises.","bge-small-zh-v1_5_description":"Un modèle chinois compact offrant des intégrations agiles et précises.","bge-small-zh_description":"Un modèle chinois agile pour des plongements efficaces et précis.",binary_description:"Les intégrations sont emballées sous la forme int8. Beaucoup plus efficace pour le stockage, la recherche et la transmission.",bulk:"Intégration par lots",bulk_embedding_failed:"Échec de la création d'une tâche d'intégration par lots",buy_more_quota:"Rechargez cette clé API avec plus de jetons",buy_poster:"Acheter une copie papier",cancel_button:"Annuler",click_upload_btn_above:"Cliquez sur le bouton de téléchargement ci-dessus pour commencer.",clip_v2_description:"jina-clip-v2 est un modèle de style CLIP de 0,9 B qui apporte trois avancées majeures : la prise en charge multilingue de 89 langues, une résolution d'image élevée à 512x512 et l'apprentissage de la représentation Matryoshka pour les plongements tronqués.",clip_v2_title:"clip-v2 : Incorporations multilingues et multimodales",code:"code",colbert_dimensions_explain:"La taille dimensionnelle de l'intégration par jeton.",compatible:"Mode compatible",compatible_explain:"Suit le même format de requête que nos modèles d'intégration de texte. Cela vous permet de basculer entre les modèles sans modifier la requête. Notez que la saisie d'image n'est pas prise en charge dans ce mode.",contact_sales:"Contacter le service commercial",contact_sales_description:"Contactez notre équipe commerciale",cosine_similarity:"Similitude cosinus",debugging:"Test",delete_pair:"Supprimer",description:"@:landing_page.embedding_desc1",dimensions:"Dimensions de sortie",dimensions_error:"La taille de la dimension doit être comprise entre 1 et 1024.",dimensions_explain:"Des dimensions plus petites permettent un stockage et une récupération efficaces, avec un impact minimal grâce à la représentation Matryoshka.",dimensions_warning:"Nous vous recommandons de conserver la taille de la dimension au-dessus de {_minDimension} pour des raisons de performances.",document:"Document",download:"Télécharger",edit_text1_text:"Modifier le texte de gauche",edit_text2_text:"Modifier le texte correct",embedding_done:"{_Count} phrases ont bien été intégrées.",embedding_none_description:"N'utilisez aucun modèle d'intégration",example_inputs:"Exemples d'entrées",faq:"@:contact_us_page.faq",faqs_v2:{answer0:"Pour des informations détaillées sur nos processus de formation, nos sources de données et nos évaluations, veuillez vous référer à notre rapport technique disponible sur arXiv.",answer1:"Jina CLIP <code>jina-clip-v2</code> est un modèle d'intégration multimodal avancé qui prend en charge les tâches de recherche de texte-texte, de texte-image, d'image-image et d'image-texte. Contrairement au CLIP OpenAI d'origine, qui peine à effectuer une recherche de texte-texte, Jina CLIP excelle en tant que récupérateur de texte. <code>jina-clip-v2</code> offre une amélioration des performances de 3 % par rapport à <code>jina-clip-v1</code> dans les tâches de recherche de texte-image et de texte-texte, prend en charge 89 langues pour la recherche d'images multilingues, traite des images à plus haute résolution (512x512) et réduit les besoins de stockage avec les représentations Matryoshka. Vous pouvez en savoir plus à ce sujet dans notre rapport technique.",answer17:"Oui, <code>jina-clip-v2</code> et <code>jina-clip-v1</code> peuvent intégrer à la fois des images et des textes. L'intégration de modèles sur d'autres modalités sera annoncée prochainement !",answer18:"Pour toute question concernant le réglage fin de nos modèles avec des données spécifiques, veuillez nous contacter pour discuter de vos besoins. Nous sommes ouverts à l’exploration de la manière dont nos modèles peuvent être adaptés pour répondre à vos besoins.",answer19:"Oui, nos services sont disponibles sur les marketplaces AWS, Azure et GCP. Si vous avez des besoins spécifiques, veuillez nous contacter à sales AT jina.ai.",answer3:"Depuis sa sortie le 18 septembre 2024, <code>jina-embeddings-v3</code> est le meilleur modèle multilingue et se classe 2e au classement anglais du MTEB pour les modèles avec moins d'un milliard de paramètres. v3 prend en charge un total de 89 langues, dont les 30 langues les plus performantes : arabe, bengali, chinois, coréen, danois, espagnol, finnois, français, géorgien, allemand, grec, hindi, indonésien, italien, japonais, letton, néerlandais, norvégien, polonais, portugais, roumain, russe, slovaque, suédois, thaï, turc, ukrainien, ourdou et vietnamien. Pour plus de détails, veuillez consulter le rapport technique <code>jina-embeddings-v3</code>.",answer4:"Nos modèles permettent une longueur de saisie allant jusqu'à 8192 jetons, ce qui est nettement plus élevé que la plupart des autres modèles. Un jeton peut aller d'un seul caractère, comme « a », à un mot entier, comme « pomme ». Le nombre total de caractères pouvant être saisis dépend de la longueur et de la complexité des mots utilisés. Cette capacité de saisie étendue permet à nos modèles <code>jina-embeddings-v3</code> et <code>jina-clip</code> d'effectuer une analyse de texte plus complète et d'obtenir une plus grande précision dans la compréhension du contexte, en particulier pour les données textuelles volumineuses.",answer5:"Un seul appel API peut traiter jusqu'à 2 048 phrases ou textes, facilitant ainsi une analyse approfondie du texte en une seule requête.",answer6:"Vous pouvez utiliser soit <code>url</code>, soit <code>bytes</code> dans le champ <code>input</code> de la requête API. Pour <code>url</code>, indiquez l'URL de l'image que vous souhaitez traiter. Pour <code>octets</code>, encodez l'image au format base64 et incluez-la dans la requête. Le modèle renverra les intégrations de l'image dans la réponse.",answer7:"Dans les évaluations sur les benchmarks MTEB anglais, multilingue et LongEmbed, <code>jina-embeddings-v3</code> surpasse les derniers embeddings propriétaires d'OpenAI et Cohere sur les tâches en anglais, et surpasse <code>multilingual-e5-large-instruct</code> sur toutes les tâches multilingues. Avec une dimension de sortie par défaut de 1024, les utilisateurs peuvent tronquer les dimensions d'intégration jusqu'à 32 sans sacrifier les performances, grâce à l'intégration de Matryoshka Representation Learning (MRL).",answer8:"La transition est simplifiée, car <a class='text-primary' href='https://api.jina.ai/v1/embeddings'>notre point de terminaison d'API</a> correspond aux schémas JSON d'entrée et de sortie du modèle <code>text-embedding-3-large</code> d'OpenAI. Cette compatibilité garantit que les utilisateurs peuvent facilement remplacer le modèle OpenAI par le nôtre lorsqu'ils utilisent le point de terminaison d'OpenAI.",answer9:`Les jetons sont calculés en fonction de la longueur du texte et de la taille de l'image. Pour le texte de la requête, les jetons sont comptés de manière standard. Pour les images, les étapes suivantes sont réalisées :

1. Taille de la mosaïque : chaque image est divisée en mosaïques. Pour <code>jina-clip-v2</code>, les mosaïques mesurent 512 x 512 pixels, tandis que pour <code>jina-clip-v1</code>, elles mesurent 224 x 224 pixels.
2. Couverture : le nombre de mosaïques nécessaires pour couvrir l'image d'entrée est calculé. Même si les dimensions de l'image ne sont pas parfaitement divisibles par la taille de la mosaïque, les mosaïques partielles sont comptées comme des mosaïques complètes.
3. Nombre total de mosaïques : le nombre total de mosaïques couvrant l'image détermine le coût. Par exemple, une image de 600 x 600 pixels serait couverte par 2 x 2 mosaïques (4 mosaïques) dans la v2 et 3 x 3 mosaïques (9 mosaïques) dans la v1.
4. Calcul du coût : Pour <code>jina-clip-v2</code>, chaque tuile coûte 4000 jetons, tandis que pour <code>jina-clip-v1</code>, chaque tuile coûte 1000 jetons.

Exemple :
Pour une image de dimensions 600x600 pixels :

• Avec <code>jina-clip-v2</code>
• L'image est divisée en tuiles de 512x512 pixels.
• Le nombre total de tuiles requises est de 2 (horizontal) x 2 (vertical) = 4 tuiles.
• Le coût pour <code>jina-clip-v2</code> sera de 4*4000 = 16000 jetons.

• Avec <code>jina-clip-v1</code>
• L'image est divisée en tuiles de 224x224 pixels.
• Le nombre total de tuiles requises est de 3 (horizontal) x 3 (vertical) = 9 tuiles.
• Le coût de jina-clip-v1 sera de 9*1000 = 9000 jetons.`,question0:"Comment les modèles jina-embeddings-v3 ont-ils été formés ?",question1:"Quels sont les modèles jina-clip et puis-je les utiliser pour la recherche de texte et d'images ?",question17:"Fournissez-vous des modèles pour intégrer des images ou du son ?",question18:"Les modèles Jina Embedding peuvent-ils être ajustés avec des données privées ou d’entreprise ?",question19:"Vos points de terminaison peuvent-ils être hébergés en privé sur AWS, Azure ou GCP ?",question3:"Quelles langues vos modèles prennent-ils en charge ?",question4:"Quelle est la longueur maximale d’une seule phrase saisie ?",question5:"Quel est le nombre maximum de phrases que je peux inclure dans une seule demande ?",question6:"Comment envoyer des images aux modèles jina-clip ?",question7:"Comment les modèles Jina Embeddings se comparent-ils aux derniers embeddings d'OpenAI et de Cohere ?",question8:"Dans quelle mesure la transition entre Text-Embedding-3-Large d'OpenAI et votre solution est-elle transparente ?",question9:"Comment les jetons sont-ils calculés lors de l'utilisation des modèles jina-clip ?",title:"Questions courantes liées aux intégrations"},feature_8k1:"8192 longueur de jeton",feature_8k_description1:"Pionnier du premier modèle d'intégration open source avec une longueur de 8 192 jetons, permettant la représentation d'un chapitre entier dans un seul vecteur.",feature_cheap:"20x moins cher",feature_cheap_v1:"5x moins cher",feature_cheap_v1_description1:"Commencez par des essais gratuits et profitez d’une structure tarifaire simple. Accédez à de puissantes intégrations pour seulement 20 % du coût d'OpenAI.",feature_multilingual:"Proposant des modèles bilingues allemand-anglais, chinois-anglais, entre autres, idéaux pour les applications multilingues.",feature_on_premises:"La confidentialité avant tout",feature_on_premises_description1:"Déployez en toute transparence nos modèles d'intégration directement dans votre Virtual Private Cloud (VPC). Actuellement pris en charge sur AWS Sagemaker, avec des intégrations à venir pour Microsoft Azure et Google Cloud Platform. Pour des déploiements Kubernetes sur mesure, contactez notre équipe commerciale pour obtenir une assistance spécialisée.",feature_on_premises_description2:"Déployez les modèles Jina Embeddings dans AWS Sagemaker, et bientôt dans Microsoft Azure et Google Cloud Services, ou contactez notre équipe commerciale pour obtenir des déploiements Kubernetes personnalisés pour votre cloud privé virtuel et vos serveurs sur site.",feature_on_premises_description3:"Déployez les modèles Jina Embeddings dans AWS Sagemaker et Microsoft Azure, et bientôt dans Google Cloud Services, ou contactez notre équipe commerciale pour obtenir des déploiements Kubernetes personnalisés pour votre cloud privé virtuel et vos serveurs sur site.",feature_on_premises_description4:"Déployez les modèles Jina Embedding et Reranker sur site à l'aide d'AWS SageMaker, Microsoft Azure ou Google Cloud Services, en garantissant que vos données restent sous votre contrôle en toute sécurité.",feature_solid:"Le meilleur de sa catégorie",feature_solid_description1:"Développé à partir de nos recherches universitaires de pointe et rigoureusement testé par rapport aux modèles SOTA pour garantir des performances inégalées.",feature_top_perform1:"Intégration transparente",feature_top_perform_description1:"Entièrement compatible avec l'API d'OpenAI. S'intègre sans effort à plus de 10 bases de données vectorielles et systèmes RAG pour une expérience utilisateur fluide.",file_required:"Le fichier est requis",file_size_exceed:"Dépasser la taille maximale du fichier {_size}",file_type_not_supported:"Type de fichier non pris en charge",fill_example:"Remplissez un exemple",float_description:"Les intégrations sont renvoyées sous forme de liste de nombres à virgule flottante. Le plus courant et le plus facile à utiliser.",free:"Gratuit",generate_api_key_error:"La génération de la clé API a échoué.",generating_visualization:"Génération de visualisation...",get_new_key_button:"Obtenir une nouvelle clé",get_new_key_button_explain:"Opter pour une nouvelle clé entraînera la perte de l’historique d’utilisation associé à l’ancienne clé.",get_new_key_survey:"Remplissez l'enquête, aidez-nous à comprendre votre utilisation et obtenez une nouvelle clé API gratuitement !",includes:"Jetons valables pour :",index_and_search:"Index et recherche",index_and_search1:"Index et recherche",input:"Demande",input_api_key_error1:"Votre clé API n'est pas valide !",input_length:"Longueur d'entrée",input_type:"Intégrer en tant que document/requête",input_type_explain:"La même entrée peut servir soit de requête, soit d'incorporation de document, selon son rôle de recherche.",integrate:"Intégrer","jina-clip-v1_description":"Modèles d'intégration multimodaux pour les images et le texte anglais","jina-clip-v2_description":"Incorporations multilingues et multimodales pour textes et images","jina-colbert-v1-en_description":"ColBERT amélioré avec une longueur de jeton de 8 000 pour les tâches d'intégration et de reclassement","jina-colbert-v2_description":"Le meilleur ColBERT multilingue avec des performances de pointe en matière d'intégration et de reclassement","jina-embedding-b-en-v1_description":"La première version du modèle Jina Embedding, l'OG.","jina-embeddings-v2-base-code_description":"Optimisé pour la recherche de code et de docstring","jina-embeddings-v2-base-de_description":"Intégrations bilingues allemand-anglais avec performances SOTA","jina-embeddings-v2-base-en_description":"À égalité avec text-embedding-ada002 d'OpenAI","jina-embeddings-v2-base-es_description":"Intégrations bilingues espagnol-anglais avec performances SOTA","jina-embeddings-v2-base-zh_description":"Intégrations bilingues chinois-anglais avec performances SOTA","jina-embeddings-v2-small-en_description":"Optimisé pour une faible latence et une faible empreinte mémoire","jina-embeddings-v3_description":"Modèle d'intégration multilingue Frontier avec performances SOTA","jina-reranker-v1-base-en_description":"Notre premier modèle de reranker maximisant la pertinence de la recherche et du RAG","jina-reranker-v1-tiny-en_description":"Le modèle de reclassement le plus rapide, le mieux adapté pour classer un grand nombre de documents de manière fiable","jina-reranker-v1-turbo-en_description":"La meilleure combinaison de vitesse d'inférence rapide et de scores de pertinence précis","jina-reranker-v2-base-multilingual_description":"Le dernier et le meilleur modèle de reclassement avec prise en charge multilingue, des appels de fonctions et de la recherche de code.",key:"Clé API",key_enter_placeholder:"Veuillez saisir votre clé API",key_enter_placeholder_to_topup:"Saisissez la clé API que vous souhaitez recharger",key_to_top_up:"Vous avez une autre clé API à recharger ? Collez-la ci-dessus et cliquez sur « Enregistrer ».",key_warn:"Assurez-vous de stocker votre clé API dans un endroit sûr. Sinon vous devrez générer une nouvelle clé",key_warn_v2:"C'est votre clé unique. Conservez-la en toute sécurité !",language_explain:"Ce modèle prend en charge de manière optimale la langue {_langue}.",last_7_days:"Usage",late_chunking:"Morceau tardif",late_chunking_explain:"Appliquez la technique de découpage tardif pour exploiter les capacités de contexte long du modèle afin de générer des intégrations de morceaux contextuels.",learn_more:"Apprendre encore plus",learn_poster:"Découvrez comment nous l'avons fait",learning1:"En savoir plus sur les intégrations",learning1_description:"Par où commencer avec les intégrations ? Nous avons ce qu'il vous faut. Découvrez les intégrations de A à Z avec notre guide complet.",length:"Longueur du jeton",manage_billing:"Gérer la facture",manage_billing_tip:"Gérez vos informations de facturation, obtenez des factures et configurez la recharge automatique.",manage_quota1:"Clé API et facturation",max_file_size:"Taille maximale autorisée : {_maxSize}.",maximize_tooltip:"Maximisez ce panneau avec Shift+1",mistake_contact:"Si vous pensez qu'il s'agit d'une erreur, veuillez nous contacter.",mminput_placeholder:"Texte, URL de l'image, chaîne base64 de l'image",model_required:"Le modèle est requis",more_models:"{_numMore} autres modèles",more_than_two2:"Veuillez saisir plus de deux documents, c'est-à-dire plus de deux lignes.",multi_embedding:"Multi-vecteur",multi_embedding_explain:"Ce modèle renverra un sac d'intégrations contextualisées pour une entrée donnée. Chaque jeton de l'entrée est mappé à un vecteur dans la sortie.",multilingual:"Prise en charge multilingue",multimodal:"Multimodal",multimodal_explain:"Ce modèle peut encoder à la fois du texte et des images, ce qui le rend idéal pour les tâches de recherche multimodales.",new:"Nouveau modèle",no_data1:"Ajoutez une paire de phrases pour calculer la similarité",none:"Aucun",normalized:"Normalisation L2",normalized_explain:"Adapte l'intégration de sorte que sa norme euclidienne (L2) devienne 1, en préservant la direction. Utile lorsque l'aval implique un produit scalaire, une classification et une visualisation.",oncsp:"Sur CSP",onprem:"Sur site",open_tensorboard:"Ouvrir le visualiseur",opensource:"Système d'exploitation",opensource_explain:"Ce modèle est open source et disponible sur Hugging Face. Cliquez sur ce bouton pour voir le modèle sur Hugging Face.",original_documents:"Phrases à intégrer",original_documents_hint:"Entrez vos phrases ici. Chaque nouvelle ligne sera considérée comme une phrase/un document distinct.",output:"Réponse",output_dim:"Dimensions",output_dim_explain:"La dimension de sortie d'un vecteur d'incorporation de ce modèle est {_outputDim}.",output_dimension:"Dimensions de sortie",pairwise_test:"Par paire",per_k:"/ 1 000 jetons",per_m:"/ 1 million de jetons",please_fill_docs_first:"Veuillez d'abord saisir quelques phrases ci-dessous avant la recherche.",please_select_model:"Veuillez sélectionner un modèle d'intégration ou un modèle de reclassement",poster:"L'évolution des intégrations Poster",poster_description:"Découvrez l'affiche idéale pour votre espace, présentant des infographies captivantes ou des visuels à couper le souffle retraçant l'évolution des modèles d'intégration de texte depuis 1950.",pricing:"Tarification des API",pricing_desc:"La tarification des API est basée sur l'utilisation des jetons. Une clé API vous donne accès à tous les produits de base de la recherche.",protectData1:"Les données et documents de demande ne sont pas utilisés pour les modèles de formation.",protectData2:"Chiffrement des données en transit (TLS 1.2+) et au repos (AES-GCM 256).",protectData3:"Conforme SOC 2 et RGPD.",protect_data:"Protégez vos données",public_cloud_integration:"Avec <b>{_numPartners}</b> fournisseurs de services cloud",public_cloud_integration_desc:"Votre entreprise utilise-t-elle AWS ou Azure ? Déployez ensuite directement nos modèles de fondation de recherche sur ces plateformes dans votre entreprise, afin que vos données restent sécurisées et conformes.",query:"Requête",raise_issue:"Soulever un problème",rank_none_description:"N'utilisez aucun modèle de reranker",read_api_docs:"Spécifications de l'API",read_release_note:"Lire la note de publication","reader-lm-05b_description":"Un petit modèle de langage pour convertir du HTML brut en Markdown","reader-lm-15b_description":"Un petit modèle de langage pour convertir du HTML brut en Markdown",recharge_threshold:"Seuil de recharge",refresh:"Rafraîchir",refresh_key_tooltip1:"Obtenez une nouvelle clé API gratuitement",refresh_token_count1:"Actualiser pour obtenir les jetons disponibles de la clé API actuelle",regenerate:"Régénérer",remaining:"Jetons disponibles",remaining_left:"Il vous reste <b>{_leftTokens}</b> jetons dans la clé API ci-dessous.",request_number:"Heures de demande",request_path:"Point de terminaison de la demande",results_as_final_result:"#docs comme résultat final",results_fed_to_reranker:"#docs nourris pour reclasser",retry:"Recommencez",return_base64:"Base64 (sous forme de chaîne)",return_binary:"Binaire (emballé sous forme d'int8)",return_float:"Par défaut (comme float)",return_format:"Format des intégrations",return_format_explain:"Outre le flottant, vous pouvez lui demander de revenir sous forme binaire pour une récupération vectorielle plus rapide, ou sous forme d'encodage base64 pour une transmission plus rapide.",return_format_title:"Type de données de retour",return_ubinary:"Binaire (emballé sous forme de uint8)",right_api_key_to_charge:"Veuillez saisir la bonne clé API pour recharger",running:"Actif",score:"Score",search:"Recherche",search_hint:"Tapez pour rechercher dans les phrases répertoriées ci-dessous",select_classify_model:"Sélectionner un classificateur",select_embedding_model:"Sélectionnez les intégrations",select_rerank_model:"Sélectionnez un reclasseur",show_api_key:"Afficher la clé API",size:"Paramètres",size_explain:"Le nombre de paramètres dans le modèle est {_size}, notez qu'il ne s'agit pas de la taille du fichier modèle.",sleeping:"Inactif",start_batch:"Démarrer l'intégration par lots",start_embedding:"Indice",status_explain:"Notre architecture sans serveur peut décharger certains modèles pendant les périodes de faible utilisation. Pour les modèles actifs, les réponses sont immédiates. Les modèles inactifs nécessitent quelques secondes pour se charger lors de la demande initiale. Après l'activation, les demandes ultérieures sont traitées plus rapidement.",task_type:"Tâche en aval",task_type_classification:"Classification",task_type_classification_explain:"Classification des textes.",task_type_explain:"Sélectionnez la tâche en aval pour laquelle les intégrations seront utilisées. Le modèle renverra les intégrations optimisées pour cette tâche.",task_type_none_explain:"Aucun adaptateur ne sera utilisé. Une intégration générique sera renvoyée, utile pour le débogage ou le piratage.",task_type_retrieval_passage:"Passage de récupération",task_type_retrieval_passage_explain:"Intégration de documents dans une tâche de récupération de documents de requête.",task_type_retrieval_query:"Requête de récupération",task_type_retrieval_query_explain:"Intégration de requêtes dans une tâche de récupération de document de requête.",task_type_separation:"Séparation",task_type_separation_explain:"Regroupement de documents, visualisation de corpus.","task_type_text-matching":"Correspondance de texte","task_type_text-matching_explain":"Similarité sémantique de texte, recherche symétrique générale, recommandation, recherche de similitudes, déduplication.",tax_may_apply:"Selon votre emplacement, vous pouvez être facturé en USD, EUR ou dans d'autres devises. Des taxes peuvent s'appliquer.",text1:"Gauche",text2:"Droite",three_ways:"Trois façons d'acheter",three_ways_desc:"Abonnez-vous à notre API, achetez auprès de fournisseurs cloud ou obtenez une licence commerciale pour votre organisation.",title:"API d'intégration",token_example:`Un tweet compte environ 20 jetons, un article de presse environ 1 000 jetons et le roman de Charles Dickens "A Tale of Two Cities" compte plus d'un million de jetons.`,token_length_explain:"La longueur maximale de la séquence de jetons d'entrée est de {_tokenLength} pour ce modèle.",tokens:"Jetons",tools:"Outils",top_up_button:"Recharger l'ancienne clé",top_up_button_explain:"L'intégration de cette clé API offre une solution plus professionnelle, éliminant le besoin de changements de clé fréquents. Les données d'utilisation sont conservées et accessibles à tout moment.",top_up_warning_message1:"La clé API actuelle contient {_remainedTokens} jetons restants et sera remplacée par une nouvelle clé avec {_freeTokens} jetons. Vous pouvez continuer à utiliser ou recharger l'ancienne clé si vous l'avez stockée en toute sécurité. Comment veux-tu procéder?",top_up_warning_title:"Remplacer l'ancienne clé API",total_documents:"Progression de l'intégration : {_Processed}/{_Count} phrases.",tuning:"Affiner",turnstile_error:"Nous ne pouvons pas générer de clé API car nous n'avons pas pu vérifier si vous êtes humain.",turnstile_unsupported:"Nous ne pouvons pas générer de clé API car votre navigateur n'est pas pris en charge.",ubinary_description:"Les intégrations sont emballées sous forme de uint8. Beaucoup plus efficace pour le stockage, la recherche et la transmission.",upload:"Télécharger",upload_file:"Cliquez ici pour télécharger un fichier",usage:"Usage",usage_amount:"Jetons",usage_history:"Utilisation au cours des 7 derniers jours",usage_history_explain:"Les données ne sont pas en temps réel et peuvent être retardées de quelques minutes.",usage_reason:"Description",usage_reason_consume:"Utilisé",usage_reason_purchase:"Acheté",usage_reason_transfer_in:"Transfert en",usage_reason_transfer_out:"Transfert sortant",usage_reason_trial:"Procès",usage_rerank:"Usage",usage_time:"Date et heure",v3_description:"<code>jina-embeddings-v3</code> est un modèle d'intégration de texte multilingue de pointe avec 570 M de paramètres et 8 192 tokens de longueur, surpassant les dernières intégrations propriétaires d'OpenAI et de Cohere sur MTEB. Lisez notre article de blog et notre article de recherche ci-dessous.",v3_title:"v3 : Incorporations multilingues Frontier",vector_database_integration1:"Intégrations",vector_database_integration2:"Notre API d'intégration est nativement intégrée à diverses bases de données renommées, magasins de vecteurs, frameworks RAG et LLMOps. Pour commencer, copiez et collez simplement votre clé API dans l'une des intégrations répertoriées pour un démarrage rapide et transparent.",vector_database_integration3:"Notre API Embedding & Reranker est nativement intégrée à diverses bases de données renommées, magasins de vecteurs, frameworks RAG et LLMOps. Pour commencer, copiez et collez simplement votre clé API dans l'une des intégrations répertoriées pour un démarrage rapide et transparent.",vector_database_integration_description:"Intégrez de manière transparente et facile l'API Jina Embeddings à l'une des bases de données vectorielles, des cadres d'orchestration LLM et des applications RAG ci-dessous. Nos tutoriels vous montreront comment.",view_details:"Voir les détails",visualization_example:"Mapper toutes les phrases de cette section sur un espace vectoriel 3D",visualization_example_you_can:"Utilisez notre API ci-dessous, vous pouvez le faire aussi !",visualize:"Visualiser",visualize_done:"La visualisation est terminée, vous pouvez maintenant cliquer sur le bouton du haut pour ouvrir le visualiseur.",wait_for_processing:"Votre demande est en cours de traitement.",wait_stripe:"Ouverture du paiement Stripe, veuillez patienter",what_are_embedding:"Que sont les intégrations ?",what_are_embedding_answer:`Imaginez que vous appreniez à un ordinateur à saisir le sens nuancé des mots et des expressions. Les méthodes traditionnelles, qui reposaient sur des systèmes rigides fondés sur des règles, ont échoué car le langage est trop complexe et fluide. Entrez les intégrations de texte : une solution puissante qui traduit le texte dans un langage de nombres, plus précisément en vecteurs dans un espace de grande dimension.

Considérez les expressions « temps ensoleillé » et « ciel dégagé ». Pour nous, ils dressent un tableau similaire. À travers le prisme des intégrations, ces phrases sont transformées en vecteurs numériques proches les uns des autres dans cet espace multidimensionnel, capturant leur parenté sémantique. Cette proximité dans l’espace vectoriel ne concerne pas seulement la similitude des mots ou des expressions ; il s'agit de comprendre le contexte, les sentiments et même les nuances subtiles du sens.

Pourquoi cette avancée est-elle importante ? Tout d’abord, il comble le fossé entre la richesse du langage humain et l’efficacité informatique des algorithmes. Les algorithmes excellent dans l’analyse des chiffres, pas dans l’interprétation des textes. En convertissant le texte en vecteurs, les intégrations permettent à ces algorithmes de « comprendre » et de traiter le langage d'une manière qui était auparavant hors de portée.

Les applications pratiques sont vastes et variées. Qu'il s'agisse de recommander un contenu qui correspond à vos intérêts, d'alimenter une IA conversationnelle qui semble étonnamment humaine ou même de détecter des modèles subtils dans de grands volumes de texte, les intégrations sont la clé. Ils permettent aux machines d’effectuer des tâches telles que l’analyse des sentiments, la traduction linguistique et bien plus encore, avec une compréhension du langage de plus en plus nuancée et raffinée.`,what_is_a_token:"Un jeton en traitement de texte est une unité, souvent un mot. Par exemple, « Jina AI est géniale ! » devient cinq jetons, y compris la ponctuation.",why_do_you_need:"Choisir les bonnes intégrations",why_do_you_need_after:"Tirant parti des réseaux neuronaux profonds et des LLM, nos modèles d'intégration représentent les données multimodales dans un format rationalisé, améliorant la compréhension des machines, le stockage efficace et permettant des applications d'IA avancées. Ces intégrations jouent un rôle crucial dans la compréhension des données, l'amélioration de l'engagement des utilisateurs, la suppression des barrières linguistiques et l'optimisation des processus de développement.",why_do_you_need_before:"Nos modèles d'intégration sont conçus pour couvrir diverses applications de recherche et GenAI.",why_need_1_description:"Notre modèle d'intégration de base, optimisé par JinaBERT, est conçu pour un large spectre d'applications. Il excelle dans la compréhension de textes détaillés, ce qui le rend idéal pour la recherche sémantique, la classification de contenu et l'analyse linguistique complexe. Sa polyvalence est inégalée, prenant en charge la création d'outils avancés d'analyse des sentiments, de résumés de texte et de systèmes de recommandation personnalisés.",why_need_1_title:"Intégrations à usage général",why_need_2_description:"Nos modèles bilingues facilitent la communication entre les langues, en améliorant les plateformes multilingues, le support client mondial et la découverte de contenu multilingue. Conçus pour maîtriser les traductions allemand-anglais et chinois-anglais, ces modèles simplifient les interactions et favorisent la compréhension entre divers groupes linguistiques.",why_need_2_title:"Intégrations bilingues",why_need_3_description:"Conçu sur mesure pour les développeurs, notre modèle d'intégration de code optimise les tâches de codage telles que la synthèse, la génération de code et les révisions automatiques. Il augmente la productivité en offrant des informations plus approfondies sur les structures du code et en suggérant des améliorations, ce qui le rend essentiel pour développer des plugins IDE avancés, une documentation automatique et des outils de débogage de pointe.",why_need_3_title:"Intégrations de code",why_need_4_description:"Jina CLIP est notre dernier modèle d'intégration multimodal pour l'image et le texte. Une grande amélioration par rapport à OpenAI CLIP est que ce modèle unique peut être utilisé pour les tâches de récupération texte-texte, ainsi que pour les tâches de récupération texte-image, image-texte et image-image ! Donc un modèle, deux modalités, quatre directions de recherche !",why_need_4_title:"Intégrations multimodales",write_email_here:"Veuillez saisir l'adresse e-mail à laquelle vous souhaitez recevoir le lien de téléchargement une fois terminé.",you_can_leave:"Vous pouvez quitter cette page et nous vous enverrons le lien de téléchargement une fois terminé."},z={description:"Intégrations multimodales et multilingues de classe mondiale."},y={contractType:{department:"Licence départementale",poc:"Preuve de concept (3 à 6 mois)",standard:"Licence d'entreprise standard",title:"Type de contrat"},department:{businessSponsor:"Commanditaire de l'unité commerciale",executionModel:"Modèle d'exécution",growth:{high:"Potentiel clair à l'échelle de l'entreprise",highDesc:"Initiative stratégique avec adoption prévue à l’échelle de l’entreprise",limited:"Limité au département",limitedDesc:"Concentrez-vous sur les besoins d'un seul service avec un support standard",steady:"Potentiel pour d’autres départements",steadyDesc:"Extension prévue à 2-3 départements dans les 12 mois"},growthTitle:"Trajectoire de croissance",sponsorDescription:"Sponsor exécutif dédié qui soutient la mise en œuvre, fournit une orientation stratégique et garantit l'allocation des ressources"},descriptions:{contractType:{department:"Déploiement dans un seul service avec possibilité d'extension ultérieure",poc:"Déploiement d'essai pour tester les modèles dans votre environnement et vos cas d'utilisation spécifiques",standard:"Déploiement complet de l'entreprise avec utilisation illimitée du modèle"},department:{growth:"Projets visant à étendre l'utilisation du modèle à d'autres services",sponsorship:"Indique si un service générateur de revenus soutient le déploiement"},features:{csm:"Responsable de la réussite personnelle des clients pour des conseils et un soutien stratégiques",priority:"Réponse rapide garantie pour les problèmes critiques",training:"Aide à la pré-formation ou au réglage fin des modèles pour vos données spécifiques"},models:{clip:"Traiter à la fois les images et le texte pour les applications multimodales",colbert:"Modèle spécialisé pour la récupération de documents de haute précision",embeddings:"Modèle d'intégration de texte pour la recherche sémantique et la similarité de texte",reader:"Convertit le contenu HTML en format Markdown propre",reranker:"Affinez les résultats de recherche pour une meilleure pertinence"},payment:{annual:"Paiement annuel unique pour une comptabilité simplifiée",quarterly:"Paiements réguliers tous les trois mois"},poc:{duration:"Calendrier de test et de validation des performances du modèle dans votre environnement",metrics:"Suivre les indicateurs de performance clés et l'efficacité du modèle"},support:{enterprise:"Couverture d'assistance complète avec la plus haute priorité",premium:"Des heures de consultation supplémentaires et des délais de réponse plus rapides",standard:"Assistance technique de base et conseils de mise en œuvre"},usage:{business:"Combien d'entreprises différentes utiliseront des applications basées sur nos modèles",consumer:"Combien d'utilisateurs finaux interagiront avec nos modèles chaque mois"}},features:{csm:"Responsable de la réussite client dédié",priority:"SLA de réponse prioritaire (4 heures)",title:"Fonctionnalités supplémentaires",training:"Prise en charge de la formation sur modèle personnalisé"},interests:"Je suis intéressé par cette licence commerciale configurée (${_Price})",labels:{basePrice:"Prix de base",custom:"Contactez le service commercial pour connaître les tarifs personnalisés",discountApplied:"Remise appliquée",included:"Inclus dans le prix de base",learnMore:"Apprendre encore plus",priceQuarterly:"Prix par trimestre",selectAll:"Sélectionner tous les modèles",selectSupport:"Sélectionnez le niveau de support",totalPrice:"Prix total",upTo:"Jusqu'à {count}"},messaging:{additionalFeatures:"Fonctionnalités supplémentaires incluses :",baseModelIncluded:"Modèle d'intégration de base (jina-embeddings-v3) inclus",deptIncludes:"La licence du département comprend :",deptReviews:"Réunions trimestrielles de revue d'activité",deptRoadmap:"Planification de la feuille de route de l'expansion de l'entreprise",deptSponsor:"Séances d'alignement des sponsors exécutifs",deptWorkshops:"Ateliers de collaboration interdépartementale",enterpriseAlert:"Votre niveau d'utilisation suggère une opportunité à l'échelle de l'entreprise. Planifions un appel pour discuter d'un accord d'entreprise personnalisé.",noModelsSelected:"Aucun modèle supplémentaire sélectionné. Utilisation du modèle d'incorporation de base.",pocCheckins:"Contrôles bimensuels avec l'équipe technique",pocIncludes:"Le forfait POC comprend :",pocMetrics:"Tableau de bord de suivi des indicateurs de réussite",pocMigration:"Prise en charge de la migration vers une licence complète",pocTemplate:"Modèle de documentation des résultats du POC",selectedModels:"Modèles sélectionnés :",standardFeatures:"Fonctionnalités de la licence standard :",supportTierIncluded:"{tier} niveau d'assistance inclus {heures}",usageTierBusiness:"Niveau d'utilisation professionnelle : jusqu'à {count} comptes professionnels",usageTierConsumer:"Niveau d'utilisation grand public : jusqu'à {count} utilisateurs actifs par mois"},models:{clip:"jina-clip-v2",colbert:"jina-colbert-v2",description:"Choisissez les modèles à inclure dans votre package commercial",embeddings:"jina-embeddings-v3",lm:"lecteur-lm",reranker:"jina-reranker-v2",title:"Sélectionnez les modèles"},payment:{annual:"Facturation annuelle (10% de remise)",features:"Fonctionnalités incluses",quarterly:"Facturation trimestrielle",title:"Conditions de paiement"},poc:{description:"Le POC inclut le suivi des indicateurs de réussite et le chemin de mise à niveau vers la licence complète",duration:"Durée du POC (mois)"},pricing:{annual:"année",cta:"Parlez à notre équipe commerciale",disclaimer:"Ce calculateur de prix fournit une estimation. Votre prix final peut varier en fonction d'exigences spécifiques, d'engagements de volume et de configurations personnalisées. Contactez notre équipe commerciale pour obtenir un devis détaillé.",frequency:"${price} / {frequency}",oneTime:"${price} une fois",pocTotal:"Prix POC de {mois} : ${price}",quarterly:"quart",title:"Prix estimé"},short_title:"Licence de configuration",subtitle:"Configurez votre licence d'entreprise pour les modèles Jina AI",support:{enterprise:"Prime",hoursQuarter:"{heures} heures/trimestre",premium:"Standard",standard:"Léger",title:"Niveau de soutien"},title:"Configurateur de licences d'entreprise",tooltips:{annualDiscount:"Économisez 10 % en payant annuellement",businessSponsor:"Avoir un sponsor d'unité commerciale peut vous donner droit à des remises supplémentaires",pocDuration:"Sélectionnez la durée de votre période de preuve de concept",supportTier:"Choisissez le niveau de support qui correspond le mieux à vos besoins",usageLimit:"Contactez-nous pour des tarifs personnalisés si vous dépassez ces limites"},usage:{business:"B2B (Comptes d'affaires)",businessCount:"Nombre de comptes d'entreprise",businessDescription:"Nombre de comptes commerciaux distincts utilisant nos modèles",consumer:"B2C (utilisateurs finaux)",consumerCount:"Utilisateurs actifs mensuels",consumerDescription:"Nombre d'utilisateurs finaux actifs mensuels sur toutes les applications",title:"Configuration d'utilisation"}},P={answer1:"Jina AI est spécialisée dans les technologies d'IA multimodales, y compris le réglage de modèle, le service de modèle, le réglage rapide et le service rapide. Nous exploitons des outils avancés tels que Kubernetes et des architectures sans serveur pour créer des solutions robustes, évolutives et prêtes pour la production.",answer10:"Nous proposons différentes options de licence en fonction de la nature du projet et des besoins du client. Les conditions détaillées peuvent être discutées avec notre équipe de vente.",answer11:"Nous fournissons des services dans le monde entier, avec notre siège social basé à Berlin, en Europe, et des bureaux supplémentaires à Pékin et Shenzhen.",answer12:"Oui, nous proposons une assistance sur site, en particulier pour les clients situés à proximité de nos bureaux à Berlin, Pékin et Shenzhen. Pour les autres sites, nous nous efforçons de fournir la meilleure assistance à distance possible et pouvons organiser une assistance sur site si nécessaire.",answer2:"Notre expertise couvre un large spectre, englobant les grands modèles de langage, le texte, l'image, la vidéo, la compréhension audio, la recherche neuronale et l'art génératif.",answer3:"Oui, nos solutions sont conçues pour être évolutives et prêtes pour la production. Nous construisons nos solutions à l'aide de technologies cloud natives qui permettent une mise à l'échelle efficace et des performances fiables dans les environnements de production.",answer4:"Nos services sont polyvalents et adaptables, ce qui les rend adaptés à un large éventail de secteurs, notamment le commerce électronique, la technologie juridique, le marketing numérique, les jeux, la santé, la finance et bien d'autres.",answer5:"Vous pouvez entrer en contact avec notre équipe commerciale via le formulaire de contact sur cette page. Nous serions ravis de discuter des exigences de votre projet et de la manière dont nos solutions peuvent aider votre entreprise.",answer6:"Nous fournissons un support continu pour assurer le bon fonctionnement de nos solutions. Cela inclut le dépannage, les mises à jour régulières et les améliorations basées sur vos commentaires et vos besoins.",answer7:"La durée du projet varie en fonction de la complexité et de la portée du projet. Après avoir compris vos besoins, nous pouvons vous fournir une estimation plus précise.",answer8:"La sécurité des données est notre priorité absolue. Nous adhérons à des politiques et réglementations strictes en matière de protection des données pour garantir la sécurité et la confidentialité de vos données.",answer9:"Le prix dépend de la complexité et des exigences du projet. Nous offrons à la fois des modèles de tarification basés sur le projet et des honoraires. Veuillez contacter notre équipe de vente pour plus d'informations.",question1:"Dans quoi Jina AI est-elle spécialisée ?",question10:"Quelles sont les conditions de licence de vos solutions ?",question11:"Quelle est votre zone de service ?",question12:"Proposez-vous une assistance sur site ?",question2:"Avec quels types d'IA Jina AI travaille-t-elle ?",question3:"Vos solutions sont-elles évolutives et prêtes pour la production ?",question4:"Quelles industries peuvent bénéficier des solutions de Jina AI ?",question5:"Comment démarrer un projet avec Jina AI ?",question6:"Quel accompagnement apportez-vous après la mise en place d'une solution ?",question7:"Quelle est la durée type d'un projet ?",question8:"Comment Jina AI protège-t-elle mes données ?",question9:"Quelle est la structure tarifaire de vos services ?"},I="FAQ",j={text:"Adieu.",toggle_btn:"Gardez ce panneau ouvert lors de votre prochaine visite",warning_message:"Ce panneau s'ouvrira automatiquement lorsque vous visiterez jina.ai. Vous devrez le fermer pour voir le contenu du site Web. Activer ce paramètre ?",warning_title:"Afficher au démarrage"},C={description:"Affiner les intégrations sur des données spécifiques à un domaine pour une meilleure qualité de recherche",intro:"Votre entreprise. Vos données. Votre modèle"},k={description:"Renforcez votre entreprise avec des solutions de réglage fin sur site"},R={api_key:"Entrez votre clé API.",back:"Dos",base_model_selected:"Modèle de base sélectionné",click_start:"Acceptez les conditions et commencez les réglages.",confirm_title:"Confirmer le travail de réglage fin",confirm_your_email:"Saisissez à nouveau votre adresse e-mail pour confirmer le travail de réglage fin. Les mises à jour et le lien de téléchargement seront envoyés à cet e-mail.",consent0:"J'accepte que des données synthétiques pour le réglage fin du modèle soient générées sur la base de mes instructions.",consent1:"Je reconnais que le modèle final et les données synthétiques seront accessibles au public sur Hugging Face.",consent2:"Je comprends que cette fonctionnalité est en version bêta et Jina AI n'offre aucune garantie. Les prix et l'UX peuvent changer.",continue:"Continuer",cost_1m_token:"Chaque tâche de réglage fin consomme 1 million de jetons. Assurez-vous de disposer de suffisamment de jetons ou rechargez votre solde. Vous pouvez également générer une nouvelle clé API. Chaque clé API est livrée avec 1 million de jetons gratuits.",doc_explain:"Décrivez à quoi devrait ressembler un document correspondant.",domain_explain:"Fournissez une description détaillée de la manière dont les intégrations affinées seront utilisées. Ceci est essentiel pour générer des données synthétiques de haute qualité qui amélioreront les performances de vos intégrations.",domain_explain2:"Il existe trois manières de spécifier vos besoins : une instruction générale, une URL ou une description du document de requête. Choisissez-en un.",domain_hint:"Décrivez le domaine pour lequel vous souhaitez affiner votre recherche.",email_not_match:"Les adresses e-mail ne correspondent pas. Veuillez vérifier.",failed_job:"La demande de réglage fin a échoué. Voir la raison ci-dessous.",find_on_huggingface:"Trouver des résultats sur Hugging Face",general_instruction:"Ou, instruction générale",general_instruction_caption:"Fournissez une description détaillée de la manière dont les intégrations affinées seront utilisées.",general_instruction_explain:"Décrivez votre domaine sous forme de texte libre. Vous pouvez l'imaginer comme une « invite » comme dans ChatGPT.",how_it_works:"Découvrez le processus de réglage fin.",job_acknowledged:"Votre travail de réglage fin a été mis en file d'attente. Vous recevrez un e-mail lorsque le travail commencera. Le processus complet prend souvent 20 minutes.",new_key:"Obtenir une nouvelle clé",not_enough_token:"Pas assez de jetons dans cette clé API. Veuillez recharger votre solde ou utiliser une autre clé API.",placeholder:"Réclamations d'assurance automobile",preview:"Aperçu",query_doc:"Description du document de requête",query_doc_caption:"Décrivez à quoi ressemble la requête et à quoi ressemble le document correspondant dans votre domaine.",query_explain:"Décrivez à quoi ressemble une requête.",reset:"Recommencer",select_base_model:"Choisissez un modèle d'intégration de base pour un réglage précis.",select_base_model_explain:"Sélectionnez un modèle de base comme point de départ pour le réglage fin. En règle générale, base-en est un bon choix, mais pour les tâches dans d'autres langues, envisagez d'utiliser un modèle bilingue.",start_tuning:"Commencer le réglage fin",url:"Ou l'URL de la page Web",url_caption:"Reportez-vous au contenu d'une URL pour un réglage précis.",url_explain:"URL publique d'une page Web contenant le contenu que vous souhaitez affiner.",use_url:"Utilisez plutôt l'URL. L'activer signifie que nous nous baserons sur le contenu de la page de cette URL pour générer des données synthétiques à des fins de réglage.",wait_for_processing:"Veuillez patienter pendant que nous accédons à votre requête...",which_domain:"Domaine de réglage fin",write_email_explain:"La mise au point prend du temps. Nous communiquerons par e-mail sur le début, la progression, l'achèvement et tout problème lié à votre travail de mise au point, ainsi que des détails sur le modèle affiné et l'ensemble de données de formation."},w={address_beijing:"Pékin, Chine",address_berlin:"Berlin, Allemagne (siège social)",address_shenzhen:"Shenzhen, en Chine",address_sunnyvale:"Sunnyvale, Californie",all_rights_reserved:"Tous les droits sont réservés.",api_documentation:"Documentation de l'API",company:"Entreprise",developers:"Développeurs",docs:"Documents",enterprise:"Entreprise",get_api_key:"Obtenir la clé API Jina",offices:"Des bureaux",power_users:"Utilisateurs avancés",privacy:"Confidentialité",privacy_policy:"politique de confidentialité",privacy_settings:"Gérer les cookies",security:"Sécurité",sefo:"Fondation Recherche",soc2:"Nous sommes conformes aux normes SOC 2 Type 1 et 2 de l'American Institute of Certified Public Accountants (AICPA).",status:"Statut de l'API",status_short:"Statut",tc:"termes et conditions",tc1:"Termes"},S="Obtenez votre clé API",M={stars:"Étoiles"},T={description:"Déclarations de base avec connaissances Web",title:"Vérification des faits",usage:"Utilisation de la mise à la terre"},D={about_us:"À propos de nous",api_docs:"Documentation de l'API",api_docs_explain:"Génération automatique de code pour votre IDE ou LLM copilote",company:"Entreprise",contact_us:"Contacter le service commercial",developers_others:"Plus d'outils de développement",enterprise_others:"Plus d'outils d'entreprise",for_developers:"Pour les développeurs",for_developers_description:"Découvrez une pile d'IA multimodale open source complète conçue pour les développeurs.",for_enterprise:"Pour les entreprises",for_enterprise_description:"Découvrez des stratégies d'IA multimodales évolutives adaptées aux besoins de l'entreprise.",for_power_users:"Pour les utilisateurs expérimentés",for_power_users_description:"Utilisez nos outils multimodaux simplifiés pour améliorer votre productivité.",internship1:"Programme de stage",jobs:"Rejoignez-nous",join_discord:"Rejoignez notre communauté Discord",logos:"Télécharger le logo",maximize:"⇧1",maximize_btn:"Maximiser",news:"Nouvelles",open_day:"Journée portes ouvertes",open_in_full:"Afficher tous les produits d'entreprise dans une nouvelle fenêtre",power_users_others:"Plus d'outils pour les utilisateurs expérimentés",products:"Des produits"},E={description:"Partagez et découvrez les éléments de base des applications d'IA multimodales"},N={sentence_similarity:"Incorporation de phrases",updated_about:"Mise à jour sur"},U={project1:"Activation de la recherche de haute précision dans les données de maillage 3D à l'aide d'informations de nuage de points.",project10:"Exploitation de la vision par ordinateur pour améliorer l'accessibilité numérique des sites Web du gouvernement.",project11:"Perfectionnement du LLM pour une société de conseil afin d'optimiser l'analyse des données financières.",project12:"Stratégies marketing avancées en affinant les modèles texte-image pour le transfert de style.",project2:"Conception d'un moteur de recherche basé sur le contenu pour les courts métrages d'animation.",project3:"Amélioration des taux de conversion du commerce électronique en affinant les modèles d'intégration.",project4:"Exécution d'un réglage rapide pour accroître l'efficacité d'une société de conseil aux entreprises.",project5:"Compréhension de la scène de jeu et annotation automatique pionnière pour une entreprise de jeu de premier plan.",project6:"Mise en œuvre de l'extension des entrées en temps réel pour une entreprise de chatbot, améliorant l'expérience utilisateur.",project7:"Technologie juridique révolutionnée en permettant une recherche efficace dans de longs documents juridiques.",project8:"Prise en charge d'un service d'art génératif à haut débit pour les opérations à grande échelle.",project9:"Réaliser l'exploration de processus et la modélisation à l'aide de modèles de langage avancés."},B={description:"Modèles multimodaux de pointe disponibles pour l'inférence"},J={copy_full_prompt:"Copier l'invite complète",embedding:"Intégrations",how_to_use_meta_prompt:"Comment utiliser",meta_prompt:"Utiliser la méta-invite pour la génération de code",meta_prompt_description:"Le méta-invite guide les LLM (comme ChatGPT et Claude) à travers toutes nos API Search Foundation, rendant la génération de code plus facile et de meilleure qualité.",reranker:"Reclasseur",which_to_go:"Lequel intégrer avec {_vendor} ?"},O={answer1:"Premier cycle, maîtrise et doctorat. les étudiants du monde entier, intéressés par des domaines tels que la recherche, l'ingénierie, le marketing et les ventes, sont encouragés à postuler. Nous accueillons également des stages non techniques dans les domaines du marketing, des ventes, de l'assistance à la direction, etc. Nous recherchons des personnes passionnées prêtes à devenir les pionnières de l'IA multimodale avec nous.",answer10:"Oui, notre programme de stage offre une rémunération compétitive.",answer11:"En tant que stagiaire Jina AI, vous obtiendrez une expérience pratique de travail sur des projets stimulants, apprendrez des experts de l'industrie, ferez partie d'une communauté dynamique et aurez l'opportunité d'apporter de réelles contributions à notre travail de pionnier dans l'IA multimodale.",answer2:"Les stages doivent être effectués sur place dans l'un de nos bureaux, situés à Berlin, Pékin et Shenzhen.",answer3:"Oui, Jina AI offre une assistance raisonnable dans le processus de visa pour les candidats retenus.",answer4:"Oui, Jina AI fournit une couverture raisonnable du coût de la vie aux stagiaires pendant la période de stage.",answer5:"Oui, il est possible de travailler sur votre mémoire de maîtrise pendant votre stage à Jina AI, généralement applicable aux étudiants des universités allemandes. Cependant, vous devez avoir une communication préalable et l'accord du superviseur de votre université. Notez que nous n'aidons pas les étudiants à trouver des conseillers.",answer6:"Le processus de candidature comprend la soumission de votre formulaire de candidature, un CV, une lettre de motivation exprimant votre intérêt et votre motivation, ainsi que tout lien professionnel pertinent tel que GitHub ou LinkedIn. Nous évaluons les candidats en fonction de leur performance lors de l'entretien et de leur performance dans leur université.",answer7:"Oui, les stagiaires retenus peuvent recevoir une lettre de recommandation à la fin de leur stage, signée par notre PDG.",answer8:"La durée du stage varie en fonction de la fonction et du projet. Cependant, il varie généralement de trois à six mois.",answer9:"Oui, nous accueillons les candidatures de tous les horizons académiques. Nous apprécions votre passion et votre engagement à apprendre autant que votre expérience antérieure.",question1:"Qui peut postuler au programme de stage Jina AI ?",question10:"Est-ce un stage rémunéré ?",question11:"Quelles opportunités aurai-je en tant que stagiaire Jina AI ?",question2:"Où se déroulera le stage ?",question3:"Jina AI assiste-t-elle dans les processus de visa ?",question4:"Jina AI offre-t-elle des indemnités ou des avantages aux stagiaires ?",question5:"Puis-je travailler sur mon mémoire de Master pendant le stage à Jina AI ?",question6:"En quoi consiste le processus de candidature ?",question7:"Jina AI fournit-elle une lettre de recommandation post-stage ?",question8:"Quelle est la durée du stage ?",question9:"Puis-je postuler si je n'ai pas d'expérience préalable en IA ?"},G={about_internship_program:"À propos du programme de stages",about_internship_program_desc1:"Nous sommes ravis d'offrir cette opportunité unique à des personnes talentueuses de se joindre à notre équipe dynamique et de contribuer à des projets novateurs dans le domaine de l'intelligence artificielle. Ce stage est conçu pour vous fournir une expérience pratique précieuse, un mentorat et une exposition aux technologies de pointe qui façonnent l'avenir de l'IA.",about_internship_program_desc2:"Chez Jina AI, nous comprenons l'importance de nourrir et d'exploiter les jeunes talents. Nous reconnaissons que les stagiaires apportent de nouvelles perspectives, de l'enthousiasme et de la créativité à la table, revigorant notre équipe avec de nouvelles idées et approches. En offrant des stages, nous visons à favoriser la croissance des futurs leaders de l'industrie de l'IA tout en leur offrant une expérience du monde réel dans un environnement favorable et stimulant.",alumni:"ANCIENS",alumni_network:"Notre réseau dynamique d'anciens élèves",application:"Application",application_desc:"Embarquez pour un voyage transformateur avec Jina AI. Notre programme de stages complet invite tous les esprits passionnés qui aspirent à façonner l'avenir de l'intelligence artificielle. Rejoignez-nous pour acquérir une expérience du monde réel, travailler sur des projets stimulants et collaborer avec certains des esprits les plus brillants de l'industrie de l'IA.",apply:"Appliquer maintenant",autumn:"Automne",description:"Appel mondial à étudiants : Stage en recherche, ingénierie, marketing, vente et plus encore.",dev_rel_intern:"Stagiaire Relations Développeurs",enthusiastic:"ENTHOUSIASTE",explore_stories_from_our_interns:"Découvrez les histoires de nos stagiaires",explore_stories_from_our_interns1:"Laissez-vous inspirer par les parcours de nos stagiaires",innovative:"INNOVANT",intern_work1:"Modèles LLM affinés pour de meilleures incorporations",intern_work2:"Exploration du potentiel de Retrieval Augmented Generation",intern_work3:"Publication d'un article sur le thème des incorporations de phrases",intern_work4:"Injecter une vitalité juvénile continue dans l’équipe",intern_work5:"Techniques de quantification éprouvées pour compresser LLM",intern_work6:"Créer et promouvoir une campagne convaincante pour PromptPerfect",intern_work7:"JinaColBERT V2 rapidement développé et amélioré",recruiting_and_administrative_intern:"Stagiaire en recrutement et en administration",researcher_intern:"Chercheur stagiaire",self_motivated:"AUTO-MOTIVÉ",software_engineer_intern:"Stagiaire ingénieur logiciel",spring:"Printemps",submit_application:"Lancez votre aventure avec Jina AI",subtitle:"Notre programme de stages à temps plein offre une expérience de travail pratique grâce à des projets de stages bien conçus dans un large éventail de domaines.",subtitle1:"Appel mondial aux étudiants : stagiaires en recherche, ingénierie, marketing, ventes et plus encore pour lancer ensemble l'IA multimodale.",summer:"Été",title:"Programme de stage",who_do_we_look_for:"Qui cherchons-nous ?",who_do_we_look_for_desc:"Nous valorisons la diversité et encourageons les candidats de profils et d'horizons divers à rejoindre notre programme de stages. Les opportunités de stage sont proposées dans plusieurs départements, notamment l'ingénierie, la conception, la gestion des produits, les ventes et la gestion des comptes, le marketing et la gestion de la communauté.",winter:"Hiver"},V={description:"Déployez un projet local en tant que service cloud. Radicalement facile, pas de mauvaises surprises."},F={description:"Un fintuner expérimental pour les LLM open-source"},Q={description:"Créez des applications d'IA multimodales sur le cloud"},W={description:"Plus de modalité, plus de mémoire, moins de coût",example_1:"Qui es-tu?",example_2:"Je suis un service de chat LLM créé par Jina AI"},H={add:"Ajouter une clé",add_key_explain:"Ajoutez une autre clé API à votre compte. Les clés ajoutées peuvent être gérées, rechargées ou supprimées à tout moment.",add_shared_key:"Ajouter à mes clés",add_success:"Une clé {_key} a été ajoutée avec succès.",advance_settings:"Ouvrir les paramètres avancés",advanced_feature:"Fonctionnalité avancée pour clé premium uniquement.",auto_recharge_enable_success:"La recharge automatique pour la clé {_key} a été activée avec succès.",auto_recharge_title:"Activer la recharge automatique ?",auto_reminder:"Rappel de solde faible",auto_reminder_cancel_message:"Êtes-vous sûr de vouloir annuler le rappel automatique pour cette clé ?",auto_reminder_cancel_title:"Annuler le rappel automatique",auto_reminder_description:"Recevez des alertes par e-mail automatiques lorsque le solde de vos jetons descend en dessous des seuils définis. Vous pouvez configurer jusqu'à trois seuils.",auto_reminder_email:"Adresse e-mail pour les rappels",auto_reminder_info:"La notification sera envoyée à {_email} lorsque le solde du jeton descendra en dessous de {_threshold} jetons.",auto_reminder_threshold:"Rappeler si",auto_reminder_threshold_error:"Le seuil doit être compris entre 1 et 1T.",auto_reminder_toggle:"Activer le rappel automatique, veuillez noter que seule la clé premium peut activer cette fonctionnalité.",available_resources:"Jetons disponibles",balance:"Jetons disponibles",balance_primary_key:"Solde de la clé primaire",cancel:"Annuler",confirm:"Confirmer",copy:"Copier la clé",copy_share_link:"Copier le lien",description:"Gérez les clés API pour tous les services Jina AI : Embeddings, Reader, Reranker, etc.",do_it_later:"Fais-le plus tard",email:"E-mail",existing_key:"Clé existante",filter_by:"Filtrer par clé",free_key:"Clé gratuite",generate_new_key:"Générer une nouvelle clé",generate_new_key_tooltip:"Générez une nouvelle clé API avec un solde vide. Vous pourrez recharger le solde plus tard.",generate_success:"Une nouvelle clé {_key} a été générée avec succès.",get_free_key:"Créer une clé API",ignore:"Ignorer",invalid_email:"Email invalide",invalid_key:"Clé invalide",is_primary:"Votre clé API principale. Vous pouvez la modifier après vous être connecté.",last_used:"Dernière utilisation",last_used_at:"Dernière activité",login:"Se connecter",login_explain:"Gérez plusieurs clés API et suivez leur utilisation, le tout dans un seul compte.",login_explain_long:"Connectez-vous pour stocker et gérer en toute sécurité vos clés API. Suivez l'historique d'utilisation, gérez plusieurs clés et ne perdez jamais l'accès à vos informations d'identification.",login_required:"Veuillez vous connecter avant d'ajouter la clé partagée.",login_via:"connecté via {_provider}",logout:"Se déconnecter",logout_message:"Vos clés API restent stockées en toute sécurité dans votre compte. Connectez-vous à tout moment pour les gérer.",logout_success:"Déconnexion réussie",no_key_title:"Besoin d'une clé API ?",no_key_with_login:"Vous n'avez pas encore créé de clé API. Générez-en une maintenant et obtenez des jetons gratuits pour commencer.",no_key_without_login:"Vous avez déjà un compte ? Connectez-vous pour accéder à vos clés API ou cliquez sur '{_button}' pour en créer un nouveau.",no_transferable_keys:"Il n'y a pas d'autres clés disponibles pour le transfert, veuillez d'abord ajouter une nouvelle clé.",ok:"D'ACCORD",primary_key:"Définir comme clé primaire",primary_key_set:"Définissez avec succès {_apiKey} comme clé primaire.",primary_key_set_caption:"Cette clé sera utilisée dans toutes les démos, exemples et terrains de jeux sur jina.ai.",purchase:"Acheter des jetons",recharge_threshold_confirm_message:"Êtes-vous sûr de vouloir modifier le seuil de rechargement automatique à {_threshold} jetons ?",recharge_threshold_confirm_title:"Modifier le seuil de rechargement automatique",remove:"Retirer la clé",remove_explain:"La suppression de la clé de votre liste n'affectera pas le service, les opérations dépendantes ou les autres utilisateurs qui l'ont stockée. La clé reste fonctionnelle et peut être ajoutée à nouveau à tout moment.",remove_message:"Etes-vous sûr de vouloir supprimer cette clé ? La clé reste fonctionnelle et peut être ajoutée à nouveau à tout moment.",remove_primary_key:"Veuillez définir une autre clé comme clé primaire avant de supprimer la clé primaire actuelle.",remove_success:"La clé {_key} a été supprimée avec succès.",remove_title:"Retirer la clé",revoke:"Révoquer la clé",revoke_error:"La clé que vous avez saisie ne correspond pas à la clé que vous essayez de révoquer.",revoke_explain:"La révocation d'une clé la désactivera immédiatement pour tous les utilisateurs qui l'ont stockée, et tout le solde restant et les propriétés associées seront définitivement inutilisables. Cette action ne peut pas être annulée.",revoke_label:"Veuillez confirmer la révocation de cette clé en la saisissant ci-dessous",revoke_message:"Etes-vous sûr de vouloir révoquer cette clé ? Une fois révoquée, cette clé deviendra définitivement invalide pour tous les utilisateurs qui l'ont stockée. Tout le solde restant et les propriétés associées seront définitivement inutilisables. Cette action ne peut pas être annulée.",revoke_success:"La clé {_key} a été révoquée avec succès.",revoke_title:"Révoquer la clé",save:"Sauvegarder",settings:"Paramètres",share:"Partager la clé",share_key_confirm_message:`Le destinataire pourra consulter, gérer et recharger le solde de cette clé. Vous conserverez les mêmes droits. 
Veuillez noter que le lien expirera dans 24 heures.`,share_key_confirm_title:"Partager la clé API",share_key_expired_at:"Le lien partagé expirera à {_time} !",share_key_expired_message:"Le lien partagé de la clé a expiré. Veuillez demander au propriétaire de la clé de la partager à nouveau.",share_key_expired_title:"Lien partagé expiré",share_key_message:"{_user} a partagé une clé API avec vous. Ajoutez-la pour gérer la clé et son solde.",share_link_copied:"Partager le lien copié",shared_from:"Clé partagée par {_user}",shared_key:"Clé partagée",subscribed_key:"Clé Premium",title:"API de la fondation Jina Search",to_dashboard:"Gérer les clés",top_up:"Recharger",total_keys:"Clés totales",transfer_before_revoke:"Transférez les jetons payés restants avant de révoquer la clé.",transfer_explain:"Transférez en toute transparence vos jetons payants restants vers un autre compte pour une plus grande flexibilité et une sécurité renforcée dans la gestion de vos ressources.",transfer_label:"Transférer vers",transfer_message:"Êtes-vous sûr de vouloir transférer vos jetons payants restants {_tokens} de {_source} vers {_target} ?",transfer_success:"Les jetons ont été transférés avec succès de {_source} vers {_target}.",transfer_title:"Jetons de transfert",usage_history:"Historique d'utilisation",usage_summary:"7 derniers jours : {_usage} jetons"},K={GlobalQA:{description:"Appuyez sur la touche « / » sur n'importe quelle page pour ouvrir la boîte de questions. Tapez votre requête et appuyez sur « Entrée » pour recevoir des réponses directement liées au contenu de la page. Cette fonctionnalité est optimisée par PromptPerfect.",title:"RAG sur la page"},Recommender:{description:"Ouvrez la boîte de recommandation sur n'importe quelle page d'actualités avec « Maj+2 ». Sélectionnez le modèle de reclassement pour découvrir les 5 meilleurs articles liés à cette page d'actualités. Profitez de cette fonctionnalité en temps réel, optimisée par notre API Reranker.",title:"Article associé"},SceneXplainTooltip:{description:"Passez votre curseur sur n'importe quelle image sur les pages d'actualités ou dans notre catalogue de rédaction pour révéler la description de cette image. Les descriptions sont précalculées par SceneXplain et intégrées dans l'attribut ALT de l'image pour plus d'accessibilité.",title:"Sous-titrage des images"},explain:"Découvrez les fonctionnalités cachées sur notre site Web"},X={also_available_on:"Également disponible sur les places de marché",also_available_on1:"Disponible sur les places de marché de votre cloud d'entreprise",ask_how_your_question:"Merci de décrire votre problème",autotune:"Réglage automatique",avatar:"Générateur d'avatars",badge:{"clip-v2":"sortie du clip-v2 !","readerlm-v2":"Sortie de ReaderLM-v2 !",v2:"Sortie v2 !",v3:"Version v3 !"},browser_info_title:"Informations sur le navigateur",build_js:"Construire avec JavaScript",build_python:"Construire avec Python",ccbync:"Ce modèle est sous licence CC BY-NC 4.0. Utilisez-le via l'API ou notre image officielle AWS/Azure ; ou contactez le service commercial pour un déploiement sur site.",checkout_our_solution_for_you:"Découvrez notre solution sur mesure pour vous",classifier:"Classificateur",coming_soon:"À venir",contact_sales:"Contact",copied_to_clipboard:"Copié dans le presse-papier",copy:"Copie",developers:"Développeurs",developers_desc:"Libérez toute la puissance de l'IA multimodale avec des technologies cloud natives de pointe et une infrastructure open source.",download_pdf:"Télécharger le PDF",embedding:"Incorporations",embedding_desc1:"Incorporations multimodales multilingues à contexte long les plus performantes pour les applications de recherche, RAG et agents.",embedding_paper_desc:"Jina Embeddings constitue un ensemble de modèles d'incorporation de phrases hautes performances aptes à traduire diverses entrées textuelles en représentations numériques, capturant ainsi l'essence sémantique du texte. Bien que ces modèles ne soient pas exclusivement conçus pour la génération de texte, ils excellent dans des applications telles que la récupération dense et la similarité textuelle sémantique. Cet article détaille le développement de Jina Embeddings, en commençant par la création d'un ensemble de données par paires et triplets de haute qualité. Il souligne le rôle crucial du nettoyage des données dans la préparation des ensembles de données, donne un aperçu approfondi du processus de formation du modèle et se termine par une évaluation complète des performances à l'aide du Massive Textual Embedding Benchmark (MTEB).",embedding_paper_title:"Jina Embeddings : un nouvel ensemble de modèles d'intégration de phrases hautes performances",embeddings:"Intégrations",enterprise:"Entreprise",enterprise_desc:"Boostez votre activité avec des solutions d'IA multimodales évolutives, sécurisées et sur mesure.",enterprise_desc_v2:"Essayez nos modèles d'intégration de classe mondiale pour améliorer vos systèmes de recherche et RAG. Commencez par un essai gratuit !",enterprise_desc_v3:"Nos modèles de frontière constituent la base de recherche pour les systèmes de recherche d'entreprise et RAG de haute qualité.",error:"Il y a eu un problème avec l'opération de récupération : {message}",find_your_portal:"Trouvez votre portail",finding_faq:"Générer une réponse basée sur les connaissances de la FAQ ci-dessous",for:"Pour",for_better_search:"Pour une meilleure recherche",for_developers:"Pour les développeurs",for_enterprise:"Pour les entreprises",for_power_users:"Pour les utilisateurs expérimentés",get_api_now:"API",get_started:"Commencer",go_to_product_homepage:"Accéder à la page d'accueil du produit",grounding:"Mise à la terre",how_to:"Comment",include_experiment:"Inclut nos projets expérimentaux et archivés dans la solution.",join_community:"Communauté",key_manager:"Gérer la clé API",learn_more_embeddings:"En savoir plus sur les intégrations",learn_more_reader:"En savoir plus sur le lecteur",learn_more_reranker:"En savoir plus sur le reclassement",llm:"Modèles d'intégration LLM",llm_desc:"Nous fournissons une collection de modèles d'intégration de phrases hautes performances, comprenant entre 35 millions et 6 milliards de paramètres. Ils sont excellents pour améliorer la recherche neuronale, le reclassement, la similarité des phrases, les recommandations, etc. Préparez-vous à améliorer votre expérience d'IA !",mentioned_products:"Produits mentionnés :",mmstack:"Pile multimodale",mmstack_desc:"Au fil des années, nous avons développé une variété de logiciels open source pour aider les développeurs à créer une meilleure GenAI et à rechercher des applications plus rapidement.",models:"Modèles",more:"Plus",multimodal:"Multimodal",multimodal_ai:"IA multimodale",new:"Nouveau",newsroom:"Rédaction",num_publications:"{_total} publications au total.","on-prem-deploy":"Déploiement sur site","on-premises":"Sur site",opensource:"Open source",our_customer:"Nos clients",our_customer_explain:"Les entreprises de toutes tailles font confiance à la Search Foundation de Jina AI pour alimenter leurs outils et produits. Vous pouvez également le faire.",our_publications:"Nos publications",parameters:"Paramètres",podcast:"Podcast",power_users:"Utilisateurs avancés",power_users_desc:"Ingénierie automatique pour votre productivité quotidienne.",powered_by_promptperfect:'Propulsé par la fonctionnalité "Optimisation des invites" et "Invite en tant que service" de PromptPerfect',pricing:"Tarifs",proposing_solution:"Proposer une solution basée sur les produits Jina AI...",read_more:"En savoir plus",reader:"Lecteur",require_full_question:"Veuillez décrire votre problème avec plus de détails.",reranker:"Reclasseur",researcher_desc:"Découvrez comment nos modèles de recherche de frontière ont été formés à partir de zéro, consultez nos dernières publications. Rencontrez notre équipe chez EMNLP, SIGIR, ICLR, NeurIPS et ICML !",researchers:"Des chercheurs",sdk:"SDK",sdk_desc:"Vous souhaitez créer des applications AIGC de haut niveau à l'aide des API PromptPerfect, SceneXplain, BestBanner, JinaChat, Rationale ? Nous avons ce qu'il vous faut! Essayez notre SDK facile à utiliser et démarrez en quelques minutes.",sdk_docs:"Lire des documents",sdk_example:"Exemple",search_foundation:"Fondation de recherche",source_code:"Code source",starter_kit:"Kit de démarrage",supercharged1:"survoltée!",tokenizer:"Segmenteur",trusted_by:"CONFIÉ PAR",try_it_for_free:"Commencez instantanément — aucune carte de crédit ni inscription requise !",try_our_saas:"Essayez notre solution hébergée, un remplacement immédiat de l'API d'intégration d'OpenAI.",version_notify:"Vous consultez une ancienne version de ce site. Pour accéder aux dernières fonctionnalités, rendez-vous sur {_link}",view_browser_info:"Afficher les informations du navigateur",your_portal_to:"Votre portail vers",your_search_foundation1:"Votre base de recherche"},Y={description:"Applications Langchain en production avec Jina & FastAPI"},$={description:"Informations juridiques, conditions de service, politique de confidentialité et autres documents importants sur les produits et services de Jina AI.",download_type1:"Télécharger l'attestation SOC 2 Type 1",download_type2:"Télécharger l'attestation SOC 2 Type 2",request_audit:"Demander un rapport d'audit",title:"Informations légales"},Z={api:"API de Jina AI",browse_catalog:"Parcourir le catalogue",contact_sales_about_it:"Contactez le service commercial à ce sujet",deploy_it_on:"Déployez-le sur",description:"Nous avons fait évoluer les modèles de recherche depuis le premier jour. Jetez un œil à l'évolution de notre modèle ci-dessous : survolez ou cliquez pour découvrir chaque étape.",find_on_hf:"Trouvez-le sur HuggingFace",search_for:"Recherchez-le sur notre site",search_models:"Filtrer par nom de modèle",title:"Nos modèles de recherche de fondations",use_it_via:"Utilisez-le via"},ee={back_to_models:"Retour aux modèles",comparison:{btn:"Comparer",select_models:"Choisissez les modèles à comparer"},error:"Impossible de charger le modèle",input_type:{"3d":"3D",audio:"Audio",code:"Code",document:"Document",graph:"Graphique",image:"Image","multi-vector":"Multi-vecteur",other:"Autre",ranking:"Classements",tabular:"Tabulaire",text:"Texte","text (code)":"Texte (code)","text (document)":"Texte (Document)","text (html)":"Texte (HTML)","text (json)":"Texte (JSON)","text (markdown)":"Texte (Markdown)","text (query)":"Texte (requête)",timeseries:"Séries chronologiques",vector:"Vecteur",video:"Vidéo"},loading:"Chargement des détails du modèle...",metadata:{api_link:"API Jina",arxiv:"Article ArXiv",aws_link:"AWS SageMaker",azure_link:"Microsoft Azure",deprecated_by:"Obsolète par",gcp_link:"Google Cloud",huggingface_link:"Visage qui fait un câlin",input_type:"Saisir",license:"Licence",license_link:"Licence commerciale",output_type:"Sortir","reader-api_link":"API Jina",related_models:"Modèles associés",release_blog:"Publication de publication",release_date:"Date de sortie"},search:{no_results:"Aucun modèle trouvé correspondant à « {query} »",placeholder:"Rechercher par nom, tags ou type..."},sections:{availability:"Disponibilité",blogs:"Blogs qui mentionnent ce modèle",external_links:"Liens et ressources externes",guidance:{"ReaderLM-v2":"Le modèle est accessible via un bloc-notes Google Colab illustrant la conversion HTML vers Markdown, l'extraction JSON et le suivi des instructions. Pour les tâches HTML vers Markdown, les utilisateurs peuvent saisir du code HTML brut sans instructions de préfixe, tandis que l'extraction JSON nécessite un formatage de schéma spécifique. La fonction d'assistance create_prompt facilite la création d'invites pour les deux tâches. Bien que le modèle fonctionne sur le niveau GPU T4 gratuit de Colab (nécessitant vllm et triton), il présente des limitations sans prise en charge de bfloat16 ou de Flash Attention 2. RTX 3090/4090 est recommandé pour une utilisation en production. Le modèle sera disponible sur AWS SageMaker, Azure et GCP Marketplace, sous licence CC BY-NC 4.0 pour une utilisation non commerciale.","jina-clip-v1":"Pour déployer efficacement Jina CLIP v1, les équipes doivent tenir compte à la fois de ses capacités et de ses besoins en ressources. Le modèle traite les images en mosaïques de 224 x 224 pixels, chaque mosaïque consommant 1 000 jetons de capacité de traitement. Pour des performances optimales, implémentez un prétraitement d'image efficace pour correspondre à ces dimensions. Bien que le modèle excelle dans le traitement de texte court et long, il ne prend actuellement en charge que la saisie en anglais. Les équipes doivent soigneusement réfléchir à l'utilisation des jetons : le texte nécessite environ 1,1 jeton par mot, tandis que les images sont traitées en mosaïques (par exemple, une image de 750 x 500 pixels nécessite 12 mosaïques, consommant 12 000 jetons). Le modèle est disponible via l'API Jina Embeddings et en tant que version open source sur Hugging Face sous la licence Apache 2.0, offrant une flexibilité dans les options de déploiement. Pour les environnements de production, envisagez d'utiliser les options de déploiement AWS Marketplace ou Azure, qui fournissent des configurations d'infrastructure optimisées.","jina-clip-v2":"Pour un déploiement optimal, les utilisateurs doivent prendre en compte plusieurs facteurs clés. Le modèle nécessite un matériel compatible CUDA pour un traitement efficace, avec des besoins en mémoire évolutifs en fonction de la taille du lot et de la résolution de l'image. Pour optimiser les coûts et les performances de l'API, redimensionnez les images à 512 x 512 pixels avant le traitement. Les images plus grandes sont automatiquement mises en mosaïque, ce qui augmente l'utilisation des jetons et le temps de traitement. Le modèle excelle dans la mise en correspondance des images avec du texte descriptif dans différentes langues, mais peut avoir des difficultés avec des concepts abstraits ou du contenu hautement spécialisé dans un domaine. Il est particulièrement efficace pour la recherche de produits de commerce électronique, les systèmes de recommandation de contenu et les applications de recherche visuelle, mais peut ne pas convenir aux tâches nécessitant une analyse détaillée visuelle fine ou une expertise de domaine hautement spécialisée. Lorsque vous utilisez la fonction de représentation Matryoshka, tenez compte du compromis entre la réduction des dimensions et les performances. Si les intégrations à 64 dimensions maintiennent de bonnes performances, les applications critiques peuvent bénéficier de dimensions plus élevées.","jina-colbert-v1-en":"Pour déployer efficacement Jina-ColBERT-v1-en, les équipes doivent prendre en compte plusieurs aspects pratiques. Le modèle nécessite un GPU compatible CUDA pour des performances optimales, bien que l'inférence CPU soit possible pour le développement. Pour le traitement des documents, la limite de 8 192 jetons se traduit par environ 6 000 mots, ce qui le rend adapté à la plupart des types de documents, y compris les articles universitaires, la documentation technique et le contenu long. Les équipes doivent mettre en œuvre un prétraitement efficace des documents pour gérer les limites de jetons et envisager le traitement par lots pour l'indexation à grande échelle. Bien que le modèle excelle dans le contenu en anglais, il n'est pas conçu pour les applications multilingues ou la recherche interlinguistique. Pour les déploiements de production, implémentez des stratégies de découpage de documents appropriées et envisagez d'utiliser des index de similarité vectorielle (comme FAISS) pour une récupération efficace. Le modèle est particulièrement efficace lorsqu'il est intégré dans des pipelines RAG à l'aide de cadres comme RAGatouille, qui simplifient la mise en œuvre de modèles de recherche complexes.","jina-colbert-v2":"Pour déployer efficacement Jina-ColBERT-v2, les équipes doivent prendre en compte plusieurs aspects pratiques. Le modèle nécessite un matériel compatible CUDA pour des performances optimales et prend en charge des longueurs de document allant jusqu'à 8 192 jetons (extensibles à 12 288) tout en limitant les requêtes à 32 jetons. Pour le déploiement en production, le modèle est disponible via l'API Jina Search Foundation, la place de marché AWS et Azure, avec une version non commerciale accessible via Hugging Face. Lors de la mise en œuvre, les équipes doivent spécifier si elles intègrent des requêtes ou des documents, car le modèle utilise un codage asymétrique. Le modèle n'est pas conçu pour le traitement en temps réel de collections de documents extrêmement volumineuses sans indexation appropriée, et bien qu'il excelle dans la récupération multilingue, il peut afficher des performances légèrement inférieures sur des tâches spécialisées spécifiques à un domaine par rapport aux modèles affinés pour ces domaines spécifiques.","jina-embedding-b-en-v1":"Pour un déploiement optimal, le modèle nécessite un GPU compatible CUDA, bien que sa taille modérée permette une inférence efficace sur du matériel standard. Le modèle accepte des séquences d'entrée d'une longueur maximale de 512 jetons et est particulièrement adapté aux environnements de production où une génération d'intégration cohérente et fiable est cruciale. Il fonctionne mieux sur le contenu en anglais et est idéal pour des applications telles que la recherche sémantique, la comparaison de similarité de documents et les systèmes de recommandation de contenu. Les équipes doivent envisager d'utiliser les nouvelles versions v2 ou v3 pour les nouveaux projets, car elles offrent des performances améliorées et une prise en charge linguistique plus large. Le modèle n'est pas recommandé pour les tâches nécessitant une compréhension multilingue ou des connaissances spécialisées dans un domaine en dehors du texte anglais général.","jina-embeddings-v2-base-code":"Pour déployer efficacement le code de base Jina Embeddings v2, les équipes doivent prendre en compte plusieurs aspects pratiques. Le modèle s'intègre parfaitement aux bases de données vectorielles populaires telles que MongoDB, Qdrant et Weaviate, ce qui facilite la création de systèmes de recherche de code évolutifs. Pour des performances optimales, implémentez un prétraitement de code approprié pour gérer la limite de 8 192 jetons, qui s'adapte généralement à la plupart des définitions de fonctions et de classes. Bien que le modèle prenne en charge 30 langages de programmation, il affiche les meilleures performances dans les six langages principaux : Python, JavaScript, Java, PHP, Go et Ruby. Les équipes doivent envisager d'utiliser le traitement par lots pour l'indexation de code à grande échelle afin d'optimiser les performances. La compatibilité RAG du modèle le rend particulièrement efficace pour les tâches automatisées de génération de documentation et de compréhension de code, bien que les équipes doivent mettre en œuvre des stratégies de segmentation appropriées pour les bases de code très volumineuses. Pour les déploiements de production, envisagez d'utiliser le point de terminaison AWS SageMaker pour l'inférence gérée et implémentez des stratégies de mise en cache appropriées pour optimiser les performances des requêtes.","jina-embeddings-v2-base-de":"Pour déployer efficacement Jina Embeddings v2 Base German, les organisations doivent prendre en compte plusieurs aspects pratiques. Le modèle s'intègre parfaitement aux bases de données vectorielles populaires telles que MongoDB, Qdrant et Weaviate, ce qui facilite la création de systèmes de recherche bilingues évolutifs. Pour des performances optimales, implémentez un prétraitement de texte approprié pour gérer efficacement la limite de 8 192 jetons, ce qui permet généralement de gérer environ 15 à 20 pages de texte. Bien que le modèle excelle à la fois dans le contenu allemand et anglais, il est particulièrement efficace lorsqu'il est utilisé pour des tâches de récupération multilingue où les langues de requête et de document peuvent différer. Les organisations doivent envisager de mettre en œuvre des stratégies de mise en cache pour le contenu fréquemment consulté et d'utiliser le traitement par lots pour l'indexation de documents à grande échelle. L'intégration AWS SageMaker du modèle offre un chemin fiable vers le déploiement en production, bien que les équipes doivent surveiller l'utilisation des jetons et mettre en œuvre une limitation de débit appropriée pour les applications à fort trafic. Lorsque vous utilisez le modèle pour les applications RAG, envisagez de mettre en œuvre la détection de langue pour optimiser la construction d'invites en fonction de la langue d'entrée.","jina-embeddings-v2-base-en":"Pour déployer efficacement Jina Embeddings v2 Base English, les équipes doivent prendre en compte plusieurs aspects pratiques. Le modèle nécessite un matériel compatible CUDA pour des performances optimales, bien que son architecture efficace signifie qu'il peut fonctionner sur des GPU grand public. Il est disponible via plusieurs canaux : téléchargement direct depuis Hugging Face, déploiement AWS Marketplace ou API Jina AI avec 1 M de jetons gratuits. Pour les déploiements de production, AWS SageMaker dans la région us-east-1 offre la solution la plus évolutive. Le modèle excelle dans l'analyse de texte à usage général, mais peut ne pas être le meilleur choix pour la terminologie scientifique hautement spécialisée ou le jargon spécifique à un domaine sans réglage précis. Lors du traitement de documents longs, pensez à les diviser en morceaux sémantiques significatifs plutôt qu'en divisions arbitraires pour maintenir l'intégrité du contexte. Pour des résultats optimaux, implémentez un prétraitement de texte approprié et assurez-vous que les données d'entrée sont propres et bien formatées.","jina-embeddings-v2-base-es":"Pour utiliser efficacement ce modèle, les organisations doivent garantir l'accès à une infrastructure GPU compatible CUDA pour des performances optimales. Le modèle s'intègre parfaitement aux principales bases de données vectorielles et aux frameworks RAG, notamment MongoDB, Qdrant, Weaviate et Haystack, ce qui le rend facilement déployable dans les environnements de production. Il excelle dans des applications telles que la recherche de documents bilingues, les systèmes de recommandation de contenu et l'analyse de documents multilingues. Bien que le modèle fasse preuve d'une polyvalence impressionnante, il est particulièrement optimisé pour les scénarios bilingues espagnol-anglais et peut ne pas être le meilleur choix pour les applications monolingues ou les scénarios impliquant d'autres paires de langues. Pour des résultats optimaux, les textes d'entrée doivent être correctement formatés en espagnol ou en anglais, bien que le modèle gère efficacement le contenu multilingue. Le modèle prend en charge le réglage fin pour les applications spécifiques à un domaine, mais cela doit être abordé en tenant soigneusement compte de la qualité et de la distribution des données de formation.","jina-embeddings-v2-base-zh":"Le modèle nécessite 322 Mo de stockage et peut être déployé via plusieurs canaux, notamment AWS SageMaker (région US-East-1) et l'API Jina AI. Bien que l'accélération GPU ne soit pas obligatoire, elle peut améliorer considérablement la vitesse de traitement des charges de travail de production. Le modèle excelle dans diverses applications, notamment l'analyse de documents, la recherche multilingue et la récupération d'informations multilingues, mais les utilisateurs doivent noter qu'il est spécifiquement optimisé pour les scénarios bilingues chinois-anglais. Pour des résultats optimaux, le texte d'entrée doit être correctement segmenté et, bien que le modèle puisse gérer jusqu'à 8 192 jetons, il est recommandé de diviser les documents extrêmement longs en morceaux sémantiquement significatifs pour de meilleures performances. Le modèle peut ne pas convenir aux tâches nécessitant un traitement en temps réel de textes très courts, pour lesquels des modèles spécialisés à faible latence peuvent être plus appropriés.","jina-embeddings-v3":"Pour déployer efficacement Jina Embeddings v3, les équipes doivent tenir compte de leur cas d'utilisation spécifique pour sélectionner l'adaptateur de tâches approprié : retrieval.query et retrieval.passage pour les applications de recherche, séparation pour les tâches de clustering, classification pour la catégorisation et correspondance de texte pour la similarité sémantique. Le modèle nécessite un matériel compatible CUDA pour des performances optimales, bien que son architecture efficace signifie qu'il nécessite beaucoup moins de mémoire GPU que des alternatives plus importantes. Pour le déploiement en production, l'intégration AWS SageMaker offre un chemin simplifié vers l'évolutivité. Le modèle excelle dans les applications multilingues, mais peut nécessiter une évaluation supplémentaire pour les langues à faibles ressources. Bien qu'il prenne en charge les documents longs jusqu'à 8 192 jetons, des performances optimales sont obtenues avec la fonction de segmentation tardive pour les textes très longs. Les équipes doivent éviter d'utiliser le modèle pour des tâches nécessitant une génération en temps réel ou un raisonnement complexe : il est conçu pour l'intégration et la récupération, et non pour la génération de texte ou la réponse directe aux questions.","jina-reranker-v1-base-en":"Le modèle nécessite un matériel compatible CUDA pour des performances optimales et est accessible via les points de terminaison API et les options de déploiement AWS SageMaker. Bien qu'il puisse traiter des séquences extrêmement longues, les utilisateurs doivent tenir compte du compromis entre la longueur du contexte et le temps de traitement : la latence du modèle augmente considérablement avec des documents plus longs, de 156 ms pour 256 jetons à 7 068 ms pour 4 096 jetons avec une requête de 512 jetons. Pour les déploiements de production, il est recommandé de mettre en œuvre un pipeline en deux étapes où la recherche vectorielle fournit des candidats initiaux pour le reclassement. Le modèle est spécifiquement optimisé pour le contenu en anglais et peut ne pas fonctionner de manière optimale sur des documents multilingues ou riches en code. Lors de l'intégration avec les systèmes RAG, les utilisateurs doivent soigneusement ajuster le nombre de documents envoyés pour le reclassement en fonction de leurs besoins de latence, 100 à 200 documents offrant généralement un bon équilibre entre qualité et performances.","jina-reranker-v1-tiny-en":"Pour déployer efficacement ce modèle, les organisations doivent privilégier les scénarios dans lesquels la vitesse de traitement et l'efficacité des ressources sont des considérations essentielles. Le modèle est particulièrement adapté aux déploiements d'informatique de pointe, aux applications mobiles et aux systèmes de recherche à haut débit où les exigences de latence sont strictes. Bien qu'il fonctionne exceptionnellement bien dans la plupart des tâches de reclassement, il est important de noter que pour les applications nécessitant le plus haut niveau de précision de classement, le modèle de base peut toujours être préférable. Le modèle nécessite une infrastructure GPU compatible CUDA pour des performances optimales, bien que son architecture efficace signifie qu'il peut fonctionner efficacement sur du matériel moins puissant que ses homologues plus volumineux. Pour le déploiement, le modèle s'intègre parfaitement aux principales bases de données vectorielles et aux frameworks RAG, et il est disponible via l'API Reranker et AWS SageMaker. Lors du réglage fin pour des domaines spécifiques, les utilisateurs doivent soigneusement équilibrer la qualité des données de formation avec l'architecture compacte du modèle pour maintenir ses caractéristiques de performances.","jina-reranker-v1-turbo-en":"Le modèle nécessite un matériel compatible CUDA pour des performances optimales et peut être déployé via AWS SageMaker ou accessible via des points de terminaison API. Pour les déploiements de production, les organisations doivent mettre en œuvre un pipeline en deux étapes où la recherche vectorielle fournit des candidats initiaux pour le reclassement. Bien que le modèle prenne en charge 8 192 jetons, les utilisateurs doivent tenir compte de l’impact de la latence des séquences plus longues : le temps de traitement augmente avec la longueur du document. Le point idéal pour la plupart des applications est le reclassement de 100 à 200 candidats par requête, ce qui équilibre qualité et vitesse. Le modèle est spécifiquement optimisé pour le contenu en anglais et peut ne pas fonctionner de manière optimale sur les documents multilingues. Les besoins en mémoire sont nettement inférieurs à ceux du modèle de base, ne nécessitant généralement que 150 Mo de mémoire GPU contre 550 Mo, ce qui le rend adapté au déploiement sur des instances plus petites et permet des économies de coûts importantes dans les environnements cloud.","jina-reranker-v2-base-multilingual":"Pour un déploiement optimal, le modèle nécessite un GPU compatible CUDA et peut être consulté via plusieurs canaux, notamment l'API Reranker, les principaux frameworks RAG comme Haystack et LangChain, ou déployé de manière privée via des places de marché cloud. Le modèle excelle dans les scénarios nécessitant une compréhension précise des barrières linguistiques et des types de données, ce qui le rend idéal pour les entreprises mondiales travaillant avec du contenu multilingue, de la documentation API ou des référentiels de code. Sa vaste fenêtre de contexte de 524 288 jetons permet de traiter des documents volumineux ou des bases de code entières en un seul passage. Les équipes doivent envisager d'utiliser ce modèle lorsqu'elles ont besoin d'améliorer la précision de la recherche dans plusieurs langues, ont besoin de capacités d'appel de fonctions pour les systèmes RAG agentiques ou souhaitent améliorer la fonctionnalité de recherche de code dans des bases de code multilingues. Le modèle est particulièrement efficace lorsqu'il est utilisé en conjonction avec des systèmes de recherche vectorielle, où il peut améliorer considérablement le classement final des documents récupérés.","reader-lm-05b":"Pour déployer efficacement Reader LM 0.5B, les organisations doivent s'assurer que leur infrastructure peut gérer les exigences CUDA du modèle, bien que son architecture efficace signifie qu'il peut s'exécuter sur des GPU grand public. Le modèle fonctionne mieux avec une entrée HTML brute et ne nécessite pas de préfixes ou d'instructions spéciaux. Pour des performances optimales, implémentez le mécanisme de détection de répétition fourni pour éviter les boucles de jetons potentielles dans la génération de sortie. Bien que le modèle prenne en charge plusieurs langues et diverses structures HTML, il est spécifiquement conçu pour l'extraction de contenu et la conversion de démarques. Il ne doit pas être utilisé pour des tâches telles que la génération de texte, la synthèse ou la réponse directe à des questions. Le modèle est disponible via AWS SageMaker pour le déploiement en production, et un bloc-notes Google Colab est fourni pour les tests et l'expérimentation. Les équipes doivent savoir que même si le modèle peut gérer des documents extrêmement longs jusqu'à 256 000 jetons, le traitement d'entrées aussi volumineuses peut nécessiter des stratégies de gestion de la mémoire supplémentaires.","reader-lm-15b":"Pour déployer efficacement Reader LM 1.5B, les organisations doivent se concentrer sur des scénarios impliquant le traitement complexe de documents HTML où la précision et l'efficacité sont primordiales. Le modèle nécessite une infrastructure GPU compatible CUDA pour des performances optimales, bien que son architecture efficace signifie qu'il peut fonctionner efficacement sur du matériel plus modeste par rapport aux alternatives plus grandes. Pour les déploiements de production, le modèle est disponible via AWS SageMaker et Azure Marketplace, offrant des options d'intégration flexibles. Bien que le modèle excelle dans la conversion HTML en Markdown, il est important de noter qu'il est spécifiquement optimisé pour cette tâche et peut ne pas convenir à la génération de texte à usage général ou à d'autres tâches NLP. Lors du traitement de documents extrêmement longs (approchant 512 000 jetons), les utilisateurs doivent être conscients que les performances peuvent se dégrader car cela dépasse les paramètres de formation du modèle. Pour des résultats optimaux, implémentez les mécanismes de détection de répétition fournis et envisagez d'utiliser la recherche contrastive pendant l'inférence pour maintenir la qualité de sortie.",title:"Conseils"},image_size:"Taille de l'image d'entrée",language:"Prise en charge linguistique",methods:{"ReaderLM-v2":"Construit sur Qwen2.5-1.5B-Instruction, la formation de ReaderLM-v2 impliquait un ensemble de données HTML-markdown-1m d'un million de documents HTML, avec une moyenne de 56 000 jetons chacun. Le processus de formation comprenait : 1) une préformation à long contexte utilisant l'attention en anneau et RoPE pour étendre le contexte de 32 000 à 256 000 jetons, 2) un réglage fin supervisé avec des ensembles de données raffinés, 3) une optimisation directe des préférences pour l'alignement des sorties et 4) un réglage de renforcement en mode auto-jeu. La préparation des données a suivi un pipeline en trois étapes (Draft-Refine-Critique) alimenté par Qwen2.5-32B-Instruction, avec des modèles spécialisés formés pour des tâches spécifiques avant la fusion via une interpolation de paramètres linéaires.","jina-clip-v1":"L'architecture du modèle représente une innovation significative dans la conception de l'IA multimodale, combinant un encodeur de texte Jina BERT v2 adapté avec l'encodeur d'image de pointe EVA-02 de l'Académie d'intelligence artificielle de Pékin. L'encodeur de texte prend en charge des séquences allant jusqu'à 12 288 jetons - plus de 100 fois plus longues que la limite de 77 jetons du CLIP d'origine - tandis que l'encodeur d'image traite efficacement 16 jetons de patch. Le processus de formation suit une nouvelle approche en trois étapes : premièrement, aligner les paires image-légende tout en maintenant la compréhension du texte grâce à un entraînement par paires de textes entrelacés ; deuxièmement, incorporer des descriptions textuelles plus longues des images générées par l'IA ; et enfin, utiliser des triplets de texte négatifs durs pour améliorer les capacités de distinction sémantique. Cette méthodologie de formation unique permet au modèle de maintenir des performances élevées à la fois pour les légendes courtes et les descriptions textuelles détaillées tout en préservant une forte compréhension visuelle.","jina-clip-v2":"Jina CLIP v2 utilise une architecture sophistiquée à double encodeur qui combine un encodeur de texte Jina XLM-RoBERTa (561 millions de paramètres) avec un encodeur de vision EVA02-L14 (304 millions de paramètres). L'encodeur de texte traite le contenu dans 89 langues avec une fenêtre de contexte massive de 696 320 jetons, tandis que l'encodeur de vision gère les images haute résolution jusqu'à 512 x 512 pixels. Le modèle introduit un apprentissage de représentation Matryoshka innovant, qui permet un ajustement dynamique des dimensions d'intégration de 1024 à 64 dimensions tout en préservant les performances. Cette architecture traite à la fois le texte et les images via leurs encodeurs respectifs, les projetant dans un espace sémantique partagé où les concepts similaires s'alignent indépendamment de leur modalité ou de leur langue d'origine.","jina-colbert-v1-en":"Le modèle utilise une architecture d'interaction tardive innovante qui modifie fondamentalement le fonctionnement de la recherche de documents. Au lieu de comparer des documents entiers en une seule fois, il traite les requêtes et les documents indépendamment jusqu'à l'étape de correspondance finale, en utilisant une version adaptée de l'approche ColBERT. L'architecture combine deux composants clés : un encodeur de document qui traite le texte jusqu'à 8 192 jetons (plus de 16 fois plus longtemps que les transformateurs standard) et un encodeur de requête qui crée des représentations précises au niveau du jeton. Chaque jeton de la requête et du document obtient son propre vecteur d'intégration à 128 dimensions, préservant ainsi les informations sémantiques fines qui seraient perdues dans les modèles à vecteur unique. Le mécanisme d'interaction tardive permet ensuite une correspondance efficace jeton par jeton entre les requêtes et les documents, en utilisant des opérations de regroupement maximal et de sommation pour calculer les scores de pertinence finaux sans nécessiter de comparaisons coûteuses de tous à tous.","jina-colbert-v2":"Le modèle s'appuie sur l'architecture ColBERT, introduisant un mécanisme sophistiqué d'interaction tardive qui modifie fondamentalement la manière dont les requêtes et les documents sont mis en correspondance. À la base, il utilise une dorsale XLM-RoBERTa modifiée avec 560M de paramètres, améliorée par des intégrations de position rotatives et optimisée avec une attention flash. Le processus de formation comprend deux étapes clés : une préformation initiale avec diverses données faiblement supervisées provenant de différentes langues, suivie d'un réglage fin avec des données de triplets étiquetées et une distillation supervisée. Ce qui rend cette approche unique est la mise en œuvre de l'apprentissage de représentation Matryoshka, qui permet au modèle de produire des intégrations dans plusieurs dimensions (128, 96 ou 64) à partir d'un seul processus de formation, permettant une optimisation dynamique du stockage sans réentraînement.","jina-embedding-b-en-v1":"Le modèle utilise une architecture basée sur un encodeur T5 améliorée avec un pooling moyen pour générer des représentations de longueur fixe. Formé sur l'ensemble de données Linnaeus-Clean soigneusement organisé, qui contient 385 millions de paires de phrases de haute qualité filtrées à partir de 1,6 milliard de paires initiales, le modèle a subi un processus de formation en deux phases. La première phase a utilisé l'apprentissage contrastif avec perte InfoNCE sur les paires de textes, tandis que la deuxième phase a incorporé la formation de triplets pour affiner la capacité du modèle à distinguer les contenus similaires des contenus différents. Cette approche de formation innovante, combinée à un filtrage rigoureux des données, y compris la détection de la langue et la vérification de la cohérence, permet au modèle de capturer efficacement des relations sémantiques nuancées.","jina-embeddings-v2-base-code":"Le modèle atteint ses performances impressionnantes grâce à une architecture spécialisée conçue spécifiquement pour la compréhension du code. À la base, il utilise un réseau neuronal basé sur un transformateur avec 161 millions de paramètres, formés sur divers ensembles de données de langage de programmation mettant l'accent sur six langages principaux : Python, JavaScript, Java, PHP, Go et Ruby. Ce qui rend cette architecture unique est sa fenêtre de contexte étendue de 8 192 jetons, lui permettant de traiter des fonctions entières ou plusieurs fichiers à la fois tout en conservant la compréhension sémantique. Le modèle génère des intégrations denses de 768 dimensions qui capturent à la fois la structure syntaxique et la signification sémantique du code, lui permettant de comprendre les relations entre différents segments de code même lorsqu'ils utilisent des modèles de programmation ou une syntaxe différents pour atteindre le même objectif.","jina-embeddings-v2-base-de":"Le modèle atteint ses impressionnantes capacités bilingues grâce à une architecture innovante qui traite à la fois des textes allemands et anglais dans un espace d'intégration unifié de 768 dimensions. À la base, il utilise un réseau neuronal basé sur un transformateur avec 161 millions de paramètres, soigneusement formés pour comprendre les relations sémantiques dans les deux langues. Ce qui rend cette architecture particulièrement efficace est son approche de minimisation des biais, spécialement conçue pour éviter le piège courant consistant à privilégier les structures grammaticales anglaises - un problème identifié dans des recherches récentes sur les modèles multilingues. La fenêtre de contexte étendue du modèle de 8 192 jetons lui permet de traiter des documents entiers ou plusieurs pages de texte en un seul passage, en maintenant la cohérence sémantique sur le contenu long dans les deux langues.","jina-embeddings-v2-base-en":"L'architecture du modèle combine une dorsale BERT Small avec un mécanisme innovant bidirectionnel symétrique ALiBi (Attention with Linear Biases), éliminant ainsi le besoin d'intégrations positionnelles traditionnelles. Ce choix architectural permet au modèle d'extrapoler bien au-delà de sa longueur d'entraînement de 512 jetons, en gérant des séquences allant jusqu'à 8 192 jetons sans dégradation des performances. Le processus de formation comportait deux phases clés : une pré-formation initiale sur l'ensemble de données C4, suivie d'un affinement sur la collection organisée par Jina AI de plus de 40 ensembles de données spécialisés. Ces données de formation diverses, comprenant des exemples négatifs difficiles et des paires de phrases variées, garantissent des performances robustes dans différents domaines et cas d'utilisation. Le modèle produit des vecteurs denses de 768 dimensions qui capturent des relations sémantiques nuancées, obtenues avec un nombre relativement modeste de 137 millions de paramètres.","jina-embeddings-v2-base-es":"Au cœur de ce modèle se trouve une architecture innovante basée sur l'ALiBi (Attention with Linear Biases) bidirectionnel symétrique, une approche sophistiquée qui permet de traiter des séquences allant jusqu'à 8 192 jetons sans intégrations positionnelles traditionnelles. Le modèle utilise une architecture BERT modifiée avec 161 millions de paramètres, incorporant des unités linéaires fermées (GLU) et des techniques de normalisation de couche spécialisées. L'entraînement suit un processus en trois étapes : pré-entraînement initial sur un corpus de texte massif, suivi d'un réglage fin avec des paires de textes soigneusement sélectionnées, et enfin, un entraînement négatif dur pour améliorer la discrimination entre des contenus similaires mais sémantiquement distincts. Cette approche, combinée à des intégrations en 768 dimensions, permet au modèle de capturer des relations sémantiques nuancées tout en maintenant l'efficacité informatique.","jina-embeddings-v2-base-zh":"L'architecture du modèle combine une dorsale basée sur BERT avec un ALiBi bidirectionnel symétrique (Attention with Linear Biases), permettant un traitement efficace de longues séquences sans la limitation traditionnelle de 512 jetons. Le processus de formation suit une approche en trois phases soigneusement orchestrée : pré-formation initiale sur des données bilingues de haute qualité, suivie d'étapes de réglage primaire et secondaire. Cette stratégie de formation méthodique, associée aux 161 millions de paramètres du modèle et à sa sortie en 768 dimensions, permet d'atteindre une efficacité remarquable tout en maintenant des performances équilibrées dans les deux langues. Le mécanisme ALiBi bidirectionnel symétrique représente une innovation significative, permettant au modèle de gérer des documents d'une longueur maximale de 8 192 jetons, une capacité auparavant limitée aux solutions propriétaires.","jina-embeddings-v3":"L'architecture du modèle représente une innovation significative dans la technologie d'intégration, construite sur une base jina-XLM-RoBERTa avec 24 couches et améliorée avec des adaptateurs d'adaptation de faible rang (LoRA) spécifiques aux tâches. Les adaptateurs LoRA sont des composants de réseau neuronal spécialisés qui optimisent le modèle pour différentes tâches telles que la récupération, la classification ou le clustering sans augmenter de manière significative le nombre de paramètres - ils ajoutent moins de 3 % au total des paramètres. Le modèle intègre Matryoshka Representation Learning (MRL), permettant de réduire de manière flexible les intégrations de 1024 à 32 dimensions tout en préservant les performances. La formation impliquait un processus en trois étapes : pré-formation initiale sur du texte multilingue de 89 langues, réglage fin sur des textes appariés pour la qualité de l'intégration et formation d'adaptateur spécialisé pour l'optimisation des tâches. Le modèle prend en charge des longueurs de contexte jusqu'à 8 192 jetons via Rotary Position Embeddings (RoPE), avec une technique innovante d'ajustement de fréquence de base qui améliore les performances sur les textes courts et longs.","jina-reranker-v1-base-en":"Le modèle utilise une architecture d'attention croisée basée sur BERT qui diffère fondamentalement des approches traditionnelles basées sur l'intégration. Au lieu de comparer les intégrations de documents précalculées, il effectue des interactions dynamiques au niveau des jetons entre les requêtes et les documents, ce qui lui permet de capturer les nuances contextuelles que les simples mesures de similarité ne parviennent pas à saisir. Les 137 millions de paramètres de l'architecture sont soigneusement structurés pour permettre une compréhension sémantique approfondie tout en maintenant l'efficacité informatique. Une innovation remarquable est sa capacité à gérer des séquences allant jusqu'à 262 144 jetons, bien au-delà des limitations typiques des modèles, grâce à des techniques d'optimisation sophistiquées qui maintiennent des vitesses d'inférence rapides malgré la fenêtre de contexte accrue.","jina-reranker-v1-tiny-en":"Le modèle utilise une architecture simplifiée à quatre couches basée sur JinaBERT avec ALiBi (Attention with Linear Biases) bidirectionnel symétrique, permettant un traitement efficace des séquences longues. Son développement s'appuie sur une approche avancée de distillation des connaissances où un modèle d'enseignant plus grand et plus performant (jina-reranker-v1-base-en) guide le processus de formation, permettant au modèle plus petit d'apprendre des comportements de classement optimaux sans nécessiter de données de formation approfondies du monde réel. Cette méthodologie de formation innovante, combinée à des optimisations architecturales telles que des couches cachées réduites et des mécanismes d'attention efficaces, permet au modèle de maintenir des classements de haute qualité tout en réduisant considérablement les besoins de calcul. Le résultat est un modèle qui atteint une efficacité remarquable sans compromettre sa capacité à comprendre les relations complexes entre documents.","jina-reranker-v1-turbo-en":"Le modèle atteint son efficacité grâce à une architecture innovante à six couches qui compresse les capacités de reclassement sophistiquées de son homologue plus grand en seulement 37,8 millions de paramètres, soit une réduction spectaculaire par rapport aux 137 millions du modèle de base. Cette conception simplifiée utilise la distillation des connaissances, où le modèle de base plus grand agit comme un enseignant, entraînant la variante turbo à correspondre à son comportement tout en utilisant moins de ressources. L'architecture conserve le mécanisme d'attention croisée basé sur BERT pour les interactions au niveau des jetons entre les requêtes et les documents, mais l'optimise pour la vitesse grâce à un nombre de couches réduit et une allocation de paramètres efficace. Le modèle prend en charge des séquences allant jusqu'à 8 192 jetons, ce qui permet une analyse complète des documents tout en maintenant des vitesses d'inférence rapides grâce à des techniques d'optimisation sophistiquées.","jina-reranker-v2-base-multilingual":"Le modèle utilise une architecture cross-encoder améliorée avec Flash Attention 2, permettant une comparaison directe entre les requêtes et les documents pour une évaluation plus précise de la pertinence. Formé selon un processus en quatre étapes, le modèle établit d'abord les capacités en anglais, puis intègre progressivement les données multilingues et multilingues, avant d'être affiné avec des exemples de résultats négatifs. Cette approche de formation innovante, combinée à l'implémentation de Flash Attention 2, permet au modèle de traiter des séquences allant jusqu'à 524 288 jetons tout en maintenant une vitesse exceptionnelle. L'efficacité de l'architecture lui permet de gérer des tâches de reclassement complexes dans plusieurs langues avec un débit 6 fois supérieur à celui de son prédécesseur, tout en garantissant une évaluation précise de la pertinence grâce à une interaction directe entre la requête et le document.","reader-lm-05b":"Le modèle utilise une architecture innovante « superficielle mais large » spécialement optimisée pour les opérations de copie sélective plutôt que pour la génération de texte créatif. Construit sur une base de décodeur uniquement avec 24 couches et 896 dimensions cachées, le modèle utilise des mécanismes d'attention spécialisés avec 14 têtes de requête et 2 têtes de valeur-clé pour traiter efficacement les séquences d'entrée. Le processus de formation comportait deux étapes distinctes : d'abord avec du HTML plus court et plus simple (32 000 jetons) pour apprendre les modèles de conversion de base, puis avec du HTML complexe et réel (128 000 jetons) pour gérer les cas difficiles. Le modèle intègre la recherche contrastive pendant la formation et implémente un mécanisme de détection de répétition pour éviter les problèmes de dégénérescence tels que les boucles de jetons. Un aspect unique de son architecture est le mécanisme d'attention en anneau en zigzag, qui permet au modèle de gérer des séquences extrêmement longues jusqu'à 256 000 jetons tout en maintenant des performances stables.","reader-lm-15b":"Le modèle utilise une architecture innovante « superficielle mais large » qui remet en question les approches de mise à l'échelle traditionnelles dans la conception de modèles de langage. Au cœur de ce modèle se trouvent 28 couches de transformateurs configurées avec 12 têtes de requête et 2 têtes de valeur-clé, créant un équilibre unique qui optimise les opérations de copie sélective tout en maintenant une compréhension sémantique approfondie. L'architecture présente une taille cachée de 1536 et une taille intermédiaire de 8960, soigneusement réglées pour gérer des séquences allant jusqu'à 256 000 jetons. Le processus de formation comportait deux étapes distinctes : d'abord en se concentrant sur le HTML court et simple avec des séquences de 32 000 jetons, puis en passant au HTML long et dur avec 128 000 jetons, en implémentant l'attention en zigzag pour un traitement efficace. Cette approche, combinée à une recherche contrastive et à des mécanismes de détection de répétition spécialisés, permet au modèle d'éviter les problèmes courants tels que la dégénérescence et les boucles ennuyeuses qui affectent généralement les modèles de langage plus petits gérant des tâches de traitement de documents complexes.",title:"Méthodes"},model_comparison:"Comparaison de modèles",model_details:"Détails du modèle",model_io_graph:"Graphique d'E/S {_number}",model_name:"Nom",output_dimension:"Dimension de sortie",overview:{"ReaderLM-v2":"ReaderLM-v2 est un modèle de langage de 1,5 milliard de paramètres qui convertit le HTML brut en Markdown ou JSON, gérant jusqu'à 512 000 jetons de longueur d'entrée/sortie combinée avec prise en charge de 29 langues. Contrairement à son prédécesseur qui traitait la conversion HTML vers Markdown comme une tâche de « copie sélective », la version 2 l'aborde comme un processus de traduction, permettant une gestion supérieure des éléments complexes tels que les clôtures de code, les listes imbriquées, les tableaux et les équations LaTeX. Le modèle maintient des performances cohérentes sur différentes longueurs de contexte et introduit des capacités de génération directe de HTML vers JSON avec des schémas prédéfinis.","jina-clip-v1":"Jina CLIP v1 révolutionne l'IA multimodale en étant le premier modèle à exceller aussi bien dans les tâches de récupération de texte à texte que de texte à image. Contrairement aux modèles CLIP traditionnels qui peinent à gérer les scénarios uniquement textuels, ce modèle atteint des performances de pointe dans toutes les combinaisons de récupération tout en conservant une taille de paramètre remarquablement compacte de 223 M. Le modèle répond à un défi industriel critique en éliminant le besoin de modèles distincts pour le traitement de texte et d'image, réduisant ainsi la complexité du système et la surcharge de calcul. Pour les équipes qui créent des systèmes de recherche, des moteurs de recommandation ou des outils d'analyse de contenu, Jina CLIP v1 offre une solution unique et efficace qui gère à la fois le texte et le contenu visuel avec une précision exceptionnelle.","jina-clip-v2":"Jina CLIP v2 révolutionne l'IA multimodale en comblant le fossé entre la compréhension visuelle et textuelle dans 89 langues. Ce modèle résout les défis critiques du commerce électronique mondial, de la gestion de contenu et de la communication interculturelle en permettant une correspondance précise entre image et texte, quelles que soient les barrières linguistiques. Pour les entreprises en expansion internationale ou gérant du contenu multilingue, il élimine le besoin de modèles distincts par langue ou de pipelines de traduction complexes. Le modèle est particulièrement efficace dans les scénarios nécessitant une recherche visuelle précise au-delà des frontières linguistiques, comme la découverte de produits sur le marché mondial ou la gestion d'actifs numériques multilingues.","jina-colbert-v1-en":"Jina-ColBERT-v1-en révolutionne la recherche de texte en résolvant un défi critique dans la recherche d'informations : atteindre une grande précision sans sacrifier l'efficacité de calcul. Contrairement aux modèles traditionnels qui compressent des documents entiers en vecteurs uniques, ce modèle maintient une compréhension précise au niveau du jeton tout en ne nécessitant que 137 millions de paramètres. Pour les équipes qui créent des applications de recherche, des systèmes de recommandation ou des plateformes de découverte de contenu, Jina-ColBERT-v1-en élimine le compromis traditionnel entre la qualité de la recherche et les performances du système. Le modèle est particulièrement efficace dans les scénarios où une compréhension nuancée du texte est cruciale, comme la recherche de documentation technique, la récupération de documents universitaires ou toute application où la capture de relations sémantiques subtiles peut faire la différence entre trouver la bonne information et manquer un contenu critique.","jina-colbert-v2":"Jina-ColBERT-v2 est un modèle révolutionnaire de recherche d'informations multilingue qui résout le défi crucial d'une recherche efficace et de haute qualité dans plusieurs langues. En tant que premier modèle multilingue de type ColBERT à générer des intégrations compactes, il répond au besoin croissant de solutions de recherche multilingue évolutives et rentables dans les applications mondiales. Les organisations qui traitent du contenu multilingue, des plateformes de commerce électronique aux systèmes de gestion de contenu, peuvent exploiter ce modèle pour fournir des résultats de recherche précis dans 89 langues tout en réduisant considérablement les coûts de stockage et de calcul grâce à ses capacités innovantes de réduction des dimensions.","jina-embedding-b-en-v1":"Jina Embedding B v1 est un modèle d'intégration de texte spécialisé conçu pour transformer le texte anglais en représentations numériques de grande dimension tout en préservant le sens sémantique. Le modèle répond au besoin critique d'intégrations de texte efficaces et précises dans les environnements de production, particulièrement utile pour les organisations nécessitant un équilibre entre efficacité de calcul et qualité d'intégration. Avec ses 110 millions de paramètres générant des intégrations de 768 dimensions, il constitue une solution pratique pour les équipes mettant en œuvre des systèmes de recherche sémantique, de regroupement de documents ou de recommandation de contenu sans nécessiter de ressources de calcul importantes.","jina-embeddings-v2-base-code":"Jina Embeddings v2 Base Code relève un défi crucial dans le développement de logiciels modernes : naviguer et comprendre efficacement de grandes bases de code. Pour les équipes de développement qui ont du mal à découvrir et à documenter le code, ce modèle transforme la façon dont les développeurs interagissent avec le code en permettant la recherche en langage naturel dans 30 langages de programmation. Contrairement aux outils de recherche de code traditionnels qui s'appuient sur une correspondance de modèles exacte, ce modèle comprend la signification sémantique derrière le code, permettant aux développeurs de trouver des extraits de code pertinents à l'aide de descriptions en anglais simple. Cette capacité est particulièrement précieuse pour les équipes qui gèrent de grandes bases de code héritées, les développeurs qui intègrent de nouveaux projets ou les organisations qui cherchent à améliorer les pratiques de réutilisation et de documentation du code.","jina-embeddings-v2-base-de":"Jina Embeddings v2 Base German répond à un défi crucial dans le commerce international : combler le fossé linguistique entre les marchés allemand et anglais. Pour les entreprises allemandes qui se développent dans les territoires anglophones, où un tiers des entreprises génèrent plus de 20 % de leurs ventes mondiales, une compréhension bilingue précise est essentielle. Ce modèle transforme la façon dont les organisations gèrent le contenu multilingue en permettant une compréhension et une récupération transparentes du texte en allemand et en anglais, ce qui le rend inestimable pour les entreprises qui mettent en œuvre des systèmes de documentation internationaux, des plateformes de support client ou des solutions de gestion de contenu. Contrairement aux approches traditionnelles basées sur la traduction, ce modèle mappe directement les significations équivalentes dans les deux langues sur le même espace d'intégration, ce qui permet des opérations bilingues plus précises et plus efficaces.","jina-embeddings-v2-base-en":"Jina Embeddings v2 Base English est un modèle d'intégration de texte open source révolutionnaire qui résout le défi crucial du traitement de longs documents tout en maintenant une grande précision. Les organisations qui ont du mal à analyser des documents juridiques, des articles de recherche ou des rapports financiers volumineux trouveront ce modèle particulièrement utile. Il se distingue par la gestion de documents d'une longueur maximale de 8 192 jetons, soit 16 fois plus que les modèles traditionnels, tout en égalant les performances des solutions propriétaires d'OpenAI. Avec une taille compacte de 0,27 Go et une utilisation efficace des ressources, il offre une solution accessible aux équipes cherchant à mettre en œuvre une analyse avancée des documents sans surcharge de calcul excessive.","jina-embeddings-v2-base-es":"Jina Embeddings v2 Base Spanish est un modèle d'intégration de texte bilingue révolutionnaire qui répond au défi crucial de la recherche et de l'analyse d'informations multilingues entre le contenu espagnol et anglais. Contrairement aux modèles multilingues traditionnels qui ont souvent tendance à privilégier des langues spécifiques, ce modèle offre des performances véritablement équilibrées en espagnol et en anglais, ce qui le rend indispensable pour les organisations opérant sur les marchés hispanophones ou gérant du contenu bilingue. La caractéristique la plus remarquable du modèle est sa capacité à générer des intégrations alignées géométriquement : lorsque des textes en espagnol et en anglais expriment la même signification, leurs représentations vectorielles se regroupent naturellement dans l'espace d'intégration, ce qui permet une recherche et une analyse interlingues transparentes.","jina-embeddings-v2-base-zh":"Jina Embeddings v2 Base Chinese est le premier modèle open source à gérer de manière transparente les textes chinois et anglais avec une longueur de contexte sans précédent de 8 192 jetons. Ce modèle bilingue répond à un défi crucial dans les entreprises mondiales : la nécessité d'un traitement précis et détaillé des documents en chinois et en anglais. Contrairement aux modèles traditionnels qui ont du mal à assurer la compréhension interlinguistique ou qui nécessitent des modèles distincts pour chaque langue, ce modèle mappe les significations équivalentes dans les deux langues sur le même espace d'intégration, ce qui le rend inestimable pour les organisations qui se développent à l'échelle mondiale ou qui gèrent du contenu multilingue.","jina-embeddings-v3":"Jina Embeddings v3 est un modèle d'intégration de texte multilingue révolutionnaire qui transforme la façon dont les organisations gèrent la compréhension et la récupération de texte dans plusieurs langues. Fondamentalement, il résout le défi crucial de maintenir des performances élevées dans plusieurs langues et tâches tout en gardant les exigences de calcul gérables. Le modèle excelle particulièrement dans les environnements de production où l'efficacité est importante : il atteint des performances de pointe avec seulement 570 millions de paramètres, ce qui le rend accessible aux équipes qui ne peuvent pas se permettre la surcharge de calcul de modèles plus volumineux. Les organisations qui ont besoin de créer des systèmes de recherche multilingues évolutifs ou d'analyser du contenu au-delà des barrières linguistiques trouveront ce modèle particulièrement utile.","jina-reranker-v1-base-en":"Jina Reranker v1 Base English révolutionne l'affinement des résultats de recherche en s'attaquant à une limitation critique des systèmes de recherche vectorielle traditionnels : l'incapacité à capturer les relations nuancées entre les requêtes et les documents. Bien que la recherche vectorielle avec similarité cosinus fournisse des résultats initiaux rapides, elle passe souvent à côté de signaux de pertinence subtils que les utilisateurs humains comprennent intuitivement. Ce reranker comble cette lacune en effectuant une analyse sophistiquée au niveau des jetons des requêtes et des documents, offrant une amélioration remarquable de 20 % de la précision de la recherche. Pour les organisations qui ont des difficultés avec la précision de la recherche ou qui mettent en œuvre des systèmes RAG, ce modèle offre une solution puissante qui améliore considérablement la qualité des résultats sans nécessiter une refonte complète de l'infrastructure de recherche existante.","jina-reranker-v1-tiny-en":"Jina Reranker v1 Tiny English représente une avancée majeure en matière d'affinement de recherche efficace, conçu spécifiquement pour les organisations nécessitant un reranking hautes performances dans des environnements aux ressources limitées. Ce modèle relève le défi crucial de maintenir la qualité de la recherche tout en réduisant considérablement les frais généraux de calcul et les coûts de déploiement. Avec seulement 33 millions de paramètres (une fraction de la taille des rerankers classiques), il offre des performances remarquablement compétitives grâce à des techniques innovantes de distillation des connaissances. La caractéristique la plus surprenante du modèle est sa capacité à traiter les documents près de cinq fois plus rapidement que les modèles de base tout en conservant plus de 92 % de leur précision, ce qui rend l'affinement de la recherche de niveau entreprise accessible aux applications où les ressources informatiques sont limitées.","jina-reranker-v1-turbo-en":"Jina Reranker v1 Turbo English répond à un défi crucial dans les systèmes de recherche de production : le compromis entre la qualité des résultats et l'efficacité informatique. Alors que les rerankers traditionnels offrent une précision de recherche améliorée, leurs exigences de calcul les rendent souvent peu pratiques pour les applications en temps réel. Ce modèle brise cette barrière en offrant 95 % de la précision du modèle de base tout en traitant les documents trois fois plus rapidement et en utilisant 75 % de mémoire en moins. Pour les organisations confrontées à des problèmes de latence de recherche ou de coûts de calcul, ce modèle offre une solution convaincante qui maintient un raffinement de recherche de haute qualité tout en réduisant considérablement les besoins en infrastructure et les coûts opérationnels.","jina-reranker-v2-base-multilingual":"Jina Reranker v2 Base Multilingual est un modèle de codeur croisé conçu pour améliorer la précision de la recherche au-delà des barrières linguistiques et des types de données. Ce reranker relève le défi crucial de la récupération précise d'informations dans des environnements multilingues, particulièrement utile pour les entreprises mondiales qui doivent affiner les résultats de recherche dans différentes langues et types de contenu. Avec la prise en charge de plus de 100 langues et des capacités uniques en matière d'appel de fonctions et de recherche de code, il constitue une solution unifiée pour les équipes qui ont besoin d'affiner la recherche avec précision dans le contenu international, la documentation API et les bases de code multilingues. La conception compacte de 278 millions de paramètres du modèle le rend particulièrement attrayant pour les organisations qui cherchent à équilibrer hautes performances et efficacité des ressources.","reader-lm-05b":"Reader LM 0.5B est un modèle de langage spécialisé conçu pour résoudre le défi complexe de la conversion de documents HTML en texte Markdown propre et structuré. Ce modèle répond à un besoin essentiel des pipelines de traitement de données modernes : transformer efficacement un contenu Web désordonné en un format idéal pour les LLM et les systèmes de documentation. Contrairement aux modèles de langage à usage général qui nécessitent des ressources de calcul massives, Reader LM 0.5B permet un traitement HTML de qualité professionnelle avec seulement 494 millions de paramètres, ce qui le rend accessible aux équipes disposant de ressources informatiques limitées. Les organisations qui traitent du contenu Web, de l'automatisation de la documentation ou de la création d'applications basées sur LLM trouveront ce modèle particulièrement utile pour rationaliser leurs flux de travail de préparation de contenu.","reader-lm-15b":"Reader LM 1.5B représente une avancée majeure dans le traitement efficace des documents, en relevant le défi crucial de la conversion de contenus Web complexes en formats propres et structurés. Ce modèle de langage spécialisé s'attaque à un problème fondamental des pipelines d'IA modernes : la nécessité de traiter et de nettoyer efficacement le contenu HTML pour les tâches en aval sans s'appuyer sur des systèmes fragiles basés sur des règles ou sur des modèles de langage volumineux et gourmands en ressources. Ce modèle est particulièrement remarquable dans sa capacité à surpasser les modèles 50 fois plus grands tout en conservant une empreinte de paramètres étonnamment compacte de 1,54 milliard. Les organisations qui traitent des contenus Web à grande échelle, l'automatisation de la documentation ou les systèmes de gestion de contenu trouveront ce modèle particulièrement utile pour sa capacité à gérer des documents extrêmement longs tout en offrant une précision supérieure dans la conversion HTML en Markdown.",title:"Aperçu"},parameter_size:"Paramètres",performance:{"ReaderLM-v2":"Dans des tests comparatifs complets, ReaderLM-v2 surpasse des modèles plus grands comme Qwen2.5-32B-Instruct et Gemini2-flash-expr sur les tâches HTML vers Markdown. Pour l'extraction du contenu principal, il atteint un ROUGE-L de 0,84, un Jaro-Winkler de 0,82 et une distance de Levenshtein nettement inférieure (0,22) par rapport à ses concurrents. Dans les tâches HTML vers JSON, il maintient des performances compétitives avec des scores F1 de 0,81 et un taux de réussite de 98 %. Le modèle traite à 67 jetons/s en entrée et 36 jetons/s en sortie sur un GPU T4, avec des problèmes de dégénérescence considérablement réduits grâce à un entraînement par perte contrastive.","jina-clip-v1":"Jina CLIP v1 démontre des améliorations remarquables par rapport au CLIP original d'OpenAI dans tous les tests de performance. En recherche de texte uniquement, il atteint une augmentation de performance de 165 % avec un score de 0,429 par rapport au 0,162 de CLIP. Pour les tâches liées aux images, il montre des améliorations constantes : 2 % de mieux dans la recherche de texte vers image (0,899), 6 % dans la recherche d'image vers texte (0,803) et 12 % dans la recherche d'image vers image (0,916). Le modèle brille particulièrement dans les tâches de classification visuelle à zéro coup, en catégorisant avec succès les images sans formation préalable sur des domaines spécifiques. Lorsqu'il est évalué sur des tests de performance standard comme MTEB pour la recherche de texte, CIFAR-100 pour les tâches d'image et Flickr8k/30k et MSCOCO Captions pour les performances intermodales, il surpasse systématiquement les modèles spécialisés à modalité unique tout en maintenant des performances compétitives dans les tâches intermodales.","jina-clip-v2":"Le modèle atteint des performances de pointe avec une précision de 98,0 % sur les tâches de récupération d'image en texte Flickr30k, surpassant à la fois son prédécesseur et NLLB-CLIP-SigLIP. Dans les scénarios multilingues, il démontre jusqu'à 4 % d'amélioration par rapport à NLLB-CLIP-SigLIP dans les tâches de récupération d'images multilingues, bien qu'il ait moins de paramètres que son plus grand concurrent. Le modèle conserve de solides performances même lorsque les intégrations sont compressées - la réduction des dimensions de 75 % préserve toujours plus de 99 % des performances sur les tâches de texte, d'image et intermodales. Sur les benchmarks MTEB multilingues complets, il atteint 69,86 % sur la récupération et 67,77 % sur les tâches de similarité sémantique, ce qui le place en compétition avec les modèles d'intégration de texte spécialisés.","jina-colbert-v1-en":"Jina-ColBERT-v1-en démontre des améliorations remarquables par rapport aux modèles de base dans divers benchmarks. Sur la collection de données BEIR, il atteint des performances supérieures dans plusieurs catégories : 49,4 % sur Arguana (contre 46,5 % pour ColBERTv2), 79,5 % sur FEVER (contre 78,8 %) et 75,0 % sur TREC-COVID (contre 72,6 %). Plus impressionnant encore, il montre une amélioration spectaculaire par rapport au benchmark LoCo pour la compréhension du contexte long, avec un score de 83,7 % par rapport aux 74,3 % de ColBERTv2. Le modèle excelle particulièrement dans les scénarios nécessitant une compréhension sémantique détaillée, surpassant les modèles d'intégration traditionnels tout en maintenant l'efficacité de calcul grâce à son approche innovante d'interaction tardive. Ces améliorations sont obtenues tout en maintenant le nombre de paramètres du modèle à un modeste 137 M, ce qui le rend à la fois puissant et pratique pour les déploiements de production.","jina-colbert-v2":"Lors de tests en conditions réelles, Jina-ColBERT-v2 démontre des capacités exceptionnelles sur plusieurs tests. Il obtient une amélioration de 6,5 % par rapport au ColBERT-v2 original sur les tâches en anglais, avec un score moyen de 0,521 sur 14 tests BEIR. Plus impressionnant encore, il surpasse les méthodes de récupération traditionnelles basées sur BM25 dans toutes les langues testées sur les tests MIRACL, montrant une force particulière dans les scénarios multilingues. Le modèle maintient ces performances élevées même en utilisant des dimensions d'intégration réduites - le passage de 128 à 64 dimensions entraîne une baisse de performance de seulement 1,5 % tout en réduisant de moitié les besoins de stockage. Cela se traduit par des économies de coûts significatives en production : par exemple, le stockage de 100 millions de documents avec des vecteurs à 64 dimensions coûte 659,62 $ par mois sur AWS, contre 1 319,24 $ pour 128 dimensions.","jina-embedding-b-en-v1":"Lors d'évaluations en conditions réelles, Jina Embedding B v1 démontre des capacités impressionnantes, notamment dans les tâches de similarité textuelle sémantique. Le modèle atteint des performances de pointe sur STS12 avec un score de 0,751, surpassant les modèles établis comme all-mpnet-base-v2 et all-minilm-l6-v2. Il affiche de solides performances dans divers tests tout en maintenant des temps d'inférence efficaces. Cependant, les utilisateurs doivent noter que le modèle est spécifiquement optimisé pour le contenu en anglais et peut ne pas fonctionner de manière optimale sur des tâches multilingues ou spécifiques au code. Le modèle a depuis été remplacé par jina-embeddings-v2-base-en et jina-embeddings-v3, qui offrent des performances améliorées dans une gamme plus large de cas d'utilisation.","jina-embeddings-v2-base-code":"Lors de tests en conditions réelles, Jina Embeddings v2 Base Code démontre des capacités exceptionnelles, en tête du peloton dans neuf des quinze tests cruciaux de CodeNetSearch. Comparé aux modèles de géants du secteur comme Microsoft et Salesforce, il atteint des performances supérieures tout en conservant une empreinte plus efficace. Le modèle excelle particulièrement dans la compréhension de code interlinguistique, en faisant correspondre avec succès des extraits de code fonctionnellement équivalents dans différents langages de programmation. Sa fenêtre de contexte de 8 192 jetons s'avère particulièrement précieuse pour les fonctions volumineuses et les fichiers de code complexes, surpassant considérablement les modèles traditionnels qui ne gèrent généralement que quelques centaines de jetons. L'efficacité du modèle est évidente dans sa taille compacte de 307 Mo (non quantifiée), permettant une inférence rapide tout en maintenant une grande précision dans les tâches de recherche et de similarité de code.","jina-embeddings-v2-base-de":"Lors de tests en conditions réelles, Jina Embeddings v2 Base German fait preuve d'une efficacité et d'une précision exceptionnelles, notamment dans les tâches de recherche multilingue. Le modèle surpasse le modèle de base E5 de Microsoft tout en étant moins d'un tiers de sa taille, et égale les performances du modèle E5 large bien qu'il soit sept fois plus petit. Dans les tests de référence clés, notamment WikiCLIR pour la recherche de l'anglais vers l'allemand, STS17 et STS22 pour la compréhension bidirectionnelle des langues et BUCC pour l'alignement précis du texte bilingue, le modèle démontre systématiquement des capacités supérieures. Sa taille compacte de 322 Mo permet un déploiement sur du matériel standard tout en maintenant des performances de pointe, ce qui le rend particulièrement efficace pour les environnements de production où les ressources de calcul sont un facteur important.","jina-embeddings-v2-base-en":"Lors de tests en conditions réelles, Jina Embeddings v2 Base English démontre des capacités exceptionnelles dans plusieurs tests de performance. Il surpasse le text-embedding-ada-002 d'OpenAI dans plusieurs indicateurs clés : classification (73,45 % contre 70,93 %), reclassement (85,38 % contre 84,89 %), récupération (56,98 % contre 56,32 %) et résumé (31,6 % contre 30,8 %). Ces chiffres se traduisent par des avantages pratiques dans des tâches telles que la classification de documents, où le modèle montre une capacité supérieure à catégoriser des textes complexes, et dans les applications de recherche, où il comprend mieux les requêtes des utilisateurs et trouve des documents pertinents. Cependant, les utilisateurs doivent noter que les performances peuvent varier lorsqu'il s'agit de contenu hautement spécialisé dans un domaine spécifique non représenté dans les données de formation.","jina-embeddings-v2-base-es":"Dans des évaluations comparatives complètes, le modèle démontre des capacités exceptionnelles, notamment dans les tâches de recherche interlinguistiques où il surpasse des modèles multilingues nettement plus volumineux comme E5 et BGE-M3, bien qu'il ne représente que 15 à 30 % de leur taille. Le modèle atteint des performances supérieures dans les tâches de recherche et de clustering, montrant une force particulière dans la mise en correspondance de contenus sémantiquement équivalents entre les langues. Lorsqu'il est testé sur le benchmark MTEB, il présente des performances robustes dans diverses tâches, notamment la classification, le clustering et la similarité sémantique. La fenêtre de contexte étendue de 8 192 jetons s'avère particulièrement précieuse pour le traitement de documents longs, affichant des performances constantes même avec des documents couvrant plusieurs pages - une capacité qui manque à la plupart des modèles concurrents.","jina-embeddings-v2-base-zh":"Dans les tests de référence du classement MTEB chinois (C-MTEB), le modèle affiche des performances exceptionnelles parmi les modèles de moins de 0,5 Go, excellant particulièrement dans les tâches en chinois. Il surpasse considérablement le text-embedding-ada-002 d'OpenAI dans les applications spécifiques au chinois tout en maintenant des performances compétitives dans les tâches en anglais. Une amélioration notable de cette version est la distribution affinée des scores de similarité, qui résout les problèmes d'inflation des scores présents dans la version préliminaire. Le modèle fournit désormais des scores de similarité plus distincts et logiques, garantissant une représentation plus précise des relations sémantiques entre les textes. Cette amélioration est particulièrement évidente dans les tests comparatifs, où le modèle montre une discrimination supérieure entre les contenus liés et non liés dans les deux langues.","jina-embeddings-v3":"Le modèle démontre un rapport efficacité/performance exceptionnel dans les tests en conditions réelles, surpassant à la fois les alternatives open source et les solutions propriétaires d'OpenAI et de Cohere sur les tâches en anglais tout en excellant dans les scénarios multilingues. Plus surprenant encore, il obtient de meilleurs résultats que e5-mistral-7b-instruct, qui possède 12 fois plus de paramètres, ce qui met en évidence son efficacité remarquable. Dans les évaluations de référence MTEB, il obtient un score moyen de 65,52 sur toutes les tâches, avec des performances particulièrement élevées en matière de précision de classification (82,58) et de similarité de phrases (85,80). Le modèle maintient des performances constantes dans toutes les langues, obtenant un score de 64,44 sur les tâches multilingues. Lorsqu'il utilise MRL pour la réduction des dimensions, il conserve de bonnes performances même à des dimensions inférieures - par exemple, en maintenant 92 % de ses performances de récupération à 64 dimensions par rapport aux 1024 dimensions complètes.","jina-reranker-v1-base-en":"Dans des tests comparatifs complets, le modèle démontre des améliorations exceptionnelles sur les indicateurs clés, atteignant une augmentation de 8 % du taux de réussite et une augmentation de 33 % du rang réciproque moyen par rapport à la recherche vectorielle de base. Sur le test de référence BEIR, il obtient un score moyen de 0,5588, surpassant les autres rerankers de BGE (0,5032), BCE (0,4969) et Cohere (0,5141). Ses performances sont particulièrement impressionnantes sur le test de référence LoCo, où il obtient un score moyen de 0,873, nettement devant ses concurrents en matière de compréhension de la cohérence locale et de classement sensible au contexte. Le modèle montre une force particulière dans l'évaluation du contenu technique, obtenant des scores de 0,996 sur les tâches qasper_abstract et de 0,962 sur l'analyse des rapports gouvernementaux, bien qu'il affiche des performances relativement inférieures (0,466) sur les tâches de résumé des réunions.","jina-reranker-v1-tiny-en":"Dans des évaluations comparatives complètes, le modèle démontre des capacités exceptionnelles qui remettent en question le compromis conventionnel entre taille et performances. Sur le benchmark BEIR, il atteint un score NDCG-10 de 48,54, conservant 92,5 % des performances du modèle de base tout en étant seulement un quart de sa taille. Plus impressionnant encore, dans les benchmarks LlamaIndex RAG, il maintient un taux de réussite de 83,16 %, égalant presque les modèles plus grands tout en traitant les documents beaucoup plus rapidement. Le modèle excelle particulièrement en termes de débit, traitant les documents presque cinq fois plus rapidement que le modèle de base tout en utilisant 13 % de mémoire en moins que la variante turbo. Ces mesures se traduisent par des performances réelles qui rivalisent ou dépassent des modèles beaucoup plus grands comme mxbai-rerank-base-v1 (184 M de paramètres) et bge-reranker-base (278 M de paramètres).","jina-reranker-v1-turbo-en":"Dans des tests comparatifs complets, la variante turbo fait preuve d'une efficacité remarquable sans compromis significatifs en termes de précision. Sur le test de référence BEIR, elle atteint un score NDCC-10 de 49,60, conservant 95 % des performances du modèle de base (52,45) tout en surpassant de nombreux concurrents plus importants comme bge-reranker-base (47,89, 278 millions de paramètres). Dans les applications RAG, elle maintient un taux de réussite impressionnant de 83,51 % et un MRR de 0,6498, ce qui montre une force particulière dans les tâches de récupération pratiques. Les améliorations de vitesse du modèle sont encore plus frappantes : il traite les documents trois fois plus rapidement que le modèle de base, avec une mise à l'échelle du débit presque linéaire avec un nombre de paramètres réduit. Cependant, les utilisateurs doivent noter des performances légèrement inférieures sur les tâches de classement extrêmement nuancées où le nombre total de paramètres de modèles plus volumineux offre des avantages marginaux.","jina-reranker-v2-base-multilingual":"Lors d'évaluations en conditions réelles, le modèle démontre des capacités exceptionnelles dans divers tests de performance. Il atteint des performances de pointe dans le classement AirBench pour les systèmes RAG et affiche de bons résultats dans les tâches multilingues, notamment dans l'ensemble de données MKQA couvrant 26 langues. Le modèle excelle particulièrement dans les tâches de données structurées, obtenant des scores de rappel élevés à la fois dans l'appel de fonctions (test de performance ToolBench) et dans la correspondance de schémas SQL (test de performance NSText2SQL). Plus impressionnant encore, il fournit ces résultats tout en traitant les documents 15 fois plus rapidement que des modèles comparables comme bge-reranker-v2-m3, ce qui le rend pratique pour les applications en temps réel. Cependant, les utilisateurs doivent noter que des performances optimales nécessitent un GPU compatible CUDA pour l'inférence.","reader-lm-05b":"Lors de tests en conditions réelles, Reader LM 0.5B affiche des ratios efficacité/performance impressionnants sur plusieurs indicateurs. Le modèle atteint un score ROUGE-L de 0,56, indiquant une forte préservation du contenu, et maintient un faible taux d'erreur de jeton de 0,34, montrant une hallucination minimale. Dans les évaluations qualitatives sur 22 sources HTML diverses, notamment des articles de presse, des billets de blog et des pages de commerce électronique en plusieurs langues, il montre une force particulière dans la préservation de la structure et l'utilisation de la syntaxe Markdown. Le modèle excelle dans la gestion de pages Web modernes complexes où les scripts et les CSS en ligne peuvent s'étendre à des centaines de milliers de jetons - un scénario où les approches traditionnelles basées sur des règles échouent souvent. Cependant, il est important de noter que même si le modèle fonctionne exceptionnellement bien sur les tâches simples de conversion HTML en Markdown, il peut nécessiter un traitement supplémentaire pour les pages très dynamiques ou très chargées en JavaScript.","reader-lm-15b":"Dans des évaluations comparatives complètes, Reader LM 1.5B démontre des capacités exceptionnelles qui défient les normes du secteur. Le modèle atteint un score ROUGE-L de 0,72 et un taux d'erreur de jeton de 0,19, surpassant considérablement les modèles plus grands comme GPT-4 (0,43 ROUGE-L, 0,50 TER) et Gemini-1.5-Pro (0,42 ROUGE-L, 0,48 TER) dans les tâches de conversion HTML vers Markdown. Ses performances brillent particulièrement dans les évaluations qualitatives sur quatre dimensions clés : l'extraction d'en-tête, l'extraction de contenu principal, la préservation de la structure riche et l'utilisation de la syntaxe Markdown. Le modèle maintient systématiquement une grande précision sur divers types de documents, des articles de presse et des billets de blog aux pages de destination et aux messages de forum, dans plusieurs langues, dont l'anglais, l'allemand, le japonais et le chinois. Ces performances sont obtenues lors du traitement de documents d'une longueur maximale de 256 000 jetons, éliminant ainsi le besoin d'opérations de découpage coûteuses qui sont généralement requises avec des modèles plus volumineux.",title:"Performance"},performance_metrics:"Indicateurs de performance",publications:"Publications",tags:"Mots clés",token_length:"Longueur du jeton d'entrée",usage_requirements:"Utilisation et exigences",using_model:"Disponible via"},select_model:"Sélectionnez un modèle dans la liste pour afficher les détails",sort:{direction:{asc:"Ascendant",desc:"Descendant",name:"Direction"},label:"Trier",name:"Nom",parameter_size:"Taille",release_date:"Date"},title:"{_modelName} - Recherche de modèles de fondation",warnings:{deprecated:"Ce modèle est obsolète pour les modèles plus récents."}},se={back_to_newsroom:"Retour à la salle de presse",categories:"Catégories",copy_link:"Copiez le lien vers cette section",in_this_article:"Dans cet article",learn_more:"Apprendre encore plus",news_not_found:"Article non trouvé",redirect_to_news:"Redirection vers la rédaction dans 5 secondes..."},ne={academic:"Académique",academic_research:"Publications académiques",author:"Filtrer par auteur",description:"Lisez les dernières nouvelles et mises à jour de Jina AI.",description1:"Créer des innovations en matière d'IA, un mot à la fois.",engineering_group:"Groupe d'ingénierie",engineering_group_date:"31 mai 2021",minutes_read:"minutes lues",most_recent_articles:"Articles les plus récents",news_description:"Pour Jina 2.0, nous avons écouté la communauté. Vraiment, profondément écouté. « Quels sont vos points douloureux ? » nous avons demandé, anticipant avec impatience des commentaires précieux",news_title:"Search All the Things : nous organisons un concours MEME pour Jina 2.0",photos:"Photos",product:"Filtrer par produit",search:"Rechercher par titre",tech_blog:"Blog technique",title:"Rédaction",top_stories:"Meilleures histoires"},te=`🎉 Notre premier livre, "Neural Search — From Prototype to Production with Jina" est officiellement sorti aujourd'hui !`,ie={description:"Une occasion exclusive d'avoir une vue d'initié sur Jina AI.",engage:"Nous encourageons fortement un dialogue interactif tout au long de la journée. L'échange de pensées et de perspectives est inestimable pour nous. Les collaborations potentielles issues de ces discussions pourraient contribuer de manière significative à un avenir plus intégré et innovant.",engage_title:"Engagez-vous avec nous",experience:"Nous avons organisé une visite immersive de trois heures pour nos invités, disponible en allemand, anglais, français, espagnol, chinois et russe. La visite couvre un examen approfondi de nos avancées en matière d'IA multimodale, notre point de vue sur le paysage de l'IA, suivi d'un examen détaillé de projets spécifiques. Nous terminerons par une discussion de groupe pour faciliter l'échange d'idées et de points de vue. Une option déjeuner est également disponible sur demande.",experience_title:"Le voyage d'un initié",group_size:"Estimation du nombre de visiteurs",impact:"Comprenez comment nos contributions à la communauté open-source et notre travail dans la technologie d'IA multimodale font de Jina AI un acteur influent de l'innovation en IA. Nous visons à jouer un rôle important dans les processus décisionnels, en veillant à ce que les progrès de la technologie de l'IA profitent à tous.",impact_title:"Incidence et influence",introduction:"Jina AI est ravie d'ouvrir ses portes à des entités et des organisations estimées intéressées par les progrès et l'avenir de l'intelligence artificielle. Nous offrons cette opportunité exclusive aux acteurs politiques, aux ONG, aux OBNL et aux secteurs de l'investissement d'avoir une vue d'initié de nos opérations et de nos visions ici à notre siège de Berlin.",motivation_min_length_v1:"Veuillez fournir une motivation plus détaillée.",motivation_placeholder_v2:"Partager vos motivations nous aidera à améliorer votre expérience.",motivation_to_attend_v2:"Pourquoi êtes-vous intéressé par notre journée portes ouvertes ?",one_hour:"1 heure",organization:"Organisation",organization_website:"Site Web de l'organisation",organization_website_placeholder:"URL de la page d'accueil ou du profil LinkedIn de votre organisation",preferred_date:"Date de préférence",preferred_language:"Langue préférée de la visite",preferred_products:"Quels produits vous intéressent ?",subtitle:"Un aperçu de l'avenir de l'IA multimodale",title:"Journée portes ouvertes",tutor_subtitle:"Une visite de trois heures méticuleusement organisée, vous rapprochant du cœur du travail révolutionnaire de Jina AI dans la technologie d'IA multimodale.",tutor_title:"Une plongée profonde exclusive dans",vision:"Rejoignez-nous pour un aperçu complet du paysage de l'IA tel que nous le voyons. Notre discussion se concentrera sur le potentiel des grands modèles de langage, de l'IA multimodale et de l'impact de la technologie open source pour façonner l'avenir de l'innovation mondiale.",vision_title:"Notre vision pour l'avenir"},re={answer1:"Nous proposons des visites en allemand, anglais, français, espagnol, chinois et russe.",answer2:"La visite dure généralement environ trois heures.",answer3:"Le déjeuner est facultatif et peut être organisé sur demande.",answer4:"Notre journée portes ouvertes est principalement conçue pour les groupes professionnels, tels que les politiciens, les ONG, les OBNL et les investisseurs. Cependant, nous faisons parfois des exceptions en fonction du profil de l'individu.",answer5:"Nous pouvons accueillir une variété de tailles de groupe. Veuillez indiquer la taille de votre groupe dans le formulaire d'inscription, et nous confirmerons les détails avec vous.",answer6:"Il y a une section dans le formulaire d'inscription où vous pouvez spécifier vos domaines d'intérêt ou toute demande spéciale. Nous ferons de notre mieux pour adapter la visite en fonction de vos besoins.",answer7:"Pour le moment, nous proposons uniquement des visites à notre siège social de Berlin situé à Kreuzberg. Nos bureaux de Pékin et de Shenzhen ne sont actuellement pas ouverts aux visites.",question1:"Quelles langues proposez-vous pour la visite ?",question2:"Quelle est la durée de la tournée ?",question3:"Le déjeuner est-il fourni ?",question4:"Les particuliers peuvent-ils s'inscrire à la journée portes ouvertes ?",question5:"De combien de personnes un groupe peut-il être composé pour la journée portes ouvertes ?",question6:"Comment puis-je spécifier les zones d'intérêt pour la visite ?",question7:"Des visites sont-elles disponibles dans vos bureaux de Pékin ou de Shenzhen ?"},ae={description:"Un cloud open source de grands modèles multimodaux servant de framework"},oe={commercial_licence:{chip_label:"Exclusif pour les petites entreprises",company_size_note:"Exclusif aux entreprises de moins de 50 employés ou de 500 000 $ de chiffre d'affaires",cta_button:"Commencer",download_title:"Télécharger la licence commerciale",feature_api_desc:"Tester avant achat",feature_api_title:"Accès gratuit aux tests API",feature_consulting:"Deux heures de consultation avec nos experts modèles",feature_consulting_desc:"Deux (2) heures de services de consultation technique par période de licence.",feature_future_support:"Accès aux futurs modèles CC BY-NC sans autorisation",feature_future_support_desc:"Tout nouveau modèle publié par le Concédant sous CC-BY-NC-4.0 pendant la période de licence.",feature_models:"Utilisation commerciale illimitée de nos modèles CC BY-NC",feature_models_desc:"Utiliser les modèles à des fins commerciales, y compris pour une utilisation interne ou une intégration dans des applications destinées aux clients.",price_amount:"1 000 $",price_period:"/ quart",read_the_terms:"Examen des conditions de licence",read_the_terms_btn:"Termes",read_the_terms_desc:"Examinez les droits et les limitations de la licence commerciale avant l'achat",subtitle:"Tous les modèles dont vous avez besoin pour une meilleure recherche",test_before_purchase:"Essayez avant d'acheter",test_before_purchase_desc:"Obtenez 1 million de jetons API gratuits ou utilisez notre modèle Hugging Face pour valider les performances",title:"Licence d'équipe",try_api:"Essayez d'abord l'API"},free_hour_consult:"Consultation gratuite d'une heure",free_hour_consult_description:"Une heure de consultation gratuite avec nos équipes produit et ingénierie pour discuter des meilleures pratiques pour votre cas d'utilisation",full_commercial:"Utilisation commerciale sans restriction",full_commercial_description:"Vous pouvez utiliser l'API à des fins commerciales sans aucune restriction.",higher_limit:"Limite de taux beaucoup plus élevée",higher_limit_description:"Obtenez jusqu'à 1000 RPM pour r.jina.ai et 100 RPM pour s.jina.ai ; plus de détails dans la section limite de débit.",key_manager:"Gestion de base des clés",key_manager_description:"Gérez plusieurs clés API dans un seul compte, suivez l'historique d'utilisation et rechargez les jetons.",no_commercial:"Utilisation non commerciale uniquement (CC-BY-NC)",no_commercial_description:"Vous ne pouvez utiliser l'API qu'à des fins non commerciales. Pour une utilisation commerciale, veuillez recharger votre clé API.",on_prem:"Avec une licence commerciale pour une utilisation sur site",on_prem_explain:"Achetez une licence commerciale pour utiliser nos modèles sur site.",premium_key:"Clé Premium avec des limites de débit beaucoup plus élevées",premium_key_description:"Bénéficiez de limites de débit beaucoup plus élevées et d'un accès à des fonctionnalités premium, consultez le tableau des limites de débit pour plus de détails.",premium_key_manager:"Gestion avancée des clés",premium_key_manager_description:"Fonctionnalités de base et avancées telles que le rappel automatique, la révocation et le transfert de jetons.",priority_support:"Assistance technique prioritaire",priority_support_description:"Réponse par e-mail garantie sur les problèmes et incidents techniques dans les 24 heures.",secured_by_stripe:"Paiement sécurisé via Stripe",standard_key:"Clé standard",standard_key_description:"Accès à tous les produits API de Jina Search Foundation avec une limite de débit standard.",via_api:"Avec l'API Jina Search Foundation",via_api_explain:"Le moyen le plus simple d'accéder à tous nos produits. Rechargez vos jetons au fur et à mesure."},le="Alimenté par",ue="Imprimer",de={archived:"Archivé",cloud_native:"Nuage natif",core:"Cœur",data_structure:"Structure de données",embedding_serving:"Intégration du service",embedding_tuning:"Intégration du réglage",graduated:"Diplômé",incubating:"Incubation",kubernetes:"Kubernetes",large_size_model:"Modèle grande taille",linux_foundation:"Fondation Linux",llm1:"LLMOps",mid_size_model:"Modèle de taille moyenne",model_serving:"Modèle de service",model_tuning:"Réglage du modèle",observability:"Observabilité",orchestration:"Orchestration",prompt_serving:"Service rapide",prompt_tuning:"Réglage rapide",rag1:"CHIFFON",sandbox:"bac à sable",small_size_model:"Modèle de petite taille",vector_database:"Base de données vectorielle",vector_store:"Magasin de vecteurs"},ce={description:"Premier outil pour une ingénierie rapide",image_model:"Modèles d'images",intro:"Premier outil pour une ingénierie rapide",intro1:"Le premier outil pour une ingénierie rapide",optimized:"Votre tâche est d'être mon partenaire de brainstorming et de fournir des idées créatives et des suggestions pour un sujet ou un problème donné. Votre réponse doit inclure des idées originales, uniques et pertinentes qui pourraient aider à résoudre le problème ou à approfondir le sujet de manière intéressante. Veuillez noter que votre réponse doit également tenir compte des exigences ou contraintes spécifiques de la tâche.",optimized_title:"Invite optimisée",original:"Votre rôle est d'être mon partenaire de brainstorming.",original_title:"Invite d'origine",text_model:"Modèles de texte"},pe={features:[{description:"Basculez facilement entre la génération de contenu et l'optimisation rapide, faites passer la qualité de votre contenu au niveau supérieur.",name:"Assistant",title:"Dose quotidienne de productivité."},{description:"Vous ne savez pas comment rédiger une instruction efficace ? Mettez simplement votre idée, en un clic, obtenez une meilleure instruction.",name:"Optimisation rapide",title:"De meilleurs intrants, de meilleurs résultats"},{description:"Comprenez l'ambiance de chaque modèle d'IA en comparant leur sortie de la même invite.",name:"Comparez les modèles",title:"Comparaison des modèles côte à côte."},{description:"Peut-être le moyen le plus simple de déployer vos invites en tant qu'API pour l'intégration.",name:"Déployer des invites",title:"Pas d'opérations, déployez simplement."},{description:"Personnalisez vos propres agents LLM et démarrez une simulation multi-agents. Découvrez comment ils collaborent ou s'affrontent dans un environnement virtuel pour atteindre l'objectif.",name:"Multi-agent",title:"Découvrez comment les agents collaborent"}],get_started:"Commencez avec PromptPerfect"},me={api_key:"Clé API rechargée",free_key:"Clé API gratuite",generation:"Votre clé API est prête !",generation_caption:"Votre clé API a été générée à {_purchasedTime} et est prête à être utilisée !",success:"Merci pour votre achat!",success_caption:"Votre commande a été finalisée à {_purchasedTime}. Votre clé API a été rechargée et est prête à être utilisée !"},ge="Achetez maintenant",ve={batch_explain:"Cette API prend en charge les opérations par lots, autorisant jusqu'à 512 documents par requête, chaque document contenant jusqu'à 8192 jetons. L'exploitation intelligente des opérations par lots peut réduire considérablement le nombre de requêtes et améliorer les performances.",classifier:"Entraîner un classificateur à l'aide d'exemples étiquetés",classifier_few_shot:"Classer les entrées à l'aide d'un classificateur à quelques coups entraîné",classifier_few_shot_token_counting:"Jetons comptés comme : input_tokens",classifier_latency:"Le temps de réponse varie en fonction de la taille de l'entrée",classifier_token_counting:"Les jetons sont comptés comme suit : input_tokens × num_iters",classifier_zero_shot:"Classer les entrées à l'aide de la classification à coup zéro",classifier_zero_shot_token_counting:"Jetons comptés comme : input_tokens + label_tokens",deepsearch:"Raisonner, rechercher et itérer pour trouver la meilleure réponse",depends:"dépend de la taille de l'entrée",description:"Description",embeddings:"Convertir du texte/des images en vecteurs de longueur fixe",endpoint:"Point de terminaison de l'API",explain:"Les limites de débit sont suivies de deux manières : <b>RPM</b> (requêtes par minute) et <b>TPM</b> (jetons par minute). Les limites sont appliquées par IP/clé API et peuvent être atteintes en fonction du seuil (RPM ou TPM) atteint en premier. Notez que lorsque la clé API est fournie dans la demande, les limites de débit sont suivies par clé et non par adresse IP.",gjinaai:"Fonder une déclaration sur des connaissances Web",input_token_counting:"Comptez le nombre de jetons dans la demande d'entrée.",latency:"Latence moyenne",no_token_counting:"Le jeton n'est pas comptabilisé comme une utilisation.",output_token_counting:"Comptez le nombre de jetons dans la réponse de sortie.",premium_rate:"Avec un potentiel de limites de taux plus élevées",product:"Produit",requestType:"Demande autorisée",reranker:"Classer les documents par requête",rjinaai:"Convertir l'URL en texte compatible LLM",sjinaai:"Recherchez sur le Web et convertissez les résultats en texte adapté au LLM",tbd:"À déterminer",title:"Limite de taux",tokenCounting:"Comptage de l'utilisation des jetons",tokenizer:"Tokeniser et segmenter un texte long",total_token_counting:"Comptez le nombre total de jetons dans l’ensemble du processus.",understanding:"Comprendre la limite de débit",understanding_description:"Les limites de débit correspondent au nombre maximal de requêtes pouvant être adressées à une API en une minute par adresse IP/clé API (RPM). Découvrez ci-dessous les limites de débit pour chaque produit et niveau.",wAPIkey:"avec clé API",wPremium:"avec clé API Premium",woAPIkey:"sans clé API"},fe={decision:"Décision",description:"Outils d'aide à la décision IA ultimes",intro:"Voir les deux faces de la médaille, prendre des décisions rationnelles"},_e={beta:"Expérimental",better_input:"Améliorez la qualité d’entrée dès le début",better_input_description:"Vous rencontrez des problèmes avec la sortie de votre agent ou du système RAG ? Cela peut être dû à une mauvaise qualité d'entrée.",check_price_table:"Consultez le tableau des prix",copy:"Copie",demo:{advanced_parameter_explain:"Paramètres spécifiques qui ne sont utilisés que pour {_product}.",advanced_parameters:"Spécifique",advanced_usage:"Utilisation avancée",ask_llm:"Demandez LLM sans et avec recherche de mise à la terre",ask_llm_directly:"Demandez directement à LLM",ask_llm_with_search_grounding:"Demandez LLM avec mise à la terre de recherche",ask_question:"Poser une question",ask_question_hint:"Saisissez une question et combinez-la avec le contenu récupéré pour que LLM génère une réponse",basic_usage:"Utilisation de base",basic_usage1:"Lire une URL",basic_usage2:"Rechercher une requête",basic_usage3:"Mise à la terre",common_parameter_explain:"Paramètres communs pouvant être utilisés pour {_product1}, {_product2} et {_product3}.",common_parameters:"Commun",copy:"Copie",fetch:"Récupérer du contenu",get_response:"Avoir une réponse",grounding_result_false:"Cette affirmation est fausse.",grounding_result_true:"Cette affirmation est vraie.",headers:{auth_token:"Ajouter une clé API pour une limite de débit plus élevée",auth_token_explain:"Entrez votre clé API Jina pour accéder à une limite de débit plus élevée. Pour obtenir les dernières informations sur les limites de taux, veuillez vous référer au tableau ci-dessous.",auto:"Auto",auto_explain:"Sélectionne automatiquement le moteur optimal pour l'URL.",base:"Résolution de redirection",base_explain:"Choisissez si vous souhaitez résoudre l'URL de destination finale après avoir suivi toutes les redirections. Activez cette option pour suivre la chaîne de redirection complète.",browser:"Défaut",browser_explain:"Moteur le plus compatible offrant un bon équilibre entre qualité et vitesse.",browser_locale:"Paramètres régionaux du navigateur",browser_locale_explain:"Contrôlez les paramètres régionaux du navigateur pour afficher la page. De nombreux sites Web proposent un contenu différent en fonction des paramètres régionaux.",custom_script:"Pré-exécuter du JavaScript personnalisé",custom_script_explain:"Exécute le code JavaScript de prétraitement, en acceptant soit une chaîne de code en ligne, soit un point de terminaison d'URL de script distant",deepdive:"Analyse de sources profondes",deepdive_explain:"Recherche plus de sources et lit les documents complets pour une vérification approfondie des faits. Légèrement plus lent mais plus précis et plus de références.",default:"Défaut",default_explain:"Le pipeline par défaut optimisé pour la plupart des sites Web et des entrées LLM.",direct:"La vitesse avant tout",direct_explain:"Moteur le plus rapide, mais peut avoir des difficultés avec quelques sites Web lourds en JavaScript.",engine:"Lire le moteur",engine_explain:"Choisissez le moteur à utiliser pour analyser le contenu de l'URL donnée. Affectez la qualité, la vitesse et la compatibilité du résultat.",file:"Fichier PDF/HTML local",file_explain:"Utilisez Reader sur vos fichiers PDF et HTML locaux en les téléchargeant. Ne prend en charge que les fichiers PDF et HTML.",html:"HTML",html_explain:"Renvoie documentElement.outerHTML.",image_caption:"Légende",image_caption_explain:"Sous-titre toutes les images à l'URL spécifiée, en ajoutant « Image [idx] : [caption] » comme balise alt pour celles qui n'en ont pas. Cela permet aux LLM en aval d'interagir avec les images dans des activités telles que le raisonnement et la synthèse.",images_summary:"Rassemblez toutes les images à la fin",images_summary_all:"Résumé (Toutes les images)",images_summary_all_explain:"Un résumé des images collectées est inclus, sans filtrage des doublons.",images_summary_explain:'Une section "Images" sera créée à la fin. Cela donne aux LLM en aval un aperçu de tous les visuels de la page, ce qui peut améliorer le raisonnement.',images_summary_true:"Résumé (filtré)",images_summary_true_explain:"Un résumé des images collectées est inclus, mais les images en double sont filtrées.",instruction_explain:"Extraire des informations par instruction",invalid_json:"Schéma JSON non valide",json_response:"Réponse JSON",json_response_explain:"La réponse sera au format JSON, contenant l'URL, le titre, le contenu et l'horodatage (si disponible). En mode Recherche, il renvoie une liste de cinq entrées, chacune suivant la structure JSON décrite.",json_schema_explain:"Extraction HTML vers JSON avec schéma JSON",links_summary:"Rassemblez tous les liens à la fin",links_summary_all:"Résumé (Tous les liens)",links_summary_all_explain:"Un résumé des liens collectés est inclus, sans filtrage des doublons.",links_summary_explain:`Une section "Boutons & Liens" sera créée à la fin. Cela aide les LLM ou les agents Web en aval à naviguer sur la page ou à entreprendre d'autres actions.`,links_summary_no:"Aucun résumé (par défaut)",links_summary_no_explain:"Aucune section récapitulative ne sera créée à la fin.",links_summary_true:"Résumé (filtré)",links_summary_true_explain:"Un résumé des liens collectés est inclus, mais les liens en double sont filtrés.",markdown:"Réduction",markdown_explain:"Renvoie le markdown directement à partir du HTML, en contournant le filtrage de lisibilité.",mode:"Mode lecture ou recherche",mode_explain:"Le mode Lecture permet d'accéder au contenu d'une URL, tandis que le mode Recherche vous permet de rechercher une requête sur le Web, en appliquant le mode Lecture à chaque URL de résultat de recherche.",no_cache:"Contourner le cache",no_cache_explain:"Notre serveur API met en cache le contenu des modes Lecture et Recherche pendant un certain temps. Pour contourner ce cache, définissez cet en-tête sur true.",no_gfm:"Désactivé",no_gfm_explain:"Fonctionnalités GFM (Github Flavored Markdown) désactivées.",no_gfm_table:"Pas de tableau GFM",no_gfm_table_explain:"Désactivez le tableau GFM mais conservez les éléments HTML du tableau en réponse.",opt_out_gfm:"Markdown à saveur Github",opt_out_gfm_explain:"Fonctionnalités d'activation/désactivation de GFM (Github Flavored Markdown).",pageshot:"Capture de page",pageshot_explain:"Renvoie l'URL de l'image de la capture d'écran de la page complète (avec le meilleur effort).",post_with_url:"Utiliser la méthode POST",post_with_url_explain:"Utilisez POST au lieu de la méthode GET avec une URL transmise dans le corps. Utile pour créer des SPA avec un routage basé sur le hachage.",proxy_server:"Utiliser un serveur proxy",proxy_server_explain:"Notre serveur API peut utiliser votre proxy pour accéder aux URL, ce qui est utile pour les pages accessibles uniquement via des proxys spécifiques.",references:"Références",references_explain:"Liste séparée par des virgules de références fournies par l'utilisateur (URL)",remove_all_images:"Supprimer toutes les images",remove_all_images_explain:"Supprimez toutes les images de la réponse.",remove_selector:"Sélecteur d'exclusion",remove_selector_explain:"Fournit une liste de sélecteurs CSS pour supprimer les éléments spécifiés de la page. Utile lorsque vous souhaitez exclure des parties spécifiques de la page comme les en-têtes, les pieds de page, etc.",respond_with:"Utiliser ReaderLM-v2",respond_with_explain:"Utilise ReaderLM-v2 pour la conversion HTML en Markdown, afin de fournir des résultats de haute qualité pour les sites Web aux structures et contenus complexes. Coûte 3x jetons !",result_count:"Limite de résultat",result_count_explain:"Le nombre de résultats de recherche à renvoyer.",return_format:"Format du contenu",return_format_explain:"Vous pouvez contrôler le niveau de détail de la réponse pour éviter un filtrage excessif. Le pipeline par défaut est optimisé pour la plupart des sites Web et des entrées LLM.",screenshot:"Capture d'écran",screenshot_explain:"Renvoie l'URL de l'image du premier écran.",search_engine:"Moteur de recherche",search_engine_explain:"Choisissez le moteur à utiliser pour la recherche. Affectez la qualité, la rapidité, la compatibilité du résultat.",set_cookie:"Cookie de transfert",set_cookie_explain:"Notre serveur API peut transmettre vos paramètres de cookies personnalisés lors de l'accès à l'URL, ce qui est utile pour les pages nécessitant une authentification supplémentaire. Notez que les demandes contenant des cookies ne seront pas mises en cache.",site_selector:"Recherche sur site",site_selector_explain:"Renvoie les résultats de la recherche uniquement à partir du site Web ou du domaine spécifié. Par défaut, il recherche sur l'ensemble du Web.",stream_mode:"Mode flux",stream_mode_explain:"Le mode flux est avantageux pour les grandes pages cibles, ce qui laisse plus de temps à la page pour s'afficher complètement. Si le mode standard génère un contenu incomplet, envisagez d’utiliser le mode Stream.",target_selector:"Sélecteur de cible",target_selector_explain:"Fournit une liste de sélecteurs CSS pour se concentrer sur des parties plus spécifiques de la page. Utile lorsque le contenu souhaité ne s'affiche pas dans les paramètres par défaut.",text:"Texte",text_explain:"Renvoie document.body.innerText.",token_budget:"Budget symbolique",token_budget_explain:"Limite le nombre maximal de jetons utilisés pour cette demande. Le dépassement de cette limite entraînera l'échec de la demande.",viewport:"Configuration de la fenêtre d'affichage",viewport_explain:"Configurer les dimensions de la fenêtre d'affichage du navigateur pour un rendu réactif",vlm:"VLM",vlm_explain:"Idéal pour les pages courtes contenant des contenus multimédias riches et des mises en page complexes.",wait_for_selector:"Attendre le sélecteur",wait_for_selector_explain:"Fournit une liste de sélecteurs CSS pour attendre que des éléments spécifiques apparaissent avant de revenir. Utile lorsque le contenu souhaité ne s'affiche pas dans les paramètres par défaut.",with_gfm:"Activé",with_gfm_explain:"Fonctionnalités GFM (Github Flavored Markdown) activées.",with_iframe:"Activer l'extraction d'iframe",with_iframe_explain:"Extrait et traite le contenu de tous les iframes intégrés dans l'arborescence DOM",with_shadow_dom:"Activer l'extraction Shadow DOM",with_shadow_dom_explain:"Parcourt et extrait le contenu de toutes les racines Shadow DOM dans le document",x_timeout:"Temps mort",x_timeout_explain:"Durée maximale d'attente pour le chargement de la page Web. Notez qu'il ne s'agit PAS du temps total de la requête de bout en bout."},how_to_stream:"Pour traiter le contenu dès qu'il devient disponible, définissez l'en-tête de la requête en mode flux. Cela minimise le temps jusqu'à ce que le premier octet soit reçu. Exemple en curl :",how_to_use1:"Ajoutez https://r.jina.ai/ à n'importe quelle URL de votre code ou outil où l'accès LLM est attendu. Cela renverra le contenu principal de la page dans un texte clair et convivial LLM.",how_to_use2:"Ajoutez https://s.jina.ai/ à votre requête. Cela appellera le moteur de recherche et renverra les 5 premiers résultats avec leurs URL et leur contenu, chacun dans un texte clair et convivial LLM.",how_to_use3:"Ajoutez https://g.jina.ai/ à votre déclaration. Cela appellera le moteur de jugement et renverra le pourcentage de véracité, une valeur booléenne indiquant si la déclaration est vraie ou fausse, un résumé de la raison et une liste de références.",key_required:"Clé API requise pour utiliser ce point de terminaison",learn_more:"Apprendre encore plus",open:"Ouvrir dans un nouvel onglet",params_classification:"Paramètres",raw_html:"HTML brut",reader_output:"Sortie du lecteur",reader_response:"Réponse du lecteur",reader_search_hint:"Si vous utilisez cette URL dans du code, n'oubliez pas de coder l'URL.",reader_url:"URL du lecteur",reader_url_hint:"Cliquez ci-dessous pour obtenir le contenu via notre API Reader",requires_post_method:"Cette fonction nécessite la méthode POST. En téléchargeant votre fichier local, la méthode POST sera automatiquement activée.",search_params:"Paramètres/en-têtes de recherche",search_query_rewrite:"Veuillez noter que contrairement à la démo présentée ci-dessus, en pratique, vous ne recherchez pas la question d'origine sur le Web pour vous ancrer. Ce que les gens font souvent, c'est réécrire la question d'origine ou utiliser des questions à sauts multiples. Ils lisent les résultats récupérés, puis génèrent des requêtes supplémentaires pour recueillir plus d'informations si nécessaire avant d'arriver à une réponse finale.",select_mode:"Sélectionner le mode",show_read_demo:"Découvrez comment Reader lit une URL",show_search_demo:"Découvrez comment Reader effectue des recherches sur le Web",slow_warning:"Cela peut prendre jusqu'à 30 secondes et coûte jusqu'à 300 000 jetons par demande.",standard_usage:"Utilisation standard",stream_mode:"Mode flux",stream_mode_explain:"Le mode flux est utile lorsque la page cible est volumineuse à afficher. Si vous trouvez que le mode standard vous donne un contenu incomplet, essayez le mode flux.",stream_mode_explain1:"Le mode streaming est utile lorsque vous constatez que le mode standard fournit un résultat incomplet. En effet, le mode streaming attendra un peu plus longtemps jusqu'à ce que la page soit entièrement rendue. Utilisez l'en-tête accept pour basculer le mode de streaming :",tagline:"Essayez la démo",try_demo:"Démo",use_headers:"Le comportement de l'API Reader peut être contrôlé avec les en-têtes de requête. Voici une liste complète des en-têtes pris en charge.",waiting_for_reader:"En attendant le résultat de l'API Reader d'abord...",warn_grounding_message:"Ce processus peut prendre jusqu'à 30 secondes et consommer jusqu'à 300 000 jetons par demande de mise à la terre. Certains navigateurs peuvent mettre fin à la demande en raison de la longue latence. Nous vous recommandons donc de copier le code et de l'exécuter à partir de votre terminal.",warn_grounding_title:"Latence élevée et utilisation de jetons",your_query:"Entrez votre requête",your_query_hint:"Tapez une question qui nécessite les dernières informations ou connaissances du monde.",your_statement:"Votre déclaration de vérification des faits",your_url:"Entrez votre URL",your_url_hint:"Cliquez ci-dessous pour récupérer directement le code source de la page"},description:"Lisez les URL et effectuez des recherches sur le Web pour de meilleurs LLM de base.",dont_panic_api_key_is_free:"Ne pas paniquer! Chaque nouvelle clé API contient un million de jetons gratuits !",faq_v1:{answer1:"L'API Reader est gratuite et ne nécessite pas de clé API. Ajoutez simplement « https://r.jina.ai/ » à votre URL.",answer10:"Non, l'API Reader ne peut traiter que le contenu provenant d'URL accessibles au public.",answer11:"Si vous demandez la même URL dans les 5 minutes, l'API Reader renverra le contenu mis en cache.",answer12:"Malheureusement non.",answer13:"Oui, vous pouvez soit utiliser le support PDF natif du Reader (https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4), soit utiliser la version HTML d'arXiv (https:// r.jina.ai/https://arxiv.org/html/2310.19923v4)",answer14:"Reader sous-titre toutes les images à l'URL spécifiée et ajoute « Image [idx] : [caption] » comme balise alt (si elles en manquent initialement). Cela permet aux LLM en aval d'interagir avec les images dans le raisonnement, la synthèse, etc.",answer15:"L'API Reader est conçue pour être hautement évolutive. Il est automatiquement mis à l'échelle en fonction du trafic en temps réel et le nombre maximal de requêtes simultanées est désormais d'environ 4 000. Nous le maintenons activement comme l'un des produits principaux de Jina AI. N'hésitez donc pas à l'utiliser en production.",answer16:"Veuillez trouver les dernières informations sur les limites de taux dans le tableau ci-dessous. Notez que nous travaillons activement à l'amélioration de la limite de débit et des performances de l'API Reader, le tableau sera mis à jour en conséquence.",answer17:"Reader-LM est un nouveau modèle de langage de petite taille (SLM) conçu pour l'extraction et le nettoyage de données à partir du Web ouvert. Il convertit le HTML brut et bruyant en markdown propre, en s'inspirant de Jina Reader. En mettant l'accent sur la rentabilité et la petite taille du modèle, Reader-LM est à la fois pratique et puissant. Il est actuellement disponible sur les marchés AWS, Azure et GCP. Si vous avez des exigences spécifiques, veuillez nous contacter à sales AT jina.ai.",answer2:"L'API Reader utilise un proxy pour récupérer n'importe quelle URL, rendant son contenu dans un navigateur pour extraire le contenu principal de haute qualité.",answer3:"Oui, l'API Reader est open source et disponible sur le référentiel Jina AI GitHub.",answer4:"L'API Reader traite généralement les URL et renvoie le contenu dans un délai de 2 secondes, bien que les pages complexes ou dynamiques puissent nécessiter plus de temps.",answer5:"Le scraping peut être compliqué et peu fiable, en particulier avec des pages complexes ou dynamiques. L'API Reader fournit une sortie rationalisée et fiable de texte propre et prêt pour LLM.",answer6:"L'API Reader renvoie le contenu dans la langue d'origine de l'URL. Il ne fournit pas de services de traduction.",answer7:"Si vous rencontrez des problèmes de blocage, veuillez contacter notre équipe d'assistance pour obtenir de l'aide et une résolution.",answer8:"Bien qu'elle soit principalement conçue pour les pages Web, l'API Reader peut extraire le contenu des PDF affichés au format HTML sur des sites Web comme arXiv, mais elle n'est pas optimisée pour l'extraction PDF générale.",answer9:"Actuellement, l'API Reader ne traite pas le contenu multimédia, mais les améliorations futures incluront le sous-titrage d'images et le résumé vidéo.",question1:"Quels sont les coûts associés à l’utilisation de l’API Reader ?",question10:"Est-il possible d'utiliser l'API Reader sur des fichiers HTML locaux ?",question11:"L'API Reader met-elle en cache le contenu ?",question12:"Puis-je utiliser l'API Reader pour accéder au contenu derrière une connexion ?",question13:"Puis-je utiliser l'API Reader pour accéder au PDF sur arXiv ?",question14:"Comment fonctionne la légende d’image dans Reader ?",question15:"Quelle est l’évolutivité du Reader ? Puis-je l’utiliser en production ?",question16:"Quelle est la limite de débit de l’API Reader ?",question17:"Qu'est-ce que Reader-LM ? Comment puis-je l'utiliser ?",question2:"Comment fonctionne l'API Reader ?",question3:"L'API Reader est-elle open source ?",question4:"Quelle est la latence typique de l’API Reader ?",question5:"Pourquoi devrais-je utiliser l'API Reader au lieu de gratter la page moi-même ?",question6:"L'API Reader prend-elle en charge plusieurs langues ?",question7:"Que dois-je faire si un site Web bloque l’API Reader ?",question8:"L'API Reader peut-elle extraire le contenu des fichiers PDF ?",question9:"L'API Reader peut-elle traiter le contenu multimédia des pages Web ?",title:"Questions courantes liées aux lecteurs"},fast:"Rapide",fast_stream:"Streaming de données immédiat",fast_stream_description:"Besoin de données rapidement ? Notre API Reader peut diffuser des données pour minimiser la latence.",free:"Libre pour toujours",free_description:"L’API Reader est gratuite ! Il ne nécessite aucune carte de crédit ni secret API. Cela ne consommera pas votre quota de jetons.",is_free:"La meilleure partie? C'est gratuit!",is_free_description:"L'API Reader est disponible gratuitement et offre une limite de débit et une tarification flexibles. Construit sur une infrastructure évolutive, il offre une accessibilité, une concurrence et une fiabilité élevées. Nous nous efforçons d'être votre solution de mise à la terre préférée pour vos LLM.",lm_v2_description:"ReaderLM-v2 est un modèle de langage de 1,5 milliard de paramètres spécialisé dans la conversion HTML vers Markdown et l'extraction HTML vers JSON. Il prend en charge les documents jusqu'à 512 000 jetons dans 29 langues et offre une précision 20 % supérieure à celle de son prédécesseur.",lm_v2_title:"ReaderLM v2 : petit modèle de langage pour HTML vers Markdown et JSON",open:"Ouvrir dans un nouvel onglet",original_pdf:"PDF original",rate_limit:"Limite de taux",read_grounding_release_note:"Lire la note de publication",reader_also_read_images:"Les images de la page Web sont automatiquement sous-titrées à l'aide d'un modèle de langage de vision dans le lecteur et formatées sous forme de balises alt d'image dans la sortie. Cela donne à votre LLM en aval juste assez d'indices pour intégrer ces images dans ses processus de raisonnement et de synthèse. Cela signifie que vous pouvez poser des questions sur les images, en sélectionner des spécifiques ou même transmettre leurs URL à un VLM plus puissant pour une analyse plus approfondie !",reader_description:"Convertissez une URL en entrée compatible LLM, en ajoutant simplement <code>r.jina.ai</code> devant.",reader_do_grounding:"Lecteur pour vérification des faits",reader_do_grounding_explain:"Le nouveau point de terminaison de mise à la terre offre une expérience de vérification des faits de bout en bout, en temps quasi réel. Il prend une déclaration donnée, la fonde à l'aide de résultats de recherche Web en temps réel et renvoie un score de factualité et les références exactes utilisées. Vous pouvez facilement fonder des déclarations pour réduire les hallucinations du LLM ou améliorer l'intégrité du contenu écrit par l'homme.",reader_do_pdf_explain:"Oui, Reader prend en charge nativement la lecture de PDF. Il est compatible avec la plupart des PDF, y compris ceux contenant de nombreuses images, et il est ultra-rapide ! En combinaison avec un LLM, vous pouvez facilement créer une IA ChatPDF ou d'analyse de documents en un rien de temps.",reader_do_search:"Lecteur pour la mise à la terre de la recherche",reader_do_search_explain:"Les LLM ont un seuil de connaissances, ce qui signifie qu'ils ne peuvent pas accéder aux dernières connaissances mondiales. Cela conduit à des problèmes tels que la désinformation, les réponses obsolètes, les hallucinations et d’autres problèmes factuels. La mise à la terre est absolument essentielle pour les applications GenAI. Reader vous permet de baser votre LLM avec les dernières informations du Web. Ajoutez simplement https://s.jina.ai/ à votre requête, et Reader effectuera une recherche sur le Web et renverra les cinq premiers résultats avec leurs URL et leur contenu, chacun dans un texte clair et convivial LLM. De cette façon, vous pouvez toujours garder votre LLM à jour, améliorer sa factualité et réduire les hallucinations.",reader_reads_images:"Reader lit aussi les images !",reader_reads_pdf:"Reader lit également les PDF !",reader_result:"Résultat du lecteur",table:{td_1_0:"Lire une URL renvoie son contenu, utile pour vérifier la mise à la terre",td_1_1:"20 tr/min",td_1_2:"200 tr/min",td_1_3:"Basé sur les jetons de sortie",td_1_4:"3 secondes",td_1_5:"3 secondes",td_2_0:"La recherche sur le Web renvoie les 5 premiers résultats, utiles pour ancrer la recherche",td_2_1:"5 tours",td_2_2:"40 tr/min",td_2_3:"Basé sur les jetons de sortie pour les 5 résultats de recherche",td_2_4:"10 secondes",td_2_5:"10 secondes",th0:"Point de terminaison",th1:"Description",th2:"Limite de débit sans clé API",th3:"Limite de débit avec clé API",th4:"Schéma de comptage de jetons",th5:"Latence moyenne",th6:"Latence moyenne"},title:"API de lecteur",usage:"Usage",usage_details_false:"Afficher uniquement les utilisations de base",usage_details_null:"Afficher les utilisations de base et avancées",usage_details_true:"Afficher uniquement les utilisations avancées",want_higher_rate_limit:"Vous souhaitez une limite de débit plus élevée jusqu'à 1 000 tr/min ? Nous pouvons vous soutenir !",what_is1:"Qu’est-ce que Reader ?",what_is_answer_long:"Introduire des informations Web dans les LLM est une étape importante de la mise à la terre, mais cela peut être un défi. La méthode la plus simple consiste à gratter la page Web et à alimenter le code HTML brut. Cependant, le scraping peut être complexe et souvent bloqué, et le HTML brut est encombré d'éléments superflus tels que des balises et des scripts. L'API Reader résout ces problèmes en extrayant le contenu principal d'une URL et en le convertissant en texte clair et convivial LLM, garantissant ainsi une saisie de haute qualité pour vos systèmes d'agent et RAG.",what_is_desc:"Un proxy qui accède à n'importe quelle URL et transforme le contenu principal en texte brut optimisé pour les LLM."},he={confirm_message:"Il reste {_leftTokens} jetons à votre clé API. L'envoi du texte intégral des articles {_numArticles} à l'API Reranker, en utilisant le modèle {_selectedReranker} pour découvrir les articles associés à la page actuelle, réduira considérablement le nombre de jetons de votre clé API {_APIKey}. Voulez-vous poursuivre?",confirm_title:"Avertissement : utilisation élevée des jetons",out_of_quota:"Cette clé API est à court de jetons. Veuillez recharger votre compte ou utiliser une autre clé API.",recommend:"Obtenez le top 5",recommended_articles:"Top 5 des articles similaires"},qe={benchmark:{description0:"LlamaIndex a évalué diverses combinaisons d'intégrations et de reclassements pour RAG, en menant une étude de réplication qui mesurait le rang réciproque moyen. Les résultats mettent en évidence l’amélioration significative de la qualité de recherche apportée par Jina Reranker, un avantage indépendant des intégrations spécifiques utilisées.",description1:"BIER (Benchmarking IR) évalue l'efficacité de récupération d'un modèle, y compris la pertinence et le NDCG. Un score BIER plus élevé est corrélé à des correspondances et à un classement des résultats de recherche plus précis.",description2:"Grâce au benchmark LoCo, nous avons mesuré la compréhension d'un modèle de la cohérence et du contexte locaux, ainsi que le classement spécifique à la requête. Un score LoCo plus élevé reflète une meilleure capacité à identifier et hiérarchiser les informations pertinentes.",description3:"Le MTEB (Multilingual Text Embedding Benchmark), dans son ensemble, teste les capacités d'un modèle en matière d'intégration de texte, y compris le clustering, la classification, la récupération et d'autres mesures. Cependant, pour notre comparaison, nous avons utilisé uniquement les tâches de reclassement du MTEB.",title:"Référence",title0:"LamaIndex",title1:"BÉIR",title2:"Locomotive",title3:"MTEB"},benchmark_description:"À titre de comparaison, nous avons inclus trois autres principaux reclasseurs de BGE (BAAI), BCE (Netease Youdao) et Cohere dans l'indice de référence. Comme le montrent les résultats ci-dessous, Jina Reranker détient le score moyen le plus élevé dans toutes les catégories pertinentes pour le reclassement, ce qui en fait un leader incontesté parmi ses pairs.",benchmark_title:"Référence de performances",choose_turbo:"Obtenez jusqu'à 5x d'accélération avec reranker-turbo",choose_turbo_description:"Nous proposons également deux nouveaux modèles de reranker open source : jina-reranker-v1-turbo-en et jina-reranker-v1-tiny-en, ce dernier n'a que 30 millions de paramètres et quatre couches. Ces deux nouveaux rerankers bénéficient d’une vitesse d’inférence 5 fois plus rapide que le modèle de base pour un très faible coût en termes de qualité. Ils sont parfaits pour les applications nécessitant un reclassement en temps réel. Lisez le benchmark ci-dessous.",customize_urself:"Changez-le et voyez comment la réponse change !",customize_urself_pl:"Changez-les et voyez comment la réponse change !",description:"Récupérateur neuronal de classe mondiale pour maximiser la pertinence de la recherche.",description_rich:"Optimisez la pertinence de la recherche et la précision du RAG avec notre API de reranker de pointe.",example_input_document:"Exemples de documents candidats à classer",example_input_query:"Exemple de requête",faq_v1:{answer1:"Le prix de l'API Reranker est aligné sur notre structure tarifaire de l'API d'intégration. Cela commence avec 1 million de jetons gratuits pour chaque nouvelle clé API. Au-delà des jetons gratuits, différents packages sont disponibles à l'achat. Pour plus de détails, veuillez visiter notre section tarification.",answer10:"Oui, nos services sont disponibles sur les marketplaces AWS, Azure et GCP. Si vous avez des besoins spécifiques, veuillez nous contacter à sales AT jina.ai.",answer11:"Si vous êtes intéressé par un reranker affiné et adapté à des données de domaine spécifiques, veuillez contacter notre équipe commerciale. Notre équipe répondra à votre demande dans les plus brefs délais.",answer3:"<code>jina-reranker-v2-base-multilingual</code> excelle dans la prise en charge multilingue, surpassant <code>bge-reranker-v2-m3</code> et offrant un débit 15 fois plus rapide que <code>jina-reranker-v1-base-en</code>. Il prend également en charge les tâches d'agent et la récupération de code. <code>jina-colbert-v2</code> améliore <code>ColBERTv2</code>, offrant des performances de récupération 6,5 % supérieures et ajoutant une prise en charge multilingue pour 89 langues. Il propose des tailles d'intégration contrôlées par l'utilisateur pour une efficacité et une précision optimales.",answer4:"Oui, <code>jina-reranker-v2-base-multilingual</code> et <code>jina-colbert-v2</code> sont tous deux open source et disponibles sous la licence CC-BY-NC 4.0. Vous êtes libre d'utiliser, de partager et d'adapter les modèles à des fins non commerciales.",answer5:"Oui, <code>jina-reranker-v2-base-multilingual</code> et <code>jina-colbert-v2</code> prennent en charge plus de 100 langues, dont l'anglais, le chinois et d'autres langues mondiales majeures. Ils sont optimisés pour les tâches multilingues et surpassent les modèles précédents.",answer6:"La longueur maximale du jeton de requête est de 512. Il n’y a aucune limite de jetons pour les documents.",answer7:"Vous pouvez reclasser jusqu'à 2 048 documents par requête.",answer8:"Il n’y a pas de notion de taille de lot contrairement à notre API Embedding. Vous ne pouvez envoyer qu’un seul tuple de document de requête par requête, mais le tuple peut inclure jusqu’à 2 048 documents candidats.",answer9:"La latence varie de 100 millisecondes à 7 secondes, en fonction en grande partie de la longueur des documents et de la requête. Par exemple, le reclassement de 100 documents de 256 jetons chacun avec une requête de 64 jetons prend environ 150 millisecondes. L'augmentation de la longueur du document à 4 096 jetons augmente le temps à 3,5 secondes. Si la longueur de la requête est augmentée à 512 jetons, le temps augmente encore à 7 secondes.",question1:"Combien coûte l’API Reranker ?",question10:"Vos points de terminaison peuvent-ils être hébergés en privé sur AWS, Azure ou GCP ?",question11:"Proposez-vous un reranker affiné sur les données spécifiques à un domaine ?",question3:"Quelle est la différence entre les deux rerankers ?",question4:"Les Jina Rerankers sont-ils open source ?",question5:"Les rerankers prennent-ils en charge plusieurs langues ?",question6:"Quelle est la longueur maximale des requêtes et des documents ?",question7:"Quel est le nombre maximum de documents que je peux reclasser par requête ?",question8:"Quelle est la taille du lot et combien de tuples de documents de requête puis-je envoyer en une seule requête ?",question9:"À quelle latence puis-je m'attendre lors du reclassement de 100 documents ?",title:"Questions courantes liées au reranker"},feature_on_premises_description2:"Déployez Jina Reranker sur AWS Sagemaker, et bientôt dans Microsoft Azure et Google Cloud Services, ou contactez notre équipe commerciale pour obtenir des déploiements Kubernetes personnalisés pour votre cloud privé virtuel et vos serveurs sur site.",feature_on_premises_description3:"Déployez Jina Reranker sur AWS Sagemaker et Microsoft Azure et bientôt dans Google Cloud Services, ou contactez notre équipe commerciale pour obtenir des déploiements Kubernetes personnalisés pour votre cloud privé virtuel et vos serveurs sur site.",feature_solid_description:"Développé à partir de nos recherches universitaires de pointe et rigoureusement testé par rapport aux rerankers SOTA pour garantir des performances inégalées.",how_it_works:"Voici comment cela fonctionne:",how_it_works_v1:{description1:"Un système de recherche utilise embeddings/BM25 pour trouver un large ensemble de documents potentiellement pertinents en fonction de la requête de l'utilisateur.",description2:"Le reranker prend ensuite ces résultats et les analyse à un niveau plus granulaire, en tenant compte des nuances de la manière dont les termes de requête interagissent avec le contenu du document.",description3:"Il réorganise les résultats de recherche, en plaçant ceux qu'il juge les plus pertinents en haut, sur la base de cette analyse plus approfondie.",title1:"Récupération initiale",title2:"Reclassement",title3:"Résultats améliorés"},improve_performance:"Amélioration garantie par rapport à la recherche vectorielle",improve_performance_description:"Nos évaluations ont démontré des améliorations pour les systèmes de recherche utilisant Jina Reranker avec +8 % de taux de réussite et +33 % de classement réciproque moyen.",learning1:"En savoir plus sur le Reranker",learning1_description:"Qu'est-ce qu'un reranker ? Pourquoi la recherche vectorielle ou la similarité cosinus ne suffisent-elles pas ? Découvrez les rerankers de A à Z avec notre guide complet.",read_more_about_benchmark:"En savoir plus sur le benchmark",read_more_about_turbo:"En savoir plus sur les modèles turbo et minuscules",read_more_about_v2:"Jina Reranker v2 est le meilleur reranker de sa catégorie sorti le 25 juin 2024 ; il est conçu pour Agentic RAG. Il offre une prise en charge des appels de fonctions, une récupération multilingue pour plus de 100 langues, des capacités de recherche de code et offre une accélération 6x par rapport à la v1. En savoir plus sur le modèle v2.",reranker_description:"Essayez notre API de reclassement de pointe pour maximiser la pertinence de votre recherche et la précision de RAG. Commencer gratuitement !",show_v2benchmark:"Afficher le benchmark pour le modèle v2 (dernier)",table:{number_token_document:"Nombre de jetons dans chaque document",number_token_query:"Nombre de jetons dans la requête",title:"Vous trouverez ci-dessous le coût en temps nécessaire au reclassement d'une requête et de 100 documents en millisecondes :"},title:"API de reclassement",top_n:"Nombre de documents retournés",top_n_explain:"Le nombre de documents les plus pertinents à renvoyer pour la requête.",try_embedding:"Essayez d'intégrer l'API gratuitement",try_reranker:"Essayez l'API de reclassement gratuitement",v2_features:{description1:"Reranker v2 permet la récupération de documents dans plus de 100 langues, quel que soit le langage de requête.",description2:"Reranker v2 classe les extraits de code et les signatures de fonctions en fonction de requêtes en langage naturel, idéal pour les applications Agentic RAG.",description3:"Reranker v2 classe les tables les plus pertinentes en fonction de requêtes en langage naturel, aidant ainsi à trier différents schémas de table et à identifier le plus pertinent avant de générer une requête SQL.",title1:"Récupération multilingue",title2:"Appel de fonction et recherche de code",title3:"Prise en charge des données tabulaires et structurées"},v2benchmark:{descBeir:"Scores NDCG 10 rapportés pour différents modèles de reclassement pour l'ensemble de données Beir",descCodeSearchNet:"Scores MRR 10 rapportés pour différents modèles de reclassement pour l'ensemble de données CodeSearchNet",descMKQA:"Rappel de 10 scores rapportés pour différents modèles de reclassement pour l'ensemble de données MKQA",descNSText2SQL:"Rappel de 3 scores rapportés pour différents modèles de reclassement pour l'ensemble de données NSText2SQL",descRTX4090:"Scores de débit (documents récupérés en 50 ms) rapportés pour différents modèles de reclassement sur un GPU RTX 4090.",descToolBench:"Rappel de 3 scores rapportés pour différents modèles de reclassement pour l'ensemble de données ToolBench",titleBeir:"BEIR (Benchmark hétérogène sur diverses tâches IR)",titleCodeSearchNet:"CodeSearchNet. Le benchmark est une combinaison de requêtes aux formats docstring et en langage naturel, avec des segments de code étiquetés pertinents pour les requêtes.",titleMKQA:"MKQA (Questions et réponses sur les connaissances multilingues)",titleNSText2SQL:"NSText2SQL",titleRTX4090:"Débit de Jina Reranker v2 sur RTX4090",titleToolBench:"Banc d'outils. Le benchmark collecte plus de 16 000 API publiques et les instructions correspondantes générées synthétiquement pour les utiliser dans des paramètres à API unique et multi-API."},vs_table:{col0:"Reclasseur",col0_1:"Précision et pertinence de recherche améliorées",col0_2:"Filtrage initial et rapide",col0_3:"Récupération de texte générale pour des requêtes étendues",col1:"Recherche de vecteurs",col1_1:"Détaillé : sous-document et segment de requête",col1_2:"Large : documents entiers",col1_3:"Intermédiaire : divers segments de texte",col2:"BM25",col2_1:"Haut",col2_2:"Moyen",col2_3:"Faible",col3_1:"Non requis",col3_2:"Haut",col3_3:"Faible, utilise un index prédéfini",col4_1:"Haut",col4_2:"Haut",col4_3:"Non requis",col5_1:"Supérieur pour les requêtes nuancées",col5_2:"Équilibré entre efficacité et précision",col5_3:"Cohérent et fiable pour un large éventail de requêtes",col6_1:"Très précis avec une compréhension contextuelle approfondie",col6_2:"Rapide et efficace, avec une précision modérée",col6_3:"Hautement évolutif, avec une efficacité établie",col7_1:"gourmand en ressources avec une mise en œuvre complexe",col7_2:"Peut ne pas capturer le contexte ou les nuances approfondies des requêtes",col7_3:"Peut sous-performer pour les recherches très spécifiques ou contextuelles",header0:"Meilleur pour",header1:"Granularité",header2:"Complexité du temps de requête",header3:"Complexité du temps d’indexation",header4:"Complexité du temps de formation",header5:"Qualité de la recherche",header6:"Forces",header7:"Faiblesses",subtitle:"Le tableau ci-dessous fournit une comparaison complète du Reranker, de la recherche Vector/Embeddings et du BM25, mettant en évidence leurs forces et leurs faiblesses dans diverses catégories.",title:"Comparaison de Reranker, Vector Search et BM25"},what_is:"Qu'est-ce qu'un reranker ?",what_is_answer_long:`L’objectif d’un système de recherche est de trouver les résultats les plus pertinents rapidement et efficacement. Traditionnellement, des méthodes telles que BM25 ou tf-idf ont été utilisées pour classer les résultats de recherche en fonction de la correspondance des mots clés. Des méthodes récentes, telles que la similarité cosinus basée sur l'intégration, ont été implémentées dans de nombreuses bases de données vectorielles. Ces méthodes sont simples mais peuvent parfois passer à côté des subtilités du langage et, plus important encore, de l'interaction entre les documents et l'intention d'une requête.

C'est là que le « reranker » brille. Un reranker est un modèle d'IA avancé qui prend l'ensemble initial de résultats d'une recherche (souvent fourni par une recherche basée sur des intégrations/jetons) et les réévalue pour s'assurer qu'ils correspondent plus étroitement à l'intention de l'utilisateur. Il va au-delà de la correspondance superficielle des termes pour considérer l’interaction plus profonde entre la requête de recherche et le contenu des documents.`,what_is_answer_long_ending:"Le reranker peut améliorer considérablement la qualité de la recherche car il opère au niveau des sous-documents et des sous-requêtes, ce qui signifie qu'il examine les mots et expressions individuels, leur signification et leurs relations les uns avec les autres dans la requête et les documents. Cela se traduit par un ensemble de résultats de recherche plus précis et contextuellement pertinents.",what_is_desc:"Un reranker est un modèle d'IA qui affine les résultats de recherche à partir d'une recherche vectorielle ou d'un modèle de récupération dense. En savoir plus."},be={caption_image_desc:"Générez une description textuelle de l’image.",caption_image_title:"Légende de l'image",description:"Explorez la narration d'images au-delà des pixels",example1:"Cette vidéo semble être une séquence de nature mettant en vedette un charmant lapin blanc et un papillon dans un champ herbeux. Le lapin interagit avec le papillon de différentes manières, mettant en valeur leur relation unique. L'environnement naturel offre une toile de fond pittoresque, rehaussant la beauté de cette scène simple mais captivante.",generate_story_desc:"Créez une histoire inspirée de l’image, comportant souvent des dialogues ou des monologues de ses personnages.",generate_story_title:"Générer une histoire",intro1:"Solution d'IA leader pour les légendes d'images et les résumés vidéo",json_image_desc:"Générez un format JSON structuré à partir de l'image à l'aide d'un schéma prédéfini. Cela permet une extraction de données spécifiques de l’image.",json_image_title:"Extraire JSON de l'image",summarize_video_desc:"Générez un résumé concis de la vidéo, mettant en évidence les événements clés.",summarize_video_title:"Résumer la vidéo",visual_q_a_desc:"Répondez à une requête basée sur le contenu de l'image.",visual_q_a_title:"Questions et réponses visuelles"},xe={ask_on_current_page:"Interrogez la page actuelle sur...",find_solution:"Générer une solution pour...",hint:"Recherchez des produits, des actualités et vos questions",hotkey:"Appuyez sur la touche / pour effectuer une recherche sur cette page",hotkey1:"Presse",hotkey2:"pour basculer",hotkey_long1:"À tout moment, appuyez sur",hotkey_long3:"pour ouvrir la barre de recherche",more_results:"{_numMore} autres résultats",placeholder:"Posez n'importe quelle question sur cette page",proposing_solution:"Générer une réponse basée sur le contenu de la page...",required:"Veuillez décrire votre question avec plus de détails.",results:"résultats"},Ae={description:"Naviguer, interagir, affiner : réinventez la découverte de produits"},Le={description:"Combler le fossé sémantique dans votre infrastructure de recherche existante"},ze={"Hacker News":"Actualités des pirates",LinkedIn:"LinkedIn",facebook:"Facebook",reddit:"Reddit",rss:"flux RSS",share_btn:"Partager",twitter:"X (Twitter)"},ye={click_to_learn_more:"Cliquez pour en savoir plus",contextualization:"Contextualisation",contextualization_desc:"Les rerankers ajustent les résultats de recherche initiaux en fonction d'une pertinence contextuelle profonde. requête. Cela affine le classement pour mieux correspondre à ce que les utilisateurs sont susceptibles de trouver utiles.",coreInfra:"Infra de base",coreInfra_desc:"Core Infra fournit une couche cloud native pour développer, déployer et orchestration des modèles de base de recherche à la fois dans le cloud public et sur site, permettant aux services d'évoluer sans effort.",embedding_serving:"Intégration du service",embedding_serving_description:"Fournir des intégrations via un microservice robuste et évolutif utilisant des technologies cloud natives.",embedding_tech:"Intégrations",embedding_tech_description:`Chez Jina AI, nous exploitons la puissance de la technologie intégrée pour révolutionner diverses applications d’IA. Cette technologie sert de méthode unifiée pour représenter et compresser efficacement divers types de données, garantissant ainsi l’absence de perte d’informations critiques. Notre objectif est de transformer des ensembles de données complexes en un format d’intégration universellement compréhensible, essentiel pour une analyse précise et perspicace de l’IA.

Les intégrations sont fondamentales, en particulier dans des applications telles que la reconnaissance précise d’images et de voix, où elles permettent de discerner des détails et des nuances fins. Dans le traitement du langage naturel, les intégrations améliorent la compréhension du contexte et des sentiments, conduisant à des outils d’IA conversationnelle et de traduction linguistique plus précis. Ils jouent également un rôle crucial dans le développement de systèmes de recommandation sophistiqués qui nécessitent une compréhension approfondie des préférences des utilisateurs pour différentes formes de contenu, telles que le texte, l'audio et la vidéo.`,embedding_tuning:"Intégration du réglage",embedding_tuning_description:"Optimiser les intégrations de haute qualité en intégrant l'expertise du domaine pour des performances améliorées spécifiques aux tâches.",embeddings:"Intégrations",embeddings_desc:"Les intégrations sont la pierre angulaire du système de recherche moderne, représentant les données multimodales en vecteurs de nombres. Ce processus permet une compréhension plus nuancée et contextuelle du contenu, bien au-delà de la simple correspondance de mots clés.",for_developers:"Pour les développeurs",for_enterprise:"Pour les entreprises",for_power_users:"Pour les utilisateurs expérimentés",grounding:"Mise à la terre",grounding_desc:"Le lecteur affine les entrées et les résultats via les LLM. Ils améliorent la qualité, la lisibilité et la factualité de la réponse finale.",model_serving:"Modèle de service",model_serving_description:"Le déploiement de modèles affinés dans un environnement de production, nécessitant généralement des ressources importantes telles que l'hébergement de GPU. MLOps, mettant l'accent sur le service des modèles de taille moyenne à grande de manière évolutive, efficace et fiable.",model_tuning:"Réglage du modèle",model_tuning_description:"Également connu sous le nom de réglage fin, consiste à ajuster les paramètres d'un modèle pré-formé sur un nouvel ensemble de données, souvent spécifique à une tâche, pour améliorer ses performances et l'adapter à une application spécifique.",personalization:"Personnalisation",personalization_desc:"Utilisation de données synthétiques guidées par les instructions de l'utilisateur pour former automatiquement un modèle d'intégration et de reclassement spécifique au domaine.",preprocessing:"Prétraitement",preprocessing_desc:"Le prétraitement implique le nettoyage, la normalisation et la transformation des données brutes dans un format digeste par le système de recherche.",promptOps:"InviteOps",promptOps_desc:"Prompt Ops améliore les entrées et sorties du système de recherche, y compris celles utilisées dans l'expansion des requêtes, l'entrée LLM et la réécriture des résultats. Cela garantit que la recherche comprend mieux et donne de meilleurs résultats.",prompt_serving:"Service rapide",prompt_serving_description:"Enveloppez et servez des invites via une API, sans héberger de modèles lourds. L'API appelle un service public de modèle de grande langue et gère l'orchestration des entrées et des sorties dans une chaîne d'opérations.",prompt_tech:"Ingénierie des invites et des agents",prompt_tech_description:`Chez Jina AI, nous reconnaissons l'ingénierie rapide comme essentielle pour interagir avec les grands modèles de langage (LLM). À mesure que ces modèles progressent, la complexité des invites augmente, englobant un raisonnement et une logique complexes. Cette avancée souligne la croissance étroitement liée des LLM et de la sophistication rapide.

Nous prévoyons un avenir dans lequel les LLM agiront comme des compilateurs, les invites devenant le nouveau langage de programmation. Ce changement suggère que les futures compétences technologiques pourraient se concentrer davantage sur une maîtrise rapide que sur le codage traditionnel. Notre engagement chez Jina AI est d'être leader dans ce domaine de transformation, en rendant l'IA avancée accessible et pratique pour une utilisation quotidienne en maîtrisant ce « langage » émergent.`,prompt_tuning:"Réglage rapide",prompt_tuning_description:"Le processus d'élaboration et de raffinement des invites d'entrée afin de guider sa sortie vers des réponses spécifiques et souhaitées.",representation:"Représentation",representation_desc:"Les intégrations transforment les données multimodales en un format uniforme et vectorisé. Cela permet au système de recherche de comprendre et de catégoriser le contenu au-delà de simples mots-clés.",rerankers:"Reclasseur",rerankers_desc:"Les rerankers prennent les résultats initiaux des intégrations et les affinent, garantissant que les résultats les plus pertinents sont présentés à l'utilisateur. Ceci est crucial pour fournir des résultats de recherche de haute qualité qui répondent à l’intention de l’utilisateur."},Pe={care_most:"Qu’est-ce qui vous tient le plus à cœur ?",care_most_options:{accuracy:"Précision",cost:"Coût",other:"Autre",scalability:"Évolutivité",speed:"Vitesse"},care_most_required:"Lorsque vous choisissez un service, qu’est-ce qui vous importe le plus ?",company_size:"Quelle est la taille de votre entreprise ?",company_size_required:"Dites-nous que la taille de votre entreprise nous aide à fournir un meilleur service",company_url:"Quel est le site Internet de votre entreprise ?",company_url_required:"Dites-nous que le site Web de votre entreprise nous aide à fournir un meilleur service",contactName:"Votre nom",contactName_required:"Comment devrions-nous vous adresser ?",contactTitle:"Quel est votre titre de poste ?",contactTitle_required:"Votre titre de poste est requis",contact_us:"Contactez-nous",domain_required:"Dites-nous que votre domaine de travail nous aide à fournir un meilleur service",email:"E-mail",email_contact:"Votre email de contact",email_invalid:"Le courriel est invalide",email_required:"L'e-mail est requis",fine_tuned_embedding:"Intéressé par des intégrations affinées et adaptées à vos données et à votre cas d'utilisation ? Discutons!",fine_tuned_reranker:"Intéressé par des rerankers affinés et adaptés à vos données et à votre cas d'utilisation ? Discutons!",full_survey:"Répondez à l'enquête complète et obtenez une réponse plus rapide de notre équipe",get_new_key:"Obtenez votre clé API",get_update_blog_posts:"Recevez les dernières mises à jour pour les articles du blog",get_update_embeddings:"Obtenez les dernières mises à jour pour les intégrations",send:"Envoyer",sign_up:"S'inscrire",subscribe:"S'abonner",tell_domain:"Dites-nous votre domaine",usage_type:"Quel type d’utilisation vous décrit le mieux ?",usage_type_options:{other:"Autre",poc:"Preuve de concept",production:"Production",research:"Recherche"},usage_type_required:"Dites-nous que votre type d'utilisation nous aide à fournir un meilleur service",used_product:"Quel modèle utilisez-vous ?",used_product_required:"Sélectionnez le modèle que vous utilisez ou qui vous intéresse"},Ie={description:"Techniques d'agent pour augmenter votre LLM et le pousser au-delà de ses limites"},je="Table des matières",Ce={advance_usage:"Utilisez la requête POST pour plus de fonctionnalités",basic_usage:"Utiliser la requête GET pour compter les jetons",basic_usage_explain:"Vous pouvez simplement envoyer une requête GET pour compter le nombre de jetons dans votre texte.",change_content:"Modifiez le « contenu » et voyez le résultat en direct",chars:"personnages",chinese:"Chinois",chunk:"Gros morceau",chunk_all:"Tous les morceaux",chunking:"Rédiger de longs documents à la vitesse de l'éclair !",chunking_explain:"Vous pouvez également utiliser l'API Segmenter pour découper de longs documents en morceaux plus petits, ce qui facilite leur traitement dans les intégrations ou les reclassements. Nous exploitons des repères structurels courants et créons un ensemble de règles et d'heuristiques qui fonctionnent bien avec divers types de contenu, par exemple les langages Markdown, HTML, LaTeX et CJK.",chunking_short:"Morceautage",chunks_in_total:"{_numChunks} morceaux au total",count_tokens_hint:"<b>{_numTokens}</b> jetons, {_numChars} caractères.",description:"Coupez un long texte en morceaux et effectuez la tokenisation.",description_long:"Notre API Segmenter est essentielle pour aider les LLM à gérer les entrées dans les limites du contexte et à optimiser les performances du modèle. Elle permet aux développeurs de compter les jetons et d'extraire les segments de texte pertinents, garantissant ainsi un traitement efficace des données et une gestion des coûts.",description_long1:"API gratuite pour segmenter un texte long en morceaux et en tokenisation.",english:"Anglais",explain:"Un segmenteur est un composant essentiel qui convertit le texte en jetons ou en morceaux, qui sont les unités de données de base traitées par un modèle d'intégration/de reclassement ou LLM. Les jetons peuvent représenter des mots entiers, des parties de mots ou même des caractères individuels.",faq_v1:{answer1:"L'utilisation de l'API Segmenter est gratuite. En fournissant votre clé API, vous pouvez accéder à une limite de débit plus élevée et votre clé ne sera pas facturée.",answer10:"Outre les langues occidentales, le découpage fonctionne également bien avec le chinois, le japonais et le coréen.",answer2:"Sans clé API, vous pouvez accéder à l'API Segmenter à une vitesse limite de 20 RPM.",answer3:"Avec une clé API, vous pouvez accéder à l'API Segmenter à une vitesse limite de 200 RPM. Pour les utilisateurs payants premium, la limite de vitesse est de 1 000 RPM.",answer4:"Non, votre clé API n'est utilisée que pour accéder à une limite de débit plus élevée.",answer5:"Oui, l'API Segmenter est multilingue et prend en charge plus de 100 langues.",answer6:"Les requêtes GET sont uniquement utilisées pour compter le nombre de jetons dans un texte, ce qui vous permet de l'intégrer facilement comme compteur dans votre application. Les requêtes POST prennent en charge davantage de paramètres et de fonctionnalités, telles que le renvoi des N premiers/derniers jetons.",answer7:"Vous pouvez envoyer jusqu'à 64 000 caractères par demande.",answer8:"La fonction de découpage segmente les documents longs en morceaux plus petits en fonction d'indices structurels communs, garantissant une segmentation précise du texte en morceaux significatifs. Il s'agit essentiellement d'un (gros !) modèle d'expression régulière qui segmente le texte en fonction de certaines caractéristiques syntaxiques qui correspondent souvent à des limites sémantiques, telles que les fins de phrases, les sauts de paragraphe, la ponctuation et certaines conjonctions. Il ne s'agit pas d'un découpage sémantique. Cette (grosse) expression régulière est aussi puissante qu'elle peut l'être dans les limites des expressions régulières. Elle équilibre complexité et performances. Bien qu'une véritable compréhension sémantique ne soit pas possible avec les expressions régulières, elles se rapprochent bien du contexte grâce à des indices structurels communs.",answer9:"Si l'entrée contient des jetons spéciaux, notre API Segmenter les placera dans le champ « special_tokens ». Cela vous permet de les identifier facilement et de les gérer en conséquence pour vos tâches en aval, par exemple en les supprimant avant d'introduire le texte dans un LLM pour éviter les attaques par injection.",question1:"Combien coûte l'API Segmenter ?",question10:"Le chunking prend-il en charge d'autres langues que l'anglais ?",question2:"Si je ne fournis pas de clé API, quelle est la limite de débit ?",question3:"Si je fournis une clé API, quelle est la limite de débit ?",question4:"Allez-vous facturer les jetons à partir de ma clé API ?",question5:"L'API Segmenter prend-elle en charge plusieurs langues ?",question6:"Quelle est la différence entre les requêtes GET et POST ?",question7:"Quelle est la longueur maximale que je peux tokeniser par requête ?",question8:"Comment fonctionne la fonction de fragmentation ? S'agit-il d'une fragmentation sémantique ?",question9:"Comment gérez-vous les jetons spéciaux tels que « endoftext » dans l'API Segmenter ?",title:"Questions courantes liées au segmenteur"},free_api:"L'utilisation de l'API Segmenter est gratuite. En fournissant votre clé API, vous pouvez accéder à une limite de débit plus élevée et votre clé ne sera pas facturée.",input_text:"Saisir du texte",is_free:"L'API Segmenter est gratuite !",is_free_description:"En fournissant votre clé API, vous pouvez accéder à une limite de débit plus élevée et votre clé ne sera pas facturée.",japanese:"japonais",korean:"coréen",parameters:{auth_token:"Ajouter une clé API pour une limite de débit plus élevée",auth_token_explain:"Saisissez votre clé API Jina pour accéder à une limite de débit plus élevée. Pour obtenir les dernières informations sur la limite de débit, veuillez vous référer au tableau ci-dessous.",head:"Renvoyer les N premiers jetons",head_explain:"Renvoie les N premiers jetons du contenu donné. Limite exclusive. Ne peut pas être utilisé avec 'tail'.",learn_more:"Apprendre encore plus",max_chunk_length:"Longueur maximale de chaque morceau",max_chunk_length_explain:"Nombre maximal de caractères dans chaque bloc. En pratique, la longueur du bloc peut être inférieure à cette valeur s'il existe une limite naturelle dans le texte.",return_chunks:"Remettre les morceaux",return_chunks_explain:"Découper l'entrée en segments sémantiquement significatifs tout en gérant une grande variété de types de texte et de cas limites en fonction d'indices structurels communs.",return_tokens:"Remettre les jetons",return_tokens_explain:"Renvoyer les jetons et leurs identifiants correspondants dans la réponse. Basculer pour voir la visualisation du résultat.",tail:"Renvoie les N derniers jetons",tail_explain:"Renvoie les N derniers jetons du contenu donné. Limite exclusive. Ne peut pas être utilisé avec 'head'.",type:"Segmenteur",type_explain:"Choisissez le tokeniseur à utiliser.",used_by_models:"Utilisé dans {_usedBy}."},remove_boundary_cues:"Supprimer les sauts de ligne",remove_boundary_cues_explain:"Supprimez tous les sauts de ligne (les principaux repères de limite) de l'entrée, cela rend le problème plus difficile et voyez comment la réponse change !",show_space:"Afficher les espaces de début/de fin",table:{td_1_0:"Tokenisez les textes, comptez et obtenez les premier/dernier N jetons.",td_1_1:"20 tours/minute",td_1_2:"200 tr/min",td_1_3:"1000 tr/min",td_1_4:"Sans frais",td_1_5:"800 ms"},title:"API de segmentation",token_index:"Index du jeton : {_index}",usage:"Usage",visualization:"Visualisation",what_is:"Qu'est-ce qu'un segmenteur ?"},ke={cta:"Traduire en code {_lang}",select_language:"Langue"},Re={description:"Une base de données vectorielles Python dont vous avez juste besoin - ni plus, ni moins"},we="zzz",Se={PRODUCT_DESCRIPTION:e,SEO_TAG_LINE:s,about_us_page:n,api_general_faq:t,autotune:i,avatar:r,best_banner:a,beta:o,billing_general_faq:l,blog_tags:u,book2024:d,cclicence:c,classifier:p,clip_as_service:m,cloud:g,contact_us_page:v,copy:f,copy_to_clipboard_success:_,dalle_flow:h,deepsearch:q,"dev-gpt":{description:"Votre équipe de développement virtuelle"},disco_art:b,doc_array:x,download:A,embedding:L,embeddings:z,estimator:y,faq:P,faq_button:I,farewell:j,finetuner:C,finetuner_plus:k,finetuning:R,footer:w,get_new_key:S,github:M,grounding:T,header:D,hub:E,huggingface:N,impact_snapshots:U,inference:B,integrations:J,internship_faq:O,internship_page:G,jcloud:V,jerboa:F,jina:Q,jina_chat:W,key_manager:H,lab_dialog:K,landing_page:X,langchain_serve:Y,legal_page:$,model_graph:Z,models:ee,news_page:se,newsroom_page:ne,notice:te,open_day:ie,open_day_faq:re,open_gpt:ae,paywall:oe,powered_by:le,print:ue,project_status:de,prompt_perfect:ce,promptperfect:pe,purchase:me,purchase_now:ge,rate_limit:ve,rationale:fe,reader:_e,recommender:he,reranker:qe,scenex:be,searchbar:xe,searchscape:Ae,semantic:Le,share:ze,spectrum:ye,subscribe_system:Pe,think_gpt:Ie,toc:je,tokenizer:Ce,translator:ke,vectordb:Re,zzz:we};export{e as PRODUCT_DESCRIPTION,s as SEO_TAG_LINE,n as about_us_page,t as api_general_faq,i as autotune,r as avatar,a as best_banner,o as beta,l as billing_general_faq,u as blog_tags,d as book2024,c as cclicence,p as classifier,m as clip_as_service,g as cloud,v as contact_us_page,f as copy,_ as copy_to_clipboard_success,h as dalle_flow,q as deepsearch,Se as default,b as disco_art,x as doc_array,A as download,L as embedding,z as embeddings,y as estimator,P as faq,I as faq_button,j as farewell,C as finetuner,k as finetuner_plus,R as finetuning,w as footer,S as get_new_key,M as github,T as grounding,D as header,E as hub,N as huggingface,U as impact_snapshots,B as inference,J as integrations,O as internship_faq,G as internship_page,V as jcloud,F as jerboa,Q as jina,W as jina_chat,H as key_manager,K as lab_dialog,X as landing_page,Y as langchain_serve,$ as legal_page,Z as model_graph,ee as models,se as news_page,ne as newsroom_page,te as notice,ie as open_day,re as open_day_faq,ae as open_gpt,oe as paywall,le as powered_by,ue as print,de as project_status,ce as prompt_perfect,pe as promptperfect,me as purchase,ge as purchase_now,ve as rate_limit,fe as rationale,_e as reader,he as recommender,qe as reranker,be as scenex,xe as searchbar,Ae as searchscape,Le as semantic,ze as share,ye as spectrum,Pe as subscribe_system,Ie as think_gpt,je as toc,Ce as tokenizer,ke as translator,Re as vectordb,we as zzz};

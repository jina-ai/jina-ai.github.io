{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1：適用於文字和圖像的真正多模態 Embeddings 模型",
  "html": "<p>Jina CLIP v1（<code>jina-clip-v1</code>）是一個新的多模態嵌入模型，它擴展了 OpenAI 的<a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">原始 CLIP 模型</a>的功能。透過這個新模型，使用者只需要一個嵌入模型，就能在純文本和文本-圖像跨模態檢索方面獲得最先進的效能。Jina AI 在純文本檢索方面比 OpenAI CLIP 的效能提升了 165%，在圖像對圖像檢索方面提升了 12%，而在文本對圖像和圖像對文本任務方面的效能則相同或略有提升。這種增強的效能使得 Jina CLIP v1 在處理多模態輸入時不可或缺。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> 在<a href=\"#compare_table\" rel=\"noreferrer\">所有檢索類別</a>中都優於 OpenAI CLIP。</div></div><p>在本文中，我們首先將討論原始 CLIP 模型的缺點，以及我們如何使用獨特的協同訓練方法來解決這些問題。然後，我們將展示我們的模型在各種檢索基準測試上的效果。最後，我們將提供詳細說明，指導使用者如何透過我們的 Embeddings API 和 Hugging Face 開始使用 Jina CLIP v1。</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">用於多模態 AI 的 CLIP 架構</h2><p>2021 年 1 月，OpenAI 發布了 <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a>（對比語言-圖像預訓練）模型。CLIP 擁有簡單但巧妙的架構：它將兩個嵌入模型（一個用於文本，一個用於圖像）組合成單一模型，並共享同一個輸出嵌入空間。其文本和圖像嵌入可以直接相互比較，使得文本嵌入和圖像嵌入之間的距離能夠反映出該文本描述圖像的程度，反之亦然。</p><p>這在多模態信息檢索和零樣本圖像分類中證明非常有用。無需進一步特殊訓練，CLIP 就能很好地將圖像分類到具有自然語言標籤的類別中。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"使用在火星上有紅色月亮的太空人作為示例的圖像到文本翻譯示意圖。\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>原始 CLIP 中的文本嵌入模型是一個只有 6300 萬參數的自定義神經網絡。在圖像方面，OpenAI 發布的 CLIP 提供了一系列 <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> 和 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT 模型</a>。每個模型都經過了各自模態的預訓練，然後通過帶有標題的圖像進行訓練，為準備好的圖像-文本對生成相似的嵌入。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"流程圖，標有「嵌入空間」，連接到「圖像編碼器」和「文本編碼器」，帶有「分心男友」標籤。\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>這種方法產生了令人印象深刻的結果。特別值得注意的是它的零樣本分類效能。例如，即使訓練數據中沒有包含<a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">太空人</a>的標記圖像，CLIP 也能基於其對文本和圖像中相關概念的理解，正確識別太空人的圖片。</p><p>然而，OpenAI 的 CLIP 有兩個重要缺點：</p><ul><li>首先是其文本輸入容量非常有限。它最多可以接受 77 個標記的輸入，但<a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">實證分析顯示</a>，在實際使用中它產生嵌入時使用的標記不超過 20 個。這是因為 CLIP 是從帶有標題的圖像訓練而來，而標題往往非常簡短。這與目前支持數千個標記的文本嵌入模型形成對比。</li><li>其次，在純文本檢索場景中，其文本嵌入的效能非常差。圖像標題是一種非常有限的文本類型，無法反映文本嵌入模型預期支持的廣泛使用場景。</li></ul><p>在大多數實際使用案例中，純文本和圖像-文本檢索是結合使用的，或者至少兩者都可用於任務。為純文本任務維護第二個嵌入模型實際上使 AI 框架的規模和複雜性翻倍。</p><p>Jina AI 的新模型直接解決了這些問題，<code>jina-clip-v1</code> 利用近年來的進展，為涉及文本和圖像模態所有組合的任務帶來了最先進的效能。</p><h2 id=\"introducing-jina-clip-v1\">Jina CLIP v1 簡介</h2><p>Jina CLIP v1 保留了 OpenAI 原始 CLIP 的架構：兩個經過協同訓練以產生相同嵌入空間輸出的模型。</p><p>在文本編碼方面，我們改編了 <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> 架構，該架構用於 <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 模型</a>。這個架構支持最先進的 8k 標記輸入窗口，並輸出 768 維向量，能夠從更長的文本中產生更準確的嵌入。這比原始 CLIP 模型支持的 77 個標記輸入多出 100 多倍。</p><p>在圖像嵌入方面，我們使用了北京人工智能研究院的最新模型：<a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code> 模型</a>。我們已經實證比較了多個圖像 AI 模型，在相似預訓練的跨模態環境中對它們進行測試，<code>EVA-02</code> 明顯優於其他模型。它的模型規模也與 Jina BERT 架構相當，因此圖像和文本處理任務的計算負載大致相同。</p><p>這些選擇為使用者帶來了重要的好處：</p><ul><li>在所有基準測試和所有模態組合上都有更好的效能，特別是在純文本嵌入效能方面有很大提升。</li><li><code>EVA-02</code> 在圖像-文本和純圖像任務上都表現出實證上的優越效能，加上 Jina AI 的額外訓練，提升了純圖像效能。</li><li>支持更長的文本輸入。<a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings 的 8k 標記</a>輸入支持使其能夠處理詳細的文本信息並將其與圖像關聯。</li><li>因為這個多模態模型即使在非多模態場景中也具有高效能，所以在空間、計算、代碼維護和複雜性方面都能節省大量成本。</li></ul><h3 id=\"training\">訓練</h3><p>我們高效能多模態 AI 方案的一部分是我們的訓練數據和程序。我們注意到，圖像標題中使用的文本非常短是 CLIP 類模型在純文本效能不佳的主要原因，我們的訓練明確設計用於解決這個問題。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"流程圖展示了使用模型和編碼器在三個任務中優化文本和標題-圖像相似性\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>訓練分為三個步驟：</p><ol><li>使用帶標題的圖像數據來學習對齊圖像和文本嵌入，並與具有相似含義的文本對交錯進行。這種協同訓練同時優化這兩種任務。在這個階段，模型的純文本效能會下降，但不會像僅使用圖像-文本對訓練那樣嚴重。</li><li>使用合成數據，將圖像與由 AI 模型生成的更長且描述該圖像的文本對齊。同時繼續使用純文本對進行訓練。在這個階段，模型學會了關注與圖像相關的更長文本。</li><li>使用具有<a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">難負樣本</a>的文本三元組來進一步提升純文本效能，通過學習進行更精細的語義區分。同時，繼續使用圖像和長文本的合成對進行訓練。在這個階段，純文本效能顯著提升，而模型不會失去任何圖像-文本能力。</ol><p>如需了解有關訓練和模型架構的更多詳細信息，請閱讀<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">我們最近的論文</a>：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">對比語言-圖像預訓練（CLIP）被廣泛用於訓練模型，通過將圖像和文本映射到固定大小的向量，使其在共同的嵌入空間中對齊。這些模型對多模態信息檢索和相關任務至關重要。然而，CLIP 模型在純文本任務中的表現通常不如專門的文本模型。這導致信息檢索系統需要為純文本和多模態任務分別維護不同的嵌入和模型，造成效率低下。我們提出了一種新穎的多任務對比訓練方法來解決這個問題，並使用它來訓練 jina-clip-v1 模型，在文本-圖像和文本-文本檢索任務上都達到了最先進的性能。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">多模態嵌入的新突破</h2><p>我們評估了 Jina CLIP v1 在純文本、純圖像以及涉及兩種輸入模態的跨模態任務中的表現。我們使用 <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">MTEB 檢索基準</a>來評估純文本性能。對於純圖像任務，我們使用了 <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a> 基準。對於跨模態任務，我們在 <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a> 和 <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a> 上進行評估，這些都包含在 <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a> 中。</p><p>結果總結如下表：</p>\n\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n\n<p>從這些結果中可以看出，<code>jina-clip-v1</code> 在所有類別中都優於 OpenAI 的原始 CLIP，並且在純文本和純圖像檢索方面表現顯著更好。平均來看，性能提升了 46%。</p><p>你可以在<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">我們最近的論文</a>中找到更詳細的評估。</p><h2 id=\"getting-started-with-embeddings-api\">Embeddings API 入門</h2><p>你可以使用 <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings API</a> 輕鬆地將 Jina CLIP v1 整合到你的應用程序中。</p><p>下面的代碼展示了如何使用 Python 中的 <code>requests</code> 包調用 API 來獲取文本和圖像的嵌入。它將文本字符串和圖像 URL 傳遞給 Jina AI 服務器，並返回兩種編碼。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">☝️</div><div class=\"kg-callout-text\">記得將 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> 替換為已激活的 Jina API 密鑰。你可以從 <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina Embeddings 網頁</a>獲得一個包含一百萬個免費令牌的試用密鑰。</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">與主要 LLM 框架的整合</h3><p>Jina CLIP v1 已經可以在 <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> 和 <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a> 中使用：</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>：使用 <code>JinaEmbedding</code> 與 <code>MultimodalEmbedding</code> 基類，並調用 <code>get_image_embeddings</code> 或 <code>get_text_embeddings</code>。</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>：使用 <code>JinaEmbeddings</code>，並調用 <code>embed_images</code> 或 <code>embed_documents</code>。</li></ul><h3 id=\"pricing\">定價</h3><p>文本和圖像輸入都按令牌消耗計費。</p><p>對於英文文本，<a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">我們通過實證計算</a>得出，平均每個詞需要 1.1 個令牌。</p><p>對於圖像，我們計算覆蓋圖像所需的 224x224 像素瓦片數量。某些瓦片可能部分為空白，但計費相同。每個瓦片處理成本為 1,000 個令牌。</p><p><strong>範例</strong></p><p>對於一張 750x500 像素的圖像：</p><ol><li>圖像被分為 224x224 像素的瓦片。<ol><li>計算瓦片數量，將寬度（像素）除以 224，然後向上取整。<br>     750/224 ≈ 3.35 → 4</li><li>對高度（像素）重複相同操作：<br>     500/224 ≈ 2.23 → 3</li></ol></li><li>此範例中所需的瓦片總數為：<br>           4（水平）x 3（垂直）= 12 個瓦片</li><li>成本將為 12 x 1,000 = 12,000 個令牌</li></ol><h3 id=\"enterprise-support\">企業支援</h3><p>我們為購買 <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">110 億令牌</a>生產部署方案的用戶推出新的權益。這包括：</p><ul><li>與我們的產品和工程團隊進行三小時的諮詢，討論您的具體使用案例和需求。</li><li>為您的 RAG（檢索增強生成）或向量搜索用例定制的 Python notebook，展示如何將 Jina AI 的模型整合到您的應用中。</li><li>指派專屬客戶經理和優先電子郵件支援，以確保您的需求得到及時有效的滿足。</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Hugging Face 上的開源 Jina CLIP v1</h2><p>Jina AI 致力於開源搜索基礎，為此，我們在 <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a> 上以 <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">Apache 2.0 許可</a>免費提供此模型。</p><p>你可以在 <code>jina-clip-v1</code> 的 Hugging Face 模型頁面上找到在自己的系統或雲端安裝上下載和運行此模型的示例代碼。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 · Hugging Face</div><div class=\"kg-bookmark-description\">我們正在通過開源和開放科學來推進和民主化人工智能。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">總結</h2><p>Jina AI 的最新模型 — <code>jina-clip-v1</code> — 代表了多模態嵌入模型的重大進展，相比 OpenAI 的 CLIP 提供了顯著的性能提升。在純文本和純圖像檢索任務中取得了顯著改進，同時在文本到圖像和圖像到文本任務中也表現出色，為複雜的嵌入用例提供了一個有前途的解決方案。</p><p>由於資源限制，目前這個模型僅支援英文文字。我們正在努力擴展其功能以支援更多語言。</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzmán",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Jina AI 的新型多模態嵌入模型不僅在文本圖像檢索方面優於 OpenAI CLIP，同時還是一個出色的圖像嵌入模型和最先進的文本嵌入模型。你不再需要為不同模態使用不同的模型了。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
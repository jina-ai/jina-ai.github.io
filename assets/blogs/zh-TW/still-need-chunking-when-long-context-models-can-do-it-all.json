{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "當長上下文模型能夠處理所有內容時，還需要分塊嗎？",
  "html": "<p>在 2023 年 10 月，我們推出了 <code>jina-embeddings-v2</code>，這是第一個能處理長達 8,192 個 tokens 輸入的開源嵌入模型系列。在此基礎上，我們今年發布了 <code>jina-embeddings-v3</code>，在保持相同的輸入長度支援下提供了更多增強功能。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>在本文中，我們將深入探討長文本嵌入，並回答一些問題：將如此大量的文本整合到單個向量中何時是實用的？分段是否能改善檢索效果？如果能，要如何做？在分段文本時如何保留文檔不同部分的上下文？</p><p>為了回答這些問題，我們將比較幾種生成嵌入的方法：</p><ul><li>長文本嵌入（編碼最多 8,192 個 tokens 的文檔）與短文本（即在 192 個 tokens 處截斷）。</li><li>不分塊與簡單分塊和<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">後期分塊</a>的比較。</li><li>在簡單分塊和後期分塊中使用不同的塊大小。</li></ul><h2 id=\"is-long-context-even-useful\">長文本處理真的有用嗎？</h2><p>長文本嵌入模型能夠在單個嵌入中編碼長達十頁的文本，為大規模文本表示開啟了新的可能性。但這真的有用嗎？根據很多人的說法……並不。</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">來源：</span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">How AI Is Built 播客中 Nils Reimer 的引述</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">brainlag 的推文</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">egorfine 在 Hacker News 的評論</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">andy99 在 Hacker News 的評論</span></a></p></figcaption></figure><p>我們將透過深入研究長文本處理能力、長文本何時有用以及何時應該（或不應該）使用它來回應這些疑慮。但首先，讓我們聽聽這些懷疑者的意見，並看看長文本嵌入模型面臨的一些問題。</p><h2 id=\"problems-with-long-context-embeddings\">長文本嵌入的問題</h2><p>假設我們正在為文章建立文件搜索系統，比如我們 <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">Jina AI 部落格</a>上的文章。有時一篇文章可能涵蓋多個主題，像是我們<a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">關於參加 ICML 2024 會議的報告</a>，其中包含：</p><ul><li>介紹部分，涵蓋有關 ICML 的一般資訊（參與人數、地點、範圍等）。</li><li>我們的工作展示（<code>jina-clip-v1</code>）。</li><li>在 ICML 上展示的其他有趣研究論文的摘要。</li></ul><p>如果我們只為這篇文章創建一個嵌入，那麼這個嵌入就代表了三個不同主題的混合：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">圖 1：當對涵蓋多個主題的文檔進行嵌入時，產生的向量代表了所有段落的混合，可能會失去每個獨立段落中包含的明確、特定資訊。</span></figcaption></figure><p>這會導致幾個問題：</p><ul><li><strong>表示稀釋：</strong>雖然文本中的所有主題可能都有關聯，但用戶的搜索查詢可能只與其中一個相關。然而，單個嵌入（在這種情況下，是整篇部落格文章的嵌入）在向量空間中只是一個點。隨著更多文本被加入模型的輸入中，嵌入會轉向捕捉文章的整體主題，使其在表示特定段落內容時效果降低。</li><li><strong>容量限制：</strong>嵌入模型產生固定大小的向量，與輸入長度無關。隨著輸入內容的增加，模型在向量中表示所有這些資訊變得更加困難。這就像把一張圖片縮小到 16×16 像素 — 如果你縮小一個簡單物體的圖片，比如蘋果，你仍然可以從縮小的圖片中得到意義。但如果是縮小柏林的街道地圖？那就不行了。</li><li><strong>資訊損失：</strong>在某些情況下，即使是長文本嵌入模型也會達到其極限；許多模型支援最多 8,192 個 tokens 的文本編碼。更長的文檔需要在嵌入前被截斷，導致資訊損失。如果與用戶相關的資訊位於文檔的末尾，它根本就不會被嵌入捕捉到。</li><li><strong>你可能需要文本分段：</strong>某些應用需要文本特定段落的嵌入而不是整個文檔的嵌入，比如在文本中識別相關段落。</li></ul><h2 id=\"long-context-vs-truncation\">長文本與截斷的比較</h2><p>為了看看長文本處理是否真的有價值，讓我們來看兩種檢索場景的表現：</p><ul><li>編碼最多 8,192 個 tokens 的文檔（約 10 頁文本）。</li><li>在 192 個 tokens 處截斷文檔並進行編碼。</li></ul><p>我們將使用以下方法來比較結果：使用 <code>jina-embeddings-v3</code> 的 nDCG@10 檢索指標。我們測試了以下數據集：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>數據集</th>\n<th>描述</th>\n<th>查詢示例</th>\n<th>文檔示例</th>\n<th>平均文檔長度（字符）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>一個全文醫療檢索數據集，包含 3,244 個查詢，文檔主要來自 PubMed。</td>\n<td>\"使用飲食治療哮喘和濕疹\"</td>\n<td>\"他汀類藥物使用與乳腺癌存活率：芬蘭全國隊列研究 最近的研究表明 [...]\"</td>\n<td>326,753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>一個基於查詢的會議摘要數據集，需要對相關會議片段進行摘要。</td>\n<td>\"是教授提出這個問題並建議使用知識工程技巧 [...]\"</td>\n<td>\"項目經理：現在可以嗎？{語音聲音} 好的。抱歉？好的，大家都準備好開始會議了嗎？[...]\"</td>\n<td>37,445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>包含長篇故事及其相關特定內容問題的問答數據集。</td>\n<td>\"Sophia 在巴黎擁有什麼樣的生意？\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53,336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>一個多跳問答數據集，最多需要 5 個推理步驟，使用模板設計以避免捷徑。</td>\n<td>\"The Seeker（The Who 樂隊的歌曲）的作曲家獲得了什麼獎項？\"</td>\n<td>\"段落 1：\\nMargaret，布里恩伯爵夫人\\nMarguerite d'Enghien（出生於 1365 年 - 逝世於 1394 年後），是統治者 suo jure [...]\"</td>\n<td>30,854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>一個需要整合分散劇情的電視劇劇本摘要數據集，包含電視劇劇本和摘要。</td>\n<td>\"Penny 得到了一把新椅子，Sheldon 很喜歡直到他發現她是從 [...]\"</td>\n<td>\"[外景。拉斯維加斯城市（庫存）- 夜晚]\\n[外景。阿伯納西住宅 - 車道 -- 夜晚]\\n（路燈柱上的燈 [...]\"</td>\n<td>1,613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>我們可以看到，編碼超過 192 個 token 可以帶來顯著的性能提升：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 2：長文本嵌入性能與短文本嵌入性能的比較</span></figcaption></figure><p>然而，在不同數據集上，我們看到的改進程度不一：</p><ul><li>對於 <strong>NFCorpus</strong>，截斷幾乎沒有影響。這是因為標題和摘要都在文檔的開頭，而這些內容與典型的用戶搜索詞高度相關。無論是否截斷，最相關的資料都在 token 限制範圍內。</li><li><strong>QMSum</strong> 和 <strong>NarrativeQA</strong> 被視為\"閱讀理解\"任務，用戶通常在文本中搜索特定事實。這些事實經常分散在文檔的各個細節中，可能超出 192 token 的截斷限制。例如，在 NarrativeQA 文檔《Percival Keene》中，\"誰是偷 Percival 午餐的惡霸？\"這個問題的答案遠超出了這個限制。同樣，在 <strong>2WikiMultiHopQA</strong> 中，相關信息分散在整個文檔中，需要模型在多個章節中導航和綜合知識以有效回答查詢。</li><li><strong>SummScreenFD</strong> 是一個旨在識別與給定摘要相對應的劇本的任務。由於摘要包含了分散在整個劇本中的信息，編碼更多的文本可以提高摘要與正確劇本匹配的準確性。</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">分段文本以提升檢索性能</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">接下來，我們討論三個相似的概念。為避免混淆，我們將它們定義如下：<br>• <b><strong style=\"white-space: pre-wrap;\">分段（Segmentation）</strong></b>：在輸入文本中檢測邊界標記，例如句子或固定數量的 token。<br>• <b><strong style=\"white-space: pre-wrap;\">簡單分塊（Naive chunking）</strong></b>：在編碼之前根據分段標記將文本分成塊。<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">延遲分塊（Late chunking）</strong></b></a>：先編碼文檔，然後再進行分段（保留塊之間的上下文）。</div></div><p>我們可以使用各種方法來首先對文檔進行分段，而不是將整個文檔嵌入到一個向量中：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 3：對文本片段應用\"固定大小\"、\"基於句子\"和\"語義\"分塊方法</span></figcaption></figure><p>一些常見的方法包括：</p><ul><li><strong>固定大小分段：</strong>文檔被分成固定數量 token 的片段，這個數量由嵌入模型的分詞器決定。這確保了片段的分詞與整個文檔的分詞相對應（按特定字符數分段可能導致不同的分詞結果）。</li><li><strong>按句子分段：</strong>文檔被分成句子，每個塊由 <em>n</em> 個句子組成。</li><li><strong>按語義分段：</strong>每個片段對應多個句子，嵌入模型決定連續句子的相似性。具有高嵌入相似性的句子被分配到同一個塊中。</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">你可以使用 <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a>（我們的免費 API）輕鬆地執行分段，它可以根據文檔結構將長文本分成塊並進行分詞。</div></div><p>為了簡單起見，我們在本文中使用固定大小分段。</p><h3 id=\"document-retrieval-using-naive-chunking\">使用簡單分塊進行文檔檢索</h3><p>一旦我們執行了固定大小分段，我們可以根據這些片段對文檔進行簡單分塊：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 4：基於分段過程中檢測到的邊界標記進行簡單分塊。</span></figcaption></figure><p>使用 <code>jina-embeddings-v3</code>，我們將每個塊編碼成準確捕捉其語義的嵌入向量，然後將這些嵌入存儲在向量數據庫中。</p><p>在運行時，模型將用戶的查詢編碼為查詢向量。我們將其與向量數據庫中的塊嵌入進行比較，以找到具有最高餘弦相似度的塊，然後將相應的文檔返回給用戶：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 5：使用樸素分塊實現的文檔檢索：(1) 根據邊界提示將文檔集合分割成塊，(2) embedding 模型對所有塊進行編碼，並將產生的 embeddings 存儲在數據庫中，(3) 當查詢進來時，embedding 模型對其進行編碼，數據庫確定最相似的塊。最後，我們從數據庫中為該塊存儲的文檔 ID 識別相關文檔並將其返回給用戶。</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">樸素分塊的問題</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 6：當把文本分成句子時，無法解析對文本早期部分的引用。</span></figcaption></figure><p>雖然樸素分塊解決了長上下文 embedding 模型的一些限制，但它也有其缺點：</p><ul><li><strong>無法看到全局：</strong>在文檔檢索中，小塊的多個 embeddings 可能無法捕捉文檔的整體主題。這就像只見樹木而不見森林。</li><li><strong>缺失上下文問題：</strong>如圖 6 所示，由於缺少上下文信息，無法準確解釋這些塊。</li><li><strong>效率：</strong>更多的塊需要更多的存儲空間，並增加檢索時間。</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">後期分塊解決了上下文問題</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">為了解決缺失上下文的問題，我們提出了一種新方法，稱為「後期分塊」，在我們之前的博客文章中有詳細描述：<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">第一部分</strong></b></a>、<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">第二部分</strong></b></a>、<a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">第三部分</strong></b></a>、<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">研究論文</strong></b></a>。</div></div><p>後期分塊分兩個主要步驟進行：</p><ol><li>首先，它利用模型的長上下文能力將整個文檔編碼為 token embeddings。這保留了文檔的完整上下文。</li><li>然後，它通過對特定序列的 token embeddings 進行平均池化來創建塊 embeddings，這些序列對應於在分割過程中識別的邊界提示。</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 7：後期分塊與樸素分塊的比較。</span></figcaption></figure><p>這種方法的主要優勢在於 token embeddings 是上下文化的 —— 意味著它們自然捕捉了與文檔其他部分的引用和關係。由於 embedding 過程發生在分塊之前，每個塊都保留了對更廣泛文檔上下文的認知，從而解決了困擾樸素分塊方法的上下文缺失問題。</p><p>對於超過模型最大輸入大小的文檔，我們可以使用「長後期分塊」：</p><ol><li>首先，我們將文檔分解為重疊的「宏塊」。每個宏塊的大小都適合模型的最大上下文長度（例如，8,192 個 tokens）。</li><li>模型處理這些宏塊以創建 token embeddings。</li><li>一旦我們有了 token embeddings，我們就進行標準的後期分塊 —— 應用平均池化來創建最終的塊 embeddings。</li></ol><p>這種方法允許我們處理任何長度的文檔，同時仍然保持後期分塊的優勢。可以將其視為兩階段過程：首先使文檔適合模型處理，然後應用常規的後期分塊程序。</p><p>簡而言之：</p><ul><li><strong>樸素分塊：</strong>將文檔分割成小塊，然後分別對每個塊進行編碼。</li><li><strong>後期分塊：</strong>一次性對整個文檔進行編碼以創建 token embeddings，然後基於分段邊界通過池化 token embeddings 來創建塊 embeddings。</li><li><strong>長後期分塊：</strong>將大型文檔分割成適合模型上下文窗口的重疊宏塊，對這些塊進行編碼以獲得 token embeddings，然後像正常一樣應用後期分塊。</li></ul><p>如需該想法的更詳細描述，請查看我們的<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">論文</a>或上述提到的博客文章。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">要分塊還是不分塊？</h2><p>我們已經看到長上下文 embedding 通常優於較短文本 embeddings，並概述了樸素分塊和後期分塊策略。現在的問題是：分塊是否比長上下文 embedding 更好？</p><p>為了進行公平比較，我們在開始分割之前將文本值截斷到模型的最大序列長度（8,192 tokens）。我們使用固定大小的分割，每段 64 個 tokens（對於樸素分割和後期分塊都是如此）。讓我們比較三種情況：</p><ul><li><strong>不分割：</strong>我們將每個文本編碼成單個 embedding。這導致與之前的實驗（見圖 2）相同的分數，但我們在這裡包含它們以便更好地比較。</li><li><strong>樸素分塊：</strong>我們對文本進行分割，然後根據邊界提示應用樸素分塊。</li><li><strong>後期分塊：</strong>我們對文本進行分割，然後使用後期分塊來確定 embeddings。</li></ul><p>對於後期分塊和樸素分割，我們使用塊檢索來確定相關文檔（如本文前面的圖 5 所示）。</p><p>結果顯示沒有明顯的贏家：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 8：不分塊 vs 樸素分塊 vs 後期分塊</span></figcaption></figure><ul><li><strong>對於事實檢索，樸素分塊表現更好：</strong>對於 QMSum、NarrativeQA 和 2WikiMultiHopQA 數據集，模型必須識別文檔中的相關段落。在這裡，樸素分塊明顯優於將所有內容編碼為單個 embedding，因為可能只有少數塊包含相關信息，而這些塊比整個文檔的單個 embedding 能更好地捕捉這些信息。</li><li><strong>後期分塊在連貫的文檔和相關上下文中效果最佳：</strong>對於涵蓋連貫主題且用戶搜索整體主題而非具體事實的文檔（如 NFCorpus），後期分塊略優於不分塊，因為它平衡了文檔範圍的上下文和局部細節。然而，雖然後期分塊通過保留上下文通常比簡單分塊效果更好，但當在包含大量無關信息的文檔中搜索孤立事實時，這種優勢可能成為負擔 - 正如在 NarrativeQA 和 2WikiMultiHopQA 的性能下降中所見，增加的上下文反而會造成更多干擾。</li></ul><h3 id=\"does-chunk-size-make-a-difference\">分塊大小是否有影響？</h3><p>分塊方法的有效性實際上取決於數據集，這突顯了內容結構扮演著關鍵角色：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">圖 9：簡單分塊和後期分塊的不同分塊大小比較。</span></figcaption></figure><p>如我們所見，在較小的分塊大小下，後期分塊通常優於簡單分塊，因為較小的簡單分塊包含的上下文太少，而較小的後期分塊保留了整個文檔的上下文，使其在語義上更有意義。NarrativeQA 數據集是個例外，因為其中包含太多無關的上下文，導致後期分塊表現較差。在較大的分塊大小下，簡單分塊顯示出明顯的改善（有時甚至超過後期分塊），這是由於增加的上下文，而後期分塊的性能則逐漸下降。</p><h2 id=\"takeaways-when-to-use-what\">要點：何時使用什麼？</h2><p>在這篇文章中，我們研究了不同類型的文檔檢索任務，以更好地理解何時使用分段以及何時後期分塊有幫助。那麼，我們學到了什麼？</p><h3 id=\"when-should-i-use-long-context-embedding\">何時應該使用長上下文嵌入？</h3><p>一般來說，在嵌入模型的輸入中包含盡可能多的文檔文本不會損害檢索準確性。然而，長上下文嵌入模型通常關注文檔的開頭，因為它們包含標題和介紹等對判斷相關性更重要的內容，但模型可能會錯過文檔中間的內容。</p><h3 id=\"when-should-i-use-naive-chunking\">何時應該使用簡單分塊？</h3><p>當文檔涵蓋多個方面，或用戶查詢針對文檔中的特定信息時，分塊通常可以提高檢索性能。</p><p>最終，分段決策取決於多個因素，如是否需要向用戶顯示部分文本（例如 Google 在搜索結果預覽中顯示相關段落），這使得分段變得必要，或者計算和內存的限制，在這種情況下，由於增加了檢索開銷和資源使用，分段可能較不利。</p><h3 id=\"when-should-i-use-late-chunking\">何時應該使用後期分塊？</h3><p>透過在創建分塊之前對整個文檔進行編碼，後期分塊解決了文本段因缺少上下文而失去意義的問題。這在每個部分都與整體相關的連貫文檔中特別有效。我們的實驗表明，後期分塊在將文本分成較小的分塊時特別有效，正如我們的<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">論文</a>所示。然而，有一個注意事項：如果文檔的部分內容彼此無關，包含這個更廣泛的上下文實際上可能會使檢索性能變差，因為它為嵌入增加了噪音。</p><h2 id=\"conclusion\">結論</h2><p>在長上下文嵌入、簡單分塊和後期分塊之間的選擇取決於您的檢索任務的具體要求。長上下文嵌入對於具有一般查詢的連貫文檔很有價值，而分塊在用戶尋找文檔中的特定事實或信息的情況下表現出色。後期分塊通過在較小的段落中保留上下文連貫性進一步提升了檢索效果。最終，了解您的數據和檢索目標將指導最佳方法，在準確性、效率和上下文相關性之間取得平衡。</p><p>如果您正在探索這些策略，可以考慮嘗試 <code>jina-embeddings-v3</code> —— 其先進的長上下文功能、後期分塊和靈活性使其成為多樣化檢索場景的絕佳選擇。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3：前沿多語言嵌入模型</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 是一個具有 570M 參數和 8192 token 長度的前沿多語言文本嵌入模型，在 MTEB 上的表現優於 OpenAI 和 Cohere 最新的專有嵌入。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "比較不同切分策略對長文本嵌入模型的性能表現，以找出最適合你需求的方法。",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
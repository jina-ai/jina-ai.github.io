{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "深入理解 Late Chunking：真相與誤解（下篇）",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">強烈建議先<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">閱讀第一部分</a>，因為本文提供了更深入的視角，主要針對常見誤解和比較。<b><strong style=\"white-space: pre-wrap;\">建議閱讀順序：</strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">第一部分</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、第二部分、</strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">研究論文</strong></b></a><b><strong style=\"white-space: pre-wrap;\">。</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking：使用長上下文嵌入模型的上下文塊嵌入</div><div class=\"kg-bookmark-description\">許多使用案例需要檢索較小的文本段落，而基於密集向量的檢索系統通常在處理較短的文本段落時表現更好，因為語義較不容易在嵌入中被過度壓縮。因此，實踐者經常將文本文件拆分成較小的塊並分別進行編碼。然而，以這種方式創建的塊嵌入可能會失去來自周圍塊的上下文信息，導致次優表示。在本文中，我們介紹了一種稱為 late chunking 的新方法，該方法首先嵌入長文本的所有標記，在 transformer 模型之後、平均池化之前才進行分塊 - 因此稱為\"late\"。由此產生的塊嵌入捕捉了完整的上下文信息，在各種檢索任務中取得了優異的結果。該方法具有通用性，可應用於各種長上下文嵌入模型，且無需額外訓練。為了進一步提高 late chunking 的效果，我們提出了一種專用的嵌入模型微調方法。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>長文件分塊存在兩個問題：首先是<strong>確定斷點</strong>——即如何對文件進行分段。你可能會考慮固定的 token 長度、固定的句子數量，或更進階的技術，如<a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">正則表達式或語義分段模型</a>。準確的分塊邊界不僅可以改善搜尋結果的可讀性，還能確保送入 LLM 的 RAG 系統中的分塊是精確且充分的——不多不少。</p><p>第二個問題是每個分塊中的<strong>上下文損失</strong>。一旦文件被分段，大多數人的下一個邏輯步驟是在批處理中分別嵌入每個分塊。然而，這會導致原始文件的全局上下文喪失。許多先前的研究首先解決第一個問題，認為更好的邊界檢測可以改善語義表示。例如，\"語義分塊\"將嵌入空間中具有高餘弦相似度的句子分組，以最小化語義單元的破壞。</p><p>從我們的觀點來看，這兩個問題<em>幾乎</em>是正交的，可以分別解決。如果必須優先處理，<strong>我們認為第二個問題更為關鍵。</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">問題 2：<b>上下文信息</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">保留</td>\n<td>喪失</td>\n</tr>\n<tr>\n<td><b>問題 1：斷點</b></td>\n<td>好</td>\n<td style=\"text-align:center\">理想情況</td>\n<td>搜尋結果差</td>\n</tr>\n<tr>\n<td></td>\n<td>差</td>\n<td style=\"text-align:center\">搜尋結果好，但結果可能不易讀或不適合 LLM 推理</td>\n<td>最糟糕的情況</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"late-chunking-for-context-loss\">Late Chunking 解決上下文損失</h2><p><strong>Late chunking</strong> 首先解決第二個問題：<strong>上下文損失</strong>。它<em>不是</em>為了找到理想的斷點或語義邊界。你仍然需要使用正則表達式、啟發式方法或其他技術將長文件分割成小塊。但是，與其在分段後立即嵌入每個分塊，late chunking 首先在一個上下文視窗中編碼整個文件（對於 <code>jina-embeddings-v3</code> 是 8192-token）。然後，它按照邊界提示對每個分塊進行平均池化——因此在 late chunking 中使用\"late\"一詞。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagram comparing &quot;Naive Chunking&quot; and &quot;Late Chunking&quot; methods for processing long documents with labeled steps.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Late chunking 仍然需要邊界提示，但關鍵區別在於這些提示的使用時機。在 late chunking 中，只有在整個文件被嵌入後才應用這些提示，它們用於確定池化範圍。</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">Late Chunking 對不良邊界提示具有彈性</h2><p>真正有趣的是，實驗表明 late chunking 消除了對完美語義邊界的需求，這部分解決了上述第一個問題。事實上，應用於固定 token 邊界的 late chunking 表現優於具有語義邊界提示的原始分塊。當與 late chunking 配對時，像使用固定長度邊界這樣的簡單分段模型，其表現與進階邊界檢測算法相當。我們測試了三種不同大小的嵌入模型，結果表明它們在所有測試數據集中都一致地從 late chunking 中受益。話雖如此，嵌入模型本身仍然是表現最重要的因素——<strong>沒有任何情況下，使用 late chunking 的較弱模型會優於不使用它的較強模型。</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Scatter plot chart showing the percentage of relative improvements across various models against a baseline, with a vertical \" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">相對於基準的檢索改進（即使用固定 token 長度邊界提示和原始分塊的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">）。作為消融研究的一部分，我們用不同的邊界提示（固定 token 長度、句子邊界和語義邊界）和不同的模型（</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\">和 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">）測試了 late chunking。基於它們在 MTEB 上的表現，這三個嵌入模型的排名是：</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">。然而，這個實驗的重點不在於評估嵌入模型本身的性能，而是在於理解更好的嵌入模型如何與後期分塊和邊界提示互動。關於實驗的細節，請參閱我們的研究論文。</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>請注意，對不良邊界具有彈性並不意味著我們可以忽視它們—它們對於人類和 LLM 的可讀性仍然很重要。我們是這樣看待這個問題的：在優化分段（即前面提到的第一個問題）時，我們可以完全專注於可讀性，而不必擔心語義/上下文的損失。後期分塊可以處理好或壞的斷點，所以你只需要關注可讀性。</p><h2 id=\"late-chunking-is-bidirectional\">後期分塊是雙向的</h2><p>關於後期分塊的另一個常見誤解是，其條件塊嵌入僅依賴於先前的塊而不會\"向前看\"。這是不正確的。實際上，<strong>後期分塊中的條件依賴是雙向的</strong>，而不是單向的。這是因為嵌入模型（一個僅編碼器的 transformer）中的注意力矩陣是完全連接的，不像自回歸模型中使用的遮罩三角矩陣。形式上，第 k 個塊的嵌入 $v_k \\sim Q(c_k|D)$，而不是 $v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$，其中 $Q$ 表示語言模型的因子分解。這也解釋了為什麼後期分塊不依賴於精確的邊界放置。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagrams of a transformer model with detailed encoder on the left and decoder on the right, labeled with tokens, embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">與使用遮罩自注意力的僅解碼器模型不同，嵌入模型通常是僅編碼器的，具有完整的注意力矩陣。這意味著每個標記嵌入都受同一上下文窗口內所有其他標記的條件影響，對於 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">，這包括最多 8191 個其他標記。因此，塊嵌入在兩個方向上都攜帶全局上下文信息。</span></figcaption></figure><h2 id=\"late-chunking-can-be-trained\">後期分塊可以被訓練</h2><p>後期分塊<em>不</em>需要對嵌入模型進行額外訓練。它可以應用於任何使用平均池化的長上下文嵌入模型，這使它對實踐者來說非常有吸引力。話雖如此，如果你在處理問答或查詢文檔檢索等任務，通過一些微調仍然可以進一步提高性能。具體來說，訓練數據由以下元組組成：</p><ul><li><strong>查詢</strong>（例如，問題或搜索詞）。</li><li>包含相關信息以回答查詢的<strong>文檔</strong>。</li><li>文檔中的<strong>相關片段</strong>，這是直接回答查詢的特定文本塊。</li></ul><p>模型通過將查詢與其相關片段配對進行訓練，使用像 InfoNCE 這樣的對比損失函數。這確保了相關片段在嵌入空間中與查詢緊密對齊，而不相關的片段則被推得更遠。因此，模型學會在生成塊嵌入時專注於文檔中最相關的部分。<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">更多詳情，請參考我們的研究論文。</a></p><h2 id=\"late-chunking-vs-contextual-retrieval\">後期分塊 vs 上下文檢索</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Introducing Contextual Retrieval</div><div class=\"kg-bookmark-description\">Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>在後期分塊推出後不久，Anthropic 引入了一個稱為<strong>上下文檢索</strong>的獨立策略。Anthropic 的方法是一種解決上下文丟失問題的暴力方法，其工作方式如下：</p><ol><li>每個塊都與完整文檔一起發送給 LLM。</li><li>LLM 為每個塊添加相關上下文。</li><li>這產生更豐富和更具信息性的嵌入。</li></ol><p>在我們看來，這本質上是<strong>上下文豐富化</strong>，其中使用 LLM 將全局上下文明確地硬編碼到每個塊中，這在<strong>成本</strong>、<strong>時間</strong>和<strong>存儲</strong>方面都很昂貴。此外，目前還不清楚這種方法是否對塊邊界具有彈性，因為 LLM 依賴於準確和可讀的塊來有效地豐富上下文。相比之下，如上所示，後期分塊對邊界提示具有高度彈性。由於嵌入大小保持不變，它不需要額外的存儲空間。儘管利用了嵌入模型的完整上下文長度，<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">它仍然比使用 LLM 生成豐富化要快得多</a>。在我們研究論文的定性研究中，<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">我們展示了 Anthropic 的上下文檢索與後期分塊的性能相似</a>。然而，後期分塊通過利用僅編碼器 transformer 的固有機制，提供了一個更低級、更通用和更自然的解決方案。</p><h2 id=\"which-embedding-models-support-late-chunking\">哪些嵌入模型支持後期分塊？</h2><p>後期分塊並不是 <code>jina-embeddings-v3</code> 或 <code>v2</code> 的專利。它是一種相當通用的方法，可以應用於任何使用平均池化的長上下文嵌入模型。例如，在這篇文章中，我們展示了 <code>nomic-v1</code> 也支持它。我們熱烈歡迎所有嵌入提供商在他們的解決方案中實現對後期分塊的支持。</p><p>作為模型使用者，在評估新的嵌入模型或 API 時，你可以按照以下步驟檢查它是否可能支持後期分塊：</p><ol><li><strong>單一輸出</strong>：模型/API 是否只提供每句話的一個最終 embedding，而不是 token 級別的 embeddings？如果是，那麼它可能無法支援延遲分塊（特別是對於那些網路 API）。</li><li><strong>長文本支援</strong>：該模型/API 是否能處理至少 8192 個 tokens 的上下文？如果不能，延遲分塊就不適用——更準確地說，為短上下文模型改裝延遲分塊是沒有意義的。如果可以，請確保它在處理長文本時確實表現良好，而不是僅僅宣稱支援。你通常可以在模型的技術報告中找到這些資訊，例如在 LongMTEB 或其他長文本基準測試上的評估。</li><li><strong>平均池化</strong>：對於自託管模型或在池化前提供 token 級別 embeddings 的 API，檢查其預設池化方法是否為平均池化。使用 CLS 或最大池化的模型與延遲分塊不相容。</li></ol><p>總之，如果一個 embedding 模型支援長文本且預設使用平均池化，它就能輕鬆支援延遲分塊。查看我們的 <a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">GitHub 儲存庫以獲取實現細節和進一步討論</a>。</p><h2 id=\"conclusion\">結論</h2><p>那麼，什麼是延遲分塊？延遲分塊是一種使用長文本 embedding 模型生成塊 embeddings 的直接方法。它快速、對邊界線索具有彈性，且高效。這不是一種啟發式方法或過度工程——而是一種基於對 transformer 機制深入理解的精心設計。</p><p>如今，圍繞 LLM 的炒作是不可否認的。在許多情況下，可以由像 BERT 這樣的較小模型有效解決的問題，反而被交給了 LLM 處理，這是由於人們對更大、更複雜解決方案的迷戀。大型 LLM 提供商推動其模型的更廣泛採用，而 embedding 提供商則提倡使用 embeddings，這並不令人驚訝——他們都在發揮各自的商業優勢。但最終，重要的不是炒作，而是行動，是什麼真正有效。讓社群、產業，最重要的是時間，來揭示哪種方法才是真正更精簡、更高效，且經得起時間考驗的。</p><p>請務必閱讀<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">我們的研究論文</a>，我們也鼓勵你在各種場景下對延遲分塊進行基準測試，並與我們分享你的反饋。</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "探索 Late Chunking 系列的第二部分，深入探討為何它是進行塊狀嵌入和提升搜尋／RAG 效能的最佳方法。",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "二進位嵌入：擁有所有 AI 功能，只需 3.125% 的容量",
  "html": "<p>Embeddings 已成為多種人工智慧和自然語言處理應用的基石，提供了一種將文本含義表示為高維向量的方法。然而，隨著模型規模的增長和人工智慧模型處理的數據量增加，傳統 embeddings 的計算和儲存需求也隨之攀升。Binary embeddings 作為一種緊湊高效的替代方案被引入，在大幅降低資源需求的同時仍能保持高性能。</p><p>Binary embeddings 是一種降低資源需求的方法，可以將 embedding 向量的大小減少高達 96%（在 Jina Embeddings 的情況下可達 96.875%）。用戶可以在其人工智慧應用中利用緊湊的 binary embeddings 的優勢，同時將精確度損失降到最低。</p><h2 id=\"what-are-binary-embeddings\">什麼是 Binary Embeddings？</h2><p>Binary embeddings 是一種特殊的數據表示形式，將傳統的高維浮點向量轉換為二進制向量。這不僅壓縮了 embeddings，還保留了幾乎所有向量的完整性和實用性。這種技術的精髓在於即使在轉換後仍能保持數據點之間的語義和關係距離。<br><br>Binary embeddings 背後的魔力在於量化，這是一種將高精度數字轉換為低精度數字的方法。在人工智慧建模中，這通常意味著將 embeddings 中的 32 位浮點數轉換為較少位元的表示形式，如 8 位整數。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">二值化是將所有標量值轉換為 0 或 1 的過程，就像將彩色圖像轉換為僅有黑白像素的圖像。圖片：神奈川沖浪裏（1831）by 葛飾（Hokusai）</span></figcaption></figure><p>Binary embeddings 將這一過程推向極致，將每個值簡化為 0 或 1。將 32 位浮點數轉換為二進制位元可以將 embedding 向量的大小減少 32 倍，減少幅度達 96.875%。因此，在轉換後的 embeddings 上進行向量運算會快得多。在某些微晶片上使用硬體加速可以使二值化向量的比較速度提升超過 32 倍。</p><p>在這個過程中難免會損失一些資訊，但當模型性能很好時，這種損失可以降到最低。如果不同事物的非量化 embeddings 有最大的差異，那麼二值化更有可能很好地保留這種差異。否則，就很難正確解釋這些 embeddings。</p><p>Jina Embeddings 模型在這方面訓練得非常穩健，使其特別適合二值化。</p><p>這種緊湊的 embeddings 使得新的人工智慧應用成為可能，特別是在資源受限的環境中，如移動設備和時間敏感的應用場景。</p><p>如下圖所示，這些成本和運算時間的優勢僅帶來相對較小的性能損失。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10：使用</em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">標準化折損累計增益</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">計算前 10 個結果的分數。</em></i></figcaption></figure><p>對於 <code>jina-embeddings-v2-base-en</code>，二值量化將檢索準確率從 47.13% 降低到 42.05%，損失約 10%。對於 <code>jina-embeddings-v2-base-de</code>，這種損失僅為 4%，從 44.39% 降至 42.65%。</p><p>Jina Embeddings 模型在生成二進制向量時表現如此出色，是因為它們經過訓練可以創建更均勻分布的 embeddings。這意味著與其他模型的 embeddings 相比，兩個不同的 embeddings 在更多維度上可能會相距更遠。這種特性確保了這些距離能更好地通過二進制形式來表示。</p><h2 id=\"how-do-binary-embeddings-work\">Binary Embeddings 如何運作？</h2><p>為了理解其工作原理，考慮三個 embeddings：<em>A</em>、<em>B</em> 和 <em>C</em>。這三個都是完整的浮點向量，而不是二值化的向量。現在，假設 <em>A</em> 到 <em>B</em> 的距離大於 <em>B</em> 到 <em>C</em> 的距離。對於 embeddings，我們通常使用<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">餘弦距離</a>，所以：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>如果我們將 <em>A</em>、<em>B</em> 和 <em>C</em> 二值化，我們可以使用<a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">漢明距離</a>更有效地測量距離。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">立方體上的漢明距離。左：A 到 B 的距離為 1。右：B 到 C 的距離為 2。</span></figcaption></figure><p>讓我們把 <em>A</em>、<em>B</em> 和 <em>C</em> 的二值化版本稱為 <em>A<sub>bin</sub></em>、<em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em>。</p>\n<p>對於二進制向量，如果 <em>A<sub>bin</sub></em> 和 <em>B<sub>bin</sub></em> 之間的餘弦距離大於 <em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em> 之間的距離，那麼 <em>A<sub>bin</sub></em> 和 <em>B<sub>bin</sub></em> 之間的漢明距離大於或等於 <em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em> 之間的漢明距離。</p>\n<p>所以如果：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>那麼對於漢明距離：</p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>理想情況下，當我們對 embeddings 進行二值化時，我們希望完整 embeddings 之間的相同關係在二進制 embeddings 中也成立。這意味著如果一個浮點餘弦距離大於另一個，那麼它們二值化版本之間的漢明距離也應該更大：</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>我們無法對所有 embeddings 三元組都實現這一點，但我們可以使其對幾乎所有三元組都成立。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">藍點對應完整浮點向量，紅點對應其二值化版本。</span></figcaption></figure><p>對於二進制向量，我們可以將每個維度視為存在（1）或不存在（0）。兩個向量在非二進制形式中的距離越遠，在任一維度上一個有正值而另一個有負值的機率就越高。這意味著在二進制形式中，很可能會有更多維度中一個是 0 而另一個是 1。這使它們在漢明距離上相距更遠。</p><p>相反的情況適用於距離較近的向量：非二進制向量越接近，在任何維度上都是 0 或都是 1 的機率就越高。這使它們在漢明距離上更接近。</p><p>Jina Embeddings 模型之所以特別適合二值化，是因為我們使用負面挖掘和其他微調實踐來訓練它們，特別是增加不相似事物之間的距離並減少相似事物之間的距離。這使得 embeddings 更穩健，對相似性和差異性更敏感，並使二進制 embeddings 之間的漢明距離與非二進制之間的餘弦距離更成比例。</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">使用 Jina AI 的 Binary Embeddings 可以節省多少？</h2><p>採用 Jina AI 的 binary embedding 模型不僅可以降低時間敏感應用的延遲，還能帶來可觀的成本效益，如下表所示：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>模型</th>\n<th>每 2.5 億<br/>embeddings<br/>所需記憶體</th>\n<th>檢索基準<br/>平均</th>\n<th>AWS 預估價格<br/>（x2gb 實例<br/>每 GB/月 $3.8）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32-bit 浮點 embeddings</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>Binary embeddings</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>僅有大約 10% 的檢索準確度下降，就能達到超過 95% 的儲存空間節省。</p><p>這比使用 <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">OpenAI 的 Ada 2 模型</a>或 <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Cohere 的 Embed v3</a>所產生的二進制向量節省更多。這兩個模型產生的嵌入維度都在 1024 維以上。相較之下，Jina AI 的嵌入僅有 768 維，但效能仍然相當，這意味著在量化之前就已經比其他模型更小了，同時還保持著相同的準確度。</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">二進制向量可以節省記憶體、計算時間、傳輸頻寬和磁碟儲存空間，在多個方面帶來經濟效益</strong></b>。</div></div><p>這些節省也具有環保意義，可以減少稀有材料的使用和能源消耗。</p><h2 id=\"get-started\">開始使用</h2><p>要使用 <a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a> 獲取二進制嵌入，只需在 API 呼叫中添加 <code>encoding_type</code> 參數。使用值 <code>binary</code> 可以獲得以有符號整數編碼的二進制嵌入，或使用 <code>ubinary</code> 獲得無符號整數。</p><h3 id=\"directly-access-jina-embedding-api\">直接存取 Jina Embedding API</h3><p>使用 <code>curl</code>：</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>或透過 Python <code>requests</code> API：</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>使用上述 Python <code>request</code>，透過檢查 <code>response.json()</code> 您將得到以下回應：</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>這是兩個以 96 個 8 位元有符號整數儲存的二進制嵌入向量。要將它們解壓縮為 768 個 0 和 1，您需要使用 <code>numpy</code> 函式庫：</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>結果是一個只包含 0 和 1 的 768 維向量：</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">在 Qdrant 中使用二進制量化</h3><p>您也可以使用 <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">Qdrant 的整合函式庫</a>直接將二進制嵌入放入您的 Qdrant 向量存儲中。由於 Qdrant 內部已實作 <code>BinaryQuantization</code>，您可以將其作為整個向量集合的預設配置，這樣就可以在不改變程式碼的情況下檢索和儲存二進制向量。</p><p>請參見以下示例程式碼：</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# 提供 Jina API 金鑰並選擇一個可用的模型。\n# 您可以在此取得免費試用金鑰：https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # or \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n# 從 API 獲取嵌入向量\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# 將嵌入向量索引到 Qdrant\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>要設定搜尋，您應該使用 <code>oversampling</code> 和 <code>rescore</code> 參數：</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">使用 LlamaIndex</h3><p>要在 LlamaIndex 中使用 Jina 二進位嵌入向量，在實例化 <code>JinaEmbedding</code> 物件時將 <code>encoding_queries</code> 參數設定為 <code>binary</code>：</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# 您可以從 https://jina.ai/embeddings/ 獲取免費試用金鑰\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">支援二進位嵌入向量的其他向量資料庫</h3><p>以下向量資料庫提供二進位向量的原生支援：</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB by DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">範例</h2><p>為了展示二進位嵌入向量的效果，我們從 <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a> 選取了一些摘要，並使用 <code>jina-embeddings-v2-base-en</code> 獲取了它們的 32 位元浮點數和二進位向量。然後我們將它們與示例查詢「3D segmentation」的嵌入向量進行比較。</p><p>從下表可以看出，前三個答案是相同的，前五個中有四個匹配。使用二進位向量產生了幾乎相同的最佳匹配結果。</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>排名</th>\n<th>漢明<br/>距離</th>\n<th>匹配文本</th>\n<th>餘弦值</th>\n<th>匹配文本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "32 位元對於像是 AI 模型這樣既強大又不精確的東西來說太過精密了。所以我們去掉了其中的 31 位！二元嵌入更小、更快，而且表現優異。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
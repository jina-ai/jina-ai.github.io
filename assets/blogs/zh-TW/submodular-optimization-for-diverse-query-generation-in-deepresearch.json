{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "DeepResearch 中用於產生多樣化查詢的次模優化",
  "html": "<p>在實作 DeepResearch 時，至少有兩個地方需要產生多樣化的查詢。首先，你必須<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">根據使用者輸入產生網路搜尋查詢</a>（直接將使用者輸入丟進搜尋引擎並不是個好主意）。其次，許多 DeepResearch 系統包含一個<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">「研究規劃器」，將原始問題分解為子問題</a>，同時呼叫代理程式獨立解決這些子問題，然後合併它們的結果。無論是處理查詢還是子問題，我們的期望都保持不變：它們必須與原始輸入相關，並且具有足夠的多樣性，才能提供獨特的視角。通常，我們需要限制查詢的數量，以避免在不必要地請求搜尋引擎或使用代理程式詞元上浪費金錢。</p><p>雖然理解查詢生成的重要性，但大多數開源 DeepResearch 實作並沒有認真對待這種最佳化。他們只是直接提示這些約束。有些人可能會要求 大模型 額外一輪來評估查詢並使其多樣化。以下是大多數實作基本方法的一個範例：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">使用 大模型 產生多樣化查詢的兩種不同提示詞。頂部的提示詞使用簡單的指令。底部的提示詞更複雜且結構化。給定原始查詢和要產生的查詢數量，我們期望產生的查詢具有足夠的多樣性。在本範例中，我們使用</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\">作為 大模型 ，原始查詢是</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">。</span></figcaption></figure><p>在本文中，我想演示一種更嚴謹的方法，使用句子 向量模型 和<strong>次模最佳化</strong>來解決最佳查詢生成問題。早在我的博士研究期間，次模最佳化就是我最喜歡的技術之一，與 L-BFGS 一起。我將展示如何應用它來生成一組在基數約束下的多樣化查詢，這可以顯著提高 DeepResearch 系統的整體品質。</p><h2 id=\"query-generation-via-prompting\">透過提示詞生成查詢</h2><p>首先，我們想檢查提示詞是否是產生多樣化查詢的有效方法。我們也想了解複雜的提示詞是否比簡單的提示詞更有效。讓我們運行一個實驗，比較以下兩個提示詞，以找出答案：</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">簡單提示詞</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">結構化提示詞</span></p></figcaption></figure><p>我們使用 <code>gemini-2.5-flash</code> 作為 大模型 ，原始查詢為 <code>\"embeddings and rerankers\"</code>，並測試簡單和結構化提示詞，以迭代方式生成從 1 到 20 個查詢。然後，我們使用 <code>jina-embeddings-v3</code> 和 <code>text-matching</code> 任務來測量原始查詢和生成的查詢之間的句子相似度，以及生成的查詢本身內的相似度。以下是視覺化圖表。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">兩種提示詞在「生成的查詢內」分析（右邊兩個圖）中顯示相似的模式，中位數餘弦相似度在不同的查詢計數中保持較高（0.4-0.6 範圍）。當查詢數量很大時，簡單的提示詞似乎更擅長使查詢多樣化，而結構化提示詞則保持與原始查詢略好的相關性，將相關性保持在 0.6 左右。</span></figcaption></figure><p>看看右側的兩個圖，可以看到簡單和結構化提示詞都表現出餘弦相似度分數的較大變異數，許多達到 0.7-0.8 的相似度，表明某些生成的查詢幾乎相同。此外，隨著生成更多查詢，兩種方法都難以保持多樣性。我們沒有看到相似度隨著查詢計數的增加而呈現明顯的下降趨勢，而是觀察到相對穩定（且高）的相似度水平，表明額外的查詢通常會複製現有的觀點。</p><p>一種解釋是 Wang 等人 (2025) 發現，即使有提示詞引導， 大模型 通常不成比例地反映了主要群體的意見，表明對常見觀點的偏見。這是因為 大模型 的訓練資料可能過度代表某些觀點，導致模型生成與這些普遍觀點一致的變體。Abe 等人 (2025) 也發現，基於 大模型 的查詢擴展傾向於流行的解釋，而忽略了其他的解釋。例如，「人工智慧的好處是什麼？」可能會產生常見的好處，如自動化、效率、道德，但會錯過不太明顯的好處，如藥物發現。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">問題形式化</h2><p>有人可能會認為我們之前的實驗沒有定論，我們應該改進提示詞並再次嘗試。雖然提示詞肯定可以在一定程度上改變結果，但更重要的是我們已經學到了一些東西：僅僅增加生成的查詢數量使我們更有可能獲得多樣化的查詢。壞消息是，我們也得到了很多重複的查詢作為副產品。</p><p>但是，既然生成大量查詢很便宜，最終會產生<em>一些</em>好的查詢，為什麼我們不將其視為子集選擇問題呢？</p><p>在數學上，我們可以這樣表述這個問題：給定一個原始輸入 $q_0$，一組由大模型使用提示詞工程生成的候選查詢 $V=\\{q_1, q_2, \\cdots, q_n\\}$。選擇一個子集 $X\\subseteq V$，其中包含 $k$ 個查詢，目標是在最小化冗餘的同時最大化覆蓋範圍。</p><p>不幸的是，從 $n$ 個候選查詢中找到 $k$ 個查詢的最佳子集需要檢查 $\\binom{n}{k}$ 種組合，這是一個指數級複雜度的問題。僅僅 20 個候選查詢和 $k=5$ 的情況下，就有 15,504 種組合。</p><h3 id=\"submodular-function\">次模函數</h3><p>在嘗試暴力解決子集選擇問題之前，讓我們先向讀者介紹<strong>次模性 (submodularity)</strong> 和<strong>次模函數 (submodular function)</strong> 的概念。它們對許多人來說可能聽起來很陌生，但你可能聽說過「邊際效益遞減」的概念——次模性就是這個概念的數學表示。</p><p>考慮在一個大型建築物中放置 Wi-Fi 路由器以提供網路覆蓋。你安裝的第一個路由器提供了巨大的價值——它覆蓋了之前沒有覆蓋的很大區域。第二個路由器也增加了相當大的價值，但它的一些覆蓋區域與第一個路由器重疊，因此邊際效益低於第一個。隨著你繼續添加路由器，每個額外的路由器覆蓋的新區域越來越少，因為大多數空間已經被現有的路由器覆蓋。最終，第 10 個路由器可能提供的額外覆蓋範圍非常小，因為建築物已經被很好地覆蓋了。</p><p>這種直覺抓住了次模性的本質。在數學上，一個集合函數 $f: 2^V \\rightarrow \\mathbb{R}$ 是<strong>次模的</strong>，如果對於所有 $A \\subseteq B \\subseteq V$ 和任何元素 $v \\notin B$：</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>用通俗的語言來說：將一個元素添加到一個較小的集合所帶來的好處，至少與將同一個元素添加到一個包含該較小集合的較大集合所帶來的好處一樣多。</p><p>現在讓我們將這個概念應用於我們的查詢生成問題。人們可能立即意識到，查詢選擇表現出自然的<strong>邊際效益遞減</strong>：</p><ul><li>我們選擇的第一個查詢覆蓋了全新的語義空間</li><li>第二個查詢應該覆蓋不同的方面，但一些重疊是不可避免的</li><li>隨著我們添加更多查詢，每個額外的查詢覆蓋的新領域越來越少</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">來自 </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">我在 AAAI 2013 上的一個舊投影片</span></a><span style=\"white-space: pre-wrap;\">，我在其中用一袋球來解釋次模性。向袋子中添加更多的球可以改善「設施」，但相對改善變得越來越小，如右側 y 軸上遞減的 delta 值所示。</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">基於向量模型的次模函數設計</h2><p>令 $\\mathbf{e}_i \\in \\mathbb{R}^d$ 為查詢 $q_i$ 的向量模型，使用句子向量模型（例如，<code>jina-embeddings-v3</code>）獲得。設計我們的目標函數主要有兩種方法：</p><h3 id=\"approach-1-facility-location-coverage-based\">方法 1：設施選址（基於覆蓋範圍）</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>這個函數衡量了選定的集合 $X$「覆蓋」所有候選查詢的程度，其中：</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ 是餘弦相似度</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ 確保與原始查詢的相關性</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ 衡量選定集合 $X$ 對候選查詢 $j$ 的覆蓋程度</li></ul><p>一個需要注意的是，這個函數只是<em>隱式地</em>鼓勵多樣性。它沒有明確地懲罰選定集合 $X$ 內的相似性。多樣性的出現是因為選擇相似的查詢會提供遞減的覆蓋範圍回報。</p><h3 id=\"approach-2-explicit-coverage-diversity\">方法 2：顯式覆蓋範圍 + 多樣性</h3><p>為了更直接地控制多樣性，我們可以結合覆蓋範圍和一個顯式的多樣性項：</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>其中多樣性成分可以表示為：</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>這個多樣性項衡量了選定的查詢和未選定的查詢之間的總相似性——當我們選擇與剩餘候選查詢不同的查詢時，它會最大化（一種圖割函數的形式）。</p><h3 id=\"difference-between-two-approaches\">兩種方法之間的差異</h3><p>兩種公式都保持了次模性。</p><p>設施選址函數是一個著名的次模函數。由於 max 運算，它表現出次模性：當我們向選定的集合中添加一個新的查詢 $q$ 時，每個候選查詢 $j$ 都會被我們集合中「最好」的查詢（相似度最高的查詢）覆蓋。與將 $q$ 添加到一個較大的集合 $B \\supseteq A$ 相比，將 $q$ 添加到一個較小的集合 $A$ 更有可能改善各種候選查詢的覆蓋範圍，因為在較大的集合 $B$ 中，許多候選查詢已經被很好地覆蓋。</p><p>在圖割多樣性函數中，多樣性項 $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ 是次模的，因為它衡量了選定集合和未選定集合之間的「割」。與將一個新的查詢添加到一個較大的選定集合相比，將一個新的查詢添加到一個較小的選定集合會創建更多與未選定查詢的新連接。</p><p>設施選址方法依賴於通過覆蓋範圍競爭來實現<em>隱式</em>多樣性，而顯式方法直接衡量和優化多樣性。因此，兩者都是有效的，但顯式方法使您可以更直接地控制相關性-多樣性的權衡。</p><h2 id=\"implementations\">實作</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">完整的實作可以在 Github 上找到。</span></p></figcaption></figure><p>由於我們的函數是次模的，因此我們可以使用<strong>貪婪演算法</strong>，該演算法提供 $(1-1/e) \\approx 0.63$ 的近似保證：</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>這是優化設施選址（基於覆蓋範圍）的程式碼——即具有隱式多樣性的程式碼。</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>平衡參數 $\\alpha$ 控制相關性和多樣性之間的權衡：</p><ul><li><strong>高 $\\alpha$（例如，0.8）</strong>：優先考慮與原始查詢的相關性，可能會犧牲多樣性</li><li><strong>低 $\\alpha$（例如，0.2）</strong>：優先考慮選定查詢之間的多樣性，可能會偏離原始意圖</li><li><strong>中等 $\\alpha$（例如，0.4-0.6）</strong>：平衡方法，通常在實踐中效果良好</li></ul><h3 id=\"lazy-greedy-algorithm\">惰性貪婪演算法</h3><p>人們可能會注意到上面的程式碼：</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>我們在每次迭代中計算<strong>所有</strong>剩餘候選查詢的邊際效益。我們可以做得更好。</p><p><strong>惰性貪婪演算法</strong>是一種巧妙的優化，它利用次模性來避免不必要的計算。關鍵的見解是：如果元素 A 在第 $t$ 次迭代中比元素 B 具有更高的邊際效益，那麼由於次模性，A 在第 $t+1$ 次迭代中仍然比 B 具有更高的邊際效益。</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Lazy greedy 的運作方式如下：</p><ol><li>維護一個優先佇列，其中的元素依照其邊際收益排序。</li><li>僅重新計算頂部元素的邊際收益。</li><li>如果在重新計算後，它仍然是最高的，則選取它。</li><li>否則，將其重新插入到正確的位置並檢查下一個頂部元素。</li></ol><p>由於我們避免了重新計算明顯不會被選取的元素的邊際收益，因此可以顯著提高速度。</p><h3 id=\"results\">結果</h3><p>讓我們再次運行實驗。我們使用相同的簡單提示詞來生成 1 到 20 個不同的查詢，並執行與之前相同的餘弦相似度測量。對於次模優化，我們使用不同的 k 值從 20 個生成的候選查詢中選取查詢，並如之前一樣測量相似度。結果表明，透過次模優化選取的查詢更加多樣化，且顯示較低的集合內相似度。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查詢 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查詢 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查詢 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查詢 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">最終問題：為什麼次模公式很重要</h2><p>您可能想知道：為什麼要費力地將其表述為次模優化問題？為什麼不直接使用啟發式方法或其他優化方法？</p><p>簡而言之，次模公式將一個特別的「選擇多樣化查詢」啟發式方法轉變為一個具有<strong>可證明保證</strong>、<strong>高效演算法</strong>和可衡量目標的嚴格優化問題。</p><h3 id=\"guaranteed-efficiency\">保證效率</h3><p>一旦我們證明我們的目標函數是次模的，我們就可以獲得強大的理論保證和一個高效的演算法。與檢查 $\\binom{n}{k}$ 個組合相比，以 $O(nk)$ 時間運行的貪婪演算法實現了 $(1-1/e) \\approx 0.63$ 的最佳解近似值。這意味著我們的貪婪解始終至少與最佳可能解一樣好 63%。<strong>沒有啟發式方法可以保證這一點。</strong></p><p>此外，由於次模函數的數學結構，lazy greedy 演算法在實踐中速度更快。加速來自於<strong>遞減回報</strong>：在早期迭代中較差的選擇不太可能在以後成為好的選擇。因此，lazy greedy 通常只需要重新計算少數幾個頂部候選者的收益，而不是檢查所有 $n$ 個候選者。</p><h3 id=\"no-need-for-hand-crafted-heuristics\">無需手工製作的啟發式方法</h3><p>如果沒有原則性的框架，您可能會求助於特別的規則，例如「確保查詢具有小於 0.7 的餘弦相似度」或「平衡不同的關鍵字類別」。這些規則很難調整，並且不能推廣。次模優化為您提供了一種有原則的、數學基礎的方法。您可以使用驗證集系統地調整超參數，並監控生產系統中的解決方案品質。當系統產生不良結果時，您可以獲得明確的指標來除錯問題所在。</p><p>最後，次模優化是一個經過充分研究的領域，擁有數十年的研究成果，讓您可以利用貪婪演算法之外的進階演算法（例如加速貪婪演算法或局部搜尋）、關於某些公式何時效果最佳的理論見解，以及處理其他約束（例如預算限制或公平性要求）的擴展。</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">對於那些對次模優化感興趣的人，我建議訪問此網站以了解更多資訊。</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "許多人都知道查詢多樣性在 DeepResearch 中的重要性，但很少人知道如何透過子模組優化來嚴格地解決它。",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
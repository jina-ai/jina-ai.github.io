{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "文字-圖像全域對比對準與詞符-區塊局部對準",
  "html": "<p>在實驗 <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali 風格</a>的模型時，我們的一位工程師使用最近發布的 <code>jina-clip-v2</code> 模型創建了一個視覺化呈現。他映射了給定圖文對之間的 token embeddings 和 patch embeddings 的相似度，創建了熱力圖覆蓋層，產生了一些有趣的視覺洞察。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>不幸的是，<strong>這只是一個啟發式的視覺化</strong>——而不是一個明確或有保證的機制。雖然 CLIP 類型的全局對比對齊可能（而且經常會）<em>偶然</em>在 patches 和 tokens 之間創建粗略的局部對齊，但這是一個<strong>意外的副作用</strong>，而不是模型的刻意目標。讓我解釋原因。</p><h2 id=\"understand-the-code\">理解程式碼</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>讓我們從高層次來分解程式碼的功能。注意，<code>jina-clip-v2</code> 實際上並沒有提供任何預設的 API 來訪問 token 層級或 patch 層級的 embeddings——這個視覺化需要一些事後修補才能工作。</p><p><strong>計算詞級別 embeddings</strong></p><p>通過設置 <code>model.text_model.output_tokens = True</code>，調用 <code>text_model(x=...,)[1]</code> 將返回一個 <code>(batch_size, seq_len, embed_dim)</code> 的第二個元素作為 token embeddings。它接收一個輸入句子，用 Jina CLIP tokenizer 進行分詞，然後通過平均對應的 token embeddings 將子詞 tokens 重新組合成\"詞\"。它通過檢查 token 字串是否以 <code>_</code> 字符開頭（在基於 SentencePiece 的 tokenizer 中很典型）來檢測新詞的開始。它產生一個詞級別 embeddings 列表和一個詞列表（所以\"Dog\"是一個 embedding，\"and\"是一個 embedding 等）。</p><p><strong>計算 patch 級別 embeddings</strong></p><p>對於圖像塔，<code>vision_model(..., return_all_features=True)</code> 將返回 <code>(batch_size, n_patches+1, embed_dim)</code>，其中第一個 token 是 <code>[CLS]</code> token。從中，程式碼提取每個 patch 的 embeddings（即視覺 transformer 的 patch tokens）。然後將這些 patch embeddings 重塑成一個 2D 網格，<code>patch_side × patch_side</code>，接著上採樣以匹配原始圖像解析度。</p><p><strong>視覺化詞-patch 相似度</strong></p><p>相似度計算和後續的熱力圖生成是標準的\"事後\"可解釋性技術：選擇一個文本 embedding，計算與每個 patch embedding 的餘弦相似度，然後生成一個熱力圖，顯示哪些 patches 與該特定 token embedding 具有最高相似度。最後，它循環遍歷句子中的每個 token，在左側以粗體突出顯示該 token，並在右側的原始圖像上覆蓋基於相似度的熱力圖。所有幀被編譯成一個動態 GIF。</p><h2 id=\"is-it-meaningful-explainability\">這是有意義的可解釋性嗎？</h2><p>從<em>純程式碼</em>的角度來看，是的，邏輯是連貫的，並且會為每個 token 產生一個熱力圖。你會得到一系列顯示 patch 相似度的幀，所以腳本\"做到了它所說的事情\"。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>看上面的例子，我們可以看到像 <code>moon</code> 和 <code>branches</code> 這樣的詞似乎與原始圖像中相應的視覺 patches 良好對齊。但這裡的關鍵問題是：這是有意義的對齊，還是我們只是看到一個幸運的巧合？</p><p>這是一個更深層的問題。要理解這些注意事項，讓我們回顧一下 <strong>CLIP 是如何訓練的</strong>：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 結合了文本編碼器（Jina XLM-RoBERTa，561M 參數）和視覺編碼器（EVA02-L14，304M 參數）。右側的每個彩色方塊代表批次中的完整句子或圖像——而不是單個 tokens 或 patches。</span></figcaption></figure><ul><li>CLIP 使用<strong>全局</strong>對比對齊來處理整個圖像和整個文本。在訓練過程中，圖像編碼器產生一個單一向量（池化表示），文本編碼器產生另一個單一向量；CLIP 被訓練使得這些向量在匹配的文本圖像對中相匹配，在其他情況下不匹配。</li><li>在\"patch X 對應於 token Y\"層面上<strong>沒有明確的監督</strong>。模型並未直接被訓練來突出顯示\"圖像的這個區域是狗，那個區域是貓\"等。相反，它被教導整個圖像表示應該與整個文本表示相匹配。</li><li>因為 CLIP 的架構在圖像端是 Vision Transformer，在文本端是文本 transformer——兩者形成獨立的編碼器——所以沒有原生地將 patches 與 tokens 對齊的交叉注意力模組。相反，你在每個塔中只得到純<strong>自注意力</strong>，再加上最終的全局圖像或文本 embeddings 投影。</li></ul><p>簡而言之，這是一個啟發式的視覺化。任何給定的 patch embedding 可能與特定的 token embedding 接近或遠離，這有點是自然形成的。這更像是一個<em>事後可解釋性技巧</em>，而不是模型的穩健或官方的\"注意力\"。</p><h2 id=\"why-might-local-alignment-emerge\">為什麼可能會出現局部對齊？</h2><p>那麼，為什麼我們有時會發現詞-patch 層級的局部對齊？事情是這樣的：儘管 CLIP 是在<em>全局</em>圖像-文本對比目標上訓練的，它仍然使用自注意力（在基於 ViT 的圖像編碼器中）和 transformer 層（用於文本）。在這些自注意力層中，圖像表示的不同部分可以相互作用，就像文本表示中的詞一樣。通過在大規模圖像-文本數據集上的訓練，模型自然會發展出內部潛在結構，幫助它將整體圖像與其相應的文本描述匹配。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>局部對齊</strong>可能在這些潛在表示中出現，至少有兩個原因：</p><ol><li><strong>共現模式</strong>：如果模型看到許多\"狗\"的圖像旁邊有許多\"貓\"的圖像（通常用這些詞標記或描述），它可以學習大致對應於這些概念的潛在特徵。因此，\"狗\"的 embedding 可能會接近於描繪狗形狀或紋理的局部 patches。這在 patch 層面上<em>並非</em>明確監督，而是從狗圖像/文本對的重複關聯中自然產生的。</li><li><strong>自注意力</strong>：在 Vision Transformer 中，各個圖像塊之間會相互關注。顯著的圖像塊（如狗的臉部）可能會形成一個一致的潛在「特徵」，因為模型試圖為整個場景產生一個全局準確的表示。如果這有助於最小化整體對比損失，這種特徵就會被強化。</li></ol><h2 id=\"theoretical-analysis\">理論分析</h2><p>CLIP 的對比學習目標是要最大化匹配的圖文對之間的餘弦相似度，同時最小化不匹配對之間的相似度。假設文本和圖像編碼器分別生成 token 和圖像塊嵌入：</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>全局相似度可以表示為局部相似度的聚合：</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>當特定的 token-patch 對在訓練數據中經常共同出現時，模型通過累積梯度更新來強化它們之間的相似度：</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>，其中 $C$ 是共同出現的次數。這導致 $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$ 顯著增加，促進了這些配對的更強局部對齊。然而，對比損失會在所有 token-patch 對之間分配梯度更新，限制了特定配對更新的強度：</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>這防止了個別 token-patch 相似度的顯著強化。</p><h2 id=\"conclusion\">結論</h2><p>CLIP 的 token-patch 視覺化利用了文本和圖像表示之間偶然的、自發的對齊。這種對齊雖然很有趣，但源於 CLIP 的<strong>全局對比訓練</strong>，缺乏精確且可靠的可解釋性所需的結構穩健性。產生的視覺化結果經常表現出<strong>噪聲和不一致性</strong>，限制了其在深入解釋應用中的實用性。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>像 <strong>ColBERT</strong> 和 <strong>ColPali</strong> 這樣的後期交互模型通過<strong>在架構上嵌入明確的、細粒度的對齊</strong>來解決文本 token 和圖像塊之間的這些限制。通過獨立處理模態並在後期階段執行有針對性的相似度計算，這些模型確保每個文本 token 都能與相關的圖像區域產生有意義的關聯。</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP 可以視覺化 token 與圖像區塊的相似度，但這比較像是一種事後的可解釋性技巧，而不是模型中穩健或官方的「注意力機制」（attention）。",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "DeepSearch/DeepResearch 的片段選擇和 URL 排名",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">DeepSearch/DeepResearch 實作指南</div><div class=\"kg-bookmark-description\">QPS 退場，深度當道。DeepSearch 成為新標準。透過讀取-搜尋-推理循環來尋找答案。了解它的本質及如何建構它。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>如果您已經閱讀過我們的 DeepSearch/DeepResearch 實作指南，讓我們深入探討一些能<em>大幅</em>提升質量的細節。在這篇文章中，我們將聚焦於兩個關鍵挑戰：<strong>利用 embeddings 從長網頁中選擇片段</strong>以及<strong>使用 rerankers 來為爬取 URL 進行優先級排序。</strong></p><p>有些人可能還記得我們之前的結論，即「embeddings 只對類似 STS 任務（語義文本相似度）的查詢去重有用，而 rerankers 甚至不是我們原始 DeepSearch 實作的一部分。」事實證明，這兩者仍然相當有價值 - 只是不是以人們常見的方式。我們一直遵循<em>最精簡</em>的路徑。我們不會為了證明它們的存在價值或我們作為 embedding 和 reranker 提供者的價值而添加組件。<strong>我們關注的是搜尋真正需要的基本要素。</strong></p><p>經過數週的實驗和迭代，我們發現了這兩者在 DeepSearch/DeepResearch 系統中不尋常但有效的使用方式。透過應用它們，我們顯著改善了 <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> 的質量（歡迎試用）。我們想與在這個領域工作的同行分享這些見解。</p><h2 id=\"select-snippet-from-long-content\">從長內容中選擇片段</h2><p>問題是這樣的：在<a href=\"https://jina.ai/reader\">使用 Jina Reader 讀取網頁內容</a>後，我們需要將其作為知識項目添加到 agent 的上下文中以進行推理。雖然將完整內容直接放入 LLM 的上下文視窗是最簡單的方式，但考慮到 token 成本和生成速度，這並不是最佳選擇。在實踐中，我們需要識別內容中與問題最相關的部分，並有選擇地只將這些部分作為知識添加到 agent 的上下文中。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">我們討論的是即使經過 Jina Reader 的 markdown 清理後，內容仍然太長的情況。這種情況經常出現在長頁面中，如 GitHub issues、Reddit 討論串、論壇討論和部落格文章（包括我們自己在 jina.ai/news 上的許多文章）。</div></div><p>基於 LLM 的過濾存在相同的成本和延遲問題，讓我們尋找更小型模型的解決方案：我們需要更小更便宜，<strong>但仍然支援多語言的模型</strong> – 這是一個關鍵因素，因為我們無法保證查詢或文件始終是英文的。</p><p>一邊是問題（原始查詢或差距問題），另一邊是大量的 markdown 內容，其中大部分內容都是不相關的。我們需要為查詢選擇最相關的片段。這類似於 RAG 社群自 2023 年以來一直在處理的分塊問題 - 使用檢索模型只檢索相關的塊放入上下文視窗進行摘要。然而，在我們的情況下有兩個關鍵差異：</p><ol><li>來自有限文件的有限塊。如果每個塊包含大約 500 個 tokens，那麼一個典型的長網頁文件大約有 200,000 個 tokens（p50）到 1,000,000 個 tokens（p99），我們使用 Jina Reader 每步擷取 4-5 個 URL，這將產生大約數百個塊 - 意味著數百個 embedding 向量和數百個餘弦相似度。這可以輕鬆地用 JavaScript 在記憶體中管理，無需向量資料庫。</li><li>我們需要連續的塊來形成有效的知識片段。我們不能接受結合分散句子的片段，如 <code>[1-2, 6-7, 9, 14, 17, ...]</code>。更有用的知識片段應該遵循像 <code>[3-15, 17-24, ...]</code> 這樣的模式 - 始終保持文本連續性。這讓 LLM 更容易從知識源複製和引用，並減少產生幻覺。</li></ol><p>其餘的都是實踐者抱怨的問題：每個塊不能太長，因為 embedding 模型無法很好地處理長上下文；分塊會導致上下文丟失，使塊嵌入變得獨立同分布；而且如何找到最佳的邊界提示，既要保持可讀性又要保持語義？如果您理解我們在說什麼，那麼您在 RAG 實作中可能也被這些問題困擾過。</p><p>但長話短說 - <strong>使用 <code>jina-embeddings-v3</code> 的延遲分塊完美解決了這三個問題。</strong>延遲分塊保留了每個塊的上下文資訊，<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">對邊界提示不敏感</a>，而 <code>jina-embeddings-v3</code> 本身在<em>非對稱</em>多語言檢索任務中達到了 SOTA。感興趣的讀者可以查看我們的部落格文章或論文了解詳情，但這裡是整體實作。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">此圖說明了片段選擇算法，其工作方式類似於 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\">。該過程首先將長文檔分割成固定長度的塊，然後使用開啟延遲分塊的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> 進行嵌入。在計算每個塊與查詢之間的相似度分數後，滑動視窗會在相似度分數上移動，以找到平均值最高的視窗。</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">延遲分塊的真正含義與非含義：第二部分</div><div class=\"kg-bookmark-description\">延遲分塊探索的第 2 部分，深入探討為什麼它是塊嵌入和改善搜尋/RAG 性能的最佳方法。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3：使用任務特定 LoRA 的多語言嵌入</div><div class=\"kg-bookmark-description\">我們介紹 jina-embeddings-v3，這是一個具有 5.7 億參數的新型文本嵌入模型，在多語言數據和長上下文檢索任務中達到了最先進的性能，支援長達 8192 個 tokens 的上下文長度。該模型包含一組任務特定的低秩適應（LoRA）適配器，用於為查詢-文檔檢索、聚類、分類和文本匹配生成高質量嵌入。在 MTEB 基準測試中的評估顯示，jina-embeddings-v3 在英語任務上優於 OpenAI 和 Cohere 的最新專有嵌入，同時在所有多語言任務上相比 multilingual-e5-large-instruct 取得更優越的性能。透過 1024 的預設輸出維度，使用者可以靈活地將嵌入維度降低到 32，而不會損害性能，這得益於套娃表示學習。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">延遲分塊：使用長上下文嵌入模型的上下文塊嵌入</div><div class=\"kg-bookmark-description\">許多使用案例需要檢索較小的文本部分，而基於密集向量的檢索系統通常在較短的文本段上表現更好，因為語義不太可能在嵌入中過度壓縮。因此，實踐者經常將文本文檔分割成較小的塊並分別編碼。然而，以這種方式創建的塊嵌入可能會失去來自周圍塊的上下文信息，導致次優表示。在本文中，我們介紹了一種稱為延遲分塊的新方法，該方法首先嵌入長文本的所有 tokens，在 transformer 模型之後、均值池化之前才進行分塊 - 因此稱為延遲。產生的塊嵌入捕獲完整的上下文信息，在各種檢索任務中取得更優異的結果。該方法足夠通用，可以應用於各種長上下文嵌入模型，且無需額外訓練即可工作。為了進一步提高延遲分塊的效果，我們提出了一種專門的嵌入模型微調方法。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">使用後期分塊和 Conv1D 類似的平均池化來選擇與問題最相關的片段。</span></p></figcaption></figure><p>確保在呼叫 Jina Embeddings API 時設定以下的 retrieval <code>task</code>、<code>late_chunking</code> 和 <code>truncate</code> 參數：</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">在嵌入問題時，請確保將 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> 更改為 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> 並關閉 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code></p></figcaption></figure><p>完整實作可以在 Github 上找到：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main · jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">為下一次閱讀排序 URL</h2><p>問題是這樣的：在 DeepSearch 會話期間，你可能會從搜尋引擎結果頁面（SERP）收集大量 URL，並在閱讀個別網頁時發現更多（那些頁面內的連結）。唯一 URL 的總數很容易達到數百個。再次強調，直接將所有 URL 丟進 LLM 的上下文是低效的——這會浪費寶貴的上下文視窗空間，而更麻煩的是，<strong>我們發現 LLM 基本上是隨機挑選 URL。</strong>引導 LLM 朝向最有可能包含你所需答案的 URL 是至關重要的。</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">在 DeepSearch 中使用 Jina Reader 爬取頁面的最佳選項。這會在單獨的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> 欄位中收集所有頁面內連結，並從 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\"> 欄位中移除它們。</span></p></figcaption></figure><p>將這個問題想像成一個上下文內的 PageRank，我們需要在一個會話期間權衡數百個 URL。我們根據多個因素對 URL 進行排名，這些因素結合了最後更新時間、域名頻率、路徑結構，最重要的是與查詢的語義相關性，來創建一個綜合分數。記住，我們只能使用在實際訪問 URL <em>之前</em> 可用的資訊：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>頻率信號</strong>：出現在不同來源多次的 URL 會獲得額外的權重。來自在搜索結果中頻繁出現的域名的 URL 會得到提升，因為熱門域名通常包含權威內容。</p><p><strong>路徑結構</strong>：我們分析 URL 路徑以識別內容集群。在常見路徑層次結構中的 URL 獲得更高的分數，對更深層路徑應用衰減因子。</p><p><strong>語義相關性</strong>：我們使用 <code>jina-reranker-v2-base-multilingual</code> 來評估問題與每個 URL 文本資訊之間的語義相關性，這是<a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">一個經典的重排序問題</a>。每個 URL 的文本資訊來自：</p><ul><li>來自 SERP API 結果的標題和摘要（<code>https://s.jina.ai/</code> 與 <code>'X-Respond-With': 'no-content'</code>）</li><li>頁面內 URL 的錨文本（<code>https://r.jina.ai</code> 與 <code>'X-With-Links-Summary': 'all'</code>）</li></ul><p><strong>最後更新時間</strong>：某些 DeepSearch 查詢是時間敏感的，所以最近更新的 URL 比舊的更有價值。在不是像 Google 那樣的主要搜尋引擎的情況下，可靠地確定最後更新時間是具有挑戰性的。我們實現了一個多層次方法，結合以下信號並提供一個具有信心分數的時間戳，在需要時優先考慮更新的內容。</p><ul><li>SERP API 過濾器（例如 s.jina.ai 的 <code>tbs</code> 參數用於按最近程度過濾）</li><li>HTTP 標頭分析（Last-Modified、ETag）</li><li>元數據提取（meta 標籤、Schema.org 時間戳）</li><li>內容模式識別（HTML 中可見的日期）</li><li>針對 WordPress、Drupal 和 Ghost 等平台的 CMS 特定指標</li></ul><p><strong>受限內容：</strong>社交媒體平台上的某些內容是受限的或者只是在付費牆後面，在不登入或違反其服務條款的情況下，沒有合法的方式獲取這些內容。我們應該主動維護一個問題 URL 和主機名列表來降低它們的排名，避免在無法訪問的內容上浪費時間。</p><p><strong>域名多樣性：</strong>在某些情況下，權重最高的 URL 都來自相同的主機名，這可能使 DeepSearch 陷入局部最優，並降低最終結果的質量。檢查上面的例子，其中所有頂級 URL 都來自 StackOverflow。為了提高多樣性，我們可以實施探索-利用方法，從每個主機名選擇排名最高的前 k 個 URL。</p><p>URL 排名的完整實作可以在我們的 Github 上找到。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main · jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Face🤗 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset · Issue #7175 · huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">請記得將 URL 權重放入代理的上下文中，並指示 LLM 遵守這些權重。</span></p></figcaption></figure><h2 id=\"conclusion\">結論</h2><p>自從我們的 DeepSearch 系統於 2025 年 2 月 2 日發布以來，我們發現了兩個能夠大幅提升品質的實作細節。值得注意的是，這兩者都以「上下文」的方式使用多語言嵌入和重排序器——與這些模型通常需要的預計算索引相比，運作規模要小得多。這解釋了我們最初的疏忽。</p><p>這點出了搜尋技術未來的一個有趣兩極化現象。考慮一個類似於 Kahneman 雙系統理論的框架：</p><ul><li>快思考（grep、BM25、SQL）：快速、有規則的模式匹配，計算需求最小。</li><li>慢思考（LLM）：具有深度上下文理解的全面推理，需要大量計算。</li><li>中思考（嵌入、重排序器）：處於模糊地帶？對於簡單的模式匹配來說太「進階」/語義化，但又缺乏真正的推理能力。</li></ul><p>我們可能正在見證一種雙分架構的興起，其中輕量級、高效的 SQL/BM25 處理初始內容檢索，直接輸入到強大的 LLM 進行深度處理。這些 LLM 越來越多地整合了之前需要專門中層模型的語義功能。中思考模型的剩餘角色轉向特定的上下文任務：過濾、去重複，以及完整推理效率不高的有限範圍操作。</p><p>然而，選擇關鍵片段和排序 URL 仍然是直接影響 DeepSearch/DeepResearch 系統品質的基本組件。我們希望我們的見解能激發您自己實作的改進。</p><p>查詢擴展仍然是另一個關鍵的品質決定因素。我們正在積極評估多種方法——從基本的提示詞重寫到小型語言模型和基於推理的方法。請期待我們即將發布的相關發現。敬請關注。</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "掌握這兩個細節可以讓你的 DeepSearch 從普普通通變成超強大：從冗長網頁中挑選最佳摘要片段，以及在爬取前對 URL 進行排序。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "scaling-test-time-compute-for-embedding-models",
  "id": "675a84f80ce9930001b86f09",
  "uuid": "49f876f3-0d50-4555-8f9e-136473f720ac",
  "title": "調整嵌入模型的測試時計算資源",
  "html": "<p>自從 OpenAI 發布 <a href=\"https://openai.com/o1/?ref=jina-ai-gmbh.ghost.io\">O1 模型</a>以來，AI 社群中最受討論的話題之一就是<strong>擴展測試時計算</strong>。這指的是在推理階段——AI 模型針對輸入生成輸出的階段——而非在預訓練期間分配額外的計算資源。一個著名的例子是\"思維鏈\"多步推理，它使模型能夠進行更廣泛的內部思考，例如評估多個可能的答案、更深入的規劃，在得出最終回應前進行自我反思。這種策略提高了答案品質，特別是在複雜的推理任務中。阿里巴巴最近發布的 <a href=\"https://huggingface.co/Qwen/QwQ-32B-Preview?ref=jina-ai-gmbh.ghost.io\">QwQ-32B-Preview</a> 模型就遵循了這種通過增加測試時計算來改進 AI 推理的趨勢。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">在這種情況下，\"擴展\"主要指在推理過程中增加計算能力（如處理能力或時間）。它不是指<b><strong style=\"white-space: pre-wrap;\">橫向擴展</strong></b>（在多個系統間分配任務）或實現<b><strong style=\"white-space: pre-wrap;\">加速</strong></b>（減少處理時間）。</div></div><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1.mp4\" poster=\"https://img.spacergif.org/v1/900x432/0a/spacer.png\" width=\"900\" height=\"432\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:10</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">使用 OpenAI 的 O1 模型時，用戶可以明顯注意到，當模型構建推理鏈來解決問題時，多步推理需要額外的時間。 </span></p></figcaption>\n        </figure><p>在 Jina AI，我們更專注於 embeddings 和 rerankers，而不是 LLMs，所以對我們來說，從這個角度考慮擴展測試時計算是很自然的：<em>\"思維鏈\"如何應用於 embedding 模型？</em>雖然一開始可能不太直觀，但本文探討了一個新的視角，並展示了如何將擴展測試時計算應用於 <code>jina-clip</code> 來對分佈外（OOD）圖像進行分類——解決原本不可能完成的任務。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">我們的實驗專注於寶可夢識別，這對 embedding 模型來說是一個有趣的挑戰。雖然類似 CLIP 的模型在一般的圖像-文本匹配方面表現出色，但如果沒有微調，它們在特定領域或 OOD 圖像上可能會遇到困難。通過給模型更多\"思考\"時間，我們發現多目標分類——類似於\"思維鏈\"——可以在不對 embedding 模型本身進行任何微調的情況下提高準確率。</span></figcaption></figure><h2 id=\"case-study\">案例研究</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z?ref=jina-ai-gmbh.ghost.io#scrollTo=CJt5zwA9E2jB\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-15.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>我們的實驗使用 <a href=\"https://huggingface.co/datasets/TheFusion21/PokemonCards?ref=jina-ai-gmbh.ghost.io\">TheFusion21/PokemonCards 數據集</a>進行寶可夢分類，該數據集包含數千張寶可夢交換卡片圖像。<strong>這個任務是圖像分類</strong>，其中輸入是剪裁過的寶可夢卡片藝術作品（去除所有文字/描述），輸出是從預定義的名稱集合中選擇正確的寶可夢名稱。這個任務對 CLIP embedding 模型來說是一個特別有趣的挑戰，因為：</p><ul><li>寶可夢的名稱和視覺表現對模型來說是特定領域的、分佈外的概念，使直接分類變得困難</li><li>每個寶可夢都有清晰的視覺特徵，可以分解為基本元素（形狀、顏色、姿勢），這些是 CLIP 可能更容易理解的</li><li>卡片藝術作品提供了一致的視覺格式，同時通過不同的背景、姿勢和藝術風格引入複雜性</li><li>這個任務需要同時整合多個視覺特徵，類似於語言模型中的複雜推理鏈</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"835\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">我們剪裁寶可夢卡片圖像以移除所有文字信息（標題、頁腳、描述），以防止因這些文字中出現寶可夢名稱而進行簡單猜測。這些寶可夢的類別標籤是 [</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol G</span></code><span style=\"white-space: pre-wrap;\">，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Weedle</span></code><span style=\"white-space: pre-wrap;\">，</span></figcaption></figure><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Azumarill</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Bulbasaur</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Venusaur</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aggron</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Beedrill δ</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Alakazam</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Dratini</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Arcanine</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Blaine's Moltres</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Celebi & Venusaur-GX</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">]</span></figcaption></figure><h3 id=\"baseline\">基準方法</h3><p>基準方法使用寶可夢卡牌圖片和名稱之間的簡單直接比較。首先，我們裁剪每張寶可夢卡片圖像以移除所有文字資訊（標題、頁尾、描述），以防止 CLIP 模型因這些文字中出現的寶可夢名稱而進行簡單猜測。然後，我們使用 <code>jina-clip-v1</code> 和 <code>jina-clip-v2</code> 模型對裁剪後的圖像和寶可夢名稱進行編碼，以獲得其各自的嵌入表示。分類是通過計算這些圖像和文字嵌入之間的餘弦相似度來完成的——每個圖像都與具有最高相似度分數的名稱匹配。這在卡牌藝術作品和寶可夢名稱之間建立了一對一的直接匹配，而不需要任何額外的上下文或屬性資訊。以下偽代碼總結了基準方法。</p><pre><code class=\"language-python\"># Preprocessing\ncropped_images = [crop_artwork(img) for img in pokemon_cards]  # Remove text, keep only art\npokemon_names = [\"Absol\", \"Aerodactyl\", ...]  # Raw Pokemon names\n\n# Get embeddings using jina-clip-v1\nimage_embeddings = model.encode_image(cropped_images)\ntext_embeddings = model.encode_text(pokemon_names)\n\n# Classification by cosine similarity\nsimilarities = cosine_similarity(image_embeddings, text_embeddings)\npredicted_names = [pokemon_names[argmax(sim)] for sim in similarities]\n\n# Evaluate\naccuracy = mean(predicted_names == ground_truth_names)</code></pre><h3 id=\"chain-of-thoughts-for-classification\">分類的\"思維鏈\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--10-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>我們不是直接將圖像與名稱匹配，而是將寶可夢識別分解為一個結構化的視覺屬性系統。我們定義了五個關鍵屬性組：主要顏色（如\"白色\"、\"藍色\"）、基本形態（如\"一隻狼\"、\"一隻有翅膀的爬行動物\"）、關鍵特徵（如\"單一白角\"、\"大翅膀\"）、體型（如\"四足狼形\"、\"有翅膀且纖細\"）和背景場景（如\"外太空\"、\"綠色森林\"）。</p><p>對於每個屬性組，我們創建特定的文本提示（如\"這隻寶可夢的身體主要是{}顏色\"）並配對相關選項。然後我們使用模型計算圖像與每個屬性選項之間的相似度分數。這些分數通過 softmax 轉換為概率，以獲得更校準的置信度衡量。</p><p>完整的思維鏈（CoT）結構包含兩個部分：<code>classification_groups</code> 描述提示組，以及 <code>pokemon_rules</code> 定義每個寶可夢應匹配的屬性選項。例如，阿勃梭魯應該在顏色上匹配\"白色\"，在形態上匹配\"狼形\"。完整的 CoT 如下所示（我們稍後將解釋如何建構這個）：</p><pre><code class=\"language-python\">pokemon_system = {\n    \"classification_cot\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokémon's body is mainly {} in color.\",\n            \"options\": [\n                \"white\",    # Absol, Absol G\n                \"gray\",     # Aggron\n                \"brown\",    # Aerodactyl, Weedle, Beedrill δ\n                \"blue\",     # Azumarill\n                \"green\",    # Bulbasaur, Venusaur, Celebi&Venu, Caterpie\n                \"yellow\",   # Alakazam, Ampharos\n                \"red\",      # Blaine's Moltres\n                \"orange\",   # Arcanine\n                \"light blue\"# Dratini\n            ]\n        },\n        \"primary_form\": {\n            \"prompt\": \"It looks like {}.\",\n            \"options\": [\n                \"a wolf\",         # Absol, Absol G\n                \"an armored dinosaur\",  # Aggron\n                \"a winged reptile\",     # Aerodactyl\n                \"a rabbit-like creature\", # Azumarill\n                \"a toad-like creature\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"a caterpillar larva\",    # Weedle, Caterpie\n                \"a wasp-like insect\",     # Beedrill δ\n                \"a fox-like humanoid\",     # Alakazam\n                \"a sheep-like biped\",      # Ampharos\n                \"a dog-like beast\",        # Arcanine\n                \"a flaming bird\",          # Blaine's Moltres\n                \"a serpentine dragon\"      # Dratini\n            ]\n        },\n        \"key_trait\": {\n            \"prompt\": \"Its most notable feature is {}.\",\n            \"options\": [\n                \"a single white horn\", # Absol, Absol G\n                \"metal armor plates\",  # Aggron\n                \"large wings\",         # Aerodactyl, Beedrill δ\n                \"rabbit ears\",         # Azumarill\n                \"a green plant bulb\",  # Bulbasaur, Venusaur, Celebi&Venu\n                \"a small red spike\",   # Weedle\n                \"big green eyes\",      # Caterpie\n                \"a mustache and spoons\", # Alakazam\n                \"a glowing tail orb\",  # Ampharos\n                \"a fiery mane\",        # Arcanine\n                \"flaming wings\",       # Blaine's Moltres\n                \"a tiny white horn on head\" # Dratini\n            ]\n        },\n        \"body_shape\": {\n            \"prompt\": \"The body shape can be described as {}.\",\n            \"options\": [\n                \"wolf-like on four legs\",   # Absol, Absol G\n                \"bulky and armored\",        # Aggron\n                \"winged and slender\",       # Aerodactyl, Beedrill δ\n                \"round and plump\",          # Azumarill\n                \"sturdy and four-legged\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"long and worm-like\",       # Weedle, Caterpie\n                \"upright and humanoid\",     # Alakazam, Ampharos\n                \"furry and canine\",         # Arcanine\n                \"bird-like with flames\",    # Blaine's Moltres\n                \"serpentine\"                # Dratini\n            ]\n        },\n        \"background_scene\": {\n            \"prompt\": \"The background looks like {}.\",\n            \"options\": [\n                \"outer space\",      # Absol G, Beedrill δ\n                \"green forest\",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&Venu\n                \"a rocky battlefield\", # Absol, Aggron, Aerodactyl\n                \"a purple psychic room\", # Alakazam\n                \"a sunny field\",     # Ampharos\n                \"volcanic ground\",   # Arcanine\n                \"a red sky with embers\", # Blaine's Moltres\n                \"a calm blue lake\"   # Dratini\n            ]\n        }\n    },\n    \n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,      \n            \"body_shape\": 0,    \n            \"background_scene\": 2   \n        },\n        \"Absol G\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,       \n            \"body_shape\": 0,     \n            \"background_scene\": 0    \n        },\n        // ...\n    }\n}\n</code></pre><p>最終的分類將這些屬性概率結合在一起——我們不是進行單一的相似度比較，而是進行多個結構化比較並匯總其概率，以做出更明智的決定。</p><pre><code class=\"language-python\"># Classification process\ndef classify_pokemon(image):\n   # Generate all text prompts\n   all_prompts = []\n   for group in classification_cot:\n       for option in group[\"options\"]:\n           prompt = group[\"prompt\"].format(option)\n           all_prompts.append(prompt)\n\n   # Get embeddings and similarities\n   image_embedding = model.encode_image(image)\n   text_embeddings = model.encode_text(all_prompts)\n   similarities = cosine_similarity(image_embedding, text_embeddings)\n\n   # Convert to probabilities per attribute group\n   probabilities = {}\n   for group_name, group_sims in group_similarities:\n       probabilities[group_name] = softmax(group_sims)\n\n   # Score each Pokemon based on matching attributes\n   scores = {}\n   for pokemon, rules in pokemon_rules.items():\n       score = 0\n       for group, target_idx in rules.items():\n           score += probabilities[group][target_idx]\n       scores[pokemon] = score\n\n   return max(scores, key=scores.get)</code></pre><h3 id=\"complexity-analysis\">複雜度分析</h3><p>假設我們要將一張圖像分類為 <code>N</code> 個寶可夢名稱之一。基準方法需要計算 <code>N</code> 個文本嵌入（每個寶可夢名稱一個）。相比之下，我們的擴展測試時間計算方法需要計算 <code>Q</code> 個文本嵌入，其中</p><code>Q</code> 是所有問題中問題選項組合的總數。兩種方法都需要計算一個圖像嵌入並執行最終的分類步驟，所以我們將這些共同操作排除在比較之外。在這個案例研究中，我們的 <code>N=13</code> 和 <code>Q=52</code>。</p><p>在極端情況下，當 <code>Q = N</code> 時，我們的方法基本上就會退化為基準方法。然而，有效擴展測試時計算的關鍵在於：</p><ul><li>精心構造問題以增加 <code>Q</code></li><li>確保每個問題都能提供關於最終答案的不同、有意義的線索</li><li>設計盡可能正交的問題以最大化它們的聯合資訊增益。</li></ul><p>這種方法類似於\"二十個問題\"遊戲，每個問題都經過策略性選擇，以有效縮小可能的答案範圍。</p><h3 id=\"evaluation\">評估</h3><p>我們的評估在 117 張測試圖像上進行，涵蓋 13 個不同的寶可夢類別。結果如下：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>jina-clip-v1</th>\n<th>jina-clip-v2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>31.36%</td>\n<td>16.10%</td>\n</tr>\n<tr>\n<td>CoT</td>\n<td>46.61%</td>\n<td>38.14%</td>\n</tr>\n<tr>\n<td>Improvement</td>\n<td>+15.25%</td>\n<td>+22.04%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>可以看到，在這個非常規或 OOD 任務上，相同的 CoT 分類對兩個模型都帶來了顯著的改進（分別提升了 15.25% 和 22.04%）。這也表明一旦構建了 <code>pokemon_system</code>，<strong>相同的 CoT 系統可以有效地在不同模型間遷移；且無需微調或後期訓練。</strong></p><p>值得注意的是 v1 在寶可夢分類上相對較強的基準性能（31.36%）。該模型是在 <a href=\"https://www.youtube.com/watch?v=HsGyxVUN1SA&ref=jina-ai-gmbh.ghost.io\">LAION-400M（包含寶可夢相關內容）</a>上訓練的。相比之下，v2 是在 DFN-2B（抽樣 400M 實例）上訓練的，這是一個品質更高但過濾更嚴格的數據集，可能排除了寶可夢相關內容，這解釋了 V2 在這個特定任務上較低的基準性能（16.10%）。</p><h3 id=\"constructing-pokemonsystem-effectively\">有效構建 <code>pokemon_system</code></h3><p>我們的擴展測試時計算方法的有效性很大程度上取決於我們如何構建 <code>pokemon_system</code>。構建這個系統有不同的方法，從手動到完全自動化。</p><h4 id=\"manual-construction\">手動構建</h4><p>最直接的方法是手動分析寶可夢數據集並創建屬性組、提示和規則。領域專家需要識別關鍵的視覺屬性，如顏色、形狀和獨特特徵。然後為每個屬性編寫自然語言提示，列舉每個屬性組的可能選項，並將每個寶可夢映射到其正確的屬性選項。雖然這提供了高質量的規則，但耗時且難以擴展到更大的 <code>N</code>。</p><h4 id=\"llm-assisted-construction\">LLM 輔助構建</h4><p>我們可以利用 LLM 來加速這個過程，通過提示它們來生成分類系統。一個結構良好的提示應要求基於視覺特徵的屬性組、自然語言提示模板、全面且互斥的選項，以及每個寶可夢的映射規則。LLM 可以快速生成初稿，不過其輸出可能需要驗證。</p><pre><code class=\"language-txt\">I need help creating a structured system for Pokemon classification. For each Pokemon in this list: [Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...], create a classification system with:\n\n1. Classification groups that cover these visual attributes:\n   - Dominant color of the Pokemon\n   - What type of creature it appears to be (primary form)\n   - Its most distinctive visual feature\n   - Overall body shape\n   - What kind of background/environment it's typically shown in\n\n2. For each group:\n   - Create a natural language prompt template using \"{}\" for the option\n   - List all possible options that could apply to these Pokemon\n   - Make sure options are mutually exclusive and comprehensive\n\n3. Create rules that map each Pokemon to exactly one option per attribute group, using indices to reference the options\n\nPlease output this as a Python dictionary with two main components:\n- \"classification_groups\": containing prompts and options for each attribute\n- \"pokemon_rules\": mapping each Pokemon to its correct attribute indices\n\nExample format:\n{\n    \"classification_groups\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokemon's body is mainly {} in color\",\n            \"options\": [\"white\", \"gray\", ...]\n        },\n        ...\n    },\n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,  # index for \"white\"\n            ...\n        },\n        ...\n    }\n}</code></pre><p>一個更穩健的方法是將 LLM 生成與人工驗證相結合。首先，LLM 生成初始系統。然後，專家審查並修正屬性分組、選項完整性和規則準確性。LLM 根據這些反饋改進系統，這個過程不斷迭代直到達到滿意的質量。這種方法在效率和準確性之間取得平衡。</p><h4 id=\"automated-construction-with-dspy\">使用 DSPy 自動構建</h4><p>對於完全自動化的方法，我們可以使用 DSPy 來迭代優化 <code>pokemon_system</code>。這個過程從一個簡單的 <code>pokemon_system</code> 開始，可以是手動或由 LLM 編寫的初始提示。每個版本都在保留集上進行評估，使用準確率作為 DSPy 的反饋信號。基於這個性能，生成優化的提示（即新版本的 <code>pokemon_system</code>）。這個循環重複直到收斂，在整個過程中，嵌入模型保持完全不變。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">使用 DSPy 找到最佳的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>pokemon_system</span></code><span style=\"white-space: pre-wrap;\"> CoT 設計；調整過程只需要為每個任務執行一次。</span></figcaption></figure><h2 id=\"why-scale-test-time-compute-for-embedding-models\">為什麼要擴展嵌入模型的測試時計算？</h2><p>因為擴展預訓練最終會在經濟上變得難以承受。</p><p>自 Jina 嵌入套件發布以來——包括 <code>jina-embeddings-v1</code>、<code>v2</code>、<code>v3</code>、<code>jina-clip-v1</code>、<code>v2</code> 和 <code>jina-ColBERT-v1</code>、<code>v2</code>——每次通過擴展預訓練進行的模型升級都伴隨著更多成本。例如，我們的第一個模型 <code>jina-embeddings-v1</code> 於 2023 年 6 月發布，擁有 1.1 億參數。當時訓練它的成本在 5,000 到 10,000 美元之間，取決於如何計算。對於 <code>jina-embeddings-v3</code>，雖然改進顯著，但主要來自於投入資源的增加。頂級模型的成本軌跡已經從數千美元上升到數萬美元，對於較大的 AI 公司，甚至達到了數億美元。雖然在預訓練中投入更多的金錢、資源和數據會產生更好的模型，但邊際回報最終會使進一步擴展在經濟上變得不可持續。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/plot--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/plot--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/plot--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/plot--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/plot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">嵌入模型的擴展定律。英語任務的平均 MTEB 性能與模型參數數量的關係圖。每個點代表一個嵌入模型。突出顯示了代表所有模型的趨勢線，其中多語言模型用青色點表示。該圖表通過從 MTEB 排行榜中選取前 100 個嵌入模型創建，排除了沒有大小信息的模型（通常是閉源或專有模型）。明顯的惡意提交也被過濾掉了。</span></figcaption></figure><p>另一方面，現代嵌入模型正變得越來越強大：多語言、多任務、多模態，並具有強大的零樣本和指令跟隨能力。這種多功能性為算法改進和擴展測試時計算留下了很大空間。</p><p>問題就變成了：用戶願意為他們深切關心的查詢付出多少成本？如果為固定的預訓練模型容忍更長的推理時間能顯著改善結果質量，許多人會認為這是值得的。在我們看來，擴展嵌入模型的測試時計算還有大量未開發的潛力。這代表著一個轉變，從單純增加訓練時的模型大小，轉向在推理階段增加計算努力以實現更好的性能。</p><h2 id=\"conclusion\">結論</h2><p>我們對 <code>jina-clip-v1/v2</code> 的測試時計算案例研究顯示了幾個關鍵發現：</p><ol><li>我們在非常規或分佈外（OOD）數據上取得了更好的性能，無需對嵌入進行任何微調或後期訓練。</li><li>系統通過迭代細化相似度搜索和分類標準，做出了更細微的區分。</li><li>通過引入動態提示調整和迭代推理，我們將嵌入模型的推理過程從單一查詢轉變為更複雜的思維鏈。</li></ol><p>這個案例研究僅僅觸及了測試時計算可能性的表面。在算法上還有很大的擴展空間。例如，我們可以開發方法來迭代選擇最能有效縮小答案空間的問題，類似於\"二十個問題\"遊戲中的最佳策略。通過擴展測試時計算，我們可以推動嵌入模型超越其當前限制，使它們能夠處理曾經看似遙不可及的更複雜、更細緻的任務。</p>",
  "comment_id": "675a84f80ce9930001b86f09",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-12T07:38:48.000+01:00",
  "updated_at": "2024-12-12T17:54:17.000+01:00",
  "published_at": "2024-12-12T17:54:17.000+01:00",
  "custom_excerpt": "Better results scale with compute—more on learning, more on search. A good pretrained model takes you far, but test-time compute takes you further. It's important to recognize this new paradigm of scaling test-time compute, even for embedding models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/scaling-test-time-compute-for-embedding-models/",
  "excerpt": "更好的效果隨著運算能力而擴展——更多學習，更多搜尋。一個優秀的預訓練模型能帶你走很遠，但測試階段的運算能力能讓你走得更遠。即便是對於 embedding 模型，認識到這種擴展測試階段運算能力的新範式也很重要。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
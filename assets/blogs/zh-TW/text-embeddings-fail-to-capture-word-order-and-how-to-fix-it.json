{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "文字嵌入無法捕捉詞序以及如何解決這個問題",
  "html": "<p>最近，<a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> 的創始人 Christoph Schuhmann 分享了一個關於文本嵌入模型的有趣觀察：</p><blockquote>當句子中的詞語被隨機打亂時，它們的文本嵌入之間的餘弦相似度與原句相比仍然保持驚人的高度。</blockquote><p>例如，讓我們看兩個句子：<code>Berlin is the capital of Germany</code> 和 <code>the Germany Berlin is capital of</code>。儘管第二個句子毫無意義，但文本嵌入模型卻無法真正區分它們。使用 <code>jina-embeddings-v3</code>，這兩個句子的餘弦相似度得分為 0.9295。</p><p>詞序並非嵌入模型不太敏感的唯一問題。語法轉換可能會極大地改變句子的含義，但對嵌入距離的影響卻很小。例如，<code>She ate dinner before watching the movie</code> 和 <code>She watched the movie before eating dinner</code> 的餘弦相似度為 0.9833，儘管它們的動作順序完全相反。</p><p>如果沒有<a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">特殊訓練</a>，否定詞的一致性嵌入也是出了名的困難 —— <code>This is a useful model</code> 和 <code>This is not a useful model</code> 在嵌入空間中看起來幾乎完全相同。通常，將文本中的詞替換為相同類別的其他詞，比如將\"today\"改為\"yesterday\"，或改變動詞時態，對嵌入的改變也沒有你想象的那麼大。</p><p>這帶來了嚴重的影響。考慮兩個搜索查詢：<code>Flight from Berlin to Amsterdam</code> 和 <code>Flight from Amsterdam to Berlin</code>。它們的嵌入幾乎完全相同，<code>jina-embeddings-v3</code> 給它們的餘弦相似度為 0.9884。對於旅遊搜索或物流這樣的實際應用來說，這個缺陷是致命的。</p><p>在本文中，我們探討嵌入模型面臨的挑戰，研究它們在詞序和詞語選擇方面持續存在的問題。我們分析了跨語言類別的關鍵失效模式——包括方向、時間、因果、比較和否定上下文——同時探索提高模型性能的策略。</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">為什麼打亂的句子會有驚人的相近餘弦分數？</h2><p>起初，我們認為這可能是由於模型組合詞義的方式——它為每個詞創建一個嵌入（在我們上面的示例句子中有 6-7 個詞），然後通過平均池化將這些嵌入平均在一起。這意味著最終的嵌入幾乎沒有詞序信息。不管值的順序如何，平均值都是相同的。</p><p>然而，即使使用 CLS 池化的模型（通過觀察特殊的第一個詞來理解整個句子，應該對詞序更敏感）也存在相同的問題。例如，<code>bge-1.5-base-en</code> 對句子 <code>Berlin is the capital of Germany</code> 和 <code>the Germany Berlin is capital of</code> 仍然給出 0.9304 的餘弦相似度分數。</p><p>這指出了嵌入模型訓練方式的一個局限性。雖然語言模型在預訓練期間最初學習句子結構，但在對比訓練——我們用來創建嵌入模型的過程中，似乎失去了一些這種理解。</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">文本長度和詞序如何影響嵌入相似度？</h2><p>為什麼模型在處理詞序時會遇到困難？首先想到的是文本的長度（以 token 為單位）。當文本被送入編碼函數時，模型首先生成一個 token 嵌入列表（即，每個分詞後的詞都有一個專門的向量來表示其含義），然後對它們進行平均。</p><p>為了了解文本長度和詞序如何影響嵌入相似度，我們生成了一個<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">包含 180 個不同長度合成句子的數據集</a>，分別包含 3、5、10、15、20 和 30 個 token。我們還隨機打亂了 token 以形成每個句子的變體：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>以下是一些例子：</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">長度 (tokens)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">原始句子</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">打亂後的句子</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>我們將使用我們自己的 <code>jina-embeddings-v3</code> 模型和開源模型 <code>bge-base-en-v1.5</code> 對數據集進行編碼，然後計算原始句子和打亂句子之間的餘弦相似度：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>長度 (tokens)</th>\n<th>平均餘弦相似度</th>\n<th>餘弦相似度標準差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.947</td>\n<td>0.053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.909</td>\n<td>0.052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.924</td>\n<td>0.031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.918</td>\n<td>0.019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.899</td>\n<td>0.021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.874</td>\n<td>0.025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>現在我們可以生成一個箱形圖，這使得餘弦相似度的趨勢更加清晰：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">圖 1：使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> 和 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\">（未經微調）的打亂句子按句子長度的相似度分布</span></figcaption></figure><p>如我們所見，嵌入的平均餘弦相似度存在明顯的線性關係。文本越長，原始句子和隨機打亂句子之間的平均餘弦相似度得分越低。這可能是由於\"詞語位移\"造成的，也就是詞在隨機打亂後離開其原始位置的距離。在較短的文本中，token 可以被打亂到的\"位置\"較少，所以不能移動太遠，而較長的文本有更多潛在的排列組合，詞可以移動更大的距離。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 2：依照字詞數量的句子組合</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">我們在此停止列表，因為組合數是字數的階乘。當我們到達三十個字時，就會有 265 個廿九（2.652528598 E+32）種組合。</div></div><p>如下圖所示（餘弦相似度 vs. 平均字詞位移），文本越長，字詞位移就越大：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 3：使用打亂句子數據集的餘弦相似度 vs. 平均字詞位移，展示平均字詞位移與餘弦不相似度的相關性。</span></figcaption></figure><p>字詞嵌入取決於局部上下文，也就是最靠近它們的字詞。在短文本中，重新排列字詞不會大幅改變該上下文。然而，對於較長的文本，一個字詞可能會被移動到離其原始上下文很遠的地方，這可能會大幅改變其字詞嵌入。因此，打亂較長文本中的字詞會產生比較短文本更遠的嵌入距離。上圖顯示，無論是使用平均池化的 <code>jina-embeddings-v3</code>，還是使用 CLS 池化的 <code>bge-base-en-v1.5</code>，都呈現相同的關係：打亂較長的文本並使字詞位移更遠會導致較小的相似度分數。</p><h2 id=\"do-bigger-models-solve-the-problem\">更大的模型能解決問題嗎？</h2><p>通常，當我們面臨這類問題時，一個常見的策略是直接使用更大的模型。但更大的文本嵌入模型真的能更有效地捕捉字詞順序資訊嗎？根據文本嵌入模型的擴展定律（<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">在我們的 <code>jina-embeddings-v3</code> 發布文章中提到</a>），更大的模型通常會提供更好的性能：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 4：嵌入模型的擴展定律，顯示 MTEB 性能如何隨參數數量擴展。</span></figcaption></figure><p>但更大的模型真的能更有效地捕捉字詞順序資訊嗎？我們測試了 BGE 模型的三個變體：<a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>、<a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a> 和 <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>，參數規模分別為 3300 萬、1.1 億和 3.35 億。</p><p>我們將使用與之前相同的 180 個句子，但不考慮長度資訊。我們將使用這三個模型變體對原始句子及其隨機打亂的版本進行編碼，並繪製平均餘弦相似度：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 5：使用打亂句子數據集的模型規模對字詞順序敏感度的影響，使用 </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">、</span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\"> 和 </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">。</span></figcaption></figure><p>雖然我們可以看到較大的模型對字詞順序變化更敏感，但差異很小。即使是規模大得多的 <code>bge-large-en-v1.5</code> 在區分打亂和未打亂的句子時，最多也只是略微好一點。在決定嵌入模型對字詞重新排序的敏感度時，其他因素也會發揮作用，特別是訓練方案的差異。此外，餘弦相似度是衡量模型區分能力的一個非常有限的工具。然而，我們可以看到模型規模並不是主要考慮因素。我們不能簡單地讓模型變得更大來解決這個問題。</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">現實世界中的字詞順序和字詞選擇</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">在本文的大部分內容中，我們使用的是 <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\">jina-embeddings-v2</code></a>（<i><em class=\"italic\" style=\"white-space: pre-wrap\">不是</em></i> 我們最新的模型 <code spellcheck=\"false\" style=\"white-space: pre-wrap\">jina-embeddings-v3</code>），因為 <code spellcheck=\"false\" style=\"white-space: pre-wrap\">v2</code> 小得多，因此在我們的本地 GPU 上實驗時速度更快，參數量為 1.37 億，相比 <code spellcheck=\"false\" style=\"white-space: pre-wrap\">v3</code> 的 5.8 億。</div></div><p>如我們在導言中提到的，字詞順序並不是嵌入模型唯一面臨的挑戰。更現實的實際挑戰是關於字詞<em>選擇</em>。有許多方式可以改變句子中的字詞——這些方式在嵌入中並沒有得到很好的反映。我們可以把「She flew from Paris to Tokyo」改成「She drove from Tokyo to Paris」，而嵌入仍然保持相似。我們已經將這些變化分類如下：</p><!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>類別</th>\n<th>範例 - 左</th>\n<th>範例 - 右</th>\n<th>餘弦相似度（<code>jina</code>）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>方向性</td>\n<td>She flew from Paris to Tokyo</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>時間性</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>因果性</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>比較性</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>否定</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">請注意，這些是我們在工作中觀察到的常見情況，並不一定代表一個完整的分類體系。</div></div><p>上表顯示了一些「失敗案例」，其中文本嵌入模型無法捕捉細微的詞序變化。這符合我們的預期：文本嵌入模型缺乏推理能力。例如，模型並不理解 \"from\" 和 \"to\" 之間的關係。文本嵌入模型執行語意匹配，語意通常在 token 層級捕捉，然後在池化後被壓縮成單一的密集向量。相比之下，<a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">在萬億 token 規模的大型數據集上訓練的 LLM（自回歸模型）開始展示出推理的突現能力</a>。</p><p>這讓我們思考，我們能否通過對比學習使用三元組來微調嵌入模型，使查詢和正例更接近，同時推開查詢和負例？</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 6：對比學習的效果：將查詢和正例拉近，將負例推遠。</span></figcaption></figure><p>例如，\"Flight from Amsterdam to Berlin\" 可以被視為 \"Flight from Berlin to Amsterdam\" 的負例對。事實上，在 <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v1</code> 技術報告</a>（Michael Guenther 等人）中，我們已經小規模地解決了這個問題：我們在一個由大型語言模型生成的<a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">否定數據集</a>（包含 10,000 個示例）上微調了 <code>jina-embeddings-v1</code> 模型。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>報告中的結果很有希望：</p><blockquote>我們觀察到，在所有模型大小中，在三元組數據（包括我們的否定訓練數據集）上進行微調都顯著提升了性能，特別是在 HardNegation 任務上。</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 7：表格顯示了不同大小的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap\"> 模型在成對訓練和組合三元組/成對訓練下的 EasyNegation 和 HardNegation 分數。</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">圖 8：不同版本 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap\"> 訓練策略的性能比較。</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">使用精選數據集微調文本嵌入模型</h2><p>在前面的部分中，我們探討了關於文本嵌入的幾個關鍵觀察：</p><ol><li>較短的文本更容易在捕捉詞序方面出錯。</li><li>增加文本嵌入模型的大小並不一定能改善詞序理解。</li><li>對比學習可能為這些問題提供潛在的解決方案。</li></ol><p>基於這些認識，我們在我們的否定和詞序數據集（總共約 11,000 個訓練樣本）上微調了 <code>jina-embeddings-v2-base-en</code> 和 <code>bge-base-en-1.5</code>：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>為了幫助評估微調效果，我們生成了一個包含 1,000 個三元組的<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">數據集</a>，每個三元組包含一個 <code>query</code>、<code>positive (pos)</code> 和 <code>negative (neg)</code> 案例：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>這是一個示例行：</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>這些三元組旨在覆蓋各種失敗案例，包括由於詞序變化導致的<strong>方向性</strong>、<strong>時間性</strong>和<strong>因果性</strong>意義轉變。</p><p>我們現在可以在三個不同的評估集上評估模型：</p><ol><li>180 個合成句子的集合（來自本文早前部分），隨機打亂。</li><li>5 個手動檢查的例子（來自上面的方向性/因果性等表格）。</li><li>94 個來自我們剛剛生成的<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">三元組數據集</a>的精選三元組。</li></ol><p>以下是微調前後對打亂句子的差異：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>句子長度（tokens）</th>\n<th>平均餘弦相似度（<code>jina</code>）</th>\n<th>平均餘弦相似度（<code>jina-ft</code>）</th>\n<th>平均餘弦相似度（<code>bge</code>）</th>\n<th>平均餘弦相似度（<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n\n<!--kg-card-end: html-->\n<p>結果很明顯：儘管微調過程只花了五分鐘，我們在隨機打亂的句子數據集上看到了顯著的性能改善：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">圖 9：使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> 和 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\">（已微調）在打亂句子上的相似度分布。</span></figcaption></figure><p>我們在方向、時間、因果和比較案例上也看到了改善。模型表現出明顯的性能提升，這反映在平均餘弦相似度的下降上。最大的性能提升出現在否定案例上，這是因為我們的微調數據集包含了 10,000 個否定訓練範例。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>類別</th>\n<th>範例 - 左</th>\n<th>範例 - 右</th>\n<th>平均餘弦相似度 (<code>jina</code>)</th>\n<th>平均餘弦相似度 (<code>jina-ft</code>)</th>\n<th>平均餘弦相似度 (<code>bge</code>)</th>\n<th>平均餘弦相似度 (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>方向</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>時間</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>因果</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>比較</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>否定</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">結論</h2><p>在這篇文章中，我們深入探討了文本嵌入模型面臨的挑戰，特別是它們在有效處理詞序方面的困難。總的來說，我們識別出了五種主要的失敗類型：<strong>方向</strong>、<strong>時間</strong>、<strong>因果</strong>、<strong>比較</strong>和<strong>否定</strong>。在這些查詢中，詞序非常重要，如果您的使用場景涉及其中任何一種，了解這些模型的局限性是很有價值的。</p><p>我們還進行了一個快速實驗，將以否定為重點的數據集擴展到覆蓋所有五種失敗類別。結果令人鼓舞：通過精心選擇的「硬性否例」進行微調，使模型更好地識別哪些項目應該組合在一起，哪些不應該。話雖如此，還有更多工作要做。未來的步驟包括深入研究數據集的大小和質量如何影響性能。</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "文字嵌入模型在捕捉微妙的語言細微差別時仍有困難，例如詞序、方向性關係、時序、因果關係、比較以及否定等。了解這些挑戰對於提升模型效能至關重要。",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
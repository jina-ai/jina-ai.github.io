{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "바이너리 임베딩: AI의 모든 것을, 3.125%의 용량으로",
  "html": "<p>임베딩은 다양한 AI와 자연어 처리 애플리케이션의 핵심이 되었으며, 텍스트의 의미를 고차원 벡터로 표현하는 방법을 제공합니다. 하지만 모델의 크기가 커지고 AI 모델이 처리하는 데이터의 양이 증가함에 따라, 전통적인 임베딩의 계산 및 저장 요구사항이 급증했습니다. 바이너리 임베딩은 높은 성능을 유지하면서도 리소스 요구사항을 크게 줄이는 컴팩트하고 효율적인 대안으로 도입되었습니다.</p><p>바이너리 임베딩은 임베딩 벡터의 크기를 최대 96%(Jina Embeddings의 경우 96.875%)까지 줄여 이러한 리소스 요구사항을 완화하는 한 가지 방법입니다. 사용자는 정확도를 최소한으로 손실하면서 AI 애플리케이션에서 컴팩트한 바이너리 임베딩의 성능을 활용할 수 있습니다.</p><h2 id=\"what-are-binary-embeddings\">바이너리 임베딩이란 무엇인가?</h2><p>바이너리 임베딩은 전통적인 고차원 부동소수점 벡터를 바이너리 벡터로 변환하는 특수한 형태의 데이터 표현입니다. 이는 임베딩을 압축할 뿐만 아니라 벡터의 무결성과 유용성을 거의 모두 유지합니다. 이 기법의 본질은 변환 후에도 데이터 포인트 간의 의미론적 관계와 거리를 유지하는 능력에 있습니다.<br><br>바이너리 임베딩의 마법은 양자화에 있습니다. 이는 고정밀 숫자를 저정밀 숫자로 변환하는 방법입니다. AI 모델링에서 이는 종종 임베딩의 32비트 부동소수점 숫자를 8비트 정수와 같은 더 적은 비트의 표현으로 변환하는 것을 의미합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">이진화는 모든 스칼라 값을 0 또는 1로 변환하는 것으로, 컬러 이미지를 검은색과 흰색 픽셀만 있는 이미지로 변환하는 것과 같습니다. 이미지: 神奈川沖浪裏 (1831) by 葛飾 (Hokusai)</span></figcaption></figure><p>바이너리 임베딩은 이를 극한으로 가져가 각 값을 0 또는 1로 줄입니다. 32비트 부동소수점 숫자를 이진 숫자로 변환하면 임베딩 벡터의 크기가 32배 줄어들어 96.875%의 감소를 가져옵니다. 결과적으로 벡터 연산이 훨씬 더 빨라집니다. 일부 마이크로칩에서 사용 가능한 하드웨어 가속을 사용하면 벡터가 이진화될 때 벡터 비교 속도가 32배 이상 증가할 수 있습니다.</p><p>이 과정에서 일부 정보는 불가피하게 손실되지만, 모델이 매우 우수한 성능을 보일 때 이 손실은 최소화됩니다. 서로 다른 것들의 비양자화 임베딩이 최대한 다르다면, 이진화는 그 차이를 잘 보존할 가능성이 더 높습니다. 그렇지 않으면 임베딩을 정확하게 해석하기 어려울 수 있습니다.</p><p>Jina Embeddings 모델은 정확히 그런 방식으로 매우 강건하게 학습되어 이진화에 매우 적합합니다.</p><p>이러한 컴팩트한 임베딩은 특히 모바일과 시간에 민감한 사용과 같은 리소스 제약이 있는 환경에서 새로운 AI 애플리케이션을 가능하게 합니다.</p><p>아래 차트에서 보듯이 이러한 비용과 컴퓨팅 시간의 이점은 상대적으로 적은 성능 비용으로 얻을 수 있습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10: </em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">Normalized Discounted Cumulative Gain</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">을 사용하여 상위 10개 결과에 대해 계산된 점수.</em></i></figcaption></figure><p><code>jina-embeddings-v2-base-en</code>의 경우, 이진 양자화는 검색 정확도를 47.13%에서 42.05%로 약 10% 감소시킵니다. <code>jina-embeddings-v2-base-de</code>의 경우, 이 손실은 44.39%에서 42.65%로 단 4%에 불과합니다.</p><p>Jina Embeddings 모델이 이진 벡터 생성에서 이렇게 우수한 성능을 보이는 이유는 더 균일한 임베딩 분포를 만들도록 학습되었기 때문입니다. 이는 두 개의 서로 다른 임베딩이 다른 모델의 임베딩보다 더 많은 차원에서 서로 더 멀리 떨어져 있을 가능성이 높다는 것을 의미합니다. 이러한 특성은 그 거리가 이진 형태에서도 더 잘 표현되도록 보장합니다.</p><h2 id=\"how-do-binary-embeddings-work\">바이너리 임베딩은 어떻게 작동하는가?</h2><p>이것이 어떻게 작동하는지 보기 위해 세 개의 임베딩 <em>A</em>, <em>B</em>, <em>C</em>를 고려해 보겠습니다. 이 세 개는 모두 이진화되지 않은 완전한 부동소수점 벡터입니다. 이제 <em>A</em>에서 <em>B</em>까지의 거리가 <em>B</em>에서 <em>C</em>까지의 거리보다 크다고 가정해 봅시다. 임베딩에서는 일반적으로 <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">코사인 거리</a>를 사용합니다. 따라서: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p><em>A</em>, <em>B</em>, <em>C</em>를 이진화하면 <a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">해밍 거리</a>를 사용하여 거리를 더 효율적으로 측정할 수 있습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">큐브에서의 해밍 거리. 왼쪽: A에서 B까지의 거리는 1. 오른쪽: B에서 C까지의 거리는 2.</span></figcaption></figure><p><em>A</em>, <em>B</em>, <em>C</em>의 이진화된 버전을 각각 <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em>, <em>C<sub>bin</sub></em>이라고 부릅시다.</p>\n<p>이진 벡터의 경우, <em>A<sub>bin</sub></em>과 <em>B<sub>bin</sub></em> 사이의 코사인 거리가 <em>B<sub>bin</sub></em>과 <em>C<sub>bin</sub></em> 사이의 거리보다 크다면, <em>A<sub>bin</sub></em>과 <em>B<sub>bin</sub></em> 사이의 해밍 거리는 <em>B<sub>bin</sub></em>과 <em>C<sub>bin</sub></em> 사이의 해밍 거리보다 크거나 같습니다.</p>\n<p>따라서: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>해밍 거리의 경우: </p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>이상적으로, 임베딩을 이진화할 때 전체 임베딩의 관계가 이진 임베딩에서도 동일하게 유지되기를 원합니다. 이는 부동소수점 코사인에서 한 거리가 다른 거리보다 크다면, 이진화된 등가물 사이의 해밍 거리에서도 더 커야 한다는 것을 의미합니다:</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>모든 임베딩 삼중항에 대해 이것을 참으로 만들 수는 없지만, 거의 모든 경우에 대해 참이 되도록 할 수 있습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">파란색 점은 전체 부동소수점 벡터에 해당하고 빨간색 점은 이진화된 등가물에 해당합니다. </span></figcaption></figure><p>이진 벡터에서는 모든 차원을 존재(1) 또는 부재(0)로 처리할 수 있습니다. 두 벡터가 비이진 형태에서 서로 멀수록, 하나의 차원에서 하나는 양수 값을 가지고 다른 하나는 음수 값을 가질 확률이 높아집니다. 이는 이진 형태에서 하나는 0을 가지고 다른 하나는 1을 가지는 차원이 더 많을 가능성이 높다는 것을 의미합니다. 이로 인해 해밍 거리로 측정했을 때 더 멀리 떨어지게 됩니다.</p><p>반대로 더 가까운 벡터들에게는 반대가 적용됩니다: 비이진 벡터들이 가까울수록, 모든 차원에서 둘 다 0을 가지거나 둘 다 1을 가질 확률이 높아집니다. 이는 해밍 거리로 측정했을 때 더 가깝게 만듭니다.</p><p>Jina Embeddings 모델이 이진화에 매우 적합한 이유는 네거티브 마이닝과 다른 미세 조정 방법을 사용하여 특히 유사하지 않은 것들 사이의 거리를 늘리고 유사한 것들 사이의 거리를 줄이도록 학습되었기 때문입니다. 이는 임베딩을 더 강건하게 만들고, 유사성과 차이점에 더 민감하게 만들며, 이진 임베딩 간의 해밍 거리를 비이진 임베딩 간의 코사인 거리에 더 비례하게 만듭니다.</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">Jina AI의 바이너리 임베딩으로 얼마나 절약할 수 있는가?</h2><p>Jina AI의 바이너리 임베딩 모델을 채택하면 시간에 민감한 애플리케이션의 지연 시간을 낮출 뿐만 아니라, 아래 표에서 볼 수 있듯이 상당한 비용 이점도 얻을 수 있습니다:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Memory per<br/>250 million<br/>embeddings</th>\n<th>Retrieval<br/>benchmark<br/>average</th>\n<th>Estimated price on AWS<br/>($3.8 per GB/month<br/>with x2gb instances)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32-bit floating point embeddings</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>Binary embeddings</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>이러한 95% 이상의 절감은 검색 정확도의 ~10% 감소만을 동반합니다.</p><p>이는 <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">OpenAI의 Ada 2 모델</a>이나 <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Cohere의 Embed v3</a>의 이진화된 벡터를 사용하는 것보다 더 큰 절감 효과입니다. 두 모델 모두 1024차원 이상의 출력 임베딩을 생성합니다. Jina AI의 임베딩은 768차원만을 가지면서도 다른 모델들과 비슷한 성능을 보이며, 동일한 정확도에서도 양자화 이전에 더 작은 크기를 가집니다.</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">이진 벡터는 메모리, 연산 시간, 전송 대역폭, 디스크 저장 공간을 절약하여 여러 측면에서 재정적 이점을 제공합니다</strong></b>. </div></div><p>이러한 절감은 희소 자원과 에너지 사용을 줄여 환경적으로도 도움이 됩니다.</p><h2 id=\"get-started\">시작하기</h2><p><a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a>를 사용하여 이진 임베딩을 얻으려면, API 호출에 <code>encoding_type</code> 파라미터를 추가하고 부호가 있는 정수로 인코딩된 이진화 임베딩을 얻으려면 <code>binary</code> 값을, 부호 없는 정수의 경우 <code>ubinary</code> 값을 사용하면 됩니다.</p><h3 id=\"directly-access-jina-embedding-api\">Jina Embedding API 직접 접근하기</h3><p><code>curl</code> 사용:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>또는 Python <code>requests</code> API를 통해:</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>위의 Python <code>request</code>로 <code>response.json()</code>을 검사하면 다음과 같은 응답을 받게 됩니다:</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>이는 96개의 8비트 부호 있는 정수로 저장된 두 개의 이진 임베딩 벡터입니다. 이를 768개의 0과 1로 풀어내려면 <code>numpy</code> 라이브러리를 사용해야 합니다:</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>결과는 0과 1로만 이루어진 768차원 벡터입니다:</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Qdrant에서 이진 양자화 사용하기</h3><p><a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">Qdrant의 통합 라이브러리</a>를 사용하여 이진 임베딩을 Qdrant 벡터 저장소에 직접 저장할 수 있습니다. Qdrant는 내부적으로 <code>BinaryQuantization</code>을 구현했기 때문에, 전체 벡터 컬렉션에 대한 사전 설정 구성으로 사용할 수 있어 코드를 다르게 수정하지 않고도 이진 벡터를 검색하고 저장할 수 있습니다.</p><p>아래 예시 코드를 참조하세요:</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Jina API 키를 제공하고 사용 가능한 모델 중 하나를 선택하세요.\n# 여기에서 무료 평가판 키를 받을 수 있습니다: https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # 또는 \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # small 변형의 경우 512\n\n# API에서 임베딩 가져오기\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Qdrant에 임베딩 인덱싱하기\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>검색을 구성하려면 <code>oversampling</code>과 <code>rescore</code> 매개변수를 사용해야 합니다:</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">LlamaIndex 사용하기</h3><p>LlamaIndex에서 Jina 바이너리 임베딩을 사용하려면 <code>JinaEmbedding</code> 객체를 인스턴스화할 때 <code>encoding_queries</code> 매개변수를 <code>binary</code>로 설정하세요:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# https://jina.ai/embeddings/에서 무료 평가판 키를 받을 수 있습니다\nJINA_API_KEY = \"<YOUR API KEY>\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">바이너리 임베딩을 지원하는 다른 벡터 데이터베이스</h3><p>다음 벡터 데이터베이스들이 바이너리 벡터를 기본적으로 지원합니다:</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">DataStax의 AstraDB</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">예시</h2><p>바이너리 임베딩의 실제 사용을 보여드리기 위해 <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a>에서 초록들을 선택하여 <code>jina-embeddings-v2-base-en</code>을 사용해 32비트 부동소수점과 바이너리 벡터를 모두 생성했습니다. 그런 다음 \"3D segmentation\"이라는 예시 쿼리의 임베딩과 비교했습니다.</p><p>아래 표에서 볼 수 있듯이 상위 3개 답변은 동일하고 상위 5개 중 4개가 일치합니다. 바이너리 벡터를 사용하면 거의 동일한 상위 매치를 생성합니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">바이너리</th>\n  <th colspan=\"2\">32비트 부동소수점</th>\n</tr>\n<tr>\n<th>순위</th>\n<th>해밍<br/>거리</th>\n<th>일치하는 텍스트</th>\n<th>코사인</th>\n<th>일치하는 텍스트</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "AI 모델처럼 견고하고 부정확한 것에는 32-bit는 너무 많은 정밀도입니다. 그래서 우리는 그중 31개를 없앴습니다! Binary embedding은 더 작고, 더 빠르며, 매우 높은 성능을 보여줍니다.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
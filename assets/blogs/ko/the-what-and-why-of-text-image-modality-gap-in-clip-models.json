{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "CLIP 모델에서 텍스트-이미지 모달리티 갭의 의미와 이유",
  "html": "<p><a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">시맨틱 임베딩</a>은 챗봇과 AI 아트 모델을 포함한 현대 AI 모델의 핵심입니다. 때로는 사용자에게 보이지 않지만, 표면 아래에 여전히 존재합니다.</p><p>임베딩 이론은 두 가지 부분으로 구성됩니다:</p><ol><li>사물 — AI 모델 외부의 것들, 텍스트와 이미지 같은 것들 — 은 그 사물들에 대한 데이터로부터 AI 모델이 생성한 벡터로 표현됩니다.</li><li>AI 모델 외부의 사물들 간의 관계는 그 벡터들 간의 공간적 관계로 표현됩니다. 우리는 AI 모델이 그러한 방식으로 작동하는 벡터를 생성하도록 특별히 훈련시킵니다.</li></ol><p>이미지-텍스트 멀티모달 모델을 만들 때, 우리는 그림의 임베딩과 그 그림을 설명하거나 관련된 텍스트의 임베딩이 상대적으로 가깝게 위치하도록 모델을 훈련시킵니다. 두 벡터가 나타내는 것들 — 이미지와 텍스트 — 사이의 의미적 유사성이 두 벡터 간의 공간적 관계에 반영됩니다.</p><p>예를 들어, 오렌지 이미지의 임베딩 벡터와 \"신선한 오렌지\"라는 텍스트가 같은 이미지와 \"신선한 사과\"라는 텍스트보다 더 가까울 것이라고 합리적으로 예상할 수 있습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration on a black background showing an orange and an apple with arrows between them and quotes reading &quot;A fresh orange\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이것이 임베딩 모델의 목적입니다: 우리가 관심을 가지는 특성들 — 이미지에 묘사된 과일의 종류나 텍스트에 언급된 과일의 종류 같은 — 이 그들 사이의 거리에 보존되는 표현을 생성하는 것입니다.</p><p>하지만 멀티모달리티는 다른 무언가를 도입합니다. 오렌지 사진이 \"신선한 오렌지\"라는 텍스트보다 사과 사진에 더 가깝고, \"신선한 사과\"라는 텍스트가 사과 이미지보다 다른 텍스트에 더 가까울 수 있다는 것을 발견할 수 있습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Black background featuring an apple on the left and an orange on the right with annotated arrows marked &quot;A fresh apple.&quot; and \" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이것이 Jina AI의 <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP 모델</a> (<code>jina-clip-v1</code>)을 포함한 멀티모달 모델에서 실제로 일어나는 현상입니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>이를 테스트하기 위해, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k 테스트 세트</a>에서 1,000개의 텍스트-이미지 쌍을 샘플링했습니다. 각 쌍은 5개의 캡션 텍스트(따라서 엄밀히 말하면 쌍이 아님)와 하나의 이미지를 포함하며, 5개의 텍스트 모두 동일한 이미지를 설명합니다.</p><p>예를 들어, 다음 이미지 (<code>1245022983_fb329886dd.jpg</code> in the Flickr8k dataset):</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"A young girl in a pink skirt playing with a frisbee in an urban outdoor setting with cars and bikes present.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>이미지의 다섯 개 캡션:</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>우리는 Jina CLIP을 사용하여 이미지와 텍스트를 임베딩한 후:</p><ol><li>이미지 임베딩과 해당 캡션 텍스트의 임베딩 간의 코사인 유사도를 비교했습니다.</li><li>동일한 이미지를 설명하는 5개의 캡션 텍스트 임베딩을 가져와 서로 간의 코사인 유사도를 비교했습니다.</li></ol><p>그 결과 그림 1에서 볼 수 있듯이 놀라울 정도로 큰 차이가 나타났습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graph with two curves showing the distribution of Cosine Similarity for Image2Text and Text2Text pairs with labeled axes.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 1: Jina CLIP에서 매칭되는 이미지-텍스트 쌍과 텍스트-텍스트 쌍 간의 코사인 유사도 값 분포.</span></figcaption></figure><p>몇 가지 예외를 제외하고, 매칭되는 텍스트 쌍들은 매칭되는 이미지-텍스트 쌍들보다 훨씬 더 가깝습니다. 이는 Jina CLIP이 텍스트를 임베딩 공간의 한 부분에, 이미지는 그로부터 상대적으로 멀리 떨어진 대체로 분리된 부분에 인코딩하고 있다는 것을 강력하게 시사합니다. 텍스트와 사진 사이의 이 공간이 바로 <em>멀티모달 간극</em>입니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagram on black background depicting 'Images' on left, 'Texts' on bottom, with labeled 'Multimodal Gap' in the center.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>멀티모달 임베딩 모델은 우리가 관심을 가지는 의미적 정보 이상을 인코딩하고 있습니다: 그들은 입력의 매체를 인코딩하고 있습니다. Jina CLIP에 따르면, 속담처럼 한 장의 그림이 천 마디 말의 가치가 있는 것은 아닙니다. 그것은 어떤 말로도 완전히 동등하게 표현할 수 없는 내용을 가지고 있습니다. 아무도 그렇게 훈련시키지 않았음에도 불구하고, 입력 매체를 임베딩의 의미론에 인코딩합니다.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">우리가 단순히 사진과 텍스트를 비교하는 한에서는 이것이 문제가 되지 않지만, 진정한 멀티모달 모델은 예를 들어 \"이것은 사과입니다\"라는 텍스트가 오렌지에 대한 텍스트보다 사과 사진에 더 잘 매칭된다는 것을 알려줄 수 있어야 합니다. 현재 형태의 CLIP 스타일 모델은 그렇게 할 수 없습니다.</div></div><p>이 현상은 <em>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>] 논문에서 \"모달리티 간극\"이라고 불리며 연구되었습니다. 모달리티 간극은 임베딩 공간에서 한 매체의 입력과 다른 매체의 입력 사이의 공간적 분리를 의미합니다. 모델이 의도적으로 그러한 간극을 가지도록 훈련되지는 않았지만, 이는 멀티모달 모델에서 광범위하게 발생합니다.</p><p>Jina CLIP의 모달리티 간극에 대한 우리의 조사는 <a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>의 연구에 크게 기반을 두고 있습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">모달리티 간극은 어디서 오는가?</h2><p>Liang et al. [2022]은 모달리티 간극의 세 가지 주요 원인을 확인했습니다:</p><ul><li>\"원뿔 효과\"라고 부르는 초기화 편향.</li><li>이 편향을 \"학습 해제\"하기 매우 어렵게 만드는 훈련 중의 온도(무작위성) 감소.</li><li>멀티모달 모델에서 널리 사용되는, 의도치 않게 간극을 강화하는 대조 학습 절차.</li></ul><p>각각을 차례로 살펴보겠습니다.</p><h3 id=\"cone-effect\">원뿔 효과</h3><p>CLIP 또는 CLIP 스타일 아키텍처로 구축된 모델은 실제로 두 개의 별도 임베딩 모델이 연결된 것입니다. 이미지-텍스트 멀티모달 모델의 경우, 아래 도식과 같이 텍스트를 인코딩하는 모델 하나와 이미지를 인코딩하는 완전히 별도의 모델이 있습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이 두 모델은 텍스트가 이미지를 잘 설명할 때 이미지 임베딩과 텍스트 임베딩이 상대적으로 가까워지도록 훈련됩니다.</p><p>두 모델의 가중치를 무작위로 초기화한 다음, 이미지와 텍스트 쌍을 함께 제시하고 두 출력 간의 거리를 최소화하도록 처음부터 훈련시켜 이런 모델을 만들 수 있습니다. <a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">최초의 OpenAI CLIP 모델</a>은 이런 방식으로 훈련되었습니다. 하지만 이는 많은 이미지-텍스트 쌍과 계산 비용이 많이 드는 훈련이 필요합니다. 첫 CLIP 모델의 경우, OpenAI는 인터넷에서 캡션이 달린 자료로부터 4억 개의 이미지-텍스트 쌍을 수집했습니다.</p><p>최근의 CLIP 스타일 모델들은 사전 훈련된 컴포넌트를 사용합니다<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">.</a> 이는 각 컴포넌트를 좋은 단일 모드 임베딩 모델로 따로 훈련시키는 것을 의미하며, 하나는 텍스트용이고 다른 하나는 이미지용입니다. 그런 다음 이 두 모델은 이미지-텍스트 쌍을 사용하여 함께 추가 훈련되는데, 이를 <em>대조적 튜닝</em>이라고 합니다. 정렬된 이미지-텍스트 쌍을 사용하여 매칭되는 텍스트와 이미지 임베딩은 더 가깝게, 매칭되지 않는 것들은 더 멀어지도록 가중치를 천천히 \"조정\"합니다.</p><p>이 접근 방식은 일반적으로 구하기 어렵고 비용이 많이 드는 이미지-텍스트 쌍 데이터가 더 적게 필요하며, 캡션 없이 구하기가 훨씬 쉬운 일반 텍스트와 이미지를 대량으로 사용합니다. Jina CLIP (<code>jina-clip-v1</code>)은 이 후자의 방법을 사용하여 훈련되었습니다. 일반 텍스트 데이터를 사용하여 텍스트 인코딩용 <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2 모델</a>을 사전 훈련했고, 사전 훈련된 <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">EVA-02 이미지 인코더</a>를 사용한 다음, <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a>에 설명된 대로 다양한 대조적 훈련 기법을 사용하여 추가 훈련을 진행했습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 2: Jina CLIP에서 쌍 훈련 전 이미지와 텍스트 임베딩의 2차원 초기 위치.</span></figcaption></figure><p>이미지-텍스트 쌍으로 훈련하기 전에 이 두 사전 훈련 모델의 출력을 살펴보면 중요한 점을 발견할 수 있습니다. 위의 그림 2는 사전 훈련된 EVA-02 인코더가 생성한 이미지 임베딩과 사전 훈련된 JinaBERT v2가 생성한 텍스트 임베딩을 2차원으로 <a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">UMAP 투영</a>한 것이며, 회색 선은 매칭된 이미지-텍스트 쌍을 나타냅니다. 이는 교차 모달 훈련 이전의 상태입니다.</p><p>결과는 일종의 절단된 \"원뿔\" 형태로, 한쪽 끝에는 이미지 임베딩이, 다른 쪽 끝에는 텍스트 임베딩이 있습니다. 이 원뿔 모양은 2차원 투영으로는 잘 표현되지 않지만 위 이미지에서 대략적으로 볼 수 있습니다. 모든 텍스트는 임베딩 공간의 한 부분에 클러스터를 이루고, 모든 이미지는 다른 부분에 클러스터를 이룹니다. 만약 훈련 후에도 텍스트가 매칭되는 이미지보다 다른 텍스트와 더 유사하다면, 이 초기 상태가 큰 이유일 것입니다. 이미지와 텍스트를, 텍스트와 텍스트를, 그리고 이미지와 이미지를 가장 잘 매칭시키는 목표는 이 원뿔 모양과 완전히 호환됩니다.</p><p>모델은 태생적으로 편향되어 있으며, 학습한 것이 이를 바꾸지는 않습니다. 아래의 그림 3은 이미지-텍스트 쌍을 사용한 전체 훈련 후 출시된 Jina CLIP 모델에 대한 동일한 분석입니다. 오히려 멀티모달 간격이 더 두드러져 보입니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 3: Jina CLIP에서 쌍 훈련 후 이미지와 텍스트 임베딩의 2차원 위치.</span></figcaption></figure><p>광범위한 훈련 후에도 Jina CLIP은 여전히 매체를 메시지의 일부로 인코딩합니다.</p><p>순수하게 무작위 초기화를 사용하는 더 비용이 많이 드는 OpenAI 접근 방식을 사용해도 이 편향은 사라지지 않습니다. 원래 OpenAI CLIP 아키텍처를 가져와서 모든 가중치를 완전히 무작위화한 다음 위와 같은 분석을 수행했습니다. 결과는 여전히 그림 4와 같이 절단된 원뿔 모양입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 4: 완전히 무작위화된 가중치와 훈련이 전혀 없는 Jina CLIP에서 이미지와 텍스트 임베딩의 2차원 초기 위치.</span></figcaption></figure><p>이 편향은 구조적인 문제이며, 해결책이 없을 수 있습니다. 만약 그렇다면, 우리는 훈련 중에 이를 수정하거나 완화하는 방법만을 찾을 수 있습니다.</p><h3 id=\"training-temperature\">훈련 온도</h3><p>AI 모델 훈련 중에는 일반적으로 약간의 무작위성을 추가합니다. 훈련 샘플 배치가 모델의 가중치를 얼마나 변경해야 하는지 계산한 다음, 실제로 가중치를 변경하기 전에 이러한 변경에 작은 무작위 요소를 추가합니다. 우리는 열역학에서 무작위성을 사용하는 방식과 유사하게, 무작위성의 양을 <em>온도</em>라고 부릅니다.</p><p>높은 온도는 모델을 매우 빠르게 크게 변화시키는 반면, 낮은 온도는 모델이 훈련 데이터를 볼 때마다 변할 수 있는 양을 줄입니다. 결과적으로 높은 온도에서는 개별 임베딩이 훈련 중에 임베딩 공간에서 많이 움직일 것으로 예상할 수 있고, 낮은 온도에서는 훨씬 더 천천히 움직일 것입니다.</p><p>AI 모델 훈련의 모범 사례는 높은 온도로 시작하여 점진적으로 낮추는 것입니다. 이는 가중치가 무작위이거나 목표로 하는 위치에서 멀리 떨어져 있을 때 초기에 모델이 학습에서 큰 도약을 할 수 있도록 돕고, 이후에는 더 안정적으로 세부 사항을 학습할 수 있게 합니다.</p><p>Jina CLIP 이미지-텍스트 쌍 훈련은 0.07의 온도(이는 비교적 높은 온도입니다)로 시작하여 아래 그림 5와 같이 훈련 단계에 걸쳐 지수적으로 0.01로 낮아집니다. 온도 대 훈련 단계 그래프입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 5: Jina CLIP에서 쌍 훈련 중 온도 감소.</span></figcaption></figure><p>우리는 온도를 높이는 것 - 무작위성을 추가하는 것 - 이 원뿔 효과를 줄이고 전반적으로 이미지 임베딩과 텍스트 임베딩을 더 가깝게 만들 수 있는지 알고 싶었습니다. 그래서 고정된 온도 0.1(매우 높은 값)로 Jina CLIP을 다시 훈련했습니다. 각 훈련 에포크 후, 그림 1과 같이 이미지-텍스트 쌍과 텍스트-텍스트 쌍 사이의 거리 분포를 확인했습니다. 결과는 아래 그림 6과 같습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 6: 학습 온도가 높을 때 시간이 지남에 따라 모달리티 간의 격차가 줄어듭니다.</span></figcaption></figure><p>보시다시피, 높은 온도를 유지하면 멀티모달 격차가 극적으로 줄어듭니다. 학습 중에 임베딩이 많이 움직일 수 있도록 하면 초기 임베딩 분포의 편향을 극복하는 데 큰 도움이 됩니다.</p><p>하지만 이는 대가가 따릅니다. 우리는 6가지 다른 검색 테스트를 사용하여 모델의 성능을 테스트했습니다: <a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a> 데이터셋에서 3개의 텍스트-텍스트 검색 테스트와 3개의 텍스트-이미지 검색 테스트를 진행했습니다. 모든 테스트에서 학습 초기에 성능이 급격히 떨어졌다가 매우 천천히 상승하는 것을 그림 7에서 확인할 수 있습니다:</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 7: 학습 중 성능. 처음에는 초기 상태에서 급격한 하락이 있고 그 후 매우 느린 상승이 있습니다.</span></figcaption></figure><p>이러한 지속적인 높은 온도를 사용하여 Jina CLIP과 같은 모델을 학습시키는 것은 매우 시간이 많이 걸리고 비용이 많이 들 것입니다. 이론적으로는 가능하지만, 이는 실용적인 해결책이 아닙니다.</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">대조 학습과 거짓 음성 문제</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>는 CLIP 스타일의 멀티모달 모델을 학습시키는 데 사용하는 메커니즘인 표준 대조 학습 방식이 멀티모달 격차를 강화하는 경향이 있다는 것도 발견했습니다.</p><p>대조 학습은 기본적으로 단순한 개념입니다. 이미지 임베딩과 텍스트 임베딩이 있고 이들이 더 가깝게 있어야 한다는 것을 알고 있으므로, 학습 중에 모델의 가중치를 조정하여 그렇게 만듭니다. 우리는 천천히 진행하며 가중치를 작은 양만큼 조정하고, 두 임베딩이 얼마나 멀리 떨어져 있는지에 비례하여 조정합니다: 더 가까이 있으면 더 멀리 있을 때보다 변화가 더 작습니다.</p><p>이 기술은 매칭되는 임베딩을 더 가깝게 만드는 것뿐만 아니라, 매칭되지 않을 때 더 멀어지게 만드는 것도 포함할 때 훨씬 더 잘 작동합니다. 우리는 함께 있어야 하는 이미지-텍스트 쌍뿐만 아니라, 분명히 떨어져 있어야 하는 쌍도 필요합니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이는 몇 가지 문제를 야기합니다:</p><ol><li>우리의 데이터 소스는 전적으로 매칭되는 쌍들로만 구성되어 있습니다. 사람이 관련이 없다고 확인한 텍스트와 이미지의 데이터베이스를 만들거나, 웹 스크래핑이나 다른 비지도 또는 준지도 기술을 통해 쉽게 구축할 수 없습니다.</li><li>겉으로 보기에 완전히 관련이 없어 보이는 이미지-텍스트 쌍조차도 반드시 그렇지는 않습니다. 우리는 이러한 부정적인 판단을 객관적으로 할 수 있는 의미론 이론이 없습니다. 예를 들어, 현관에 누워있는 고양이 사진은 \"소파에서 자고 있는 남자\"라는 텍스트와 완전히 부정적인 매칭이 아닙니다. 둘 다 무언가 위에 누워있다는 공통점이 있습니다.</li></ol><p>이상적으로는 확실히 관련이 있고 관련이 없는 것으로 알려진 이미지-텍스트 쌍으로 학습하고 싶지만, 관련이 없는 것으로 알려진 쌍을 얻을 수 있는 명확한 방법이 없습니다. 사람들에게 \"이 문장이 이 사진을 설명하나요?\"라고 물어보고 일관된 답변을 기대하는 것은 가능합니다. 하지만 \"이 문장이 이 사진과 전혀 관련이 없나요?\"라고 물어보고 일관된 답변을 얻는 것은 훨씬 더 어렵습니다.</p><p>대신, 우리는 학습 데이터에서 무작위로 사진과 텍스트를 선택하여 관련이 없는 이미지-텍스트 쌍을 얻고, 실제로는 거의 항상 잘못된 매칭이 될 것이라고 기대합니다. 이것이 실제로 작동하는 방식은 학습 데이터를 배치로 나누는 것입니다. Jina CLIP을 학습할 때는 32,000개의 매칭되는 이미지-텍스트 쌍을 포함하는 배치를 사용했지만, 이 실험에서는 배치 크기가 16개에 불과했습니다.</p><p>아래 표는 Flickr8k에서 무작위로 샘플링한 16개의 이미지-텍스트 쌍입니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>매칭되지 않는 쌍을 얻기 위해, 배치의 모든 사진을 매칭되는 것을 제외한 모든 텍스트와 결합합니다. 예를 들어, 다음 쌍은 매칭되지 않는 이미지와 텍스트입니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>캡션:</strong> 분홍색 옷을 입은 소녀가 꽃을 따고 있다.</p><p>하지만 이 절차는 다른 이미지와 매칭되는 모든 텍스트가 동일하게 나쁜 매칭이라고 가정합니다. 이것은 항상 사실이 아닙니다. 예를 들어:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>캡션:</strong> 개가 눈더미 옆에 앉아있다.</p><p>이 텍스트가 이 이미지를 정확히 설명하지는 않지만, 둘 다 개라는 공통점이 있습니다. 이 쌍을 매칭되지 않는 것으로 취급하면 \"개\"라는 단어를 모든 개의 이미지로부터 멀어지게 만드는 경향이 있습니다.</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>는 이러한 불완전한 비매칭 쌍들이 모든 이미지와 텍스트를 서로 멀어지게 만든다는 것을 보여줍니다.</p><p>우리는 완전히 무작위로 초기화된 <code>vit-b-32</code> 이미지 모델과 비슷하게 무작위화된 JinaBERT v2 텍스트 모델을 사용하여 학습 온도를 0.02(적당히 낮은 온도)로 고정하고 그들의 주장을 검증하고자 했습니다. 우리는 두 가지 학습 데이터 세트를 구성했습니다:</p><ul><li>하나는 Flickr8k에서 무작위로 추출한 배치로, 위에서 설명한 대로 비매칭 쌍을 구성했습니다.</li><li>다른 하나는 각 배치에 같은 이미지의 여러 복사본을 다른 텍스트와 함께 의도적으로 구성했습니다. 이는 상당수의 \"비매칭\" 쌍이 실제로는 서로 올바른 매칭이 되도록 보장합니다.</li></ul><p>그런 다음 각각의 학습 데이터셋으로 두 모델을 1 에포크 동안 학습시키고, 각 모델에 대해 Flickr8k 데이터셋의 1,000개 텍스트-이미지 쌍 간의 평균 코사인 거리를 측정했습니다. 무작위 배치로 학습한 모델의 평균 코사인 거리는 0.7521이었고, 의도적으로 매칭되는 \"비매칭\" 쌍이 많이 포함된 모델의 평균 코사인 거리는 0.7840이었습니다. 잘못된 \"비매칭\" 쌍의 영향은 매우 큽니다. 실제 모델 학습이 훨씬 더 길고 훨씬 더 많은 데이터를 사용한다는 점을 고려하면, 이 효과가 어떻게 증가하여 전체적으로 이미지와 텍스트 사이의 격차를 높이는지 알 수 있습니다.</p><h2 id=\"the-medium-is-the-message\">매체가 곧 메시지다</h2><p>캐나다의 커뮤니케이션 이론가 <a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Marshall McLuhan</a>은 1964년 저서 <a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\"><em>Understanding Media: The Extensions of Man</em></a>에서 \"매체가 곧 메시지다\"라는 문구를 만들어냈습니다. 이는 메시지가 독립적이지 않다는 점을 강조하기 위한 것이었습니다. 메시지는 그 의미에 강력한 영향을 미치는 맥락 속에서 우리에게 전달되며, 그는 그 맥락의 가장 중요한 부분 중 하나가 커뮤니케이션 매체의 본질이라고 주장했습니다.</p><p>멀티모달리티 갭은 AI 모델에서 나타나는 일련의 창발적 의미론적 현상을 연구할 수 있는 독특한 기회를 제공합니다. 아무도 Jina CLIP에게 학습 데이터의 매체를 인코딩하라고 말하지 않았지만, 그냥 그렇게 했습니다. 멀티모달 모델에 대한 문제를 완전히 해결하지는 못했지만, 적어도 문제의 원인에 대한 좋은 이론적 이해를 가지고 있습니다.</p><p>우리는 동일한 종류의 편향으로 인해 모델들이 아직 우리가 찾지 못한 다른 것들도 인코딩하고 있다고 가정해야 합니다. 예를 들어, 다국어 임베딩 모델에서도 같은 문제가 있을 것입니다. 특히 유사한 학습 방법이 널리 사용되고 있기 때문에, 두 개 이상의 언어에 대한 공동 학습은 아마도 언어 간에 동일한 갭을 야기할 것입니다. 이 갭 문제에 대한 해결책은 매우 광범위한 영향을 미칠 수 있습니다.</p><p>더 광범위한 모델들에서 초기화 편향에 대한 조사는 새로운 통찰력으로 이어질 것입니다. 임베딩 모델에게 매체가 메시지라면, 우리가 인식하지 못한 채 우리의 모델들에 무엇이 더 인코딩되고 있는지 누가 알 수 있을까요?</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "CLIP 모델을 단순히 텍스트와 이미지를 검색하고 점수로 결과를 정렬하는 데 사용할 수는 없습니다. 왜일까요? 양식 격차(modality gap) 때문입니다. 이것이 무엇이고 어디서 발생하는 걸까요?",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
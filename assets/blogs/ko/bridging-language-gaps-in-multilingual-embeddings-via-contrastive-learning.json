{
  "slug": "bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning",
  "id": "67066bd652567c0001d0f2cd",
  "uuid": "1130051f-f343-4eb2-9956-9b574c212704",
  "title": "대조 학습을 통한 다국어 임베딩의 언어 격차 해소",
  "html": "<p>다국어 모델에서 가장 큰 도전 과제 중 하나는 \"<strong>언어 간극</strong>\"입니다 — 서로 다른 언어로 된 동일한 의미의 구문이 제대로 정렬되거나 군집화되지 않는 현상을 말합니다. 이상적으로는 한 언어의 텍스트와 다른 언어로 된 그 등가물이 유사한 표현 — 즉, 서로 매우 가까운 임베딩 — 을 가져야 하며, 이를 통해 교차 언어 애플리케이션이 서로 다른 언어의 텍스트를 동일하게 처리할 수 있어야 합니다. 하지만 모델들은 종종 텍스트의 언어를 미묘하게 표현하여 \"언어 간극\"을 만들고, 이는 교차 언어 성능을 저하시킵니다.</p><p>이 글에서는 이러한 언어 간극과 그것이 텍스트 임베딩 모델의 성능에 미치는 영향에 대해 살펴보겠습니다. 우리는 <code>jina-xlm-roberta</code> 모델과 최신 <code>jina-embeddings-v3</code>를 사용하여 동일 언어 내의 패러프레이즈와 서로 다른 언어 쌍 간의 번역에 대한 의미적 정렬을 평가하는 실험을 수행했습니다. 이러한 실험들은 서로 다른 학습 조건에서 유사하거나 동일한 의미를 가진 구문들이 얼마나 잘 군집화되는지 보여줍니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-6.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>또한 우리는 대조 학습 중 <strong>병렬 다국어 데이터</strong>의 도입과 같이 교차 언어 의미적 정렬을 개선하기 위한 학습 기법을 실험했습니다. 이 글에서 우리의 통찰과 결과를 공유하겠습니다.</p><h2 id=\"multilingual-model-training-creates-and-reduces-the-language-gap\"><strong>다국어 모델 학습이 언어 간극을 만들고 줄이다</strong></h2><p>텍스트 임베딩 모델의 학습은 일반적으로 두 가지 주요 부분으로 구성된 다단계 프로세스를 포함합니다:</p><ol><li><a href=\"https://aclanthology.org/2023.acl-long.49/?ref=jina-ai-gmbh.ghost.io\"><strong>마스크 언어 모델링</strong></a> (MLM): 사전 학습은 일반적으로 일부 토큰이 무작위로 마스킹된 매우 큰 양의 텍스트를 포함합니다. 모델은 이러한 마스킹된 토큰을 예측하도록 학습됩니다. 이 과정은 모델에게 구문, 어휘 의미론, 실제 세계의 제약에서 발생할 수 있는 토큰 간의 선택 의존성을 포함하여 학습 데이터의 언어 또는 언어들의 패턴을 가르칩니다.</li><li><a href=\"https://paperswithcode.com/task/contrastive-learning?ref=jina-ai-gmbh.ghost.io\"><strong>대조 학습</strong></a>: 사전 학습 후, 모델은 의미적으로 유사한 텍스트의 임베딩을 더 가깝게 만들고 (선택적으로) 유사하지 않은 것들을 더 멀리 밀어내기 위해 큐레이트되거나 반-큐레이트된 데이터로 추가 학습됩니다. 이 학습은 의미적 유사성이 이미 알려져 있거나 적어도 신뢰할 수 있게 추정된 텍스트의 쌍, 삼중항, 또는 그룹을 사용할 수 있습니다. 여러 하위 단계가 있을 수 있으며 이 과정에 대한 다양한 학습 전략이 있는데, 새로운 연구가 자주 발표되고 있지만 최적의 접근 방식에 대한 명확한 합의는 없습니다.</li></ol><p>언어 간극이 어떻게 발생하고 어떻게 해소될 수 있는지 이해하기 위해서는 두 단계의 역할을 모두 살펴볼 필요가 있습니다.</p><h3 id=\"masked-language-pretraining\"><strong>마스크 언어 사전 학습</strong></h3><p>텍스트 임베딩 모델의 교차 언어 능력 중 일부는 사전 학습 중에 획득됩니다.</p><p>동족어와 차용어를 통해 모델은 대량의 텍스트 데이터에서 일부 교차 언어 의미 정렬을 학습할 수 있습니다. 예를 들어, 영어 단어 <em>banana</em>와 프랑스어 단어 <em>banane</em> (그리고 독일어 <em>Banane</em>)는 철자가 충분히 비슷하고 자주 사용되어 임베딩 모델이 \"banan-\"과 비슷한 단어들이 언어 간에 유사한 분포 패턴을 가진다는 것을 학습할 수 있습니다. 이를 통해 다른 언어에서 같아 보이지 않는 단어들도 유사한 의미를 가질 수 있다는 것을 어느 정도 배울 수 있고, 심지어 문법 구조가 어떻게 번역되는지도 파악할 수 있습니다.</p><p>하지만 이는 명시적인 학습 없이 일어납니다.</p><p>우리는 <code>jina-embeddings-v3</code>의 사전 학습된 백본인 <code>jina-xlm-roberta</code> 모델이 마스크 언어 사전 학습에서 교차 언어 등가성을 얼마나 잘 학습했는지 테스트했습니다. 영어 문장들과 그것들의 독일어, 네덜란드어, 중국어 간체, 일본어 번역의 2차원 <a href=\"https://pair-code.github.io/understanding-umap/?ref=jina-ai-gmbh.ghost.io\">UMAP 문장 표현</a>을 플롯했습니다. 결과는 아래 그림과 같습니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png\" class=\"kg-image\" alt=\"Multilingual scatterplot showing word embeddings' alignment across five languages on UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_roberta_mlm_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">영어 문장과 그에 대한 독일어, 네덜란드어, 중국어, 일본어 번역의 2차원 UMAP 투영입니다. 회색 선은 비영어 문장을 그들이 번역된 영어 문장과 연결합니다.<br><br>이러한 문장들은 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-xlm-roberta</code> 임베딩 공간에서 강하게 언어별 클러스터를 형성하는 경향이 있지만, 2차원 투영의 부작용일 수 있는 몇몇 이상치들을 볼 수 있습니다.</div></div><p>사전 학습이 동일 언어의 문장 임베딩을 매우 강하게 클러스터링했음을 볼 수 있습니다. 이는 훨씬 더 높은 차원의 공간에서의 분포를 2차원으로 투영한 것이므로, 예를 들어 영어 문장의 좋은 번역인 독일어 문장이 여전히 해당 영어 소스에 가장 가까운 독일어 문장일 수 있습니다. 하지만 이는 영어 문장의 임베딩이 의미적으로 동일하거나 거의 동일한 독일어 문장보다 다른 영어 문장에 더 가까울 가능성이 높다는 것을 보여줍니다.</p><p>또한 독일어와 네덜란드어가 다른 언어 쌍들보다 훨씬 더 가까운 클러스터를 형성하는 것을 주목하세요. 이는 비교적 밀접하게 관련된 두 언어에서 놀라운 일이 아닙니다. 독일어와 네덜란드어는 때때로 부분적으로 상호 이해가 가능할 정도로 유사합니다.</p><p>일본어와 중국어도 다른 언어들보다 서로 더 가깝게 나타납니다. 같은 방식으로 관련되어 있지는 않지만, 일본어 쓰기에는 일반적으로 <em>kanji</em> (漢字), 중국어로는 <em>hànzì</em>를 사용합니다. 일본어는 이러한 문자의 대부분을 중국어와 공유하며, 두 언어는 하나 또는 여러 개의 한자를 함께 사용하여 쓰는 많은 단어를 공유합니다. MLM의 관점에서 이는 네덜란드어와 독일어 사이의 시각적 유사성과 같은 종류입니다.</p><p>각각 두 문장씩 있는 두 언어만 살펴보면 이 \"언어 간극\"을 더 간단하게 볼 수 있습니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png\" class=\"kg-image\" alt=\"Graph illustrating linguistic relationships with color-coded lines, data points for English and German phrases, and an &quot;MLM P\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image--8-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image--8-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>MLM이 자연스럽게 텍스트를 언어별로 클러스터링하는 것처럼 보이므로, \"my dog is blue\"와 \"my cat is red\"는 독일어 상응어들과 멀리 떨어져 함께 클러스터링됩니다. 이전 블로그 포스트에서 논의된 \"<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/?ref=jina-ai-gmbh.ghost.io\">양식 간극</a>\"과 달리, 우리는 이것이 언어 간의 표면적 유사성과 차이점에서 발생한다고 생각합니다: 유사한 철자, 동일한 문자 시퀀스의 사용, 그리고 가능하게는 형태론과 구문 구조의 유사성 — 공통된 단어 순서와 단어를 구성하는 공통된 방식.</p><p>요약하면, 모델이 MLM 사전 학습에서 교차 언어 등가성을 어느 정도 학습하더라도, 이는 언어별로 텍스트를 클러스터링하려는 강한 편향을 극복하기에 충분하지 않습니다. 이는 큰 언어 간극을 남깁니다.</p><h3 id=\"contrastive-learning\"><strong>대조 학습</strong></h3><p>이상적으로, 우리는 임베딩 모델이 언어에 무관심하고 임베딩에서 일반적인 의미만 인코딩하기를 원합니다. 그러한 모델에서는 언어별 클러스터링이 보이지 않고 언어 간극이 없을 것입니다. 한 언어의 문장은 아래 그림과 같이 좋은 번역과 매우 가깝고 같은 언어라 하더라도 다른 의미를 가진 다른 문장과는 멀리 있어야 합니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png\" class=\"kg-image\" alt=\"Graph displays &quot;Clustering by Meaning&quot; with multilingual labels, emphasizing abstract concepts on a dark backdrop.\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-10---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-10---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-10---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>MLM 사전 학습은 그것을 달성하지 못하므로, 우리는 임베딩에서 텍스트의 의미적 표현을 개선하기 위해 추가적인 <em>대조 학습</em> 기법을 사용합니다.</p><p>대조 학습은 의미가 유사하거나 다른 것으로 알려진 텍스트 쌍과, 한 쌍이 다른 쌍보다 더 유사한 것으로 알려진 삼중항을 사용합니다. 학습 중에 가중치는 텍스트 쌍과 삼중항 간의 이러한 알려진 관계를 반영하도록 조정됩니다.</p><p>우리의 대조 학습 데이터셋에는 30개 언어가 포함되어 있지만, 쌍과 삼중항의 97%는 단일 언어이며, 단 3%만이 교차 언어 쌍이나 삼중항을 포함합니다. 하지만 이 3%만으로도 극적인 결과를 만들어냅니다: <code>jina-embeddings-v3</code>의 임베딩 UMAP 투영에서 보여지듯이, 임베딩은 거의 언어 클러스터링을 보이지 않으며 의미적으로 유사한 텍스트는 언어에 관계없이 가까운 임베딩을 생성합니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png\" class=\"kg-image\" alt=\"Scatter plot on black background showing language distribution post-contrastive training with UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_contrastive_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이를 확인하기 위해 STS17 데이터셋에서 <code>jina-xlm-roberta</code>와 <code>jina-embeddings-v3</code>가 생성한 표현의 스피어만 상관관계를 측정했습니다.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Spearman 상관관계</strong></b></a>는 순위 상관관계를 측정합니다. 즉, 두 개의 정렬된 목록이 얼마나 같은지를 측정합니다. 실제 점수보다는 어떤 항목이 다른 항목보다 상위 또는 하위에 순위가 매겨지는지가 더 중요하기 때문에, 이는 임베딩 모델을 서로 비교하고 인간 점수와 비교하는 좋은 메커니즘입니다.</div></div><p>아래 표는 번역된 텍스트의 의미적 유사성 순위 간의 스피어만 상관관계를 보여줍니다. 영어 문장 세트를 가져와서 특정 참조 문장의 임베딩과의 유사성을 측정하고 가장 유사한 것부터 가장 덜 유사한 순서로 정렬합니다. 그런 다음 이러한 모든 문장을 다른 언어로 번역하고 순위 매기기 과정을 반복합니다. 이상적인 교차 언어 임베딩 모델에서는 두 정렬된 목록이 동일하고 스피어만 상관관계가 1.0이 될 것입니다.</p><p>아래 차트와 표는 <code>jina-xlm-roberta</code>와 <code>jina-embeddings-v3</code>를 사용하여 영어와 STS17 벤치마크의 다른 6개 언어를 비교한 결과를 보여줍니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png\" class=\"kg-image\" alt=\"Bar chart comparing Spearman Correlation for English paired with AR, DE, ES, FR, IT, NL, colored in red and blue by alphabet \" loading=\"lazy\" width=\"2000\" height=\"1056\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-4---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-4---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-4---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png 2085w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong><code>jina-xlm-roberta</code></strong></th>\n<th><strong><code>jina-embeddings-v3</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English ↔ Arabic</td>\n<td>0.1581</td>\n<td><strong>0.7977</strong></td>\n</tr>\n<tr>\n<td>English ↔ German</td>\n<td>0.2136</td>\n<td><strong>0.8366</strong></td>\n</tr>\n<tr>\n<td>English ↔ Spanish</td>\n<td>0.1049</td>\n<td><strong>0.8509</strong></td>\n</tr>\n<tr>\n<td>English ↔ French</td>\n<td>0.1659</td>\n<td><strong>0.8378</strong></td>\n</tr>\n<tr>\n<td>English ↔ Italian</td>\n<td>0.2293</td>\n<td><strong>0.8674</strong></td>\n</tr>\n<tr>\n<td>English ↔ Dutch</td>\n<td>0.2387</td>\n<td><strong>0.8398</strong></td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><p>여기서 원래 사전 학습과 비교했을 때 대조 학습이 만드는 엄청난 차이를 볼 수 있습니다. 학습 데이터의 3%만이 교차 언어 데이터임에도 불구하고, <code>jina-embeddings-v3</code> 모델은 사전 학습에서 얻은 언어 격차를 거의 제거할 만큼 충분한 교차 언어 의미론을 학습했습니다.</p><h2 id=\"english-vs-the-world-can-other-languages-keep-up-in-alignment\">영어 vs 세계: 다른 언어들이 정렬에서 따라잡을 수 있을까?</h2><p>우리는 <code>jina-embeddings-v3</code>를 89개 언어로 학습시켰으며, 특히 30개의 매우 널리 사용되는 문자 언어에 중점을 두었습니다. 대규모 다국어 학습 코퍼스를 구축하기 위한 우리의 노력에도 불구하고, 영어는 여전히 우리가 대조 학습에 사용한 데이터의 거의 절반을 차지합니다. 충분한 텍스트 자료가 있는 널리 사용되는 글로벌 언어를 포함한 다른 언어들은 학습 세트의 방대한 영어 데이터에 비해 여전히 상대적으로 적게 대표됩니다.</p><p>이러한 영어의 우세함을 고려할 때, 영어 표현이 다른 언어들의 표현보다 더 잘 정렬되어 있을까요? 이를 탐구하기 위해 후속 실험을 진행했습니다.</p><p>우리는 \"앵커\"와 \"긍정\"이라는 1,000개의 영어 텍스트 쌍으로 구성된 <a href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><code>parallel-sentences</code></a> 데이터셋을 구축했습니다. 여기서 긍정 텍스트는 앵커 텍스트에 의해 논리적으로 함의됩니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/parallel-sentences · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-3.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/parallel-sentences.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>예를 들어, 아래 표의 첫 번째 행을 보세요. 이 문장들은 의미가 완전히 동일하지는 않지만, 호환되는 의미를 가지고 있습니다. 이들은 동일한 상황을 정보적으로 설명합니다.</p><p>그런 다음 이 쌍들을 GPT-4를 사용하여 독일어, 네덜란드어, 중국어(간체), 중국어(번체), 일본어의 5개 언어로 번역했습니다. 마지막으로 품질을 보장하기 위해 수동으로 검사했습니다.</p><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Anchor</strong></th>\n<th><strong>Positive</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>Two young girls are playing outside in a non-urban environment.</td>\n<td>Two girls are playing outside.</td>\n</tr>\n<tr>\n<td>German</td>\n<td>Zwei junge Mädchen spielen draußen in einer nicht urbanen Umgebung.</td>\n<td>Zwei Mädchen spielen draußen.</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>Twee jonge meisjes spelen buiten in een niet-stedelijke omgeving.</td>\n<td>Twee meisjes spelen buiten.</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>两个年轻女孩在非城市环境中玩耍。</td>\n<td>两个女孩在外面玩。</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>兩個年輕女孩在非城市環境中玩耍。</td>\n<td>兩個女孩在外面玩。</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>2人の若い女の子が都市環境ではない場所で遊んでいます。</td>\n<td>二人の少女が外で遊んでいます。</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><p>그런 다음 <code>jina-embeddings-v3</code>로 각 텍스트 쌍을 인코딩하고 그들 사이의 코사인 유사도를 계산했습니다. 아래 그림과 표는 각 언어의 코사인 유사도 점수 분포와 평균 유사도를 보여줍니다:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png\" class=\"kg-image\" alt=\"Graph showing cosine similarity distributions for textual pairs in English, German, Dutch, Chinese, and Japanese against dens\" loading=\"lazy\" width=\"1060\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/1_monolingual_distribution_triplets.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/1_monolingual_distribution_triplets.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png 1060w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>0.9078</td>\n</tr>\n<tr>\n<td>German</td>\n<td>0.8949</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>0.8844</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>0.8876</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>0.8933</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>0.8895</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><p>학습 데이터에서 영어가 우세함에도 불구하고, <code>jina-embeddings-v3</code>는 독일어, 네덜란드어, 일본어, 그리고 두 형태의 중국어에서 영어만큼 잘 의미적 유사성을 인식합니다.</p><h2 id=\"breaking-language-barriers-cross-lingual-alignment-beyond-english\"><strong>언어 장벽 허물기: 영어를 넘어선 교차 언어 정렬</strong></h2><p>일반적으로 교차 언어 표현 정렬 연구는 영어를 포함하는 언어 쌍을 연구합니다. 이러한 초점은 이론상 실제 상황을 가리킬 수 있습니다. 모델이 다른 언어 쌍이 제대로 지원되는지 검토하지 않고, 단순히 모든 것을 영어에 최대한 가깝게 표현하도록 최적화할 수 있습니다.</p><p>이를 탐구하기 위해 <code>parallel-sentences</code> 데이터셋을 사용하여 영어 이외의 이중 언어 쌍에 대한 교차 언어 정렬을 연구하는 실험을 수행했습니다.</p><p>아래 표는 서로 다른 언어 쌍 간의 동등한 텍스트 - 공통된 영어 원문의 번역본 - 사이의 코사인 유사도 분포를 보여줍니다. 이상적으로는 모든 쌍이 코사인 1을 가져야 합니다 - 즉, 동일한 의미 임베딩을 갖습니다. 실제로는 이런 일이 일어날 수 없지만, 좋은 모델이라면 번역 쌍에 대해 매우 높은 코사인 값을 가질 것으로 예상됩니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png\" class=\"kg-image\" alt=\"Density graph charting cross-lingual cosine similarities for language pairs using jina-embeddings-v3 model.\" loading=\"lazy\" width=\"978\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png 978w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language Pair</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>German ↔ Dutch</td>\n<td>0.8779</td>\n</tr>\n<tr>\n<td>German ↔ Japanese</td>\n<td>0.8664</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ↔ Japanese</td>\n<td>0.8534</td>\n</tr>\n<tr>\n<td>Dutch ↔ Chinese (Simplified)</td>\n<td>0.8479</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ↔ Chinese (Traditional)</td>\n<td>0.8758</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>서로 다른 언어 간의 유사도 점수가 동일 언어의 호환 가능한 텍스트보다 약간 낮지만, 여전히 매우 높습니다. 네덜란드어/독일어 번역의 코사인 유사도는 독일어의 호환 가능한 텍스트 간의 유사도에 거의 근접합니다.</p><p>독일어와 네덜란드어가 매우 유사한 언어이기 때문에 이는 놀랍지 않을 수 있습니다. 마찬가지로, 여기서 테스트된 두 가지 중국어 변형은 실제로 두 개의 다른 언어가 아니라 같은 언어의 스타일이 다른 형태일 뿐입니다. 하지만 네덜란드어와 중국어 또는 독일어와 일본어와 같이 매우 다른 언어 쌍에서도 의미적으로 동등한 텍스트 간에 매우 강한 유사성을 보이는 것을 알 수 있습니다.</p><p>이런 매우 높은 유사도 값이 ChatGPT를 번역기로 사용한 부작용일 수 있다고 생각했습니다. 이를 테스트하기 위해 <a href=\"https://help.ted.com/hc/en-us/articles/360018572954-How-do-I-find-transcripts-for-TED-and-TEDx-talks?ref=jina-ai-gmbh.ghost.io\">TED Talks의 영어와 독일어 인간 번역 대본</a>을 다운로드하여 정렬된 번역 문장들이 같은 높은 상관관계를 가지는지 확인했습니다.</p><p>아래 그림에서 볼 수 있듯이 결과는 기계 번역된 데이터보다 더 강력했습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png\" class=\"kg-image\" alt=\"Graph of cross-lingual alignment density EN-DE with peak around cosine similarity 1.0, titled &quot;jina-embeddings-v3: Cross-ling\" loading=\"lazy\" width=\"988\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png 988w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-much-does-cross-language-data-contribute-to-cross-language-alignment\">교차 언어 데이터가 교차 언어 정렬에 얼마나 기여하나요?</h2><p>사라지는 언어 격차와 높은 수준의 교차 언어 성능은 명시적으로 교차 언어인 훈련 데이터의 매우 작은 부분에 비해 불균형해 보입니다. 대조 훈련 데이터의 단 3%만이 모델에게 언어 간 정렬 방법을 구체적으로 가르칩니다.</p><p>그래서 우리는 교차 언어가 실제로 어떤 기여를 하고 있는지 테스트해보았습니다.</p><p>작은 실험을 위해 교차 언어 데이터 없이 <code>jina-embeddings-v3</code>를 완전히 재훈련하는 것은 비용이 너무 많이 들기 때문에, Hugging Face에서 <a href=\"https://huggingface.co/FacebookAI/xlm-roberta-base?ref=jina-ai-gmbh.ghost.io\"><code>xlm-roberta-base</code> 모델</a>을 다운로드하여 <code>jina-embeddings-v3</code> 훈련에 사용한 데이터의 일부를 사용하여 대조 학습을 추가로 진행했습니다. 우리는 구체적으로 교차 언어 데이터의 양을 조정하여 두 가지 경우를 테스트했습니다: 하나는 교차 언어 데이터가 없는 경우이고, 다른 하나는 쌍의 20%가 교차 언어인 경우입니다. 아래 표에서 훈련 메타 파라미터를 확인할 수 있습니다:</p>\n<!--kg-card-begin: html-->\n<table id=\"e30425bb-015e-4956-8872-b1b64cdd7ad0\" class=\"simple-table\"><tbody><tr id=\"daa3cfcc-9012-411b-8da3-05c7c6f4b371\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><strong>Backbone</strong></td><td id=\"DU<d\" class=\"\"><strong>% Cross-Language</strong></td><td id=\"@Feo\" class=\"\"><strong>Learning Rate</strong></td><td id=\"fZNx\" class=\"\"><strong>Loss Function</strong></td><td id=\"Rv}\\\" class=\"\"><strong>Temperature</strong></td></tr><tr id=\"f3a2d068-d902-4fc1-8c89-269a5ebbb135\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base </code><strong>without</strong> X-language data</td><td id=\"DU<d\" class=\"\">0%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr><tr id=\"52887e22-326c-46cd-b79c-d6dcd110c1d2\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base</code><strong> with </strong>X-language data</td><td id=\"DU<d\" class=\"\">20%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>그런 다음 <a href=\"https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io\">MTEB의 STS17과 STS22 벤치마크</a>와 스피어만 상관계수를 사용하여 두 모델의 교차 언어 성능을 평가했습니다. 결과는 다음과 같습니다:</p><h3 id=\"sts17\"><strong>STS17</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png\" class=\"kg-image\" alt=\"Bar graph showing Spearman correlation for language pairs on STS17 with and without parallel corpus.\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-6---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-6---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-6---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"de30bf7f-d1a9-43f4-8e5f-15bf65c59674\" class=\"simple-table\"><tbody><tr id=\"5c6440a6-eba9-404a-a5f1-88099bc6702d\"><td id=\"{N[x\" class=\"\"><strong>Language Pair</strong></td><td id=\"jGmJ\" class=\"\"><strong>With parallel corpora</strong></td><td id=\"p<ZH\" class=\"\"><strong>Without parallel corpora</strong></td></tr><tr id=\"33f08461-58aa-43f3-9ed1-577f8676e99d\"><td id=\"{N[x\" class=\"\">English ↔ Arabic</td><td id=\"jGmJ\" class=\"\"><strong>0.6418</strong></td><td id=\"p<ZH\" class=\"\">0.5875</td></tr><tr id=\"9875386d-2043-4d9d-8252-e53ec525ec29\"><td id=\"{N[x\" class=\"\">English ↔ German</td><td id=\"jGmJ\" class=\"\">0.7364</td><td id=\"p<ZH\" class=\"\"><strong>0.7390</strong></td></tr><tr id=\"15d28a12-3a80-4176-9984-69b5d8a7d8ff\"><td id=\"{N[x\" class=\"\">English ↔ Spanish</td><td id=\"jGmJ\" class=\"\"><strong>0.6968</strong></td><td id=\"p<ZH\" class=\"\">0.6799</td></tr><tr id=\"21821558-c8b9-4c34-8ec5-9db3ca7d9328\"><td id=\"{N[x\" class=\"\">English ↔ French</td><td id=\"jGmJ\" class=\"\"><strong>0.7066</strong></td><td id=\"p<ZH\" class=\"\">0.6944</td></tr><tr id=\"a2e3b5e5-8e4a-4270-abff-5d059ff6be72\"><td id=\"{N[x\" class=\"\">English ↔ Italian</td><td id=\"jGmJ\" class=\"\"><strong>0.7232</strong></td><td id=\"p<ZH\" class=\"\">0.7070</td></tr><tr id=\"95daf20f-2a82-4431-8581-a4ce24d81462\"><td id=\"{N[x\" class=\"\">English ↔ Dutch</td><td id=\"jGmJ\" class=\"\"><strong>0.7597</strong></td><td id=\"p<ZH\" class=\"\">0.7468</td></tr><tr id=\"4fd4c45a-a69e-4e33-b057-9c5f10b99fdb\"><td id=\"{N[x\" class=\"\">English ↔ Turkish</td><td id=\"jGmJ\" class=\"\"><strong>0.6933</strong></td><td id=\"p<ZH\" class=\"\">0.6050</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"sts22\"><strong>STS22</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png\" class=\"kg-image\" alt=\"Chart comparing models of language alignment, showing Spearman correlation scores for eight language pairs with and without p\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-7---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-7---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-7---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"e53757df-8a0e-42ec-ba05-715baa3c77cd\" class=\"simple-table\"><tbody><tr id=\"45c43b8c-b1b7-4ac0-91b0-1e07025f1b92\"><td id=\"OF=p\" class=\"\"><strong>언어 쌍</strong></td><td id=\"P<\\i\" class=\"\"><strong>병렬 코퍼스 사용</strong></td><td id=\"LAtp\" class=\"\"><strong>병렬 코퍼스 미사용</strong></td></tr><tr id=\"696ebe11-3eda-49ef-8dfe-608f9b71430b\"><td id=\"OF=p\" class=\"\">English ↔ Spanish</td><td id=\"P<\\i\" class=\"\"><strong>0.7710</strong></td><td id=\"LAtp\" class=\"\">0.7675</td></tr><tr id=\"eb4c1d81-7d98-453d-af3f-95b2adccfb55\"><td id=\"OF=p\" class=\"\">Simplified Chinese ↔ English</td><td id=\"P<\\i\" class=\"\"><strong>0.6885</strong></td><td id=\"LAtp\" class=\"\">0.6860</td></tr><tr id=\"533cefd3-b30e-4d6a-9350-8f5d28b17ba6\"><td id=\"OF=p\" class=\"\">Spanish ↔ Italian</td><td id=\"P<\\i\" class=\"\"><strong>0.6829</strong></td><td id=\"LAtp\" class=\"\">0.6814</td></tr><tr id=\"d3ecdd71-44bb-4a3b-9ac2-8cc90a785d5f\"><td id=\"OF=p\" class=\"\">German ↔ French</td><td id=\"P<\\i\" class=\"\"><strong>0.5763</strong></td><td id=\"LAtp\" class=\"\">0.5496</td></tr><tr id=\"c6242853-4da7-4369-b1f1-1a27262a487a\"><td id=\"OF=p\" class=\"\">German ↔ English</td><td id=\"P<\\i\" class=\"\">0.5439</td><td id=\"LAtp\" class=\"\"><strong>0.5566</strong></td></tr><tr id=\"31a4a5ba-199c-4904-b926-ff0561aac1b5\"><td id=\"OF=p\" class=\"\">Polish ↔ English</td><td id=\"P<\\i\" class=\"\">0.6966</td><td id=\"LAtp\" class=\"\"><strong>0.7156</strong></td></tr><tr id=\"4f529d81-e8c9-4e5d-a705-36e357abebc3\"><td id=\"OF=p\" class=\"\">German ↔ English</td><td id=\"P<\\i\" class=\"\"><strong>0.5832</strong></td><td id=\"LAtp\" class=\"\">0.5478</td></tr><tr id=\"cd1429f7-c810-4a0e-9dca-88e2c83157bc\"><td id=\"OF=p\" class=\"\">French ↔ Polish</td><td id=\"P<\\i\" class=\"\">0.8451</td><td id=\"LAtp\" class=\"\">0.8451</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>테스트한 대부분의 언어 쌍에서 교차 언어 학습 데이터가 거의 또는 전혀 개선을 가져오지 않는다는 것을 보고 놀랐습니다. 더 큰 데이터셋으로 완전히 학습된 모델에서도 이러한 결과가 유지될지는 확실히 알 수 없지만, 명시적인 교차 언어 학습이 큰 도움이 되지 않는다는 증거를 제공합니다.</p><p>하지만 STS17에는 영어/아랍어와 영어/터키어 쌍이 포함되어 있다는 점에 주목해야 합니다. 이 두 언어는 우리의 학습 데이터에서 상대적으로 적게 대표되는 언어입니다. 우리가 사용한 XML-RoBERTa 모델은 아랍어가 2.25%, 터키어가 2.32%로, 우리가 테스트한 다른 언어들에 비해 훨씬 적은 비중으로 사전 학습되었습니다. 이 실험에서 사용한 작은 대조 학습 데이터셋에서는 아랍어가 1.7%, 터키어가 1.8%에 불과했습니다.</p><p>이 두 언어 쌍만이 교차 언어 데이터로 학습했을 때 명확한 차이를 보인 유일한 경우입니다. 우리는 명시적인 교차 언어 데이터가 학습 데이터에서 덜 대표되는 언어들에 대해 더 효과적이라고 생각하지만, 결론을 내리기 전에 이 영역을 더 탐구할 필요가 있습니다. 대조 학습에서 교차 언어 데이터의 역할과 효과는 Jina AI가 활발히 연구하고 있는 분야입니다.</p><h2 id=\"conclusion\">결론</h2><p>마스크드 언어 모델링과 같은 기존의 언어 사전 학습 방법은 \"언어 간격\"을 남깁니다. 이는 서로 다른 언어의 의미적으로 유사한 텍스트들이 원래 있어야 할 만큼 가깝게 정렬되지 않는다는 것을 의미합니다. Jina Embeddings의 대조 학습 방식이 이러한 간격을 줄이거나 심지어 제거하는 데 매우 효과적이라는 것을 보여드렸습니다.</p><p>이것이 작동하는 이유는 완전히 명확하지 않습니다. 우리는 대조 학습에서 명시적인 교차 언어 텍스트 쌍을 사용하지만, 매우 적은 양으로만 사용하며, 이것들이 실제로 양질의 교차 언어 결과를 보장하는 데 얼마나 큰 역할을 하는지는 불분명합니다. 더 통제된 조건에서 명확한 효과를 보여주려는 우리의 시도는 명확한 결과를 도출하지 못했습니다.</p><p>하지만 <strong><code>jina-embeddings-v3</code>가 사전 학습 언어 간격을 극복했다는 것은 분명하며, 이는 다국어 애플리케이션을 위한 강력한 도구가 되었습니다.</strong> 여러 언어에 걸쳐 동일한 강력한 성능이 필요한 모든 작업에 바로 사용할 수 있습니다.</p><p><code>jina-embeddings-v3</code>는 우리의 <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Embeddings API</a>(백만 토큰 무료)를 통해 또는 AWS나 Azure를 통해 사용할 수 있습니다. 이러한 플랫폼 외부에서 또는 회사 내부에서 사용하고 싶으시다면, CC BY-NC 4.0 라이선스 하에 있다는 점을 기억해 주세요. 상업적 사용에 관심이 있으시다면 <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io\">저희에게 연락</a>해 주시기 바랍니다.</p>",
  "comment_id": "67066bd652567c0001d0f2cd",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/gap-blog-1.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-09T13:41:10.000+02:00",
  "updated_at": "2024-10-10T20:15:26.000+02:00",
  "published_at": "2024-10-09T14:42:22.000+02:00",
  "custom_excerpt": "Multilingual models often face a \"language gap,\" where similar phrases in different languages don't align. We show how contrastive learning can bridge this gap, enhancing cross-language performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/",
  "excerpt": "다국어 모델은 종종 서로 다른 언어의 유사한 문구들이 일치하지 않는 \"언어 간극\" 문제에 직면합니다. 대조 학습(contrastive learning)이 이러한 간극을 어떻게 해소하여 언어 간 성능을 향상시킬 수 있는지 보여드리겠습니다.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon green squares form intricate patterns on a black digital background, creating a dynamic, abstract design.",
  "feature_image_caption": null
}
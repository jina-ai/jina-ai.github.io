{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "텍스트-이미지 전역적 대조 정렬과 토큰-패치 지역적 정렬",
  "html": "<p><a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali 스타일</a> 모델을 실험하는 동안, 우리 엔지니어 중 한 명이 최근 출시된 <code>jina-clip-v2</code> 모델을 사용하여 시각화를 만들었습니다. 그는 주어진 이미지-텍스트 쌍에 대해 토큰 임베딩과 패치 임베딩 간의 유사도를 매핑하여 흥미로운 시각적 통찰을 제공하는 히트맵 오버레이를 만들었습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>안타깝게도, <strong>이것은 단순한 휴리스틱 시각화</strong>일 뿐입니다 - 명시적이거나 보장된 메커니즘이 아닙니다. CLIP과 같은 전역 대조 정렬이 패치와 토큰 사이에 대략적인 지역 정렬을 <em>우연히</em> 만들 수는 있지만, 이는 모델의 의도된 목표가 아닌 <strong>의도하지 않은 부작용</strong>입니다. 그 이유를 설명하겠습니다.</p><h2 id=\"understand-the-code\">코드 이해하기</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>코드가 하는 일을 개략적으로 살펴보겠습니다. <code>jina-clip-v2</code>는 기본적으로 토큰 레벨이나 패치 레벨 임베딩에 접근하는 API를 제공하지 않는다는 점에 주의하세요 - 이 시각화는 사후 패치 작업이 필요했습니다.</p><p><strong>단어 레벨 임베딩 계산</strong></p><p><code>model.text_model.output_tokens = True</code>를 설정하면, <code>text_model(x=...,)[1]</code>를 호출했을 때 토큰 임베딩에 대한 <code>(batch_size, seq_len, embed_dim)</code> 두 번째 요소를 반환합니다. 입력 문장을 받아 Jina CLIP 토크나이저로 토큰화한 다음, 해당하는 토큰 임베딩을 평균내어 서브워드 토큰을 다시 \"단어\"로 그룹화합니다. 토큰 문자열이 <code>_</code> 문자로 시작하는지 확인하여(SentencePiece 기반 토크나이저의 전형적인 특징) 새로운 단어의 시작을 감지합니다. 단어 레벨 임베딩 리스트와 단어 리스트를 생성합니다(\"Dog\"가 하나의 임베딩, \"and\"가 하나의 임베딩 등).</p><p><strong>패치 레벨 임베딩 계산</strong></p><p>이미지 타워의 경우, <code>vision_model(..., return_all_features=True)</code>는 <code>(batch_size, n_patches+1, embed_dim)</code>을 반환하며, 여기서 첫 번째 토큰은 <code>[CLS]</code> 토큰입니다. 여기서 코드는 각 패치에 대한 임베딩(즉, 비전 트랜스포머의 패치 토큰)을 추출합니다. 그런 다음 이러한 패치 임베딩을 2D 그리드인 <code>patch_side × patch_side</code>로 재구성하고, 이를 원본 이미지 해상도에 맞게 업샘플링합니다.</p><p><strong>단어-패치 유사도 시각화</strong></p><p>유사도 계산과 후속 히트맵 생성은 표준적인 \"사후\" 해석 기법입니다: 텍스트 임베딩을 선택하고, 모든 패치 임베딩과의 코사인 유사도를 계산한 다음, 특정 토큰 임베딩과 가장 유사도가 높은 패치를 보여주는 히트맵을 생성합니다. 마지막으로 문장의 각 토큰을 순환하면서 왼쪽에서 해당 토큰을 굵게 강조하고, 오른쪽의 원본 이미지 위에 유사도 기반 히트맵을 오버레이합니다. 모든 프레임이 애니메이션 GIF로 컴파일됩니다.</p><h2 id=\"is-it-meaningful-explainability\">의미 있는 설명 가능성일까요?</h2><p><em>순수한 코드</em> 관점에서는, 네, 로직이 일관적이며 각 토큰에 대한 히트맵을 생성할 것입니다. 패치 유사도를 강조하는 일련의 프레임을 얻게 되므로, 스크립트는 \"의도한 대로 작동\"합니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>위의 예시를 보면, <code>moon</code>과 <code>branches</code>와 같은 단어들이 원본 이미지에서 해당하는 시각적 패치와 잘 정렬되는 것처럼 보입니다. 하지만 핵심 질문은 이것입니다: 이것이 의미 있는 정렬일까요, 아니면 우연한 일치일까요?</p><p>이는 더 깊은 질문입니다. 주의사항을 이해하기 위해 <strong>CLIP이 어떻게 훈련되는지</strong> 상기해보겠습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"영어와 다국어 텍스트 처리를 위한 입력에서 출력까지의 단계를 보여주는 JINA-CLIP-V2 모델 다이어그램.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2는 텍스트 인코더(Jina XLM-RoBERTa, 561M 파라미터)와 비전 인코더(EVA02-L14, 304M 파라미터)를 결합합니다. 오른쪽의 각 색상 사각형은 배치의 전체 문장이나 이미지를 나타냅니다 - 개별 토큰이나 패치가 아닙니다.</span></figcaption></figure><ul><li>CLIP은 전체 이미지와 전체 텍스트 사이에 <strong>전역</strong> 대조 정렬을 사용합니다. 훈련 중에 이미지 인코더는 단일 벡터(풀링된 표현)를 생성하고, 텍스트 인코더는 또 다른 단일 벡터를 생성합니다. CLIP은 매칭되는 텍스트-이미지 쌍에 대해서는 이들이 일치하고, 그렇지 않은 경우에는 불일치하도록 훈련됩니다.</li><li>'패치 X가 토큰 Y에 해당한다'는 수준의 <strong>명시적인 지도가 없습니다.</strong> 모델은 \"이미지의 이 영역이 개이고, 저 영역이 고양이다\" 등을 직접 강조하도록 훈련되지 않습니다. 대신, 전체 이미지 표현이 전체 텍스트 표현과 일치하도록 훈련됩니다.</li><li>CLIP의 아키텍처는 이미지 측에서는 Vision Transformer를, 텍스트 측에서는 텍스트 트랜스포머를 사용하여 별도의 인코더를 형성하기 때문에, 패치를 토큰에 자연스럽게 정렬하는 교차 어텐션 모듈이 없습니다. 대신, 각 타워에서 순수하게 <strong>셀프 어텐션</strong>만 있고, 전역 이미지나 텍스트 임베딩을 위한 최종 프로젝션이 있을 뿐입니다.</li></ul><p>요약하면, 이는 휴리스틱 시각화입니다. 특정 패치 임베딩이 특정 토큰 임베딩과 가깝거나 멀다는 것은 어느 정도 자연스럽게 나타나는 현상입니다. 이는 모델의 강력하거나 공식적인 \"어텐션\"이라기보다는 <em>사후 해석 트릭</em>에 가깝습니다.</p><h2 id=\"why-might-local-alignment-emerge\">왜 지역 정렬이 나타날까요?</h2><p>그렇다면 왜 때때로 단어-패치 레벨의 지역 정렬이 관찰될까요? 여기에 핵심이 있습니다: CLIP이 <em>전역</em> 이미지-텍스트 대조 목표로 훈련되더라도, 여전히 셀프 어텐션(ViT 기반 이미지 인코더에서)과 트랜스포머 층(텍스트용)을 사용합니다. 이러한 셀프 어텐션 층 내에서, 이미지 표현의 다른 부분들이 서로 상호작용할 수 있으며, 이는 텍스트 표현에서도 단어들이 그러한 것과 같습니다. 대규모 이미지-텍스트 데이터셋으로 훈련하면서, 모델은 자연스럽게 전체 이미지를 해당하는 텍스트 설명과 매칭하는 데 도움이 되는 내부 잠재 구조를 발달시킵니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이러한 잠재 표현에서 <strong>지역 정렬</strong>이 나타날 수 있는 이유는 최소한 두 가지입니다:</p><ol><li><strong>공동 발생 패턴</strong>: 모델이 \"개\" 옆에 있는 많은 \"고양이\" 이미지를 보고 (흔히 이러한 단어로 레이블이나 설명이 붙은), 이러한 개념에 대략적으로 해당하는 잠재 특징을 학습할 수 있습니다. 따라서 \"개\"에 대한 임베딩이 개와 비슷한 형태나 텍스처를 묘사하는 지역 패치와 가까워질 수 있습니다. 이는 패치 레벨에서 <em>명시적으로</em> 지도되지 않았지만, 개 이미지/텍스트 쌍 사이의 반복된 연관성에서 나타납니다.</li><li><strong>Self-attention</strong>: Vision Transformers에서 패치들은 서로 상호 작용합니다. 모델이 전체 장면의 정확한 단일 표현을 생성하려고 하기 때문에, 특징적인 패치들(개의 얼굴과 같은)은 일관된 잠재 \"서명\"을 갖게 될 수 있습니다. 이것이 전체 대비 손실을 최소화하는 데 도움이 된다면 강화될 것입니다.</li></ol><h2 id=\"theoretical-analysis\">이론적 분석</h2><p>CLIP의 대조 학습 목표는 일치하는 이미지-텍스트 쌍 간의 코사인 유사도를 최대화하고 일치하지 않는 쌍에 대해서는 최소화하는 것입니다. 텍스트와 이미지 인코더가 각각 토큰과 패치 임베딩을 생성한다고 가정합니다:</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>전역 유사도는 지역 유사도들의 집계로 표현될 수 있습니다:</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>특정 토큰-패치 쌍이 훈련 데이터에서 자주 동시 발생할 때, 모델은 누적 그래디언트 업데이트를 통해 이들의 유사성을 강화합니다:</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, 여기서 $C$는 동시 발생 횟수입니다. 이는 $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$가 크게 증가하게 되어 이러한 쌍들의 더 강한 지역적 정렬을 촉진합니다. 하지만, 대조 손실은 모든 토큰-패치 쌍에 걸쳐 그래디언트 업데이트를 분산시켜, 특정 쌍에 대한 업데이트의 강도를 제한합니다:</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>이는 개별 토큰-패치 유사도의 상당한 강화를 방지합니다.</p><h2 id=\"conclusion\">결론</h2><p>CLIP의 토큰-패치 시각화는 텍스트와 이미지 표현 사이의 우연한, 창발적 정렬을 활용합니다. 이 정렬은 흥미롭지만 CLIP의 <strong>전역 대조 학습</strong>에서 비롯되며 정확하고 신뢰할 수 있는 설명 가능성에 필요한 구조적 견고성이 부족합니다. 결과적으로 시각화는 종종 <strong>노이즈와 비일관성</strong>을 보이며, 이는 심층적인 해석 응용에 대한 유용성을 제한합니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>ColBERT</strong>와 <strong>ColPali</strong>와 같은 후기 상호작용 모델들은 텍스트 토큰과 이미지 패치 사이의 <strong>명시적이고 세밀한 정렬을 구조적으로 임베딩</strong>함으로써 이러한 한계를 해결합니다. 모달리티를 독립적으로 처리하고 후반부에 목표된 유사도 계산을 수행함으로써, 이러한 모델들은 각 텍스트 토큰이 관련된 이미지 영역과 의미 있게 연관되도록 보장합니다.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP은 토큰-패치 유사성을 시각화할 수 있지만, 이는 모델의 강건하거나 공식적인 \"어텐션\"이라기보다는 사후 해석 가능성을 위한 트릭에 가깝습니다. 그 이유는 다음과 같습니다.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "what-we-learned-at-iclr2025",
  "id": "68336ebaa4451f0001adcbad",
  "uuid": "90b46828-a059-4879-82a8-19f4c4e4675b",
  "title": "ICLR2025에서 배운 점",
  "html": "<p>ICLR 2025는 세계에서 가장 크고 영향력 있는 머신러닝 컨퍼런스 중 하나로, NeurIPS 및 ICML과 함께 높은 영향력을 가진 AI 연구를 위한 3대 주요 행사입니다. 올해 ICLR은 아시아에서 처음으로 개최되는 역사적인 이정표를 세웠으며, 4월 24일부터 28일까지 싱가포르 EXPO에서 열렸습니다. 이 시기는 2025년 1월 말 실리콘밸리에 충격을 주고 중국의 빠르게 발전하는 AI 연구를 보여준 \"DeepSeek 순간\" 이후 몇 달 지나지 않은 완벽한 시점이었습니다. 2024년 2월에 발효된 중국-싱가포르 30일 상호 비자 면제 협정과 함께, 우리는 컨퍼런스에서 중국인 참가자가 전례 없이 급증하는 것을 목격했습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1106\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png 2126w\" sizes=\"(min-width: 720px) 720px\"></figure><p>올해 우리 팀은 Sedigheh Eslami, Andreas Koukounas, Wang Feng 및 CEO Han Xiao가 더 나은 검색을 위한 <code>jina-clip-v2</code> 및 <code>ReaderLM-v2</code>에 대한 최신 연구를 보여주는 세 편의 연구 논문을 발표하며 싱가포르 여행을 가게 되어 매우 기뻤습니다. AI 세계의 나머지 부분은 점점 더 커지는 모델을 위한 군비 경쟁에 갇힌 것처럼 보이지만, 우리는 작은 모델이 설계를 올바르게 하면 그 이상의 성능을 발휘할 수 있다는 것을 증명하면서 일반적인 흐름에 역행하기로 결정했습니다.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1391\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png 2108w\" sizes=\"(min-width: 720px) 720px\"></figure><p>그러니 커피를 들고 편안하게 앉아 ICLR 연구 중 흥미로운 부분을 살펴보겠습니다. 먼저 작음이 왜 강력할 수 있는지에 대한 우리의 견해부터 시작합니다.</p><h2 id=\"mitigate-the-gap-improving-cross-modal-alignment-in-clip\">격차 완화: CLIP에서 교차 모달 정렬 개선</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2406.17639\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">격차 완화: CLIP에서 교차 모달 정렬 개선을 위한 접근 방식 조사</div><div class=\"kg-bookmark-description\">대조 언어-이미지 사전 훈련 (CLIP)은 제로샷 분류 및 교차 모달 비전-언어 작업에서 놀라운 개선을 보여주었습니다. 그러나 기하학적 관점에서 CLIP 向量模型 (Embedding) 공간에는 현저한 모달리티 격차가 있는 것으로 밝혀졌습니다. 이 격차로 인해 向量模型 (Embedding) 공간이 지나치게 희소하고 연결이 끊어지며, 서로 다른 모달리티가 초구의 뚜렷한 하위 영역에 조밀하게 분포됩니다. 이 연구에서 우리는 세 가지 주요 질문에 답하고자 합니다. 1. 다중 모달 인코더 간에 파라미터 공간을 공유하면 모달리티 격차가 줄어들까요? 2. 모달리티 내 분리를 통해 단일 모달 向量模型 (Embedding)을 밀어내면 격차를 완화할 수 있을까요? 3. 이러한 격차 감소 접근 방식은 다운스트림 성능에 어떤 영향을 미칠까요? 이러한 질문에 답하기 위해 AlignCLIP을 설계했으며, 광범위한 실험을 통해 AlignCLIP이 向量模型 (Embedding)의 교차 모달 정렬에서 눈에 띄는 향상을 달성하고, 그 결과 모달리티 격차를 줄이는 동시에 여러 제로샷 및 미세 조정 다운스트림 평가에서 성능을 향상시킨다는 것을 보여줍니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Sedigheh Eslami</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-17.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1279\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png 2120w\" sizes=\"(min-width: 720px) 720px\"></figure><p>CLIP 모델은 이미지-텍스트 작업에서 뛰어나지만 <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">\"모달리티 격차\"</a>로 어려움을 겪습니다. 이미지와 텍스트 向量模型 (Embedding)이 별도의 영역에 클러스터링되어 성능이 제한됩니다. Hasso Plattner Institute에서 박사 과정을 밟는 동안 우리 인턴인 Sedigheh Eslami가 주도한 이 연구는 이러한 근본적인 문제를 해결합니다.</p><p>우리는 간단한 벡터 변환이 向量模型 (Embedding) 구조를 손상시킨다는 것을 발견했습니다. 대신 <strong>AlignCLIP</strong>은 의미론적으로 정규화된 분리 목표와 함께 공유 인코더 파라미터를 사용합니다. 이 이중 접근 방식은 제로샷 및 미세 조정 작업에서 성능을 향상시키면서 모달리티 격차를 성공적으로 줄입니다.</p><p><strong>주요 내용:</strong></p><ul><li>모달리티 격차는 중요한 CLIP 성능 병목 현상입니다.</li><li>파라미터 공유 + 의미론적 분리는 모달 차이를 효과적으로 연결합니다.</li><li>이 접근 방식은 다운스트림 평가에서 측정 가능한 이점을 제공합니다.</li></ul><h2 id=\"jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images\">jina-clip-v2: 텍스트 및 이미지를 위한 다국어 다중 모달 向量模型 (Embeddings)</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2412.08802\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-clip-v2: 텍스트 및 이미지를 위한 다국어 다중 모달 向量模型 (Embeddings)</div><div class=\"kg-bookmark-description\">대조 언어-이미지 사전 훈련 (CLIP)은 교차 모달 정보 검색 및 다중 모달 이해 작업에 널리 사용되었습니다. 그러나 CLIP 모델은 주로 교차 모달 비전-언어 작업에 최적화되어 있으며 단일 모드 텍스트 작업에서는 성능이 저조합니다. 또한 이러한 모델은 종종 영어 데이터 세트에서 훈련되므로 다국어 이해가 부족합니다. 또한 시각적 이해 관점에서 이전 CLIP 기반 모델은 시각적으로 풍부한 문서에 대한 이해가 부족합니다. 이 연구에서 우리는 텍스트 전용 및 교차 모달 작업을 모두 지원하기 위해 다중 작업 및 다단계 대조 학습 패러다임을 통해 텍스트 쌍, 삼중 항 및 이미지-텍스트 쌍에서 훈련된 대조 비전-언어 모델인 jina-clip-v2를 제안합니다. 우리는 다국어 텍스트 인코더를 사용하고 힌디어, 중국어, 독일어, 프랑스어 등을 포함한 29개의 비영어권 언어의 다국어 텍스트와 시각적으로 풍부한 문서의 이미지를 포함하도록 훈련 데이터 세트를 확장합니다. 우리는 모델의 성능을 평가하고 jina-clip-v2가 영어 및 다국어 설정 모두에서 제로샷 텍스트 전용 검색, 의미론적 텍스트 유사성 및 교차 모달 검색 작업에서 최첨단 CLIP 기반 모델보다 눈에 띄는 개선을 달성한다는 것을 보여줍니다. jina-clip-v2는 또한 向量模型 (Embedding) 차원에서 유연성을 제공하여 사용자가 표현의 세분성을 선택할 수 있도록 합니다. jina-clip-v2는 https://huggingface.co/jinaai/jina-clip-v2에서 공개적으로 사용할 수 있습니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-18.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1115\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png 2268w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이것은 다중 작업, 다단계 대조 학습 접근 방식을 사용하여 텍스트 전용 및 교차 모달 작업을 모두 지원하는 다국어 다중 모달 向量模型 (Embedding) 모델인 <code>jina-clip-v2</code>의 배경이 되는 논문입니다. 이 모델은 텍스트 인코더 (Jina XLM-RoBERTa, 5억 6,100만 개의 파라미터)와 비전 인코더 (EVA02-L14, 3억 400만 개의 파라미터)를 결합하여 총 8억 6,500만 개의 파라미터를 갖습니다. 우리는 29개의 비영어권 언어의 다국어 텍스트와 시각적으로 풍부한 문서에서 훈련하며, 유연한 向量模型 (Embedding) 차원성을 위해 Matryoshka Representation Learning을 사용합니다.</p><p><strong>주요 내용:</strong></p><ul><li>공유 온도 파라미터를 사용하여 단일 배치에서 이미지-텍스트 및 텍스트-텍스트 데이터를 혼합하면 모달리티 정보 비대칭으로 인해 별도의 훈련보다 성능이 저하됩니다.</li><li>교차 모달 정렬을 위한 훈련은 본질적으로 순수한 텍스트 向量模型 (Embedding) 품질을 저하시키며, 근본적인 절충점을 보여줍니다.</li><li>向量模型 (Embedding)을 1,024차원에서 256차원으로 줄이면 성능 손실이 1% 미만으로 발생하여 고차원 표현에서 엄청난 비효율성을 드러냅니다.</li></ul><h2 id=\"readerlm-v2-small-language-model-for-html-to-markdown-and-json\">ReaderLM-V2: HTML에서 Markdown 및 JSON으로의 변환을 위한 小模型 (SLM)</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2503.01151\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ReaderLM-v2: HTML에서 Markdown 및 JSON으로의 변환을 위한 小模型 (SLM)</div><div class=\"kg-bookmark-description\">효율적인 웹 콘텐츠 추출을 위해 설계된 15억 개의 파라미터를 가진 컴팩트한 언어 모델인 ReaderLM-v2를 소개합니다. 우리 모델은 최대 512K 词元 (Tokens)의 문서를 처리하여 지저분한 HTML을 높은 정확도로 깨끗한 Markdown 또는 JSON 형식으로 변환하여 大模型 (LLM)의 기반을 다지는 데 이상적인 도구입니다. 모델의 효과는 (1) 웹 콘텐츠 추출을 반복적으로 작성, 개선 및 비판하여 고품질의 다양한 훈련 데이터를 생성하는 3단계 데이터 합성 파이프라인과 (2) 지속적인 사전 훈련과 다중 목표 최적화를 결합한 통합 훈련 프레임워크라는 두 가지 주요 혁신에서 비롯됩니다. 집중적인 평가 결과 ReaderLM-v2는 신중하게 큐레이팅된 벤치마크, 특히 100K 词元 (Tokens)을 초과하는 문서에서 GPT-4o-2024-08-06 및 기타 더 큰 모델보다 15-20% 더 우수한 성능을 보이며, 훨씬 낮은 계산 요구 사항을 유지합니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Feng Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-19.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1196\" height=\"912\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png 1196w\" sizes=\"(min-width: 720px) 720px\"></figure><p>이것은 효율적인 웹 콘텐츠 추출을 위해 설계된 15억 개의 파라미터를 가진 소형 언어 모델인 <code>ReaderLM-v2</code>의 기반이 되는 논문입니다. 이 모델은 최대 512K 개의 词元 (Tokens)의 문서를 처리하여 지저분한 HTML을 깔끔한 Markdown 또는 JSON 형식으로 변환합니다. 우리의 접근 방식은 지속적인 사전 학습, 지도 학습 미세 조정, 직접 선호도 최적화 및 자체 플레이 반복 튜닝을 결합한 통합 학습 프레임워크를 통해 반복적인 개선을 통해 고품질 학습 데이터를 생성하는 3단계 데이터 합성 파이프라인(DRAFT-REFINE-CRITIQUE)을 결합합니다. ReaderLM-v2는 벤치마크에서 GPT-4o 및 기타 더 큰 모델보다 15-20% 더 나은 성능을 보이며, 특히 100K 词元 (Tokens)을 초과하는 문서에서 뛰어난 성능을 보이는 동시에 훨씬 낮은 계산 요구 사항을 유지합니다.</p><p><strong>주요 내용:</strong></p><ul><li>1.5B 파라미터 모델은 HTML 추출에서 GPT-4o 및 32B 모델보다 15-20% 더 나은 성능을 보여주며, 작업별 미세 조정이 도메인 전문 지식에 대한 원시 규모보다 중요하다는 것을 입증합니다.</li><li>이 모델은 4단계 \"자체 플레이\"에서 자체 학습 데이터를 생성하여 사람이 큐레이팅한 데이터 세트보다 더 나은 데이터 세트를 만들고 재귀적 피드백을 통해 지속적으로 성능을 향상시킵니다.</li><li>이 모델은 학습 중에 치명적인 词元 (Token) 반복으로 어려움을 겪었지만, 차별적 표현을 장려하기 위해 대조 손실을 추가함으로써 이러한 퇴행 문제를 완전히 제거했습니다.</li></ul><h2 id=\"tips-text-image-pretraining-with-spatial-awareness\">TIPS: 공간 인식을 통한 텍스트-이미지 사전 학습</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2410.16512\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TIPS: 공간 인식을 통한 텍스트-이미지 사전 학습</div><div class=\"kg-bookmark-description\">이미지-텍스트 표현 학습은 최근 몇 년 동안 매우 인기를 얻었지만, 기존 모델은 공간 인식이 부족하고 밀집된 이해 작업에 대한 직접적인 적용 가능성이 제한적인 경향이 있습니다. 이러한 이유로, 자기 지도 이미지 전용 사전 학습은 명시적인 지도 신호가 없음에도 불구하고 여전히 많은 밀집된 시각 애플리케이션(예: 깊이 추정, 의미론적 분할)에 대한 기본 방법입니다. 본 논문에서는 공간 인식을 갖춘 새로운 범용 이미지-텍스트 모델을 제안함으로써 이미지-텍스트 및 자기 지도 학습 간의 이러한 격차를 해소하고 밀집된 글로벌 비전 작업에 효과적으로 사용할 수 있습니다. 공간 인식을 갖춘 텍스트-이미지 사전 학습(TIPS)이라고 하는 우리의 방법은 두 가지 간단하고 효과적인 통찰력을 활용합니다. 첫째, 텍스트 지도에 대해: 시끄러운 웹 이미지 캡션을 합성적으로 생성된 텍스트 설명으로 대체하면 공간적으로 인식된 표현을 학습하기 위한 훨씬 풍부한 신호로 인해 밀집된 이해 성능이 크게 향상된다는 것을 밝힙니다. 우리는 시끄럽고 합성적인 캡션을 결합하여 밀집되고 글로벌한 이해 작업 모두에서 개선을 가져오는 적응형 학습 방법을 제안합니다. 둘째, 학습 기술에 대해: 대조적인 이미지-텍스트 학습과 자기 지도 마스크 이미지 모델링을 결합하여 공간적 일관성을 장려하고 다운스트림 애플리케이션에 대한 실질적인 개선을 이끌어낼 것을 제안합니다. 이러한 두 가지 아이디어를 바탕으로 우리는 선별된 공개 이미지 세트에서 학습된 변환기 아키텍처를 사용하여 모델을 확장합니다. 우리의 실험은 총 16개의 데이터 세트를 포함하는 8개의 작업에서 수행되며, 여러 이미지 전용 및 이미지-텍스트 작업 모두에 대해 밀집되고 글로벌한 이해 모두에서 강력한 즉시 사용 가능한 성능을 보여줍니다. 코드 및 모델은 https://github.com/google-deepmind/tips에서 릴리스됩니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-24.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Kevis-Kokitsi Maninis</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-20.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png 2210w\" sizes=\"(min-width: 720px) 720px\"></figure><p>대조 학습으로 학습된 비전-언어 모델은 글로벌 이미지-텍스트 정렬에는 뛰어나지만 밀집된 공간 이해 작업에는 실패합니다. TIPS는 대조 학습과 마스크 이미지 모델링을 결합하고 공간 관계를 인코딩하는 합성적으로 생성된 캡션을 사용하여 작업별 미세 조정 없이도 밀집되고 글로벌한 이해에 적합한 向量模型 (Embeddings)을 생성합니다. 이 접근 방식은 더 나은 문서 이해 및 다중 모드 검색 애플리케이션을 위해 공간 인식을 向量模型 (Embedding)에 통합할 수 있는 방법을 보여줍니다.</p><p><strong>주요 내용:</strong></p><ul><li>공간 설명이 포함된 합성 캡션은 공간적으로 인식된 표현을 학습하기 위한 시끄러운 웹 캡션보다 더 풍부한 학습 신호를 제공합니다.</li><li>대조적인 이미지-텍스트 학습과 자기 지도 목표를 결합하면 글로벌 및 밀집된 이해 사이의 격차가 해소됩니다.</li><li>다양한 작업에 대한 즉시 사용 가능한 성능은 다양한 비전 애플리케이션에서 특수화된 미세 조정의 필요성을 없애줍니다.</li></ul><h2 id=\"cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies\">Cut Cross-Entropy: 큰 어휘에 대한 메모리 효율적인 손실 계산</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2411.09009\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">큰 어휘 언어 모델에서 손실을 줄이세요</div><div class=\"kg-bookmark-description\">언어 모델이 점점 더 커짐에 따라 어휘도 커집니다. 이로 인해 학습 중 LLM의 메모리 공간이 하나의 레이어, 즉 손실 계산의 교차 엔트로피로 불균형하게 이동했습니다. 교차 엔트로피는 각 입력 词元 (Token) 및 어휘 항목 쌍에 대한 항목이 있는 로짓 행렬을 구축하고, 작은 모델의 경우 LLM의 나머지 부분보다 10배 더 많은 메모리를 소비합니다. 우리는 모든 词元 (Token)에 대한 로짓을 글로벌 메모리로 구체화하지 않고 교차 엔트로피 손실을 계산하는 방법인 Cut Cross-Entropy(CCE)를 제안합니다. 오히려 CCE는 올바른 词元 (Token)에 대한 로짓만 계산하고 즉석에서 모든 로짓에 대한 로그 합계를 평가합니다. 우리는 행렬 곱셈과 플래시 메모리에서 어휘에 대한 로그 합계 감소를 수행하는 사용자 정의 커널을 구현하여 교차 엔트로피 계산에 대한 글로벌 메모리 소비를 무시할 수 있도록 합니다. 이것은 극적인 효과가 있습니다. Gemma 2(2B) 모델을 예로 들어 CCE는 손실 계산의 메모리 공간을 24GB에서 1MB로, 분류기 헤드의 총 학습 시간 메모리 소비를 28GB에서 1GB로 줄입니다. CCE의 처리량을 개선하기 위해 softmax의 고유한 희소성을 활용하고 기울기에 대한 무시할 수 있는(즉, 수치 정밀도 이하) 기여를 갖는 기울기 계산 요소를 건너뛸 것을 제안합니다. 실험은 메모리 소비의 극적인 감소가 학습 속도 또는 수렴을 희생하지 않고 달성된다는 것을 보여줍니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-25.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Erik Wijmans</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-21.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"1226\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png 1904w\" sizes=\"(min-width: 720px) 720px\"></figure><p>교차 엔트로피 계산은 큰 어휘 언어 모델에서 메모리 사용량을 지배하며, batch_size × vocabulary_size에 비례하는 로짓 행렬의 구체화를 필요로 합니다. CCE는 사용자 정의 CUDA 커널을 사용하여 즉석에서 필요한 구성 요소만 계산하도록 계산을 재구성하여 메모리 소비를 기가바이트에서 메가바이트로 줄이면서 동일한 학습 역학을 유지합니다. 이를 통해 제한된 하드웨어에서 더 큰 어휘로 向量模型 (Embedding) 및 重排器 (Reranker) 모델을 학습할 수 있으며, 특히 다국어 및 도메인별 애플리케이션에 유용합니다.</p><p><strong>주요 내용:</strong></p><ul><li>교차 엔트로피 손실 계산은 큰 어휘 모델의 경우 학습 메모리의 90%를 소비할 수 있으며, 주요 병목 현상이 됩니다.</li><li>로그 합계 지수 항의 즉석 계산은 수학적 근사 없이 전체 로짓 행렬을 구체화할 필요성을 없애줍니다.</li><li>사용자 정의 커널 구현은 정확한 수렴 속성을 유지하면서 극적인 메모리 감소를 가능하게 합니다.</li></ul><h2 id=\"flexprefill-context-aware-sparse-attention-for-long-sequences\">FlexPrefill: 긴 시퀀스를 위한 컨텍스트 인식 희소 주의</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.20766\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">FlexPrefill: 효율적인 긴 시퀀스 추론을 위한 컨텍스트 인식 희소 주의 메커니즘</div><div class=\"kg-bookmark-description\">대규모 언어 모델 (LLM, Large Language Model)은 긴 시퀀스 추론, 특히 프롬프트 길이가 길어질수록 복잡도가 제곱으로 증가하는 어텐션 사전 채우기 단계에서 계산상의 어려움에 직면합니다. 이러한 문제를 완화하기 위한 이전의 노력들은 고정된 희소 어텐션 패턴에 의존하거나 제한된 사례를 기반으로 희소 어텐션 패턴을 식별하는 데 의존했습니다. 그러나 이러한 방법들은 다양한 입력 요구에 효율적으로 적응할 수 있는 유연성이 부족했습니다. 본 논문에서는 각 입력 및 어텐션 헤드의 특정 요구 사항을 충족하기 위해 희소 어텐션 패턴과 계산 예산을 실시간으로 동적으로 조정하는 유연한 희소 사전 채우기 메커니즘인 FlexPrefill을 소개합니다. 우리 방법의 유연성은 두 가지 주요 혁신을 통해 입증됩니다. 1) 쿼리 인식 희소 패턴 결정: Jensen-Shannon 발산을 측정하여 이 구성 요소는 쿼리 특정의 다양한 어텐션 패턴과 미리 정의된 어텐션 패턴 사이를 적응적으로 전환합니다. 2) 누적 어텐션 기반 인덱스 선택: 이 구성 요소는 다양한 어텐션 패턴을 기반으로 계산될 쿼리-키 인덱스를 동적으로 선택하여 어텐션 점수의 합이 미리 정의된 임계값을 충족하도록 합니다. FlexPrefill은 프롬프트를 기반으로 각 어텐션 헤드의 희소 패턴과 희소 비율을 적응적으로 최적화하여 긴 시퀀스 추론 작업의 효율성을 향상시킵니다. 실험 결과는 기존 방법에 비해 속도와 정확도 모두에서 상당한 개선을 보여주며 LLM 추론을 위한 더욱 유연하고 효율적인 솔루션을 제공합니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-26.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xunhao Lai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-22.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1882\" height=\"1254\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png 1882w\" sizes=\"(min-width: 720px) 720px\"></figure><p>긴 시퀀스 트랜스포머 추론은 이차 어텐션 복잡성으로 인해 어려움을 겪습니다. FlexPrefill은 Jensen-Shannon 발산을 사용하여 헤드별로 희소 어텐션 패턴을 동적으로 결정하고 누적 어텐션 점수를 기반으로 계산 예산을 적응적으로 할당하여 다양한 콘텐츠 유형에서 최소한의 정확도 손실로 상당한 속도 향상을 달성합니다. 이 방법은 검색 시스템을 위한 긴 문서의 효율적인 처리를 가능하게 하며, 더 작은 언어 모델이 더 나은 문서 이해를 위해 확장된 컨텍스트를 처리할 수 있도록 합니다.</p><p><strong>주요 내용:</strong></p><ul><li>콘텐츠 유형에 적응된 동적 희소 어텐션 패턴이 다양한 입력 특성에서 고정된 희소 전략보다 성능이 뛰어납니다.</li><li>어텐션 점수 누적을 기반으로 한 헤드별 적응형 예산 할당은 실시간으로 계산 분산을 최적화합니다.</li><li>컨텍스트 인식 희소성은 모델 재학습 없이 0.1% 정확도 손실로 13.7배의 속도 향상을 달성합니다.</li></ul><h2 id=\"effective-post-training-embedding-compression-via-temperature-control\">온도 제어를 통한 효과적인 사후 학습 向量模型 (Embeddings) 압축</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://openreview.net/forum?id=szRmEM8Kx5\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">온도 제어를 통한 효과적인 사후 학습 向量模型 (Embeddings) 압축</div><div class=\"kg-bookmark-description\">고정 크기의 학습된 표현 (밀집 표현 또는 向量模型 (Embeddings))은 언어, 시각 또는 음성 양식 전반에 걸쳐 많은 머신 러닝 애플리케이션에서 널리 사용됩니다. 이 논문은...</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-37.ico\" alt=\"\"><span class=\"kg-bookmark-author\">OpenReview.net</span><span class=\"kg-bookmark-publisher\">Georgiana Dinu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/pdf_icon_blue.svg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1230\" height=\"906\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png 1230w\" sizes=\"(min-width: 720px) 720px\"></figure><p>대조 학습에서 온도 스케일링은 학습된 向量模型 (Embeddings)의 고유 차원에 상당한 영향을 미치며, 낮은 온도는 더 압축 가능한 표현을 생성합니다. 이 논문은 온도 집계 방법이 검색 성능을 유지하면서 向量模型 (Embeddings) 차원을 10배까지 줄일 수 있음을 보여주며, 클러스터링 효과와 검색 정확도 간의 절충점을 보여줍니다. 이는 메모리 제약 조건이 프로덕션 애플리케이션에 중요한 밀집 검색 시스템의 효율적인 배포를 가능하게 합니다.</p><p><strong>주요 내용:</strong></p><ul><li>대조 학습에서 낮은 온도 값은 더 효과적으로 압축되는 더 낮은 고유 차원을 가진 向量模型 (Embeddings)을 생성합니다.</li><li>온도 집계 기술은 검색 작업 전반에서 품질 저하를 최소화하면서 10배의 압축률을 달성합니다.</li><li>학습 중 온도에 대한 체계적인 제어는 압축-성능 절충점을 최적화하기 위한 직접적인 메커니즘을 제공합니다.</li></ul><h2 id=\"attention-in-large-language-models-yields-efficient-zero-shot-re-rankers\">대규모 언어 모델의 어텐션은 효율적인 제로샷 重排器 (Reranker)를 생성합니다.</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2410.02642\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">대규모 언어 모델의 어텐션은 효율적인 제로샷 重排器 (Reranker)를 생성합니다.</div><div class=\"kg-bookmark-description\">정보 검색 (IR) 시스템은 현대 디지털 생활에서 중요한 역할을 수행했으며 검색 증강 생성을 통해 생성형 AI의 새로운 시대에서 지속적인 유용성을 확고히 했습니다. 강력한 언어 처리 기능과 뛰어난 다재다능함을 갖춘 대규모 언어 모델 (LLM, Large Language Model)은 IR 시스템에서 제로샷 重排器 (Reranker)로 널리 사용됩니다. 지금까지 LLM 기반 重排器 (Reranker) 방법은 강력한 생성 기능에 의존하여 특수 모델 또는 강력한 독점 모델로 사용이 제한됩니다. 이러한 제한 사항을 고려하여 다음과 같은 질문을 던집니다. LLM이 重排 (Re-ranking)를 수행하는 데 자기 회귀 생성이 필요하고 최적인가? 우리는 LLM 내에 생성형 AI를 통해 잠재력을 최대한 활용하지 못할 수 있는 重排 (Re-ranking)와 관련된 풍부한 신호가 있다고 가정합니다. 이러한 신호를 보다 직접적으로 활용하기 위해 정확하고 효율적인 重排 (Re-ranking)를 위해 검색 쿼리로 인한 어텐션 패턴의 변화를 활용하는 새로운 방법인 컨텍스트 내 重排 (ICR)를 제안합니다. LLM의 고유한 편향을 완화하기 위해 콘텐츠가 없는 쿼리를 사용하는 보정 방법을 제안합니다. 생성이 없기 때문에 ICR은 $N$개의 문서를 重排 (Re-ranking)하는 데 두 번 ($O(1)$)의 순방향 패스만 필요하므로 최소 $O(N)$ 순방향 패스가 필요한 생성형 重排器 (Reranker) 방법보다 훨씬 효율적입니다. 또한 우리의 새로운 설계는 특수 교육 없이도 모든 LLM에 ICR을 적용할 수 있게 하여 잘 구성된 순위를 보장합니다. 표준 단일 홉 및 다중 홉 정보 검색 벤치마크에서 두 개의 인기 있는 오픈 웨이트 LLM을 사용한 광범위한 실험 결과에 따르면 ICR은 RankGPT보다 성능이 뛰어나면서도 실제 대기 시간을 60% 이상 단축합니다. 자세한 분석을 통해 ICR의 성능은 특히 더 복잡한 重排 (Re-ranking) 신호가 필요한 작업에서 강력하다는 것을 보여줍니다. 우리의 연구 결과는 텍스트 생성 외에도 오픈 웨이트 LLM을 활용하는 새로운 방법에 대한 추가 탐색을 요구합니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-27.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Shijie Chen</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-23.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd_nbgJG03yk0oOma7ozLQ6zutKGy3ngkVLKUwmp3ie6UPp2RR6qjwsWzwwtP0QzyAneCTD24nrPXpA085rkjtS_HmlrkHbrksxSjsaknx1lb9OtgtlACmwoOZkoRK9oPb4Haf?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>컨텍스트 내 重排 (Re-ranking) (ICR)는 LLM에서 어텐션 패턴 변화를 활용하여 텍스트 생성 없이 문서 重排 (Re-ranking)을 수행하여 계산 복잡도를 O(N log N)에서 O(1)로 줄입니다. 이 방법은 레이어와 헤드에 걸쳐 어텐션 가중치를 집계하여 관련성 점수를 계산하고 콘텐츠가 없는 쿼리 보정을 통해 LLM 편향을 완화합니다. 이 접근 방식은 특수 미세 조정 또는 비용이 많이 드는 생성 프로세스 없이 오픈 웨이트 모델을 사용하여 효율적인 重排 (Re-ranking)을 가능하게 합니다.</p><p><strong>주요 내용:</strong> </p><ul><li>LLM의 어텐션 패턴은 텍스트 생성이 필요 없이 효과적인 문서 重排 (Re-ranking)을 위한 충분한 신호를 포함합니다.</li><li>콘텐츠가 없는 쿼리 보정은 어텐션 기반 점수 메커니즘의 고유한 편향을 성공적으로 완화합니다.</li><li>ICR은 특히 복잡한 다중 홉 검색 작업에서 생성형 방법에 비해 우수한 성능과 효율성을 달성합니다.</li></ul><h2 id=\"bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization\">직접 선호도 최적화를 위한 쌍별 데이터의 상관 관계 브리징 및 모델링</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2408.07471\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">직접 선호도 최적화를 위한 쌍별 데이터의 상관 관계 브리징 및 모델링</div><div class=\"kg-bookmark-description\">직접 선호도 최적화 (DPO, Direct preference optimization)는 널리 사용되는 오프라인 선호도 최적화 알고리즘으로, 쌍별 선호도 데이터를 사용하여 대규모 언어 모델 (LLM, large language models)을 인간이 원하는 행동에 맞추는 것을 목표로 합니다. 그러나 쌍별 데이터 내에서 승리 응답과 패배 응답의 생성은 일반적으로 격리되어 있어 이들 간의 상관 관계가 약해지고 최적의 정렬 성능을 저해합니다. 이 문제를 해결하기 위해 쌍별 데이터에서 상관 관계를 연결하고 모델링하기 위한 효과적인 프레임워크인 BMC를 제안합니다. 첫째, 대상 수정 (targeted modifications)을 통해 쌍별 선호도 신호의 일관성과 정보성을 높여 승리 응답을 참조하여 패배 응답을 개선함으로써 의사 승리 응답을 합성합니다. 둘째, DPO만으로는 이러한 상관 관계를 모델링하고 미묘한 변화를 포착하기에 불충분하다는 것을 확인합니다. 따라서 학습 중에 정책 모델의 신뢰도를 동적으로 활용하여 토큰 (Tokens) 수준 상관 관계 학습을 제안합니다. QA, 수학 및 명령 추종 작업에 대한 포괄적인 실험은 DPO를 포함한 경쟁력 있는 기준선을 크게 능가하는 접근 방식의 효과를 입증합니다. 또한 심층적인 정량적 분석을 통해 DPO보다 우수한 성능을 보이는 이유를 밝히고 다른 DPO 변형에 대한 다재다능함을 보여줍니다. 리포지토리는 https://github.com/YJiangcm/BMC 에서 확인할 수 있습니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-28.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Yuxin Jiang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-24.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd94pEqUtljUS8gF3KonDFqs9umcVCfXEASHlAeCjl07YMviucHiIj1doZIe5_VHSVxthzhgA_ta0E90vQVcunSRj0UnHsubFzD75ow-EfNICcDadQvdtUx-WOZGt9v9rFB_4E?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>기존 DPO는 선호도 쌍에서 선택된 응답과 거부된 응답 간의 상관 관계가 약하여 정렬 효과가 제한됩니다. BMC는 승리 응답과 패배 응답 사이를 보간하는 의사 선호 응답을 합성한 다음 정책 모델 신뢰도를 사용하여 토큰 수준 상관 관계 모델링을 적용합니다. 2단계 접근 방식은 먼저 대상 수정 (targeted modifications)을 통해 선호도 쌍을 연결한 다음 학습 중에 세분화된 상관 관계를 모델링하여 학습 신호 품질을 향상시킵니다.</p><p><strong>주요 내용:</strong></p><ul><li>선호도 데이터에서 선택된 응답과 거부된 응답 간의 약한 상관 관계는 모델 정렬에 대한 DPO 효과를 크게 제한합니다.</li><li>선호도 쌍 간의 보간으로 의사 선호 응답을 합성하면 최적화를 위한 더 풍부한 학습 신호가 제공됩니다.</li><li>정책 신뢰도를 사용하는 토큰 수준 상관 관계 모델링은 훈련 신호에 동적으로 가중치를 부여하여 선호도 데이터의 미묘한 변화를 포착합니다.</li></ul><h2 id=\"taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer\">TAID: 효율적인 지식 전송을 위한 시간 적응 보간 증류 (Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer)</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2501.16937\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TAID: 언어 모델에서 효율적인 지식 전송을 위한 시간 적응 보간 증류 (Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models)</div><div class=\"kg-bookmark-description\">인과적 언어 모델은 놀라운 기능을 보여주었지만 크기로 인해 리소스가 제한된 환경에 배포하는 데 상당한 어려움이 있습니다. 대규모 교사 모델에서 소규모 학생 모델로 지식을 전송하는 데 널리 사용되는 기술인 지식 증류 (Knowledge distillation)는 모델 압축을 위한 유망한 접근 방식을 제시합니다. 중요한 문제점은 교사 모델과 학생 모델 간의 주요 차이점, 즉 상당한 용량 격차, 모드 평균화 및 모드 붕괴에 있으며, 이는 증류 중에 장벽으로 작용합니다. 이러한 문제를 해결하기 위해 학생과 교사 분포를 적응형 중간 분포를 통해 동적으로 보간하여 학생의 초기 분포에서 교사의 분포로 점진적으로 이동하는 새로운 지식 증류 접근 방식인 $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$를 소개합니다. TAID가 모드 붕괴를 방지하는 능력을 보여주는 이론적 분석을 제공하고 모드 평균화와 모드 붕괴의 균형을 맞추면서 용량 격차를 해결하는 데 효과적임을 경험적으로 보여줍니다. 포괄적인 실험은 명령 조정 및 사전 훈련 시나리오 모두에서 다양한 모델 크기와 아키텍처에서 TAID의 우수한 성능을 입증합니다. 또한 언어 작업을 위한 $\\texttt{TAID-LLM-1.5B}$ 및 비전 언어 작업을 위한 $\\texttt{TAID-VLM-2B}$의 두 가지 최첨단 컴팩트 기반 모델을 개발하여 TAID의 실제적인 영향을 보여줍니다. 이러한 결과는 TAID가 고성능 및 효율적인 모델을 생성하는 데 효과적임을 입증하여 보다 접근 가능한 AI 기술 개발을 발전시킵니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-29.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Makoto Shing</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-25.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe9EWtq20jDVieU8M2BPDP5kENd3oSJKwIKNnKq_9bB4mb9vtNvjK-RMx8ZB29EhnyjIST90b2HRNek6bSkPXFlOxzTPhUAjf86d6iBCphJtgjfcxrCdY__HcDW9ADgVla1mVWBpQ?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"312\"></figure><p>지식 증류 (Knowledge distillation)는 대형 모델과 소형 모델 간에 지식을 전송할 때 용량 격차, 모드 평균화 및 모드 붕괴로 인한 문제에 직면합니다. TAID는 학생 및 교사 분포 사이를 보간하는 동적 중간 교사를 도입하여 훈련 진행 상황에 따라 대상 분포를 점진적으로 조정합니다. 이 접근 방식은 이론적 보장을 통해 모드 붕괴를 방지하고 다양한 모델 크기에서 우수한 성능을 달성하여 컴팩트하면서도 유능한 언어 모델 개발을 가능하게 합니다.</p><p><strong>주요 내용:</strong></p><ul><li>훈련 중에 적응하는 동적 중간 교사는 고정된 교사 증류에 비해 더 부드러운 학습 궤적을 제공합니다.</li><li>TAID는 적응형 보간을 통해 모드 붕괴를 방지하는 동시에 다양한 용량 격차에서 지식 전송의 균형을 유지합니다.</li><li>이 방법을 사용하면 특수 아키텍처나 광범위한 하이퍼파라미터 조정 없이 최첨단 컴팩트 모델을 훈련할 수 있습니다.</li></ul><h2 id=\"svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression\">SVD-LLM: 대규모 언어 모델 압축을 위한 절단 인식 특이값 분해 (Truncation-Aware Singular Value Decomposition for Large Language Model Compression)</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2403.07378\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">SVD-LLM: 대규모 언어 모델 압축을 위한 절단 인식 특이값 분해 (Truncation-aware Singular Value Decomposition for Large Language Model Compression)</div><div class=\"kg-bookmark-description\">대규모 언어 모델 (LLM, Large Language Models)의 발전은 상당한 크기로 인해 실용적인 배포를 위해 LLM 압축 방법이 필요합니다. 특이값 분해 (SVD, Singular Value Decomposition)는 LLM 압축을 위한 유망한 솔루션을 제공합니다. 그러나 최첨단 SVD 기반 LLM 압축 방법에는 두 가지 주요 제한 사항이 있습니다. 더 작은 특이값을 절단하면 압축 손실이 커질 수 있으며, SVD 절단 후 압축된 가중치에 대한 업데이트가 부족합니다. 이 연구에서는 기존 방법의 제한 사항을 해결하는 SVD 기반 사후 훈련 LLM 압축 방법인 SVD-LLM을 제안합니다. SVD-LLM은 특이값과 압축 손실 간의 직접적인 매핑을 보장하기 위해 절단 인식 데이터 화이트닝 기술을 통합합니다. 또한 SVD-LLM은 SVD 압축 후 정확도 저하를 보상하기 위해 순차적 저랭크 근사를 사용하여 파라미터 업데이트를 채택합니다. 10개의 데이터 세트와 3개의 다른 LLM 제품군에서 7개의 모델을 3개의 다른 규모로 평가합니다. 결과는 특히 높은 모델 압축률에서 최첨단 기술에 대한 SVD-LLM의 우수성을 입증합니다. 코드는 https://github.com/AIoT-MLSys-Lab/SVD-LLM 에서 확인할 수 있습니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-30.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xin Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-26.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXenhj46Ar7NKFevDTmA3FK2dnjd7nQxdhULJ1H3Je-2OKoQN6_Ov8km-AvIldpEriENz2Q465hq2yoOZ1lLAle7ijbMgSK0ME9UxNeIN3yqyRFtRO_3FFXEyXdI04wndPS17a-3?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>기존 SVD 기반 압축 방법은 근사화 중에 입력 활성화를 고려하지 못하고 절단 후 미세 조정이 부족합니다. SVD-LLM은 활성화 분포를 고려하는 절단 인식 데이터 화이트닝을 통합하고 압축 후 LoRA 기반 미세 조정을 적용합니다. 이 방법은 특이값과 압축 손실 간의 이론적 연결을 설정하여 구조화된 가지치기 및 양자화 접근 방식보다 뛰어난 원칙적인 압축 결정을 가능하게 합니다.</p><p><strong>주요 내용:</strong></p><ul><li>입력 활성화를 고려하는 절단 인식 데이터 화이트닝은 활성화에 구애받지 않는 방법보다 SVD 압축 효과를 크게 향상시킵니다.</li><li>압축 후 LoRA 미세 조정은 저랭크 인수 분해의 이점을 유지하면서 정확도 저하를 보상합니다.</li><li>특이값을 압축 손실에 연결하는 이론적 분석은 휴리스틱 접근 방식보다 뛰어난 원칙적인 절단 결정을 가능하게 합니다.</li></ul><h2 id=\"see-what-you-are-told-visual-attention-sink-in-large-multimodal-models\">당신이 들은 것을 보세요: 대규모 멀티모달 모델의 시각적 주의 싱크 (Visual Attention Sink in Large Multimodal Models)</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2503.03321\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">당신이 들은 것을 보세요: 대규모 멀티모달 모델의 시각적 주의 싱크 (Visual Attention Sink in Large Multimodal Models)</div><div class=\"kg-bookmark-description\">대규모 멀티모달 모델 (LMM, Large multimodal models)은 트랜스포머 디코더에서 텍스트와 시각적 토큰 간의 어텐션 메커니즘을 활용하여 이미지를 \"봅니다\". 이상적으로, 이러한 모델은 텍스트 토큰과 관련된 주요 시각적 정보에 집중해야 합니다. 그러나 최근 연구 결과에 따르면 LMM은 해당 텍스트와 관련이 없는 특정 시각적 토큰에 대해서도 높은 어텐션 가중치를 일관되게 할당하는 경향이 있는 것으로 나타났습니다. 본 연구에서는 이러한 관련 없는 시각적 토큰의 출현 배경을 조사하고 그 특징을 살펴봅니다. 연구 결과, 이러한 현상은 특정 숨겨진 상태 차원의 과도한 활성화로 인해 발생하며, 이는 언어 모델에서 발견되는 어텐션 싱크와 유사합니다. 따라서 우리는 이러한 현상을 시각적 어텐션 싱크라고 부릅니다. 특히, 우리의 분석 결과 높은 어텐션 가중치를 받음에도 불구하고 관련 없는 시각적 싱크 토큰을 제거해도 모델 성능에 영향을 미치지 않는 것으로 나타났습니다. 결과적으로 우리는 이러한 토큰에 대한 어텐션을 잉여 자원으로 재활용하여 이미지에 대한 집중도를 높이기 위해 어텐션 예산을 재분배합니다. 이를 위해 우리는 이미지 중심 헤드에서 어텐션을 재분배하는 VAR (Visual Attention Redistribution)이라는 방법을 도입했습니다. 이미지 중심 헤드는 시각적 정보에 본질적으로 집중하는 것으로 확인되었습니다. VAR은 추가적인 학습, 모델 또는 추론 단계 없이 다양한 LMM에 원활하게 적용하여 일반적인 시각-언어 작업, 시각적 환각 작업 및 시각 중심 작업을 포함한 광범위한 작업에서 성능을 향상시킬 수 있습니다. 실험 결과 VAR을 통해 LMM이 내부 어텐션 메커니즘을 조정하여 시각적 정보를 보다 효과적으로 처리할 수 있으며, LMM의 멀티모달 기능을 향상시키는 새로운 방향을 제시할 수 있음을 보여줍니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-31.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Seil Kang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-27.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrHjLu7qX5TOjtDH10svBjs-6rihxNRpgS3Bq9r8qtY9UvOC4LqyBo-NDWeESuRrv-vj6btANt6doA4IneaENN1712o3kzHhQwx20PR62b8JKDA5jIjCNgKAhXoCp9bEcbadyfPA?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>대규모 멀티모달 모델은 해당 텍스트 토큰과 관련이 없는 특정 시각적 토큰에 높은 어텐션 가중치를 일관되게 할당하는 \"시각적 어텐션 싱크 (visual attention sink)\"라는 현상을 보입니다. 이러한 관련 없는 시각적 토큰은 언어 모델의 어텐션 싱크와 유사하게 특정 숨겨진 상태 차원에서 과도한 활성화로 인해 발생합니다. VAR (Visual Attention Redistribution) 방법은 이미지 중심 어텐션 헤드를 식별하고 싱크 토큰에서 의미 있는 시각적 콘텐츠로 어텐션 예산을 재분배하여 추가 학습 없이 시각-언어 작업 전반에서 성능을 향상시킵니다.</p><p><strong>주요 내용:</strong> </p><ul><li>시각적 싱크 토큰은 기본 언어 모델에서 상속된 고정된 차원에서 극단적인 활성화 크기로 식별할 수 있습니다.</li><li>시각적 싱크 토큰을 제거해도 높은 어텐션 가중치를 받음에도 불구하고 모델 성능에 영향을 미치지 않으며 이는 계산 자원 낭비를 의미합니다.</li><li>VAR은 싱크 토큰에서 의미 있는 시각적 콘텐츠로 어텐션을 재분배하여 일반적인 시각-언어, 환각 감소 및 시각 중심 작업에서 성능을 향상시킵니다.</li></ul><h2 id=\"towards-semantic-equivalence-of-tokenization-in-multimodal-llm\">멀티모달 LLM에서 토큰화의 의미론적 등가성 확보</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2406.05127\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">멀티모달 LLM에서 토큰화의 의미론적 등가성 확보</div><div class=\"kg-bookmark-description\">MLLM (Multimodal Large Language Models)은 시각-언어 작업 처리에서 뛰어난 능력을 보여주었습니다. MLLM의 핵심 중 하나는 입력 시각 신호를 LLM에 가장 유용한 특징 표현으로 효율적으로 변환하는 비전 토큰화입니다. 그러나 비전과 언어 간의 의미론적 정렬에 필수적인 기존 비전 토크나이저는 여전히 문제가 있습니다. 기존 방법은 시각적 입력을 과도하게 조각화하여 시각적 의미론적 무결성을 손상시킵니다. 이를 해결하기 위해 본 논문에서는 동적 클러스터링 알고리즘을 통해 시각적 특징을 의미론적 단위로 그룹화하여 이미지 복잡성에 따라 토큰 수를 유연하게 결정하는 새로운 동적 의미론적 등가 비전 토크나이저 (SeTok)를 제안합니다. 결과적으로 생성된 비전 토큰은 의미론적 무결성을 효과적으로 유지하고 저주파 및 고주파 시각적 특징을 모두 캡처합니다. SeTok을 탑재한 제안된 MLLM (Setokim)은 실험 결과에서 입증되었듯이 다양한 작업에서 뛰어난 성능을 보여줍니다. 프로젝트 페이지는 https://chocowu.github.io/SeTok-web/에 있습니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Shengqiong Wu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-28.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXtLgOhJPcWaPKTtGm7D3lK6tc7EhPQGXleGMHMqYG_KFVTxCSBGOd8z6xovad6UMgDjTWPBFfKqD4J2gSD6L6YXpSaTlGNNrLWiViAlfPkKinc9jNjsD2Ulnrh0tZQ74RR62tvQ?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>멀티모달 LLM의 기존 비전 토큰화 방법은 고정된 패치를 사용하여 시각적 입력을 조각화하여 의미론적 무결성을 손상시키고 시각-언어 정렬 불량을 초래합니다. SeTok (Semantic-Equivalent Vision Tokenizer)은 토큰 수를 이미지 복잡성에 따라 조정하여 시각적 특징을 일관된 의미론적 단위로 그룹화하는 동적 클러스터링을 통해 이를 해결합니다. 이 시스템은 언어와의 의미론적 정렬을 위한 대조 손실과 이미지 재구성을 위한 픽셀 수준의 세부 정보를 보존하기 위한 재구성 손실의 두 가지 학습 목표를 사용합니다.</p><p><strong>주요 내용:</strong> </p><ul><li>고정 패치 토큰화는 임의의 패치 경계를 넘어 객체를 조각화하여 시각적 의미론적 무결성을 파괴합니다.</li><li>동적 클러스터링 알고리즘은 고정된 그리드 구조가 아닌 이미지 의미론적 복잡성을 기반으로 최적의 토큰 수를 적응적으로 결정할 수 있습니다.</li><li>이중 목표 학습은 언어와의 의미론적 정렬과 재구성 작업을 위한 충분한 시각적 세부 정보 보존 사이의 균형을 맞춥니다.</li></ul><h2 id=\"hymba-a-hybrid-head-architecture-for-small-language-models\">Hymba: 소형 언어 모델을 위한 하이브리드 헤드 아키텍처</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2411.13676\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Hymba: 소형 언어 모델을 위한 하이브리드 헤드 아키텍처</div><div class=\"kg-bookmark-description\">향상된 효율성을 위해 트랜스포머 어텐션 메커니즘과 SSM (state space models)을 통합하는 하이브리드 헤드 병렬 아키텍처를 특징으로 하는 소형 언어 모델 제품군인 Hymba를 제안합니다. 어텐션 헤드는 고해상도 리콜을 제공하고 SSM 헤드는 효율적인 컨텍스트 요약을 가능하게 합니다. 또한 프롬프트에 접두사로 추가되어 중요한 정보를 저장하고 어텐션 메커니즘과 관련된 \"강제 참석\" 부담을 완화하는 학습 가능한 메타 토큰을 도입합니다. 이 모델은 계층 간 KV (key-value) 공유 및 부분 슬라이딩 윈도우 어텐션을 통합하여 더욱 최적화되어 컴팩트한 캐시 크기를 제공합니다. 개발 과정에서 동일한 설정에서 다양한 아키텍처를 비교하는 제어된 연구를 수행했으며 제안된 아키텍처의 상당한 이점을 확인했습니다. 특히 Hymba는 소형 LM에 대한 최첨단 결과를 달성합니다. Hymba-1.5B-Base 모델은 2B 미만의 모든 공개 모델을 능가하고 평균 정확도가 1.32% 더 높고 캐시 크기가 11.67배 감소하고 처리량이 3.49배 향상된 Llama-3.2-3B보다 뛰어납니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-33.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xin Dong</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-29.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqTXEnWvnqhnbdUKsRw-mZ8hzuqwIbX_dnFwiY4BHa8DF4ViWiekeIRVlRBtQkJF8a2EPv5U_H5kvqxFQfCg0jGplWefzce1RHzHBd17D93k6DpE3vNurR0Ufg7kMEJ_C4IeDBZQ?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"552\" height=\"573\"></figure><p>Hymba는 각 계층 내에서 트랜스포머 어텐션 메커니즘과 SSM (state space models)을 병렬로 결합하여 동시 고해상도 리콜 및 효율적인 컨텍스트 요약을 가능하게 하는 하이브리드 헤드 아키텍처를 도입합니다. 이 아키텍처는 학습 가능한 메타 토큰, 계층 간 키-값 공유 및 부분 슬라이딩 윈도우 어텐션을 통합하여 컴팩트한 캐시 크기를 달성합니다. Hymba-1.5B는 모든 2B 미만 모델을 능가하고 Llama-3.2-3B보다 뛰어난 성능을 보이면서 캐시 감소율 11.67배, 처리량 향상률 3.49배를 달성했습니다.</p><p><strong>주요 내용:</strong> </p><ul><li>병렬 하이브리드 헤드 아키텍처는 상호 보완적인 메커니즘의 동시 처리를 가능하게 함으로써 어텐션 및 SSM 구성 요소의 순차적 스태킹보다 뛰어난 성능을 제공합니다.</li><li>학습 가능한 메타 토큰은 압축된 세계 지식 역할을 하며 softmax 어텐션 메커니즘의 \"강제 참석\" 부담을 완화합니다.</li><li>계층 간 키-값 공유 및 슬라이딩 윈도우 어텐션 최적화는 성능 저하 없이 극적인 캐시 크기 감소를 달성합니다.</li></ul>",
  "comment_id": "68336ebaa4451f0001adcbad",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-25T21:25:46.000+02:00",
  "updated_at": "2025-05-26T00:06:37.000+02:00",
  "published_at": "2025-05-26T00:06:37.000+02:00",
  "custom_excerpt": "We collect some most interesting papers in ICLR 2025, featuring TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba etc.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "63340e5387b80b004db80543",
      "name": "Events",
      "slug": "events",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/events/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "63340e5387b80b004db80543",
    "name": "Events",
    "slug": "events",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/events/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-we-learned-at-iclr2025/",
  "excerpt": "ICLR 2025에서 가장 흥미로운 논문들을 모았습니다. TIPS, FlexPrefill, 제로샷 재정렬기 (Zero-Shot Rerankers), SVD-LLM, Hymba 등이 있습니다.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
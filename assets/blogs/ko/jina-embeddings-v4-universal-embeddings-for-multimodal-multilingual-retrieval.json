{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4: 멀티모달 다국어 검색을 위한 유니버설 벡터 모델 (Embeddings)",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - 검색 파운데이션 모델</div><div class=\"kg-bookmark-description\">멀티모달 및 다국어 검색을 위한 유니버설 向量模型 (Embeddings) 모델</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">검색 파운데이션 모델</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: 멀티모달 다국어 검색을 위한 유니버설 向量模型 (Embeddings)</div><div class=\"kg-bookmark-description\">jina-embeddings-v4를 소개합니다. 이는 38억 개의 파라미터를 가진 멀티모달 向量模型 (Embeddings) 모델로서, 후기 상호 작용 스타일에서 단일 벡터와 다중 벡터 向量模型 (Embeddings)을 모두 지원하는 새로운 아키텍처를 통해 텍스트 및 이미지 표현을 통합합니다. 이 모델은 쿼리 기반 정보 검색, 교차 모달 의미 유사성 및 프로그래밍 코드 검색을 포함한 다양한 검색 시나리오에서 성능을 최적화하기 위해 작업별 Low-Rank Adaptation (LoRA) 어댑터를 통합합니다. 종합적인 평가 결과, jina-embeddings-v4는 단일 모달 및 교차 모달 검색 작업 모두에서 최첨단 성능을 달성하며, 특히 테이블, 차트, 다이어그램 및 혼합 미디어 형식과 같이 시각적으로 풍부한 콘텐츠를 처리하는 데 강점을 보입니다. 이 기능의 평가를 용이하게 하기 위해 시각적으로 풍부한 이미지 검색을 위해 특별히 설계된 새로운 벤치마크인 Jina-VDR도 소개합니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">저희는 오픈 소스와 오픈 사이언스를 통해 인공 지능을 발전시키고 대중화하기 위한 여정에 있습니다.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>오늘 저희는 텍스트 및 이미지를 위한 새로운 38억 개의 파라미터 유니버설 向量模型 (Embeddings) 모델인 <code>jina-embeddings-v4</code>를 출시합니다. 여기에는 쿼리-문서 검색, 의미 매칭 및 코드 검색을 포함하여 가장 인기 있는 검색 작업에 대한 성능을 최적화하는 작업별 LoRA 어댑터 세트가 포함되어 있습니다. <code>jina-embeddings-v4</code>는 MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP 및 ViDoRe 벤치마크에서 멀티모달 및 다국어 작업에 대한 최첨단 검색 성능을 달성하며, 특히 테이블, 차트, 다이어그램 및 혼합된 콘텐츠와 같이 시각적으로 풍부한 콘텐츠를 처리하는 데 강점을 보입니다. 이 모델은 단일 벡터와 다중 벡터 向量模型 (Embeddings)을 모두 지원합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">시각적 문서 검색 및 멀티모달 벤치마크에서 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">의 성능. 상자 그림 분포는 6가지 벤치마크 범주에 걸쳐 向量模型 (Embeddings) 모델의 평균 점수 및 성능 변동성을 보여줍니다. ViDoRe (비전 문서 검색), Jina-VDR (종합적인 시각적 문서 검색), Wikimedia Commons 검색 (다국어 문서-설명 매칭), GitHub README 검색 (코드 문서 검색), Tweet Stock 검색 (재무 차트 분석) 및 CLIP 벤치마크 (일반 텍스트-이미지 검색)입니다. Jina-embeddings-v4 변형 (청록색으로 강조 표시됨)은 시각적으로 풍부한 문서 작업에서 최첨단 성능을 보여주며, 다중 벡터 버전은 특수 시각적 문서 벤치마크에서 가장 높은 점수 (ViDoRe에서 90.2, Jina-VDR에서 80.2)를 달성하고 일반 멀티모달 검색 작업에서 경쟁력 있는 성능 (CLIP 벤치마크에서 84.1)을 유지합니다. 모델은 각 벤치마크 범주 내에서 평균 성능별로 순위가 매겨지며, 개별 데이터 포인트는 여러 평가 작업에 걸쳐 점수 분포를 보여줍니다.</span></figcaption></figure><p><code>jina-embeddings-v4</code>는 저희의 가장 야심찬 向量模型 (Embeddings) 모델입니다. 오픈 소스 모델인 <code>jina-embeddings-v4</code>는 주요 제공업체의 선도적인 폐쇄 소스 向量模型 (Embeddings) 모델보다 뛰어난 성능을 제공하며, 다국어 검색에서 OpenAI의 <code>text-embedding-3-large</code>보다 12% 더 나은 성능 (66.49 vs 59.27), 긴 문서 작업에서 28% 향상 (67.11 vs 52.42), 코드 검색에서 <code>voyage-3</code>보다 15% 더 나은 성능 (71.59 vs 67.23)을 제공하고 Google의 <code>gemini-embedding-001</code> 성능과 일치합니다. 이를 통해 v4는 오늘날 사용 가능한 가장 강력한 오픈 소스 유니버설 向量模型 (Embeddings) 모델이 되어 연구원과 개발자에게 <a href=\"https://arxiv.org/abs/2506.18902\">종합적인 기술 보고서를 통해</a> 훈련 과정, 아키텍처 결정 및 모델 가중치에 대한 완전한 투명성을 갖춘 엔터프라이즈급 멀티모달 向量模型 (Embeddings) 기능을 제공합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">5가지 검색 벤치마크에서 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">의 성능. 차트는 텍스트 검색, 코드 검색, 다국어 검색, 긴 컨텍스트 검색 및 의미 텍스트 유사성 (STS) 벤치마크에서 각 모델의 평균 점수가 있는 상자 그림 분포를 보여줍니다. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (청록색으로 강조 표시됨)는 모든 평가 범주에서 경쟁력 있거나 최첨단 성능을 보여주며, 특히 텍스트 검색 및 STS에서 강력한 결과를 보입니다. 모델은 각 벤치마크 범주 내에서 평균 성능별로 순위가 매겨지며, 개별 데이터 포인트는 여러 평가 작업에 걸쳐 점수 분포를 보여줍니다.</span></figcaption></figure><h2 id=\"new-architecture\">새로운 아키텍처</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\"></span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">의 아키텍처. 이 모델은 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> 백본 (38억 개의 파라미터)을 기반으로 구축되었습니다. 텍스트 및 이미지 입력은 공유 경로를 통해 처리됩니다. 이미지는 먼저 비전 인코더를 통해 词元 (Tokens) 시퀀스로 변환된 다음, 두 모달리티 모두 컨텍스트 주의 계층이 있는 언어 모델 디코더에 의해 공동으로 처리됩니다. 세 개의 작업별 LoRA 어댑터 (각각 6천만 개의 파라미터)는 고정된 백본 가중치를 수정하지 않고 검색, 텍스트 매칭 및 코드 작업에 대한 특수 최적화를 제공합니다. 이 아키텍처는 이중 출력 모드를 지원합니다. (1) 효율적인 유사성 검색을 위해 평균 풀링을 통해 생성된 단일 벡터 向量模型 (Embeddings) (2048차원, 128로 절단 가능) 및 (2) 후기 상호 작용 검색 전략을 위해 프로젝션 계층을 통해 词元 (Tokens)당 128차원의 다중 벡터 向量模型 (Embeddings)입니다.</span></figcaption></figure><p><code>jina-embeddings-v3</code>에서 업그레이드된<code>jina-embeddings-v4</code>는 텍스트 전용에서 멀티모달 向量模型 (Embeddings)으로의 패러다임 전환을 나타냅니다. v3가 작업별 LoRA 어댑터로 텍스트 向量模型 (Embeddings) 최적화에 중점을 둔 반면, v4는 통합된 표현으로 텍스트 및 시각적 콘텐츠를 모두 포함하는 증가하는 요구 사항을 해결합니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>측면</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>백본 모델</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>매개변수 (기본)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>매개변수 (어댑터 포함)</td>\n<td>572M</td>\n<td>3.8B + 어댑터당 60M</td>\n</tr>\n<tr>\n<td>모달리티</td>\n<td>텍스트 전용</td>\n<td>텍스트 + 이미지 (멀티모달)</td>\n</tr>\n<tr>\n<td>최대 입력 길이</td>\n<td>8,192 Tokens (Tokens)</td>\n<td>32,768 Tokens (Tokens)</td>\n</tr>\n<tr>\n<td>이미지 처리</td>\n<td>없음</td>\n<td>최대 2천만 화소, 시각적으로 풍부한 문서</td>\n</tr>\n  <tr>\n<td>다국어 지원</td>\n<td>89개 언어</td>\n<td>29+개 언어</td>\n</tr>\n<tr>\n<td>벡터 유형</td>\n<td>단일 벡터만</td>\n<td>단일 벡터 + 다중 벡터 (지연 상호 작용)</td>\n</tr>\n<tr>\n<td>단일 벡터 차원</td>\n<td>1024 (MRL로 32까지 절단 가능)</td>\n<td>2048 (MRL로 128까지 절단 가능)</td>\n</tr>\n<tr>\n<td>다중 벡터 차원</td>\n<td>사용 불가</td>\n<td>토큰당 128</td>\n</tr>\n<tr>\n<td>작업 LoRA 특화</td>\n<td>• 비대칭 검색<br>• 의미론적 유사성<br>• 분류<br>• 분리</td>\n<td>• 비대칭 검색<br>• 의미론적 유사성<br>• 코드 검색</td>\n</tr>\n<tr>\n<td>훈련 단계</td>\n<td>3단계: 사전 훈련 → 向量模型 (Embeddings) 미세 조정 → 어댑터 훈련</td>\n<td>2단계: 결합 쌍 훈련 → 작업별 어댑터 훈련</td>\n</tr>\n<tr>\n<td>손실 함수</td>\n<td>InfoNCE, CoSent, 확장된 삼중 손실</td>\n<td>단일/다중 벡터에 대한 결합 InfoNCE + KL 발산</td>\n</tr>\n<tr>\n<td>위치 인코딩</td>\n<td>RoPE (회전 기준 주파수 조정)</td>\n<td>M-RoPE (멀티모달 회전 위치 向量模型 (Embeddings))</td>\n</tr>\n<tr>\n<td>교차 모달 처리</td>\n<td>해당 없음</td>\n<td>통합 인코더 (모달리티 격차 감소)</td>\n</tr>\n<tr>\n<td>MRL 지원</td>\n<td>예</td>\n<td>예</td>\n</tr>\n<tr>\n<td>어텐션 구현</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">백본</h3><p>v4에서 가장 중요한 아키텍처 변경 사항은 백본이 <code>XLM-RoBERTa</code>에서 <code>Qwen2.5-VL-3B-Instruct</code>로 변경된 것입니다. 이 결정은 이미지를 토큰 시퀀스로 변환하고 텍스트와 함께 처리하여 이중 인코더 아키텍처에 존재하는 <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">모달리티 격차</a>를 제거하는 \"진정한 멀티모달 처리\"를 가능하게 하는 보편적인 向量模型 (Embeddings) 모델을 만들려는 v4의 핵심 목표에 의해 주도되었습니다.</p><p>백본 선택은 몇 가지 주요 설계 목표와 일치합니다. Qwen2.5-VL의 문서 이해 능력은 테이블, 차트 및 스크린샷과 같은 시각적으로 풍부한 콘텐츠를 처리하는 v4의 강점을 직접적으로 지원합니다. 동적 해상도 기능을 통해 v4는 아키텍처에 지정된 대로 최대 2천만 화소로 크기가 조정된 이미지를 처리할 수 있습니다. 고급 위치 인코딩은 v4가 OpenAI CLIP의 0.15에 비해 0.71의 정렬 점수로 우수한 교차 모달 정렬을 달성할 수 있도록 하는 기반을 제공합니다.</p><h3 id=\"lora-adapters\">LoRA 어댑터</h3><p>v4는 효과 및 사용자 채택에 대한 학습된 교훈을 반영하여 v3의 5가지 작업에서 3가지 집중된 작업으로 간소화합니다.</p><ul><li><strong>비대칭 검색</strong> (v3의 쿼리/통과 어댑터 통합)</li><li><strong>대칭 유사성</strong> (STS 작업에 대한 v3의 텍스트 일치와 동일)</li><li><strong>코드 검색</strong> (v2-code에서 학습, v3에서 누락)</li></ul><p>이 통합은 v3의 분류 및 분리 어댑터를 제거하고 가장 영향력 있는 向量模型 (Embeddings) 사용 사례인 검색 및 STS에 v4를 집중시킵니다.</p><h3 id=\"output-embeddings\">출력 向量模型 (Embeddings)</h3><p>v4는 단일 벡터 및 다중 벡터 向量模型 (Embeddings)을 모두 지원하는 이중 출력 시스템을 도입하는 반면, v3는 단일 벡터 출력만 제공했습니다. 이는 다양한 검색 시나리오를 해결합니다.</p><ul><li><strong>단일 벡터 모드</strong>: 효율적인 유사성 검색을 위한 2048차원 向量模型 (Embeddings) (MRL을 통해 128까지 절단 가능)</li><li><strong>다중 벡터 모드</strong>: <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">지연 상호 작용 검색</a>을 위한 토큰당 128차원</li></ul><p>이 이중 접근 방식은 특히 시각적으로 풍부한 문서 검색에서 다중 벡터 표현으로 더 큰 효과를 제공하는 동시에 표준 유사성 작업에 대한 효율성을 유지합니다. 시각적 작업에서 단일 벡터 모드보다 다중 벡터의 일관된 7-10% 성능 이점은 지연 상호 작용이 멀티모달 콘텐츠에 대한 근본적으로 더 나은 의미론적 일치를 제공함을 시사합니다.</p><h3 id=\"parameter-size\">매개변수 크기</h3><p>v4는 v3보다 6.7배 더 크지만 (3.8B 대 570M 매개변수), 텍스트 전용 성능 개선은 실제로 미미하며, 매개변수 확장이 주로 텍스트 향상보다는 멀티모달 요구 사항에 의해 주도되었음을 시사합니다. 핵심 텍스트 벤치마크에서 v4는 MMTEB에서 66.49를 달성한 반면 v3는 58.58 (14% 개선)이고 MTEB-EN에서는 55.97을 달성한 반면 v3는 54.33 (3% 개선)입니다. 코드 검색의 경우 v4는 CoIR에서 71.59를 기록한 반면 v3는 55.07 (30% 개선)이고, 긴 문서 성능은 LongEmbed에서 v4가 67.11인 반면 v3는 55.66 (21% 개선)입니다. v4의 멀티모달 기능을 고려할 때 상당한 확장이 정당화됩니다. 시각적 문서 검색 (Jina-VDR)에서 84.11 nDCG@5 및 ViDoRe 벤치마크에서 90.17을 달성합니다. 이는 v3에는 완전히 없는 기능입니다. 따라서 매개변수 증가는 경쟁력 있는 텍스트 성능을 유지하면서 멀티모달 기능에 대한 투자를 나타내며, 통합 아키텍처는 기존 이중 인코더 접근 방식에 비해 0.15에 비해 0.71 교차 모달 정렬을 달성하면서 별도의 텍스트 및 비전 모델의 필요성을 제거합니다.</p><h2 id=\"getting-started\">시작하기</h2><p>빠른 분위기 확인을 위해 Search Foundation 툴박스에서 텍스트-이미지 데모를 사용해 보십시오. 당사 웹사이트의 문서 이미지 컬렉션을 준비했으며 사용자 지정 이미지 URL을 추가할 수도 있습니다. 쿼리를 입력하고 Enter 키를 눌러 순위가 매겨진 결과를 확인하십시오. OCR 또는 콘텐츠 기반 이미지 검색과 같이 다시 검색할 수 있으며, 영어가 아닌 쿼리를 사용해 볼 수도 있습니다.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">데모는 다음에서 사용할 수 있습니다. </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> 이 데모를 사용하면 기본 API 키의 토큰이 소모됩니다. 또한 서버에서 해당 URL의 모든 이미지를 다운로드해야 하므로 데모가 약간 느리게 보일 수 있으며 이미지에 대한 캐시가 구현되지 않았습니다.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">API를 통해</h3><p>아래 코드는 <code>jina-embeddings-v4</code>를 사용하는 방법을 보여줍니다. 텍스트 문자열, base64로 인코딩된 이미지 또는 이미지 URL을 전달할 수 있습니다. 신규 사용자는 1,000만 개의 무료 Tokens (Tokens)으로 Jina API 키를 얻을 수 있습니다.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>제한된 GPU 리소스 때문에 현재 Embedding API는 <code>jina-embeddings-v4</code>가 최대 32K개의 词元 (Tokens)을 처리할 수 있는 기본 기능에도 불구하고 최대 8K 词元 (Tokens) 길이의 문서만 지원합니다. 8K 词元 (Tokens)을 초과하는 더 긴 컨텍스트를 요구하는 애플리케이션 (예: <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>)의 경우, CSP를 통해 모델을 배포하거나 모델을 자체 호스팅하는 것이 좋습니다.</p><h3 id=\"via-csp-marketplaces\">CSP 마켓플레이스를 통해</h3><p><code>jina-embeddings-v4</code>는 AWS, Azure 및 GCP에서 곧 직접 사용할 수 있으며, 가격은 해당 플랫폼에 게시될 예정입니다.</p><h3 id=\"via-huggingface\">HuggingFace를 통해</h3><p>연구 및 실험 목적으로 Hugging Face 페이지에서 로컬로 모델을 사용할 수 있습니다. 작동 방식을 보여주는 Google Colab 노트북을 준비했습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">결론</h2><p><code>jina-embeddings-v4</code>는 현재까지 가장 큰 도약이라고 할 수 있습니다. 38억 개의 파라미터를 가진 범용 向量模型 (Embedding) 모델로, 텍스트와 이미지를 통합된 경로를 통해 처리하여 밀집 검색 및 지연 상호 작용 검색을 모두 지원하며, 특히 시각적으로 풍부한 문서 검색에서 Google, OpenAI 및 Voyage AI의 독점 모델보다 성능이 뛰어납니다. 그러나 이러한 기능은 고립적으로 나타난 것이 아니라 근본적인 한계를 해결해 온 4세대의 결과입니다.</p><p>2022년 초에 <code>jina-embeddings-v1</code>으로 시작했을 때, 모든 사람이 더 많은 데이터가 더 나은 성능을 의미한다고 가정했습니다. 우리는 그 반대를 증명했습니다. 즉, 15억 쌍을 필터링하여 3억 8,500만 개의 고품질 예제로 줄이는 것이 훨씬 더 큰 데이터 세트보다 성능이 뛰어났습니다. 교훈은 큐레이션이 수집보다 낫다는 것입니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model’s awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>그러나 사용자는 계속해서 BERT의 512 词元 (Tokens) 제한에 부딪혔습니다. 더 긴 시퀀스에서 훈련하는 것은 비용이 많이 드는 것처럼 보였지만, <code>jina-embeddings-v2</code>는 훈련은 짧게, 배포는 길게 하는 우아한 솔루션을 제시했습니다. ALiBi의 선형 어텐션 바이어스는 512 词元 (Tokens)에서 훈련된 모델이 추론 시 8,192 词元 (Tokens)을 원활하게 처리할 수 있도록 합니다. 더 적은 컴퓨팅으로 더 많은 기능을 얻었습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI’s proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><code>jina-embeddings-v2</code>의 성공은 또 다른 제약 조건을 드러냈습니다. 즉, 서로 다른 작업에는 서로 다른 최적화가 필요했습니다. 별도의 모델을 구축하는 대신 <code>jina-embeddings-v3</code>는 모든 작업에 대해 5억 7천만 개의 기본 모델을 사용자 정의하기 위해 작은 6천만 개의 LoRA 어댑터를 사용했습니다. 하나의 모델이 5개의 전문 모델이 되었습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>작업 전문화에도 불구하고 우리는 텍스트 전용으로 남아 있었지만 사용자는 시각적 이해가 필요했습니다. <code>jina-clip-v1</code> 및 <code>jina-clip-v2</code>와 같은 표준 CLIP 기반 모델은 별도의 인코더를 사용하여 서로 다른 형식의 유사한 콘텐츠가 멀리 떨어져 있는 \"양식 간 격차\"를 만듭니다. 최근에 출시된 <code>jina-reranker-m0</code>처럼 <code>jina-embeddings-v4</code>는 이 문제를 완전히 제거했습니다. 즉, 하나의 통합된 경로가 모든 것을 처리하여 격차를 해소하는 대신 메웁니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><code>jina-embeddings-v4</code>와 <code>jina-reranker-m0</code>는 모두 근본적인 변화를 공유합니다. 즉, 인코더 전용 모델 대신 大模型 (LLM)을 백본으로 사용합니다. 이것은 우연이 아닙니다. 대부분 놓치는 깊은 장점을 반영합니다. 인코더 전용 모델은 이미지가 텍스트와 별도로 클러스터링되는 \"양식 간 격차\"를 만듭니다. 디코더 전용 모델은 진정한 혼합 양식 표현 및 설명 가능성을 포함하여 인코더 전용 아키텍처로는 달성할 수 없었던 가능성을 열어줍니다.</p><p>핵심 통찰력: 向量模型 (embeddings)과 생성은 모두 의미론적 이해와 관련이 있습니다. 생성을 잘하는 大模型 (LLM)은 당연히 표현 능력도 뛰어납니다. 미래는 <strong>동일한 검색 기반 모델</strong>에서 向量模型 (embedding)과 重排器 (reranking)가 나타나는 통합 아키텍처에 있다고 생각하며, Jina AI는 바로 그 방향으로 나아가고 있습니다.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina 向量 모델 (Embeddings) v4는 38억 개의 파라미터를 가진 다중 모드 및 다국어 검색을 위한 범용 向量 모델 (embedding model)로, 단일 벡터 및 다중 벡터 向量 모델 (embedding) 출력을 모두 지원합니다.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
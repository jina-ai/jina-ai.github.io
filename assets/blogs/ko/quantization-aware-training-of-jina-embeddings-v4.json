{
  "slug": "quantization-aware-training-of-jina-embeddings-v4",
  "id": "685d4b76f1bef30001fc5449",
  "uuid": "6b06b483-2d13-4f1d-8d9d-147fa6dffe4b",
  "title": "jina-embeddings-v4의 양자화 인식 훈련",
  "html": "양자화는 AI에서 확장 문제를 해결하기 위해 널리 사용됩니다. 이름 때문에 복잡하게 들리지만, 숫자를 반올림하여 차지하는 공간을 줄이는 것뿐입니다. 이는 더 작은 메모리와 저장 공간을 차지하는 더 작은 임베딩 벡터 (Embeddings)를 의미하며, 벡터를 비교하는 데 걸리는 시간이 줄어들어 정보 검색 속도가 빨라집니다. 양자화는 모델이 처리하는 데이터의 종류나 사용 사례에 상관없이 순전히 수치적인 기술이므로, 많은 비용이 드는 도메인 지식 없이도 개선을 가져올 수 있습니다.\n\n양자화는 정밀도를 희생해야 하는 오래된 트레이드오프와 공짜는 없다는 클리셰와 관련이 있을 것이라고 예상할 수 있습니다. 이 글에서는 *양자화 인식 훈련* (QAT, quantization-aware training)을 통해 손실 없이 만드는 방법을 보여드리겠습니다. 이 기술은 공간이 중요한 애플리케이션에서 필요한 더 작은 임베딩을 제공하기 위해 <code>jina-embeddings-v4</code>에서 사용됩니다.\n\n<h2 id=\"overview-of-quantization-techniques\">양자화 기술 개요</h2>\n\n모델 양자화는 일반적으로 다음 네 가지 중 하나를 의미합니다.\n\n<ul>\n<li>사후 훈련 양자화 (PTQ, Post-training quantization)</li>\n<li>양자화된 임베딩 출력을 위한 훈련 (Output QAT)</li>\n<li>완전 양자화된 모델을 위한 훈련 (Full QAT)</li>\n<li>기존의 양자화되지 않은 모델에서 새로운 양자화된 모델을 증류</li>\n</ul>\n\n사후 훈련 양자화 (PTQ, Post-training quantization)는 훈련된 임베딩 모델을 있는 그대로 받아들이고 어떤 방식으로든 수정하지 않습니다. 모델에서 생성된 부동 소수점 값의 가장 중요하지 않은 자릿수를 버리는 문제입니다. 숫자를 반올림하고 때로는 범위를 조정합니다.\n\n출력 QAT는 최적의 감소된 정밀도 벡터를 생성하도록 임베딩 모델을 미세 조정하는 것을 의미합니다. 이는 모델을 수정하는 것을 의미하지만 모델 가중치의 정밀도를 변경하지 않으므로 크기가 줄어들지 않습니다. 출력 벡터 크기만 줄어듭니다.\n\nFull QAT는 완전히 훈련된 완전 정밀도 모델로 시작하여 모델 가중치의 정밀도를 낮춘 다음 수정된 모델의 성능을 미세 조정합니다. 이는 미세 조정 작업을 수행하는 대가로 더 작은 임베딩뿐만 아니라 훨씬 더 작은 모델을 생성합니다.\n\n증류 (Distillation)는 기존 모델의 성능에 맞게 새로운 모델을 훈련하는 프로세스입니다. 이는 양자화된 모델로 처음부터 설계된 새로운 모델을 만들고 기존 모델을 사용하여 기존 모델에 최대한 가깝게 수행될 때까지 훈련하는 데 필요한 만큼의 훈련 데이터를 생성하는 것을 의미합니다.\n\n이러한 네 가지 접근 방식의 이점은 아래 표에 요약되어 있습니다.\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>접근 방식</th>\n<th>더 작은 임베딩?</th>\n<th>훈련 필요?</th>\n<th>모델 압축?</th>\n<th>더 빠른 추론?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>PTQ</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Output QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Full QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td><strong>Distillation</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><em>(더 작은 모델로)</em></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n\n네 가지 모두 더 작은 임베딩을 생성하지만 PTQ를 제외하고는 모두 추가 훈련이 필요하며 Full QAT와 Distillation만이 더 빠르고 새로운 모델을 생성합니다. Full QAT와 Distillation은 Output QAT보다 훨씬 더 많은 훈련이 필요하기 때문에 구현하는 데 훨씬 더 많은 비용이 듭니다.\n\n이 글에서는 임베딩 모델의 크기나 속도를 변경하지 않는 PTQ와 Output QAT만 살펴보겠습니다.\n\n<h2 id=\"experimental-setup\">실험 설정</h2>\n\n이러한 실험에서 기준 모델은 2048차원에서 32비트 정밀도 부동 소수점 (FP32) 벡터를 생성하는 검색 어댑터가 있는 <code>jina-embeddings-v4</code>입니다. 따라서 각 임베딩의 크기는 8196바이트 또는 8kB입니다.\n\n<a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR 벤치마크</a> 스위트의 쿼리-문서 검색 벤치마크 작업을 사용하여 여러 실험 조건을 연구했습니다. 검색 프로세스는 벡터 간의 코사인 유사도를 사용하여 쿼리와 가장 일치하는 문서를 찾고 순위를 매깁니다.\n\n<ul>\n<li><strong>기준</strong> — 양자화 없이 <code>jina-embeddings-v4</code> 임베딩 벡터의 성능. 이러한 실험은 모두 모델의 베타 버전을 사용했으며 릴리스 성능이 다소 좋습니다.</li>\n<li><strong>PTQ</strong> — 모델을 변경하지 않고 출력 벡터를 이진 벡터로 양자화했습니다.</li>\n<li><strong>Output QAT</strong> — 양자화된 조건에서 성능을 향상시키기 위해 출력 벡터를 양자화하고 검색 어댑터에 미세 조정을 적용했습니다.</li>\n</ul>\n\n<h3 id=\"quantization-levels\">양자화 수준</h3>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"816\" height=\"636\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png 816w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 1: 사후 양자화 임베딩 크기 비교.</span></figcaption></figure>\n\n네 가지 다른 수준의 양자화를 실험했습니다.\n\n<ul>\n<li><strong>8비트 정수</strong> — FP32 값은 -128에서 127 범위의 정수로 줄어들어 임베딩이 4배 줄어들어 <strong>2048바이트</strong>가 됩니다.</li>\n<li><strong>4비트 정수</strong> - 4비트 정수와 동일하지만 -8에서 7 범위로 매핑하여 벡터 크기를 8배 줄여 <strong>1024바이트</strong>로 줄입니다.</li>\n<li><strong>삼항 양자화 —</strong> 모든 값은 -1, 0, 1의 세 값 중 하나로 매핑됩니다. 최적으로 저장하면 각 차원이 1.6비트로 줄어들어 임베딩 벡터의 크기가 약 40배 줄어들어 약 <strong>230바이트</strong>가 됩니다.</li>\n<li><strong>이진 양자화</strong> — <code>torch.sign</code> 데이터 유형을 사용하여 FP32 스칼라 값을 1비트로 변환합니다. 이 데이터 유형은 저장하는 데 1비트가 걸리는 두 개의 값만 제공합니다. 이렇게 하면 2048차원 임베딩 벡터가 8192바이트에서 <strong>128바이트</strong>로 64배 줄어듭니다.</li>\n</ul>\n\n<h3 id=\"scaling\">스케일링</h3>\n\n이진 양자화의 경우 양자화는 매우 간단합니다. 벡터 값이 0보다 크거나 양수이면 1로 매핑됩니다. 그렇지 않으면 -1로 매핑됩니다.\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1159\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png 1159w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 2: 이진 양자화. 모든 음수 값은 -1이 되고 다른 모든 값은 1이 됩니다.</span></figcaption></figure>\n\n다른 양자화 시나리오의 경우 값을 범위로 정규화한 다음 양자화 수준에서 허용하는 가장 가까운 값으로 반올림했습니다. 임베딩 벡터는 -∞와 +∞ (또는 실제로는 매우 큰 양수 및 음수) 사이의 스케일 숫자로 구성됩니다. 두 개의 숫자 $max$와 $min$을 사용하여 양자화할 값을 스케일링합니다.\n\n삼항 양자화의 경우 각 벡터 구성 요소 $v$를 다음과 같이 변환합니다.\n\n<ul>\n<li>$v$ ≥ $max$이면 $v$는 1이 됩니다.</li>\n<li>$v$ ≤ $min$이면 $v$는 -1이 됩니다.</li>\n<li>$min$ &lt; $v$ &lt; $max$이면 $v$는 0이 됩니다.</li>\n</ul>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"220\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png 1030w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 3: 삼항 양자화. 간격이 정의되고 그 안의 값이 0이 됩니다. 모든 낮은 값은 -1이 되고 모든 높은 값은 1이 됩니다.</span></figcaption></figure>\n\n4비트 정수의 경우:\n\n<ul>\n<li>$v$ ≥ $max$이면 $v$는 7이 됩니다.</li>\n<li>$v$ ≤ $min$이면 $v$는 -8이 됩니다.</li>\n<li>$min$ &lt; $v$ &lt; $max$이면 $v$는 $16*(v - min)/(max - min) - 8$이 되고 가장 가까운 정수로 반올림됩니다. 이렇게 하면 값이 $[-8,7]$ 범위로 스케일링됩니다.</li>\n</ul>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 4: 4비트 양자화. 간격이 정의되고 모든 값이 정의된 범위 [-8,7]로 정규화됩니다.</span></figcaption></figure>\n\n8비트 정수의 경우:\n\n<ul>\n<li>$v$ ≥ $max$이면 $v$는 127이 됩니다.</li>\n<li>$v$ ≤ $min$이면 $v$는 -128이 됩니다.</li>\n<li>$min$ &lt; $v$ &lt; $max$이면 $v$는 $256*(v - min)/(max - min) - 128$이 되고 가장 가까운 정수로 반올림됩니다. 이렇게 하면 값이 $[-128,127]$ 범위로 스케일링됩니다.</li>\n</ul>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"219\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 5: 8비트 양자화. 간격이 정의되고 모든 값이 정의된 범위 [-128,127]로 정규화됩니다.</span></figcaption></figure>\n\n$max$와 $min$을 계산하기 위해 두 가지 접근 방식을 사용했습니다.\n\n<ul>\n<li><strong>최소/최대</strong> — 데이터를 일괄 처리하고 각 일괄 처리마다 가장 높은 벡터 구성 요소와 가장 낮은 벡터 구성 요소를 식별하여 $max$를 가장 높게, $min$을 가장 낮게 설정했습니다.</li>\n<li><strong>일괄 처리에서 롤링 평균</strong> — 각 일괄 처리마다 벡터 구성 요소의 평균과 표준 편차를 계산했습니다. 모든 일괄 처리를 처리하면서 평균과 표준 편차의 이동 평균을 유지했습니다. $avg$가 일괄 처리 평균 값의 현재 이동 평균이고 $std$가 표준 편차의 현재 이동 평균인 경우 각 일괄 처리마다:</li>\n</ul>\n\n$max = avg + std$<br>$min = avg - std$\n\n<h3 id=\"qat-fine-tuning\">QAT 미세 조정</h3>\n\nPTQ 실험의 경우 모델을 있는 그대로 사용하고 위에서 설명한 방법을 사용하여 생성된 임베딩을 양자화했습니다.\n\nOutput QAT의 경우 *straight-through estimation*을 사용하여 모델을 미세 조정했습니다. 이는 손실 (즉, 오류)을 계산하기 전에 양자화 프로세스를 되돌려 값의 전체 정밀도를 복원한 다음 해당 손실 메트릭을 사용하여 모델을 미세 조정하는 것을 의미합니다.<p>각각의 경우 10,000단계를 거쳐 미세 조정했으며, 500단계마다 체크포인트를 저장했습니다. 그런 다음 <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR</a> 벤치마크에서 가장 높은 점수를 받은 체크포인트를 유지했습니다.</p><h3 id=\"asymmetric-quantization\">비대칭 양자화</h3><p>PTQ와 Output QAT는 임베딩 벡터의 크기를 줄이지만, 모델 크기나 추론 속도를 줄이지는 않습니다. 모든 절감 효과는 저장된 문서 임베딩의 크기와 검색 속도에 있습니다.</p><p>결과적으로 쿼리 벡터를 양자화하거나 검색 시 양자화되지 않은 상태로 두는 것을 모두 테스트했습니다. 어느 쪽이든 저장된 임베딩 벡터의 크기를 변경하지 않기 때문입니다.</p><h2 id=\"results\">결과</h2><p>총 9가지 조건을 테스트했으며, 아래 표에 요약되어 있습니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>조건 이름</th>\n<th>미세 조정</th>\n<th>양자화 수준</th>\n<th>스케일링 전략</th>\n<th>양자화된 쿼리</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>기준선</td>\n<td>❌</td>\n<td>해당 없음</td>\n<td>해당 없음</td>\n<td>해당 없음</td>\n</tr>\n<tr>\n<td>PTQ Both</td>\n<td>❌</td>\n<td>이진</td>\n<td>해당 없음</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>PTQ Docs Only</td>\n<td>❌</td>\n<td>이진</td>\n<td>해당 없음</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td><strong>✓</strong></td>\n<td>이진</td>\n<td>해당 없음</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td><strong>✓</strong></td>\n<td>이진</td>\n<td>해당 없음</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td><strong>✓</strong></td>\n<td>삼진</td>\n<td>롤링 평균</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td><strong>✓</strong></td>\n<td>4비트</td>\n<td>롤링 평균</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td><strong>✓</strong></td>\n<td>8비트</td>\n<td>롤링 평균</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td><strong>✓</strong></td>\n<td>8비트</td>\n<td>최소/최대</td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>표 2: 실험 조건</em></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>조건 이름</th>\n<th>평균 점수</th>\n<th>기준선과의 차이</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>기준선</td>\n<td>60.10</td>\n<td>해당 없음</td>\n</tr>\n<tr>\n<td>PTQ Binary</td>\n<td>58.33</td>\n<td>-1.78</td>\n</tr>\n<tr>\n<td>PTQ Binary Docs Only</td>\n<td>59.08</td>\n<td>-1.02</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td>59.22</td>\n<td>-0.89</td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td>60.81</td>\n<td>+0.70</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td>59.49</td>\n<td>-0.62</td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td>61.73</td>\n<td>+1.62</td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td>61.67</td>\n<td>+1.56</td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td>61.29</td>\n<td>+1.19</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>표 3: 12개의 NanoBEIR 벤치마크에 대한 각 조건의 평균 점수(정확도 %)</em></p><p>위 표에서 양자화를 위한 미세 조정이 점수를 향상시킨다는 것을 알 수 있습니다. <strong>PTQ Binary</strong> 조건과 <strong>QAT Binary</strong> 조건의 유일한 차이점은 미세 조정이며, 점수 차이는 상당합니다. 마찬가지로, 동일한 미세 조정으로만 구분되는 <strong>PTQ Binary Docs Only</strong> 조건과 <strong>QAT Binary Docs Only</strong> 조건 간에 거의 2%의 점수 향상을 확인할 수 있습니다.</p><p>놀랍지 않게도 양자화 정도가 적을수록 점수가 일반적으로 향상되는 것을 알 수 있습니다. 4비트 양자화가 삼진보다, 삼진이 이진보다 더 나은 점수를 얻습니다. 그러나 8비트로 더 나아가는 것은 아무것도 개선하지 못한 것 같습니다.</p><p>이진 경우에만 쿼리를 양자화하지 않은 상태로 두는 것을 테스트했지만, 이는 성능을 향상시키는 것으로 보입니다.</p><p>마지막으로, 테스트 결과 롤링 평균 스케일링 방법이 단순한 최소/최대 접근 방식보다 성능이 우수함을 시사합니다.</p><h2 id=\"conclusion\">결론</h2><p>양자화는 임베딩 모델에 대한 몇 가지 중요한 운영상의 이점을 제공합니다. 임베딩 벡터의 크기를 크게 줄이고 정보 검색 속도를 높이기 때문입니다. 간단한 사후 훈련 양자화 (PTQ, Post-Training Quantization)는 메모리 및 스토리지 측면에서 즉각적인 이점을 제공하지만, 실험 결과 양자화 인식 훈련 (QAT, Quantization-Aware Training)이 불가피한 정밀도 손실을 크게 완화하는 것으로 나타났습니다. 미세 조정은 지속적으로 더 나은 점수를 산출했습니다.</p><p>양자화 정도는 성능에 직접적인 영향을 미치며, 이는 값의 정밀도를 낮추는 데 기반한 방법에서 예상할 수 있는 결과입니다. 덜 적극적인 양자화 (예: 4비트)는 일반적으로 더 적극적인 방법 (예: 이진)보다 성능이 우수하지만, 놀랍게도 8비트와 4비트 양자화 간에는 성능 차이가 크지 않았습니다. 일부 부정확성 임계값에 도달할 때까지는 양자화 정도에 따른 차이가 거의 없는 것으로 보입니다.</p><p>스케일링 전략도 중요하며, 롤링 평균 방법이 고정된 최소/최대 접근 방식보다 우수한 결과를 보였습니다. 데이터와 관련된 스케일링 값을 사용하는 것이 훨씬 더 효과적인 것으로 보이며 추가 탐구가 필요합니다.</p><p>양자화를 통해 더 적은 비용으로 임베딩 모델을 더 많이 활용할 수 있습니다. 이 기사에서는 양자화에 대한 모든 옵션을 탐색하지는 않지만, 쉽게 접근할 수 있는 두 가지 옵션을 탐색하고 있으며, 이들은 실제로 제공할 수 있는 이점이 있습니다. 사용자 비용을 더욱 절감할 수 있도록 양자화 전략을 개선하고 개선하기 위해 노력하고 있으며, 가까운 시일 내에 <code>jina-embeddings-v4</code>에 대한 이진 지원을 릴리스할 예정입니다.</p>",
  "comment_id": "685d4b76f1bef30001fc5449",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading---2025-06-30T114820.483.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-06-26T15:30:30.000+02:00",
  "updated_at": "2025-06-30T21:14:36.000+02:00",
  "published_at": "2025-06-30T21:14:36.000+02:00",
  "custom_excerpt": "Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/quantization-aware-training-of-jina-embeddings-v4/",
  "excerpt": "양자화는 더 작은 向量模型 (Embeddings)을 제공합니다. 미세 조정된 양자화가 손실 없는 向量模型 (Embeddings)을 제공한다는 것을 보여드리겠습니다.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
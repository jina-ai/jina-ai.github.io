{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "ModernBERT로부터 우리는 무엇을 배워야 하는가?",
  "html": "<p>2018년, Google이 BERT를 발표했을 때 이는 현재의 LLM 물결이 오기 훨씬 전부터 NLP 분야의 게임 체인저였습니다. 지금도 많은 Small Language Model들이 BERT를 기반으로 구축되어 있습니다. 2024년 12월, <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a>는 최근 LLM 개발에서 배운 것들을 이러한 작은 모델들에 적용했습니다. 주요 변화는? 더 나은 매개변수 효율성, 코드 이해 및 긴 문맥 처리입니다.</p><p>이 포스트에서는 우리가 잘 알고 있는 두 모델과 ModernBERT를 비교해보겠습니다: <code>jina-XLM-RoBERTa</code>(<code>jina-embeddings-v3</code>의 다국어 백본)와 <code>RoBERTa-large</code>입니다. 각 모델을 살펴보겠습니다:</p><ul><li><strong>ModernBERT</strong>(2024년 12월)는 최근 출시된 SLM으로, Answer.AI, LightOn, HuggingFace가 공동으로 개발했습니다. 8,192 토큰 문맥 윈도우를 위한 RoPE와 <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">GeGLU 레이어</a>와 같은 현대적 최적화를 활용하여 효율성을 유지하면서 성능을 향상시켰습니다.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong>(2024년 9월)는 Meta의 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a>를 기반으로 한 다국어 텍스트 임베딩 모델입니다. 원래 <code>XLM-RoBERTa</code>가 XLM 대규모 다국어 데이터셋을 사용하여 <code>RoBERTa</code>를 개선한 반면, <code>jina-XLM-RoBERTa</code>는 확장된 문맥 학습, <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> 구현, <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a> 지원으로 한 걸음 더 나아갔습니다. 이 모델은 <code>jina-embeddings-v3</code>의 백본으로 사용됩니다.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a>(2019년 7월)는 Meta가 개발한 BERT의 개선 버전으로 3억 5500만 개의 매개변수를 가지고 있습니다. 확장된 학습, 더 큰 데이터셋, 동적 마스킹과 같은 혁신을 통해 <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a>, <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a>를 포함한 주요 벤치마크에서 인상적인 결과를 달성했습니다. 이는 텍스트 분류부터 질문 답변까지 다양한 NLP 작업에 적합합니다.</li></ul><p>이 모델들을 세 가지 핵심 측면에서 비교함으로써, 모델 개발자들을 위한 ModernBERT의 효과적인 설계 선택을 강조하고 향후 BERT 유사 모델을 위한 핵심 개발 통찰을 식별하고자 합니다. 또한 <code>jina-embeddings-v3</code> 개발에서 얻은 교훈을 공유하고 <code>jina-embeddings-v4</code>와 <code>jina-reranker-v3</code>를 위한 계획된 개선 사항을 논의하겠습니다.</p><h2 id=\"modernberts-parameter-efficiency\">ModernBERT의 매개변수 효율성</h2><p>먼저 ModernBERT의 매개변수 효율성 접근 방식을 살펴보겠습니다 - 이는 최근 LLM 개발에서 얻은 여러 핵심 통찰을 가져왔습니다. ModernBERT는 깊지만 얇은 아키텍처, 제어된 어휘 크기, 작은 모델에서 시작하는 점진적 모델 업스케일링이라는 세 가지 핵심 전략을 활용합니다.</p><h3 id=\"deep-and-thin-architecture\">Deep-And-Thin 아키텍처</h3><p>ModernBERT-large는 28개 레이어로 더 깊어진 반면, <code>jina-XLM-RoBERTa</code>와 <code>RoBERTa-large</code>는 24개로 운영됩니다. 흥미로운 점은 추가 레이어에도 불구하고 <code>RoBERTa-large</code>와 매개변수 수가 비슷하다는 것입니다. <code>jina-XLM-RoBERTa</code>는 89개 언어를 처리해야 하기 때문에 더 많은 매개변수가 필요한 반면, 다른 두 모델은 영어에만 집중합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">작은 LLM에서는 너비(hidden unit 수)보다 깊이(레이어 수)가 더 중요합니다. deep-and-thin 모델 구조는 추상적 개념을 포착하는데 뛰어나 우수한 최종 성능을 보입니다.</span></figcaption></figure><p>트랜스포머의 매개변수 대부분은 어텐션과 완전연결 레이어에서 옵니다. ModernBERT는 \"얇게\" 가면서도 경쟁력을 유지합니다 - RoBERTa-large가 24개 레이어에 걸쳐 4,096 hidden unit을 사용하는 것에 비해 28개 레이어에 걸쳐 2,624 hidden unit을 사용합니다. 이 \"더 깊지만\" 얇은 구성으로 모델을 비대화하지 않고도 성능 목표를 달성할 수 있습니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parameters</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Hidden states</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Intermediate dims</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Attention heads</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Vocabulary size</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>이 접근 방식은 Meta의 <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> 연구와 일치하는데, 이 연구는 작은 모델의 경우 복잡한 패턴을 포착하고 성능을 향상시키는 데 있어 너비보다 깊이가 더 중요하다는 것을 발견했습니다. 본질적으로, 병렬 처리를 위해 더 넓은 레이어를 갖는 것보다 더 많은 트랜스포머 레이어를 통해 정보를 처리하는 능력이 더 가치가 있다는 것입니다.</p><p>이 deep-and-thin 아키텍처의 성능을 데이터로 살펴보겠습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">전통적인 shallow-fat 아키텍처를 사용하는 비슷한 모델들과 비교했을 때, ModernBERT는 비슷한 매개변수 수를 유지하면서도 검색과 STS 같은 주요 작업에서 더 나은 결과를 보여줍니다.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Average</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><code>jina-XLM-RoBERTa</code>를 예로 들어보면 - <code>RoBERTa-large</code>의 shallow-fat 아키텍처를 기반으로 하되 어휘를 50K에서 250K 토큰으로 늘리고 더 많은 데이터로 학습했습니다. 그럼에도 ModernBERT가 약간 앞서는데, 이는 아키텍처의 변화가 효율성 측면에서 실질적인 차이를 만들어내고 있음을 시사합니다.</p><h3 id=\"vocabulary-size-matters\">어휘 크기의 중요성</h3><p>먼저, 트랜스포머에서 어휘 매개변수가 어떻게 계산되는지 살펴보겠습니다. 모든 트랜스포머에서 <code>어휘 매개변수 = 고유 토큰 수 × hidden size</code>입니다. <code>jina-XLM-RoBERTa</code>를 예로 들면: 250K 토큰과 1,024 차원으로, 실제 언어 작업을 처리하기도 전에 어휘 인코딩만을 위해 256M 매개변수가 필요합니다!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">트랜스포머에서 첫 번째 레이어는 토큰을 가중치 행렬, 즉 어휘 가중치를 사용하여 은닉 상태로 매핑합니다. 모든 UTF-8 코드 포인트(1,112,064)를 1,024 차원의 은닉층과 함께 사용한다고 가정하면, 토큰 변환만을 위해 거대한 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> 개의 파라미터가 필요합니다. 대형 LLM(100B+ 파라미터)은 이러한 오버헤드를 처리할 수 있지만, 작은 모델에는 심각한 제약이 됩니다. 이것이 바로 우리가 BPE와 같은 토크나이저를 사용하여 일반적인 UTF-8 코드 포인트를 단일 토큰으로 효율적으로 병합하는 이유입니다.</span></figcaption></figure><p>하지만 여기서 중요한 점은 다음과 같습니다: <strong>어휘 가중치는 어텐션 메커니즘에 기여하지 않으며, 단순히 조회 테이블일 뿐입니다.</strong> 고정된 파라미터 예산으로 작동하는 SLM의 경우, 더 큰 어휘는 실제 언어 처리를 수행하는 어텐션 레이어에 사용할 수 있는 파라미터가 줄어든다는 것을 의미합니다. 이는 영어 전용 ModernBERT-large가 더 작음에도 불구하고 다국어 <code>jina-XLM-RoBERTa</code>보다 성능이 더 우수한 이유를 설명합니다 - <code>jina-XLM-RoBERTa</code>는 다중 언어를 지원하기 위해 더 많은 파라미터(47%!)를 할당합니다. ModernBERT의 집중된 어휘는 성능을 개선할 뿐만 아니라 추론 속도도 향상시켜 리소스가 제한된 애플리케이션에 특히 효과적입니다.</p><p>이제 어휘 가중치를 제외한 <em>핵심</em> 모델 파라미터만 살펴보면, ModernBERT는 실제로 동료들보다 더 많은 연산 능력을 가지고 있습니다: ModernBERT는 <code>jina-XLM-RoBERTa</code>보다 19% 더 많은 파라미터를, <code>RoBERTa-large</code>보다 15% 더 많은 파라미터를 <em>실제</em> 언어 모델링에 할당합니다!</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>모델 사양</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>언어 지원</td>\n<td>영어만</td>\n<td>89개 언어</td>\n<td>영어만</td>\n</tr>\n<tr>\n<td>어휘 크기</td>\n<td>50.4K</td>\n<td>250K</td>\n<td>50.3K</td>\n</tr>\n<tr>\n<td>총 파라미터</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>어휘 파라미터</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>어휘 파라미터 비율</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>핵심 모델 파라미터</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">\"가중치 타일링\"을 통한 모델 확장</h3><p><a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a> 백본을 구축하면서, 처음부터 SLM을 훈련하는 것이 리소스 집약적이고 복잡하다는 것을 발견했습니다. ModernBERT는 <strong>가중치 타일링</strong>이라는 스마트한 초기화 접근 방식으로 이 문제를 해결합니다 - 본질적으로 더 작은 base 버전의 가중치로부터 ModernBERT-large를 부트스트랩합니다.</p><p>이 기술은 완전히 새로운 것은 아닙니다 - DeepMind의 <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> 작업을 기반으로 하며 Microsoft의 <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2 모델</a>에서도 나타납니다. 하지만 여기서의 적용은 SLM 훈련 병목 현상을 해결하는 데 특히 효과적입니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT는 Gopher 팀의 깊이 초기화 전략을 사용하여 22개에서 28개 레이어로 확장됩니다. 추가 레이어(23-28)의 경우, ModernBERT-base의 원래 22개 레이어의 가중치를 사용하여 각 레이어를 초기화합니다. 각 레이어의 가중치 행렬에 대해서는 Phi-2의 중앙 타일링 접근 방식을 사용합니다. 작동 방식은 다음과 같습니다: ModernBERT-base 가중치를 가져와서 ModernBERT-large의 행렬 중앙에 배치합니다. 여전히 비어 있는 가장자리는 어떻게 할까요? 원래 가중치를 순환적으로 감싸서 채웁니다.</span></figcaption></figure><p>이 초기화 전략은 ModernBERT-large에 상당한 이점을 제공합니다 - 처음부터 시작하는 대신 더 작은 버전에서 사전 학습된 패턴을 활용합니다. 이는 <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">이 크기 범위의 언어 모델을 확장하는 데 특히 효과적</a>임이 입증되었습니다.</p><blockquote>우리는 웜 스타트된 모델이 (추가된 파라미터로 인한) 높은 초기 손실에서 빠르게 회복하여 기본 모델과 매우 가까운 손실에 도달한다는 것을 발견했습니다. 417M 파라미터를 3배 이상 확장하면서도 수렴할 때까지 처음부터 훈련된 동등한 신규 모델보다 더 나은 성능을 유지할 수 있었습니다. 이는 이득이 훈련 초기에만 국한되지 않았음을 의미합니다. 그러나 더 큰 크기에서는, 특히 너비 확장에서 수렴 시 달성되는 상대적 이득이 감소합니다.</blockquote><p>순환적 가중치 래핑은 단순한 편의성이 아닙니다 - 어텐션 행렬이 자연스럽게 주기적 패턴을 보이는 방식과 잘 부합합니다. Gopher의 연구는 이 접근 방식이 SLM(9B 미만 파라미터)에서 특히 빛을 발하지만, 더 큰 모델 영역으로 이동할수록 이점이 점차 감소하기 시작한다는 것을 보여줍니다.</p><h2 id=\"modernberts-code-modeling\">ModernBERT의 코드 모델링</h2><p>ModernBERT는 코드에 최적화된 토크나이저와 훈련 데이터를 통해 코드 이해에 특화된 접근 방식을 제공합니다. 이러한 코드 처리를 위한 미세 조정은 이해와 검색 작업 모두에서 효과를 발휘합니다.</p><p>우리는 <code>jina-embeddings-v2-code</code> 코퍼스를 사용하여 세 가지 모델을 백본으로 비교하는 벤치마크를 실행했습니다: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, 그리고 <code>RoBERTa-large</code>. 테스트는 <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - 텍스트 설명을 코드 스니펫과 매칭하는 것이었습니다. ModernBERT는 전반적으로 두 대안을 모두 능가했습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">이 격차는 이해가 됩니다 - </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\">와 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> 모두 훈련 중에 프로그래밍 언어를 접하지 않았습니다. 반면, ModernBERT-large는 상당한 양의 코드를 포함한 2조 개의 토큰으로 훈련되었습니다. 이러한 프로그래밍 구문과 패턴에 대한 노출은 코드 관련 작업에서 명확한 이점을 제공합니다. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\">가 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">보다 약간 앞서는 것은 더 큰 다국어 훈련 데이터 때문일 것입니다 - 같은 아키텍처에 더 많은 노출이 있었기 때문입니다. 그럼에도 불구하고, 둘 다 ModernBERT-large에 크게 뒤처집니다.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>작업</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">토크나이저의 장점</h3><p>ModernBERT가 코드를 잘 다루는 이유를 살펴보겠습니다. ModernBERT는 표준 BERT/RoBERTa 토크나이저 대신 코드 학습에 특화된 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">OLMo 토크나이저</a>를 사용합니다.</p><p>토크나이저는 UTF-8 텍스트를 벡터로 매핑되는 토큰으로 분할합니다. 이는 모델이 실제로 처리하는 대상입니다. 학습 과정에서 자주 발생하는 문자열을 단일 토큰으로 결합하는 방법을 학습합니다. 차이점은 무엇일까요? 표준 토크나이저는 <code>init</code>를 <code>in</code> + <code>it</code>으로 분할하여 프로그래밍 컨텍스트를 놓칠 수 있습니다. 하지만 ModernBERT의 코드 인식 토크나이저는 이를 분할하지 않고 그대로 처리합니다.</p><p>공백 처리에서 흥미로운 점이 있습니다. ModernBERT는 Python의 선행 공백을 단일 토큰으로 유지하고 4개와 8개의 공백을 구분합니다 - 이는 코드 구조에 매우 중요합니다. 반면에 <strong><code>jina-XLM-RoBERTa</code>는 연속된 모든 공백을 단일 <code>_</code>로 축소하고, RoBERTa-large는 각 공백을 개별 토큰으로 처리합니다.</strong> 이는 ModernBERT의 인코더가 코드를 처리할 때 더 깔끔하고 의미 있는 입력을 받는 반면, 다른 모델들은 분절되고 덜 일관된 토큰으로 작업한다는 것을 의미합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT는 Python의 선행 공백을 단일 토큰으로 유지하고 4개와 8개의 공백을 구분합니다 - 이는 코드 구조에 매우 중요합니다. 반면 다른 모델들은 분절되고 덜 일관된 토큰으로 작업합니다.</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">ModernBERT의 긴 컨텍스트 처리</h2><p>ModernBERT는 방대한 학습 코퍼스(8,192 토큰 샘플로 구성된 300B 토큰)와 전역 및 지역 어텐션을 결합한 고급 기술 덕분에 긴 텍스트 처리에서 큰 진전을 이루었습니다.</p><p>긴 문서 처리 능력을 평가하기 위해 13개 언어를 포괄하는 종합적인 긴 텍스트 벤치마크인 <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">MLDR 데이터셋</a>을 사용했습니다. ModernBERT가 현재 영어만 지원하므로, ModernBERT와 <code>jina-XLM-RoBERTa</code>를 벤치마크하기 위해 MLDR의 영어 부분집합에 초점을 맞췄습니다. 두 모델 모두 8K 토큰 입력을 처리할 수 있지만, <code>RoBERTa-large</code>는 512 토큰 제한으로 인해 긴 텍스트 분석에 부적합하여 이 벤치마크에서 제외되었습니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>ModernBERT의 우수한 성능은 단순히 광범위한 긴 텍스트 학습 때문만이 아닙니다. 전역 및 지역 어텐션 메커니즘의 혁신적인 조합 덕분입니다. <code>jina-XLM-RoBERTa</code>가 모든 레이어에 계산 비용이 많이 드는 전역 어텐션을 적용하는 것과 달리, ModernBERT는 더 효율적인 접근 방식을 취합니다. 전역 어텐션(3번째 레이어마다 사용되며 <code>theta</code>는 160,000)과 지역 어텐션(128 토큰 슬라이딩 윈도우 사용, <code>theta</code>는 100,000)을 번갈아 사용합니다. 이러한 하이브리드 전략은 높은 성능을 유지하면서 학습 시간을 크게 단축합니다.</p><blockquote>ModernBERT에서는 3번째 레이어마다 RoPE theta 160,000의 전역 어텐션을 사용하고, 나머지 레이어는 RoPE theta 10,000의 128 토큰 지역 슬라이딩 윈도우 어텐션을 사용합니다. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">쓴 교훈?</h2><p>스케일링 법칙과 <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">쓴 교훈</a>은 주요 성능 향상이 주로 파라미터 수와 학습 데이터 증가에서 온다고 제시합니다. 이 원칙은 코퍼스를 확장하고 작업별 적응을 위해 LoRA를 사용하는 우리의 접근 방식을 이끌었습니다.</p><p>하지만 ModernBERT의 성공은 우리가 아키텍처 최적화의 힘을 과소평가했음을 보여줍니다. 이는 SLM이 반드시 파라미터를 늘리지 않고도 더 나은 데이터-모델 효율성을 통해 뛰어난 결과를 달성할 수 있음을 보여줍니다. 최근 <a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">Stella Embeddings 기술 보고서</a>는 이러한 발견을 강화하며, 현재의 임베딩 모델 학습 방법이 코퍼스나 모델 크기를 늘리지 않고도 개선될 수 있음을 보여줍니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">임베딩 모델의 스케일링 법칙. 영어 작업에 대한 평균 MTEB 성능이 모델 파라미터 수에 대해 플롯되었습니다. 각 점은 임베딩 모델을 나타냅니다. 모든 모델을 나타내는 추세선이 강조되어 있고, 다국어 모델은 시안색으로 강조되어 있습니다. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">가 유사한 크기의 모델들보다 우수한 성능을 보여주며, 이전 버전인 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">보다 초선형적 개선을 보여줍니다. 이 그래프는 MTEB 리더보드에서 상위 100개 임베딩 모델을 선택하여 작성되었으며, 크기 정보가 없는 모델(일반적으로 비공개 또는 독점 모델)은 제외되었습니다. 명백한 트롤링으로 확인된 제출도 필터링되었습니다. </span></figcaption></figure><p>앞으로 데이터 활용에 대한 더 깊은 통찰력을 얻고 ModernBERT의 기술을 구현함에 따라 계산 비용과 모델 크기가 감소할 것으로 예상됩니다. 단기적으로는 ModernBERT 논문에 설명된 간단한 개선 사항을 구현할 수 있습니다 - 특히 더 많은 코드 관련 데이터를 통합하고 코드 친화적인 토크나이저를 도입하는 것입니다. deep-and-thin 아키텍처로 전환하거나 작은 모델에서 큰 모델을 부트스트랩하는 것과 같은 더 복잡한 변경은 백본 모델을 처음부터 구축해야 하는 중기적 과제가 될 것입니다.</p><p>ModernBERT의 효율성은 주목할 만하지만, 텍스트만 처리할 수 있다는 한계는 미래의 과제를 보여줍니다. 멀티모달 임베딩 모델이 인기를 얻으면서, 우리의 다음 과제는 멀티모달 애플리케이션을 위한 입력을 처리할 수 있는 더 스마트하고 빠르며 유능한 검색 기반 모델을 개발하는 것입니다. 이러한 애플리케이션은 더 긴 컨텍스트 윈도우를 요구하며 - 이는 아직 해결해야 할 효율성 과제입니다.</p><h2 id=\"conclusion\">결론</h2><p>이 글에서 우리는 ModernBERT가 deep-and-thin 아키텍처, 최적화된 토크나이저, 웨이트 타일링을 통한 효율적인 스케일링이라는 세 가지 핵심 혁신을 통해 BERT 계열 모델을 어떻게 발전시켰는지 살펴보았습니다. 이러한 개선으로 ModernBERT는 상대적으로 작은 크기로도 다양한 작업에서 <code>RoBERTa-large</code>와 <code>jina-XLM-RoBERTa</code> 모두를 능가하는 뛰어난 성능을 제공할 수 있습니다. ModernBERT는 아키텍처 개선이 파라미터 크기보다 더 중요할 수 있음을 보여주며, 더 효율적인 모델의 가능성을 열어줍니다. 웨이트 타일링의 성공적인 활용은 학습 비용을 줄이면서도 성능을 유지하거나 향상시킬 수 있는 방법을 보여줍니다. 또한 작은 어휘와 목표지향적 최적화는 자원이 제한된 환경에서 특화된 SLM의 기회가 증가하고 있음을 시사합니다.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "더 큰 학습 데이터, 효율적인 파라미터 크기 조정, 그리고 깊지만 얇은 아키텍처를 가진 ModernBERT는 향후 BERT 계열 모델들의 방향을 제시합니다.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "query-expansion-with-llms-searching-better-by-saying-more",
  "id": "67af53142962d20001d63c71",
  "uuid": "581110d6-5791-42f7-b754-16d597390ff7",
  "title": "LLM으로 검색 확장하기: 더 많이 표현하여 더 잘 검색하기",
  "html": "<p>쿼리 확장은 시맨틱 임베딩이 등장하기 전까지 검색 시스템을 강화하는 주요 기술이었습니다. 현재 RAG와 에이전트 검색의 시대에서는 구식으로 여겨질 수 있지만, 아직 그 가치를 무시할 수 없습니다. 이 심층 분석에서는 자동 쿼리 확장을 <code>jina-embeddings-v3</code>와 LLM과 결합하여 검색 성능을 향상시키고 더 정확한 결과를 얻는 방법을 살펴보겠습니다.</p><h2 id=\"what-is-query-expansion\">쿼리 확장이란 무엇인가?</h2><p>쿼리 확장은 <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf-idf</a>나 다른 \"sparse vector\" 방식처럼 쿼리의 단어와 문서에 포함된 단어를 매칭하여 관련성을 판단하는 검색 시스템을 위해 개발되었습니다. 이는 명백한 한계가 있습니다. \"ran\"과 \"running\", 또는 \"optimise\"와 \"optimize\"와 같이 단어의 변형이 매칭을 방해합니다. 언어 인식 전처리로 이러한 문제 중 일부를 완화할 수 있지만, 모든 문제를 해결할 수는 없습니다. 전문 용어, 동의어, 관련 단어는 훨씬 더 다루기 어렵습니다. 예를 들어, \"coronavirus\"에 대한 의학 연구 쿼리는 매우 적절한 매칭임에도 불구하고 \"COVID\" 또는 \"SARS-CoV-2\"에 대해 언급하는 문서를 자동으로 식별하지 못합니다.</p><p>쿼리 확장은 이러한 문제의 해결책으로 고안되었습니다.</p><p>이 아이디어는 쿼리에 추가 단어와 구문을 더하여 좋은 매칭을 식별할 가능성을 높이는 것입니다. 이렇게 하면 \"coronavirus\" 쿼리에 \"COVID\"와 \"SARS-CoV-2\" 같은 용어가 추가될 수 있습니다. 이는 검색 성능을 크게 향상시킬 수 있습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"700\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 1: 시소러스를 이용한 쿼리 확장 플로우차트</span></figcaption></figure><p>쿼리에 어떤 용어를 추가해야 할지 결정하는 것은 쉽지 않으며, tf-idf 스타일 검색을 위한 좋은 용어를 식별하고 가중치를 부여하는 방법에 대해 많은 연구가 있었습니다. 일반적인 접근 방식은 다음과 같습니다:</p><ul><li>사람이 직접 관리하는 시소러스 사용</li><li>대규모 텍스트 코퍼스에서 관련 단어 데이터 마이닝</li><li>쿼리 로그에서 유사한 쿼리에 사용된 다른 용어 식별</li><li><a href=\"https://en.wikipedia.org/wiki/Rocchio_algorithm\">사용자 피드백을 통해</a> 좋은 쿼리 확장이 되는 단어와 구문 학습</li></ul><p>하지만 시맨틱 임베딩 모델은 쿼리 확장의 필요성을 없애야 합니다. \"coronavirus\", \"COVID\", \"SARS-CoV-2\"에 대한 좋은 텍스트 임베딩은 임베딩 벡터 공간에서 서로 매우 가까워야 합니다. 이들은 자연스럽게 증강 없이도 매칭되어야 합니다.</p><p>하지만 이론적으로는 그래야 하지만, 실제 모델이 만든 실제 임베딩은 종종 부족한 경우가 있습니다. 임베딩의 단어가 모호할 수 있으며, 적절한 단어를 사용하면 쿼리에 단어를 추가하여 더 나은 매칭으로 유도할 수 있습니다. 예를 들어, \"skin rash\"의 임베딩은 \"behaving rashly\"와 \"skin creme\"에 대한 문서는 식별하면서 \"dermatitis\"에 대해 이야기하는 의학 저널 기사는 놓칠 수 있습니다. 관련 용어를 추가하면 임베딩을 관련 없는 매칭에서 더 나은 매칭으로 유도할 가능성이 높습니다.</p><h2 id=\"llm-query-expansion\">LLM 쿼리 확장</h2><p>시소러스를 사용하거나 어휘 데이터 마이닝을 하는 대신, 우리는 LLM을 사용하여 쿼리 확장을 하는 방법을 살펴보았습니다. LLM은 몇 가지 중요한 잠재적 장점이 있습니다:</p><ul><li><strong>광범위한 어휘 지식</strong>: 대규모의 다양한 데이터셋으로 학습되었기 때문에, 적절한 시소러스를 선택하거나 올바른 데이터를 얻는 것에 대한 걱정이 덜합니다.</li><li><strong>판단 능력</strong>: 제안된 모든 확장 용어가 특정 쿼리에 반드시 적절한 것은 아닙니다. LLM이 완벽한 판단을 하지는 못할 수 있지만, 다른 방식들은 전혀 판단을 할 수 없습니다.</li><li><strong>유연성</strong>: 검색 작업의 필요에 따라 프롬프트를 조정할 수 있는 반면, 다른 접근 방식들은 경직되어 있고 새로운 도메인이나 데이터 소스에 적응하기 위해 많은 작업이 필요할 수 있습니다.</li></ul><p>LLM이 용어 목록을 제안하면, 임베딩을 위한 쿼리 확장은 전통적인 쿼리 확장 방식과 동일하게 작동합니다: 쿼리 텍스트에 용어를 추가한 다음 임베딩 모델을 사용하여 쿼리 임베딩 벡터를 생성합니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"850\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 2: LLM을 이용한 임베딩의 쿼리 확장</span></figcaption></figure><p>이를 구현하기 위해서는 다음이 필요합니다:</p><ul><li>LLM 접근 권한</li><li>LLM에서 확장 용어를 요청하기 위한 프롬프트 템플릿</li><li>텍스트 임베딩 모델</li></ul><h2 id=\"trying-it-out\">실험해보기</h2><p>이 접근 방식이 텍스트 정보 검색에 가치를 더하는지 확인하기 위해 몇 가지 실험을 수행했습니다. 우리의 테스트에는 다음이 사용되었습니다:</p><ul><li>LLM 하나: Google의 <a href=\"https://deepmind.google/technologies/gemini/flash/\">Gemini 2.0 Flash</a></li><li>LLM 쿼리 확장이 모델 간에 일반화되는지 확인하기 위해 두 가지 임베딩 모델: <code>jina-embeddings-v3</code>와 <a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><code>all-MiniLM-L6-v2</code></a></li><li>정보 검색을 위한 <a href=\"https://github.com/beir-cellar/beir\">BEIR 벤치마크</a>의 하위 집합</li></ul><p>우리는 두 가지 프롬프팅 조건에서 실험을 수행했습니다:</p><ul><li>확장 용어를 요청하기 위한 일반 프롬프트 템플릿 사용</li><li>작업별 프롬프트 템플릿 사용</li></ul><p>마지막으로, 100, 150, 250개의 다양한 수의 용어를 추가하도록 프롬프트를 작성했습니다.</p><p>우리의 코드와 결과는 재현을 위해 <a href=\"https://github.com/jina-ai/llm-query-expansion/\">GitHub에서 이용 가능</a>합니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/llm-query-expansion/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/llm-query-expansion: Query Expension for Better Query Embedding using LLMs</div><div class=\"kg-bookmark-description\">Query Expension for Better Query Embedding using LLMs - jina-ai/llm-query-expansion</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-1.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/llm-query-expansion\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"results\">결과</h2><h3 id=\"using-a-general-prompt\">일반 프롬프트 사용</h3><p>여러 번의 시행착오 끝에, 다음 프롬프트가 Gemini 2.0 Flash에서 충분히 잘 작동한다는 것을 발견했습니다:</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make \nit easier to find the relevant documents (about <span style=\"color:#AADB1E\">{size}</span> words \nper query):\n<span style=\"color:#AADB1E\">{query}</span>\n\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>이 프롬프트를 사용하면 20-50개의 쿼리를 배치로 묶어 각각에 ID를 부여하고, 각 쿼리에 확장 용어 목록을 연결하는 JSON 문자열을 반환받을 수 있습니다. 다른 LLM을 사용하는 경우, 해당 모델에 맞는 프롬프트를 찾기 위해 실험이 필요할 수 있습니다.</p><p>우리는 <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model\">비대칭 검색 어댑터</a>를 사용하여 <code>jina-embeddings-v3</code>로 이 설정을 적용했습니다. 이 접근 방식에서는 쿼리와 문서가 다르게 인코딩됩니다 - 동일한 모델이지만 다른 LoRA 확장을 사용하여 정보 검색을 위한 임베딩을 최적화합니다.</p><p>다양한 BEIR 벤치마크에 대한 결과는 아래 표와 같습니다. 점수는 nDCG@10(검색된 상위 10개 항목에 대한 <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\">정규화된 할인 누적 이득</a>)입니다.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>벤치마크</th>\n<th>확장 없음</th>\n<th>100개 용어</th>\n<th>150개 용어</th>\n<th>250개 용어</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>(사실 확인 작업)</td>\n<td>72.74</td>\n<td>73.39</td>\n<td>74.16</td>\n<td><strong>74.33</strong></td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>(의료 검색 작업)</td>\n<td>77.55</td>\n<td>76.74</td>\n<td>77.12</td>\n<td><strong>79.28</strong></td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>(금융 옵션 검색)</td>\n<td>47.34</td>\n<td><strong>47.76</strong></td>\n<td>46.03</td>\n<td>47.34</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>(의료 정보 검색)</td>\n<td>36.46</td>\n<td><strong>40.62</strong></td>\n<td>39.63</td>\n<td>39.20</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>(논증 검색 작업)</td>\n<td>26.24</td>\n<td>26.91</td>\n<td>27.15</td>\n<td><strong>27.54</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>여기서 검색어 확장이 거의 모든 경우에서 검색 성능을 향상시켰음을 볼 수 있습니다.</p><p>이 접근 방식의 견고성을 테스트하기 위해, 더 작은 임베딩 벡터를 생성하는 훨씬 작은 모델인 <code>all-MiniLM-L6-v2</code>로 동일한 테스트를 반복했습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">sentence-transformers/all-MiniLM-L6-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-29.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/all-MiniLM-L6-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>결과는 아래 표와 같습니다:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>No Expansion</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>(Fact Checking Task)</td>\n<td>64.51</td>\n<td><strong>68.72</strong></td>\n<td>66.27</td>\n<td>68.50</td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>(Medical Retrieval Task)</td>\n<td>47.25</td>\n<td>67.90</td>\n<td><strong>70.18</strong></td>\n<td>69.60</td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>(Financial Option Retrieval)</td>\n<td><strong>36.87</strong></td>\n<td>33.96</td>\n<td>32.60</td>\n<td>31.84</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>(Medical Information Retrieval)</td>\n<td>31.59</td>\n<td><strong>33.76</strong></td>\n<td>33.76</td>\n<td>33.35</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>(Argument Retrieval Task)</td>\n<td>16.90</td>\n<td><strong>25.31</strong></td>\n<td>23.52</td>\n<td>23.23</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>여기서 검색 점수가 더 크게 향상된 것을 볼 수 있습니다. 전반적으로, 더 작은 모델이 검색어 확장에서 더 많은 이점을 얻었습니다. 모든 작업에 대한 평균 개선도는 아래 표에 요약되어 있습니다:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>jina-embeddings-v3</code></td>\n<td>+1.02</td>\n<td>+0.75</td>\n<td><strong>+1.48</strong></td>\n</tr>\n<tr>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>+6.51</strong></td>\n<td>+5.84</td>\n<td>+5.88</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>두 모델 간의 순 개선도 차이가 큰 것은 아마도 <code>all-MiniLM-L6-v2</code>가 더 낮은 성능 수준에서 시작했기 때문일 것입니다. 비대칭 검색 모드에서 <code>jina-embeddings-v3</code>가 생성한 검색어 임베딩이 주요 의미론적 관계를 더 잘 포착할 수 있어서, 검색어 확장이 정보를 추가할 여지가 더 적습니다. 하지만 이 결과는 검색어 확장이 대형 모델보다 일부 사용 사례에서 선호될 수 있는 더 컴팩트한 모델의 성능을 얼마나 향상시킬 수 있는지를 보여줍니다.</p><p>그럼에도 불구하고, 검색어 확장은 <code>jina-embeddings-v3</code>와 같은 고성능 모델의 검색에도 의미 있는 개선을 가져왔지만, 이 결과는 모든 작업과 조건에서 완벽하게 일관되지는 않습니다.</p><p><code>jina-embeddings-v3</code>의 경우, FiQA와 NFCorpus 벤치마크에서는 검색어에 100개 이상의 용어를 추가하는 것이 역효과를 냈습니다. 더 많은 용어가 항상 더 좋다고 말할 수는 없지만, 다른 벤치마크의 결과는 더 많은 용어가 적어도 때때로는 더 좋다는 것을 보여줍니다.</p><p><code>all-MiniLM-L6-v2</code>의 경우, 150개 이상의 용어를 추가하는 것은 항상 역효과를 냈으며, 세 가지 테스트에서는 100개 이상을 추가해도 점수가 향상되지 않았습니다. 한 테스트(FiQA)에서는 100개의 용어를 추가하는 것조차 상당히 낮은 결과를 보였습니다. 이는 <code>jina-embeddings-v3</code>가 긴 검색어 텍스트의 의미론적 정보를 더 잘 포착하기 때문이라고 생각합니다.</p><p>두 모델 모두 FiQA와 NFCorpus 벤치마크에서는 검색어 확장에 대한 반응이 덜했습니다.</p><h2 id=\"using-task-specific-prompting\">작업별 프롬프팅 사용하기</h2><p>위에서 보고된 결과 패턴은 검색어 확장이 도움이 되지만, LLM을 사용하면 성능을 저하시키는 도움이 되지 않는 검색어 용어가 추가될 수 있음을 시사합니다. 이는 프롬프트의 일반적인 특성 때문일 수 있습니다.</p><p>우리는 SciFact와 FiQA라는 두 벤치마크를 선택하여 아래와 같은 더 도메인별 프롬프트를 만들었습니다:</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make\nit easier to find the <span style=\"background-color:red\">relevant documents</span> <span style=\"background-color:green\">scientific document \nthat supports or rejects the scientific fact in the query \nfield</span> (about <span style=\"color:#AADB1E\">{size}</span> words per query):\n<span style=\"color:#AADB1E\">{query}</span>\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>이 접근 방식은 거의 모든 경우에서 검색 성능을 향상시켰습니다:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>No Expansion</th>\n<th>100<br/>terms</th>\n<th>150<br/>terms</th>\n<th>250<br/>terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>72.74</td>\n<td><strong>75.85 (+2.46)</strong></td>\n<td>75.07 (+0.91)</td>\n<td>75.13 (+0.80)</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td>64.51</td>\n<td><strong>69.12 (+0.40)</strong></td>\n<td>68.10 (+1.83)</td>\n<td>67.83 (-0.67)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>47.34</td>\n<td>47.77 (+0.01)</td>\n<td><strong>48.20 (+1.99)</strong></td>\n<td>47.75 (+0.41)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>36.87</strong></td>\n<td>34.71 (+0.75)</td>\n<td>34.68 (+2.08)</td>\n<td>34.50 (+2.66)</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><code>all-MiniLM-L6-v2</code>로 SciFact 검색어에 250개 용어를 추가하는 경우를 제외하고 모든 조건에서 점수가 향상되었습니다. 또한, 이러한 개선으로도 <code>all-MiniLM-L6-v2</code>는 FiQA에서 자체 기준선을 넘지 못했습니다.</p><p><code>jina-embeddings-v3</code>의 경우, 100개 또는 150개의 추가 용어에서 최상의 결과가 나왔습니다. 250개 용어를 추가하는 것은 역효과를 냈습니다. 이는 용어의 의미가 목표에서 벗어나기 시작하면 검색어에 너무 많은 용어를 추가할 수 있다는 우리의 직관을 뒷받침합니다.</p><h2 id=\"key-considerations-in-query-expansion\">검색어 확장의 주요 고려사항</h2><p>검색어 확장은 분명히 임베딩 기반 검색에 이점을 가져올 수 있지만, 몇 가지 주의사항이 있습니다:</p><h3 id=\"expense\">비용</h3><p>LLM과의 상호작용은 정보 검색에 지연시간과 계산 비용을 추가하며, 상업용 LLM을 사용하는 경우 실제 비용이 발생할 수 있습니다. 이로 인한 적당한 개선이 비용을 정당화하지 못할 수 있습니다.</p><h3 id=\"prompt-engineering\">프롬프트 엔지니어링</h3><p>좋은 프롬프트 템플릿을 설계하는 것은 경험적이고 실험적인 예술입니다. 우리가 이 작업에 사용한 것들이 최적이거나 다른 LLM에 이식 가능하다고 주장하지 않습니다. 작업별 프롬프팅에 대한 우리의 실험은 프롬프트를 변경하는 것이 결과의 품질에 매우 큰 영향을 미칠 수 있음을 보여줍니다. 또한 결과는 도메인에 따라 상당히 다양합니다.</p><p>이러한 불확실성은 개발 비용을 증가시키고 유지보수성을 저해합니다. 시스템의 모든 변경—LLM, 임베딩 모델 또는 정보 도메인의 변경—은 전체 프로세스를 재확인하고 가능한 경우 재구현해야 함을 의미합니다.</p><h3 id=\"alternatives\">대안</h3><p>여기서 우리는 검색어 확장이 초기 성능이 가장 낮은 임베딩 모델에 가장 큰 개선을 가져왔음을 볼 수 있습니다. 적어도 이 실험에서 검색어 확장은 <code>all-MiniLM-L6-v2</code>와 <code>jina-embeddings-v3</code> 사이의 성능 격차를 좁히지 못했으며, <code>jina-embeddings-v3</code>는 검색어 확장으로부터 더 적은 개선을 보았습니다.</p><p>이러한 상황에서, <code>all-MiniLM-L6-v2</code> 사용자는 검색어 확장을 추구하는 대신 <code>jina-embeddings-v3</code>를 채택함으로써 더 낮은 비용으로 더 나은 결과를 얻을 수 있을 것입니다.</p><h2 id=\"future-directions\">향후 방향</h2><p>우리는 여기서 검색어 확장이 검색어 임베딩을 개선할 수 있으며, LLM이 좋은 검색어 확장 용어를 얻기 위한 간단하고 유연한 수단을 제공한다는 것을 보여주었습니다. 하지만 상대적으로 적은 이득은 더 많은 작업이 필요함을 시사합니다. 우리는 미래 연구를 위한 여러 방향을 검토하고 있습니다:</p><ul><li>문서 임베딩 생성에서 용어론적 향상의 가치 테스트.</li><li>재순위 지정과 같은 다른 AI 검색 기술에서 검색어 향상의 가능성 탐색.</li><li>시소러스와 같은 더 오래되고 계산 비용이 적은 용어 소스와 LLM 기반 검색어 확장 비교.</li><li>검색어 확장에 더 나은 성능을 보이는 언어 모델을 특별히 훈련하고 더 도메인별 훈련 제공.</li><li>추가되는 용어 수 제한. 100개는 시작하기에 너무 많을 수 있습니다.</li><li>도움이 되는 용어와 도움이 되지 않는 용어를 식별하는 방법 찾기. 검색어 확장에 우리가 부과하는 고정된 숫자는 완벽하게 맞지 않을 것이며, 제안된 용어를 동적으로 평가하고 좋은 것만 유지할 수 있다면 성능이 향상될 것입니다.</li></ul><p>이는 매우 예비적인 연구이며, 우리는 이러한 기술이 Jina AI의 검색 기반 제품에 추가적인 개선을 가져올 것으로 낙관하고 있습니다.</p>",
  "comment_id": "67af53142962d20001d63c71",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/02/query-banner.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-02-14T15:28:36.000+01:00",
  "updated_at": "2025-02-18T03:24:20.000+01:00",
  "published_at": "2025-02-18T03:24:20.000+01:00",
  "custom_excerpt": "Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael Günther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/query-expansion-with-llms-searching-better-by-saying-more/",
  "excerpt": "임베딩 모델이 도입된 이후 검색은 크게 변화했습니다. AI에서 쿼리 확장과 같은 어휘적 기술이 여전히 역할을 할까요? 우리는 그렇다고 생각합니다.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "long-context-embedding-models-are-blind-beyond-4k-tokens",
  "id": "67c868baf1c5780001164330",
  "uuid": "a9f711ab-651e-4587-8a49-793d15b21380",
  "title": "4K 토큰을 넘어서는 긴 문맥을 처리하는 임베딩 모델들의 한계",
  "html": "<p>2025년 2월, AI 연구팀은 대규모 언어 모델의 긴 컨텍스트 처리 능력을 평가하는 새로운 벤치마크를 소개하는 <a href=\"https://arxiv.org/abs/2502.05167\">NoLiMA 논문</a>을 발표했습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.05167\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">NoLiMa: Long-Context Evaluation Beyond Literal Matching</div><div class=\"kg-bookmark-description\">Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-8.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Ali Modarressi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>이 논문은 전통적인 Needle-in-a-Haystack (NIAH) 벤치마크에 큰 변화를 주어, 질문과 건초더미(무관한 텍스트) 속에 숨겨진 바늘(관련 정보) 사이의 문자 그대로의 일치를 제거했습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/niah-vs-nolima.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"150\"><figcaption><span style=\"white-space: pre-wrap;\">예를 들어, 전통적인 NIAH에서는 \"John이 파리를 방문한 연도는?\"라는 질문에 대해 바늘에 \"John은 2019년에 파리를 방문했다\"라는 내용이 직접적으로 포함될 수 있습니다. NOLIMA에서는 \"어떤 캐릭터가 프랑스에 다녀왔나요?\"라는 질문에 대해 바늘에 \"사실, Yuki는 젬퍼 오페라 하우스 옆에 살고 있다\"라는 내용이 포함됩니다 - 이는 모델이 젬퍼 오페라 하우스가 프랑스가 아닌 독일 드레스덴에 있다는 것을 알아야 합니다.</span></figcaption></figure><p>이는 현재 LLM의 중요한 한계를 보여줍니다: 표면적인 패턴 매칭에 크게 의존하며, 컨텍스트 길이가 증가할수록 깊은 연관 추론 능력이 급격히 저하됩니다.</p><p>이러한 통찰을 바탕으로, 우리는 임베딩 모델, 특히 <code>jina-embeddings-v3</code>에서도 유사한 성능 패턴이 나타나는지 조사하고자 합니다. RAG 시스템의 효과는 검색 모델의 품질에 크게 의존하기 때문에, 우리는 다음 두 가지 핵심 질문을 통해 NoLiMA의 연구를 확장하고자 합니다:</p><ul><li>임베딩 모델이 문자 그대로의 키워드 매칭을 넘어 의미적 도약이 필요할 때 다양한 컨텍스트 길이에서 바늘-건초더미 검색을 어떻게 처리하는가?</li><li>의미적으로 유사한 콘텐츠로 전략적 쿼리 확장이 이러한 성능 격차를 완화할 수 있는가?</li></ul><p>LLM에서 관찰된 뚜렷한 대조 - 어휘 매칭에서는 강력하지만 의미적 변형에는 취약함 - 는 임베딩 기반 검색 시스템도 표면적 용어 매칭을 넘어설 때 유사한 과제에 직면할 수 있음을 시사하며, 현재 의미 검색 기술의 근본적인 한계를 드러낼 수 있습니다.</p><h2 id=\"needles-and-haystacks-construction\">바늘과 건초더미 구성</h2><h3 id=\"needles-construction\">바늘 구성</h3><p>전통적인 바늘-건초더미 테스트는 검색하고자 하는 질문의 표현을 반영하는 바늘을 사용합니다. 예를 들면:</p><ul><li>질문: \"어떤 캐릭터가 드레스덴에 다녀왔나요?\"</li><li>바늘: \"Yuki는 드레스덴에 살고 있습니다.\"</li></ul><p>하지만 NoLiMA처럼, 우리는 단순한 키워드 매칭이 아닌 의미적 이해를 테스트하고자 하므로, 문서에 없는 단어를 특별히 사용하여 one-hop 변형을 두 가지 단어 순서로 만듭니다:</p><ul><li>질문: \"어떤 캐릭터가 드레스덴에 다녀왔나요?\"</li><li>바늘 (기본): \"사실, Yuki는 젬퍼 오페라 하우스 옆에 살고 있습니다.\"</li><li>바늘 (도치): \"젬퍼 오페라 하우스는 Yuki가 사는 곳 옆에 있습니다.\"</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Semperoper\">젬퍼 오페라 하우스</a>는 드레스덴에 있어, 이 one-hop 바늘의 맥락을 제공합니다.</div></div><p>논문의 방법론을 따라, 우리는 아래 예시와 같이 여러 카테고리에 걸쳐 이러한 바늘-질문 그룹(하나의 질문, <strong>하나의 one-hop 바늘</strong>, 그리고 <strong>하나의 도치된 one-hop 바늘</strong>로 구성)을 생성합니다:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>카테고리</th>\n<th>질문</th>\n<th>원본 바늘 (참고용)</th>\n<th>One-hop 바늘</th>\n<th>도치된 one-hop 바늘</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>식이 제한</td>\n<td>어떤 캐릭터가 생선 기반 음식을 먹을 수 없나요?</td>\n<td>Alice는 생선 기반 음식을 먹을 수 없습니다.</td>\n<td>그때, Alice는 수년간 비건이었다고 언급했습니다.</td>\n<td>비건이 되는 것은 Alice에게 수년간 중요했습니다.</td>\n</tr>\n<tr>\n<td>의학적 상태</td>\n<td>어떤 캐릭터가 우유를 마실 수 없나요?</td>\n<td>Bob은 우유를 마실 수 없습니다.</td>\n<td>Bob은 자신이 유당불내증이라고 설명했습니다.</td>\n<td>유당불내증은 Bob의 일상에 영향을 미쳤습니다.</td>\n</tr>\n<tr>\n<td>언어 능력</td>\n<td>어떤 캐릭터가 프랑스어를 할 수 있나요?</td>\n<td>Charlie는 프랑스어를 합니다.</td>\n<td>사실, Charlie는 소르본 대학에서 공부했습니다.</td>\n<td>소르본 대학에서 Charlie는 학위를 마쳤습니다.</td>\n</tr>\n<tr>\n<td>전문적 배경</td>\n<td>어떤 캐릭터가 음악가인가요?</td>\n<td>Diane은 음악가입니다.</td>\n<td>2013년, Diane은 시드니 오페라 하우스에서 지휘했습니다.</td>\n<td>시드니 오페라 하우스 공연은 Diane이 지휘했습니다.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">위의 이름들은 참고용입니다. 실제 바늘에서는 문화적으로 다양한 이름 목록에서 무작위로 가져옵니다.<br><br>원본 바늘(문자 그대로의 키워드 매칭)은 참고용으로만 제공되며, 우리의 실험에서는 사용되지 않습니다.</div></div><h3 id=\"haystacks-construction\">건초더미 구성</h3><p>우리는 각각 최소 50,000개의 토큰을 포함하는 10개의 퍼블릭 도메인 책으로 시작하여, 이들로부터 짧은 스니펫(250 토큰 미만)을 무작위로 연결하여 128, 256, 512, 1024, 2048, 4096, 8192 토큰의 다양한 길이의 건초더미를 만들었습니다. 그런 다음 각 건초더미에 하나의 바늘을 삽입했습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"415\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png 896w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 1: 책의 짧은 스니펫과 건초더미당 하나의 바늘로 구성된 건초더미 구성.</span></figcaption></figure><p>더 구체적인 예시로, \"사실, Yuki는 젬퍼 오페라 하우스 옆에 살고 있습니다\"라는 바늘을 50번 위치의 128-토큰 건초더미에 넣어보겠습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"508\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/text2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/text2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 2: 건초더미 속 바늘 예시.</span></figcaption></figure><p><code>jina-embeddings-v3</code>를 사용하여 텍스트를 임베딩할 때, 바늘 텍스트와 건초더미 텍스트 사이의 유사도 점수는:</p><pre><code class=\"language-bash\">Question-Haystack similarity = 0.2391\n</code></pre><p>그런 다음 이 숫자를 질문과 기본 바늘(건초더미 생성 없이, 직접 비교) 사이의 유사도 점수로 나누어 정규화합니다:</p><pre><code class=\"language-bash\">Question-Needle similarity = 0.3598\nNormalized Query-Haystack similarity = 0.2391 / 0.3598 = 0.6644\n</code></pre><p>이 정규화는 모든 모델이 두 텍스트 사이에서 동일한 유사도 점수를 생성하지 않고, <code>jina-embeddings-v3</code>가 두 텍스트 사이의 유사도를 과소 계산하는 경향이 있기 때문에 필요합니다.</p><p>각 바늘(기본 및 도치 포함)에 대해 컨텍스트 길이별로 10개의 건초더미를 생성하여, 각 건초더미에 다른 위치에 하나의 바늘을 삽입했습니다. 주어진 바늘과 컨텍스트 길이에 대해, 건초더미는 다음과 같이 보일 것입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"290\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png 800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 3: 10개의 건초더미 전체에 걸쳐 규칙적인 간격으로 배치된 바늘들.</span></figcaption></figure><p>대조군으로, 각 테스트 조건에 대해 바늘이 없는 건초더미도 하나씩 생성했습니다. 총 3,234개의 건초더미입니다. 우리는 각 건초더미를 <code>jina-embeddings-v3</code>(기본 텍스트 매칭 LoRA 사용)로 인코딩한 다음, 각 건초더미에 대해 잘라냈습니다(총 토큰이 8,192를 초과하는 경우,<code>jina-embeddings-v3</code>) 그런 다음 해당 질문을 인코딩했습니다.</p><h2 id=\"evaluation-metrics\">평가 지표</h2><p>우리의 평가 프레임워크는 다양한 컨텍스트 길이에 걸쳐 임베딩 모델의 성능을 평가하기 위해 여러 지표를 사용합니다:</p><h3 id=\"primary-metrics\">주요 지표</h3><p><strong>정규화된 유사도 점수</strong><br>핵심 지표는 질문과 전체 컨텍스트(질문-건초더미 유사도) 간의 의미적 유사도와 질문과 해당 기본 바늘(질문-바늘 유사도) 간의 기준 유사도를 모두 고려하는 정규화된 유사도 점수입니다. 이 정규화는 모델의 성능이 절대적인 유사도 점수만이 아닌 의미 있는 기준점에 비례하여 평가되도록 보장합니다. 정규화 과정은 질문과 해당 바늘 간의 직접적인 코사인 유사도 점수(우리의 기준선)를 계산하고, 질문-건초더미 유사도를 이 기준 점수로 나누는 것을 포함합니다:<br></p><p>$\\text{정규화된 유사도} = \\frac{\\cos{(q,h)}}{\\cos{(q,n)}}$</p><p><strong>무작위 확률 대비 비율</strong><br>모든 임베딩 모델에서, 서로 다른 쿼리-문서 쌍 간의 코사인 유사도 점수는 쿼리가 동일할 때만 직접 비교할 수 있습니다. 따라서 정규화된 유사도 점수를 사용하는 것 외에도, 질문이 바늘이 없는 동일한 길이의 무작위 구절보다 전체 건초더미와 더 유사한 빈도를 측정합니다.</p><h3 id=\"secondary-metrics\">부차적 지표</h3><p><strong>분리 분석</strong><br>이 지표는 모델이 관련 콘텐츠와 무관한 콘텐츠를 얼마나 잘 구별하는지 평가합니다. 여기에는 긍정적 예시(답변을 포함하는 구절)와 부정적 예시(답변을 포함하지 않는 구절) 간의 차이를 나타내는 <strong>평균 분리도</strong>와, ROC(Receiver Operating Characteristic) 곡선 아래 영역을 기반으로 구별 능력을 측정하는 <strong>AUC(Area Under the Curve) 점수</strong>가 포함됩니다.</p><p><strong>위치 효과</strong><br>우리는 바늘 위치가 성능에 미치는 영향을 위치와 유사도 점수 간의 <strong>상관 계수</strong>, 위치에 따른 성능 변화를 보여주는 <strong>회귀 기울기</strong>, 그리고 <strong>위치별 구간화된 성능 분석</strong>을 통해 분석합니다.</p><h2 id=\"findings\">연구 결과</h2><h3 id=\"degradation-of-similarity-score-and-correctness\">유사도 점수와 정확도의 저하</h3><p>우리의 결과는 컨텍스트 길이가 증가함에 따라 성능이 저하되는 것을 명확히 보여줍니다. 평균 유사도 점수는 128 토큰에서 0.37이었던 것이 8K 토큰에서는 0.10으로 떨어지며, 128과 1K 토큰 사이에서 급격한 하락을 보이는 비선형적 추세를 따릅니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 4: 컨텍스트 길이에 따른 정규화된 성능.</span></figcaption></figure><p>아래 그림에서는 바늘을 역순으로 배치해도 정규화된 유사도 점수에 거의 차이가 없음을 보여줍니다. 기본 바늘(예: \"실제로, 유키는 젬퍼 오페라 하우스 근처에 산다\")과 역순 바늘(예: \"젬퍼 오페라 하우스는 유키가 사는 곳 옆에 있다\") 모두 거의 동일한 성능을 보입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 5: 기본 순서와 역순의 성능 비교.</span></figcaption></figure><p>데이터셋의 서로 다른 의미적 연결은 다양한 성능을 보입니다. 위치-랜드마크 쌍이 가장 강력한 결과를 유지하는 반면, 식이 제한과 의료 상태 연결은 더 빠르게 저하됩니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 6: 컨텍스트 길이에 따른 그룹별 정규화된 성능.</span></figcaption></figure><p>무작위 확률과 결과를 비교하면 우리의 발견을 뒷받침합니다. 건초더미가 클수록 결과가 무작위에 가까워집니다. 즉, 주어진 질문에 대해 바늘(정답)이 없는 무작위 구절을 선택할 확률이 건초더미를 선택할 확률과 거의 비슷해집니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-12.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 7: 모델 성능과 무작위 확률(0.5) 비교.</span></figcaption></figure><p>다시 한 번, 서로 다른 의미적 연결에 따라 다양한 성능을 보입니다. 일부(식이 제한 같은)는 비교적 짧은 컨텍스트에서도 무작위 확률 이하로 떨어지는 반면, 다른 것들(위치와 랜드마크 같은)은 컨텍스트 길이에 관계없이 훨씬 더 나은 성능을 보입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 8: 그룹 성능과 무작위 확률 비교.</span></figcaption></figure><p>바늘을 역순으로 배치하는 것은 성능에 거의 영향을 미치지 않습니다. 아래 그래프는 배치된 바늘이 기본 순서로 답변을 포함하는지 역순으로 포함하는지에 따라 나눈, 무작위 확률 대비 올바른 건초더미를 선호하는 비교 비율을 보여줍니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 9: 기본 순서와 역순 - 무작위 확률 대비 성능.</span></figcaption></figure><p>기본 순서와 역순 바늘의 결과가 모두 동일한 추세를 따르는 것을 볼 수 있으므로, 이 기준에 대한 분할 분석은 더 이상 진행하지 않겠습니다.</p><h3 id=\"can-we-separate-positive-from-negative-results\">긍정적 결과와 부정적 결과를 구분할 수 있나요?</h3><p>우리의 가장 중요한 발견 중 하나는 임베딩 모델이 서로 다른 컨텍스트 길이에서 관련 콘텐츠와 무관한 콘텐츠를 얼마나 잘 구별할 수 있는지 분석한 것에서 나옵니다. 이 \"분리 분석\"은 검색의 정확도가 128과 1000 토큰 사이의 컨텍스트 길이에서 급격히 떨어지고, 그 후에는 더 느린 속도로 계속 감소한다는 것을 보여줍니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 10: 컨텍스트 길이에 따른 분리 분석.</span></figcaption></figure><p>짧은 컨텍스트(128 토큰)에서는 모델이 0.1의 평균 차이와 함께 강한 분리를 보이며, 0.81의 AUC를 달성합니다(이는 모델이 81%의 경우에 관련 구절을 무관한 구절보다 더 높게 순위를 매긴다는 것을 의미합니다). 이는 짧은 컨텍스트에서 모델이 답변을 포함하는 구절과 그렇지 않은 구절을 안정적으로 구별할 수 있다는 것을 나타냅니다.</p><p>하지만 이는 문맥이 길어질수록 급격히 악화됩니다. 1,000 토큰에서는 분리도가 60% 감소하여 0.040이 되고, AUC는 0.66으로 떨어져 성능이 현저히 저하됨을 나타냅니다. 8,000 토큰에서는 분리도가 최소(0.001)이고 AUC가 0.50에 불과해 거의 무작위 수준의 구별력을 보입니다. 이러한 패턴은 중요한 통찰을 드러냅니다: 모델이 긴 문맥에서도 합리적인 유사도 점수를 계산할 수 있을 때조차도, 이러한 점수를 사용해 관련 정보와 무관한 정보를 구분하는 것은 거의 불가능합니다. 8,000 토큰에서는 모델이 관련 콘텐츠를 구별하는 능력이 본질적으로 무작위 수준입니다.</p><p>문맥이 길어질수록 성능이 저하되는 속도가 놀랍습니다. 원시 유사도 점수는 128 토큰에서 8,000 토큰으로 갈수록 약 75% 감소하지만, 분리도 메트릭은 같은 범위에서 거의 99% 감소합니다. 더욱 우려되는 것은 효과 크기가 98.6%로 더 가파르게 감소한다는 점입니다. 이는 임베딩 모델이 긴 문맥을 다룰 때 겪는 어려움이 단순히 유사도 점수가 감소하는 것을 넘어서 - 관련 정보를 식별하는 근본적인 능력이 이전에 이해했던 것보다 훨씬 더 심각하게 무너진다는 것을 시사합니다.</p><h3 id=\"how-does-the-needle-position-affect-the-core-metrics\">바늘의 위치는 핵심 메트릭에 어떤 영향을 미치는가?</h3><p>일반적으로 바늘이 건초더미의 시작 부분에 있을 때 핵심 성능 메트릭이 가장 좋지만, 성능 저하가 반드시 문맥 중간 부분의 위치와 상관관계가 있는 것은 아닙니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 11: 문맥 길이에 따른 상대적 위치별 성능.</span></figcaption></figure><p>또한 바늘이 주어진 문맥의 시작 부분에 있을 때 성능이 가장 좋으며, 짧은 문맥에서는 바늘이 끝 부분에 위치할 때 성능이 약간 상승하는 것을 볼 수 있습니다. 하지만 모든 문맥에서 바늘이 중간 위치에 있을 때 성능이 떨어지는 것을 볼 수 있습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 12: 위치별 비교 비율.</span></figcaption></figure><h2 id=\"what-effect-does-query-expansion-have-on-the-results\">쿼리 확장이 결과에 미치는 영향은 무엇인가?</h2><p>최근 우리는 검색 시스템의 검색 성능을 향상시키기 위해 쿼리에 관련 용어를 추가하는 기술인 쿼리 확장에 대한 블로그 포스트를 발표했습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Query Expansion with LLMs: Searching Better by Saying More</div><div class=\"kg-bookmark-description\">Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Michael Günther, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/query-expansion-with-llms-searching-better-by-saying-more.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>이 포스트에서, 우리는 LLM을 사용하여 확장 용어를 생성하고, 이를 쿼리 임베딩에 추가하여 검색 성능을 향상시켰습니다. 결과는 상당한 개선을 보여주었습니다. 이제, 우리는 이 기술이 건초더미 속 바늘 찾기 검색에서 어떻게 (또는 만약) 결과를 개선할 수 있는지 살펴보고자 합니다. 예를 들어, 다음과 같은 쿼리가 주어졌을 때:</p><pre><code class=\"language-bash\">Which character has been to Dresden?\n</code></pre><p>우리는 LLM (Gemini 2.0)을 사용하여 이를 확장하고 다음과 같은 100개의 추가 용어를 추가합니다:</p><pre><code class=\"language-bash\">Which character has been to Dresden? Character: fictional character literary character protagonist antagonist figure persona role dramatis personae\\\\n\\\\nDresden: Dresden Germany; bombing of Dresden World War II historical fiction Kurt Vonnegut Slaughterhouse-Five city in Saxony Elbe River cultural landmark\\\\n\\\\nHas been to: visited traveled to journeyed to presence in appears in features in set in takes place in location setting\n\n</code></pre><h3 id=\"how-much-does-query-expansion-help-match-the-needle-to-the-haystack\">쿼리 확장은 바늘과 건초더미의 매칭을 얼마나 도와주는가?</h3><p><a href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\">원본 포스트</a>에서 설명한 대로, 우리는 세 가지 확장된 쿼리 용어 세트(100, 150, 250개 용어)를 생성했습니다. 그런 다음 이전과 동일한 실험 세트를 각각의 확장된 쿼리 용어 세트로 세 번 반복 실행했습니다.</p><p>모든 확장 세트의 결과는 쿼리 확장을 사용하지 않았을 때와 유사한 효과로(그림 4 & 7), 문맥 길이가 증가함에 따라 명확한 성능 저하를 보여주었습니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1071\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 13: 모든 확장 크기의 결합된 정규화 성능.</span></figcaption></figure><p>확장되지 않은 쿼리와 비교했을 때, 모든 쿼리 확장 조건은 문맥이 커짐에 따라 동일한 성능 저하 패턴을 보여주었습니다. 성능 저하 추세는 128과 1K 토큰 사이에서 급격한 하락이 있는 비선형적입니다:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 14: 모든 확장 크기의 결합된 비교 비율.</span></figcaption></figure><p>하지만 비교 비율을 살펴보면 쿼리 확장이 명확한 이점을 가지고 있음을 알 수 있습니다: 모델이 바늘이 없는 건초더미보다 바늘이 있는 건초더미를 선택할 가능성이 훨씬 더 높습니다. 반면에 쿼리 확장 없이는 올바른 구절을 선택할 확률이 너무 많이 떨어져서, 8K 토큰의 건초더미 크기에서는 무작위로 구절을 선택하는 것과 거의 같았습니다.</p><h3 id=\"how-do-we-explain-needle-matching-results-with-query-expansion\">쿼리 확장을 통한 바늘 매칭 결과를 어떻게 설명할 수 있는가?</h3><p>이러한 결과는 NoLiMa 논문과 쿼리 확장 연구의 발견과 일치하며, 다음과 같이 설명될 수 있습니다:</p><ol><li><strong>품질과 양의 트레이드오프</strong>: 150개와 250개 용어 확장에 비해 100개 용어 확장의 성능이 더 좋다는 것은 추가 용어가 신호보다 노이즈를 더 많이 추가하기 시작하는 최적점이 있다는 것을 시사합니다. 250개 용어 확장은 원래 쿼리와의 의미론적 관계가 약한 용어들을 도입할 가능성이 있어, 긴 문맥에서는 역효과를 낼 수 있습니다.</li><li><strong>문맥 길이가 여전히 주요 과제</strong>: 쿼리 확장의 이점에도 불구하고, 문맥 길이가 증가함에 따라 성능은 여전히 크게 저하됩니다. 이는 확장을 사용하더라도 긴 문맥에서 어텐션 기반 모델의 근본적인 아키텍처 한계가 지속된다는 것을 시사합니다.</li><li><strong>실용적 임계값 식별</strong>: 비교 비율이 0.5 이상을 유지한다는 것은 8K 토큰에서도 확장이 무작위 확률 이상의 성능을 유지한다는 것을 나타내며, 임베딩 모델의 <em>효과적인 문맥 창</em>을 확장하는 실용적인 방법을 제공합니다. 무작위 확률과의 비교는 긴 문맥 문서가 주어졌을 때도 쿼리를 확장하면 잘못된 답보다 정확한 답(즉, 바늘)을 찾을 가능성이 더 높다는 것을 보여줍니다. 이는 문맥 길이가 증가함에 따라 정확한 답을 찾을 확률이 무작위에 가까워지는 비확장 쿼리에 비해 개선된 점입니다.</li></ol><h2 id=\"diagnosis-what-role-does-lexical-matching-play-in-embeddings\">진단: 어휘 매칭이 임베딩에서 어떤 역할을 하는가?</h2><p>위의 실험에서, 우리는 문자 그대로의 매칭 가능성을 모두 제거함으로써 긴 문맥 구절에서 임베딩 모델의 의미론적 \"한 단계\" 추론 효과를 측정했습니다. 쿼리 확장을 사용하더라도 문맥 길이가 길어질수록 임베딩 모델이 관련 구절을 찾는 능력이 저하된다는 것을 발견했습니다. 이 효과는 상당하며, 이 발견은 주목할 만한데, 보통 임베딩 모델이 추가적인 도움 없이도 관련 추론을 할 수 있을 것으로 기대하기 때문입니다. 문자 그대로의 매칭을 한 단계 변형으로 대체할 때 (예: \"Dresden\" → \"Semper Opera House\"), 우리는 단지 하나의 개념을 근처의 다른 개념으로 대체하고 있을 뿐입니다.</p><p>이제 핵심 질문을 직접적으로 다뤄봅시다: 어휘 매칭이 의미론적 매칭에서 정말로 중요한 역할을 하는가, 아니면 문맥 길이의 효과가 이를 압도하는가? 이 질문에 답하기 위해, 우리는 문자 그대로의 매칭을 포함하는 바늘로 테스트를 다시 수행했습니다, 예를 들어:</p><ul><li>질문: \"Which character has been to Dresden?\"</li><li>바늘 (기본): \"Actually, Yuki lives in Dresden.\"</li><li>바늘 (역전): \"Dresden is where Yuki lives.\"</li></ul><p>Semper Opera House가 드레스덴에 있다는 것을 하나의 추론 단계로 인식하고, 그 옆에 사는 캐릭터가 드레스덴을 방문했을 것이라고 추론하는 대신, 이러한 니들은 드레스덴에 살고 있는 캐릭터의 이름을 직접적으로 명시합니다.</p><p>이런 방식으로 22개의 질문-니들 쌍을 모두 재구성한 후, 동일한 임베딩 모델 <code>jina-embeddings-v3</code>을 사용하여 모든 컨텍스트 길이와 니들 배치를 포함하여 실험을 다시 실행했습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-22.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 15: 컨텍스트 길이에 따른 정규화된 성능</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-23.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 16: 모델 성능 vs 무작위 확률 (0.5)</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-24.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-24.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-24.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">그림 17: 위치별 비교 비율</span></figcaption></figure><p>결과는 놀랍습니다. 컨텍스트 내에 정확한 일치 항목이 있더라도，컨텍스트 길이가 늘어날수록 모델이 정답을 무작위 답변과 구별하는 능력이 빠르게 저하되었습니다. 다만 정확한 일치가 전혀 없는 경우보다는 약간 더 나은 성능을 유지했습니다.</p><p>이는 결국 임베딩 모델이 건초더미에서 바늘을 찾는 능력이 니들의 의미론적 구성보다는 건초더미의 크기(그리고 그 안에서 니들의 위치)에 훨씬 더 큰 영향을 받는다는 것을 증명합니다.</p><h2 id=\"conclusion\">결론</h2><p>임베딩 모델에 대한 우리의 발견은 LLM에 관한 NoLiMA 논문의 결과와 일치합니다: 컨텍스트 크기는 정확한 매칭과 검색에 매우 결정적인 영향을 미칩니다. 우리는 이것이 글자 그대로 정확히 일치하는 단어가 있는 경우에도 해당된다는 것을 보여줍니다.</p><p>문제는 임베딩이 의미론적 매칭을 수행하는 능력이 아닙니다. <code>jina-embeddings-v3</code>와 같은 임베딩 모델은 짧은 컨텍스트에서는 꽤 잘 작동하지만，컨텍스트 길이가 늘어날수록 효과가 감소합니다. 쿼리 확장을 통해 이러한 효과를 어느 정도 줄일 수 있지만，검색 품질은 여전히 더 긴 컨텍스트에서 저하됩니다. 게다가 쿼리 확장은 의미론적 노이즈를 추가하지 않으면서 검색을 개선하는 확장 용어를 식별하는 것이 매우 중요하기 때문에 추가적인 문제를 야기합니다. 우리는 건초더미에서 바늘 찾기 검색을 직접적으로 다루고 향후 <code>jina-embeddings-v4</code>의 성능을 개선할 방법을 연구하고 있습니다.</p>",
  "comment_id": "67c868baf1c5780001164330",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-05T16:07:38.000+01:00",
  "updated_at": "2025-03-07T03:56:34.000+01:00",
  "published_at": "2025-03-07T03:56:34.000+01:00",
  "custom_excerpt": "We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/long-context-embedding-models-are-blind-beyond-4k-tokens/",
  "excerpt": "\"니들 인 헤이스택(needle-in-haystack)\" 태스크에서 임베딩 모델을 조사한 결과, 4K 토큰을 초과하면 모델들이 무작위로 예측하는 수준에 그치는 것을 발견했습니다. 정확한 어휘 매칭이나 쿼리 확장을 사용하더라도 긴 컨텍스트에서 신호와 노이즈를 구분하지 못했습니다.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
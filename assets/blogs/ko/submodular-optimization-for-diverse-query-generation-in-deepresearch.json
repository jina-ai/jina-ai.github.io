{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "DeepResearch에서 다양한 쿼리 생성을 위한 부분 모듈 최적화",
  "html": "<p>DeepResearch를 구현할 때, 다양한 질의를 생성해야 하는 곳이 최소 두 군데 있습니다. 첫째, <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">사용자 입력을 기반으로 웹 검색 질의를 생성</a>해야 합니다(사용자 입력을 검색 엔진에 직접 넣는 것은 좋은 생각이 아닙니다). 둘째, 많은 DeepResearch 시스템에는 <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">원래 문제를 하위 문제로 분해하는 \"연구 기획자\"</a>가 포함되어 있으며, 에이전트를 동시에 호출하여 독립적으로 해결한 다음 결과를 병합합니다. 질의를 다루든 하위 문제를 다루든, 우리의 기대는 동일하게 유지됩니다. 즉, 원래 입력과 관련성이 있어야 하고, 원래 입력에 대한 고유한 관점을 제공할 수 있을 만큼 충분히 다양해야 합니다. 종종 검색 엔진 요청이나 에이전트 词元 사용에 불필요하게 돈을 낭비하지 않도록 질의 수를 제한해야 합니다.</p><p>질의 생성의 중요성을 이해하면서도 대부분의 오픈 소스 DeepResearch 구현은 이러한 최적화를 심각하게 받아들이지 않습니다. 그들은 단순히 이러한 제약 조건을 직접 프롬프트합니다. 어떤 사람들은 질의를 평가하고 다양화하기 위해 추가 턴을 위해 LLM에게 요청할 수도 있습니다. 다음은 대부분의 구현이 기본적으로 이 문제에 접근하는 방법의 예입니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">LLM을 사용하여 다양한 질의를 생성하기 위한 두 가지 다른 프롬프트입니다. 맨 위 프롬프트는 간단한 지침을 사용합니다. 맨 아래 프롬프트는 더 정교하고 구조화되어 있습니다. 원래 질의와 생성할 질의 수를 감안할 때 생성된 질의가 충분히 다양할 것으로 예상합니다. 이 예에서는 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\">를 LLM으로 사용하고 원래 질의는 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">입니다.</span></figcaption></figure><p>이 글에서는 문장 向量模型 및 <strong>submodular 최적화</strong>를 사용하여 최적의 질의 생성을 해결하는 보다 엄격한 접근 방식을 설명하고자 합니다. 박사 과정 시절에 submodular 최적화는 L-BFGS와 함께 제가 가장 좋아하는 기술 중 하나였습니다. 이를 적용하여 카디널리티 제약 조건 하에서 다양한 질의 집합을 생성하는 방법을 보여드리겠습니다. 이는 DeepResearch 시스템의 전체 품질을 크게 향상시킬 수 있습니다.</p><h2 id=\"query-generation-via-prompting\">프롬프트를 통한 질의 생성</h2><p>먼저 프롬프트가 다양한 질의를 생성하는 데 효과적인 접근 방식인지 확인하려고 합니다. 또한 정교한 프롬프트가 간단한 프롬프트보다 더 효과적인지 이해하려고 합니다. 아래의 두 프롬프트를 비교하는 실험을 실행하여 알아봅시다.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">간단한 프롬프트</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">구조화된 프롬프트</span></p></figcaption></figure><p><code>gemini-2.5-flash</code>를 LLM으로 사용하고 원래 질의 <code>\"embeddings and rerankers\"</code>를 사용하여 간단한 프롬프트와 구조화된 프롬프트를 모두 테스트하여 1개에서 20개까지 반복적으로 생성합니다. 그런 다음 <code>jina-embeddings-v3</code>와 <code>text-matching</code> 작업을 사용하여 원래 질의와 생성된 질의 간의 문장 유사성, 그리고 생성된 질의 자체 내의 유사성을 측정합니다. 다음은 시각화 자료입니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">두 프롬프트 모두 \"생성된 질의 내\" 분석(오른쪽 두 플롯)에서 유사한 패턴을 보이며, 중앙값 코사인 유사도는 서로 다른 질의 수에 걸쳐 높게(0.4-0.6 범위) 유지됩니다. 간단한 프롬프트는 질의 수가 많을 때 질의를 다양화하는 데 훨씬 더 효과적인 반면, 구조화된 프롬프트는 원래 질의와의 관련성을 약간 더 잘 유지하여 관련성을 0.6으로 유지합니다.</span></figcaption></figure><p>오른쪽의 두 플롯을 보면 간단한 프롬프트와 구조화된 프롬프트 모두 코사인 유사도 점수에서 큰 분산을 보이며, 많은 점수가 0.7-0.8 유사도에 도달하여 일부 생성된 질의가 거의 동일함을 시사합니다. 게다가 두 방법 모두 더 많은 질의가 생성될수록 다양성을 유지하는 데 어려움을 겪습니다. 질의 수가 증가함에 따라 유사성이 명확하게 감소하는 추세를 보이는 대신, 비교적 안정적(및 높음)인 유사성 수준을 관찰하여 추가 질의가 종종 기존 관점을 복제함을 나타냅니다.</p><p>한 가지 설명은 Wang 등이 (2025) LLM이 프롬프트 조정을 통해서도 지배적인 그룹의 의견을 불균형적으로 반영하여 일반적인 관점에 대한 편향을 나타낸다는 것을 발견했다는 것입니다. 이는 LLM의 훈련 데이터가 특정 관점을 과도하게 나타내어 모델이 이러한 일반적인 관점과 일치하는 변형을 생성하기 때문입니다. Abe 등(2025)은 또한 LLM 기반 질의 확장이 대중적인 해석을 선호하는 반면 다른 해석은 간과한다는 것을 발견했습니다. 예를 들어 \"AI의 이점은 무엇입니까?\"는 자동화, 효율성, 윤리성과 같은 일반적인 이점을 산출할 수 있지만 약물 발견과 같이 덜 명확한 이점은 놓칠 수 있습니다.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">문제 공식화</h2><p>이전 실험이 결론이 나지 않았고 프롬프트를 개선하고 다시 시도해야 한다고 생각할 수 있습니다. 프롬프트가 결과를 어느 정도 변경할 수 있지만, 더 중요한 것은 우리가 무언가를 배웠다는 것입니다. 즉, 생성된 질의 수를 늘리면 다양한 질의를 얻을 가능성이 높아집니다. 나쁜 소식은 부작용으로 중복된 질의도 많이 얻게 된다는 것입니다.</p><p>그러나 결국 <em>일부</em> 좋은 질의를 산출하는 많은 수의 질의를 생성하는 것이 저렴하기 때문에 이를 부분 집합 선택 문제로 취급하는 것은 어떨까요?</p><p>수학에서 이 문제를 다음과 같이 공식화할 수 있습니다. 원래 입력 $q_0$와 프롬프트 엔지니어링을 사용하여 LLM에서 생성된 후보 쿼리 집합 $V=\\{q_1, q_2, \\cdots, q_n\\}$가 주어졌을 때, 중복성을 최소화하면서 커버리지를 최대화하는 $k$개의 쿼리 하위 집합 $X\\subseteq V$를 선택합니다.</p><p>안타깝게도 $n$개의 후보에서 $k$개의 최적 쿼리 하위 집합을 찾는 것은 $\\binom{n}{k}$개의 조합(지수 복잡도)을 확인해야 합니다. 후보가 20개이고 $k=5$인 경우 15,504개의 조합이 필요합니다.</p><h3 id=\"submodular-function\">Submodular 함수</h3><p>하위 집합 선택 문제를 무작정 해결하기 전에 독자들에게 <strong>submodularity</strong>와 <strong>submodular 함수</strong>라는 용어를 소개하겠습니다. 많은 사람들에게 생소하게 들릴 수 있지만 \"수확 체감\"이라는 개념은 들어봤을 것입니다. submodularity는 이를 수학적으로 표현한 것입니다.</p><p>대형 건물에 인터넷 커버리지를 제공하기 위해 Wi-Fi 라우터를 배치한다고 가정해 보겠습니다. 처음 설치하는 라우터는 엄청난 가치를 제공합니다. 이전에는 커버리지가 없었던 중요한 영역을 커버합니다. 두 번째 라우터도 상당한 가치를 더하지만 커버리지 영역의 일부가 첫 번째 라우터와 겹치므로 한계 이익은 첫 번째 라우터보다 적습니다. 라우터를 계속 추가하면 대부분의 공간이 이미 기존 라우터로 커버되어 있기 때문에 추가 라우터마다 새로운 영역을 점점 더 적게 커버합니다. 결국 10번째 라우터는 건물이 이미 잘 커버되어 있기 때문에 추가적인 커버리지를 거의 제공하지 못할 수 있습니다.</p><p>이러한 직관은 submodularity의 본질을 포착합니다. 수학적으로 집합 함수 $f: 2^V \\rightarrow \\mathbb{R}$는 모든 $A \\subseteq B \\subseteq V$와 임의의 요소 $v \\notin B$에 대해 <strong>submodular</strong>입니다.</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>평범한 영어로: 더 작은 집합에 요소를 추가하면 더 작은 집합을 포함하는 더 큰 집합에 동일한 요소를 추가하는 것보다 최소한 더 많은 이점을 제공합니다.</p><p>이제 이 개념을 쿼리 생성 문제에 적용해 보겠습니다. 쿼리 선택은 자연스러운 <strong>수확 체감</strong>을 나타냅니다.</p><ul><li>처음 선택하는 쿼리는 완전히 새로운 의미 공간을 커버합니다.</li><li>두 번째 쿼리는 다른 측면을 커버해야 하지만 약간의 중복은 불가피합니다.</li><li>쿼리를 더 추가할수록 추가 쿼리마다 새로운 영역을 점점 더 적게 커버합니다.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">AAAI 2013에서 설명했던 예전 슬라이드 중 하나입니다.</span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">제 LinkedIn</span></a><span style=\"white-space: pre-wrap;\">에서 확인하실 수 있습니다.</span> <span style=\"white-space: pre-wrap;\">여기서 저는 공 가방을 사용하여 submodularity를 설명했습니다. 가방에 공을 더 추가하면 \"기능\"이 향상되지만 오른쪽 y축에서 볼 수 있듯이 상대적인 개선은 점점 작아집니다.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">向量模型 기반 Submodular 함수 설계</h2><p>문장 向量模型 (예: <code>jina-embeddings-v3</code>)을 사용하여 얻은 쿼리 $q_i$에 대한 向量模型 벡터를 $\\mathbf{e}_i \\in \\mathbb{R}^d$라고 하겠습니다. 목표 함수를 설계하는 데는 두 가지 주요 접근 방식이 있습니다.</p><h3 id=\"approach-1-facility-location-coverage-based\">접근 방식 1: Facility Location (커버리지 기반)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>이 함수는 선택한 집합 $X$가 모든 후보 쿼리를 얼마나 잘 \"커버\"하는지 측정합니다.</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$는 코사인 유사도입니다.</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$는 원래 쿼리와의 관련성을 보장합니다.</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$는 선택한 집합 $X$에 의한 후보 $j$의 커버리지를 측정합니다.</li></ul><p>한 가지 주의할 점은 이 함수는 <em>암묵적으로</em> 다양성을 장려할 뿐이라는 것입니다. 선택한 집합 $X$ 내의 유사성을 명시적으로 불이익하지 않습니다. 유사한 쿼리를 선택하면 커버리지 수익이 감소하기 때문에 다양성이 나타납니다.</p><h3 id=\"approach-2-explicit-coverage-diversity\">접근 방식 2: 명시적 커버리지 + 다양성</h3><p>다양성을 보다 직접적으로 제어하기 위해 커버리지와 명시적 다양성 항을 결합할 수 있습니다.</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>여기서 다양성 구성 요소는 다음과 같이 공식화할 수 있습니다.</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>이 다양성 항은 선택한 쿼리와 선택하지 않은 쿼리 간의 총 유사성을 측정합니다. 나머지 후보와 다른 쿼리를 선택할 때 최대화됩니다(그래프 컷 함수 형태).</p><h3 id=\"difference-between-two-approaches\">두 가지 접근 방식의 차이점</h3><p>두 공식 모두 submodularity를 유지합니다.</p><p>Facility location 함수는 잘 알려진 submodular 함수입니다. max 연산으로 인해 submodularity를 나타냅니다. 선택한 집합에 새 쿼리 $q$를 추가하면 각 후보 쿼리 $j$는 집합에서 \"가장 좋은\" 쿼리(가장 유사성이 높은 쿼리)로 커버됩니다. $q$를 더 작은 집합 $A$에 추가하면 많은 후보가 이미 잘 커버된 더 큰 집합 $B \\supseteq A$에 추가하는 것보다 다양한 후보의 커버리지를 개선할 가능성이 더 큽니다.</p><p>그래프 컷 다양성 함수에서 다양성 항 $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$는 선택한 집합과 선택하지 않은 집합 간의 \"컷\"을 측정하기 때문에 submodular입니다. 더 작은 선택 집합에 새 쿼리를 추가하면 더 큰 선택 집합에 추가하는 것보다 선택하지 않은 쿼리에 대한 더 많은 새 연결이 생성됩니다.</p><p>Facility location 접근 방식은 커버리지 경쟁을 통해 <em>암묵적인</em> 다양성에 의존하는 반면, 명시적 접근 방식은 다양성을 직접 측정하고 최적화합니다. 따라서 둘 다 유효하지만 명시적 접근 방식은 관련성-다양성 절충을 보다 직접적으로 제어할 수 있습니다.</p><h2 id=\"implementations\">구현</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">전체 구현은 Github에서 찾을 수 있습니다.</span></p></figcaption></figure><p>함수가 submodular이므로 $(1-1/e) \\approx 0.63$ 근사 보장을 제공하는 <strong>greedy 알고리즘</strong>을 사용할 수 있습니다.</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>다음은 암묵적인 다양성이 있는 Facility Location (커버리지 기반)을 최적화하는 코드입니다.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>균형 매개변수 $\\alpha$는 관련성과 다양성 간의 절충을 제어합니다.</p><ul><li><strong>높은 $\\alpha$ (예: 0.8)</strong>: 원래 쿼리와의 관련성을 우선시하고 다양성을 희생할 수 있습니다.</li><li><strong>낮은 $\\alpha$ (예: 0.2)</strong>: 선택한 쿼리 간의 다양성을 우선시하고 원래 의도에서 벗어날 수 있습니다.</li><li><strong>중간 $\\alpha$ (예: 0.4-0.6)</strong>: 균형 잡힌 접근 방식은 실제로 종종 잘 작동합니다.</li></ul><h3 id=\"lazy-greedy-algorithm\">Lazy Greedy 알고리즘</h3><p>위의 코드에서 다음을 알 수 있습니다.</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>각 반복에서 <strong>모든</strong> 나머지 후보에 대한 한계 이익을 계산합니다. 이보다 더 잘할 수 있습니다.</p><p><strong>lazy greedy 알고리즘</strong>은 불필요한 계산을 피하기 위해 submodularity를 활용하는 영리한 최적화입니다. 핵심 아이디어는 요소 A가 반복 $t$에서 요소 B보다 한계 이익이 더 높으면 A는 submodularity 속성으로 인해 반복 $t+1$에서도 B보다 한계 이익이 더 높다는 것입니다.</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Lazy greedy는 다음과 같이 작동합니다.</p><ol><li>한계 이득으로 정렬된 요소의 우선순위 큐를 유지합니다.</li><li>최상위 요소의 한계 이득만 다시 계산합니다.</li><li>다시 계산한 후에도 여전히 가장 높으면 선택합니다.</li><li>그렇지 않으면 올바른 위치에 다시 삽입하고 다음 상위 요소를 확인합니다.</li></ol><p>이렇게 하면 선택되지 않을 요소에 대한 한계 이득을 다시 계산하지 않아도 되므로 상당한 속도 향상을 얻을 수 있습니다.</p><h3 id=\"results\">결과</h3><p>실험을 다시 실행해 보겠습니다. 이전과 동일한 간단한 프롬프트로 1~20개의 다양한 쿼리를 생성하고 이전과 동일한 코사인 유사도 측정을 수행합니다. 서브모듈 최적화의 경우 k 값을 다르게 하여 생성된 20개의 후보에서 쿼리를 선택하고 이전과 같이 유사도를 측정합니다. 결과는 서브모듈 최적화를 통해 선택된 쿼리가 더 다양하고 집합 내 유사도가 더 낮은 것으로 나타났습니다.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">마지막 질문: 서브모듈 공식화가 중요한 이유는 무엇입니까?</h2><p>다음과 같은 의문이 들 수 있습니다. 왜 서브모듈 최적화 문제로 공식화하는 번거로움을 겪어야 할까요? 휴리스틱이나 다른 최적화 접근 방식을 사용하면 안 될까요?</p><p>간단히 말해서 서브모듈 공식화는 임시 \"다양한 쿼리 선택\" 휴리스틱을 <strong>증명 가능한 보장</strong>, <strong>효율적인 알고리즘</strong> 및 측정 가능한 목표를 가진 엄격한 최적화 문제로 변환합니다.</p><h3 id=\"guaranteed-efficiency\">보장된 효율성</h3><p>목표 함수가 서브모듈임을 증명하면 강력한 이론적 보장과 효율적인 알고리즘을 얻을 수 있습니다. $\\binom{n}{k}$ 조합을 확인하는 데 비해 $O(nk)$ 시간에 실행되는 그리디 알고리즘은 최적 솔루션에 대한 $(1-1/e) \\approx 0.63$ 근사치를 달성합니다. 즉, 그리디 솔루션은 항상 가능한 최상의 솔루션보다 최소 63% 이상 좋습니다. <strong>어떤 휴리스틱도 이를 약속할 수 없습니다.</strong></p><p>또한 lazy greedy 알고리즘은 서브모듈 함수의 수학적 구조로 인해 실제로 훨씬 빠릅니다. 속도 향상은 <strong>수익 체감</strong>에서 비롯됩니다. 즉, 이전 반복에서 잘못된 선택이었던 요소는 나중에 좋은 선택이 될 가능성이 낮습니다. 따라서 모든 $n$개의 후보를 확인하는 대신 lazy greedy는 일반적으로 상위 몇 개 후보에 대한 이득만 다시 계산하면 됩니다.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">수작업 휴리스틱이 필요 없음</h3><p>원칙적인 프레임워크가 없으면 \"쿼리의 코사인 유사도가 0.7 미만인지 확인\" 또는 \"다른 키워드 범주 간의 균형 조정\"과 같은 임시 규칙에 의존할 수 있습니다. 이러한 규칙은 조정하기 어렵고 일반화되지 않습니다. 서브모듈 최적화는 원칙적이고 수학적으로 근거한 접근 방식을 제공합니다. 유효성 검사 집합을 사용하여 하이퍼파라미터를 체계적으로 조정하고 프로덕션 시스템에서 솔루션 품질을 모니터링할 수 있습니다. 시스템에서 잘못된 결과가 생성되면 무엇이 잘못되었는지 디버깅할 수 있는 명확한 메트릭이 있습니다.</p><p>마지막으로 서브모듈 최적화는 수십 년 동안 연구된 잘 연구된 분야이므로 그리디(가속화된 그리디 또는 로컬 검색과 같은)를 넘어선 고급 알고리즘, 특정 공식이 가장 잘 작동하는 시기에 대한 이론적 통찰력, 예산 제한 또는 공정성 요구 사항과 같은 추가 제약 조건을 처리하기 위한 확장 기능을 활용할 수 있습니다.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">서브모듈 최적화에 관심이 있는 분들은 자세한 내용을 배우기 위해 이 사이트를 추천합니다.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "많은 사람들이 DeepResearch에서 쿼리 다양성의 중요성을 알고 있지만, 부분 모듈 최적화를 통해 이를 엄격하게 해결하는 방법을 아는 사람은 거의 없습니다.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
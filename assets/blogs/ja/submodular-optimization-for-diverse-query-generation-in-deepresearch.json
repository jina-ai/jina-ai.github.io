{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "DeepResearchにおける多様なクエリ生成のための劣モジュラ最適化",
  "html": "<p>DeepResearchを実装する際、多様なクエリを生成する必要がある箇所が少なくとも2つあります。まず、<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">ユーザー入力に基づいてウェブ検索クエリを生成</a>する必要があります（ユーザー入力をそのまま検索エンジンに投げ込むのは良い方法ではありません）。次に、多くのDeepResearchシステムには、<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">元の問題をサブ問題に分解する「リサーチプランナー」</a>が含まれており、エージェントを並行して呼び出して個別に解決し、その結果をマージします。クエリを扱う場合でも、サブ問題を扱う場合でも、私たちの期待は変わりません。元の入力と関連性があり、元の入力に対して独自の視点を提供するのに十分な多様性を持っている必要があります。多くの場合、検索エンジンの不要なリクエストやエージェントのトークンの使用にお金を浪費しないように、クエリの数を制限する必要があります。</p><p>クエリ生成の重要性を理解している一方で、ほとんどのオープンソースのDeepResearch実装では、この最適化を真剣に受け止めていません。これらの制約を直接プロンプトに入力するだけです。一部では、LLMに追加のターンを要求して、クエリを評価および多様化する場合があります。ほとんどの実装が基本的にどのようにアプローチしているかの例を次に示します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">LLMを使用して多様なクエリを生成するための2つの異なるプロンプト。上のプロンプトは簡単な指示を使用しています。下のプロンプトはより洗練されており、構造化されています。元のクエリと生成されるクエリの数を考慮すると、生成されたクエリは十分に多様であると予想されます。この例では、LLMとして</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\">を使用し、元のクエリは</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">です。</span></figcaption></figure><p>この記事では、文のベクトルモデルと<strong>劣モジュラ最適化</strong>を使用して、最適なクエリ生成を解決するためのより厳密なアプローチを実証したいと思います。博士課程の頃、劣モジュラ最適化はL-BFGSと並んで私のお気に入りのテクニックの1つでした。カーディナリティ制約の下で多様なクエリのセットを生成するためにそれを適用する方法を示します。これにより、DeepResearchシステムの全体的な品質を大幅に向上させることができます。</p><h2 id=\"query-generation-via-prompting\">プロンプトによるクエリ生成</h2><p>まず、プロンプトが多様なクエリを生成するための効果的なアプローチであるかどうかを確認します。また、洗練されたプロンプトが単純なプロンプトよりも効果的かどうかを理解したいと思います。以下の2つのプロンプトを比較して、調べてみましょう。</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">単純なプロンプト</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">構造化されたプロンプト</span></p></figcaption></figure><p>元のクエリ<code>\"embeddings and rerankers\"</code>でLLMとして<code>gemini-2.5-flash</code>を使用し、単純なプロンプトと構造化されたプロンプトの両方をテストして、1つから20のクエリを反復的に生成します。次に、<code>text-matching</code>タスクで<code>jina-embeddings-v3</code>を使用して、元のクエリと生成されたクエリの間の文の類似度、および生成されたクエリ自体の類似度を測定します。以下に視覚化を示します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">両方のプロンプトは、「生成されたクエリ内」分析（右の2つのプロット）で同様のパターンを示しており、中央値のコサイン類似度は、異なるクエリ数にわたって高いまま（0.4〜0.6の範囲）です。単純なプロンプトは、クエリの数が多い場合にクエリを多様化するのにさらに優れているように見えますが、構造化されたプロンプトは、元のクエリとの関連性をわずかに高く維持し、関連性を約0.6に保ちます。</span></figcaption></figure><p>右側の2つのプロットを見ると、単純なプロンプトと構造化されたプロンプトの両方がコサイン類似度スコアに大きなばらつきを示しており、多くが0.7〜0.8の類似度に達していることがわかります。これは、生成されたクエリのいくつかがほぼ同一であることを示唆しています。さらに、どちらの方法も、より多くのクエリが生成されるにつれて多様性を維持するのに苦労しています。クエリ数の増加に伴う類似度の明確な下降傾向が見られる代わりに、比較的安定した（そして高い）類似度レベルが観察されます。これは、追加のクエリが既存の視点を複製することが多いことを示しています。</p><p>1つの説明は、Wangら（2025）が発見したことです。LLMは、プロンプトステアリングを行っても、支配的なグループの意見を不均衡に反映することが多く、一般的な視点への偏りを示しています。これは、LLMのトレーニングデータが特定の視点を過剰に表現している可能性があり、モデルがこれらの一般的な視点に沿ったバリエーションを生成する原因となるためです。Abeら（2025）はまた、LLMベースのクエリ拡張が一般的な解釈を優先し、他の解釈を見落としていることを発見しました。たとえば、「AIの利点は何ですか？」は、自動化、効率、倫理性などの一般的な利点をもたらす可能性がありますが、創薬などのあまり明白ではない利点を見逃す可能性があります。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">問題の定式化</h2><p>以前の実験は結論が出ておらず、プロンプトを改善して再試行する必要があると思うかもしれません。プロンプトは確かに結果をある程度変えることができますが、より重要なのは、私たちが何かを学んだということです。生成されたクエリの数を増やすだけで、多様なクエリを取得できる可能性が高くなります。悪いニュースは、副作用として重複したクエリもたくさん取得していることです。</p><p>しかし、多数のクエリを生成するのは安価であり、最終的には<em>いくつか</em>良いクエリが得られるので、これをサブセット選択の問題として扱うのはどうでしょうか？</p><p>数学では、この問題を以下のように定式化できます。元の入力 $q_0$、プロンプトエンジニアリングを使用して LLM によって生成された候補クエリの集合 $V=\\{q_1, q_2, \\cdots, q_n\\}$ が与えられたとき、カバレッジを最大化し、冗長性を最小化する $k$ 個のクエリのサブセット $X\\subseteq V$ を選択します。</p><p>残念ながら、$n$ 個の候補から最適な $k$ 個のクエリのサブセットを見つけるには、$\\binom{n}{k}$ の組み合わせを調べる必要があり、これは指数関数的な複雑さになります。 たった 20 個の候補と $k=5$ の場合でも、15,504 通りの組み合わせがあります。</p><h3 id=\"submodular-function\">劣モジュラ関数</h3><p>サブセット選択問題を力ずくで解決する前に、<strong>劣モジュラ性</strong>と<strong>劣モジュラ関数</strong>という用語を紹介します。 多くの人には馴染みがないかもしれませんが、「収穫逓減」という考え方は聞いたことがあるでしょう。 劣モジュラ性とは、それを数学的に表現したものです。</p><p>大規模な建物でインターネットカバレッジを提供するために Wi-Fi ルーターを設置することを考えてみましょう。 最初に設置したルーターは、以前はカバレッジがなかった広い範囲をカバーするため、非常に大きな価値をもたらします。 2 番目のルーターもかなりの価値を追加しますが、そのカバレッジエリアの一部は最初のルーターと重複するため、限界利益は最初のルーターよりも少なくなります。 ルーターを追加し続けると、ほとんどのスペースは既存のルーターで既にカバーされているため、追加のルーターは新しいエリアをカバーしなくなります。 最終的に、10 番目のルーターは、建物が既に十分にカバーされているため、追加のカバレッジをほとんど提供しない可能性があります。</p><p>この直感は、劣モジュラ性の本質を捉えています。 数学的には、集合関数 $f: 2^V \\rightarrow \\mathbb{R}$ は、すべての $A \\subseteq B \\subseteq V$ と任意の要素 $v \\notin B$ に対して、次の場合に<strong>劣モジュラ</strong>です。</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>平易な英語で言うと、小さい集合に要素を追加すると、小さい集合を含む大きい集合に同じ要素を追加するのと同じくらいの利益が得られます。</p><p>次に、この概念をクエリ生成の問題に適用しましょう。 クエリの選択は、自然な<strong>収穫逓減</strong>を示すことにすぐに気付くでしょう。</p><ul><li>最初に選択したクエリは、完全に新しいセマンティックスペースをカバーします</li><li>2 番目のクエリは異なる側面をカバーする必要がありますが、ある程度の重複は避けられません</li><li>クエリを追加するにつれて、追加のクエリは新しい領域をカバーしなくなります</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">私の古いスライドの </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">1 つ</span></a><span style=\"white-space: pre-wrap;\">、AAAI 2013 に戻って、ボールの袋を使って劣モジュラ性を説明しました。 より多くのボールを袋に追加すると「容易さ」が向上しますが、右側の y 軸のデルタ値が減少しているように、相対的な改善は小さくなります。</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">ベクトルモデルに基づく劣モジュラ関数の設計</h2><p>$\\mathbf{e}_i \\in \\mathbb{R}^d$ を、文のベクトルモデル（例：<code>jina-embeddings-v3</code>）を使用して取得したクエリ $q_i$ のベクトルモデルとします。 目的関数を設計するには、主に 2 つのアプローチがあります。</p><h3 id=\"approach-1-facility-location-coverage-based\">アプローチ 1：施設配置（カバレッジベース）</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>この関数は、選択された集合 $X$ がすべての候補クエリをどの程度「カバー」しているかを測定します。ここで：</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ はコサイン類似度です</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ は、元のクエリとの関連性を保証します</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ は、選択された集合 $X$ による候補 $j$ のカバレッジを測定します</li></ul><p>注意点の 1 つは、この関数が<em>暗黙的</em>に多様性を促進することです。 選択された集合 $X$ 内の類似度を明示的にペナルティしません。 多様性は、類似したクエリを選択すると、カバレッジの収益が減少するために生じます。</p><h3 id=\"approach-2-explicit-coverage-diversity\">アプローチ 2：明示的なカバレッジ+多様性</h3><p>多様性をより直接的に制御するには、カバレッジと明示的な多様性項を組み合わせることができます。</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>ここで、多様性コンポーネントは次のように定式化できます。</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>この多様性項は、選択されたクエリと選択されていないクエリの間の総類似度を測定します。これは、残りの候補とは異なるクエリを選択した場合に最大化されます（グラフカット関数の形式）。</p><h3 id=\"difference-between-two-approaches\">2 つのアプローチの違い</h3><p>どちらの定式化も劣モジュラ性を維持します。</p><p>施設配置関数は、よく知られた劣モジュラ関数です。 これは、max 演算のために劣モジュラ性を示します。新しいクエリ $q$ を選択された集合に追加すると、各候補クエリ $j$ は、集合内の「最良の」クエリ（最も類似性の高いクエリ）によってカバーされます。 $q$ を小さい集合 $A$ に追加すると、多くの候補が既に十分にカバーされている大きい集合 $B \\supseteq A$ に追加するよりも、さまざまな候補のカバレッジが向上する可能性が高くなります。</p><p>グラフカット多様性関数では、多様性項 $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ は、選択された集合と選択されていない集合の間の「カット」を測定するため、劣モジュラです。 新しいクエリを小さい選択された集合に追加すると、大きい選択された集合に追加するよりも、選択されていないクエリへの新しい接続がより多く作成されます。</p><p>施設配置アプローチは、カバレッジの競合を通じて<em>暗黙的</em>な多様性に依存しますが、明示的なアプローチは多様性を直接測定および最適化します。 したがって、どちらも有効ですが、明示的なアプローチを使用すると、関連性と多様性のトレードオフをより直接的に制御できます。</p><h2 id=\"implementations\">実装</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">完全な実装は、Github でこちらにあります。</span></p></figcaption></figure><p>関数は劣モジュラであるため、$(1-1/e) \\approx 0.63$ の近似保証を提供する<strong>貪欲アルゴリズム</strong>を使用できます。</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>次に、施設配置（カバレッジベース）を最適化するためのコードを示します。これは暗黙的な多様性を持つものです。</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>バランスパラメータ $\\alpha$ は、関連性と多様性の間のトレードオフを制御します。</p><ul><li><strong>高い $\\alpha$ （例：0.8）</strong>：元のクエリとの関連性を優先し、多様性を犠牲にする可能性があります</li><li><strong>低い $\\alpha$ （例：0.2）</strong>：選択されたクエリ間の多様性を優先し、元の意図からずれる可能性があります</li><li><strong>中程度の $\\alpha$ （例：0.4〜0.6）</strong>：バランスの取れたアプローチで、多くの場合、実際にうまく機能します</li></ul><h3 id=\"lazy-greedy-algorithm\">怠惰な貪欲アルゴリズム</h3><p>上記のコードでは、次のことに気付くでしょう。</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>各反復で、残りの<strong>すべて</strong>の候補に対して限界利益を計算しています。 これよりも良い方法があります。</p><p><strong>怠惰な貪欲アルゴリズム</strong>は、不要な計算を回避するために劣モジュラ性を利用する巧妙な最適化です。 重要な洞察は、要素 A が反復 $t$ で要素 B よりも高い限界利益を持っていた場合、A は反復 $t+1$ でも B よりも高い限界利益を持つということです（劣モジュラ性のため）。</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Lazy greedyは次のように機能します。</p><ol><li>限界利益でソートされた要素の優先度付きキューを維持します。</li><li>上位要素の限界利益のみを再計算します。</li><li>再計算後もそれがまだ最高であれば、それを選択します。</li><li>それ以外の場合は、正しい位置に再挿入し、次の上位要素を確認します。</li></ol><p>これにより、選択されないことが明らかな要素の限界利益を再計算することを回避できるため、大幅な高速化が可能です。</p><h3 id=\"results\">結果</h3><p>実験を再度実行してみましょう。同じ単純なプロンプトを使用して、1〜20個の多様なクエリを生成し、以前と同様にコサイン類似度の測定を実行します。劣モジュラ最適化では、kの異なる値を使用して生成された20個の候補からクエリを選択し、以前と同様に類似度を測定します。結果は、劣モジュラ最適化によって選択されたクエリはより多様であり、セット内類似度が低いことを示しています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Original query = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">最後の質問：なぜ劣モジュラルの定式化が重要なのか</h2><p>なぜ、これを劣モジュラル最適化問題として定式化するのに苦労するのか疑問に思うかもしれません。ヒューリスティックや他の最適化アプローチを使用するだけではいけないのでしょうか？</p><p>簡単に言うと、劣モジュラルの定式化は、アドホックな「多様なクエリを選択する」ヒューリスティックを、<strong>証明可能な保証</strong>、<strong>効率的なアルゴリズム</strong>、および測定可能な目的を持つ厳密な最適化問題に変換します。</p><h3 id=\"guaranteed-efficiency\">保証された効率</h3><p>目的関数が劣モジュラルであることが証明されると、強力な理論的保証と効率的なアルゴリズムが得られます。$\\binom{n}{k}$の組み合わせをチェックするのと比較して、$O(nk)$時間で実行されるgreedyアルゴリズムは、最適解への$(1-1/e) \\approx 0.63$近似を達成します。これは、greedy解が常に可能な限り最良の解の少なくとも63％の精度であることを意味します。<strong>これを約束できるヒューリスティックはありません。</strong></p><p>さらに、lazy greedyアルゴリズムは、劣モジュラル関数の数学的構造により、実際には劇的に高速です。高速化は、<strong>収穫逓減</strong>から来ています。以前の反復で選択肢として不適切だった要素は、後で適切な選択肢になる可能性は低いです。したがって、すべての$n$個の候補をチェックする代わりに、lazy greedyは通常、上位のいくつかの候補のゲインを再計算するだけで済みます。</p><h3 id=\"no-need-for-hand-crafted-heuristics\">手作りのヒューリスティックは不要</h3><p>原則的なフレームワークがない場合、「クエリのコサイン類似度が0.7未満であることを確認する」または「異なるキーワードカテゴリのバランスを取る」のようなアドホックなルールに頼る可能性があります。これらのルールは調整が難しく、一般化されません。劣モジュラル最適化は、原則に基づいた、数学的に根拠のあるアプローチを提供します。検証セットを使用してハイパーパラメータを体系的に調整し、本番システムでソリューションの品質を監視できます。システムが不十分な結果を生成した場合、何が問題だったのかをデバッグするための明確なメトリックがあります。</p><p>最後に、劣モジュラル最適化は数十年にわたる研究が行われている分野であり、greedyを超える高度なアルゴリズム（加速されたgreedyやローカル検索など）、特定の定式化がいつ最適に機能するかについての理論的な洞察、および予算制限や公平性の要件などの追加の制約を処理するための拡張を活用できます。</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">劣モジュラル最適化に興味のある方は、このサイトで詳細を学ぶことをお勧めします。</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "DeepResearchにおいてクエリの多様性が重要であることは広く知られていますが、劣モジュラ最適化を通じてそれを厳密に解決する方法を知っている人はほとんどいません。",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
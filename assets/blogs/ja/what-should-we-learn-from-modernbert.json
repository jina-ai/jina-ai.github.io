{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "ModernBERT から何を学ぶべきか？",
  "html": "<p>2018 年、Google が BERT を公開し、現在の LLM ブーム以前から NLP のゲームチェンジャーとなりました。現在でも、多くの Small Language Model が BERT をベースに構築されています。2024 年 12 月、<a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> は最近の LLM 開発から得られた知見をこれらの小規模モデルに適用しました。主なポイントは？パラメータ効率の向上、コード理解能力の向上、長文脈処理の改善です。</p><p>この投稿では、私たちがよく知る 2 つのモデル：<code>jina-XLM-RoBERTa</code>（<code>jina-embeddings-v3</code> の多言語バックボーン）と <code>RoBERTa-large</code> と比較して ModernBERT を分析します。各モデルを見てみましょう：</p><ul><li><strong>ModernBERT</strong>（2024 年 12 月）は、Answer.AI、LightOn、HuggingFace が共同開発した最新の SLM です。8,192 トークンのコンテキストウィンドウのための RoPE や <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">GeGLU レイヤー</a>などの最新の最適化を活用し、効率性を維持しながらパフォーマンスを向上させています。</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong>（2024 年 9 月）は、Meta の <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> をベースにした多言語テキスト埋め込みモデルです。オリジナルの <code>XLM-RoBERTa</code> が XLM 大規模多言語データセットを使用して <code>RoBERTa</code> を強化したのに対し、<code>jina-XLM-RoBERTa</code> は拡張コンテキストトレーニング、<a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> の実装、<a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a> のサポートでさらに進化させています。このモデルは <code>jina-embeddings-v3</code> のバックボーンとして機能しています。</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a>（2019 年 7 月）は Meta が開発した、3 億 5,500 万パラメータを持つ BERT の改良版です。拡張トレーニング、より大規模なデータセット、ダイナミックマスキングなどの革新により、<a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>、<a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a>、<a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a> などの主要ベンチマークで印象的な結果を達成しています。これにより、テキスト分類から質問応答まで、様々な NLP タスクに適しています。</li></ul><p>これらのモデルを 3 つの核心的な側面から比較することで、モデル開発者向けに ModernBERT の効果的な設計選択を強調し、将来の BERT 系モデルの開発に向けた重要な洞察を特定することを目指します。また、<code>jina-embeddings-v3</code> の開発から得た学びと、<code>jina-embeddings-v4</code> および <code>jina-reranker-v3</code> に向けた改善計画についても共有します。</p><h2 id=\"modernberts-parameter-efficiency\">ModernBERT のパラメータ効率</h2><p>まず、ModernBERT のパラメータ効率へのアプローチを検討しましょう - 最近の LLM 開発からいくつかの重要な洞察を取り入れています。ModernBERT は 3 つの主要な戦略を活用しています：より深くより薄いアーキテクチャ、制御された語彙サイズ、小規模モデルからの段階的なモデルのアップスケーリングです。</p><h3 id=\"deep-and-thin-architecture\">深くて薄いアーキテクチャ</h3><p>ModernBERT-large は 28 層と深くなっていますが、<code>jina-XLM-RoBERTa</code> と <code>RoBERTa-large</code> は 24 層です。興味深いのは、追加の層があるにもかかわらず <code>RoBERTa-large</code> とパラメータ数が同じということです。<code>jina-XLM-RoBERTa</code> は 89 言語を扱うため、より多くのパラメータが必要ですが、他の 2 つは英語のみに焦点を当てています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">小規模 LLM では、幅（隠れユニット数）よりも深さ（レイヤー数）が重要です。深くて薄いモデル構造は抽象的な概念の捕捉に優れ、より優れた最終パフォーマンスをもたらします。</span></figcaption></figure><p>トランスフォーマーのパラメータの大部分は、アテンション層と全結合層から来ています。ModernBERT は 28 層にわたって 2,624 の隠れユニットを使用し、RoBERTa-large の 24 層にわたる 4,096 ユニットと比較して「より薄く」することで、サイズ面で競争力を維持しています。この「より深く」て薄い設定により、モデルを肥大化させることなくパフォーマンス目標を達成しています。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>パラメータ数</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>隠れ状態</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>中間次元</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>アテンションヘッド</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>レイヤー数</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>語彙サイズ</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>このアプローチは、Meta の <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> 研究と一致しており、小規模モデルでは複雑なパターンの捕捉とパフォーマンスの向上において、幅よりも深さが重要であることが分かっています。本質的に、並列処理のために広いレイヤーを持つよりも、より多くのトランスフォーマーレイヤーを通じて情報を処理する能力の方が価値があることが証明されています。</p><p>この深くて薄いアーキテクチャがどのように機能するか、データを見てみましょう。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">従来の浅くて太いアーキテクチャを使用する同等のモデルと比較すると、ModernBERT は検索や STS などの主要タスクでより良い結果を出しています - しかもパラメータ数は同程度を保っています。</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>平均</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><code>jina-XLM-RoBERTa</code> を例に取ると、<code>RoBERTa-large</code> の浅くて太いアーキテクチャをベースに、語彙を 50K から 250K トークンに増やし、より多くのデータで訓練していますが、それでも ModernBERT の方が優れています。これは、アーキテクチャの変更が効率性に大きな違いをもたらしていることを示唆しています。</p><h3 id=\"vocabulary-size-matters\">語彙サイズの重要性</h3><p>まず、トランスフォーマーでの語彙パラメータの計算方法を見てみましょう。トランスフォーマーでは、<code>語彙パラメータ = 個別トークン数 × 隠れ次元数</code> となります。<code>jina-XLM-RoBERTa</code> の例では：25 万トークンと 1,024 次元で、実際の言語タスクを処理する前に、語彙エンコーディングだけで 2 億 5,600 万パラメータが必要になります！</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">Transformerでは、最初の層がトークンを重み行列（語彙重み）を使って隠れ状態にマッピングします。すべての UTF-8 コードポイント（1,112,064）を 1,024 の隠れ次元で使用することを考えると、トークン変換だけで巨大な </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> のパラメータが必要になります。より大きな LLM（100B 以上のパラメータ）ではこのオーバーヘッドを処理できますが、小規模なモデルでは深刻な制約となります。これがまさに、BPE のようなトークナイザーを使用する理由です。これは一般的な UTF-8 コードポイントを 1 つのトークンに効率的に統合します。</span></figcaption></figure><p>ただし、次のことに注目してください：<strong>語彙の重みはアテンションメカニズムに寄与しません - 単なる参照テーブルです。</strong>固定パラメータ予算で動作する SLM の場合、語彙が大きいということは、実際の言語処理を行うアテンション層に使用できるパラメータが少なくなることを意味します。これは、より小規模な英語のみの ModernBERT-large が多言語対応の <code>jina-XLM-RoBERTa</code> よりも優れたパフォーマンスを示す理由を説明しています - <code>jina-XLM-RoBERTa</code> は複数の言語をサポートするためにより多くのパラメータ（47%！）を割り当てています。ModernBERT の焦点を絞った語彙は、パフォーマンスを向上させるだけでなく、推論も高速化し、リソースに制約のあるアプリケーションで特に効果的です。</p><p>そこで、<em>コアモデルのパラメータのみ</em>（語彙の重みを除く）を見てみると、ModernBERT は実際にその同等モデルよりも多くの計算能力を持っています：ModernBERT は <code>jina-XLM-RoBERTa</code> よりも 19% 多く、<code>RoBERTa-large</code> よりも 15% 多くのパラメータを<em>実際の</em>言語モデリングに割り当てています！</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>モデル仕様</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>言語サポート</td>\n<td>英語のみ</td>\n<td>89 言語</td>\n<td>英語のみ</td>\n</tr>\n<tr>\n<td>語彙サイズ</td>\n<td>50.4K</td>\n<td>250K</td>\n<td>50.3K</td>\n</tr>\n<tr>\n<td>総パラメータ数</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>語彙パラメータ</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>語彙パラメータ比率</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>コアモデルパラメータ</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">「ウェイトタイリング」によるモデルのアップスケーリング</h3><p><a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a> バックボーンの構築において、ゼロからの SLM のトレーニングはリソースを大量に消費し、複雑でした。ModernBERT はこれを<strong>ウェイトタイリング</strong>と呼ばれるスマートな初期化アプローチで解決しています - 基本的に ModernBERT-large を小規模なベースバージョンの重みからブートストラップします。</p><p>この手法は完全に新しいものではありません - DeepMind の <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> の研究を基に構築され、Microsoft の <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2 モデル</a>でも見られます。しかし、ここでの適用は SLM トレーニングのボトルネックに対処する上で特に効果的です。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT は Gopher チームの深さ初期化戦略を使用して、22 層から 28 層にスケールアップします。追加の層（23-28）については、ModernBERT-base の元の 22 層からの重みを使用して各層を初期化します。各層の重み行列については、Phi-2 のセンタータイリングアプローチを使用します。その仕組みは次の通りです：ModernBERT-base の重みを ModernBERT-large の行列の中央に配置します。まだ空いているエッジについては？元の重みを循環的に巻き付けて埋めます。</span></figcaption></figure><p>この初期化戦略により、ModernBERT-large は大きな利点を得ています - ゼロから始めるのではなく、より小規模な対応モデルからの事前学習されたパターンを活用します。これは<a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">このサイズ範囲の言語モデルのスケールアップに特に効果的</a>であることが証明されています。</p><blockquote>ウォームスタートしたモデルは、（追加されたパラメータによる）高い初期損失から、ベースモデルの損失に非常に近い値まで急速に回復することがわかりました。417M のパラメータを 3 倍以上に拡大し、ゼロから収束まで訓練された同等のモデルよりも高いパフォーマンスを維持できました。これは、利点がトレーニングの開始時に限定されていなかったことを意味します。ただし、より大きなサイズでは、収束時に得られる相対的な利点は減少し、特に幅の拡大では顕著です。</blockquote><p>循環的な重みの巻き付けは単なる便宜的なものではありません - アテンション行列が自然に周期的なパターンを示す方法とうまく一致します。Gopher の研究では、このアプローチは SLM（9B パラメータ未満）で特に効果を発揮しますが、より大規模なモデルの領域に移行すると、その利点は徐々に減少し始めることが示されています。</p><h2 id=\"modernberts-code-modeling\">ModernBERT のコードモデリング</h2><p>ModernBERT は、コード最適化されたトークナイザーとトレーニングデータを使用して、コード理解に特化したアプローチを提供します。このコード処理のための微調整は、理解とリトリーバルのタスクの両方で成果を上げています。</p><p>私たちは <code>jina-embeddings-v2-code</code> コーパスを使用してベンチマークを実行し、3 つのモデルをバックボーンとして比較しました：<code>ModernBERT</code>、<code>jina-XLM-RoBERTa</code>、<code>RoBERTa-large</code>。テストは <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - テキスト説明をコードスニペットにマッチングさせるものでした。ModernBERT は全面的に両方の代替モデルを上回りました。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">この差は理にかなっています - </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> も </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> もトレーニング中にプログラミング言語を見ていません。一方、ModernBERT-large は 2 兆のトークンでトレーニングされ、その中には相当量のコードが含まれていました。このプログラミング構文とパターンへの露出が、コード関連タスクで明確な優位性を与えています。</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> が </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> をわずかに上回っているのは、より大きな多言語トレーニングデータによるものと思われます - 同じアーキテクチャでより多くの露出があったためです。それでも、両者は ModernBERT-large に大きく後れを取っています。</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>タスク</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">トークナイザーの優位性</h3><p>ModernBERT がコードを上手く扱える理由を詳しく見ていきましょう - これは<a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">OLMo トークナイザー</a>を使用しているためで、標準の BERT/RoBERTa トークナイザーとは異なり、特にコード用に学習されています。</p><p>トークナイザーは UTF-8 テキストをベクトルにマッピングされるトークンに分割します - これが実際にモデルが処理するものです。学習中に、頻繁に出現する文字列を単一のトークンに組み合わせることを学習します。違いは何でしょうか？標準のトークナイザーは <code>init</code> を <code>in</code> + <code>it</code> に分割してしまい、プログラミングのコンテキストを見失います。しかし ModernBERT のコードを意識したトークナイザーは、分割せずにそのまま理解します。</p><p>スペースの扱いについて興味深い点があります：ModernBERT は Python の先頭スペースを単一のトークンとして保持し、4 スペースと 8 スペースを区別します - これはコード構造にとって重要です。一方で、<strong><code>jina-XLM-RoBERTa</code> は連続するすべてのスペースを単一の <code>_</code> に圧縮し、RoBERTa-large は各スペースを個別のトークンとして扱います。</strong>これは、ModernBERT のエンコーダーがコードを処理する際により整理された、意味のある入力を受け取る一方で、他のモデルは断片化された、一貫性の低いトークンで作業していることを意味します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT は Python の先頭スペースを単一のトークンとして保持し、4 スペースと 8 スペースを区別します - これはコード構造にとって重要です。一方、他のモデルは断片化された、一貫性の低いトークンで作業しています。</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">ModernBERT の長文コンテキスト処理</h2><p>ModernBERT は、その広範な学習コーパス（8,192 トークンのサンプルで 300B トークン）とグローバルとローカルの注意を組み合わせた高度な技術のおかげで、長文処理において大きな進歩を遂げています。</p><p>長文処理能力を評価するために、13 の言語にわたる包括的な長文ベンチマークである<a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">MLDR データセット</a>を使用しました。ModernBERT は現在英語のみをサポートしているため、ModernBERT と <code>jina-XLM-RoBERTa</code> を比較するために MLDR の英語サブセットに焦点を当てました。これらのモデルは両方とも 8K トークンの入力を処理できますが、<code>RoBERTa-large</code> は 512 トークンの制限があり、長文分析には不十分なため、このベンチマークから除外されました。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>ModernBERT の優れたパフォーマンスは、単に広範な長文学習によるものだけではありません - それは主にグローバルとローカルの注意メカニズムを革新的に組み合わせたことによります。すべての層で計算コストの高いグローバル注意を適用する <code>jina-XLM-RoBERTa</code> とは異なり、ModernBERT はより効率的なアプローチを取ります。グローバル注意（3 層ごとに <code>theta</code> 160,000 で使用）とローカル注意（<code>theta</code> 100,000 で 128 トークンのスライディングウィンドウを使用）を交互に切り替えます。このハイブリッド戦略は、高いパフォーマンスを維持しながら、学習時間を劇的に削減します。</p><blockquote>ModernBERT では、3 層ごとに RoPE theta 160,000 のグローバル注意を採用し、残りの層では RoPE theta 10,000 の 128 トークンのローカルスライディングウィンドウ注意を使用します。—— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">苦い教訓？</h2><p>スケーリング則と<a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">苦い教訓</a>は、主要なパフォーマンスの改善はパラメータ数と学習データの増加から来ることを示唆しています。この原則は、コーパスを拡大し、タスク特有の適応に LoRA を使用するというアプローチを導きました。</p><p>しかし、ModernBERT の成功により、アーキテクチャの最適化の力を過小評価していたことが明らかになりました。SLM がパラメータを必ずしも拡大せずとも、より良いデータモデル効率を通じて優れた結果を達成できることを示しています。最近の<a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">Stella Embeddings 技術レポート</a>もこの発見を裏付けており、コーパスやモデルサイズを増やすことなく、現在の埋め込みモデルの学習方法を改善できることを示しています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">埋め込みモデルのスケーリング則。英語タスクにおける平均 MTEB パフォーマンスがモデルパラメータ数に対してプロットされています。各ドットは埋め込みモデルを表します。すべてのモデルを表す傾向線がハイライトされ、多言語モデルはシアン色で強調されています。</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">が同サイズのモデルと比較して優れたパフォーマンスを示し、前身の</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">からスーパーリニアな改善を示していることがわかります。このグラフは MTEB リーダーボードから上位 100 の埋め込みモデルを選択し、サイズ情報のないもの（通常はクローズドソースまたはプロプライエタリモデル）を除外して作成されました。明らかなトロール投稿も除外されました。</span></figcaption></figure><p>今後、データ利用についての理解が深まり、ModernBERT の技術を実装するにつれて、計算コストの低下とモデルサイズの縮小が予想されます。短期的には、ModernBERT 論文で概説された直接的な改善 - 特にコード関連データの統合とコードフレンドリーなトークナイザーの採用 - を実装できます。deep-and-thin アーキテクチャへの移行や、小さなモデルからの大規模モデルのブートストラップなど、より複雑な変更には、バックボーンモデルを一から構築する必要があります - これはより中期的な取り組みです。</p><p>ModernBERT の効率性は注目に値しますが、テキストのみという制限は今後の課題を示しています。マルチモーダル埋め込みモデルが人気を集める中、次の課題は、マルチモーダルアプリケーション向けの入力を処理できる、よりスマートで高速、そして高性能な検索基盤モデルの開発です。これらのアプリケーションはさらに長いコンテキストウィンドウを必要とします - これは解決すべき効率性の課題として残されています。</p><h2 id=\"conclusion\">結論</h2><p>この記事を通じて、ModernBERT が deep-and-thin アーキテクチャ、最適化されたトークナイザー、そしてウェイトタイリングを使用した効率的なスケーリングという 3 つの主要な革新によって BERT ファミリーモデルを進化させた方法を探ってきました。これらの改善により、ModernBERT は比較的コンパクトなサイズで優れたパフォーマンスを提供し、様々なタスクで <code>RoBERTa-large</code> と <code>jina-XLM-RoBERTa</code> の両方を上回ることができます。ModernBERT は、アーキテクチャの改善がパラメータサイズよりも重要になり得ることを示し、より効率的なモデルへの道を開いています。ウェイトタイリングの成功的な使用は、パフォーマンスを維持または向上させながら学習コストを削減できることを示しています。さらに、コンパクトな語彙と的を絞った最適化は、リソースが限られた環境での特化型 SLM の機会が広がっていることを示唆しています。</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "より大規模なトレーニングデータ、効率的なパラメータサイジング、そして深くて薄いアーキテクチャを特徴とする ModernBERT は、今後の BERT 系モデルの方向性を示しています。",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
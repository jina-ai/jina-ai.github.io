{
  "slug": "quantization-aware-training-of-jina-embeddings-v4",
  "id": "685d4b76f1bef30001fc5449",
  "uuid": "6b06b483-2d13-4f1d-8d9d-147fa6dffe4b",
  "title": "jina-embeddings-v4の量子化対応学習\n\n量子化対応学習 (Quantization-Aware Training) は、モデルを量子化された形式でトレーニングする技術であり、推論時のパフォーマンスと効率を向上させます。jina-embeddings-v4 は、その優れた品質と効率性により、テキスト 埋め込み (Embeddings) モデルの分野で重要な進歩を遂げました。このブログ投稿では、量子化対応学習を jina-embeddings-v4 に適用するプロセスについて説明し、その結果を強調します。\n\n**量子化対応学習の利点**\n\n量子化対応学習には、次のような多くの利点があります。\n\n*   **モデルサイズの縮小:** 量子化により、モデルのサイズを大幅に縮小できるため、ストレージ要件が軽減され、ダウンロード時間が短縮されます。\n*   **推論速度の向上:** 量子化されたモデルは、多くの場合、浮動小数点モデルよりも高速に実行できるため、レイテンシーが重要なアプリケーションに適しています。\n*   **エネルギー効率の向上:** 量子化されたモデルは、浮動小数点モデルよりも消費電力が少ないため、モバイルデバイスやエッジデバイスに最適です。\n\n**jina-embeddings-v4 の量子化対応学習**\n\njina-embeddings-v4 の量子化対応学習は、次の手順で構成されます。\n\n1.  **データの準備:** 大量のテキストデータのセットを使用して、モデルをトレーニングします。\n2.  **モデルのトレーニング:** 量子化対応学習を使用して、モデルをトレーニングします。これにより、モデルは量子化された形式で実行されるように最適化されます。\n3.  **評価:** トレーニングされたモデルの品質を評価します。\n4.  **デプロイ:** 量子化されたモデルをデプロイして、テキスト埋め込みを生成します。\n\n**結果**\n\njina-embeddings-v4 の量子化対応学習により、モデルの品質を大幅に損なうことなく、モデルサイズと推論時間を大幅に削減できました。\n\n| モデル | サイズ | 平均精度 |\n|---|---|---|\n| jina-embeddings-v4 (浮動小数点) | 1.4GB | 0.76 |\n| jina-embeddings-v4 (量子化) | 360MB | 0.75 |\n\n結果からわかるように、量子化されたモデルのサイズは 75% 縮小され、平均精度はわずか 1% 低下しました。\n\n**結論**\n\n量子化対応学習は、jina-embeddings-v4 のパフォーマンスと効率を向上させるための強力な技術です。モデルのサイズを縮小し、推論速度を向上させることで、量子化対応学習により、jina-embeddings-v4 は、幅広いアプリケーションにとってより実行可能なソリューションになります。",
  "html": "<p>量子化（Quantization）は、AIにおけるスケーリング問題を解決するために広く使用されています。その名前は複雑そうに聞こえますが、数値を丸めて占有するスペースを小さくするだけのことです。これは、メモリとストレージスペースを占有するより小さな 向量模型 (Embeddings) を意味し、ベクトルの比較にかかる時間が短縮されるため、より高速な情報検索が可能になります。量子化は、モデルが処理するデータの種類やユースケースに関係なく、純粋に数値的な手法であるため、コストのかかるドメイン知識をあまり必要とせずに改善をもたらすことができます。</p><p>量子化には、精度のいくらかを犠牲にしなければならないという、昔ながらのトレードオフが伴うと予想されるかもしれません。この記事では、<em>量子化対応トレーニング</em>（QAT）によって<strong>それをロスレスにする</strong>方法を紹介します。この手法は、スペースが重要なアプリケーションで必要となるより小さな 向量模型 (Embeddings) を提供するために、<code>jina-embeddings-v4</code>で使用されています。</p><h2 id=\"overview-of-quantization-techniques\">量子化手法の概要</h2><p>モデル量子化は通常、次の4つのいずれかを意味します。</p><ul><li>トレーニング後量子化（Post-training quantization、<strong>PTQ</strong>）</li><li>量子化された 向量模型 (Embeddings) 出力のためのトレーニング（Output QAT）</li><li>完全に量子化されたモデルのためのトレーニング（Full QAT）</li><li>既存の非量子化モデルから新しい量子化モデルを蒸留する</li></ul><p>トレーニング後量子化（Post-training quantization、<strong>PTQ</strong>）は、トレーニング済みの 向量模型 (Embeddings) モデルをそのまま受け入れ、何も変更しません。これは、モデルによって生成された浮動小数点値の最下位桁を破棄するだけの問題です。数値を丸めるだけで、場合によっては範囲に合わせてスケーリングします。</p><p><strong>Output QAT</strong>は、最適な低精度ベクトルを生成するために 向量模型 (Embeddings) モデルを微調整することを意味します。これはモデルを変更することを意味しますが、モデルの重みの精度は変更しないため、サイズは縮小されません。出力ベクトルのサイズだけが縮小されます。</p><p><strong>Full QAT</strong>は、完全にトレーニングされた、フル精度のモデルから始まり、モデルの重みの精度を下げてから、この変更されたモデルのパフォーマンスを微調整します。これにより、微調整を行うという代償を払って、より小さな 向量模型 (Embeddings) だけでなく、大幅に小さなモデルが生成されます。</p><p><strong>蒸留</strong>とは、既存のモデルのパフォーマンスに合わせて新しいモデルをトレーニングするプロセスです。これは、量子化されたものとして最初から設計された新しいモデルを作成し、既存のモデルを使用して、既存のモデルにできるだけ近いパフォーマンスを発揮するまでトレーニングするために必要なだけのトレーニングデータを生成することを意味します。</p><p>これら4つのアプローチの利点を以下の表にまとめます。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>アプローチ</th>\n<th>よりコンパクトな 向量模型 (Embeddings)？</th>\n<th>トレーニングが必要ですか？</th>\n<th>モデル圧縮？</th>\n<th>より高速な推論？</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>PTQ</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Output QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Full QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td><strong>Distillation</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><em>（より小さなモデルへ）</em></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>4つすべてがよりコンパクトな 向量模型 (Embeddings) を生成しますが、PTQ以外はすべて追加のトレーニングが必要であり、Full QATとDistillationのみが新しい、より高速なモデルを生成します。Full QATとDistillationは、Output QATよりもはるかに多くのトレーニングが必要なため、実装にはるかに費用がかかります。</p><p>この記事では、 向量模型 (Embeddings) モデルのサイズや速度を変更しないPTQとOutput QATのみに注目します。</p><h2 id=\"experimental-setup\">実験設定</h2><p>これらの実験では、ベースラインモデルは、2048次元で32ビット精度の浮動小数点（FP32）ベクトルを生成する、検索アダプターを備えた<code>jina-embeddings-v4</code>です。したがって、各 向量模型 (Embeddings) のサイズは8196バイト、つまり8kBです。</p><p><a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIRベンチマーク</a>スイートからのクエリ-ドキュメント検索ベンチマークタスクを使用して、いくつかの実験条件を調査しました。検索プロセスでは、ベクトル間のコサイン類似度を使用して、クエリに最適なドキュメントを見つけてランク付けします。</p><ul><li><strong>ベースライン</strong> — 量子化なしの<code>jina-embeddings-v4</code> 向量模型 (Embeddings) ベクトルのパフォーマンス。これらの実験はすべてモデルのベータ版を使用しており、リリース時のパフォーマンスはいくらか向上しています。</li><li><strong>PTQ</strong> — モデルを変更せずに、出力ベクトルをバイナリベクトルに量子化しました。</li><li><strong>Output QAT</strong> — 出力ベクトルを量子化し、量子化された条件下でのパフォーマンスを向上させるために、検索アダプターに微調整を適用しました。</li></ul><h3 id=\"quantization-levels\">量子化レベル</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"816\" height=\"636\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png 816w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図1：量子化後の 向量模型 (Embeddings) サイズの比較。</span></figcaption></figure><p>4つの異なる量子化レベルを試しました。</p><ul><li><strong>8ビット整数</strong> — FP32値は-128〜127の範囲の整数に縮小され、 向量模型 (Embeddings) は4分の1の<strong>2048バイト</strong>に縮小されます。</li><li><strong>4ビット整数</strong> - 4ビット整数と同じですが、-8から7の範囲にマッピングし、ベクトルサイズを8分の1に縮小して<strong>1024バイト</strong>にします。</li><li><strong>三値量子化—</strong> すべての値は、-1、0、1の3つの値のいずれかにマッピングされます。最適に保存されると、これにより各次元が1.6ビットに縮小され、 向量模型 (Embeddings) ベクトルのサイズが約40分の1の約<strong>230バイト</strong>に縮小されます。</li><li><strong>バイナリ量子化</strong> — <code>torch.sign</code>データ型を使用して、FP32スカラー値を1ビットに変換します。これにより、2つの値のみが提供され、保存に1ビットかかります。これにより、2048次元の 向量模型 (Embeddings) ベクトルが8192バイトから<strong>128バイト</strong>に縮小され、64分の1に縮小されます。</li></ul><h3 id=\"scaling\">スケーリング</h3><p>バイナリ量子化の場合、量子化は非常に簡単です。ベクトル値が0より大きいか正の場合、1にマッピングされます。それ以外の場合は、-1にマッピングされます。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1159\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png 1159w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図2：バイナリ量子化。すべての負の値は-1になり、その他はすべて1になります。</span></figcaption></figure><p>他の量子化シナリオでは、値を範囲に正規化し、量子化のレベルで許可される最も近い値に丸めました。 向量模型 (Embeddings) ベクトルは、-∞〜+∞（または、実際には、非常に大きな正および負の数）のスケール数で構成されます。量子化のために値をスケーリングするには、$max$と$min$の2つの数値を使用します。</p><p>三値量子化の場合、各ベクトル成分$v$を取得し、次のように変換します。</p><ul><li>$v$ ≥ $max$の場合、$v$は1になります。</li><li>$v$ ≤ $min$の場合、$v$は-1になります。</li><li>$min$ &lt; $v$ &lt; $max$の場合、$v$は0になります。</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"220\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png 1030w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図3：三値量子化。間隔が定義され、その中の値は0になります。すべての低い値は-1になり、すべての高い値は1になります。</span></figcaption></figure><p>4ビット整数の場合：</p><ul><li>$v$ ≥ $max$の場合、$v$は7になります。</li><li>$v$ ≤ $min$の場合、$v$は-8になります。</li><li>$min$ &lt; $v$ &lt; $max$の場合、$v$は$16*(v - min)/(max - min) - 8$になり、最も近い整数に丸められます。これにより、値が$[-8,7]$の範囲にスケーリングされます。</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図4：4ビット量子化。間隔が定義され、すべての値が定義された範囲[-8,7]に正規化されます。</span></figcaption></figure><p></p><p>8ビット整数の場合：</p><ul><li>$v$ ≥ $max$の場合、$v$は127になります。</li><li>$v$ ≤ $min$の場合、$v$は-128になります。</li><li>$min$ &lt; $v$ &lt; $max$の場合、$v$は$256*(v - min)/(max - min) - 128$になり、最も近い整数に丸められます。これにより、値が$[-128,127]$の範囲にスケーリングされます。</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"219\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図5：8ビット量子化。間隔が定義され、すべての値が定義された範囲[-128,127]に正規化されます。</span></figcaption></figure><p>$max$と$min$を計算するために、次の2つのアプローチを使用しました。</p><ul><li><strong>最小/最大</strong> — データをバッチで処理し、各バッチについて、最高のベクトル成分と最低のベクトル成分を特定し、$max$を最高に、$min$を最低に設定しました。</li><li><strong>バッチごとのローリング平均</strong> — 各バッチについて、ベクトル成分の平均と標準偏差を計算しました。すべてのバッチを処理する際に、平均と標準偏差の両方の移動平均を維持しました。$avg$がバッチ平均値の現在の移動平均であり、$std$が標準偏差の現在の移動平均である場合、各バッチについて：</li></ul><p>$max = avg + std$<br>$min = avg - std$</p><h3 id=\"qat-fine-tuning\">QAT微調整</h3><p>PTQ実験では、モデルをそのまま使用し、上記の方法を使用して生成された 向量模型 (Embeddings) を量子化しました。</p><p>Output QATの場合、<em>ストレートスルー推定</em>を使用してモデルを微調整しました。これは、損失（つまり、エラー）を計算する前に、量子化プロセスを逆にして、値の完全な精度を復元し、その損失メトリックを使用してモデルを微調整することを意味します。</p><p>各ケースで10,000ステップのファインチューニングを行い、500ステップごとにチェックポイントを保存しました。その後、<a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR</a>ベンチマークで最高のスコアを獲得したチェックポイントを保持しました。</p><h3 id=\"asymmetric-quantization\">非対称量子化</h3><p>PTQとOutput QATは、埋め込みベクトルのサイズを縮小しますが、モデルサイズや推論速度は低下させません。すべての節約は、保存されたドキュメント埋め込みのサイズと検索速度にあります。</p><p>その結果、クエリベクトルを量子化するか、検索時に量子化されていない状態にするかの両方をテストしました。どちらにしても、保存される埋め込みベクトルのサイズは変わらないためです。</p><h2 id=\"results\">結果</h2><p>合計9つの条件をテストし、以下の表にまとめました。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>条件名</th>\n<th>ファインチューニング</th>\n<th>量子化レベル</th>\n<th>スケーリング戦略</th>\n<th>量子化されたクエリ</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ベースライン</td>\n<td>❌</td>\n<td>n/a</td>\n<td>n/a</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Both</td>\n<td>❌</td>\n<td>Binary</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>PTQ Docs Only</td>\n<td>❌</td>\n<td>Binary</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td><strong>✓</strong></td>\n<td>Binary</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td><strong>✓</strong></td>\n<td>Binary</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td><strong>✓</strong></td>\n<td>Trinary</td>\n<td>ローリング平均</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td><strong>✓</strong></td>\n<td>4-bits</td>\n<td>ローリング平均</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td><strong>✓</strong></td>\n<td>8-bits</td>\n<td>ローリング平均</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td><strong>✓</strong></td>\n<td>8-bits</td>\n<td>Min/Max</td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>表2：実験条件</em></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>条件名</th>\n<th>平均スコア</th>\n<th>ベースラインからの差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ベースライン</td>\n<td>60.10</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Binary</td>\n<td>58.33</td>\n<td>-1.78</td>\n</tr>\n<tr>\n<td>PTQ Binary Docs Only</td>\n<td>59.08</td>\n<td>-1.02</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td>59.22</td>\n<td>-0.89</td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td>60.81</td>\n<td>+0.70</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td>59.49</td>\n<td>-0.62</td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td>61.73</td>\n<td>+1.62</td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td>61.67</td>\n<td>+1.56</td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td>61.29</td>\n<td>+1.19</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>表3：12のNanoBEIRベンチマークにおける各条件の平均スコア（正解率％）。</em></p><p>上記の表から、量子化のためのファインチューニングによってスコアが向上することがわかります。<strong>PTQ Binary</strong>条件と<strong>QAT Binary</strong>条件の唯一の違いはファインチューニングであり、スコアの差は有意です。同様に、<strong>PTQ Binary Docs Only</strong>条件と<strong>QAT Binary Docs Only</strong>条件の間でも、同じファインチューニングによってのみ区別される、約2％のスコアの向上が見られます。</p><p>当然のことながら、量子化を減らすほどスコアが全体的に向上し、4ビット量子化は3値より優れており、3値はバイナリより優れていることもわかります。ただし、さらに8ビットにしても何も改善されていないようです。</p><p>バイナリの場合にのみクエリを量子化しないままテストしましたが、これはパフォーマンスを向上させるようです。</p><p>最後に、ローリング平均スケーリング法は、単純な最小/最大アプローチよりも優れた結果を示すことがテストから示唆されています。</p><h2 id=\"conclusion\">結論</h2><p>量子化は、埋め込みベクトルのサイズを大幅に縮小し、情報検索を高速化することにより、埋め込みモデルにいくつかの重要な運用上の利点をもたらします。単純な量子化後トレーニング（PTQ）は、メモリとストレージの点で即時のメリットをもたらしますが、量子化対応トレーニング（QAT）は、避けられない精度損失を大幅に軽減することが実験で示されています。ファインチューニングは一貫してより良いスコアをもたらしました。</p><p>量子化の程度はパフォーマンスに直接影響しますが、これは値の精度を下げることに基づいた手法から予想されることです。積極性の低い量子化（例：4ビット）は、一般に、より積極的な手法（例：バイナリ）よりも優れていますが、驚くべきことに、8ビットと4ビットの量子化の間にパフォーマンスの有意な差はありませんでした。ある程度の精度閾値に達するまでは、量子化の大小にほとんど差がないようです。</p><p>スケーリング戦略も重要であり、ローリング平均法は、固定された最小/最大アプローチと比較して優れた結果を示しています。データに対して相対的なスケーリング値を使用すると、大幅に効果的であると思われ、さらなる調査に値します。</p><p>量子化を使用すると、より少ないコストで埋め込みモデルをより活用できます。この記事では、量子化のすべてのオプションを網羅しているわけではありませんが、簡単にアクセスできる2つのオプションについて検討しており、それらは提供できる本当の利点があります。ユーザーのコストをさらに削減できるように、量子化戦略の改良と改善に取り組んでおり、近い将来、<code>jina-embeddings-v4</code>のバイナリサポートをリリースする予定です。</p>",
  "comment_id": "685d4b76f1bef30001fc5449",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading---2025-06-30T114820.483.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-06-26T15:30:30.000+02:00",
  "updated_at": "2025-06-30T21:14:36.000+02:00",
  "published_at": "2025-06-30T21:14:36.000+02:00",
  "custom_excerpt": "Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/quantization-aware-training-of-jina-embeddings-v4/",
  "excerpt": "量子化は、より小さなベクトルモデル (Embeddings) を提供します。ファインチューニングされた量子化は、さらにロスレスなベクトルモデル (Embeddings) を提供することを示します。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "テキストと画像のグローバルな対照的アラインメントおよびトークンとパッチのローカルアラインメント",
  "html": "<p><a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali スタイル</a>のモデルを実験している中で、私たちのエンジニアの1人が最近リリースした <code>jina-clip-v2</code> モデルを使用して可視化を作成しました。画像とテキストのペアに対して、トークンの埋め込みとパッチの埋め込み間の類似度をマッピングし、興味深い視覚的洞察を生み出すヒートマップオーバーレイを作成しました。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>残念ながら、<strong>これは単なるヒューリスティックな可視化</strong>であり、明示的または保証された仕組みではありません。CLIP のようなグローバルな対照的アラインメントは、パッチとトークン間の大まかなローカルアラインメントを<em>偶然的に</em>生成することはありますが、これはモデルの意図的な目的ではなく、<strong>意図しない副作用</strong>です。その理由を説明しましょう。</p><h2 id=\"understand-the-code\">コードを理解する</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>コードが行っていることを高レベルで見ていきましょう。注意点として、<code>jina-clip-v2</code> はデフォルトではトークンレベルやパッチレベルの埋め込みにアクセスする API を公開していません - この可視化を実現するには事後的なパッチが必要でした。</p><p><strong>単語レベルの埋め込みを計算する</strong></p><p><code>model.text_model.output_tokens = True</code> を設定すると、<code>text_model(x=...,)[1]</code> を呼び出すことでトークン埋め込みの <code>(batch_size, seq_len, embed_dim)</code> 形式の2番目の要素が返されます。入力文を Jina CLIP トークナイザーでトークン化し、対応するトークン埋め込みを平均化することで、サブワードトークンを「単語」に戻しグループ化します。トークン文字列が <code>_</code> 文字で始まるかどうかをチェックして（SentencePiece ベースのトークナイザーの典型的な特徴）、新しい単語の開始を検出します。これにより、単語レベルの埋め込みのリストと単語のリストが生成されます（「Dog」が1つの埋め込み、「and」が1つの埋め込みなど）。</p><p><strong>パッチレベルの埋め込みを計算する</strong></p><p>画像タワーについては、<code>vision_model(..., return_all_features=True)</code> が <code>(batch_size, n_patches+1, embed_dim)</code> を返します。ここで最初のトークンは <code>[CLS]</code> トークンです。そこから、各パッチの埋め込み（つまり、ビジョントランスフォーマーのパッチトークン）を抽出します。次に、これらのパッチ埋め込みを <code>patch_side × patch_side</code> の2次元グリッドに整形し、元の画像解像度に合わせてアップサンプリングします。</p><p><strong>単語とパッチの類似度を可視化する</strong></p><p>類似度の計算と、それに続くヒートマップの生成は標準的な「事後的」解釈手法です：テキスト埋め込みを1つ選び、すべてのパッチ埋め込みとのコサイン類似度を計算し、その特定のトークン埋め込みに対して最も類似度の高いパッチを示すヒートマップを生成します。最後に、文中の各トークンを順に処理し、左側でそのトークンを太字でハイライトし、右側の元の画像に類似度ベースのヒートマップをオーバーレイします。すべてのフレームがアニメーション GIF にまとめられます。</p><h2 id=\"is-it-meaningful-explainability\">意味のある説明可能性なのか？</h2><p><em>純粋なコード</em>の観点からは、はい、ロジックは一貫しており、各トークンのヒートマップを生成します。パッチの類似度をハイライトする一連のフレームが得られるため、スクリプトは「うたい文句通りの動作」をします。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>上の例を見ると、<code>moon</code> や <code>branches</code> のような単語が、元の画像の対応する視覚的パッチとよく整列しているように見えます。しかし、ここで重要な質問があります：これは意味のある整列なのか、それとも単なる偶然の一致なのでしょうか？</p><p>これはより深い問題です。注意点を理解するために、<strong>CLIP がどのように訓練されるのか</strong>を思い出してください：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 はテキストエンコーダー（Jina XLM-RoBERTa、561M パラメーター）とビジョンエンコーダー（EVA02-L14、304M パラメーター）を組み合わせています。右側の各色付きの四角は、バッチ内の完全な文または画像を表しています - 個々のトークンやパッチではありません。</span></figcaption></figure><ul><li>CLIP は画像全体とテキスト全体の間の<strong>グローバルな</strong>対照的アラインメントを使用します。訓練時、画像エンコーダーは単一のベクトル（プールされた表現）を生成し、テキストエンコーダーは別の単一のベクトルを生成します。CLIP は、これらが一致するテキスト-画像ペアでは一致し、それ以外では不一致になるように訓練されます。</li><li>「パッチ X がトークン Y に対応する」というレベルでの<strong>明示的な教師あり学習はありません</strong>。モデルは「この領域が犬で、あの領域が猫」などを直接的にハイライトするようには訓練されていません。代わりに、画像全体の表現がテキスト全体の表現と一致するように教えられます。</li><li>CLIP のアーキテクチャは画像側に Vision Transformer、テキスト側にテキストトランスフォーマーを使用し - それぞれ別個のエンコーダーを形成しているため - パッチをトークンに自然にアラインメントするクロスアテンションモジュールはありません。代わりに、各タワーでの<strong>セルフアテンション</strong>と、グローバルな画像またはテキスト埋め込みのための最終的な投影を得ます。</li></ul><p>簡単に言えば、これはヒューリスティックな可視化です。特定のパッチ埋め込みが特定のトークン埋め込みに近いか遠いかは、ある程度創発的なものです。これは、モデルの堅牢な、または公式な「アテンション」というよりも、<em>事後的な解釈可能性のトリック</em>です。</p><h2 id=\"why-might-local-alignment-emerge\">なぜローカルアラインメントが現れるのか？</h2><p>では、なぜ時々単語-パッチレベルのローカルアラインメントが見られるのでしょうか？ここで重要なのは：CLIP は<em>グローバルな</em>画像-テキスト対照的な目的で訓練されますが、それでも（ViT ベースの画像エンコーダーでは）セルフアテンションと（テキストでは）トランスフォーマー層を使用します。これらのセルフアテンション層の中で、画像表現の異なる部分は、テキスト表現内の単語が相互作用するのと同じように、相互に作用することができます。大規模な画像-テキストデータセットでの訓練を通じて、モデルは自然に、全体的な画像を対応するテキスト記述と一致させるのに役立つ内部の潜在構造を発達させます。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>これらの潜在表現において<strong>ローカルアラインメント</strong>が現れる理由は、少なくとも2つあります：</p><ol><li><strong>共起パターン</strong>：モデルが多くの「犬」の画像を「猫」の画像の隣で見る場合（しばしばそれらの単語でラベル付けまたは記述される）、これらの概念に大まかに対応する潜在特徴を学習することができます。そのため、「犬」の埋め込みは、犬のような形状やテクスチャを描写するローカルパッチに近くなる可能性があります。これはパッチレベルで<em>明示的に教師あり学習されているわけではありません</em>が、犬の画像/テキストペアの繰り返しの関連付けから創発的に生まれます。</li><li><strong>Self-attention</strong>：Vision Transformer において、パッチは互いに attend します。モデルは全体的に正確なシーン表現を生成しようとするため、特徴的なパッチ（犬の顔など）は一貫した潜在的な「シグネチャー」を持つことになります。それが全体的な対比損失を最小化するのに役立つ場合、それは強化されます。</li></ol><h2 id=\"theoretical-analysis\">理論的分析</h2><p>CLIP の対比学習の目的は、マッチする画像とテキストのペアのコサイン類似度を最大化し、マッチしないペアの類似度を最小化することです。テキストエンコーダーと画像エンコーダーがそれぞれトークンとパッチの埋め込みを生成すると仮定します：</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>グローバルな類似度はローカルな類似度の集約として表現できます：</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>特定のトークンとパッチのペアが学習データ全体で頻繁に共起する場合、モデルは累積的な勾配更新を通じてそれらの類似度を強化します：</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>ここで $C$ は共起回数です。これにより $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$ は大きく増加し、これらのペアのローカルな整列がより強化されます。しかし、対比損失は全てのトークンとパッチのペアに勾配更新を分散させるため、特定のペアへの更新の強さは制限されます：</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>これにより、個々のトークンとパッチの類似度の大幅な強化が防がれます。</p><h2 id=\"conclusion\">結論</h2><p>CLIP のトークンとパッチの可視化は、テキストと画像の表現間の付随的な、創発的な整列を活用しています。この整列は興味深いものの、CLIP の<strong>グローバルな対比学習</strong>に起因し、正確で信頼性のある説明可能性に必要な構造的な堅牢性を欠いています。結果として得られる可視化は、しばしば<strong>ノイズと不一貫性</strong>を示し、詳細な解釈的応用における有用性が制限されます。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>ColBERT</strong> や <strong>ColPali</strong> のような後期相互作用モデルは、テキストトークンと画像パッチ間の<strong>明示的で細粒度の整列を構造的に組み込む</strong>ことで、これらの制限に対処しています。モダリティを独立して処理し、後段階で対象を絞った類似度計算を実行することで、これらのモデルは各テキストトークンが関連する画像領域と意味のある関連付けを持つことを保証します。</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP はトークン-パッチの類似性を可視化することができますが、それはモデルからの堅牢または公式な「アテンション」というよりも、事後解釈のテクニックです。これが理由です。",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
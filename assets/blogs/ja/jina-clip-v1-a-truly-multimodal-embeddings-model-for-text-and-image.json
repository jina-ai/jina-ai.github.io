{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1：テキストと画像のための真のマルチモーダル埋め込みモデル",
  "html": "<p>Jina CLIP v1（<code>jina-clip-v1</code>）は、OpenAI の<a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">オリジナル CLIP モデル</a>の機能を拡張した新しいマルチモーダル埋め込みモデルです。この新しいモデルにより、テキストのみとテキスト・画像のクロスモーダル検索の両方で最先端の性能を提供する単一の埋め込みモデルを利用できます。Jina AI は、OpenAI CLIP の性能をテキストのみの検索で 165％、画像から画像への検索で 12％向上させ、テキストから画像および画像からテキストのタスクでは同等またはわずかに優れた性能を実現しました。この強化された性能により、Jina CLIP v1 はマルチモーダル入力を扱う上で不可欠なものとなっています。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> は、<a href=\"#compare_table\" rel=\"noreferrer\">あらゆる検索カテゴリー</a>において OpenAI CLIP を改善しています。</div></div><p>本記事では、まず元の CLIP モデルの欠点と、独自の共同学習手法でそれらをどのように解決したかについて説明します。次に、様々な検索ベンチマークにおける我々のモデルの有効性を実証します。最後に、Embeddings API と Hugging Face を通じて Jina CLIP v1 を使い始める方法について詳しく説明します。</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">マルチモーダル AI のための CLIP アーキテクチャ</h2><p>2021 年 1 月、OpenAI は<a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a>（対照的言語-画像事前学習）モデルをリリースしました。CLIP は単純ながら巧妙なアーキテクチャを持っています：テキスト用と画像用の 2 つの埋め込みモデルを、単一の出力埋め込み空間を持つ単一のモデルに組み合わせています。そのテキストと画像の埋め込みは直接比較可能で、テキスト埋め込みと画像埋め込みの距離は、そのテキストがその画像をどれだけ適切に説明しているか（またはその逆）に比例します。</p><p>これは、マルチモーダル情報検索とゼロショット画像分類において非常に有用であることが証明されています。特別な追加学習なしに、CLIP は自然言語ラベルを使用して画像をカテゴリーに分類することができました。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"赤い月を背景に火星にいる宇宙飛行士を例として使用した画像からテキストへの変換を示す図。\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>オリジナルの CLIP のテキスト埋め込みモデルは、わずか 6300 万パラメータのカスタムニューラルネットワークでした。画像側では、OpenAI は<a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> と<a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT モデル</a>の選択肢と共に CLIP をリリースしました。各モデルは個々のモダリティに対して事前学習され、その後キャプション付き画像を使用して学習され、用意された画像-テキストペアに対して類似の埋め込みを生成するようになりました。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"「Embedding Space」というテキストが「Image Encoder」と「Text Encoder」にリンクし、「Distracted boyfriend」というラベルが付いたフローチャート。\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>このアプローチは印象的な結果を生み出しました。特に注目すべきはそのゼロショット分類性能です。例えば、学習データに<a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">宇宙飛行士</a>のラベル付き画像が含まれていなかったにもかかわらず、CLIP はテキストと画像における関連概念の理解に基づいて宇宙飛行士の写真を正しく識別することができました。</p><p>しかし、OpenAI の CLIP には 2 つの重要な欠点があります：</p><ul><li>1 つ目は、テキスト入力容量が非常に限られていることです。最大 77 トークンの入力を受け付けますが、<a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">実証的分析によると</a>、実際には埋め込みを生成するために 20 トークン以上は使用していません。これは CLIP が画像のキャプションから学習されており、キャプションは通常非常に短いためです。これは数千トークンをサポートする現在のテキスト埋め込みモデルとは対照的です。</li><li>2 つ目は、テキストのみの検索シナリオにおけるテキスト埋め込みの性能が非常に低いことです。画像キャプションは非常に限られた種類のテキストであり、テキスト埋め込みモデルに期待される広範なユースケースを反映していません。</li></ul><p>ほとんどの実際のユースケースでは、テキストのみと画像-テキストの検索が組み合わされているか、少なくとも両方のタスクが利用可能です。テキストのみのタスク用に 2 つ目の埋め込みモデルを維持することは、AI フレームワークのサイズと複雑さを実質的に 2 倍にします。</p><p>Jina AI の新しいモデルはこれらの問題に直接対処し、<code>jina-clip-v1</code> は過去数年間の進歩を活用して、テキストと画像のモダリティのあらゆる組み合わせを含むタスクに最先端の性能をもたらします。</p><h2 id=\"introducing-jina-clip-v1\">Jina CLIP v1 の紹介</h2><p>Jina CLIP v1 は、OpenAI のオリジナル CLIP のスキーマを維持しています：同じ埋め込み空間で出力を生成するように共同学習された 2 つのモデルです。</p><p>テキストエンコーディングには、<a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 モデル</a>で使用されている<a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> アーキテクチャを適応させました。このアーキテクチャは最先端の 8k トークン入力ウィンドウをサポートし、768 次元のベクトルを出力し、より長いテキストからより正確な埋め込みを生成します。これはオリジナルの CLIP モデルでサポートされている 77 トークン入力の 100 倍以上です。</p><p>画像埋め込みには、北京人工知能研究院の最新モデル：<a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code> モデル</a>を使用しています。我々は多数の画像 AI モデルを実証的に比較し、同様の事前学習でクロスモーダルコンテキストでテストしましたが、<code>EVA-02</code> は他のモデルを明確に上回りました。また、モデルサイズが Jina BERT アーキテクチャと同等であり、画像とテキストの処理タスクの計算負荷がほぼ同じになっています。</p><p>これらの選択は、ユーザーに重要な利点をもたらします：</p><ul><li>すべてのベンチマークとすべてのモーダルの組み合わせでより良い性能を発揮し、特にテキストのみの埋め込み性能が大幅に向上。</li><li><code>EVA-02</code> の画像-テキストおよび画像のみのタスクにおける実証的に優れた性能に加え、Jina AI の追加学習による画像のみの性能向上という利点。</li><li>より長いテキスト入力のサポート。<a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings の 8k トークン</a>入力サポートにより、詳細なテキスト情報を処理し、画像と相関付けることが可能。</li><li>このマルチモーダルモデルは非マルチモーダルシナリオでも高性能であるため、スペース、計算、コード保守、複雑さの大幅な節約が可能。</li></ul><h3 id=\"training\">学習</h3><p>高性能なマルチモーダル AI を実現するための我々のレシピの一部は、学習データと手順にあります。画像キャプションで使用されるテキストの長さが非常に短いことが、CLIP スタイルのモデルでテキストのみの性能が低い主な原因であることに気付き、我々の学習はこれを明示的に改善するように設計されています。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"モデルとエンコーダーを使用して3つのタスクでテキストとキャプション-画像の類似性を最適化することを示すフローチャート\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>学習は 3 つのステップで行われます：</p><ol><li>キャプション付き画像データを使用して画像とテキストの埋め込みを整列させることを学習し、同時に類似の意味を持つテキストペアを組み合わせます。この共同学習は、2 種類のタスクを同時に最適化します。この段階でモデルのテキストのみの性能は低下しますが、画像-テキストペアのみで学習した場合ほどは低下しません。</li><li>AI モデルによって生成された、画像をより詳細に説明する大きなテキストと画像を整列させる合成データを使用して学習します。同時にテキストのみのペアでの学習も継続します。この段階で、モデルは画像と組み合わせてより大きなテキストに注目することを学習します。</li><li><a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ハードネガティブ</a>を持つテキストトリプレットを使用して、より細かい意味的区別を学習することでテキストのみの性能をさらに改善します。同時に、画像と長いテキストの合成ペアを使用した学習を継続します。この段階で、モデルは画像-テキストの能力を失うことなく、テキストのみの性能が劇的に向上します。</li></ol><p>学習とモデルアーキテクチャの詳細については、<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">我々の最新論文</a>をお読みください：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) は、画像とテキストを固定サイズのベクトルにマッピングすることで共通の埋め込み空間に整列させるモデルを訓練するために広く使用されています。これらのモデルはマルチモーダル情報検索および関連タスクにおいて重要です。しかし、CLIP モデルは一般的にテキストのみのタスクにおいて、専用のテキストモデルと比較すると性能が劣ります。これにより、テキストのみのタスクとマルチモーダルタスクで別々の埋め込みとモデルを保持する情報検索システムには非効率が生じます。私たちはこの問題に対処するための新しいマルチタスク対照学習手法を提案し、これを使用して jina-clip-v1 モデルを訓練し、テキストと画像の検索タスクとテキストとテキストの検索タスクの両方で最先端の性能を達成しました。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">マルチモーダル埋め込みにおける最新の成果</h2><p>私たちは Jina CLIP v1 の性能を、テキストのみ、画像のみ、およびその両方の入力モダリティを含むクロスモーダルタスクにわたって評価しました。テキストのみの性能評価には <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">MTEB 検索ベンチマーク</a> を使用しました。画像のみのタスクには <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a> ベンチマークを使用しました。クロスモーダルタスクでは、<a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a> に含まれる <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a>、および <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a> で評価を行いました。</p><p>結果は以下の表にまとめられています：</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>これらの結果から、<code>jina-clip-v1</code> は OpenAI の元の CLIP をすべてのカテゴリーで上回り、特にテキストのみと画像のみの検索で大幅に優れていることがわかります。全カテゴリーの平均で、性能が 46% 向上しています。</p><p>より詳細な評価については、<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">私たちの最新の論文</a>をご覧ください。</p><h2 id=\"getting-started-with-embeddings-api\">Embeddings API を使い始める</h2><p><a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings API</a> を使用して、Jina CLIP v1 を簡単にアプリケーションに統合できます。</p><p>以下のコードは、Python の <code>requests</code> パッケージを使用して API を呼び出し、テキストと画像の埋め込みを取得する方法を示しています。テキスト文字列と画像の URL を Jina AI サーバーに渡し、両方のエンコーディングを返します。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">☝️</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> を有効な Jina API キーに置き換えることを忘れないでください。<a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina Embeddings ウェブページ</a>から 100 万トークンの無料トライアルキーを取得できます。</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">主要な LLM フレームワークとの統合</h3><p>Jina CLIP v1 は既に <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> と <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a> で利用可能です：</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>：<code>MultimodalEmbedding</code> ベースクラスで <code>JinaEmbedding</code> を使用し、<code>get_image_embeddings</code> または <code>get_text_embeddings</code> を呼び出します。</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>：<code>JinaEmbeddings</code> を使用し、<code>embed_images</code> または <code>embed_documents</code> を呼び出します。</li></ul><h3 id=\"pricing\">価格設定</h3><p>テキストと画像の入力は、トークン消費量に応じて課金されます。</p><p>英語のテキストについて、<a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">実験的に計算した結果</a>、平均して 1 単語あたり 1.1 トークンが必要となります。</p><p>画像については、224x224 ピクセルのタイルで画像を覆うのに必要なタイル数をカウントします。一部のタイルは部分的に空白かもしれませんが、同じようにカウントされます。各タイルの処理には 1,000 トークンが必要です。</p><p><strong>例</strong></p><p>750x500 ピクセルの画像の場合：</p><ol><li>画像は 224x224 ピクセルのタイルに分割されます。<ol><li>タイル数を計算するには、ピクセル幅を 224 で割り、最も近い整数に切り上げます。<br>     750/224 ≈ 3.35 → 4</li><li>高さについても同様に計算します：<br>     500/224 ≈ 2.23 → 3</li></ol></li><li>この例で必要な総タイル数は：<br>           4（水平）x 3（垂直）= 12 タイル</li><li>コストは 12 x 1,000 = 12,000 トークンとなります</li></ol><h3 id=\"enterprise-support\">エンタープライズサポート</h3><p><a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">110 億トークン</a>の Production Deployment プランを購入するユーザーに新しい特典を導入しています。これには以下が含まれます：</p><ul><li>製品チームとエンジニアリングチームとの 3 時間のコンサルテーションで、お客様の特定のユースケースと要件について話し合います。</li><li>お客様の RAG（Retrieval-Augmented Generation）またはベクトル検索のユースケースに合わせてカスタマイズされた Python ノートブックで、Jina AI のモデルをアプリケーションに統合する方法を示します。</li><li>アカウントエグゼクティブの割り当てと優先メールサポートにより、お客様のニーズに迅速かつ効率的に対応します。</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Hugging Face 上のオープンソース Jina CLIP v1</h2><p>Jina AI はオープンソースの検索基盤にコミットしており、このモデルを <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">Apache 2.0 ライセンス</a>の下で無料で <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a> で提供しています。</p><p>このモデルをダウンロードして自身のシステムやクラウドインストールで実行するためのサンプルコードは、<code>jina-clip-v1</code> の Hugging Face モデルページで見つけることができます。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 · Hugging Face</div><div class=\"kg-bookmark-description\">私たちはオープンソースとオープンサイエンスを通じて人工知能を進歩させ、民主化する旅を続けています。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">まとめ</h2><p>Jina AI の最新モデル — <code>jina-clip-v1</code> — は、マルチモーダル埋め込みモデルにおける重要な進歩を表し、OpenAI の CLIP を大きく上回る性能向上を提供します。テキストのみおよび画像のみの検索タスクにおける顕著な改善と、テキストから画像および画像からテキストのタスクにおける競争力のある性能により、複雑な埋め込みのユースケースに対する有望なソリューションとなっています。</p><p>現在のところリソースの制約により、このモデルは英語のテキストのみをサポートしています。より多くの言語に対応できるよう機能拡張を進めています。</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzmán",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Jina AI の新しいマルチモーダル埋め込みモデルは、テキストと画像の検索において OpenAI CLIP を上回るだけでなく、優れた画像埋め込みモデルであると同時に最先端のテキスト埋め込みモデルでもあります。もはや異なるモダリティに対して異なるモデルを必要としません。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "レイトチャンキングの本質と誤解：パート II",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">この記事は、より深い洞察と一般的な誤解や比較に焦点を当てているため、まず<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">パート I を読むことを強く推奨します</a>。<b><strong style=\"white-space: pre-wrap;\">推奨される読む順番：</strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">パート I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、パート II、</strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">研究論文</strong></b></a><b><strong style=\"white-space: pre-wrap;\">。</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>長文書のチャンキングには2つの問題があります。1つ目は、<strong>分割点の決定</strong>、つまり文書をどのように分割するかということです。固定トークン長、固定文数、あるいは<a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">正規表現や意味的分割モデル</a>のようなより高度な技術を考慮することができます。正確なチャンク境界は、検索結果の可読性を向上させるだけでなく、RAG システムで LLM に供給されるチャンクが正確で十分なものであることを保証します。</p><p>2つ目の問題は、各チャンクにおける<strong>文脈の喪失</strong>です。文書が分割されると、ほとんどの人が次の論理的なステップとして、各チャンクを個別にバッチ処理で埋め込みます。しかし、これは元の文書からグローバルな文脈が失われることにつながります。多くの先行研究は最初の問題に取り組み、より良い境界検出が意味的表現を改善すると主張してきました。例えば、「意味的チャンキング」は、埋め込み空間で余弦類似度の高い文をグループ化して、意味的単位の分断を最小限に抑えます。</p><p>私たちの観点から、これら2つの問題は<em>ほぼ</em>独立しており、別々に取り組むことができます。優先順位をつけるとすれば、<strong>2番目の問題がより重要だと考えます。</strong></p>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">問題2：<b>文脈情報</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">保持</td>\n<td>喪失</td>\n</tr>\n<tr>\n<td><b>問題1：分割点</b></td>\n<td>良好</td>\n<td style=\"text-align:center\">理想的なシナリオ</td>\n<td>貧弱な検索結果</td>\n</tr>\n<tr>\n<td></td>\n<td>不良</td>\n<td style=\"text-align:center\">良好な検索結果だが、結果が人間に読みやすくないか LLM の推論に適していない可能性がある</td>\n<td>最悪のシナリオ</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n\n<h2 id=\"late-chunking-for-context-loss\">文脈喪失に対する Late Chunking</h2><p><strong>Late chunking</strong> は2番目の問題、つまり<strong>文脈の喪失</strong>から取り組みます。これは理想的な分割点や意味的境界を見つけることについてのものでは<em>ありません</em>。長文書を小さなチャンクに分割するには、依然として正規表現、ヒューリスティクス、その他の技術を使用する必要があります。しかし、分割されたら直ちに各チャンクを埋め込むのではなく、late chunking ではまず文書全体を1つのコンテキストウィンドウで符号化します（<code>jina-embeddings-v3</code> の場合は8192トークン）。その後、境界の手がかりに従って各チャンクの平均プーリングを適用します—これが late chunking の「late」という名前の由来です。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagram comparing &quot;Naive Chunking&quot; and &quot;Late Chunking&quot; methods for processing long documents with labeled steps.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Late chunking でも境界の手がかりは必要ですが、重要な違いはそれらの手がかりを使用するタイミングです。Late chunking では、文書全体が埋め込まれた後にのみ手がかりが適用され、プーリングの範囲を決定するために使用されます。</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">Late Chunking は不完全な境界手がかりに対して頑健</h2><p>興味深いことに、実験によると late chunking は完璧な意味的境界の必要性を排除し、これは上述の最初の問題を部分的に解決します。実際、固定トークン境界に適用された late chunking は、意味的境界手がかりを用いたナイブなチャンキングよりも優れた性能を示します。固定長境界を使用する単純な分割モデルは、late chunking と組み合わせると、高度な境界検出アルゴリズムと同等の性能を発揮します。私たちは3つの異なるサイズの埋め込みモデルをテストし、結果はすべてのテストデータセットにおいて、それらすべてが一貫して late chunking の恩恵を受けることを示しています。そうは言っても、埋め込みモデル自体が性能に最も大きな影響を与える要因であり続けます—<strong>late chunking を使用した弱いモデルが、使用しない強いモデルを上回る例は一つもありません。</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Scatter plot chart showing the percentage of relative improvements across various models against a baseline, with a vertical \" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">ベースライン（固定トークン長境界手がかりとナイブなチャンキングを使用した</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">）に対する相対的な検索性能の改善。アブレーション研究の一環として、異なる境界手がかり（固定トークン長、文境界、意味的境界）と異なるモデル（</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">）で late chunking をテストしました。MTEB での性能に基づくと、これら3つの埋め込みモデルのランキングは：</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">。ただし、この実験の焦点は埋め込みモデル自体のパフォーマンスを評価することではなく、より優れた埋め込みモデルが後期チャンキングと境界の手がかりとどのように相互作用するかを理解することにあります。実験の詳細については、研究論文をご確認ください。</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>不適切な境界に対して耐性があるからといって、境界を無視できるというわけではありません—人間と LLM の可読性の両方にとって依然として重要です。私たちの見方は次の通りです：セグメンテーションの最適化（前述の第1の問題）において、意味やコンテキストの損失を心配することなく、可読性に完全に焦点を当てることができます。後期チャンキングは良い境界点も悪い境界点も処理できるので、考慮すべきは可読性だけです。</p><h2 id=\"late-chunking-is-bidirectional\">後期チャンキングは双方向</h2><p>後期チャンキングに関するもう1つの一般的な誤解は、条件付きチャンク埋め込みが「先を見ない」前のチャンクのみに依存しているということです。これは間違いです。<strong>後期チャンキングの条件付き依存関係は実際に双方向</strong>であり、一方向ではありません。これは、埋め込みモデル（エンコーダーのみのトランスフォーマー）の注意行列が、自己回帰モデルで使用される三角マスク行列とは異なり、完全に接続されているためです。形式的には、チャンク $k$ の埋め込み $v_k \\sim Q(c_k|D)$ であり、$v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$ ではありません。ここで $Q$ は言語モデルの因数分解を表します。これは後期チャンキングが正確な境界配置に依存しない理由も説明しています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagrams of a transformer model with detailed encoder on the left and decoder on the right, labeled with tokens, embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">マスク付き自己注意を持つデコーダーのみのモデルとは異なり、埋め込みモデルは通常、完全な注意行列を持つエンコーダーのみです。これは、各トークン埋め込みが同じコンテキストウィンドウ内の他のすべてのトークンに条件付けられていることを意味し、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">の場合、最大8191個の他のトークンを含みます。その結果、チャンク埋め込みは両方向のグローバルコンテキスト情報を保持します。</span></figcaption></figure><h2 id=\"late-chunking-can-be-trained\">後期チャンキングは訓練可能</h2><p>後期チャンキングは埋め込みモデルの追加訓練を<em>必要としません</em>。平均プーリングを使用する長いコンテキストの埋め込みモデルであれば適用可能で、実務者にとって非常に魅力的です。ただし、質問応答やクエリ-ドキュメント検索などのタスクに取り組んでいる場合は、ファインチューニングによってパフォーマンスをさらに向上させることができます。具体的には、訓練データは以下の要素からなるタプルで構成されます：</p><ul><li><strong>クエリ</strong>（質問や検索語など）</li><li>クエリに関連する情報を含む<strong>ドキュメント</strong></li><li>ドキュメント内の<strong>関連スパン</strong>（クエリに直接対応するテキストの特定のチャンク）</li></ul><p>モデルは、InfoNCE のような対照損失関数を使用して、クエリをその関連スパンとペアリングすることで訓練されます。これにより、関連スパンがクエリと埋め込み空間で密接に整列し、無関係なスパンはより遠くに押しやられます。その結果、モデルはチャンク埋め込みを生成する際に、ドキュメントの最も関連性の高い部分に焦点を当てることを学習します。<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">詳細については、研究論文をご参照ください。</a></p><h2 id=\"late-chunking-vs-contextual-retrieval\">後期チャンキング vs コンテキスト検索</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Introducing Contextual Retrieval</div><div class=\"kg-bookmark-description\">Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>後期チャンキングが導入された直後、Anthropic は<strong>コンテキスト検索</strong>と呼ばれる別の戦略を導入しました。Anthropic の手法は、失われたコンテキストの問題に対する総当たり的なアプローチで、以下のように機能します：</p><ol><li>各チャンクは完全なドキュメントと共に LLM に送信されます</li><li>LLM は各チャンクに関連するコンテキストを追加します</li><li>これにより、より豊かで情報量の多い埋め込みが得られます</li></ol><p>私たちの見解では、これは本質的に<strong>コンテキスト強化</strong>であり、グローバルコンテキストが LLM を使用して各チャンクに明示的にハードコードされます。これは<strong>コスト</strong>、<strong>時間</strong>、<strong>ストレージ</strong>の面で高価です。さらに、LLM がコンテキストを効果的に強化するために正確で読みやすいチャンクに依存するため、このアプローチがチャンク境界に対して耐性があるかどうかは不明確です。対照的に、後期チャンキングは上記で示したように境界の手がかりに対して高い耐性を持っています。埋め込みサイズは同じままなので、追加のストレージは必要ありません。埋め込みモデルの完全なコンテキスト長を活用しているにもかかわらず、<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">強化を生成するために LLM を使用するよりもはるかに高速です</a>。研究論文の定性的研究では、<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Anthropic のコンテキスト検索が後期チャンキングと同様のパフォーマンスを示すことを示しています。</a>しかし、後期チャンキングは、エンコーダーのみのトランスフォーマーの本来のメカニズムを活用することで、よりローレベルで汎用的で自然なソリューションを提供します。</p><h2 id=\"which-embedding-models-support-late-chunking\">どの埋め込みモデルが後期チャンキングをサポートしているか</h2><p>後期チャンキングは <code>jina-embeddings-v3</code> や <code>v2</code> に限定されたものではありません。平均プーリングを使用する長いコンテキストの埋め込みモデルに適用できる、かなり汎用的なアプローチです。例えば、この投稿では <code>nomic-v1</code> もサポートしていることを示しています。すべての埋め込みプロバイダーが自社のソリューションで後期チャンキングのサポートを実装することを心から歓迎します。</p><p>モデルユーザーとして、新しい埋め込みモデルや API を評価する際に、後期チャンキングをサポートしているかどうかを確認するには、以下の手順に従ってください：</p><ol><li><strong>シングル出力</strong>：モデル/API は文章ごとに単一の最終埋め込みのみを提供し、トークンレベルの埋め込みは提供しませんか？もしそうなら、後期チャンク分割をサポートできない可能性が高いです（特に Web API の場合）。</li><li><strong>長文脈サポート</strong>：モデル/API は少なくとも 8192 トークンの文脈を処理できますか？できない場合、後期チャンク分割は適用できません—より正確には、短い文脈のモデルに後期チャンク分割を適用する意味がありません。サポートしている場合は、単にサポートを謳っているだけでなく、実際に長い文脈で良好なパフォーマンスを発揮することを確認してください。これらの情報は通常、LongMTEB やその他の長文脈ベンチマークでの評価など、モデルの技術レポートで確認できます。</li><li><strong>平均プーリング</strong>：プーリング前にトークンレベルの埋め込みを提供する自己ホスト型モデルや API の場合、デフォルトのプーリング方法が平均プーリングであるかを確認してください。CLS や最大プーリングを使用するモデルは後期チャンク分割と互換性がありません。</li></ol><p>まとめると、埋め込みモデルが長文脈をサポートし、デフォルトで平均プーリングを使用している場合、後期チャンク分割を容易にサポートできます。<a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">GitHub リポジトリで実装の詳細と更なる議論をご確認ください</a>。</p><h2 id=\"conclusion\">結論</h2><p>では、後期チャンク分割とは何でしょうか？後期チャンク分割は、長文脈埋め込みモデルを使用してチャンク埋め込みを生成する簡単な方法です。高速で、境界手がかりに対して強靭で、非常に効果的です。これはヒューリスティックや過剰なエンジニアリングではなく、Transformer のメカニズムに対する深い理解に基づいた思慮深い設計です。</p><p>今日、LLM を取り巻くハイプ（誇大宣伝）は否定できません。多くの場合、BERT のような小規模なモデルで効率的に対処できる問題が、より大規模で複雑なソリューションの魅力に導かれて LLM に委ねられています。大手 LLM プロバイダーがモデルの採用を推進し、埋め込みプロバイダーが埋め込みを提唱するのは、双方が自社の商業的強みを活かそうとしているため、当然のことです。しかし最終的には、ハイプではなく、実際に機能するものが重要です。コミュニティ、業界、そして最も重要な時間が、どのアプローチが本当にリーンで効率的で持続可能なのかを明らかにするでしょう。</p><p><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">私たちの研究論文</a>を必ずお読みください。また、様々なシナリオで後期チャンク分割のベンチマークを実施し、フィードバックを共有していただければ幸いです。</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "チャンク埋め込みの最適な手法であり、検索/RAG のパフォーマンスを向上させる Late Chunking について、その理由を深く掘り下げる探求のパート 2",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
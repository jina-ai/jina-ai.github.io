{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4：マルチモーダル多言語検索のためのユニバーサルなベクトルモデル (Embeddings)",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Search Foundation Models</div><div class=\"kg-bookmark-description\">マルチモーダルおよび多言語検索のためのユニバーサルな埋め込みモデル</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Search Foundation Models</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">jina-embeddings-v4を発表します。これは、テキストと画像の表現を、遅延インタラクションスタイルでシングルベクトルとマルチベクトルの両方の埋め込みをサポートする新しいアーキテクチャを介して統合する、38億のパラメーターを持つマルチモーダル埋め込みモデルです。このモデルは、クエリベースの情報検索、クロスモーダルな意味的類似性、プログラミングコード検索など、多様な検索シナリオにわたってパフォーマンスを最適化するために、タスク固有のLow-Rank Adaptation (LoRA)アダプターを組み込んでいます。包括的な評価により、jina-embeddings-v4は、シングルモーダルおよびクロスモーダル検索タスクの両方で最先端のパフォーマンスを達成し、特にテーブル、グラフ、図、混合メディア形式などの視覚的に豊富なコンテンツの処理において強みを発揮することが実証されています。この機能の評価を容易にするために、視覚的に豊富な画像検索専用に設計された新しいベンチマークであるJina-VDRも導入します。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">私たちは、オープンソースとオープンサイエンスを通じて人工知能を進歩させ、民主化する旅を続けています。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>本日、テキストと画像に対応した、38億のパラメーターを持つ新しいユニバーサルな 埋め込み (Embeddings) モデルである<code>jina-embeddings-v4</code>をリリースします。これには、クエリ-ドキュメント検索、セマンティックマッチング、コード検索など、最も一般的な検索タスクのパフォーマンスを最適化するタスク固有のLoRAアダプターのセットが含まれています。<code>jina-embeddings-v4</code>は、MTEB、MMTEB、CoIR、LongEmbed、STS、<a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>、CLIP、ViDoReのベンチマークにおいて、マルチモーダルおよび多言語タスクで最先端の検索パフォーマンスを達成しており、特にテーブル、グラフ、図、およびそれらの混合物などの視覚的に豊富なコンテンツの処理に強みを発揮します。このモデルは、シングルベクターとマルチベクターの両方の 埋め込み (Embeddings) をサポートしています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">視覚的なドキュメント検索とマルチモーダルベンチマークにおける</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">のパフォーマンス。箱ひげ図は、6つのベンチマークカテゴリー（ViDoRe（ビジョンドキュメント検索）、Jina-VDR（包括的なビジョンドキュメント検索）、Wikimedia Commons Retrieval（多言語ドキュメント-説明マッチング）、GitHub README Retrieval（コードドキュメント検索）、Tweet Stock Retrieval（金融チャート分析）、およびCLIP Benchmark（一般的なテキスト-画像検索））における埋め込みモデルの平均スコアとパフォーマンスの変動を示しています。Jina-embeddings-v4のバリアント（シアンで強調表示）は、視覚的に豊富なドキュメントタスク全体で最先端のパフォーマンスを示しており、マルチベクターバージョンは、専門的な視覚ドキュメントベンチマークで最高のスコア（ViDoReで90.2、Jina-VDRで80.2）を達成し、一般的なマルチモーダル検索タスク（CLIP Benchmarkで84.1）で競争力のあるパフォーマンスを維持しています。モデルは各ベンチマークカテゴリー内の平均パフォーマンスでランク付けされており、個々のデータポイントは複数の評価タスクにわたるスコア分布を示しています。</span></figcaption></figure><p><code>jina-embeddings-v4</code>は、これまでで最も意欲的な 埋め込み (Embeddings) モデルです。オープンソースモデルとして、<code>jina-embeddings-v4</code>は主要プロバイダーの主要なクローズドソース 埋め込み (Embeddings) モデルを上回り、多言語検索でOpenAIの<code>text-embedding-3-large</code>よりも12％優れたパフォーマンス（66.49対59.27）、長文ドキュメントタスクで28％の改善（67.11対52.42）、コード検索で<code>voyage-3</code>よりも15％優れています（71.59対67.23）。また、Googleの<code>gemini-embedding-001</code>のパフォーマンスに匹敵します。これにより、v4は現在利用可能な最も高性能なオープンソースのユニバーサル 埋め込み (Embeddings) モデルとなり、研究者と開発者に、トレーニングプロセス、アーキテクチャの決定、および<a href=\"https://arxiv.org/abs/2506.18902\">包括的な技術レポート</a>を通じてモデルの重みに対する完全な透明性を提供し、エンタープライズグレードのマルチモーダル 埋め込み (Embeddings) 機能を提供します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">5つの検索ベンチマークにおける</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">のパフォーマンス。このチャートは、テキスト検索、コード検索、多言語検索、長文コンテキスト検索、およびセマンティックテキスト類似性（STS）ベンチマークにおける各モデルの平均スコアを示す箱ひげ図を示しています。</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">（シアンで強調表示）は、すべての評価カテゴリーで競争力のある、または最先端のパフォーマンスを示しており、特にテキスト検索とSTSで優れた結果を示しています。モデルは各ベンチマークカテゴリー内の平均パフォーマンスでランク付けされており、個々のデータポイントは複数の評価タスクにわたるスコア分布を示しています。</span></figcaption></figure><h2 id=\"new-architecture\">新しいアーキテクチャ</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">のアーキテクチャ</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">。このモデルは、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\">バックボーン（38億のパラメーター）上に構築されています。テキストと画像の入力は、共有パスウェイを介して処理されます。画像は最初にビジョンエンコーダーを介してトークンシーケンスに変換され、次に両方のモダリティがコンテキストアテンションレイヤーを持つ言語モデルデコーダーによって共同で処理されます。3つのタスク固有のLoRAアダプター（それぞれ6000万のパラメーター）は、フリーズされたバックボーンの重みを変更せずに、検索、テキストマッチング、およびコードタスクに対して特化した最適化を提供します。このアーキテクチャは、デュアル出力モードをサポートしています。（1）効率的な類似性検索のために平均プーリングを介して生成されたシングルベクター 埋め込み (Embeddings) （2048次元、128に切り捨て可能）、および（2）遅延インタラクション検索戦略のために投影レイヤーを介したマルチベクター 埋め込み (Embeddings) （トークンあたり128次元）。</span></figcaption></figure><p><code>jina-embeddings-v3</code>から<code>jina-embeddings-v4</code> は、テキストのみのベクトルモデル (Embeddings)から、マルチモーダルなベクトルモデル (Embeddings)へのパラダイムシフトを表しています。v3 がタスク固有の LoRA アダプターを使用してテキストのベクトルモデル (Embeddings)の最適化に焦点を当てていたのに対し、v4 はテキストと視覚コンテンツの両方を統一された表現でベクトルモデル (Embeddings)化するという、高まる要件に対応します。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>側面</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>バックボーンモデル</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>パラメーター (ベース)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>パラメーター (アダプターあり)</td>\n<td>572M</td>\n<td>3.8B + アダプターごとに 60M</td>\n</tr>\n<tr>\n<td>モダリティ</td>\n<td>テキストのみ</td>\n<td>テキスト + 画像 (マルチモーダル)</td>\n</tr>\n<tr>\n<td>最大入力長</td>\n<td>8,192 词元 (Tokens)</td>\n<td>32,768 词元 (Tokens)</td>\n</tr>\n<tr>\n<td>画像処理</td>\n<td>なし</td>\n<td>最大 20 メガピクセル、視覚的に豊かなドキュメント</td>\n</tr>\n  <tr>\n<td>多言語サポート</td>\n<td>89 言語</td>\n<td>29 以上の言語</td>\n</tr>\n<tr>\n<td>ベクトルの種類</td>\n<td>シングルベクトルのみ</td>\n<td>シングルベクトル + マルチベクトル (遅延相互作用)</td>\n</tr>\n<tr>\n<td>シングルベクトルの次元</td>\n<td>1024 (MRL で 32 まで切り捨て可能)</td>\n<td>2048 (MRL で 128 まで切り捨て可能)</td>\n</tr>\n<tr>\n<td>マルチベクトルの次元</td>\n<td>利用不可</td>\n<td>词元 (Token)ごとに 128</td>\n</tr>\n<tr>\n<td>タスク LoRA 特化</td>\n<td>• 非対称検索<br>• セマンティック類似性<br>• 分類<br>• 分離</td>\n<td>• 非対称検索<br>• セマンティック類似性<br>• コード検索</td>\n</tr>\n<tr>\n<td>トレーニング段階</td>\n<td>3 段階: 事前トレーニング → ベクトルモデル (Embeddings)のファインチューニング → アダプターのトレーニング</td>\n<td>2 段階: ジョイントペアトレーニング → タスク固有のアダプターのトレーニング</td>\n</tr>\n<tr>\n<td>損失関数</td>\n<td>InfoNCE、CoSent、拡張トリプレット損失</td>\n<td>シングル/マルチベクトルのジョイント InfoNCE + KL ダイバージェンス</td>\n</tr>\n<tr>\n<td>位置エンコーディング</td>\n<td>RoPE (ロータリーベース周波数チューニング)</td>\n<td>M-RoPE (マルチモーダルロータリー位置埋め込み)</td>\n</tr>\n<tr>\n<td>クロスモーダル処理</td>\n<td>N/A</td>\n<td>統一エンコーダー (モダリティギャップの削減)</td>\n</tr>\n<tr>\n<td>MRL サポート</td>\n<td>はい</td>\n<td>はい</td>\n</tr>\n<tr>\n<td>アテンション実装</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">バックボーン</h3><p>v4 における最も重要なアーキテクチャの変更は、バックボーンが <code>XLM-RoBERTa</code> から <code>Qwen2.5-VL-3B-Instruct</code> に変更されたことです。この決定は、画像を词元 (Token)シーケンスに変換し、テキストと一緒に処理することで、デュアルエンコーダーアーキテクチャに存在する<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">モダリティギャップ</a>を解消する「真のマルチモーダル処理」を可能にする、普遍的なベクトルモデル (Embedding)モデルを作成するという v4 の中核的な目標によって推進されました。</p><p>バックボーンの選択は、いくつかの重要な設計目標に沿っています。Qwen2.5-VL のドキュメント理解における卓越性は、表、グラフ、スクリーンショットなどの視覚的にリッチなコンテンツを処理する v4 の強みを直接サポートします。動的な解像度機能により、アーキテクチャで指定されているように、v4 は 20 メガピクセルにリサイズされた画像を処理できます。高度な位置エンコーディングは、OpenAI CLIP の 0.15 に対して 0.71 のアライメントスコアで、v4 が優れたクロスモーダルアライメントを実現するための基盤を提供します。</p><h3 id=\"lora-adapters\">LoRA アダプター</h3><p>V4 は、v3 の 5 つのタスクから、効果とユーザーの採用について得られた教訓を反映して、3 つの焦点を絞ったタスクに合理化します。</p><ul><li><strong>非対称検索</strong> (v3 のクエリ/パッセージアダプターを統合)</li><li><strong>対称類似性</strong> (STS タスクに対する v3 のテキストマッチングと同等)</li><li><strong>コード検索</strong> (v2-code から学習、v3 に欠落)</li></ul><p>この統合により、v3 の分類および分離アダプターが削除され、最も影響力のあるベクトルモデル (Embedding)の使用事例 (検索と STS) に v4 が焦点が当てられます。</p><h3 id=\"output-embeddings\">出力ベクトルモデル (Embeddings)</h3><p>V4 は、シングルベクトルとマルチベクトルの両方のベクトルモデル (Embeddings)をサポートするデュアル出力システムを導入しましたが、v3 はシングルベクトルの出力のみを提供しました。これは、さまざまな検索シナリオに対応します。</p><ul><li><strong>シングルベクトルモード</strong>: 効率的な類似性検索のための 2048 次元のベクトルモデル (Embeddings) (MRL 経由で 128 に切り捨て可能)</li><li><strong>マルチベクトルモード</strong>: <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">遅延相互作用検索</a>のための词元 (Token)あたり 128 次元</li></ul><p>このデュアルアプローチは、特に視覚的にリッチなドキュメント検索において、マルチベクトル表現でより優れた効果を発揮する一方で、標準的な類似性タスクの効率を維持します。視覚タスク全体でのシングルベクトルに対するマルチベクトルの 7 ～ 10% の一貫したパフォーマンスの利点は、遅延相互作用がマルチモーダルコンテンツに対して根本的により優れたセマンティックマッチングを提供することを示唆しています。</p><h3 id=\"parameter-size\">パラメーターサイズ</h3><p>v4 は v3 よりも 6.7 倍大きい (3.8B 対 570M パラメーター) ですが、テキストのみのパフォーマンスの向上は実際にはわずかであり、パラメーターのスケーリングは主にテキストの機能拡張ではなく、マルチモーダルの要件によって推進されたことが示唆されます。コアテキストベンチマークでは、v4 は MMTEB で 66.49 を達成しましたが、v3 は 58.58 (14% の改善) であり、MTEB-EN では 55.97 に対して v3 は 54.33 (3% の改善) でした。コード検索では、v4 は CoIR で 71.59 をスコアリングしましたが、v3 は 55.07 (30% の改善) であり、長いドキュメントのパフォーマンスでは、LongEmbed で v4 が 67.11 に対して v3 が 55.66 (21% の改善) を示しています。大幅なスケーリングは、v4 のマルチモーダル機能を考慮すると正当化されます。視覚ドキュメント検索 (Jina-VDR) で 84.11 nDCG@5、ViDoRe ベンチマークで 90.17 を達成しました。これらは v3 には完全に存在しない機能です。したがって、パラメーターの増加は、競争力のあるテキストパフォーマンスを維持しながら、マルチモーダル機能への投資を表しています。統合アーキテクチャにより、個別のテキストモデルとビジョンモデルの必要性がなくなり、従来のデュアルエンコーダーアプローチの 0.15 と比較して 0.71 のクロスモーダルアライメントが実現します。</p><h2 id=\"getting-started\">はじめに</h2><p>簡単な雰囲気チェックのために、Search Foundation ツールボックスでテキストから画像へのデモを試してください。当社のウェブサイトからドキュメント画像のコレクションを用意しており、独自の画像 URL を追加することもできます。クエリを入力して Enter キーを押すだけで、ランク付けされた結果が表示されます。OCR またはコンテンツベースの画像検索のようにそれを後退させることもできます。英語以外のクエリも自由にお試しください。</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">デモは次の場所で利用できます: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> このデモを使用すると、プライマリー API キーの 词元 (Token)が消費されることに注意してください。また、デモは、サーバー上のすべての画像をこれらの URL からダウンロードする必要があり、画像にキャッシュが実装されていないため、少し遅く見える場合があります。</span></p></figcaption>\n        </figure><h3 id=\"via-api\">API 経由</h3><p>以下のコードは、<code>jina-embeddings-v4</code> の使用方法を示しています。テキスト文字列、base64 エンコードされた画像、または画像 URL を渡すことができます。新規ユーザーは、1000 万個の無料 词元 (Token)を含む Jina API キーを取得できます。</p><pre><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>GPUリソースが限られているため、現在、Embedding API (埋め込みAPI) は、<code>jina-embeddings-v4</code> が本来持つ最大 32K の tokens (詞元) を処理する能力にもかかわらず、最大 8K tokens (詞元) までのドキュメントをサポートしています。8K tokens (詞元) を超えるより長いコンテキストを必要とするアプリケーション（<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a> など）については、CSP を通じてモデルをデプロイするか、モデルをセルフホスティングすることをお勧めします。</p><h3 id=\"via-csp-marketplaces\">CSP マーケットプレイス経由</h3><p><code>jina-embeddings-v4</code> は、AWS、Azure、GCP で間もなく直接利用可能になり、そこに記載されている価格で提供されます。</p><h3 id=\"via-huggingface\">HuggingFace 経由</h3><p>研究および実験目的で、Hugging Face ページからローカルでモデルを使用できます。動作を示す Google Colab ノートブックを用意しました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">結論</h2><p><code>jina-embeddings-v4</code> は、テキストと画像を統合された経路で処理する 38 億のパラメーターを持つユニバーサルな embedding (ベクトル模型) モデルであり、特に視覚的に豊富なドキュメント検索において、Google、OpenAI、Voyage AI の独自のモデルを上回り、dense および late-interaction retrieval (遅延インタラクション検索) の両方をサポートしています。しかし、この能力は孤立して生まれたものではなく、根本的な制限を解決するための 4 世代にわたる集大成です。</p><p>2022 年初頭に <code>jina-embeddings-v1</code> から始めたとき、誰もがより多くのデータがより良いパフォーマンスを意味すると考えていました。私たちはその逆を証明しました。15 億のペアをフィルタリングして 3 億 8500 万の高品質な例に絞り込むことで、はるかに大きなデータセットよりも優れたパフォーマンスを発揮しました。教訓は、コレクションよりもキュレーションが重要であるということです。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model’s awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>しかし、ユーザーは BERT の 512 tokens (詞元) の壁にぶつかり続けました。より長いシーケンスでのトレーニングはコストがかかるように思われましたが、<code>jina-embeddings-v2</code> はエレガントな解決策を明らかにしました。短い時間でトレーニングし、長くデプロイします。ALiBi の線形注意バイアスにより、512 tokens (詞元) でトレーニングされたモデルは、推論時に 8,192 tokens (詞元) をシームレスに処理できます。より少ない計算量でより多くの機能を手に入れることができました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI’s proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><code>jina-embeddings-v2</code> の成功により、別の制約が明らかになりました。異なるタスクには異なる最適化が必要でした。個別のモデルを構築するのではなく、<code>jina-embeddings-v3</code> は、小さな 60M の LoRA アダプターを使用して、570M のベースモデルを任意のタスクに合わせてカスタマイズしました。1 つのモデルが 5 つの特殊なモデルになりました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>タスクの特殊化があっても、ユーザーが視覚的な理解を必要としている間、私たちはテキストのみにとどまっていました。<code>jina-clip-v1</code> や <code>jina-clip-v2</code> などの標準的な CLIP ベースのモデルは、別々のエンコーダーを使用しており、異なる形式の類似コンテンツが大きく離れてしまう「モダリティギャップ」を生み出しています。最近リリースされた <code>jina-reranker-m0</code> と同様に、<code>jina-embeddings-v4</code> はこれを完全に排除しました。1 つの統一された経路ですべてを処理し、ギャップを埋めるのではなく、取り除きました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><code>jina-embeddings-v4</code> と <code>jina-reranker-m0</code> はどちらも根本的な変化を共有しています。それは、エンコーダーのみのモデルの代わりに LLM (大模型) をバックボーンとして使用することです。これは偶然ではありません。ほとんどの人が見逃している深い利点を反映しています。エンコーダーのみのモデルは、画像がテキストとは別にクラスター化される「モダリティギャップ」を作成します。デコーダーのみのモデルは、真の混合モダリティ表現や説明可能性など、エンコーダーのみのアーキテクチャでは達成できなかった可能性を切り開きます。</p><p>私たちの重要な洞察：ベクトルモデル (embeddings) と生成はどちらも、セマンティクスの理解に関わるものです。生成に優れた大規模言語モデル (LLM) は、当然ながら表現にも優れています。私たちは、ベクトルモデル (embedding) と重排器 (reranking) が<strong>同じ検索基盤モデル</strong>から生まれる、統一されたアーキテクチャに未来があると考えています。そして、まさにそれをJina AIが構築しようとしているのです。</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina 向量模型 (Embeddings) v4 は、38 億のパラメータを持つユニバーサルな 向量模型 (Embedding) モデルであり、マルチモーダルおよび多言語の検索に対応し、シングルベクトルとマルチベクトルの 向量模型 (Embedding) 出力をサポートします。",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "バイナリ Embeddings：AI をすべて、容量はわずか 3.125%",
  "html": "<p>埋め込み（Embeddings）は、様々な AI や自然言語処理アプリケーションの基盤となり、テキストの意味を高次元ベクトルとして表現する方法を提供しています。しかし、モデルのサイズが大きくなり、AI モデルが処理するデータ量が増加するにつれて、従来の埋め込みの計算とストレージの要件は急激に増加しています。バイナリ埋め込みは、高性能を維持しながらリソース要件を大幅に削減する、コンパクトで効率的な代替手段として導入されました。</p><p>バイナリ埋め込みは、埋め込みベクトルのサイズを最大 96%（Jina Embeddings の場合は 96.875%）削減することで、これらのリソース要件を軽減する一つの方法です。ユーザーは、精度をほとんど損なうことなく、コンパクトなバイナリ埋め込みの力を AI アプリケーションで活用できます。</p><h2 id=\"what-are-binary-embeddings\">バイナリ埋め込みとは？</h2><p>バイナリ埋め込みは、従来の高次元浮動小数点ベクトルをバイナリベクトルに変換する特殊なデータ表現形式です。これは埋め込みを圧縮するだけでなく、ベクトルの完全性と有用性をほぼ完全に保持します。この技術の本質は、変換後もデータポイント間の意味論的関係と距離を維持できることにあります。<br><br>バイナリ埋め込みの魔法は量子化にあります。これは高精度の数値を低精度の数値に変換する方法です。AI モデリングでは、通常、埋め込みの 32 ビット浮動小数点数を 8 ビット整数などの少ないビット数の表現に変換することを意味します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">2値化はすべてのスカラー値を 0 または 1 に変換することです。カラー画像を白黒のピクセルだけの画像に変換するようなものです。画像：神奈川沖浪裏（1831）葛飾北斎</span></figcaption></figure><p>バイナリ埋め込みは、これを究極まで推し進め、各値を 0 または 1 に縮小します。32 ビット浮動小数点数をバイナリ桁に変換することで、埋め込みベクトルのサイズを 32 分の 1、つまり 96.875% 削減します。結果として、得られる埋め込みのベクトル演算が大幅に高速化されます。一部のマイクロチップで利用可能なハードウェアの高速化を使用すると、ベクトルが 2 値化された場合、ベクトル比較の速度は 32 倍以上に向上する可能性があります。</p><p>このプロセスでは必然的に一部の情報が失われますが、モデルの性能が非常に高い場合、この損失は最小限に抑えられます。非量子化された埋め込みが異なるものの間で最大限に異なる場合、2 値化はその違いをうまく保持する可能性が高くなります。そうでない場合、埋め込みを正しく解釈することが困難になる可能性があります。</p><p>Jina Embeddings のモデルは、まさにそのような頑健性を持つように訓練されており、2 値化に適しています。</p><p>このようなコンパクトな埋め込みにより、特にモバイルや時間に敏感な用途など、リソースに制約のある環境で新しい AI アプリケーションが可能になります。</p><p>以下のグラフが示すように、これらのコストと計算時間の利点は、比較的小さな性能コストで得られます。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10：</em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">正規化割引累積利得</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">を使用してトップ 10 の結果について計算されたスコア。</em></i></figcaption></figure><p><code>jina-embeddings-v2-base-en</code>では、バイナリ量子化により検索精度が 47.13% から 42.05% に低下し、約 10% の損失となります。<code>jina-embeddings-v2-base-de</code>では、この損失は 44.39% から 42.65% とわずか 4% です。</p><p>Jina Embeddings のモデルがバイナリベクトルの生成で優れた性能を発揮するのは、より均一な埋め込みの分布を作成するように訓練されているためです。これは、2 つの異なる埋め込みが、他のモデルの埋め込みよりも多くの次元でより離れている可能性が高いことを意味します。この特性により、それらの距離がバイナリ形式でより適切に表現されることが保証されます。</p><h2 id=\"how-do-binary-embeddings-work\">バイナリ埋め込みはどのように機能するのか？</h2><p>これがどのように機能するかを理解するために、3 つの埋め込み：<em>A</em>、<em>B</em>、<em>C</em> を考えてみましょう。これらはすべて完全な浮動小数点ベクトルで、2 値化されたものではありません。ここで、<em>A</em> から <em>B</em> までの距離が <em>B</em> から <em>C</em> までの距離よりも大きいとします。埋め込みでは、通常<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">コサイン距離</a>を使用します：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p><em>A</em>、<em>B</em>、<em>C</em> を 2 値化すると、<a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">ハミング距離</a>でより効率的に距離を測定できます。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">立方体上のハミング距離。左：A から B までの距離は 1。右：B から C までの距離は 2。</span></figcaption></figure><p><em>A</em>、<em>B</em>、<em>C</em> の 2 値化されたバージョンをそれぞれ <em>A<sub>bin</sub></em>、<em>B<sub>bin</sub></em>、<em>C<sub>bin</sub></em> と呼びましょう。</p>\n<p>バイナリベクトルの場合、<em>A<sub>bin</sub></em> と <em>B<sub>bin</sub></em> 間のコサイン距離が <em>B<sub>bin</sub></em> と <em>C<sub>bin</sub></em> 間よりも大きい場合、<em>A<sub>bin</sub></em> と <em>B<sub>bin</sub></em> 間のハミング距離は <em>B<sub>bin</sub></em> と <em>C<sub>bin</sub></em> 間のハミング距離以上になります。</p>\n<p>したがって：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>の場合、ハミング距離については：</p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>理想的には、埋め込みを 2 値化する際、完全な埋め込みと同じ関係が 2 値化された埋め込みでも成り立つことが望ましいです。つまり、浮動小数点コサインで一方の距離が他方より大きい場合、それらの 2 値化された等価物間のハミング距離でも同様になるべきです：</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>すべての埋め込みの三つ組についてこれを真にすることはできませんが、ほとんどの場合について真にすることは可能です。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">青い点は完全な浮動小数点ベクトルを、赤い点はその 2 値化された等価物を表しています。</span></figcaption></figure><p>バイナリベクトルでは、各次元を存在する（1）か存在しない（0）かのいずれかとして扱うことができます。2 つのベクトルが非バイナリ形式で互いに離れているほど、任意の次元で一方が正の値を持ち、他方が負の値を持つ確率が高くなります。これは、バイナリ形式では、一方が 0 で他方が 1 である次元がより多くなる可能性が高いことを意味します。これによりハミング距離でより離れた位置にあることになります。</p><p>逆に、より近いベクトルについては：非バイナリベクトルが近いほど、任意の次元で両方とも 0 または両方とも 1 を持つ確率が高くなります。これによりハミング距離でより近い位置にあることになります。</p><p>Jina Embeddings のモデルが 2 値化に非常に適しているのは、ネガティブマイニングやその他の微調整の実践を使用して、特に異なるものの間の距離を増加させ、似ているものの間の距離を減少させるように訓練されているためです。これにより、埋め込みはより頑健になり、類似性と違いにより敏感になり、バイナリ埋め込み間のハミング距離が非バイナリ埋め込み間のコサイン距離により比例するようになります。</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">Jina AI のバイナリ埋め込みでどれだけ節約できるか？</h2><p>Jina AI のバイナリ埋め込みモデルを採用することで、時間に敏感なアプリケーションのレイテンシーを低減するだけでなく、以下の表が示すように、かなりのコスト削減効果も得られます：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>モデル</th>\n<th>2億5000万の<br/>埋め込みあたりの<br/>メモリ</th>\n<th>検索<br/>ベンチマーク<br/>平均</th>\n<th>AWS での推定価格<br/>（x2gb インスタンスで<br/>月額 $3.8/GB）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32 ビット浮動小数点埋め込み</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>バイナリ埋め込み</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>このような 95% 以上の削減は、検索精度の低下が約 10% に抑えられています。</p><p>これは、<a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">OpenAI の Ada 2 モデル</a>や<a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Cohere の Embed v3</a> の 2 値化ベクトルを使用する場合よりもさらに大きな削減効果です。これらのモデルは 1024 次元以上の出力埋め込みを生成します。Jina AI の埋め込みは 768 次元しかなく、量子化する前の段階で同じ精度でも他のモデルよりもサイズが小さくなっています。</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">2 値ベクトルはメモリ、計算時間、通信帯域幅、ディスク容量を節約し、複数の面でコスト面でのメリットをもたらします</strong></b>。</div></div><p>これらの節約は環境面でも効果があり、希少材料やエネルギーの使用を抑えることができます。</p><h2 id=\"get-started\">はじめに</h2><p><a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a> を使用して 2 値埋め込みを取得するには、API 呼び出しに <code>encoding_type</code> パラメータを追加し、符号付き整数としてエンコードされた 2 値化埋め込みを取得する場合は <code>binary</code>、符号なし整数の場合は <code>ubinary</code> を値として指定するだけです。</p><h3 id=\"directly-access-jina-embedding-api\">Jina Embedding API に直接アクセス</h3><p><code>curl</code> を使用する場合：</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>または Python の <code>requests</code> API を使用する場合：</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>上記の Python <code>request</code> を使用すると、<code>response.json()</code> を確認することで以下のようなレスポンスが得られます：</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>これらは 96 個の 8 ビット符号付き整数として保存された 2 つの 2 値埋め込みベクトルです。これらを 768 個の 0 と 1 に展開するには、<code>numpy</code> ライブラリを使用する必要があります：</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>結果として、0 と 1 のみからなる 768 次元のベクトルが得られます：</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Qdrant での 2 値量子化の使用</h3><p><a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">Qdrant の統合ライブラリ</a>を使用して、2 値埋め込みを Qdrant ベクトルストアに直接格納することもできます。Qdrant は内部的に <code>BinaryQuantization</code> を実装しているため、ベクトルコレクション全体のプリセット設定として使用でき、コードを変更することなく 2 値ベクトルの検索と保存が可能です。</p><p>使用方法は以下のコード例を参照してください：</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Jina API キーを提供し、利用可能なモデルの1つを選択します。\n# 無料トライアルキーはこちらで取得できます：https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # または \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # small バリアントの場合は 512\n\n# API からエンベディングを取得\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# エンベディングを Qdrant にインデックス化\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>検索を設定するには、<code>oversampling</code> と <code>rescore</code> パラメータを使用する必要があります：</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">LlamaIndex を使用する</h3><p>Jina バイナリエンベディングを LlamaIndex で使用するには、<code>JinaEmbedding</code> オブジェクトをインスタンス化する際に <code>encoding_queries</code> パラメータを <code>binary</code> に設定します：</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# 無料トライアルキーはこちらで取得できます：https://jina.ai/embeddings/\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">バイナリエンベディングをサポートする他のベクトルデータベース</h3><p>以下のベクトルデータベースはバイナリベクトルをネイティブにサポートしています：</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">DataStax の AstraDB</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">例</h2><p>バイナリエンベディングの実際の動作を示すために、<a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a> から選んだアブストラクトに対して、<code>jina-embeddings-v2-base-en</code> を使用して 32 ビット浮動小数点とバイナリベクトルの両方を取得しました。そして、サンプルクエリ「3D segmentation」に対するエンベディングと比較しました。</p><p>下の表から分かるように、上位3件は同じで、上位5件中4件が一致しています。バイナリベクトルを使用しても、ほぼ同じトップマッチが得られています。</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">バイナリ</th>\n  <th colspan=\"2\">32ビット浮動小数点</th>\n</tr>\n<tr>\n<th>ランク</th>\n<th>ハミング<br/>距離</th>\n<th>マッチしたテキスト</th>\n<th>コサイン</th>\n<th>マッチしたテキスト</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "AI モデルのような堅牢で不正確なものに対して、32 ビットの精度は大きすぎます。そこで私たちは 31 ビットを削減しました！バイナリ埋め込みはより小さく、より高速で、高性能です。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
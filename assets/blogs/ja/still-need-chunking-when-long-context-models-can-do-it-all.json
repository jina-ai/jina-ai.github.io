{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "長文脈モデルが全部処理できるのに、チャンキングはまだ必要なのか？",
  "html": "<p>2023年10月、私たちは最長8,192トークンまでの入力を処理できる初のオープンソース埋め込みモデルファミリー「<code>jina-embeddings-v2</code>」を発表しました。これを基に、今年は同様の広範な入力サポートとさらなる機能を備えた「<code>jina-embeddings-v3</code>」をリリースしました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>この投稿では、長文コンテキスト埋め込みについて掘り下げ、いくつかの疑問に答えていきます：大量のテキストを1つのベクトルにまとめることは実用的なのか？セグメント化は検索を改善するのか、そしてどのように？ドキュメントの異なる部分のコンテキストをセグメント化しながら保持するにはどうすればよいのか？</p><p>これらの疑問に答えるため、以下のような埋め込み生成方法を比較します：</p><ul><li>長文コンテキスト埋め込み（ドキュメントで最大8,192トークン）vs 短文コンテキスト（192トークンで切り捨て）</li><li>チャンク分割なし vs 単純チャンク分割 vs <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">後期チャンク分割</a></li><li>単純チャンク分割と後期チャンク分割での異なるチャンクサイズ</li></ul><h2 id=\"is-long-context-even-useful\">長文コンテキストは本当に有用なのか？</h2><p>1つの埋め込みで約10ページのテキストをエンコードできる機能により、長文コンテキスト埋め込みモデルは大規模なテキスト表現の可能性を開きます。しかし、それは本当に有用なのでしょうか？多くの人々によれば…そうではありません。</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">出典：</span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">How AI Is Built ポッドキャストでの Nils Reimer の発言</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">brainlag のツイート</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">egorfine の Hacker News コメント</span></a><span style=\"white-space: pre-wrap;\">、</span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">andy99 の Hacker News コメント</span></a></p></figcaption></figure><p>これらの懸念に対して、長文コンテキスト機能の詳細な調査、長文コンテキストが有用な場合、そしていつ使用すべき（または使用すべきでない）かを見ていきます。まずは、これらの懐疑的な意見を聞き、長文コンテキスト埋め込みモデルが直面する問題を見てみましょう。</p><h2 id=\"problems-with-long-context-embeddings\">長文コンテキスト埋め込みの問題点</h2><p><a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">Jina AI ブログ</a>のような記事のドキュメント検索システムを構築しているとします。時には1つの記事が複数のトピックをカバーすることがあります。例えば、<a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">ICML 2024カンファレンスの参加レポート</a>には以下が含まれています：</p><ul><li>ICML に関する一般的な情報（参加者数、場所、範囲など）を含む序文</li><li>私たちの研究発表（<code>jina-clip-v1</code>）</li><li>ICML で発表された他の興味深い研究論文のまとめ</li></ul><p>この記事に対して1つの埋め込みを作成すると、その埋め込みは3つの異なるトピックの混合を表現することになります：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図1：複数のトピックをカバーするドキュメントを埋め込む場合、結果のベクトルは全段落の混合を表現し、各段落に含まれる個別の具体的な情報が失われる可能性があります。</span></figcaption></figure><p>これにより以下の問題が生じます：</p><ul><li><strong>表現の希薄化：</strong>テキスト内のすべてのトピックは関連している可能性がありますが、ユーザーの検索クエリに関連するのは1つだけかもしれません。しかし、単一の埋め込み（この場合、ブログ投稿全体の埋め込み）はベクトル空間内の1点でしかありません。モデルの入力にテキストが追加されるにつれ、埋め込みは記事全体のトピックを捉えるようにシフトし、特定の段落のコンテンツを表現する効果が低下します。</li><li><strong>容量の制限：</strong>埋め込みモデルは入力の長さに関係なく、固定サイズのベクトルを生成します。入力コンテンツが増えるほど、モデルがこれらの情報をベクトルで表現することが難しくなります。これは画像を16×16ピクセルにダウンスケールするようなものです — りんごのような単純なものの画像なら、スケールダウンしても意味を理解できます。しかし、ベルリンの街路地図をスケールダウンすると？そうはいきません。</li><li><strong>情報の損失：</strong>場合によっては、長文コンテキスト埋め込みモデルでも限界に達します。多くのモデルは最大8,192トークンまでのテキストエンコードをサポートしています。より長いドキュメントは埋め込み前に切り捨てる必要があり、情報の損失につながります。ユーザーに関連する情報がドキュメントの末尾にある場合、それは埋め込みに全く捉えられません。</li><li><strong>テキストのセグメント化が必要な場合もある：</strong>一部のアプリケーションでは、ドキュメント全体ではなく、テキストの特定のセグメントの埋め込みが必要です。例えば、テキスト内の関連する段落を特定する場合などです。</li></ul><h2 id=\"long-context-vs-truncation\">長文コンテキスト vs 切り捨て</h2><p>長文コンテキストが本当に価値があるかどうかを確認するため、2つの検索シナリオのパフォーマンスを見てみましょう：</p><ul><li>最大8,192トークン（約10ページのテキスト）までのドキュメントのエンコード</li><li>192トークンでドキュメントを切り捨て、そこまでをエンコード</li></ul><p>以下の結果を比較します：<code>jina-embeddings-v3</code> の nDCG@10 検索メトリックでテストしました。以下のデータセットをテストしました：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>データセット</th>\n<th>説明</th>\n<th>クエリ例</th>\n<th>ドキュメント例</th>\n<th>平均ドキュメント長（文字数）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>主に PubMed からの 3,244 件のクエリとドキュメントを含む医療検索データセット。</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326,753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>関連する会議セグメントの要約を必要とするクエリベースの会議要約データセット。</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37,445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>長い物語と特定の内容に関する質問を含む QA データセット。</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53,336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>ショートカットを避けるようテンプレートで設計された、最大 5 段階の推論ステップを持つマルチホップ QA データセット。</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30,854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>分散したプロットの統合を必要とする TV シリーズの台本とあらすじを含む脚本要約データセット。</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1,613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>見て分かる通り、192 トークン以上をエンコードすることで顕著な性能向上が得られます：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 2：長文コンテキストの埋め込み性能と短文埋め込み性能の比較</span></figcaption></figure><p>ただし、データセットによって改善の度合いは異なります：</p><ul><li><strong>NFCorpus</strong> では、切り捨てはほとんど影響を与えません。これは、タイトルと要約がドキュメントの先頭にあり、一般的なユーザーの検索語に非常に関連性が高いためです。切り捨てられているかどうかに関わらず、最も関連性の高いデータはトークン制限内に収まっています。</li><li><strong>QMSum</strong> と <strong>NarrativeQA</strong> は、ユーザーが通常テキスト内の特定の事実を検索する「読解」タスクとみなされます。これらの事実は、しばしばドキュメント全体に散らばった詳細の中に埋め込まれており、192 トークンの制限を超えることがあります。例えば、NarrativeQA のドキュメント『Percival Keene』では、「Percival の昼食を盗むいじめっ子は誰？」という質問の答えはこの制限をはるかに超えた位置にあります。同様に、<strong>2WikiMultiHopQA</strong> では、関連情報がドキュメント全体に分散しており、クエリに効果的に答えるためにモデルは複数のセクションから知識を収集して統合する必要があります。</li><li><strong>SummScreenFD</strong> は、与えられた要約に対応する脚本を特定することを目的としたタスクです。要約は脚本全体に分散した情報を含むため、より多くのテキストをエンコードすることで、要約と正しい脚本のマッチング精度が向上します。</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">より良い検索性能のためのテキストセグメント化</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">ここから、3 つの類似した概念について説明します。混乱を避けるため、以下のように呼びます：<br>• <b><strong style=\"white-space: pre-wrap;\">セグメント化</strong></b>：入力テキストの境界キューを検出すること。例えば、文や固定数のトークンなど。<br>• <b><strong style=\"white-space: pre-wrap;\">ナイーブチャンキング</strong></b>：セグメント化のキューに基づいてテキストをチャンクに分割すること（エンコード前）。<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">レイトチャンキング</strong></b></a>：まずドキュメントをエンコードしてから、セグメント化すること（チャンク間のコンテキストを保持）。</div></div><p>ドキュメント全体を 1 つのベクトルに埋め込む代わりに、境界キューを割り当てることでドキュメントを最初にセグメント化するさまざまな方法があります：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 3：テキスト文章への「固定サイズ」、「文ベース」、「意味的」チャンキング手法の適用</span></figcaption></figure><p>一般的な方法には以下があります：</p><ul><li><strong>固定サイズによるセグメント化：</strong> ドキュメントを埋め込みモデルのトークナイザーによって決定された固定数のトークンに分割します。これにより、セグメントのトークン化が全体のドキュメントのトークン化と対応することを保証します（特定の文字数でセグメント化すると異なるトークン化につながる可能性があります）。</li><li><strong>文によるセグメント化：</strong> ドキュメントを文に分割し、各チャンクは <em>n</em> 個の文で構成されます。</li><li><strong>意味によるセグメント化：</strong> 各セグメントは複数の文に対応し、埋め込みモデルが連続する文の類似性を判断します。埋め込みの類似性が高い文は同じチャンクに割り当てられます。</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a> を使用すると、長いテキストをドキュメントの構造に基づいてチャンクやトークン化に分割する無料 API で簡単にセグメント化ができます。</div></div><p>簡単のため、この記事では固定サイズのセグメント化を使用します。</p><h3 id=\"document-retrieval-using-naive-chunking\">ナイーブチャンキングを使用したドキュメント検索</h3><p>固定サイズのセグメント化を実行した後、それらのセグメントに従ってナイーブにドキュメントをチャンク化できます：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 4：セグメント化中に検出された境界キューに基づくナイーブチャンキング。</span></figcaption></figure><p><code>jina-embeddings-v3</code> を使用して、各チャンクをその意味を正確に捉えた埋め込みにエンコードし、それらの埋め込みをベクターデータベースに保存します。</p><p>実行時には、モデルがユーザーのクエリをクエリベクトルにエンコードします。これをチャンク埋め込みのベクターデータベースと比較して、コサイン類似度が最も高いチャンクを見つけ、対応するドキュメントをユーザーに返します：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図5：ナイーブチャンキングによる文書検索の実装：（1）コレクション内の文書を境界キューに基づいてチャンクに分割、（2）埋め込みモデルがすべてのチャンクをエンコードし、結果の埋め込みをデータベースに保存、（3）クエリが入力されると、埋め込みモデルがそれをエンコードし、データベースが最も類似したチャンクを特定。最後に、データベースに保存されているチャンクの文書 ID から関連する文書を特定し、ユーザーに返す。</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">ナイーブチャンキングの問題点</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図6：テキストを文に分割する際、テキストの前の部分への参照を解決できない。</span></figcaption></figure><p>ナイーブチャンキングは長文脈埋め込みモデルの制限の一部に対処しますが、以下のような欠点もあります：</p><ul><li><strong>全体像の欠如：</strong>文書検索において、小さなチャンクの複数の埋め込みでは文書全体のトピックを捉えきれない可能性があります。木を見て森を見ない状態と考えてください。</li><li><strong>コンテキスト欠如の問題：</strong>図6に示すように、文脈情報が欠けているためチャンクを正確に解釈できません。</li><li><strong>効率性：</strong>チャンクが増えるほど、ストレージが必要になり検索時間も増加します。</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">レイトチャンキングによるコンテキスト問題の解決</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">コンテキスト欠如の問題を解決するため、私たちは「レイトチャンキング」と呼ばれる新しい手法を導入しました。詳細は以前のブログ記事をご覧ください：<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">パート I</strong></b></a>、<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">パート II</strong></b></a>、<a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">パート III</strong></b></a>、<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">研究論文</strong></b></a>。</div></div><p>レイトチャンキングは主に2つのステップで動作します：</p><ol><li>まず、モデルの長文脈処理能力を使って文書全体をトークン埋め込みにエンコードします。これにより文書の完全なコンテキストが保持されます。</li><li>次に、セグメンテーション時に特定された境界キューに対応するトークン埋め込みのシーケンスに平均プーリングを適用して、チャンク埋め込みを作成します。</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図7：レイトチャンキング vs ナイーブチャンキング</span></figcaption></figure><p>このアプローチの主な利点は、トークン埋め込みがコンテキスト化されていることです。つまり、文書の他の部分への参照や関係性を自然に捉えることができます。埋め込み処理がチャンキングの前に行われるため、各チャンクは文書の広範なコンテキストを保持し、ナイーブチャンキングが抱えるコンテキスト欠如の問題を解決します。</p><p>モデルの最大入力サイズを超える文書には、「ロングレイトチャンキング」を使用できます：</p><ol><li>まず、文書を重複する「マクロチャンク」に分割します。各マクロチャンクはモデルの最大コンテキスト長（例：8,192トークン）に収まるサイズにします。</li><li>モデルがこれらのマクロチャンクを処理してトークン埋め込みを作成します。</li><li>トークン埋め込みができたら、標準のレイトチャンキングを進め、平均プーリングを適用して最終的なチャンク埋め込みを作成します。</li></ol><p>このアプローチにより、レイトチャンキングの利点を保ちながら、任意の長さの文書を処理できます。モデルが処理できる形に文書を変換し、その後に通常のレイトチャンキング手順を適用する2段階のプロセスと考えてください。</p><p>まとめると：</p><ul><li><strong>ナイーブチャンキング：</strong>文書を小さなチャンクに分割し、各チャンクを個別にエンコードします。</li><li><strong>レイトチャンキング：</strong>文書全体を一度にエンコードしてトークン埋め込みを作成し、その後、セグメント境界に基づいてトークン埋め込みをプーリングしてチャンク埋め込みを作成します。</li><li><strong>ロングレイトチャンキング：</strong>大きな文書をモデルのコンテキストウィンドウに収まる重複マクロチャンクに分割し、それらをエンコードしてトークン埋め込みを取得し、通常通りレイトチャンキングを適用します。</li></ul><p>アイデアの詳細な説明については、私たちの<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">論文</a>や上記のブログ記事をご覧ください。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">チャンキングすべきか否か</h2><p>長文脈埋め込みが一般的に短いテキスト埋め込みよりも優れていることを既に見てきました。また、ナイーブチャンキングとレイトチャンキングの両方の戦略の概要も見てきました。ここで疑問となるのは：チャンキングは長文脈埋め込みより優れているのでしょうか？</p><p>公平な比較を行うため、セグメント化を開始する前にテキスト値をモデルの最大シーケンス長（8,192トークン）に切り詰めます。セグメントあたり64トークンの固定サイズセグメンテーションを使用します（ナイーブセグメンテーションとレイトチャンキングの両方で）。3つのシナリオを比較してみましょう：</p><ul><li><strong>セグメンテーションなし：</strong>各テキストを1つの埋め込みにエンコードします。これは前回の実験（図2参照）と同じスコアになりますが、比較のためにここに含めています。</li><li><strong>ナイーブチャンキング：</strong>テキストをセグメント化し、境界キューに基づいてナイーブチャンキングを適用します。</li><li><strong>レイトチャンキング：</strong>テキストをセグメント化し、レイトチャンキングを使用して埋め込みを決定します。</li></ul><p>レイトチャンキングとナイーブセグメンテーションの両方で、関連文書を決定するためにチャンク検索を使用します（この投稿の前半の図5に示したように）。</p><p>結果は明確な勝者を示していません：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図8：チャンキングなし vs ナイーブチャンキング vs レイトチャンキング</span></figcaption></figure><ul><li><strong>事実検索では、ナイーブチャンキングの方が優れています：</strong> QMSum、NarrativeQA、2WikiMultiHopQA データセットでは、モデルは文書内の関連する段落を特定する必要があります。ここでは、関連情報が含まれているのは少数のチャンクだけで、それらのチャンクが文書全体の単一の埋め込みよりもはるかに良く情報を捉えているため、ナイーブチャンキングが明らかに優れています。</li><li><strong>後期チャンキングは一貫性のある文書と関連コンテキストで最も効果的です：</strong>ユーザーが特定の事実ではなくテーマ全体を検索する一貫性のあるトピックを扱う文書（NFCorpusのような）では、後期チャンキングはチャンキングなしよりもわずかに優れた性能を示します。これは文書全体のコンテキストとローカルな詳細のバランスを取るためです。しかし、後期チャンキングは一般的にコンテキストを保持することでナイーブチャンキングよりも優れていますが、この利点は、ほとんどが無関係な情報を含む文書内で孤立した事実を検索する場合には欠点となる可能性があります - NarrativeQAと2WikiMultiHopQAでの性能低下に見られるように、追加されたコンテキストが役立つよりも混乱を招くことになります。</li></ul><h3 id=\"does-chunk-size-make-a-difference\">チャンクサイズは違いを生むか？</h3><p>チャンキング手法の有効性は実際にデータセットに依存し、コンテンツ構造が重要な役割を果たすことを示しています：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図9：ナイーブチャンキングと後期チャンキングのチャンクサイズの比較。</span></figcaption></figure><p>見てわかるように、後期チャンキングは一般的に小さいチャンクサイズでナイーブチャンキングを上回ります。これは、小さいナイーブチャンクはコンテキストを多く含められないのに対し、小さい後期チャンクは文書全体のコンテキストを保持し、より意味的に有意義になるためです。例外はNarrativeQAデータセットで、無関係なコンテキストが多すぎて後期チャンキングが劣ります。大きいチャンクサイズでは、ナイーブチャンキングはコンテキストが増えるため著しい改善を示し（時には後期チャンキングを上回る）、一方で後期チャンキングの性能は徐々に低下します。</p><h2 id=\"takeaways-when-to-use-what\">要点：どの場合に何を使うべきか？</h2><p>この投稿では、いつセグメンテーションを使用し、いつ後期チャンキングが役立つかをより理解するために、さまざまな種類の文書検索タスクを見てきました。では、何を学んだのでしょうか？</p><h3 id=\"when-should-i-use-long-context-embedding\">長文コンテキスト埋め込みはいつ使うべきか？</h3><p>一般的に、埋め込みモデルの入力に文書のテキストをできるだけ多く含めても検索精度を損なうことはありません。ただし、長文コンテキスト埋め込みモデルは、関連性を判断するのに重要なタイトルや導入部などを含む文書の冒頭に焦点を当てることが多く、文書の中間部分を見落とす可能性があります。</p><h3 id=\"when-should-i-use-naive-chunking\">ナイーブチャンキングはいつ使うべきか？</h3><p>文書が複数の側面をカバーしている場合や、ユーザーのクエリが文書内の特定の情報を対象としている場合、チャンキングは一般的に検索性能を向上させます。</p><p>最終的に、セグメンテーションの決定は、ユーザーへの部分テキストの表示の必要性（例：Googleが検索結果のプレビューで関連する段落を表示するように）やコンピューティングとメモリの制約など、検索オーバーヘッドとリソース使用量の増加によりセグメンテーションが好ましくない場合などの要因に依存します。</p><h3 id=\"when-should-i-use-late-chunking\">後期チャンキングはいつ使うべきか？</h3><p>チャンクを作成する前に文書全体をエンコードすることで、後期チャンキングはコンテキストの欠如によってテキストセグメントが意味を失う問題を解決します。これは特に、各部分が全体と関連する一貫性のある文書で効果的です。私たちの実験では、<a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">論文</a>で示されているように、後期チャンキングはテキストを小さなチャンクに分割する場合に特に効果的です。ただし、一つ注意点があります：文書の部分が互いに無関係な場合、このより広いコンテキストを含めることは、埋め込みにノイズを加えることで、実際に検索性能を低下させる可能性があります。</p><h2 id=\"conclusion\">結論</h2><p>長文コンテキスト埋め込み、ナイーブチャンキング、後期チャンキングの選択は、検索タスクの具体的な要件によって異なります。長文コンテキスト埋め込みは一般的なクエリを持つ一貫性のある文書に有用で、チャンキングはユーザーが文書内の特定の事実や情報を求める場合に優れています。後期チャンキングは、小さなセグメント内のコンテキストの一貫性を保持することで、さらに検索を強化します。最終的に、データと検索目標を理解することで、精度、効率性、コンテキストの関連性のバランスを取る最適なアプローチが決まります。</p><p>これらの戦略を検討している場合は、<code>jina-embeddings-v3</code>を試してみることをお勧めします。その高度な長文コンテキスト機能、後期チャンキング、柔軟性は、多様な検索シナリオに最適な選択肢となります。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "長いコンテキストの埋め込みモデルにおける異なるチャンク分割戦略のパフォーマンスを比較し、目的に最適なアプローチを見つける方法",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
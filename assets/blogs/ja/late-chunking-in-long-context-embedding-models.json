{
  "slug": "late-chunking-in-long-context-embedding-models",
  "id": "66c72e30da9a33000146d836",
  "uuid": "9eda87e2-a799-4360-bac9-6a1cd0193349",
  "title": "長文コンテキストの埋め込みモデルにおけるレイトチャンキング",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Late Chunking は現在 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code> API で利用可能です。<b><strong style=\"white-space: pre-wrap;\">推奨される読み順：パート I、</strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">パート II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、</strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">研究論文</strong></b></a><b><strong style=\"white-space: pre-wrap;\">。</strong></b></div></div><p></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What Late Chunking Really Is &amp; What It's Not: Part II</div><div class=\"kg-bookmark-description\">Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-4.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">新着！パート II：境界キューと誤解に関する詳細な解説。</span></p></figcaption></figure><p>約1年前の2023年10月、私たちは <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">世界初の8Kコンテキスト長を持つオープンソースの埋め込みモデル</a>である <code>jina-embeddings-v2-base-en</code> をリリースしました。それ以来、埋め込みモデルにおける長文コンテキストの有用性について多くの議論がありました。多くのアプリケーションにおいて、数千語に及ぶ文書を単一の埋め込み表現にエンコードすることは理想的ではありません。多くのユースケースではテキストの小さな部分を取り出す必要があり、密ベクトルベースの検索システムは通常、より小さなテキストセグメントの方が、埋め込みベクトルに意味が「過度に圧縮」される可能性が低いため、より良いパフォーマンスを発揮します。</p><p>Retrieval-Augmented Generation（RAG）は、文書を小さなテキストチャンク（例えば512トークン以内）に分割する必要がある最もよく知られたアプリケーションの1つです。これらのチャンクは通常、テキスト埋め込みモデルによって生成されたベクトル表現とともにベクターデータベースに保存されます。実行時には、同じ埋め込みモデルがクエリをベクトル表現にエンコードし、それを使用して関連する保存済みテキストチャンクを特定します。これらのチャンクはその後、大規模言語モデル（LLM）に渡され、LLMは取得したテキストに基づいてクエリに対する応答を生成します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--1-.svg\" class=\"kg-image\" alt=\"Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec\" loading=\"lazy\" width=\"1458\" height=\"307\"><figcaption><span style=\"white-space: pre-wrap;\">典型的な RAG パイプラインのチャンク分割-埋め込み-検索-生成プロセス。</span></figcaption></figure><p>要するに、より小さなチャンクの埋め込みの方が好ましいように見えます。これは下流のLLMの入力サイズが限られているためだけでなく、<strong>長いコンテキストの重要な文脈情報が単一のベクトルに圧縮されると希薄化する可能性があるという懸念</strong>もあるためです。</p><p>しかし、業界が必要とするのが512コンテキスト長の埋め込みモデルだけなら、<em>8192コンテキスト長のモデルを訓練する意味は何なのでしょうか？</em></p><p>この記事では、RAGにおける素朴なチャンク分割-埋め込みパイプラインの限界を探ることで、この重要ではあるが不快な質問を再検討します。私たちは<strong>「Late Chunking」</strong>と呼ばれる新しいアプローチを紹介します。これは、8192長の埋め込みモデルが提供する豊富な文脈情報を活用して、より効果的にチャンクを埋め込む方法です。</p><h2 id=\"the-lost-context-problem\">文脈の消失問題</h2><p>チャンク分割-埋め込み-検索-生成という単純なRAGパイプラインには課題がありません。具体的には、<strong>このプロセスは長距離の文脈依存関係を破壊する可能性があります。</strong>言い換えれば、関連する情報が複数のチャンクに分散している場合、テキストセグメントを文脈から切り離すと効果が失われる可能性があり、このアプローチは特に問題となります。</p><p>下の画像では、Wikipediaの記事が文単位でチャンクに分割されています。「its」や「the city」といったフレーズが、最初の文でのみ言及されている「Berlin」を参照していることがわかります。これにより、埋め込みモデルがこれらの参照を正しいエンティティにリンクすることが難しくなり、結果としてベクトル表現の品質が低下します。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"Comparative panels display Berlin's Wikipedia article and its chunked text to highlight clarity and readability benefits.\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png 1774w\" sizes=\"(min-width: 720px) 720px\"></figure><p>つまり、上の例のように長い記事を文単位のチャンクに分割した場合、RAGシステムは「ベルリンの人口は？」といったクエリに答えるのが困難になる可能性があります。都市名と人口が単一のチャンク内に一緒に現れることがなく、より大きな文書の文脈がないため、LLMはこれらのチャンクの1つを提示されても、「it」や「the city」といった照応参照を解決することができません。</p><p>この問題を緩和するためのヒューリスティックがいくつかあります。スライディングウィンドウによる再サンプリング、複数のコンテキストウィンドウ長の使用、文書の複数回スキャンなどです。しかし、すべてのヒューリスティックと同様に、これらのアプローチは当たり外れがあります。場合によっては機能するかもしれませんが、その効果に理論的な保証はありません。</p><h2 id=\"the-solution-late-chunking\">解決策：Late Chunking</h2><p>素朴なエンコーディングアプローチ（下の画像の左側）では、文、段落、または最大長の制限を使用して<em>事前に</em>テキストを分割します。その後、埋め込みモデルがこれらの結果として得られたチャンクに繰り返し適用されます。各チャンクの単一の埋め込みを生成するために、多くの埋め込みモデルはこれらのトークンレベルの埋め込みに<em>平均プーリング</em>を使用して、単一の埋め込みベクトルを出力します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--4-.svg\" class=\"kg-image\" alt=\"Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.\" loading=\"lazy\" width=\"1020\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">素朴なチャンク分割戦略（左）とLate Chunkingの戦略（右）の比較図。</span></figcaption></figure><p>対照的に、この記事で提案する「Late Chunking」アプローチでは、まず埋め込みモデルのトランスフォーマー層を<em>テキスト全体</em>、あるいは可能な限り多くのテキストに適用します。これにより、テキスト全体からの情報を含む各トークンのベクトル表現のシーケンスが生成されます。その後、このトークンベクトルのシーケンスの各チャンクに平均プーリングを適用し、テキスト全体の文脈を考慮した各チャンクの埋め込みを生成します。独立同分布（i.i.d.）のチャンク埋め込みを生成する素朴なエンコーディングアプローチとは異なり、<strong>Late Chunkingは前のチャンクに「条件付けられた」チャンク埋め込みのセットを作成し、それによって各チャンクにより多くの文脈情報をエンコードします。</strong></p><p>明らかにLate Chunkingを効果的に適用するには、<code>jina-embeddings-v2-base-en</code>のような長文コンテキスト埋め込みモデルが必要です。このモデルは最大8192トークン（標準的な10ページ分のテキストに相当）をサポートします。このサイズのテキストセグメントでは、さらに長いコンテキストを必要とする文脈依存関係が存在する可能性は低くなります。</p><p>重要な点として、Late Chunkingでも境界キューは必要ですが、これらのキューはトークンレベルの埋め込みを取得した<em>後</em>にのみ使用されます—そのため名前に「Late」が付いています。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>素朴なチャンク分割</th>\n<th>Late Chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>境界キューの必要性</td>\n<td>あり</td>\n<td>あり</td>\n</tr>\n<tr>\n<td>境界キューの使用</td>\n<td>前処理で直接</td>\n<td>トランスフォーマー層からトークンレベルの埋め込みを取得した後</td>\n</tr>\n<tr>\n<td>結果として得られるチャンク埋め込み</td>\n<td>i.i.d.</td>\n<td>条件付き</td>\n</tr>\n<tr>\n<td>近隣チャンクの文脈情報</td>\n<td>失われる。一部のヒューリスティック（オーバーラップサンプリングなど）で緩和</td>\n<td>長文コンテキスト埋め込みモデルによって十分に保持</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"implementation-and-qualitative-evaluation\">実装と定性的評価</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/4c9d6ee1a7679cb6c4c106e58fabaf56/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Late Chunkingの実装は上のGoogle Colabで確認できます。ここでは、可能なすべての境界キューを活用して長い文書を意味のあるチャンクに分割する、Tokenizer APIの最新機能リリースを利用しています。この機能の背後にあるアルゴリズムについての詳しい議論はXで見つけることができます。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">テキストをトークン化し、長文をチャンクに分割する無料 API。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,… <a href=\"https://t.co/AtBCSrn7nI?ref=jina-ai-gmbh.ghost.io\">pic.twitter.com/AtBCSrn7nI</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1823756993108304135?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">August 14, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><p>上記の Wikipedia の例に Late Chunking を適用すると、意味的類似性が即座に改善されることがわかります。例えば、Wikipedia 記事内の「the city」と「Berlin」の場合、「the city」を表すベクトルには「Berlin」の以前の言及とリンクする情報が含まれるため、その都市名に関するクエリにとってはるかに適切なマッチとなります。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Query</th>\n<th>Chunk</th>\n<th>Sim. on naive chunking</th>\n<th>Sim. on late chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Berlin</td>\n<td>Berlin is the capital and largest city of Germany, both by area and by population.</td>\n<td>0.849</td>\n<td>0.850</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.</td>\n<td>0.708</td>\n<td>0.825</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.</td>\n<td>0.753</td>\n<td>0.850</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>上記の数値結果で、「Berlin」という用語の埋め込みと、コサイン類似度を使用した Berlin に関する記事からの様々な文との比較を確認できます。「Sim. on IID chunk embeddings」列は、事前チャンキングを使用した「Berlin」のクエリ埋め込みとの類似度値を示し、「Sim. under contextual chunk embedding」は Late Chunking 手法での結果を表しています。</p><h2 id=\"quantitative-evaluation-on-beir\">BEIR での定量的評価</h2><p>Late Chunking の有効性をトイ例以外で検証するため、<a href=\"https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">BeIR</a> の検索ベンチマークの一部を使用してテストしました。これらの検索タスクは、クエリセット、テキストドキュメントのコーパス、各クエリに関連するドキュメントの ID 情報を格納する QRels ファイルで構成されています。</p><p>クエリに関連するドキュメントを特定するために、ドキュメントはチャンク分割され、埋め込みインデックスにエンコードされ、k-最近傍（kNN）を使用して各クエリ埋め込みに最も類似したチャンクが決定されます。各チャンクはドキュメントに対応しているため、チャンクの kNN ランキングをドキュメントの kNN ランキングに変換できます（ランキングに複数回出現するドキュメントは最初の出現のみを保持）。この結果のランキングを Ground-truth の QRels ファイルが提供するランキングと比較し、nDCG@10 などの検索メトリクスを計算します。この手順は以下に示されており、評価スクリプトは再現性のためにこのリポジトリで見つけることができます。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/late-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)</div><div class=\"kg-bookmark-description\">Late Chunking（チャンク化プーリング）の説明と評価のためのコード - jina-ai/late-chunking</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/bf0bb9d5ca928dc3fe25ae621398af0fdf5e34324e37cbeee6fa4189218c9b4d/jina-ai/late-chunking\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>私たちは様々な BeIR データセットでこの評価を実行し、ナイーブチャンキングと Late Chunking 手法を比較しました。境界手がかりを得るために、テキストを約 256 トークンの文字列に分割する正規表現を使用しました。ナイーブチャンキングと Late Chunking の両方の評価で、埋め込みモデルとして <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\"><code>jina-embeddings-v2-small-en</code></a> を使用しました。これは <code>v2-base-en</code> モデルの小型バージョンですが、8192 トークンまでの長さをサポートしています。結果は以下の表の通りです。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Avg. Document Length (characters)</th>\n<th>Naive Chunking (nDCG@10)</th>\n<th>Late Chunking (nDCG@10)</th>\n<th>No Chunking (nDCG@10)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td>1498.4</td>\n<td>64.20%</td>\n<td><strong>66.10%</strong></td>\n<td>63.89%</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td>1116.7</td>\n<td>63.36%</td>\n<td>64.70%</td>\n<td><strong>65.18%</strong></td>\n</tr>\n<tr>\n<td>FiQA2018</td>\n<td>767.2</td>\n<td>33.25%</td>\n<td><strong>33.84%</strong></td>\n<td>33.43%</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>1589.8</td>\n<td>23.46%</td>\n<td>29.98%</td>\n<td><strong>30.40%</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>62.2</td>\n<td>87.19%</td>\n<td>87.19%</td>\n<td>87.19%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>すべてのケースで、Late Chunking はナイーブなアプローチと比較してスコアを改善しました。いくつかの場合では、ドキュメント全体を単一の埋め込みにエンコードするよりも優れた性能を示しましたが、他のデータセットでは、チャンキングを全く行わない方が最良の結果を示しました（もちろん、チャンクのランキングが必要ない場合のみ、チャンキングなしが意味を持ちますが、これは実践ではまれです）。ナイーブなアプローチと Late Chunking の性能差をドキュメントの長さに対してプロットすると、ドキュメントの平均長が Late Chunking による nDCG スコアの改善度と相関していることが明らかになります。つまり、<strong>ドキュメントが長いほど、Late Chunking 戦略はより効果的になります。</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/chart--22-.svg\" class=\"kg-image\" alt=\"0から1500文字までのドキュメント長の増加に伴う相対的改善の低下を示す折れ線グラフ\" loading=\"lazy\" width=\"582\" height=\"337\"><figcaption><span style=\"white-space: pre-wrap;\">Late Chunking のナイーブチャンキングに対する改善度は、平均ドキュメント長と相関関係にあります。</span></figcaption></figure><h2 id=\"conclusion\">結論</h2><p>この記事では、長文脈埋め込みモデルの能力を活用して短いチャンクを埋め込む「Late Chunking」と呼ばれるシンプルなアプローチを紹介しました。従来の i.i.d. チャンク埋め込みが文脈情報を保持できず、最適ではない検索結果につながることを示し、Late Chunking が各チャンク内の文脈情報を維持し条件付けるためのシンプルかつ非常に効果的な解決策を提供することを示しました。Late Chunking の効果は長いドキュメントでより顕著になります—これは <code>jina-embeddings-v2-base-en</code> のような高度な長文脈埋め込みモデルによってのみ可能になった機能です。この研究が長文脈埋め込みモデルの重要性を実証するだけでなく、このトピックに関する更なる研究にも刺激を与えることを期待しています。</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking の本質と誤解：パート II</div><div class=\"kg-bookmark-description\">Late Chunking に関する探求のパート2では、なぜそれがチャンク埋め込みと検索/RAG パフォーマンスの向上に最適な方法なのかについて深く掘り下げます。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-5.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">パート II に続く：境界手がかりと誤解についての詳細な分析。</span></p></figcaption></figure>",
  "comment_id": "66c72e30da9a33000146d836",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-22T14:25:20.000+02:00",
  "updated_at": "2024-10-06T16:29:02.000+02:00",
  "published_at": "2024-08-22T17:06:17.000+02:00",
  "custom_excerpt": "Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael Günther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/late-chunking-in-long-context-embedding-models/",
  "excerpt": "文脈情報を保持しながら長文書をチャンク（断片）化することは課題の多い作業です。私たちは「Late Chunking」を提案します。これは長文脈の埋め込みモデルを活用して、より優れた検索アプリケーションのために文脈を考慮したチャンク埋め込みを生成する手法です。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating the 'Late Chunking' and 'Long Document Model' processes in machine learning on a black background.",
  "feature_image_caption": null
}
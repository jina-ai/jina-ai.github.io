{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "テキスト埋め込みが語順を捉えられない問題とその解決方法",
  "html": "<p><a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> の創設者である Christoph Schuhmann が最近、テキスト埋め込みモデルについて興味深い観察を共有しました：</p><blockquote>文章内の単語をランダムに並び替えた場合でも、その文章の埋め込みの間のコサイン類似度は、元の文章と比べて驚くほど高い値を維持します。</blockquote><p>例えば、2つの文章を見てみましょう：<code>Berlin is the capital of Germany</code> と <code>the Germany Berlin is capital of</code>です。2番目の文章は意味を成しませんが、テキスト埋め込みモデルはこれらを区別することができません。<code>jina-embeddings-v3</code>を使用すると、これらの2つの文章のコサイン類似度は0.9295となります。</p><p>埋め込みが敏感ではないのは語順だけではありません。文法的な変換は文章の意味を大きく変えることがありますが、埋め込み距離にはほとんど影響を与えません。例えば、<code>She ate dinner before watching the movie</code>と<code>She watched the movie before eating dinner</code>は、行動の順序が逆であるにもかかわらず、コサイン類似度は0.9833です。</p><p>否定も、<a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">特別なトレーニング</a>なしでは一貫して埋め込むことが難しいことで知られています。<code>This is a useful model</code>と<code>This is not a useful model</code>は埋め込み空間ではほぼ同じように見えます。「today」を「yesterday」に変更するなど、同じクラスの単語に置き換えたり、動詞の時制を変更したりしても、期待するほど埋め込みは変化しません。</p><p>これには重大な影響があります。2つの検索クエリを考えてみましょう：<code>Flight from Berlin to Amsterdam</code>と<code>Flight from Amsterdam to Berlin</code>です。これらは、<code>jina-embeddings-v3</code>で0.9884というコサイン類似度を示すほど、ほぼ同一の埋め込みを持っています。旅行検索や物流などの実世界のアプリケーションにとって、この欠点は致命的です。</p><p>この記事では、埋め込みモデルが直面する課題を検討し、語順と単語選択に関する持続的な問題を調査します。方向性、時間性、因果関係、比較、否定などの言語カテゴリーにおける主要な失敗モードを分析しながら、モデルのパフォーマンスを向上させる戦略を探ります。</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">なぜ並び替えられた文章のコサイン類似度は驚くほど近いのか？</h2><p>最初、これはモデルが単語の意味を結合する方法に起因するのではないかと考えました - 各単語（上記の例文ではそれぞれ6-7単語）の埋め込みを作成し、平均プーリングでこれらの埋め込みを平均化します。これは、最終的な埋め込みには語順の情報がほとんど含まれないことを意味します。平均は値の順序に関係なく同じです。</p><p>しかし、CLS プーリング（文全体を理解するために特別な最初の単語を見て、語順により敏感であるはず）を使用するモデルでも同じ問題があります。例えば、<code>bge-1.5-base-en</code>でも、<code>Berlin is the capital of Germany</code>と<code>the Germany Berlin is capital of</code>の文に対して0.9304のコサイン類似度を示します。</p><p>これは、埋め込みモデルのトレーニング方法の限界を示しています。言語モデルは事前学習中に文の構造を学習しますが、埋め込みモデルを作成する対照学習の過程でこの理解の一部を失うようです。</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">テキストの長さと語順は埋め込みの類似性にどのように影響するのか？</h2><p>なぜモデルは語順の処理に苦労するのでしょうか？まず思い浮かぶのはテキストの長さ（トークン数）です。テキストがエンコーディング関数に送られると、モデルはまずトークン埋め込みのリスト（つまり、各トークン化された単語にその意味を表す専用のベクトルがある）を生成し、それらを平均化します。</p><p>テキストの長さと語順が埋め込みの類似性にどのように影響するかを確認するために、3、5、10、15、20、30トークンなど、さまざまな長さの180の合成文からなる<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">データセット</a>を生成しました。また、各文のトークンをランダムに並び替えてバリエーションを作成しました：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>以下にいくつかの例を示します：</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">長さ（トークン）</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">元の文章</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">並び替えた文章</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>私たちの<code>jina-embeddings-v3</code>モデルとオープンソースモデルの<code>bge-base-en-v1.5</code>を使用してデータセットをエンコードし、元の文章と並び替えた文章の間のコサイン類似度を計算します：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>長さ（トークン）</th>\n<th>コサイン類似度の平均</th>\n<th>コサイン類似度の標準偏差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.947</td>\n<td>0.053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.909</td>\n<td>0.052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.924</td>\n<td>0.031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.918</td>\n<td>0.019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.899</td>\n<td>0.021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.874</td>\n<td>0.025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>コサイン類似度の傾向をより明確に示すボックスプロットを生成できます：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図1：</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">と</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\">（ファインチューニングなし）を使用した並び替え文の文長別類似度分布</span></figcaption></figure><p>見て分かるように、埋め込みの平均コサイン類似度には明確な線形関係があります。テキストが長くなるほど、元の文とランダムに並び替えた文の間の平均コサイン類似度スコアは低くなります。これは「単語の移動」、つまりランダムな並び替え後に単語が元の位置からどれだけ移動したかによると考えられます。短いテキストでは、トークンが移動できる「スロット」が少ないため、大きく移動できませんが、長いテキストでは潜在的な順列の数が多く、単語はより遠くまで移動することができます。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図2：単語数による文の組み合わせ</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">組み合わせの数は単語数の階乗になるため、ここで表を終えます。30単語になると、265ノニリオン（2.652528598 E+32）もの組み合わせになります。</div></div><p>下図（コサイン類似度 vs 平均単語変位）に示すように、テキストが長くなるほど、単語の変位は大きくなります：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図3：シャッフルされた文データセットにおけるコサイン類似度と平均単語変位の関係を示すグラフ。平均単語変位とコサイン非類似度の相関を表しています。</span></figcaption></figure><p>トークン埋め込みは局所的なコンテキスト（最も近い単語）に依存します。短いテキストでは、単語を並べ替えてもそのコンテキストはあまり変化しません。しかし、長いテキストでは、単語が元のコンテキストから大きく離れる可能性があり、それによってトークン埋め込みが大きく変化する可能性があります。その結果、長いテキストの単語をシャッフルすると、短いテキストよりも埋め込みの距離が大きくなります。上図は、平均プーリングを使用する <code>jina-embeddings-v3</code> と CLS プーリングを使用する <code>bge-base-en-v1.5</code> の両方で、同じ関係が成り立つことを示しています：長いテキストをシャッフルして単語をより遠くに移動させると、類似度スコアが小さくなります。</p><h2 id=\"do-bigger-models-solve-the-problem\">より大きなモデルで問題は解決するか？</h2><p>通常、このような問題に直面した場合、よくある対処法は単により大きなモデルを投入することです。しかし、より大きなテキスト埋め込みモデルが本当に単語の順序情報をより効果的に捉えることができるのでしょうか？テキスト埋め込みモデルのスケーリング則（<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">私たちの <code>jina-embeddings-v3</code> リリース記事で参照</a>）によると、より大きなモデルは一般的により良いパフォーマンスを提供します：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図4：埋め込みモデルのスケーリング則。パラメータ数に応じたMTEBパフォーマンスのスケーリングを示しています。</span></figcaption></figure><p>しかし、より大きなモデルが単語の順序情報をより効果的に捉えることができるのでしょうか？私たちは BGE モデルの3つのバリエーション：<a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>、<a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a>、<a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>をテストしました。それぞれのパラメータサイズは3,300万、1億1,000万、3億3,500万です。</p><p>先ほどと同じ180文を使用しますが、長さの情報は無視します。3つのモデルバリエーションを使用して、元の文とそのランダムシャッフルの両方をエンコードし、平均コサイン類似度をプロットします：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">図5：シャッフルされた文データセットを使用した、モデルサイズが単語順序の感度に与える影響。</span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">、</span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">、</span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">を使用。</span></figcaption></figure><p>より大きなモデルの方が単語の順序の変化に対してより敏感であることが分かりますが、その差は小さいものです。はるかに大きな <code>bge-large-en-v1.5</code> でさえ、シャッフルされた文とシャッフルされていない文を区別する能力はわずかに優れている程度です。埋め込みモデルが単語の並べ替えにどの程度敏感であるかを決定する要因には、特に訓練方法の違いなど、他の要因が関係しています。さらに、コサイン類似度はモデルの区別能力を測定するための非常に限られたツールです。しかし、モデルサイズは主要な考慮事項ではないことが分かります。単純にモデルを大きくするだけではこの問題を解決できません。</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">実世界における単語の順序と単語の選択</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">この記事の大部分では、<a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v2</code></a>（最新モデルの <code>jina-embeddings-v3</code> では<i><em class=\"italic\" style=\"white-space: pre-wrap\">ありません</em></i>）を使用しています。これは <code>v2</code> の方がはるかに小さく、ローカルGPUでの実験が速いためです。パラメータ数は <code>v3</code> の5億8,000万に対して1億3,700万です。</div></div><p>序論で述べたように、単語の順序は埋め込みモデルにとって唯一の課題ではありません。より現実的な実世界の課題は単語の<em>選択</em>についてです。文の中で単語を入れ替える方法は多くあり、それらは埋め込みにうまく反映されません。「She flew from Paris to Tokyo」を「She drove from Tokyo to Paris」に変更しても、埋め込みは類似したままです。私たちはこれをいくつかのカテゴリーに分けて調査しました：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>カテゴリー</th>\n<th>例 - 左</th>\n<th>例 - 右</th>\n<th>コサイン類似度（<code>jina</code>）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>方向性</td>\n<td>She flew from Paris to Tokyo</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>時間的</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>因果関係</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>比較</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>否定</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">これらは私たちの作業中に観察された一般的なケースであり、必ずしもカテゴリーの包括的な分類を表すものではありません。</div></div><p>上の表は、テキスト埋め込みモデルが微妙な単語の変更を捉えられない「失敗ケース」のリストを示しています。これは私たちの予想通りです: テキスト埋め込みモデルには推論する能力が欠けています。例えば、モデルは\"from\"と\"to\"の関係を理解できません。テキスト埋め込みモデルは意味的なマッチングを行いますが、その意味は通常トークンレベルで捉えられ、プーリング後に単一の密なベクトルに圧縮されます。一方、<a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">より大規模なデータセット（兆トークン規模）で訓練された LLM（自己回帰モデル）は、推論のための新たな能力を示し始めています</a>。</p><p>これにより私たちは、クエリとポジティブを近づけ、クエリとネガティブを遠ざけるように、トリプレットを使用した対照学習で埋め込みモデルを微調整できるのではないかと考えました。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 6：対照学習の効果：クエリとポジティブを近づけ、ネガティブを遠ざける</span></figcaption></figure><p>例えば、「Flight from Amsterdam to Berlin」は「Flight from Berlin to Amsterdam」のネガティブペアと考えられます。実際、<a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v1</code> の技術レポート</a>（Michael Guenther ら）では、この問題に小規模に取り組みました：大規模言語モデルで生成した 10,000 例の<a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">否定データセット</a>で <code>jina-embeddings-v1</code> モデルを微調整しました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>上記のレポートに記載された結果は有望でした：</p><blockquote>全てのモデルサイズで、トリプレットデータ（否定訓練データセットを含む）による微調整が、特に HardNegation タスクにおいてパフォーマンスを劇的に向上させることが分かりました。</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 7：</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> の各サイズモデルにおける、ペアワイズ学習とトリプレット/ペアワイズ組み合わせ学習の EasyNegation および HardNegation スコアを示す表</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 8：</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> の異なるバージョン間における訓練戦略のパフォーマンス比較</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">キュレーションされたデータセットによるテキスト埋め込みモデルの微調整</h2><p>前のセクションでは、テキスト埋め込みに関するいくつかの重要な観察を探りました：</p><ol><li>短いテキストは単語の順序を捉えるのに失敗しやすい</li><li>テキスト埋め込みモデルのサイズを大きくしても、単語の順序の理解は必ずしも改善されない</li><li>対照学習がこれらの問題に対する潜在的な解決策となりうる</li></ol><p>これを踏まえて、否定と語順のデータセット（合計約 11,000 の訓練サンプル）で <code>jina-embeddings-v2-base-en</code> と <code>bge-base-en-1.5</code> を微調整しました：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>微調整の評価を助けるため、<code>query</code>、<code>positive (pos)</code>、<code>negative (neg)</code> からなる 1,000 のトリプレットの<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">データセット</a>を生成しました：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>以下は例の行です：</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>これらのトリプレットは、単語の順序の変更による<strong>方向</strong>、<strong>時間</strong>、<strong>因果</strong>の意味の変化を含む、様々な失敗ケースをカバーするように設計されています。</p><p>これで、3 つの異なる評価セットでモデルを評価できます：</p><ol><li>180 の合成文（この投稿の前半から）をランダムにシャッフルしたセット</li><li>手動でチェックした 5 つの例（上の方向/因果関係などの表から）</li><li>先ほど生成した<a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">トリプレットデータセット</a>から選んだ 94 のキュレーションされたトリプレット</li></ol><p>以下は、微調整前後でのシャッフルされた文の違いです：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>文の長さ（トークン数）</th>\n<th>平均コサイン類似度（<code>jina</code>）</th>\n<th>平均コサイン類似度（<code>jina-ft</code>）</th>\n<th>平均コサイン類似度（<code>bge</code>）</th>\n<th>平均コサイン類似度（<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>結果は明確です：ファインチューニングのプロセスはわずか5分しかかかりませんでしたが、ランダムにシャッフルされた文のデータセットにおいて劇的なパフォーマンスの向上が見られました：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図9：シャッフルされた文の文長による類似度分布（</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">および</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\">（ファインチューニング済み）の比較）。</span></figcaption></figure><p>また、方向性、時間的、因果関係、比較のケースでも改善が見られました。モデルは平均コサイン類似度の低下に反映されるように、大幅なパフォーマンスの向上を示しています。最大のパフォーマンス向上は否定のケースで見られ、これはファインチューニングのデータセットに10,000の否定訓練例が含まれていたためです。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>カテゴリー</th>\n<th>例 - 左</th>\n<th>例 - 右</th>\n<th>平均コサイン類似度（<code>jina</code>）</th>\n<th>平均コサイン類似度（<code>jina-ft</code>）</th>\n<th>平均コサイン類似度（<code>bge</code>）</th>\n<th>平均コサイン類似度（<code>bge-ft</code>）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>方向性</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>時間的</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>因果関係</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>比較</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>否定</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">結論</h2><p>この記事では、テキスト埋め込みモデルが直面する課題、特に語順を効果的に処理することの困難さについて詳しく説明しました。具体的には、5つの主要な失敗タイプを特定しました：<strong>方向性</strong>、<strong>時間的</strong>、<strong>因果関係</strong>、<strong>比較</strong>、そして<strong>否定</strong>です。これらは語順が本当に重要なクエリのタイプであり、もしあなたのユースケースにこれらが含まれている場合、これらのモデルの限界を知っておく価値があります。</p><p>また、否定に焦点を当てたデータセットを5つの失敗カテゴリーすべてをカバーするように拡張する簡単な実験も行いました。結果は有望でした：慎重に選択された「ハードネガティブ」でファインチューニングを行うことで、モデルはどのアイテムが関連しているか、していないかをより適切に認識できるようになりました。とはいえ、まだやるべきことは残っています。今後のステップには、データセットのサイズと品質がパフォーマンスにどのように影響するかをより深く調査することが含まれます。</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "テキスト埋め込みモデルは、単語の順序、方向性のある関係、時間の順序、因果関係、比較、否定など、微妙な言語的ニュアンスを捉えることに苦労しています。これらの課題を理解することが、モデルのパフォーマンス向上への鍵となります。",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
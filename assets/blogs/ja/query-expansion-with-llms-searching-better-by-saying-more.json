{
  "slug": "query-expansion-with-llms-searching-better-by-saying-more",
  "id": "67af53142962d20001d63c71",
  "uuid": "581110d6-5791-42f7-b754-16d597390ff7",
  "title": "LLM による検索クエリの拡張：より多くを伝えることでよりよい検索を実現する",
  "html": "<p>クエリ拡張は長年、検索システムを強化するための主要な手法でしたが、semantic embeddings が登場してからは注目度が下がっています。RAG やエージェント検索が主流の現在では時代遅れと思われるかもしれませんが、まだ捨てたものではありません。この詳細な分析では、自動クエリ拡張と <code>jina-embeddings-v3</code> および LLM を組み合わせることで、検索性能を向上させ、より的確な結果を得られる方法を探ります。</p><h2 id=\"what-is-query-expansion\">クエリ拡張とは？</h2><p>クエリ拡張は、<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf-idf</a> やその他の「スパースベクトル」方式のように、クエリの単語とそれを含む文書とのマッチングで関連性を判断する検索システム向けに開発されました。これには明らかな制限があります。「ran」と「running」、または「optimise」と「optimize」のように、単語の変形形がマッチングを妨げます。言語を考慮した前処理でこれらの問題の一部は緩和できますが、すべてではありません。専門用語、同義語、関連語はさらに対処が困難です。例えば、「coronavirus」に関する医学研究の検索では、非常に良いマッチとなるはずの「COVID」や「SARS-CoV-2」について言及している文書を自動的に特定できません。</p><p>クエリ拡張はこの解決策として発明されました。</p><p>アイデアは、良いマッチを見つける可能性を高めるために、クエリに追加の単語やフレーズを加えることです。このようにして、「coronavirus」のクエリに「COVID」や「SARS-CoV-2」という用語が追加され、検索性能を大幅に向上させることができます。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"700\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 1：シソーラスを使用したクエリ拡張フローチャート</span></figcaption></figure><p>クエリにどの用語を追加すべきかを決めるのは簡単ではなく、tf-idf スタイルの検索に適した用語の特定方法や重み付けについて多くの研究がなされてきました。一般的なアプローチには以下があります：</p><ul><li>人手で作成されたシソーラスの使用</li><li>大規模テキストコーパスから関連語を抽出するデータマイニング</li><li>クエリログから類似クエリで使用された他の用語の特定</li><li><a href=\"https://en.wikipedia.org/wiki/Rocchio_algorithm\">ユーザーフィードバックから</a>良いクエリ拡張となる単語やフレーズを学習</li></ul><p>しかし、semantic embedding モデルはクエリ拡張の必要性を排除するはずでした。「coronavirus」、「COVID」、「SARS-CoV-2」の良好なテキスト埋め込みは、埋め込みベクトル空間で非常に近い位置にあるはずです。拡張なしで自然にマッチするはずです。</p><p>ただし、理論的にはそうあるべきですが、実際のモデルによって作成される実際の埋め込みは往々にして期待に及びません。埋め込みの単語は曖昧である可能性があり、適切な単語をクエリに追加することで、より良いマッチに導くことができます。例えば、「skin rash」の埋め込みは、「dermatitis」について言及する医学論文を見逃しながら、「behaving rashly」や「skin creme」に関する文書を特定してしまうかもしれません。関連する用語を追加することで、無関係なマッチから離れ、より良いマッチに向かう可能性が高くなります。</p><h2 id=\"llm-query-expansion\">LLM クエリ拡張</h2><p>シソーラスを使用したり語彙データマイニングを行ったりする代わりに、私たちは LLM を使用してクエリ拡張を行うことを検討しました。LLM には以下のような重要な潜在的利点があります：</p><ul><li><strong>幅広い語彙知識</strong>：大規模で多様なデータセットで訓練されているため、適切なシソーラスの選択や適切なデータの取得について心配する必要が少なくなります。</li><li><strong>判断能力</strong>：提案される拡張用語がすべて特定のクエリに適切とは限りません。LLM は完璧な判断を下せないかもしれませんが、他の方法ではそもそも判断を下すことができません。</li><li><strong>柔軟性</strong>：検索タスクのニーズに応じてプロンプトを調整できますが、他のアプローチは硬直的で、新しいドメインやデータソースに適応させるには多くの作業が必要になる可能性があります。</li></ul><p>LLM が用語リストを提案すると、埋め込みのためのクエリ拡張は従来のクエリ拡張方式と同じように動作します：クエリテキストに用語を追加し、埋め込みモデルを使用してクエリ埋め込みベクトルを作成します。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"850\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 2：LLM による埋め込みのクエリ拡張</span></figcaption></figure><p>これを実現するために必要なものは：</p><ul><li>LLM へのアクセス</li><li>LLM から拡張用語を取得するためのプロンプトテンプレート</li><li>テキスト埋め込みモデル</li></ul><h2 id=\"trying-it-out\">試してみる</h2><p>このアプローチがテキスト情報検索に価値を追加するかどうかを確認するために、いくつかの実験を行いました。テストでは以下を使用しました：</p><ul><li>1つの LLM：Google の <a href=\"https://deepmind.google/technologies/gemini/flash/\">Gemini 2.0 Flash</a></li><li>LLM クエリ拡張がモデル間で一般化できるかを確認するための2つの埋め込みモデル：<code>jina-embeddings-v3</code> と <a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><code>all-MiniLM-L6-v2</code></a></li><li>情報検索のための <a href=\"https://github.com/beir-cellar/beir\">BEIR ベンチマーク</a>のサブセット</li></ul><p>実験は2つのプロンプト条件で実施しました：</p><ul><li>拡張用語を取得するための一般的なプロンプトテンプレートの使用</li><li>タスク固有のプロンプトテンプレートの使用</li></ul><p>また、追加する用語数（100、150、250）を変えてプロンプトを作成しました。</p><p>コードと結果は <a href=\"https://github.com/jina-ai/llm-query-expansion/\">GitHub で公開</a>しており、再現可能です。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/llm-query-expansion/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/llm-query-expansion: Query Expension for Better Query Embedding using LLMs</div><div class=\"kg-bookmark-description\">Query Expension for Better Query Embedding using LLMs - jina-ai/llm-query-expansion</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-1.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/llm-query-expansion\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"results\">結果</h2><h3 id=\"using-a-general-prompt\">一般的なプロンプトの使用</h3><p>試行錯誤の結果、以下のプロンプトが Gemini 2.0 Flash で十分に機能することがわかりました：</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make \nit easier to find the relevant documents (about <span style=\"color:#AADB1E\">{size}</span> words \nper query):\n<span style=\"color:#AADB1E\">{query}</span>\n\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>このプロンプトにより、クエリを20～50のバンドルでバッチ処理し、各クエリに ID を付け、各クエリを拡張用語のリストに接続する JSON 文字列を返すことができます。別の LLM を使用する場合は、それに適したプロンプトを見つけるために実験が必要かもしれません。</p><p>この設定を <code>jina-embeddings-v3</code> に <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model\">非対称検索アダプター</a>を使用して適用しました。このアプローチでは、クエリと文書は異なる方法でエンコードされます — 同じモデルを使用しますが、異なる LoRA 拡張を使用して — 情報検索のための埋め込みを最適化します。</p><p>様々な BEIR ベンチマークでの結果を以下の表に示します。スコアは nDCG@10（検索された上位10項目における<a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\">正規化割引累積利得</a>）です。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>ベンチマーク</th>\n<th>拡張なし</th>\n<th>100用語</th>\n<th>150用語</th>\n<th>250用語</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>（ファクトチェックタスク）</td>\n<td>72.74</td>\n<td>73.39</td>\n<td>74.16</td>\n<td><strong>74.33</strong></td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>（医療検索タスク）</td>\n<td>77.55</td>\n<td>76.74</td>\n<td>77.12</td>\n<td><strong>79.28</strong></td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>（金融オプション検索）</td>\n<td>47.34</td>\n<td><strong>47.76</strong></td>\n<td>46.03</td>\n<td>47.34</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>（医療情報検索）</td>\n<td>36.46</td>\n<td><strong>40.62</strong></td>\n<td>39.63</td>\n<td>39.20</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>（議論検索タスク）</td>\n<td>26.24</td>\n<td>26.91</td>\n<td>27.15</td>\n<td><strong>27.54</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>ここでは、クエリ拡張がほぼすべてのケースで検索性能を向上させていることがわかります。</p><p>このアプローチの堅牢性をテストするため、より小さな埋め込みベクトルを生成する、はるかに小さなモデル <code>all-MiniLM-L6-v2</code> で同じテストを繰り返しました。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">sentence-transformers/all-MiniLM-L6-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-29.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/all-MiniLM-L6-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>以下の表に結果を示します：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>No Expansion</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>(Fact Checking Task)</td>\n<td>64.51</td>\n<td><strong>68.72</strong></td>\n<td>66.27</td>\n<td>68.50</td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>(Medical Retrieval Task)</td>\n<td>47.25</td>\n<td>67.90</td>\n<td><strong>70.18</strong></td>\n<td>69.60</td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>(Financial Option Retrieval)</td>\n<td><strong>36.87</strong></td>\n<td>33.96</td>\n<td>32.60</td>\n<td>31.84</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>(Medical Information Retrieval)</td>\n<td>31.59</td>\n<td><strong>33.76</strong></td>\n<td>33.76</td>\n<td>33.35</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>(Argument Retrieval Task)</td>\n<td>16.90</td>\n<td><strong>25.31</strong></td>\n<td>23.52</td>\n<td>23.23</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>ここでは、検索スコアのさらに大きな改善が見られます。全体的に、小さなモデルの方がクエリ拡張からより大きな恩恵を受けました。すべてのタスクにおける平均的な改善は以下の表にまとめられています：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>jina-embeddings-v3</code></td>\n<td>+1.02</td>\n<td>+0.75</td>\n<td><strong>+1.48</strong></td>\n</tr>\n<tr>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>+6.51</strong></td>\n<td>+5.84</td>\n<td>+5.88</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>2つのモデル間の正味の改善の大きな違いは、<code>all-MiniLM-L6-v2</code> の初期性能が低かったことによると考えられます。非対称検索モードにおける <code>jina-embeddings-v3</code> によって生成されるクエリ埋め込みは、主要な意味的関係をより良く捉えることができるため、クエリ拡張が情報を追加する余地が少なくなります。しかし、この結果は、大規模モデルよりも小規模モデルが好ましい場合があることを示しており、クエリ拡張によってそのような小規模モデルの性能がどれだけ向上するかを示しています。</p><p>それでもなお、<code>jina-embeddings-v3</code> のような高性能モデルでも、クエリ拡張は検索に意味のある改善をもたらしましたが、この結果はすべてのタスクと条件で完全に一貫しているわけではありません。</p><p><code>jina-embeddings-v3</code> の場合、FiQA と NFCorpus のベンチマークでは、クエリに 100 個以上の用語を追加することは逆効果でした。より多くの用語が常に良いとは言えませんが、他のベンチマークの結果は、より多くの用語が少なくとも時には効果的であることを示しています。</p><p><code>all-MiniLM-L6-v2</code> の場合、150 個以上の用語を追加することは常に逆効果でした。3つのテストでは、100 個以上の追加は性能を向上させませんでした。1つのテスト（FiQA）では、100 個の用語を追加しただけでも結果が大幅に低下しました。これは、<code>jina-embeddings-v3</code> が長いクエリテキストの意味情報をより良く捉えることができるためだと考えています。</p><p>両モデルとも、FiQA と NFCorpus のベンチマークではクエリ拡張への反応が小さくなりました。</p><h2 id=\"using-task-specific-prompting\">タスク固有のプロンプトの使用</h2><p>上記の結果のパターンは、クエリ拡張が有用である一方で、LLM を使用すると性能を低下させる不適切なクエリ用語が追加されるリスクがあることを示唆しています。これはプロンプトの一般的な性質に起因している可能性があります。</p><p>私たちは SciFact と FiQA という2つのベンチマークを取り上げ、以下のようなよりドメイン固有のプロンプトを作成しました：</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make\nit easier to find the <span style=\"background-color:red\">relevant documents</span> <span style=\"background-color:green\">scientific document \nthat supports or rejects the scientific fact in the query \nfield</span> (about <span style=\"color:#AADB1E\">{size}</span> words per query):\n<span style=\"color:#AADB1E\">{query}</span>\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>このアプローチは、ほぼすべての面で検索性能を向上させました：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>No Expansion</th>\n<th>100<br/>terms</th>\n<th>150<br/>terms</th>\n<th>250<br/>terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>72.74</td>\n<td><strong>75.85 (+2.46)</strong></td>\n<td>75.07 (+0.91)</td>\n<td>75.13 (+0.80)</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td>64.51</td>\n<td><strong>69.12 (+0.40)</strong></td>\n<td>68.10 (+1.83)</td>\n<td>67.83 (-0.67)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>47.34</td>\n<td>47.77 (+0.01)</td>\n<td><strong>48.20 (+1.99)</strong></td>\n<td>47.75 (+0.41)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>36.87</strong></td>\n<td>34.71 (+0.75)</td>\n<td>34.68 (+2.08)</td>\n<td>34.50 (+2.66)</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><code>all-MiniLM-L6-v2</code> で SciFact クエリに 250 個の用語を追加した場合を除き、すべての条件で性能が向上しました。さらに、この改善は <code>all-MiniLM-L6-v2</code> が FiQA での自身のベースラインを上回るには十分ではありませんでした。</p><p><code>jina-embeddings-v3</code> の場合、最良の結果は 100 または 150 個の追加用語で得られました。250 個の用語を追加することは逆効果でした。これは、特に用語の意味が目標から離れ始めると、クエリに追加する用語が多すぎる可能性があるという私たちの直感を裏付けています。</p><h2 id=\"key-considerations-in-query-expansion\">クエリ拡張における重要な考慮事項</h2><p>クエリ拡張は明らかに埋め込みベースの検索に利点をもたらしますが、いくつかの注意点があります：</p><h3 id=\"expense\">コスト</h3><p>LLM との対話は情報検索にレイテンシーと計算コストを追加し、商用 LLM を使用する場合は実際のコストが発生する可能性があります。この程度の改善では、そのコストを正当化できない場合があります。</p><h3 id=\"prompt-engineering\">プロンプトエンジニアリング</h3><p>良いプロンプトテンプレートの設計は経験的で実験的な技術です。この研究で使用したものが最適であるとか、他の LLM に移植可能であるとは主張しません。タスク固有のプロンプトに関する実験は、プロンプトの変更が結果の品質に非常に大きな影響を与える可能性があることを示しています。また、結果はドメインによって大きく異なります。</p><p>これらの不確実性は開発コストを増加させ、保守性を低下させます。システムの変更（LLM、埋め込みモデル、情報ドメインの変更）は、プロセス全体の再確認と場合によっては再実装を必要とします。</p><h3 id=\"alternatives\">代替案</h3><p>ここでは、クエリ拡張が最も大きな改善をもたらしたのは、初期性能が最も低い埋め込みモデルでした。少なくともこの実験では、クエリ拡張は <code>all-MiniLM-L6-v2</code> と <code>jina-embeddings-v3</code> の性能差を埋めることはできませんでした。一方、<code>jina-embeddings-v3</code> はクエリ拡張からより控えめな改善を得ました。</p><p>このような状況では、<code>all-MiniLM-L6-v2</code> のユーザーは、クエリ拡張を追求するよりも <code>jina-embeddings-v3</code> を採用する方が、より低コストでより良い結果が得られるでしょう。</p><h2 id=\"future-directions\">今後の方向性</h2><p>ここでは、クエリ拡張がクエリ埋め込みを改善できること、そして LLM が良いクエリ拡張用語を得るためのシンプルで柔軟な手段を提供することを示しました。しかし、比較的控えめな改善は、さらなる研究が必要であることを示唆しています。私たちは以下のような方向性を検討しています：</p><ul><li>文書埋め込みの生成における用語強化の価値のテスト。</li><li>リランキングなどの他の AI 検索技術におけるクエリ強化の可能性の検討。</li><li>シソーラスのような、より計算コストの低い従来の用語ソースと LLM ベースのクエリ拡張の比較。</li><li>クエリ拡張に特化した言語モデルのトレーニングと、より領域特化したトレーニングの提供。</li><li>追加する用語数の制限。100 個では最初から多すぎる可能性があります。</li><li>有用な拡張用語と不適切な拡張用語を識別する方法の探索。クエリ拡張に設定する固定数は完璧な解決策とはならず、提案された用語を動的に評価して良い用語のみを保持できれば、性能の向上が見込めます。</li></ul><p>これは非常に初期段階の研究であり、このような技術が Jina AI の検索基盤製品にさらなる改善をもたらすことを期待しています。</p>",
  "comment_id": "67af53142962d20001d63c71",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/02/query-banner.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-02-14T15:28:36.000+01:00",
  "updated_at": "2025-02-18T03:24:20.000+01:00",
  "published_at": "2025-02-18T03:24:20.000+01:00",
  "custom_excerpt": "Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael Günther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/query-expansion-with-llms-searching-better-by-saying-more/",
  "excerpt": "埋め込みモデルが導入されて以来、検索は大きく変化しました。AI においてクエリ拡張のような語彙的手法にはまだ役割があるのでしょうか？私たちはそう考えています。",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
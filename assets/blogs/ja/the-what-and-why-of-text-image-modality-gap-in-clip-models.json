{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "CLIP モデルにおけるテキスト・画像のモダリティギャップとは何か、そしてなぜ重要なのか",
  "html": "<p><a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">セマンティック埋め込み</a>は、チャットボットや AI アート モデルを含む最新の AI モデルの中核です。ユーザーからは見えないことがありますが、表面のすぐ下に潜んでいます。</p><p>埋め込みの理論は 2 つの部分から成り立っています：</p><ol><li>物事 — AI モデルの外部にあるテキストや画像などの物事は、それらの物事に関するデータから AI モデルによって作成されたベクトルによって表現されます。</li><li>AI モデルの外部にある物事の関係性は、それらのベクトル間の空間的な関係によって表現されます。私たちは、そのように機能するベクトルを作成するように AI モデルを特別にトレーニングします。</li></ol><p>画像とテキストのマルチモーダルモデルを作成する場合、画像の埋め込みとそれらの画像を説明または関連するテキストの埋め込みが比較的近くなるようにモデルをトレーニングします。これら 2 つのベクトルが表す物事（画像とテキスト）の間の意味的な類似性は、2 つのベクトル間の空間的な関係に反映されます。</p><p>例えば、オレンジの画像の埋め込みベクトルと「新鮮なオレンジ」というテキストのベクトルは、同じ画像と「新鮮なりんご」というテキストよりも近くなることが合理的に予想されます。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration on a black background showing an orange and an apple with arrows between them and quotes reading &quot;A fresh orange\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>これが埋め込みモデルの目的です：私たちが気にする特徴（画像に描かれている果物の種類やテキストで名付けられている果物など）が、それらの間の距離に保持される表現を生成することです。</p><p>しかしマルチモーダリティは別の要素をもたらします。オレンジの写真は「新鮮なオレンジ」というテキストよりもりんごの写真に近く、「新鮮なりんご」というテキストはりんごの画像よりも別のテキストに近いことがわかるかもしれません。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Black background featuring an apple on the left and an orange on the right with annotated arrows marked &quot;A fresh apple.&quot; and \" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>これは、Jina AI の <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP モデル</a>（<code>jina-clip-v1</code>）を含むマルチモーダルモデルで実際に起こることです。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>これを検証するために、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k テストセット</a>から 1,000 のテキスト-画像ペアをサンプリングしました。各ペアには 5 つのキャプションテキスト（technically にはペアではありません）と 1 つの画像が含まれており、5 つのテキストはすべて同じ画像を説明しています。</p><p>例えば、以下の画像（Flickr8k データセットの <code>1245022983_fb329886dd.jpg</code>）：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"A young girl in a pink skirt playing with a frisbee in an urban outdoor setting with cars and bikes present.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>その 5 つのキャプション：</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>Jina CLIP を使用して画像とテキストを埋め込み、次のことを行いました：</p><ol><li>画像埋め込みとそのキャプションテキストの埋め込みのコサイン類似度を比較。</li><li>同じ画像を説明する 5 つのキャプションテキストの埋め込みを取り、それらのコサイン類似度を相互に比較。</li></ol><p>結果は図 1 に示すように、驚くほど大きな差が見られました：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graph with two curves showing the distribution of Cosine Similarity for Image2Text and Text2Text pairs with labeled axes.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図 1：Jina CLIP における一致する画像-テキストペアとテキスト-テキストペア間のコサイン類似度の分布。</span></figcaption></figure><p>わずかな例外を除いて、一致するテキストペアは一致する画像-テキストペアよりもはるかに近接しています。これは、Jina CLIP が埋め込み空間の一部にテキストをエンコードし、画像を比較的遠い別の部分にエンコードしていることを強く示しています。このテキストと画像の間の空間が<em>マルチモーダルギャップ</em>です。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagram on black background depicting 'Images' on left, 'Texts' on bottom, with labeled 'Multimodal Gap' in the center.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>マルチモーダル埋め込みモデルは、私たちが気にする意味的な情報以上のものをエンコードしています：入力の媒体をエンコードしているのです。Jina CLIP によると、「一枚の絵は千の言葉に値する」という諺は当てはまりません。どれだけ言葉を重ねても完全には等価にならない内容を持っているのです。誰もトレーニングしていないのに、入力の媒体をその埋め込みの意味にエンコードしています。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">画像とテキストを比較するだけなら問題ありませんが、真のマルチモーダルモデルは、例えば「これはりんごです」というテキストが、オレンジについてのテキストよりもりんごの画像により良くマッチすることを判断できるはずです。現在の形態の CLIP スタイルのモデルにはそれができません。</div></div><p>この現象は、論文『Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning』[<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>]で「モダリティギャップ」として研究されています。モダリティギャップとは、埋め込み空間における、ある媒体の入力と別の媒体の入力との間の空間的な分離のことです。モデルはそのようなギャップを意図的にトレーニングされているわけではありませんが、マルチモーダルモデルに広く見られます。</p><p>Jina CLIP におけるモダリティギャップに関する私たちの調査は、<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>に大きく基づいています。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">モダリティギャップはどこから来るのか？</h2><p>Liang et al. [2022]は、モダリティギャップの主要な 3 つの源を特定しています：</p><ul><li>「コーン効果」と呼ばれる初期化バイアス。</li><li>トレーニング中の温度（ランダム性）の低下により、このバイアスを「アンラーン」することが非常に困難になる。</li><li>マルチモーダルモデルで広く使用される対照学習手順が、意図せずにギャップを強化する。</li></ul><p>これらを順番に見ていきましょう。</p><h3 id=\"cone-effect\">コーン効果</h3><p>CLIP または CLIP スタイルのアーキテクチャで構築されたモデルは、実際には2つの独立した埋め込みモデルが連携したものです。画像-テキストのマルチモーダルモデルの場合、以下のスキーマのように、テキストをエンコードするモデルと、画像をエンコードする完全に別個のモデルがあります。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>これら2つのモデルは、テキストが画像を適切に説明している場合、画像の埋め込みとテキストの埋め込みが比較的近くなるように訓練されます。</p><p>このようなモデルは、両方のモデルの重みをランダム化し、画像とテキストのペアを同時に提示して、2つの出力間の距離を最小化するようにゼロから訓練することで構築できます。<a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">オリジナルの OpenAI CLIP モデル</a>はこの方法で訓練されました。ただし、これには多くの画像-テキストペアと計算コストの高い訓練が必要です。最初の CLIP モデルでは、OpenAI はインターネット上のキャプション付き素材から4億組の画像-テキストペアを収集しました。</p><p>より最近の CLIP スタイルのモデルは事前訓練されたコンポーネントを使用します<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">。</a>これは、テキスト用と画像用それぞれの良好な単一モード埋め込みモデルとして、各コンポーネントを個別に訓練することを意味します。その後、これら2つのモデルは画像-テキストペアを使用して一緒に訓練され、この過程は<em>対照チューニング</em>と呼ばれます。一致する画像-テキストペアを使用して、一致するテキストと画像の埋め込みをより近づけ、一致しないものをより遠ざけるように重みを徐々に「調整」します。</p><p>この手法は一般的に、取得が困難で高コストな画像-テキストペアのデータを少なくて済み、より入手が容易なキャプションのない単純なテキストと画像を大量に使用します。Jina CLIP（<code>jina-clip-v1</code>）はこの後者の方法で訓練されました。一般的なテキストデータを使用して<a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2 モデル</a>をテキストエンコーディング用に事前訓練し、事前訓練された<a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">EVA-02 画像エンコーダー</a>を使用し、<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a>で概説されているように、様々な対照訓練技術を用いてさらに訓練しました。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図2：Jina CLIP におけるペア訓練前の画像とテキストの埋め込みの初期位置を2次元に投影したもの。</span></figcaption></figure><p>これらの2つの事前訓練されたモデルを取り、画像-テキストペアで訓練する前の出力を見ると、重要なことに気付きます。図2（上）は、事前訓練された EVA-02 エンコーダーが生成した画像埋め込みと事前訓練された JinaBERT v2 が生成したテキスト埋め込みを<a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">UMAP</a>で2次元に投影したもので、灰色の線は一致する画像-テキストペアを示しています。これはクロスモーダル訓練の前の状態です。</p><p>結果は、一方の端に画像埋め込み、もう一方の端にテキスト埋め込みがある、一種の切断された「円錐」形状になります。この円錐形状は2次元投影では十分に表現できませんが、上の画像でおおよその形を見ることができます。すべてのテキストは埋め込み空間の一部に、すべての画像は別の部分にクラスター化されています。訓練後も、テキストが一致する画像よりも他のテキストに類似している場合、この初期状態がその大きな理由です。画像と最もマッチするテキスト、テキストとテキスト、画像と画像のマッチングという目的は、この円錐形状と完全に両立します。</p><p>モデルは生まれた時から偏見を持っており、学習してもそれは変わりません。図3（下）は、画像-テキストペアを使用して完全に訓練された後のリリースされた Jina CLIP モデルの同じ分析です。マルチモーダルのギャップは、むしろさらに顕著になっています。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図3：Jina CLIP のペア訓練後の画像とテキストの埋め込みの位置を2次元に投影したもの。</span></figcaption></figure><p>広範な訓練の後でも、Jina CLIP はメディアをメッセージの一部としてエンコードし続けています。</p><p>完全にランダムな初期化を使用する、より計算コストの高い OpenAI のアプローチを使用しても、このバイアスは取り除けません。オリジナルの OpenAI CLIP アーキテクチャを取り、すべての重みを完全にランダム化して、上記と同じ分析を行いました。結果は図4に示すように、依然として切断された円錐形状になります：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図4：完全にランダム化された重みで訓練を全く行っていない Jina CLIP の画像とテキストの埋め込みの初期位置を2次元に投影したもの。</span></figcaption></figure><p>このバイアスは構造的な問題であり、解決策がない可能性があります。もしそうであれば、訓練中にこれを補正するか軽減する方法を探すしかありません。</p><h3 id=\"training-temperature\">訓練温度</h3><p>AI モデルの訓練中、通常はプロセスにランダム性を加えます。訓練サンプルのバッチがモデルの重みをどれだけ変更すべきかを計算し、実際に重みを変更する前にそれらの変更に小さなランダム要因を加えます。このランダム性の量を、熱力学での使用方法にちなんで<em>温度</em>と呼びます。</p><p>高い温度はモデルに非常に速い大きな変化を生み出し、低い温度はモデルが訓練データを見るたびに変更できる量を減少させます。結果として、高い温度では個々の埋め込みが訓練中に埋め込み空間内で大きく移動することが予想され、低い温度では移動が非常にゆっくりになります。</p><p>AI モデルの訓練のベストプラクティスは、高い温度から始めて徐々に下げることです。これにより、重みがランダムか目標から遠い初期段階で大きな学習の飛躍を可能にし、その後より安定的に詳細を学習できるようになります。</p><p>Jina CLIP の画像-テキストペア訓練は、0.07の温度（これは比較的高い温度です）から始まり、図5に示すように、訓練ステップに応じて指数関数的に0.01まで低下させます：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図5：Jina CLIP のペア訓練中の温度の減衰。</span></figcaption></figure><p>私たちは、温度を上げる — ランダム性を増やす — ことで円錐効果が減少し、画像埋め込みとテキスト埋め込みが全体的により近づくかどうかを知りたいと考えました。そこで、固定温度0.1（非常に高い値）で Jina CLIP を再訓練しました。各訓練エポックの後、図1のように、画像-テキストペアとテキスト-テキストペア間の距離の分布を確認しました。結果は以下の図6に示されています：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">図6：訓練温度が高い場合、時間とともにモダリティ間のギャップが縮小します。</span></figcaption></figure><p>ご覧の通り、高い温度を維持することでモダリティ間のギャップは劇的に縮小します。訓練中に埋め込みが大きく動くことを許容することで、埋め込み分布の初期バイアスを克服する大きな助けとなります。</p><p>しかし、これにはコストが伴います。私たちは6つの異なる検索テストでモデルのパフォーマンスをテストしました：<a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a>のデータセットから、3つのテキスト-テキスト検索テストと3つのテキスト-画像検索テストを行いました。図7に示すように、すべてのテストで訓練初期にパフォーマンスが急落し、その後非常にゆっくりと上昇することが分かります：</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">図7：訓練中のパフォーマンス。最初は初期状態から急激な低下があり、その後非常にゆっくりと上昇します。</span></figcaption></figure><p>この一定の高温度を使用して Jina CLIP のようなモデルを訓練するには、極めて時間がかかり、コストがかかるでしょう。理論的には可能ですが、これは実用的な解決策ではありません。</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">対照学習と誤った負例の問題</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> はまた、標準的な対照学習の実践—CLIP スタイルのマルチモーダルモデルを訓練するために使用するメカニズム—がマルチモーダルギャップを強化する傾向があることを発見しました。</p><p>対照学習は基本的に単純な概念です。画像埋め込みとテキスト埋め込みがあり、それらがより近くにあるべきことを知っているので、訓練中にモデルの重みを調整してそうなるようにします。ゆっくりと進め、重みを少しずつ調整し、2つの埋め込みがどれだけ離れているかに比例して調整します：近ければ近いほど、変更は小さくなります。</p><p>この技術は、一致する場合に埋め込みを近づけるだけでなく、一致しない場合にそれらを遠ざける場合の方がはるかに効果的です。一緒になるべき画像-テキストのペアだけでなく、離れているべきペアも必要です。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>これには以下の問題があります：</p><ol><li>データソースは完全に一致するペアのみで構成されています。人間が無関係であると確認したテキストと画像のデータベースを作成する人はいませんし、ウェブのスクレイピングやその他の教師なしまたは半教師あり手法で容易に構築することもできません。</li><li>表面的には完全に無関係に見える画像とテキストのペアであっても、必ずしもそうとは限りません。そのような否定的な判断を客観的に下すことができる意味論の理論は持ち合わせていません。例えば、ポーチで横たわる猫の画像は、「ソファで眠る人」というテキストと完全な不一致とは言えません。どちらも何かの上で横たわっているという共通点があります。</li></ol><p>理想的には、確実に関連がある AND 関連がない画像-テキストのペアで訓練したいところですが、関連がないことが分かっているペアを得る明確な方法はありません。人々に「このテキストはこの画像を説明していますか？」と尋ねて一貫した回答を期待することは可能です。しかし「このテキストはこの画像と全く関係がありませんか？」と尋ねて一貫した回答を得ることは、はるかに困難です。</p><p>その代わりに、訓練データからランダムに画像とテキストを選択して、関連のないペアを作成し、実質的に常に不適切なマッチングになることを期待します。実際の仕組みとしては、訓練データをバッチに分割します。Jina CLIP の訓練では、32,000個の一致する画像-テキストペアを含むバッチを使用しましたが、この実験ではバッチサイズは16のみでした。</p><p>以下の表は Flickr8k からランダムにサンプリングした16個の画像-テキストペアです：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>一致しないペアを得るために、バッチ内のすべての画像を、それに一致するもの以外のすべてのテキストと組み合わせます。例えば、以下のペアは一致しない画像とテキストです：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>キャプション：</strong>ピンク色の服を着た少女が花を摘んでいます。</p><p>しかし、この手順は、他の画像に一致するすべてのテキストが等しく不適切なマッチであると仮定しています。これは必ずしも真実ではありません。例えば：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>キャプション：</strong>犬が雪の吹きだまりの横に座っています。</p><p>このテキストはこの画像を説明してはいませんが、犬という共通点があります。このペアを不一致として扱うと、「犬」という単語をどの犬の画像からも遠ざける傾向が生まれます。</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> は、これらの不完全な不一致ペアがすべての画像とテキストを互いに遠ざけることを示しています。</p><p>私たちは、完全にランダムに初期化された <code>vit-b-32</code> 画像モデルと同様にランダム化された JinaBERT v2 テキストモデルを使用して、訓練温度を一定の0.02（やや低い温度）に設定して、彼らの主張を検証することにしました。2つの訓練データセットを構築しました：</p><ul><li>Flickr8k からランダムに抽出したバッチで、上記のように構築された不一致ペアを含むもの。</li><li>同じ画像の複数のコピーに異なるテキストを付けたものを各バッチに意図的に含むように構築したもの。これにより、「不一致」ペアの相当数が実際には互いに正しい一致であることが保証されます。</li></ul><p>その後、それぞれのデータセットで2つのモデルを1エポック訓練し、各モデルについて Flickr8k データセットの1,000のテキスト-画像ペア間の平均コサイン距離を測定しました。ランダムバッチで訓練されたモデルの平均コサイン距離は0.7521で、意図的に一致する「不一致」ペアを多く含むモデルの平均コサイン距離は0.7840でした。不正確な「不一致」ペアの影響はかなり大きいことが分かります。実際のモデル訓練ははるかに長く、はるかに多くのデータを使用することを考えると、この効果が成長し、画像とテキスト全体の間のギャップを拡大することが分かります。</p><h2 id=\"the-medium-is-the-message\">メディアがメッセージである</h2><p>カナダのコミュニケーション理論家<a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">マーシャル・マクルーハン</a>は、1964年の著書<a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">『メディア論—人間の拡張の諸相』</a>で「メディアがメッセージである」という言葉を生み出し、メッセージは自律的ではないことを強調しました。メッセージは意味に大きく影響を与える文脈の中で私たちに届き、彼は有名に、その文脈の最も重要な部分の1つがコミュニケーションメディアの性質であると主張しました。</p><p>マルチモーダリティ・ギャップは、AI モデルにおける創発的な意味現象のクラスを研究する独特の機会を提供してくれます。Jina CLIP に対して、学習データの媒体をエンコードするように誰も指示していませんでした — それなのに、そうしてしまったのです。マルチモーダルモデルの問題を解決できていないとしても、少なくとも問題の発生源については理論的によく理解できています。</p><p>同じような偏りにより、私たちのモデルは、まだ探していない他の要素もエンコードしているはずだと考えるべきです。例えば、多言語埋め込みモデルでも同様の問題が存在する可能性が高いです。特に類似の学習手法が広く使用されているため、2つ以上の言語での共同学習は、おそらく言語間で同じようなギャップを生み出しています。このギャップ問題の解決策は、非常に広範な影響を持つ可能性があります。</p><p>より広範なモデルにおける初期化バイアスの調査も、新たな洞察につながるでしょう。埋め込みモデルにとってメディアがメッセージであるならば、私たちの気付かないうちに、他にどのような情報がモデルにエンコードされているのでしょうか？</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "CLIP モデルを使用してテキストと画像を検索し、スコアで結果を並び替えるだけでは不十分です。なぜでしょうか？それはモダリティギャップが存在するからです。このモダリティギャップとは何か、そしてどこから生じるのでしょうか？",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
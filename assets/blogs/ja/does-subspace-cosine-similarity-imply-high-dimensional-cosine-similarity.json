{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "部分空間におけるコサイン類似度は高次元空間のコサイン類似度を示唆するか？",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">2024 年 1 月 25 日、OpenAI は<a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">新しい embedding モデル</a>を公開し、<i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">「短縮化」</strong></b></i>と呼ばれる新機能を導入しました。これによって開発者は、概念を効果的に表現する能力を損なうことなく、シーケンスの末尾から数値を削除して embedding を短縮できるようになりました。このイノベーションの実現可能性と理論的根拠について、この投稿で詳しく解説します。</div></div><p>考えてみてください：高次元空間における embedding ベクトルのコサイン類似度を測定する際、より低次元の部分空間での類似度は全体の類似度にどのように影響するのでしょうか？直接的な比例関係があるのでしょうか、それとも高次元データではより複雑な関係があるのでしょうか？</p><p>より具体的には、<strong>ベクトルの最初の 256 次元での高い類似度は、768 次元全体でも高い類似度を保証するのでしょうか？</strong>逆に、いくつかの次元で大きな違いがある場合、それは全体の類似度が低いことを意味するのでしょうか？これらは単なる理論的な考察ではなく、効率的なベクトル検索、データベースのインデックス作成、RAG システムのパフォーマンスにとって重要な検討事項です。</p><p>開発者は多くの場合、部分空間での高い類似度は全体の高い類似度を意味する、あるいは一つの次元での顕著な違いが全体の類似度に大きく影響するといったヒューリスティックに頼っています。問題は、これらのヒューリスティック手法が確固たる理論的根拠に基づいているのか、それとも単なる便宜的な仮定なのかということです。</p><p>この投稿では、これらの疑問について、部分空間の類似度と全体のベクトル類似度の関係について、理論と実践的な意味の両面から検討します。</p><h2 id=\"bounding-the-cosine-similarity\">コサイン類似度の境界</h2><p>ベクトル $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$ について、$\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ と $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$ に分解します。ここで $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ および $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$ であり、$m+n=d$ です。</p><p>部分空間 $\\mathbb{R}^m$ でのコサイン類似度は $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$ で与えられ、同様に部分空間 $\\mathbb{R}^n$ での類似度は $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$ となります。</p><p>元の空間 $\\mathbb{R}^d$ でのコサイン類似度は以下のように定義されます：$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>ここで、$s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$ とすると、以下が成り立ちます：$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>証明終了。</p><p>証明の最後のステップでは、コサイン類似度が常に 1 以下であることを利用しています。これが上界を形成します。同様に、\\(\\cos(\\mathbf{A},\\mathbf{B})\\) の下界は以下のように示すことができます：</p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\]、ここで $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$ です。</p><p>下界について、\\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\) と性急に結論付けることはできないことに注意してください。これはコサイン関数の値域が \\([-1, 1]\\) の範囲にあるためです。この値域により、自明な値 -1 よりも厳密な下界を確立することは不可能です。</p><p>結論として、以下の緩い境界が得られます：$$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$$そしてより厳密な境界：\\[\\begin{align*}  \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\]、ここで $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $ です。</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">Johnson-Lindenstrauss の補題との関連</h3><p>JL 補題は、任意の \\(0 &lt; \\epsilon &lt; 1\\) と任意の有限点集合 \\( S \\) in \\( \\mathbb{R}^d \\) に対して、写像 \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\) が存在する（ここで \\( k = O(\\epsilon^{-2} \\log |S|) \\)）ことを主張します。このとき、全ての \\( \\mathbf{u}, \\mathbf{v} \\in S \\) に対して、ユークリッド距離はおおよそ保存されます：<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson–Lindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>$f$ を部分空間選択のように機能させるために、対角行列を射影に使用できます。例えば、\\(5 \\times 3\\) 行列 \\(f\\) を使用する場合（ただしランダムではありません。なお、JL 補題の典型的な定式化では、ガウス分布からのランダム行列を利用する線形変換を含みます）。例えば、5 次元ベクトル空間から第 1、第 3、第 5 次元を保持したい場合、行列 \\(f\\) は以下のように設計できます：\\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>しかし、$f$ を対角行列に指定することで、射影に使用できる関数のクラスが制限されます。JL 補題は、より広いクラスの線形変換の中に適切な $f$ が存在することを保証しますが、$f$ を対角行列に制限すると、JL 補題の境界を適用するための適切な $f$ がこの制限されたクラス内に存在しない可能性があります。</p><h2 id=\"validating-the-bounds\">境界の検証</h2><p>高次元ベクトル空間におけるコサイン類似度の理論的境界を実証的に探るため、モンテカルロ シミュレーションを使用できます。この方法により、多数のランダムなベクトルペアを生成し、元の空間と部分空間の両方で類似度を計算し、理論上の上界と下界が実際にどの程度成り立つかを評価することができます。</p><p>以下の Python コードスニペットはこの概念を実装しています。高次元空間でランダムなベクトルのペアを生成し、そのコサイン類似度を計算します。次に、各ベクトルを 2 つの部分空間に分割し、各部分空間内でのコサイン類似度を計算し、部分空間の類似度に基づいて全次元のコサイン類似度の上界と下界を評価します。</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),\n          np.all(gamma * t &lt;= cos_sim)))\n</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">コサイン類似度の境界を検証するための Monte Carlo バリデーター</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Monte Carlo バリデーターのサンプル出力です。重要な点として、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>lower/upper bound satisfied</span></code><span style=\"white-space: pre-wrap;\">の条件は各ベクトルに対して個別にチェックされます。一方、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>avg. lower/upper bound</span></code><span style=\"white-space: pre-wrap;\">は、これらの境界に関する統計の概要をより直感的に示すものですが、検証プロセスには直接影響しません。</span></p></figcaption></figure><h2 id=\"understanding-the-bounds\">境界の理解</h2><p>要するに、高次元ベクトルを比較する際、全体の類似度は部分空間の最良と最悪の類似度の間に位置し、それらの部分空間が全体の中でどれだけ大きいか、または重要かによって調整されます。これが高次元におけるコサイン類似度の境界が直感的に表現するものです：最も類似した部分と最も類似していない部分のバランスを、それらの相対的なサイズや重要性で重み付けしたものです。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Illustrative comparison of two stylus pen caps and bodies with labeled sections on a black background\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">各ペンには2つの主要な構成要素があります：本体とキャップ。</span></figcaption></figure><p>複数のパーツからなる2つのオブジェクト（例えば、2本の高級ペン）の全体的な類似度を比較しようとしているとイメージしてください。各ペンには2つの主要な構成要素があります：本体とキャップ。ペン全体（本体とキャップの両方）の類似度が、私たちが決定しようとしているものです：</p><h3 id=\"upper-bound-gamma-cdot-s\">上限 ($\\gamma \\cdot s$)</h3><p>$s$ をペンの対応するパーツ間の最良のマッチとして考えてください。キャップは非常に似ているが本体は似ていない場合、$s$ はキャップの類似度となります。</p><p>$\\gamma$ は各パーツのサイズ（または重要性）に基づくスケーリング係数のようなものです。一方のペンが長い本体と短いキャップを持ち、もう一方が短い本体と長いキャップを持つ場合、$\\gamma$ はこれらの比率の違いを考慮して全体の類似度を調整します。</p><p>上限は、一部のパーツがどれだけ似ていても、全体の類似度はこの「最良のパーツの類似度」に比率係数を掛けた値を超えることができないことを示しています。</p><h3 id=\"lower-bound-gamma-cdot-t\">下限 ($\\gamma \\cdot t$)</h3><p>ここで、$t$ は最も一致しないパーツの類似度です。ペンの本体が全く異なるがキャップは似ている場合、$t$ は本体の類似度を反映します。</p><p>同様に、$\\gamma$ は各パーツの比率に基づいてこれをスケーリングします。</p><p>下限は、全体の類似度が各パーツの比率を考慮した後の「最悪のパーツの類似度」よりも悪くなることはできないことを意味します。</p><h2 id=\"implications-of-the-bounds\">境界の意味</h2><p>エンベディング、ベクター検索、検索、またはデータベースを扱うソフトウェアエンジニアにとって、これらの境界を理解することには実践的な意味があります。特に高次元データを扱う際に重要です。ベクター検索では、通常、コサイン類似度を近さの尺度として使用し、与えられたクエリベクトルに最も近い（最も類似した）ベクトルをデータベースから見つけることが多くあります。私たちが議論した境界は、そのようなタスクに部分空間の類似度を使用する際の有効性と限界について洞察を提供できます。</p><h3 id=\"using-subspace-similarity-for-ranking\">ランキングのための部分空間類似度の使用</h3><p><strong>安全性と正確性</strong>：トップkの結果のランキングと検索に部分空間類似度を使用することは効果的ですが、注意が必要です。上限は、全体の類似度が部分空間の最大類似度を超えることができないことを示しています。したがって、ベクトルのペアが特定の部分空間で高い類似度を持つ場合、高次元空間でも類似している可能性が高いことを示します。</p><p><strong>潜在的な落とし穴</strong>：しかし、下限は、1つの部分空間で低い類似度を持つ2つのベクトルが、全体としてはかなり類似している可能性があることを示唆しています。したがって、部分空間の類似度だけに依存すると、関連する結果を見逃す可能性があります。</p><h3 id=\"misconceptions-and-cautions\">誤解と注意点</h3><p><strong>部分空間の重要性の過大評価</strong>：よくある誤解は、特定の部分空間の重要性を過大評価することです。一つの部分空間での高い類似度は良い指標ですが、他の部分空間の影響により、必ずしも全体の高い類似度を保証するものではありません。</p><p><strong>負の類似度の無視</strong>：部分空間でのコサイン類似度が負の場合、その次元での対立的な関係を示します。エンジニアはこれらの負の類似度が全体の類似度にどのように影響するかに注意を払う必要があります。</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "部分空間における高い類似性は、ベクトル間の全体的な類似性の高さを保証するのでしょうか？この記事では、部分空間の類似性に関する理論と実践的な意味について検討します。",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
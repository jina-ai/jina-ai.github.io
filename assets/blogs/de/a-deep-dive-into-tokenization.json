{
  "slug": "a-deep-dive-into-tokenization",
  "id": "65afb3ee8da8040001e17061",
  "uuid": "02d119e4-ed5f-4edf-8b66-65aea1386d96",
  "title": "Ein tiefer Einblick in die Tokenisierung",
  "html": "<p>Es gibt viele H√ºrden beim Verst√§ndnis von AI-Modellen, einige davon ziemlich gro√üe, und sie k√∂nnen die Implementierung von AI-Prozessen erschweren. Aber die erste, auf die viele Menschen sto√üen, ist das Verst√§ndnis dessen, was wir meinen, wenn wir von <strong>Tokens</strong> sprechen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">Free API to tokenize texts, count and get first/last-N tokens.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\"></div></a></figure><p>Einer der wichtigsten praktischen Parameter bei der Auswahl eines AI-Sprachmodells ist die Gr√∂√üe seines Kontextfensters ‚Äî die maximale Eingabetextgr√∂√üe ‚Äî, die in Tokens angegeben wird, nicht in W√∂rtern oder Zeichen oder anderen automatisch erkennbaren Einheiten.</p><p>Dar√ºber hinaus werden Embedding-Services typischerweise ‚Äûpro Token\" berechnet, was bedeutet, dass Tokens wichtig f√ºr das Verst√§ndnis Ihrer Rechnung sind.</p><p>Dies kann sehr verwirrend sein, wenn man nicht genau wei√ü, was ein Token ist.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png\" class=\"kg-image\" alt=\"Jina Embeddings current price sheet (as of February 2024).\" loading=\"lazy\" width=\"2000\" height=\"1036\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Aktuelle Preisliste von Jina Embeddings (Stand: Februar 2024). Beachten Sie, dass die Preise pro ‚Äû1M Tokens\" angegeben sind.</span></figcaption></figure><p>Aber von allen verwirrenden Aspekten der modernen AI sind Tokens wahrscheinlich die am wenigsten komplizierten. Dieser Artikel wird versuchen zu erkl√§ren, was Tokenisierung ist, was sie bewirkt und warum wir sie auf diese Weise durchf√ºhren.</p><h2 id=\"tldr\">tl;dr</h2><p>F√ºr diejenigen, die eine schnelle Antwort suchen, um herauszufinden, wie viele Tokens sie von Jina Embeddings kaufen sollten oder eine Sch√§tzung, wie viele sie voraussichtlich kaufen m√ºssen, sind die folgenden Statistiken das, wonach Sie suchen.</p><h3 id=\"tokens-per-english-word\">Tokens pro englisches Wort</h3><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Ein Aufruf der Jina Embeddings v2 API f√ºr englische Modelle verwendet <b><strong style=\"white-space: pre-wrap;\">ungef√§hr</strong></b> <b><strong style=\"white-space: pre-wrap;\">10% mehr</strong></b> Tokens als die Anzahl der W√∂rter in Ihrem Text, <b><strong style=\"white-space: pre-wrap;\">plus zwei Tokens pro Embedding</strong></b>.</div></div><p>W√§hrend empirischer Tests, die weiter unten in diesem Artikel beschrieben werden, wurden verschiedene englische Texte mit einer Rate von etwa 10% mehr Tokens als W√∂rter in Tokens umgewandelt, wenn die englischsprachigen Modelle von Jina Embeddings verwendet wurden. Dieses Ergebnis war ziemlich robust.</p><p>Jina Embeddings v2 Modelle haben ein Kontextfenster von 8192 Tokens. Das bedeutet, dass wenn Sie einem Jina-Modell einen englischen Text mit mehr als 7.400 W√∂rtern √ºbergeben, dieser mit hoher Wahrscheinlichkeit abgeschnitten wird.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Die maximale Eingabegr√∂√üe f√ºr <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 f√ºr Englisch</strong></b> betr√§gt ungef√§hr <b><strong style=\"white-space: pre-wrap;\">7.400 W√∂rter</strong></b>.</div></div><h3 id=\"tokens-per-chinese-character\">Tokens pro chinesisches Zeichen</h3><p>F√ºr Chinesisch sind die Ergebnisse variabler. Je nach Textart variierten die Verh√§ltnisse von 0,6 bis 0,75 Tokens pro chinesischem Zeichen (Ê±âÂ≠ó). Englische Texte, die an Jina Embeddings v2 f√ºr Chinesisch √ºbergeben werden, erzeugen ungef√§hr die gleiche Anzahl von Tokens wie Jina Embeddings v2 f√ºr Englisch: etwa 10% mehr als die Anzahl der W√∂rter.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Die maximale Eingabegr√∂√üe f√ºr Chinesisch in <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 f√ºr Chinesisch und Englisch</strong></b> betr√§gt ungef√§hr <b><strong style=\"white-space: pre-wrap;\">10.500 Zeichen</strong></b> (<b><strong style=\"white-space: pre-wrap;\">Â≠óÊï∞</strong></b>), oder <b><strong style=\"white-space: pre-wrap;\">0,6 bis 0,75 Tokens pro chinesischem Zeichen, plus zwei pro Embedding.</strong></b></div></div><h3 id=\"tokens-per-german-word\">Tokens pro deutsches Wort</h3><p>Deutsche Wort-zu-Token-Verh√§ltnisse sind variabler als Englisch, aber weniger als Chinesisch. Je nach Textgenre erhielt ich durchschnittlich 20% bis 30% mehr Tokens als W√∂rter. Wenn man englische Texte an Jina Embeddings v2 f√ºr Deutsch und Englisch √ºbergibt, werden etwas mehr Tokens verwendet als bei den reinen Englisch- und Chinesisch/Englisch-Modellen: 12% bis 15% mehr Tokens als W√∂rter.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Jina Embeddings v2 f√ºr Deutsch und Englisch z√§hlt <b><strong style=\"white-space: pre-wrap;\">20% bis 30% mehr Tokens als W√∂rter, plus zwei pro Embedding</strong></b>. Die maximale Gr√∂√üe des Eingabekontexts betr√§gt ungef√§hr <b><strong style=\"white-space: pre-wrap;\">6.300 deutsche W√∂rter</strong></b>.</div></div><h3 id=\"caution\">Vorsicht!</h3><p>Dies sind einfache Berechnungen, aber sie sollten f√ºr die meisten nat√ºrlichsprachlichen Texte und die meisten Benutzer ungef√§hr stimmen. Letztendlich k√∂nnen wir nur versprechen, dass die Anzahl der Tokens immer nicht mehr als die Anzahl der Zeichen in Ihrem Text plus zwei betr√§gt. Praktisch wird es immer viel weniger sein, aber wir k√∂nnen im Voraus keine spezifische Anzahl versprechen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Ihre Ergebnisse k√∂nnen abweichen!</strong></b><br><br>Dies sind Sch√§tzungen basierend auf statistisch naiven Berechnungen. Wir garantieren nicht, wie viele Tokens eine bestimmte Anfrage ben√∂tigen wird.</div></div><p>Wenn Sie nur einen Rat brauchen, wie viele Tokens Sie f√ºr Jina Embeddings kaufen sollten, k√∂nnen Sie hier aufh√∂ren. Andere Embedding-Modelle von anderen Unternehmen als Jina AI haben m√∂glicherweise nicht die gleichen Token-zu-Wort- und Token-zu-Chinesisches-Zeichen-Verh√§ltnisse wie Jina-Modelle, aber sie werden sich im Allgemeinen nicht sehr stark unterscheiden.</p><p>Wenn Sie verstehen m√∂chten, warum das so ist, bietet der Rest dieses Artikels einen tieferen Einblick in die Tokenisierung f√ºr Sprachmodelle.</p><h2 id=\"words-tokens-numbers\">W√∂rter, Tokens, Zahlen</h2><p>Tokenisierung ist schon l√§nger Teil der Verarbeitung nat√ºrlicher Sprache als moderne AI-Modelle existieren.</p><p>Es ist ein bisschen klischeehaft zu sagen, dass alles in einem Computer nur eine Zahl ist, aber es stimmt gr√∂√ütenteils. Sprache ist jedoch von Natur aus nicht nur ein Haufen Zahlen. Sie kann Sprache sein, die aus Schallwellen besteht, oder Schrift, die aus Zeichen auf Papier besteht, oder sogar ein Bild eines gedruckten Textes oder ein Video von jemandem, der Geb√§rdensprache verwendet. Aber meistens, wenn wir √ºber die Verarbeitung nat√ºrlicher Sprache mit Computern sprechen, meinen wir Texte, die aus Sequenzen von Zeichen bestehen: Buchstaben (a, b, c usw.), Ziffern (0, 1, 2‚Ä¶), Interpunktion und Leerzeichen in verschiedenen Sprachen und Textkodierungen.</p><p>Computeringenieure nennen diese ‚ÄûStrings\".</p><p>AI-Sprachmodelle nehmen Zahlensequenzen als Eingabe. Wenn Sie also den Satz schreiben:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Aber nach der Tokenisierung erh√§lt das AI-Modell als Eingabe:</p><pre><code class=\"language-python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>Tokenisierung ist der Prozess der Umwandlung einer Eingabezeichenkette in eine spezifische Zahlensequenz, die Ihr AI-Modell verstehen kann.</p><p>Wenn Sie ein AI-Modell √ºber eine Web-API verwenden, die Benutzern pro Token berechnet wird, wird jede Anfrage in eine Zahlensequenz wie die obige umgewandelt. Die Anzahl der Tokens in der Anfrage ist die L√§nge dieser Zahlensequenz. Wenn Sie also Jina Embeddings v2 f√ºr Englisch bitten, Ihnen ein Embedding f√ºr ‚Äû<em>What is today's weather in Berlin?</em>\" zu geben, kostet Sie das 11 Tokens, weil es diesen Satz in eine Sequenz von 11 Zahlen umgewandelt hat, bevor es ihn an das AI-Modell weitergibt.</p><p>AI-Modelle, die auf der <a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io\">Transformer-Architektur</a> basieren, haben ein Kontextfenster fester Gr√∂√üe, dessen Gr√∂√üe in Tokens gemessen wird. Manchmal wird dies auch ‚ÄûEingabefenster\", ‚ÄûKontextgr√∂√üe\" oder ‚ÄûSequenzl√§nge\" genannt (besonders auf dem <a href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io\">Hugging Face MTEB Leaderboard</a>). Es bedeutet die maximale Textgr√∂√üe, die das Modell auf einmal sehen kann.</p><p>Wenn Sie also ein Embedding-Modell verwenden m√∂chten, ist dies die maximal erlaubte Eingabegr√∂√üe.</p><p>Jina Embeddings v2 Modelle haben alle ein Kontextfenster von 8.192 Tokens. Andere Modelle werden unterschiedliche (typischerweise kleinere) Kontextfenster haben. Das bedeutet, dass egal wie viel Text Sie eingeben, der mit diesem Jina Embeddings-Modell verbundene Tokenizer ihn in nicht mehr als 8.192 Tokens umwandeln muss.</p><h2 id=\"mapping-language-to-numbers\">Abbildung von Sprache auf Zahlen</h2><p>Die einfachste Art, die Logik von Tokens zu erkl√§ren, ist diese:</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Ein Token ist eine Zahl, die f√ºr einen Teil eines Strings steht.</div></div><p>Bei Modellen f√ºr nat√ºrliche Sprache ist der Teil eines Strings, f√ºr den ein Token steht, ein Wort, ein Teil eines Wortes oder ein Interpunktionszeichen. Leerzeichen erhalten in der Regel keine explizite Darstellung in der Tokenizer-Ausgabe.</p><p>Tokenisierung ist Teil einer Gruppe von Techniken in der Verarbeitung nat√ºrlicher Sprache, die <a href=\"https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io\"><em>Textsegmentierung</em></a> genannt wird, und das Modul, das die Tokenisierung durchf√ºhrt, wird sehr logisch als <strong>Tokenizer</strong> bezeichnet.</p><p>Um zu zeigen, wie Tokenisierung funktioniert, werden wir einige S√§tze mit dem kleinsten Jina Embeddings v2 Modell f√ºr Englisch tokenisieren: <code>jina-embeddings-v2-small-en</code>. Das andere rein englische Modell von Jina Embeddings ‚Äî <code>jina-embeddings-v2-base-en</code> ‚Äî verwendet den gleichen Tokenizer, also macht es keinen Sinn, extra Megabytes an AI-Modell herunterzuladen, die wir in diesem Artikel nicht verwenden werden.</p><p>Installieren Sie zuerst das <code>transformers</code> Modul in Ihrer Python-Umgebung oder Notebook. Verwenden Sie die<code>-U</code> Flag, um sicherzustellen, dass Sie auf die neueste Version aktualisieren, da dieses Modell mit einigen √§lteren Versionen nicht funktioniert:</p><pre><code class=\"language-bash\">pip install -U transformers\n</code></pre><p>Laden Sie dann <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>jina-embeddings-v2-small-en</code></a> mit <code>AutoModel.from_pretrained</code> herunter:</p><pre><code class=\"language-Python\">from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n</code></pre><p>Um einen String zu tokenisieren, verwenden Sie die <code>encode</code> Methode des <code>tokenizer</code> Member-Objekts des Modells:</p><pre><code class=\"language-Python\">model.tokenizer.encode(\"What is today's weather in Berlin?\")\n</code></pre><p>Das Ergebnis ist eine Liste von Zahlen:</p><pre><code class=\"language-Python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>Um diese Zahlen wieder in String-Form umzuwandeln, verwenden Sie die <code>convert_ids_to_tokens</code> Methode des <code>tokenizer</code> Objekts:</p><pre><code class=\"language-Python\">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])\n</code></pre><p>Das Ergebnis ist eine Liste von Strings:</p><pre><code class=\"language-Python\">['[CLS]', 'what', 'is', 'today', \"'\", 's', 'weather', 'in',\n 'berlin', '?', '[SEP]']\n</code></pre><p>Beachten Sie, dass der Tokenizer des Modells:</p><ol><li><code>[CLS]</code> am Anfang und <code>[SEP]</code> am Ende hinzugef√ºgt hat. Dies ist aus technischen Gr√ºnden notwendig und bedeutet, dass <strong>jede Anfrage f√ºr ein Embedding zwei zus√§tzliche Token kostet</strong>, zus√§tzlich zu den Token, die der Text ben√∂tigt.</li><li>Interpunktion von W√∂rtern getrennt hat, wodurch aus ‚Äû<em>Berlin?</em>\": <code>berlin</code> und <code>?</code> wird, und aus ‚Äû<em>today's</em>\": <code>today</code>, <code>'</code> und <code>s</code>.</li><li>Alles in Kleinbuchstaben umgewandelt hat. Nicht alle Modelle machen das, aber es kann beim Training mit Englisch hilfreich sein. Bei Sprachen, in denen Gro√üschreibung eine andere Bedeutung hat, ist es m√∂glicherweise weniger hilfreich.</li></ol><p>Verschiedene W√∂rter-Z√§hl-Algorithmen in verschiedenen Programmen k√∂nnten die W√∂rter in diesem Satz unterschiedlich z√§hlen. OpenOffice z√§hlt dies als sechs W√∂rter. Der Unicode-Textsegmentierungsalgorithmus (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) z√§hlt sieben W√∂rter. Andere Software kann zu anderen Zahlen kommen, je nachdem, wie sie mit Interpunktion und Klitika wie \"'s\" umgeht.</p><p>Der Tokenizer f√ºr dieses Modell produziert neun Token f√ºr diese sechs oder sieben W√∂rter, plus die zwei zus√§tzlichen Token, die bei jeder Anfrage ben√∂tigt werden.</p><p>Versuchen wir es jetzt mit einem weniger gebr√§uchlichen Ortsnamen als Berlin:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"I live in Kinshasa.\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Das Ergebnis:</p><pre><code class=\"language-Python\">['[CLS]', 'i', 'live', 'in', 'kin', '##sha', '##sa', '.', '[SEP]']\n</code></pre><p>Der Name \"Kinshasa\" wird in drei Token aufgeteilt: <code>kin</code>, <code>##sha</code> und <code>##sa</code>. Das <code>##</code> zeigt an, dass dieser Token nicht der Beginn eines Wortes ist.</p><p>Wenn wir dem Tokenizer etwas v√∂llig Fremdes geben, erh√∂ht sich die Anzahl der Token im Verh√§ltnis zur Anzahl der W√∂rter noch mehr:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"Klaatu barada nikto\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n\n['[CLS]', 'k', '##la', '##at', '##u', 'bar', '##ada', 'nik', '##to', '[SEP]']\n</code></pre><p>Drei W√∂rter werden zu acht Token, plus die <code>[CLS]</code> und <code>[SEP]</code> Token.</p><p>Die Tokenisierung im Deutschen ist √§hnlich. Mit dem <a href=\"https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina Embeddings v2 f√ºr Deutsch</a> Modell k√∂nnen wir eine √úbersetzung von \"What is today's weather in Berlin?\" auf die gleiche Weise tokenisieren wie beim englischen Modell.</p><pre><code class=\"language-Python\">german_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-de', trust_remote_code=True)\ntoken_ids = german_model.tokenizer.encode(\"Wie wird das Wetter heute in Berlin?\")\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Das Ergebnis:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Wie', 'wird', 'das', 'Wetter', 'heute', 'in', 'Berlin', '?', '&lt;/s&gt;']\n</code></pre><p>Dieser Tokenizer unterscheidet sich ein wenig vom englischen darin, dass <code>&lt;s&gt;</code> und <code>&lt;/s&gt;</code> <code>[CLS]</code> und <code>[SEP]</code> ersetzen, aber die gleiche Funktion erf√ºllen. Au√üerdem wird der Text nicht in Kleinbuchstaben normalisiert ‚Äì Gro√ü- und Kleinschreibung bleiben wie geschrieben ‚Äì da Gro√üschreibung im Deutschen eine andere Bedeutung hat als im Englischen.</p><p>(Um diese Darstellung zu vereinfachen, habe ich ein spezielles Zeichen entfernt, das den Wortanfang anzeigt.)</p><p>Versuchen wir es jetzt mit einem komplexeren Satz <a href=\"https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io\">aus einem Zeitungstext</a>:</p><blockquote>Ein Gro√üteil der milliardenschweren Bauern-Subventionen bleibt liegen ‚Äì zu genervt sind die Landwirte von b√ºrokratischen G√§ngelungen und Regelwahn.</blockquote><pre><code>sentence = \"\"\"\nEin Gro√üteil der milliardenschweren Bauern-Subventionen\nbleibt liegen ‚Äì zu genervt sind die Landwirte von \nb√ºrokratischen G√§ngelungen und Regelwahn.\n\"\"\"\ntoken_ids = german_model.tokenizer.encode(sentence)\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)</code></pre><p>Das tokenisierte Ergebnis:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Ein', 'Gro√üteil', 'der', 'mill', 'iarden', 'schwer', \n 'en', 'Bauern', '-', 'Sub', 'ventionen', 'bleibt', 'liegen', \n '‚Äì', 'zu', 'gen', 'ervt', 'sind', 'die', 'Landwirte', 'von', \n 'b√ºro', 'krat', 'ischen', 'G√§n', 'gel', 'ungen', 'und', 'Regel', \n 'wahn', '.', '&lt;/s&gt;']\n</code></pre><p>Hier sehen Sie, dass viele deutsche W√∂rter in kleinere Teile zerlegt wurden, und zwar nicht unbedingt entlang der von der deutschen Grammatik erlaubten Linien. Das Ergebnis ist, dass ein langes deutsches Wort, das f√ºr einen Wortz√§hler als nur ein Wort z√§hlen w√ºrde, f√ºr Jinas KI-Modell eine beliebige Anzahl von Token sein kann.</p><p>Machen wir dasselbe auf Chinesisch und √ºbersetzen \"What is today's weather in Berlin?\" als:</p><blockquote>ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü</blockquote><pre><code>chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)\ntoken_ids = chinese_model.tokenizer.encode(\"ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü\")\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Das tokenisierte Ergebnis:</p><pre><code class=\"language-Python\">['&lt;s&gt;', 'ÊüèÊûó', '‰ªäÂ§©ÁöÑ', 'Â§©Ê∞î', 'ÊÄé‰πàÊ†∑', 'Ôºü', '&lt;/s&gt;']\n</code></pre><p>Im Chinesischen gibt es normalerweise keine Worttrennungen im geschriebenen Text, aber der Jina Embeddings Tokenizer f√ºgt h√§ufig mehrere chinesische Zeichen zusammen:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Token string</th>\n<th>Pinyin</th>\n<th>Bedeutung</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ÊüèÊûó</td>\n<td>B√≥l√≠n</td>\n<td>Berlin</td>\n</tr>\n<tr>\n<td>‰ªäÂ§©ÁöÑ</td>\n<td>jƒ´ntiƒÅn de</td>\n<td>heute</td>\n</tr>\n<tr>\n<td>Â§©Ê∞î</td>\n<td>tiƒÅnq√¨</td>\n<td>Wetter</td>\n</tr>\n<tr>\n<td>ÊÄé‰πàÊ†∑</td>\n<td>zƒõnmey√†ng</td>\n<td>wie</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Verwenden wir einen komplexeren Satz <a href=\"https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io\">aus einer Hongkonger Zeitung</a>:</p><pre><code class=\"language-Python\">sentence = \"\"\"\nÊñ∞Ë¶èÂÆöÂü∑Ë°åÈ¶ñÊó•ÔºåË®òËÄÖÂú®‰∏ãÁè≠È´òÂ≥∞ÂâçÁöÑ‰∏ãÂçà5ÊôÇ‰æÜÂà∞Âª£Â∑ûÂú∞Èêµ3ËôüÁ∑öÔºå\nÂæûÁπÅÂøôÁöÑÁè†Ê±üÊñ∞ÂüéÁ´ôÂïüÁ®ãÔºåÂêëÊ©üÂ†¥ÂåóÊñπÂêëÂá∫Áôº„ÄÇ\n\"\"\"\ntoken_ids = chinese_model.tokenizer.encode(sentence)\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>(√úbersetzung: <em>‚ÄûAm ersten Tag, an dem die neuen Bestimmungen in Kraft waren, traf dieser Reporter um 17 Uhr, w√§hrend der Hauptverkehrszeit, an der Guangzhou Metro Linie 3 ein und war von der Station Zhujiang New Town in Richtung Flughafen unterwegs.\"</em>)</p><p>Das Ergebnis:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Êñ∞', 'Ë¶èÂÆö', 'Âü∑Ë°å', 'È¶ñ', 'Êó•', 'Ôºå', 'Ë®òËÄÖ', 'Âú®‰∏ã', 'Áè≠', \n 'È´òÂ≥∞', 'ÂâçÁöÑ', '‰∏ãÂçà', '5', 'ÊôÇ', '‰æÜÂà∞', 'Âª£Â∑û', 'Âú∞', 'Èêµ', '3', \n 'Ëôü', 'Á∑ö', 'Ôºå', 'Âæû', 'ÁπÅÂøô', 'ÁöÑ', 'Áè†Ê±ü', 'Êñ∞Âüé', 'Á´ô', 'Âïü', \n 'Á®ã', 'Ôºå', 'Âêë', 'Ê©üÂ†¥', 'Âåó', 'ÊñπÂêë', 'Âá∫Áôº', '„ÄÇ', '&lt;/s&gt;']\n</code></pre><p>Diese Tokens entsprechen keinem spezifischen chinesischen W√∂rterbuch (ËØçÂÖ∏). Zum Beispiel wird \"ÂïüÁ®ã\" - <em>q«êch√©ng</em> (aufbrechen, sich auf den Weg machen) normalerweise als ein einzelnes Wort kategorisiert, wird hier aber in seine zwei Bestandteile aufgeteilt. √Ñhnlich verh√§lt es sich mit \"Âú®‰∏ãÁè≠\", das √ºblicherweise als zwei W√∂rter erkannt w√ºrde, mit der Trennung zwischen \"Âú®\" - <em>z√†i</em> (in, w√§hrend) und \"‰∏ãÁè≠\" - <em>xi√†bƒÅn</em> (Feierabend, Hauptverkehrszeit), nicht zwischen \"Âú®‰∏ã\" und \"Áè≠\", wie es der Tokenizer hier macht.</p><p>In allen drei Sprachen stehen die Stellen, an denen der Tokenizer den Text aufteilt, in keinem direkten Zusammenhang mit den logischen Stellen, an denen ein menschlicher Leser sie trennen w√ºrde.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Der Tokenizer-Algorithmus verwendet kein konventionelles, sprachbewusstes W√∂rterbuch, daher entspricht sein Verhalten nicht der Art und Weise, wie Menschen W√∂rter z√§hlen.</div></div><p>Dies ist keine spezifische Eigenschaft von Jina Embeddings Modellen. Dieser Ansatz zur Tokenisierung ist in der KI-Modellentwicklung nahezu universell. Auch wenn zwei verschiedene KI-Modelle m√∂glicherweise keine identischen Tokenizer haben, werden sie im aktuellen Entwicklungsstand praktisch alle Tokenizer mit dieser Art von Verhalten verwenden.</p><p>Im n√§chsten Abschnitt wird der spezifische Algorithmus f√ºr die Tokenisierung und die dahinterstehende Logik erl√§utert.</p><h2 id=\"why-do-we-tokenize-and-why-this-way\">Warum tokenisieren wir? Und warum auf diese Weise?</h2><p>KI-Sprachmodelle nehmen als Eingabe Zahlensequenzen, die f√ºr Textsequenzen stehen, aber bevor das zugrunde liegende neuronale Netzwerk ausgef√ºhrt und ein Embedding erstellt wird, geschieht noch etwas mehr. Wenn eine Liste von Zahlen pr√§sentiert wird, die kleine Textsequenzen repr√§sentieren, schl√§gt das Modell jede Zahl in einem internen W√∂rterbuch nach, das einen eindeutigen Vektor f√ºr jede Zahl speichert. Diese werden dann kombiniert und bilden die Eingabe f√ºr das neuronale Netzwerk.</p><p>Das bedeutet, dass der Tokenizer <strong>jeden</strong> Eingabetext, den wir ihm geben, in Tokens umwandeln k√∂nnen <strong>muss</strong>, die im Token-Vektor-W√∂rterbuch des Modells vorkommen. Wenn wir unsere Tokens aus einem konventionellen W√∂rterbuch nehmen w√ºrden, w√ºrde das gesamte Modell beim ersten Auftreten eines Rechtschreibfehlers oder eines seltenen Eigennamens oder Fremdworts stoppen. Es k√∂nnte diese Eingabe nicht verarbeiten.</p><p>In der Verarbeitung nat√ºrlicher Sprache wird dies als Out-of-Vocabulary (OOV) Problem bezeichnet, und es tritt in allen Textarten und allen Sprachen auf. Es gibt einige Strategien zur Bew√§ltigung des OOV-Problems:</p><ol><li>Ignorieren. Alles, was nicht im W√∂rterbuch steht, durch ein \"unbekannt\"-Token ersetzen.</li><li>Umgehen. Statt ein W√∂rterbuch zu verwenden, das Textsequenzen auf Vektoren abbildet, eines verwenden, das <em>einzelne Zeichen</em> auf Vektoren abbildet. Englisch verwendet meist nur 26 Buchstaben, daher muss dies kleiner und robuster gegen OOV-Probleme sein als jedes W√∂rterbuch.</li><li>H√§ufige Teilsequenzen im Text finden, diese ins W√∂rterbuch aufnehmen und f√ºr den Rest Zeichen (Ein-Buchstaben-Tokens) verwenden.</li></ol><p>Die erste Strategie bedeutet, dass viele wichtige Informationen verloren gehen. Das Modell kann nicht einmal aus den Daten lernen, die es gesehen hat, wenn sie die Form von etwas annehmen, das nicht im W√∂rterbuch steht. Viele Dinge in gew√∂hnlichem Text sind selbst in den gr√∂√üten W√∂rterb√ºchern einfach nicht vorhanden.</p><p>Die zweite Strategie ist m√∂glich und wurde von Forschern untersucht. Sie bedeutet jedoch, dass das Modell viel mehr Eingaben akzeptieren und viel mehr lernen muss. Dies bedeutet ein viel gr√∂√üeres Modell und viel mehr Trainingsdaten f√ºr ein Ergebnis, das sich nie als besser erwiesen hat als die dritte Strategie.</p><p>KI-Sprachmodelle implementieren praktisch alle in irgendeiner Form die dritte Strategie. Die meisten verwenden eine Variante des <a href=\"https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io\">Wordpiece-Algorithmus</a> <a href=\"https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io\">[Schuster und Nakajima 2012]</a> oder eine √§hnliche Technik namens <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io\">Byte-Pair Encoding</a> (BPE). [<a href=\"https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io\">Gage 1994</a>, <a href=\"https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io\">Senrich et al. 2016</a>] Diese Algorithmen sind <em>sprachagnostisch</em>. Das bedeutet, sie funktionieren f√ºr alle geschriebenen Sprachen gleich, ohne Kenntnisse √ºber eine umfassende Liste m√∂glicher Zeichen hinaus. Sie wurden f√ºr mehrsprachige Modelle wie Googles BERT entwickelt, die einfach beliebige Eingaben aus dem Internet-Scraping verarbeiten ‚Äî hunderte von Sprachen und Texte, die keine menschliche Sprache sind, wie Computerprogramme ‚Äî sodass sie ohne komplizierte Linguistik trainiert werden konnten.</p><p>Einige Forschungen zeigen signifikante Verbesserungen bei der Verwendung von sprachspezifischeren und sprachbewussteren Tokenizern. [<a href=\"https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io\">Rust et al. 2021</a>] Aber die Entwicklung solcher Tokenizer erfordert Zeit, Geld und Expertise. Die Implementierung einer universellen Strategie wie BPE oder Wordpiece ist viel g√ºnstiger und einfacher.</p><p>Als Konsequenz gibt es jedoch keine M√∂glichkeit zu wissen, wie viele Tokens ein bestimmter Text repr√§sentiert, au√üer ihn durch einen Tokenizer laufen zu lassen und dann die Anzahl der Tokens zu z√§hlen, die dabei herauskommen. Da die kleinste m√∂gliche Teilsequenz eines Textes ein Buchstabe ist, kann man sicher sein, dass die Anzahl der Tokens nicht gr√∂√üer sein wird als die Anzahl der Zeichen (minus Leerzeichen) plus zwei.</p><p>Um eine gute Sch√§tzung zu erhalten, m√ºssen wir unserem Tokenizer viel Text zuf√ºhren und empirisch berechnen, wie viele Tokens wir im Durchschnitt erhalten, verglichen mit der Anzahl der W√∂rter oder Zeichen, die wir eingeben. Im n√§chsten Abschnitt werden wir einige nicht sehr systematische empirische Messungen f√ºr alle derzeit verf√ºgbaren Jina Embeddings v2 Modelle durchf√ºhren.</p><h2 id=\"empirical-estimates-of-token-output-sizes\">Empirische Sch√§tzungen der Token-Ausgabegr√∂√üen</h2><p>F√ºr Englisch und Deutsch habe ich den Unicode-Textsegmentierungsalgorithmus (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) verwendet, um Wortz√§hlungen f√ºr Texte zu erhalten. Dieser Algorithmus wird h√§ufig verwendet, um Textausschnitte auszuw√§hlen, wenn Sie doppelklicken. Er ist das N√§chstbeste, was es an einem universellen objektiven Wortz√§hler gibt.</p><p>Ich habe die <a href=\"https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io\">polyglot-Bibliothek</a> in Python installiert, die diesen Textsegmentierer implementiert:</p><pre><code class=\"language-bash\">pip install -U polyglot\n</code></pre><p>Um die Wortzahl eines Textes zu erhalten, k√∂nnen Sie Code wie dieses Snippet verwenden:</p><pre><code class=\"language-python\">from polyglot.text import Text\n\ntxt = \"What is today's weather in Berlin?\"\nprint(len(Text(txt).words))\n</code></pre><p>Das Ergebnis sollte <code>7</code> sein.</p><p>Um eine Tokenzahl zu erhalten, wurden Segmente des Textes an die Tokenizer verschiedener Jina Embeddings Modelle √ºbergeben, wie unten beschrieben, und jedes Mal habe ich zwei von der zur√ºckgegebenen Tokenanzahl abgezogen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\">Die hier aufgef√ºhrten Tokenzahlen <b><strong style=\"white-space: pre-wrap;\">enthalten nicht</strong></b> die zus√§tzlichen zwei Tokens am Anfang und Ende jedes tokenisierten Textes.</div></div><h3 id=\"english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en\">Englisch<br>(<code>jina-embeddings-v2-small-en</code> und <code>jina-embeddings-v2-base-en</code>)</h3><p>Zur Berechnung von Durchschnittswerten habe ich zwei englische Textkorpora von <a href=\"https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Wortschatz Leipzig</a> heruntergeladen, einer Sammlung frei herunterladbarer Korpora in verschiedenen Sprachen und Konfigurationen, die von der Universit√§t Leipzig gehostet wird:</p><ul><li>Ein Ein-Millionen-Satz-Korpus von Nachrichtendaten in Englisch aus dem Jahr 2020 (<code>eng_news_2020_1M</code>)</li><li>Ein Ein-Millionen-Satz-Korpus von <a href=\"https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io\">englischen Wikipedia</a>-Daten aus dem Jahr 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Beide sind auf <a href=\"https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io\">ihrer englischen Download-Seite</a> zu finden.</p><p>F√ºr mehr Vielfalt habe ich auch die <a href=\"https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io\">Hapgood-√úbersetzung von Victor Hugos <em>Les Mis√©rables</em></a> von Project Gutenberg und eine Kopie der King James Version der Bibel, √ºbersetzt ins Englische 1611, heruntergeladen.</p><p>F√ºr alle vier Texte habe ich die W√∂rter mit dem in <code>polyglot</code> implementierten Unicode-Segmentierer gez√§hlt und dann die von <code>jina-embeddings-v2-small-en</code> erzeugten Tokens gez√§hlt, wobei ich f√ºr jede Tokenisierungsanfrage zwei Tokens abgezogen habe. Die Ergebnisse sind wie folgt:</p>\n<!--kg-card-begin: html-->\n<table id=\"6f07d5d4-ca08-466e-92fc-e784a932e4d0\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b\"><th id=\"wvl`\" class=\"simple-table-header-color simple-table-header\">Text</th><th id=\"|<X;\" class=\"simple-table-header-color simple-table-header\">Wortzahl<br>(Unicode Segmentierer)<br></th><th id=\"GHal\" class=\"simple-table-header-color simple-table-header\">Tokenzahl<br>(Jina Embeddings v2 <br>f√ºr Englisch)<br></th><th id=\"h]mu\" class=\"simple-table-header-color simple-table-header\">Verh√§ltnis Token zu W√∂rtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id=\"7e9eda1b-54b6-40f3-be6f-b233f161e2b5\"><td id=\"wvl`\" class=\"\"><code>eng_news_2020_1M</code></td><td id=\"|<X;\" class=\"\">22.825.712</td><td id=\"GHal\" class=\"\">25.270.581</td><td id=\"h]mu\" class=\"\">1,107</td></tr><tr id=\"a81dfe1d-9143-4306-9bf3-4891ca8fb019\"><td id=\"wvl`\" class=\"\"><code>eng_wikipedia_2016_1M</code></td><td id=\"|<X;\" class=\"\">24.243.607</td><td id=\"GHal\" class=\"\">26.813.877</td><td id=\"h]mu\" class=\"\">1,106</td></tr><tr id=\"d2fff413-6e0d-4ab2-9626-4d618d99af91\"><td id=\"wvl`\" class=\"\"><code>les_miserables_en</code></td><td id=\"|<X;\" class=\"\">688.911</td><td id=\"GHal\" class=\"\">764.121</td><td id=\"h]mu\" class=\"\">1,109</td></tr><tr id=\"eb304e43-4fd3-4e02-9993-13fb0307f544\"><td id=\"wvl`\" class=\"\"><code>kjv_bible</code></td><td id=\"|<X;\" class=\"\">1.007.651</td><td id=\"GHal\" class=\"\">1.099.335</td><td id=\"h]mu\" class=\"\">1,091</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Die Verwendung pr√§ziser Zahlen bedeutet nicht, dass dies ein pr√§zises Ergebnis ist. Dass Dokumente so unterschiedlicher Genres alle zwischen 9 % und 11 % mehr Tokens als W√∂rter aufweisen, deutet darauf hin, dass man wahrscheinlich etwa 10 % mehr Tokens als W√∂rter erwarten kann, gemessen am Unicode Segmenter. Textverarbeitungsprogramme z√§hlen oft keine Satzzeichen, w√§hrend der Unicode Segmenter dies tut, sodass die Wortz√§hlungen aus Office-Software nicht unbedingt damit √ºbereinstimmen m√ºssen.</p><h3 id=\"german-jina-embeddings-v2-base-de\">Deutsch<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>F√ºr Deutsch habe ich drei Korpora von der <a href=\"https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io\">Wortschatz Leipzig Deutschen Seite</a> heruntergeladen:</p><ul><li><code>deu_mixed-typical_2011_1M</code> ‚Äî Eine Million S√§tze aus einer ausgewogenen Mischung von Texten verschiedener Genres aus dem Jahr 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> ‚Äî Eine Million S√§tze aus Nachrichtentexten von 2019.</li><li><code>deu_wikipedia_2021_1M</code> ‚Äî Eine Million S√§tze aus der deutschen Wikipedia von 2021.</li></ul><p>Und f√ºr die Vielfalt habe ich auch alle <a href=\"https://deutschestextarchiv.de/search?q=Kapital&in=metadata&ref=jina-ai-gmbh.ghost.io\">drei B√§nde von Karl Marx' <em>Kapital</em></a> vom <a href=\"https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Deutschen Textarchiv</a> heruntergeladen.</p><p>Dann bin ich nach dem gleichen Verfahren wie beim Englischen vorgegangen:</p>\n<!--kg-card-begin: html-->\n<table id=\"ad695a91-f35b-4215-bd4d-5d1415bb9812\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"7786decb-f68d-433d-8f58-3861d0350027\"><th id=\"UGp`\" class=\"simple-table-header-color simple-table-header\" style=\"width:234.2265625px\">Text</th><th id=\"|qln\" class=\"simple-table-header-color simple-table-header\">Wortzahl<br>(Unicode Segmenter)<br></th><th id=\"YXZX\" class=\"simple-table-header-color simple-table-header\">Token-Anzahl<br>(Jina Embeddings v2 <br>f√ºr Deutsch und Englisch)<br></th><th id=\"oEoQ\" class=\"simple-table-header-color simple-table-header\">Verh√§ltnis von Tokens zu W√∂rtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id=\"9cb48640-64db-4783-8bfe-c78412022a21\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_mixed-typical_2011_1M</code></td><td id=\"|qln\" class=\"\">7.924.024</td><td id=\"YXZX\" class=\"\">9.772.652</td><td id=\"oEoQ\" class=\"\">1,234</td></tr><tr id=\"32fee905-17dc-4c2c-a32d-5e6508b033bc\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_newscrawl-public_2019_1M</code></td><td id=\"|qln\" class=\"\">17.949.120</td><td id=\"YXZX\" class=\"\">21.711.555</td><td id=\"oEoQ\" class=\"\">1,210</td></tr><tr id=\"35d0c8c4-7912-4d61-829a-bb39b643aa1c\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_wikipedia_2021_1M</code></td><td id=\"|qln\" class=\"\">17.999.482</td><td id=\"YXZX\" class=\"\">22.654.901</td><td id=\"oEoQ\" class=\"\">1,259</td></tr><tr id=\"19e10367-e070-4dcc-8cbe-cfc75c43e0f9\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>marx_kapital</code></td><td id=\"|qln\" class=\"\">784.336</td><td id=\"YXZX\" class=\"\">1.011.377</td><td id=\"oEoQ\" class=\"\">1,289</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Diese Ergebnisse haben eine gr√∂√üere Streuung als das rein englische Modell, deuten aber darauf hin, dass deutscher Text im Durchschnitt 20 % bis 30 % mehr Tokens als W√∂rter ergibt.</p><p>Englische Texte ergeben mit dem deutsch-englischen Tokenizer mehr Tokens als mit dem rein englischen:</p>\n<!--kg-card-begin: html-->\n<table id=\"c31b2079-e921-4e06-a24b-8ed60ae63d8d\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5\"><th id=\"Nc<l\" class=\"simple-table-header-color simple-table-header\" style=\"width:187.78125px\">Text</th><th id=\"R@A^\" class=\"simple-table-header-color simple-table-header\">Wortzahl<br>(Unicode Segmenter)<br></th><th id=\"UUfl\" class=\"simple-table-header-color simple-table-header\">Token-Anzahl<br>(Jina Embeddings v2 <br>f√ºr Deutsch und Englisch)<br></th><th id=\"iTZS\" class=\"simple-table-header-color simple-table-header\">Verh√§ltnis von Tokens zu W√∂rtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id=\"3461fd8c-ca39-4670-8f0e-e38a4958464a\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_news_2020_1M</code></td><td id=\"R@A^\" class=\"\">24.243.607</td><td id=\"UUfl\" class=\"\">27.758.535</td><td id=\"iTZS\" class=\"\">1,145</td></tr><tr id=\"48770d4d-5855-4f5f-934f-5b2900aa56c3\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"R@A^\" class=\"\">22.825.712</td><td id=\"UUfl\" class=\"\">25.566.921</td><td id=\"iTZS\" class=\"\">1,120</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Sie sollten erwarten, 12 % bis 15 % mehr Tokens als W√∂rter zu ben√∂tigen, um englische Texte mit dem zweisprachigen Deutsch/Englisch-Modell im Vergleich zum rein englischen zu embedden.</p><h3 id=\"chinese-jina-embeddings-v2-base-zh\">Chinesisch<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Chinesisch wird typischerweise ohne Leerzeichen geschrieben und hatte bis zum 20. Jahrhundert keinen traditionellen Begriff von \"W√∂rtern\". Folglich wird die Gr√∂√üe eines chinesischen Textes typischerweise in Zeichen (<strong>Â≠óÊï∞</strong>) gemessen. Anstatt den Unicode Segmenter zu verwenden, habe ich daher die L√§nge chinesischer Texte gemessen, indem ich alle Leerzeichen entfernt und dann einfach die Zeichenl√§nge ermittelt habe.</p><p>Ich habe drei Korpora von der <a href=\"https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io\">chinesischen Korpus-Seite von Wortschatz Leipzig</a> heruntergeladen:</p><ul><li><code>zho_wikipedia_2018_1M</code> ‚Äî Eine Million S√§tze aus der chinesischsprachigen Wikipedia, extrahiert 2018.</li><li><code>zho_news_2007-2009_1M</code> ‚Äî Eine Million S√§tze aus chinesischen Nachrichtenquellen, gesammelt von 2007 bis 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> ‚Äî Eine Million S√§tze aus Nachrichtenquellen, die ausschlie√ülich traditionelle chinesische Zeichen (ÁπÅÈ´îÂ≠ó) verwenden.</li></ul><p>Zus√§tzlich habe ich f√ºr mehr Vielfalt auch <em>Die wahre Geschichte des Ah Q</em> (ÈòøQÊ≠£ÂÇ≥) verwendet, eine Novelle von Lu Xun (È≠ØËøÖ) aus den fr√ºhen 1920er Jahren. Ich habe die <a href=\"https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io\">traditionelle Zeichenversion von Project Gutenberg</a> heruntergeladen.</p>\n<!--kg-card-begin: html-->\n<table id=\"dace0ca3-97c0-481e-98e2-d2724b7bbe66\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"adc6e6ff-8afd-4915-8884-0894546a13dc\"><th id=\"bCvb\" class=\"simple-table-header-color simple-table-header\" style=\"width:223.6953125px\">Text</th><th id=\"CaUc\" class=\"simple-table-header-color simple-table-header\">Zeichenzahl<br>(Â≠óÊï∞)<br></th><th id=\"CQ{d\" class=\"simple-table-header-color simple-table-header\">Token-Anzahl<br>(Jina Embeddings v2 <br>f√ºr Chinesisch und Englisch)<br></th><th id=\"_};C\" class=\"simple-table-header-color simple-table-header\">Verh√§ltnis von Tokens zu Zeichen<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id=\"e75154ce-a33e-4af1-a983-4c4213f93c0e\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_wikipedia_2018_1M</code></td><td id=\"CaUc\" class=\"\">45.116.182</td><td id=\"CQ{d\" class=\"\">29.193.028</td><td id=\"_};C\" class=\"\">0,647</td></tr><tr id=\"605560a8-5c77-4add-a3e4-4615779b571a\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_news_2007-2009_1M</code></td><td id=\"CaUc\" class=\"\">44.295.314</td><td id=\"CQ{d\" class=\"\">28.108.090</td><td id=\"_};C\" class=\"\">0,635</td></tr><tr id=\"6e23944e-a480-4978-8550-a83404b218c4\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho-trad_newscrawl_2011_1M</code></td><td id=\"CaUc\" class=\"\">54.585.819</td><td id=\"CQ{d\" class=\"\">40.290.982</td><td id=\"_};C\" class=\"\">0,738</td></tr><tr id=\"50abbb96-06f7-4308-9c66-7c18f2a67721\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>Ah_Q</code></td><td id=\"CaUc\" class=\"\">41.268</td><td id=\"CQ{d\" class=\"\">25.346</td><td id=\"_};C\" class=\"\">0,614</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Diese Streuung der Token-zu-Zeichen-Verh√§ltnisse ist unerwartet, und besonders der Ausrei√üer f√ºr das Korpus mit traditionellen Zeichen verdient weitere Untersuchung. Dennoch k√∂nnen wir schlussfolgern, dass man f√ºr Chinesisch <em>weniger</em> Token ben√∂tigt als es Zeichen im Text gibt. Je nach Inhalt kann man mit 25% bis 40% weniger rechnen.</p><p>Englische Texte in Jina Embeddings v2 f√ºr Chinesisch und Englisch ergaben ungef√§hr die gleiche Anzahl an Token wie im rein englischen Modell:</p>\n<!--kg-card-begin: html-->\n<table id=\"061e7c3f-d109-476d-85fb-db3b369e4f35\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1200d074-3353-4815-ab66-a90e93ec349d\"><th id=\"v\\xv\" class=\"simple-table-header-color simple-table-header\" style=\"width:184.53125px\">Text</th><th id=\"qlUV\" class=\"simple-table-header-color simple-table-header\" style=\"width:165.3125px\">Wortanzahl<br>(Unicode Segmenter)<br></th><th id=\"=]?F\" class=\"simple-table-header-color simple-table-header\">Token-Anzahl<br>(Jina Embeddings v2 f√ºr Chinesisch und Englisch)<br></th><th id=\"<rlw\" class=\"simple-table-header-color simple-table-header\">Verh√§ltnis Token zu W√∂rtern<br>(auf 3 Dezimalstellen)<br></th></tr></thead><tbody><tr id=\"2fe4e02d-94fd-4513-bfcb-7f85d66b6883\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_news_2020_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">24.243.607</td><td id=\"=]?F\" class=\"\">26.890.176</td><td id=\"<rlw\" class=\"\">1,109</td></tr><tr id=\"e7f937f4-b156-4f5d-9e0b-3041d07b1b20\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">22.825.712</td><td id=\"=]?F\" class=\"\">25.060.352</td><td id=\"<rlw\" class=\"\">1,097</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h2 id=\"taking-tokens-seriously\">Token ernst nehmen</h2><p>Token sind ein wichtiges Ger√ºst f√ºr KI-Sprachmodelle, und die Forschung auf diesem Gebiet ist noch im Gange.</p><p>Einer der Bereiche, in denen sich KI-Modelle als revolution√§r erwiesen haben, ist die Entdeckung, dass sie sehr robust gegen√ºber verrauschten Daten sind. Selbst wenn ein bestimmtes Modell keine optimale Tokenisierungsstrategie verwendet, kann es bei ausreichender Netzwerkgr√∂√üe, gen√ºgend Daten und angemessenem Training lernen, das Richtige aus unvollkommenen Eingaben zu machen.</p><p>Folglich wird weniger Aufwand in die Verbesserung der Tokenisierung gesteckt als in andere Bereiche, aber das k√∂nnte sich √§ndern.</p><p>Als Nutzer von Embeddings, die man √ºber eine <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">API wie Jina Embeddings</a> bezieht, kann man nicht genau wissen, wie viele Token man f√ºr eine bestimmte Aufgabe ben√∂tigt und muss m√∂glicherweise eigene Tests durchf√ºhren, um verl√§ssliche Zahlen zu erhalten. Aber die hier angegebenen Sch√§tzungen ‚Äì etwa 110% der Wortanzahl f√ºr Englisch, etwa 125% der Wortanzahl f√ºr Deutsch und etwa 70% der Zeichenanzahl f√ºr Chinesisch ‚Äì sollten f√ºr eine grundlegende Budgetierung ausreichen.</p>",
  "comment_id": "65afb3ee8da8040001e17061",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-01-23T13:41:18.000+01:00",
  "updated_at": "2024-08-14T11:38:01.000+02:00",
  "published_at": "2024-01-31T16:10:14.000+01:00",
  "custom_excerpt": "Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-deep-dive-into-tokenization/",
  "excerpt": "Tokenisierung bedeutet bei LLMs, dass Eingabetexte in kleinere Teile f√ºr die Verarbeitung zerlegt werden. Warum werden also Embeddings nach Token abgerechnet?",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect.",
  "feature_image_caption": null
}
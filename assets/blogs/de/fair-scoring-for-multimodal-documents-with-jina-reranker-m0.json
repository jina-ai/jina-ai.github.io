{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "Faires Scoring für Multimodale Dokumente mit jina-reranker-m0",
  "html": "<p>Stellen Sie sich vor, Sie entwickeln ein Suchsystem für Sportnachrichten. Ein Benutzer sucht nach \"Tennispieler feiern Meisterschaftssieg\", und Sie müssen die relevantesten Artikel aus Ihrer Datenbank finden. Jeder Artikel enthält sowohl eine Textunterschrift als auch ein Bild - typisch für moderne Sportberichterstattung.</p><p>Ihr System muss eine <strong>Textabfrage</strong> entgegennehmen und eine <strong>Rangliste der relevantesten multimodalen Dokumente</strong> aus Ihrem Korpus zurückgeben. Klingt unkompliziert, aber es gibt ein grundlegendes Problem, das alle offensichtlichen Ansätze zunichte macht.</p><p>Folgendes passiert, wenn Sie versuchen, diese Dokumente zu bewerten. Ihr 向量模型 (Embedding)-Modell, sagen wir <code>jina-clip-v2</code>, erzeugt Ähnlichkeitswerte wie diese:</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>Artikel</th>\n            <th>Inhaltstyp</th>\n            <th>Beschreibung</th>\n            <th>Ähnlichkeitswert</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>Text</td>\n            <td>Novak Djokovic gewinnt Australian Open Finale in drei Sätzen</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>Bild</td>\n            <td>[Foto von Spieler, der Trophäe hält und lächelt]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Text</td>\n            <td>Wetterbedingte Verzögerungen beeinträchtigen den Zeitplan für Freiluftturniere</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Bild</td>\n            <td>[Foto von Tennisspielern, die springen und feiern]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Welcher Artikel ist relevanter? Artikel A hat einen hohen Textwert, aber einen niedrigen Bildwert. Artikel B hat einen niedrigen Textwert, aber einen höheren Bildwert. Die grundlegende Herausforderung besteht darin, dass <strong>man 0.72 (Text) nicht mit 0.54 (Bild) vergleichen kann</strong>, da diese Ähnlichkeitswerte auf völlig unterschiedlichen Skalen liegen.</p><h2 id=\"when-trivial-solutions-fail\">Wenn triviale Lösungen scheitern</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>Aufgrund der Modalitätslücke</strong> in <code>jina-clip-v2</code> oder in fast allen anderen CLIP-ähnlichen Modellen funktioniert kein offensichtlicher Ansatz, den Sie möglicherweise versuchen. Wenn Sie nur den höheren Wert verwenden, stoßen Sie auf die Tatsache, dass Textwerte sich um 0.2-0.8 gruppieren, während sich Bildwerte um 0.4-0.6 gruppieren. Das bedeutet, dass eine mittelmäßige Textübereinstimmung (0.6) immer eine ausgezeichnete Bildübereinstimmung (0.5) schlägt.</p><p>Das Mitteln der Werte hilft auch nicht. Das Berechnen von (0.7 + 0.3)/2 = 0.5 ergibt eine Zahl, aber was bedeutet sie eigentlich? Sie mitteln grundsätzlich bedeutungslose Größen. Ebenso ist jedes feste Gewichtungsschema willkürlich - manchmal ist Text wichtiger, manchmal Bilder, und dies hängt vollständig von der spezifischen Abfrage und dem Dokument ab.</p><p>Selbst das vorherige Normalisieren der Werte löst das Kernproblem nicht. Sie versuchen immer noch, grundsätzlich unterschiedliche Ähnlichkeitsmaße zu kombinieren, die unterschiedliche Aspekte der Relevanz erfassen.</p><h2 id=\"what-actually-happens\">Was tatsächlich passiert</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Um eine bessere Vorstellung davon zu bekommen, womit wir es zu tun haben, hier ein Beispieldokument aus dem <a href=\"https://arxiv.org/abs/2305.13631\">EDIS-Datensatz</a>, das das Bild (ein deutsches Fußballspiel) und die Bildunterschrift zeigt (<code>One More Field Where the Content Trails Germany</code>).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Beispiel für ein multimodales Dokument, das sowohl Bild- als auch Textinhalte enthält. Da wir zwei Modalitäten haben, gibt es für jede gegebene Abfrage nun </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">zwei</em></i><span style=\"white-space: pre-wrap;\"> semantische Lücken (zwischen der Abfrage und dem Text sowie zwischen der Abfrage und dem Bild). Um die besten Ergebnisse zu erzielen, sollten wir den Textinhalt der Dokumente oder den Bildinhalt durchsuchen?</span></figcaption></figure><p>Insgesamt zeigt <code>jina-clip-v2</code> viel höhere Ähnlichkeiten beim Vergleich von Abfrage zu Text als von Abfrage zu Bild im EDIS-Datensatz, zum Teil aufgrund der Art und Weise, wie das Modell trainiert wurde, und zum Teil aufgrund des Datensatzes selbst:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 2: Ähnlichkeitswerte zwischen Abfrage zu Bild (rot) und Abfrage zu Text (blau) bei Verwendung von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Daher erscheint es logisch, ein Dokument anhand seines Textes und nicht anhand seines Bildes abzurufen. Und wie wir in der folgenden Grafik sehen können, erhalten wir viel bessere Ergebnisse, wenn wir die Textabfrage <code>... for undocumented immigrants helping to establish legal status in the United States</code> mit den Textinhalten des Korpus vergleichen. Tatsächlich schlägt die Suche nach Bildern fehl, um das Ground-Truth-Dokument (gelb hervorgehoben) überhaupt abzurufen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 3: Beispiel, bei dem das Ground-Truth-Dokument (mit gelbem Rand hervorgehoben) nur über </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">'s Abfrage-zu-Text-Abruf abgerufen werden kann, wenn </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> von 3 verwendet wird.</span></figcaption></figure><p>Aber lassen Sie sich nicht täuschen. Obwohl Abfrage zu Text höhere Ähnlichkeitswerte aufweist, sind Abfrage zu Text- und Abfrage zu Bild-Ähnlichkeitswerte <em>nicht</em> vergleichbar. Dies können wir sehen, wenn wir Recall@10 betrachten, wenn wir <code>jina-clip-v2</code> verwenden, um 32 Dokumente aus dem EDIS-Datensatz abzurufen. Offensichtlich ist der Recall bei Abfrage zu <em>Bild</em> höher:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Abfrage zu Text</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Abfrage zu Bild</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Wir können dies unten sehen: Wenn wir eine Abfrage aus dem Datensatz verwenden, <code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>, können wir das Ground-Truth-Dokument nur durch seinen Bildinhalt abrufen. Die Suche nach seinem Textinhalt liefert keine Übereinstimmungen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 4: Beispiel, bei dem das Ground-Truth-Dokument (mit gelbem Rand hervorgehoben) nur über </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">'s Abfrage-zu-Bild-Abruf abgerufen werden kann, wenn </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> von 3 verwendet wird.</span></figcaption></figure><p>Wenn also Ähnlichkeitswerte implizieren, dass wir Dokumente aus ihrem Text abrufen sollten, und Recall impliziert, dass wir sie aus ihren Bildern abrufen sollten, was sollten wir wählen? Die Abbildungen 3 und 4 deuten sicherlich nicht auf einen klaren Gewinner hin. Welche Modalität stellt <em>wirklich</em> die engste Übereinstimmung zwischen unserer Abfrage und dem Dokument dar, das wir suchen? Und wenn wir Kandidaten sowohl aus der Query-to-Text- als auch aus der Query-to-Image-Abfrage zusammenführen wollen, wie können wir dann sinnvoll die Top-Übereinstimmungen auswählen, wenn wir nicht einmal Werte vergleichen können? Es ist klar, dass die bloße Verwendung von <code>jina-clip-v2</code> nicht ausreicht. Wir müssen ein weiteres Modell in den Mix werfen.</p><h2 id=\"a-simple-two-stage-pipeline\">Eine einfache zweistufige Pipeline</h2><p>Im April 2025 haben wir <code>jina-reranker-m0</code> veröffentlicht, einen mehrsprachigen multimodalen 重排器 (Reranker) zum Abrufen visueller Dokumente. Wir können die geringere Modalitätslücke unten sehen, wobei <code>jina-reranker-m0</code> vergleichbare Query-to-Text- und Query-to-Image-Ähnlichkeitswerte aufweist, im Gegensatz zu der viel größeren Lücke, die von <code>jina-clip-v2</code> gezeigt wird:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 6: Im Vergleich zu </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> zeigt </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> einen viel geringeren Unterschied zwischen Query-to-Image- (rot) und Query-to-Text- (blau) Ähnlichkeitswerten.</span></figcaption></figure><p>Vor diesem Hintergrund können wir <code>jina-reranker-m0</code> für einen zweiten Durchgang in der Abfragekette verwenden, nachdem die ersten Ergebnisse von <code>jina-clip-v2</code> abgerufen wurden:</p><p><strong>Phase 1: Abrufen von Kandidaten aus beiden Modalitäten</strong></p><ul><li>Verwenden Sie <code>jina-clip-v2</code>, um 16 Dokumente per Textsuche + 16 per Bildsuche zu erhalten.</li><li>Akzeptieren Sie, dass wir die Werte noch nicht vergleichen können.</li></ul><p><strong>Phase 2: Einheitliches Reranking</strong></p><ul><li>Führen Sie jedes (Abfrage + vollständiges Dokument)-Paar in <code>jina-reranker-m0</code> ein.</li><li>Der Reranker verarbeitet sowohl Text ALS auch Bild zusammen.</li><li>Ausgabe: Einzelner Relevanzwert auf einer einheitlichen Skala</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 5: Indizieren multimodaler Dokumente und ein zweistufiger multimodaler Abfrageprozess mit </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Wir haben die Experimente aus Tabelle 1 erweitert und verwenden nun <code>jina-clip-v2</code>, um Dokumente aus dem Korpus abzurufen, und dann <code>jina-reranker-m0</code>, um sie neu zu ordnen:</p><ol><li>Rufen Sie 32 Dokumente per Query-to-Text ab und ordnen Sie sie dann basierend auf dem Query-to-Text-Wert neu.</li><li>Rufen Sie 32 Dokumente per Query-to-Image ab und ordnen Sie sie dann basierend auf dem Query-to-Image-Wert neu.</li><li>Rufen Sie 16 Dokumente per Query-to-Text und 16 per Query-to-Image ab. Neusortierung basierend auf dem Query-to-Text- oder Query-to-Image-Wert, abhängig von der Abfragemodalität.</li><li>Rufen Sie 16 Dokumente per Query-to-Text und 16 per Query-to-Image ab. Neusortierung basierend auf dem durchschnittlichen Query-to-Text- und Query-to-Image-Wert jedes Dokuments, was einen Endwert von (Query-to-Text + Query-to-Image)/2 ergibt.</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Beachten Sie, dass wir die Zero-Shot-Leistung auf EDIS messen. Wir haben weder <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> noch <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-reranker-m0</code> mit dem Datensatz feinabgestimmt.</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Beschreibung</th>\n      <th>Recall@10 - mit jina-clip-v2</th>\n      <th>Recall@10 - mit jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 Dokumente: Query-to-Text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 Dokumente: Query-to-Image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 Dokumente: Query-to-Text<br>16 Dokumente: Query-to-Image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 Dokumente: Query-to-Text<br>16 Dokumente: Query-to-Image<br>Kombinierte durchschnittliche Reranker-Werte</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Die Experimente 1, 3 und 4 zeigen alle das gleiche Ergebnis für Recall@10 mit <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code>, da die Query-to-Text-Werte höher sind als die Query-to-Image-Werte. Daher werden die Top-Ten-Ergebnisse von den per Text abgerufenen Dokumenten dominiert.</div></div><p>Wie wir sehen können, steigt der Recall durch die Durchführung eines zweiten Durchgangs mit <code>jina-reranker-m0</code> durchweg, unabhängig von der Modalität. <strong>Die größte Steigerung sehen wir jedoch, wenn wir sowohl Text- als auch Bildinhalte aus den abgerufenen Dokumenten kombinieren</strong> und einen Recall@10 von 36,24 erreichen. Ein visuelles Beispiel zeigt, dass <code>jina-reranker-m0</code> das Ground-Truth-Dokument konsistent als erstes einstuft, unabhängig davon, ob Text- oder Bildinhalte gesucht werden:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 7: Beispielabfragen (links) und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> von 1 Ergebnis für jede Reranking-Methodik (vier Spalten rechts), die zeigt, dass die Kombination von Bild- und Textähnlichkeitswerten das Ground-Truth-Dokument konsistent als erstes einstuft.</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Während die Abbildungen 3 und 4 einen <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> von 3 für die verschiedenen Abfragemethoden zeigen, zeigt Abbildung 7 aus Platzgründen nur einen <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> von 1 für jede Abfrage.</div></div><h2 id=\"conclusions\">Schlussfolgerungen</h2><p>Dieser einfache zweistufige Ansatz führt zu einer Verbesserung des Recall um 62 %, da das System endlich das nutzt, was Menschen auf natürliche Weise tun: sowohl das, was wir lesen, als auch das, was wir sehen, berücksichtigen, um die Relevanz zu bestimmen. Die Lektion geht über die Suche hinaus: Bei multimodalen KI-Systemen werden Single-Pass-Ansätze, die Modalitäten getrennt behandeln, immer auf diese Inkompatibilitätsmauer stoßen. Zweistufige Architekturen, die breit gefächert abrufen und dann intelligent einstufen, werden immer wichtiger. Testen Sie <code>jina-reranker-m0</code> über unsere API oder auf AWS, GCP und Azure.</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "Textähnlichkeit: 0.7. Bildähnlichkeit: 0.5. Welches Dokument ist relevanter? Sie können es buchstäblich nicht sagen – und das ist das Kernproblem, das die multimodale Suche behindert. Wir lösen es mit Unified Reranking (einheitlicher Reranker).",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "build-a-rag-system-with-jina-embeddings-and-qdrant",
  "id": "65ddadd039a5b70001810ea3",
  "uuid": "e3419a3e-4a7c-43c3-ac69-6f65f00a7ad8",
  "title": "Ein RAG-System mit Jina Embeddings und Qdrant aufbauen",
  "html": "<p>Retrieval-augmented Generation ist eine schnell wachsende Gruppe von Techniken, die <a href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io\"><u>ursprünglich als Erweiterung für Large Language Models</u></a> (LLMs) konzipiert wurden, aber zunehmend als alternatives Paradigma gesehen werden, das die Rolle von KI-Modellen völlig neu denkt.</p><p>Wir zeigen Ihnen, wie Sie eine grundlegende RAG-Engine mit dem <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex Framework</a>, einem Open-Source <a href=\"https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io\">Mistral LLM</a>, <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2</a> und der <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\">KI-bereiten Vektordatenbank von Qdrant</a> erstellen können.</p><h2 id=\"rag-in-context\"><strong>RAG im Kontext</strong></h2><p>In einem RAG-System ist die Rolle des KI-Modells zwar weiterhin zentral, aber deutlich eingeschränkter. Anstatt Prompts aus seinem internen Wissen zu beantworten, das aus seinen Trainingsdaten stammt, werden ihm die relevanten Informationen in seinem Prompt präsentiert. Die Aufgabe des Modells besteht dann darin, Informationen zu verstehen und in eine Ausgabe zu synthetisieren.</p><p>Dies nutzt die Stärken von KI-Modellen und minimiert ihre Schwächen.</p><p>Zum Beispiel könnten wir dem <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\"><u>Mistral-Instruct Model</u></a> folgenden Prompt geben:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: Why is Taylor Swift planning to attend the Superbowl?\nAnswer:\n</code></pre><p>Als ich das tat, antwortete das Modell:</p><pre><code class=\"language-text\">I don't have real-time information. However, as of my last update,&nbsp;\nTaylor Swift is not officially confirmed to attend or perform at&nbsp;\nthe Superbowl. If there are any updates, I recommend checking the&nbsp;\nlatest news sources.</code></pre><p>Large Language Models sind teuer in der Aktualisierung und im Neutraining, sodass ihre internen Informationen nie vollständig aktuell sein können.</p><p>Darüber hinaus sind Large Language Models dafür bekannt zu \"halluzinieren\", d.h. plausible Antworten zu erfinden, die teilweise oder vollständig falsch sein können und überprüft werden müssen.</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: What was Weird Al Yankovic's first Taylor Swift parody?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" a spoof&nbsp;\nof Swift's song \"Blank Space.\" The parody was released in 2014&nbsp;\nas part of Yankovic's album \"Mandatory Fun.\" The song humorously&nbsp;\naddresses the obsession with celebrity gossip and scandals, as&nbsp;\nreported by the tabloid news website TMZ.</code></pre><p>Weird Al Yankovics <em>TMZ</em> wurde 2011 auf dem Album <em>Alpocalypse</em> veröffentlicht und ist eine Parodie von Swifts <a href=\"https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io\"><em><u>You Belong with Me</u></em></a>.</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" title=\"TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)\"></iframe></figure><p>Die Natur der neuronalen Netze macht Halluzinationen schwierig, wenn nicht unmöglich, vollständig zu verhindern und untergräbt stark jeden Plan, KI-Modelle als Informationsquellen zu nutzen.</p><p>In den <a href=\"https://youtu.be/uaQZIK9gvNo?t=1130&ref=jina-ai-gmbh.ghost.io\"><u>Worten von Sam Altman</u></a>, CEO von OpenAI:</p><blockquote class=\"kg-blockquote-alt\">\"Ich denke, die Leute machen einen großen Fehler, wenn sie diese Modelle [LLMs] als Datenbank betrachten. [...] Es wird irgendwie funktionieren. Aber es ist die teuerste, langsamste und ungenaueste Datenbank der Welt.\"</blockquote><p>RAG minimiert die Abhängigkeit vom internen Faktenwissen eines KI-Modells und nutzt seine größten Stärken: Verständnis natürlicher Sprache und flüssige Sprachgenerierung.</p><p>Nehmen wir ein weiteres Beispiel für eine Frage, die Mistral-Instruct nicht beantworten kann:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\n\nQuery: What is the title of Taylor Swift's next album?\nAnswer:\n\nAs of the time I am providing this answer, Taylor Swift has&nbsp;\nnot yet announced the title of her next album. She has released&nbsp;\nseveral re-recorded albums, but the title of her next original&nbsp;\nalbum remains unknown. I recommend checking her official social&nbsp;\nmedia channels or website for the latest updates.</code></pre><p>Ein RAG-System könnte eine Datenbank mit aktuellen Nachrichten haben oder im Internet nach Texten zum Titel von Taylor Swifts nächstem Album suchen. Dann würde es die gefundenen Texte in den Prompt einfügen.</p><p>Nehmen wir an, es identifiziert <a href=\"https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io\"><u>diesen AP-Artikel</u></a> als relevant für die Anfrage und erstellt den folgenden Prompt:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nFebruary 6, 2024\nLOS ANGELES (AP) — Forget her \"Reputation.\" Taylor Swift has a&nbsp;\nwhole new album coming out.\n\nAccepting the Grammy for best pop vocal album on Sunday night,&nbsp;\nTaylor said she'd been keeping a secret for two years.\n\n\"My brand-new album comes out April 19. It's called 'The Tortured&nbsp;\nPoets Department.' I'm going to go and post the cover right now&nbsp;\nbackstage,\" she announced.\n\nAnd so she did.\n\n---------------------\nGiven the context information and not prior knowledge, answer the query.&nbsp;\nPlease be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What is the title of Taylor Swift's next album?\nAnswer:</code></pre><p>Mistral-Instruct antwortete:</p><pre><code class=\"language-text\">The title of Taylor Swift's next album is 'The Tortured Poets Department'.</code></pre><p>Diese Art des Prompting reduziert auch die Wahrscheinlichkeit von Halluzinationen dramatisch. Zum Beispiel:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nAlpocalypse is the thirteenth studio album by American parody&nbsp;\nmusician \"Weird Al\" Yankovic, released on June 21, 2011.\n\nTrack List\n\n1. Perform This Way (Parody of \"Born This Way\" by Lady Gaga)\n2. CNR\n3. TMZ (Parody of \"You Belong with Me\" by Taylor Swift)\n4. Skipper Dan\n5. Polka Face (Polka medley including songs by artists such as&nbsp;\n&nbsp;&nbsp;&nbsp;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)\n6. Craigslist\n7. Party in the CIA (Parody of \"Party in the U.S.A.\" by Miley&nbsp;\n&nbsp;&nbsp;&nbsp;Cyrus)\n8. Ringtone\n9. Another Tattoo (Parody of \"Nothin' on You\" by B.o.B featuring&nbsp;\n&nbsp;&nbsp;&nbsp;Bruno Mars)\n10. If That Isn't Love\n11. Whatever You Like (Parody of \"Whatever You Like\" by T.I.)\n12. Stop Forwarding That Crap to Me\n\n---------------------\nGiven the context information and not prior knowledge, answer the&nbsp;\nquery. Please be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What was Weird Al Yankovic's first Taylor Swift parody, what&nbsp;\nyear was it released, and what song was he parodying?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" and it was&nbsp;\nreleased in 2011. He was parodying \"You Belong with Me\" by Taylor&nbsp;\nSwift.\n</code></pre><h2 id=\"anatomy-of-a-rag-system\"><strong>Anatomie eines RAG-Systems</strong></h2><p>Ein RAG-System hat mindestens zwei Komponenten:</p><ul><li>Ein generatives KI-Modell</li><li>Ein Informationsabrufsystem</li></ul><p>In diesem Artikel verwenden wir <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io\">Mistral-Instruct</a> als generatives KI-Modell und greifen darauf über die <a href=\"https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io\">Hugging Face Inference API</a> zu. Wir werden die <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\">Qdrant Vektordatenbank</a> und <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> gemeinsam für das Informationsabrufsystem verwenden.</p><p>Als Datenquelle verwenden wir den <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io\"><em><u>National Artificial Intelligence Research and Development Strategic Plan 2023 Update</u></em></a>, der im Mai 2023 vom <a href=\"https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io\">National Science and Technology Council</a> veröffentlicht wurde, einem Gremium, das den US-Präsidenten in wissenschaftlichen und technischen Fragen berät. Dieses etwa 25.000 Wörter umfassende Dokument ist ein Politik- und Strategieplanungsdokument für die amerikanische KI-Politik und -Entwicklung.</p><p>Es ist auch ein klassisches \"tl;dr\" Regierungsdokument. Der Großteil ist sehr langweilig, und wenn Sie nicht direkt betroffen sind oder ein enges Interesse an diesem Bereich haben, werden Sie es wahrscheinlich nicht lesen. Aber Sie möchten vielleicht trotzdem wissen, was darin steht.</p><p>Dieser Artikel führt Sie durch die Schritte zur Erstellung eines RAG-Systems, das Fragen zum Text und seinen Inhalten beantworten kann.</p><h2 id=\"install-the-prerequisites\"><strong>Installation der Voraussetzungen</strong></h2><p>Stellen Sie zunächst sicher, dass Sie alle relevanten Python-Bibliotheken installiert haben. Führen Sie in Ihrer Python-Umgebung Folgendes aus:</p><pre><code class=\"language-shell\">pip install llama-index qdrant-client \npip install pdfminer.six llama-index-vector-stores-qdrant\npip install llama-index-llms-openai llama-index-embeddings-jinaai \npip install llama-index-llms-huggingface \"huggingface_hub[inference]\"</code></pre><p>Wir verwenden LlamaIndex, um ein RAG-System um das Mistral-Instruct Large Language Model herum aufzubauen und <a href=\"https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io\"><u>pdfminer.six</u></a> zur Verarbeitung der PDF-Datei in einen Qdrant Vector Store.</p><h2 id=\"connect-to-mistral-instruct-via-the-hugging-face-inference-api\"><strong>Verbindung zu Mistral-Instruct über die Hugging Face Inference API</strong></h2><p>Zuerst richten wir das LLM ein. Sie benötigen Zugang zur Hugging Face Inference API, einschließlich eines Zugriffstokens. Wenn Sie ein Hugging Face-Konto haben, können Sie einen Token von <a href=\"https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io\">Ihrer Kontoeinstellungsseite</a> erhalten.</p><p>Wenn Sie kein Konto haben, <a href=\"https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io\">erstellen Sie zunächst eines</a> und dann einen <a href=\"https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io\">Zugriffstoken</a>.</p><p>Speichern Sie Ihren Token in einer Variable zur späteren Verwendung:</p><pre><code class=\"language-python\">hf_inference_api_key = '&lt;your HuggingFace Inference API token&gt;'</code></pre><p>Als Nächstes erstellen wir eine Prompt-Vorlage:</p><pre><code class=\"language-python\">from llama_index.core import PromptTemplate\n\nqa_prompt_tmpl = (\n&nbsp;&nbsp;&nbsp;&nbsp;\"Context information is below.\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"---------------------\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"{context_str}\\\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"---------------------\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Given the context information and not prior knowledge, \"\n&nbsp;&nbsp;&nbsp;&nbsp;\"answer the query. Please be brief, concise, and complete.\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"If the context information does not contain an answer to the query, \"\n&nbsp;&nbsp;&nbsp;&nbsp;\"respond with \\\"No information\\\".\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Query: {query_str}\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>Diese Vorlage weist das LLM an, nur die im Prompt bereitgestellten Kontextinformationen zu verwenden, um Fragen zu beantworten. Dieser Prompt hat sich bei der Arbeit mit Mistral-Instruct für uns bewährt, aber Sie können mit anderen Formulierungen experimentieren, um zu sehen, was für Ihren Anwendungsfall funktioniert.</p><pre><code class=\"language-python\">from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(\n    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n    token=hf_inference_api_key\n)</code></pre><p>Zum Schluss erstellen und initialisieren wir ein Objekt für das LlamaIndex-Framework, das die Verbindung zu Mistral-Instruct enthält. Der folgende Code stammt aus dem <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\"><u>vorherigen Beitrag über die Erstellung von RAG-Systemen mit LlamaIndex</u></a>.</p><p>Dies speichert die Verbindung zum Mistral LLM in der Variable <code>mistral_llm</code>.</p><h2 id=\"information-retrieval-with-qdrant-and-jina-embeddings\"><strong>Informationsabruf mit Qdrant und Jina Embeddings</strong></h2><p>Um das Abrufsystem einzurichten, benötigen Sie einen Jina Embeddings API-Schlüssel. Sie können einen kostenlosen Schlüssel mit einem vorausbezahlten Budget von 1 Million Tokens auf der <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings Website</a> erhalten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure><p>Speichern Sie Ihren Jina Embeddings-Schlüssel in einer Variable zur späteren Verwendung:</p><pre><code class=\"language-python\">jina_emb_api_key = \"&lt;your Jina Embeddings API key&gt;\"</code></pre><p>Erstellen Sie dann ein Connector-Objekt mit LlamaIndex für den Jina Embeddings-Server und wählen Sie speziell das englische monolinguale Modell:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_emb_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n)</code></pre><h3 id=\"load-text-data\"><strong>Textdaten laden</strong></h3><p>Als Nächstes laden wir das Dokument und teilen es in Absätze auf. Zuerst laden wir die PDF von der White House Website in die Variable <code>pdf_data</code>:</p><pre><code class=\"language-python\">import urllib.request\n\nuri = \"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"\npdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Als Nächstes verarbeiten wir die PDF Seite für Seite, extrahieren den Text und teilen ihn dann in Absätze auf, indem wir an doppelten Zeilenumbrüchen trennen. Diese werden in der Liste <code>text_paras</code> gespeichert:</p><pre><code class=\"language-python\">import regex as re\nfrom io import BytesIO, StringIO\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfparser import PDFParser\n\ntext_paras = []\nparser = PDFParser(BytesIO(pdf_data))\ndoc = PDFDocument(parser)\nrsrcmgr = PDFResourceManager()\nfor page in PDFPage.create_pages(doc):\n&nbsp;&nbsp;&nbsp;&nbsp;output_string = StringIO()\n&nbsp;&nbsp;&nbsp;&nbsp;device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n&nbsp;&nbsp;&nbsp;&nbsp;interpreter = PDFPageInterpreter(rsrcmgr, device)\n&nbsp;&nbsp;&nbsp;&nbsp;interpreter.process_page(page)\n&nbsp;&nbsp;&nbsp;&nbsp;page_text = output_string.getvalue()\n&nbsp;&nbsp;&nbsp;&nbsp;text_paras.extend(re.split(r'\\n\\s*\\n', page_text))\n</code></pre><p>Überprüfen Sie, ob alles geladen wurde:</p><pre><code class=\"language-python\">assert len(text_paras) == 615</code></pre><p>Als Nächstes wandeln wir diese Liste von kurzen Texten in <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io\">LlamaIndex Document-Objekte</a> um:</p><pre><code class=\"language-python\">from llama_index.core.readers import StringIterableReader\n\nrag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>Und Sie können den Text inspizieren:</p><pre><code class=\"language-python\">print(rag_docs[0].text)</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">NATIONAL ARTIFICIAL INTELLIGENCE&nbsp;\nRESEARCH AND DEVELOPMENT&nbsp;\nSTRATEGIC PLAN&nbsp;\n2023 UPDATE</code></pre><h3 id=\"set-up-a-qdrant-vector-database\"><strong>Einrichten einer Qdrant Vector-Datenbank</strong></h3><p>In diesem Artikel verwenden wir die <a href=\"https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io\"><u>Qdrant Vector Search Cloud</u></a> zur Implementierung der RAG-System-Datenbank. Die kostenlose Stufe von Qdrant enthält 1 GB Speicherplatz, was für dieses Tutorial mehr als ausreichend ist.</p><p>Sie müssen zunächst <a href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><u>ein Konto auf der Qdrant Cloud-Website erstellen</u></a>, bevor Sie fortfahren.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Vector Search Database | Qdrant Cloud</div><div class=\"kg-bookmark-description\">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cloud.qdrant.io/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Qdrant Cloud</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cloud.qdrant.io/social.png\" alt=\"\"></div></a></figure><p>Sobald Sie ein Konto haben und eingeloggt sind, müssen Sie einen Cluster erstellen. Folgen Sie den <a href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>\"Quick Start\"-Anweisungen auf der Qdrant Website</u></a>, um einen kostenlosen Cluster einzurichten und eine API sowie den Namen des Qdrant-Hostservers zu erhalten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Quickstart - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Edit on GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png\" alt=\"\"></div></a></figure><p>Speichern Sie den Schlüssel und Hostnamen in Variablen:</p><pre><code class=\"language-python\">qdrant_api_key = \"&lt;your API key&gt;\"\nqdrant_server = \"https://&lt;your server&gt;\"</code></pre><p>Als Nächstes müssen wir die relevanten Komponenten aus den Paketen <code>qdrant_client</code> und <code>llama_index</code> importieren:</p><pre><code class=\"language-python\">import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nclient = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\nvector_store = QdrantVectorStore(client=client, collection_name=\"NTSC\")</code></pre><p>Dies erstellt eine Sammlung namens <code>NTSC</code> in Ihrem kostenlosen Cluster.</p><p>In der <a href=\"https://qdrant.tech/documentation/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>Qdrant-Dokumentation</u></a> finden Sie Informationen zum Aufbau eines lokalen Datenspeichers im Speicher oder auf der Festplatte sowie zum Hosten Ihrer eigenen Qdrant-Server-Instanz in einem Docker-Container über eine Web-API.</p><h3 id=\"complete-the-rag-system\"><strong>Das RAG-System vervollständigen</strong></h3><p>Nun werden wir diese Komponenten mit Boilerplate-Code für LlamaIndex zu einem vollständigen RAG-System zusammenfügen:</p><pre><code class=\"language-python\">from llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core import (\n\t\tVectorStoreIndex,\n\t\tServiceContext,\n\t\tget_response_synthesizer,\n)\n\n# set up the service and storage contexts\nservice_context = ServiceContext.from_defaults(\n&nbsp;&nbsp;&nbsp;&nbsp;llm=mixtral_llm, embed_model=jina_embedding_model\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# create an index\nindex = VectorStoreIndex.from_documents(\n&nbsp;&nbsp;&nbsp;&nbsp;rag_docs, storage_context=storage_context, service_context=service_context\n)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n&nbsp;&nbsp;&nbsp;&nbsp;service_context=service_context,\n&nbsp;&nbsp;&nbsp;&nbsp;text_qa_template=qa_prompt,\n&nbsp;&nbsp;&nbsp;&nbsp;response_mode=\"compact\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever,\n&nbsp;&nbsp;&nbsp;&nbsp;response_synthesizer=response_synthesizer,\n)\n</code></pre><p>Dieses System verwendet Jina Embeddings, um die Absätze aus unserer Textquelle zu indexieren und wird die zwei besten Übereinstimmungen in den Kontext jedes Prompts einfügen. Um beispielsweise auf die drei besten zu ändern, modifizieren Sie einfach die <code>VectorIndexRetriever</code>-Initialisierung oben mit einem anderen <code>similarity_top_k</code>-Parameter:</p><pre><code class=\"language-python\"># configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3,\n)</code></pre><p>Jetzt ist das System einsatzbereit.</p><h3 id=\"query-a-document\"><strong>Ein Dokument abfragen</strong></h3><p>Versuchen wir eine einfache Abfrage:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat is the Biden Administration's policy with regard to AI?&nbsp;\n\"\"\")\nprint(response.response)</code></pre><p>Das Ergebnis (Ihres könnte anders aussehen):</p><pre><code class=\"language-text\">The Biden Administration prioritizes responsible AI systems that&nbsp;\nare ethical, trustworthy, and safe, and serve the public good. \nThe Administration has committed to substantial funding for AI \nR&amp;D, and has established the National AI Initiative Office to \ncoordinate AI activities across the federal government.&nbsp;\n\nThe Administration's AI policy focuses on developing evaluative\ntechniques for AI, workforce development, public-private\npartnerships, and international collaboration in AI research.</code></pre><p>Oder etwas Spezifischeres:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat protections does the AI Bill of Rights propose to offer?\n\"\"\")\n\nprint(response.response)</code></pre><pre><code class=\"language-text\">The AI Bill of Rights proposes five core protections: Safe and&nbsp;\nEffective Systems; Algorithmic Discrimination Protections; Data&nbsp;\nPrivacy; Notice and Explanation; and Human Alternatives,&nbsp;\nConsideration, and Fallback.</code></pre><p>Oder sogar sehr spezifisch:</p><pre><code class=\"language-python\">response = query_engine.query(\"Who is Kei Koizumi?\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">Kei Koizumi is the Principal Deputy Director for Policy at the Office&nbsp;\nof Science and Technology Policy (OSTP).</code></pre><p>Sie können auch fantasievollere Fragen stellen:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat rights will AI's receive under President Biden's proposed&nbsp;\nAI Bill of Rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">The proposed AI Bill of Rights does not grant rights to AI systems, but&nbsp;\nrather outlines principles to ensure that AI technologies are&nbsp;\ndeveloped and used in a manner that respects human rights and values.&nbsp;\nThe principles include safety, transparency, fairness, disclosure,&nbsp;\ndata privacy, non-discrimination, and the ability for individuals to&nbsp;\nopt out of certain AI systems.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhy is President Biden's proposing an AI Bill of Rights?\nDoes AI really need rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">President Biden's proposed AI Bill of Rights aims to ensure that AI&nbsp;\ntechnologies are developed and used ethically, legally, and with&nbsp;\nsocietal considerations in mind. The Bill of Rights is not granting&nbsp;\n\"rights\" to AI, but rather setting guidelines for the responsible&nbsp;\ndevelopment and deployment of AI systems to protect individuals and&nbsp;\ncommunities from potential negative impacts.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nHas Donald Trump weighed in on AI?\nWill he Make Humans Great Again?\n\"\"\")\nprint(response.response)\n</code></pre><pre><code class=\"language-text\">No information. The context information does not mention Donald&nbsp;\nTrump's views on AI.</code></pre><h2 id=\"jina-ai-and-qdrant\"><strong>Jina AI und Qdrant</strong></h2><p>Das 8k-Token-Kontextfenster von Jina Embeddings macht es besonders gut geeignet für Retrieval-Augmented Generation, da es mit viel größeren Textblöcken arbeiten kann. Während andere RAG-Anwendungen oft einzelne Sätze oder Satzpaare speichern müssen, konnten wir in diesem Tutorial ganze Absätze verwenden, ohne uns um deren Größe sorgen zu müssen.</p><p>Dies reduziert die Anzahl der Textelemente, die gespeichert werden müssen, um die gleiche Information zu übermitteln, senkt die Rechenkosten, spart Speicher und macht das Abrufen schneller.</p><p>Qdrants Vektordatenbank ist einfach zu konfigurieren, schnell und kosteneffektiv, und wie Sie gesehen haben, benötigt man nur wenige Zeilen Python-Code, um sie in ein RAG-System zu integrieren.</p><p>Mehr über Qdrants KI-bereite Datenbank und Cloud-Angebote erfahren Sie auf der <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>Website</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qdrant - Vector Database</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Vector Database</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/images/previews/social-preview-P.png\" alt=\"\"></div></a></figure><h2 id=\"get-in-touch\"><strong>Kontaktieren Sie uns</strong></h2><p>Jina AI ist bestrebt, zuverlässige, erschwingliche KI-Technologien für Unternehmen jeder Größe und Art bereitzustellen. Wir würden gerne von Ihren Anwendungsfällen hören und Ihnen helfen, KI in Ihre Geschäftsprozesse zu integrieren. Weitere Informationen über die Angebote von Jina AI und Kontaktmöglichkeiten finden Sie auf der <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>Jina AI Website</u></a> oder treten Sie unserer <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>Community auf Discord</u></a> bei.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Best Embeddings and Perfect Prompts</div><div class=\"kg-bookmark-description\">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-author\">Best Embeddings and Perfect Prompts</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Join the Jina AI Discord Server!</div><div class=\"kg-bookmark-description\">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://discord.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Discord</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512\" alt=\"\"></div></a></figure>",
  "comment_id": "65ddadd039a5b70001810ea3",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-02-27T10:39:28.000+01:00",
  "updated_at": "2024-03-06T15:41:18.000+01:00",
  "published_at": "2024-03-04T16:00:13.000+01:00",
  "custom_excerpt": "Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/build-a-rag-system-with-jina-embeddings-and-qdrant/",
  "excerpt": "Ein RAG-System mit Jina Embeddings v2, Qdrant-Vektordatenbank, LlamaIndex und Mistral LLM erstellen.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Promotional graphic with \"EMBEDDINGS 2\" and Drant logo separated by a red cross, symbolizing a partnership.",
  "feature_image_caption": null
}
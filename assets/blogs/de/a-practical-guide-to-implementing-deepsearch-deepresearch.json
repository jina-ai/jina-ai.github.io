{
  "slug": "a-practical-guide-to-implementing-deepsearch-deepresearch",
  "id": "67bc50b0b1b8af00014db4c9",
  "uuid": "acd44dc0-e356-4ac8-93c7-fa8bbeb33265",
  "title": "Ein praktischer Leitfaden zur Implementierung von DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://search.jina.ai/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI Deep Search</div><div class=\"kg-bookmark-description\">KI-Tiefensuche: Lesen, Verstehen, Suchen bis die beste Antwort gefunden ist.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-30.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Es ist erst Februar, und DeepSearch hat sich bereits als neuer Suchstandard für 2025 etabliert, wobei große Player wie <a href=\"https://blog.google/products/gemini/google-gemini-deep-research/\">Google</a> und <a href=\"https://openai.com/index/introducing-deep-research/\">OpenAI</a> durch ihre DeepResearch-Veröffentlichungen die Führung übernehmen (und ja, <a href=\"https://x.com/hxiao/status/1886250705415229627\">wir haben stolz am selben Tag unser Open-Source <code>node-deepresearch</code> veröffentlicht</a>). <a href=\"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\">Perplexity</a> folgte mit ihrem DeepResearch, und X AI integrierte ihre eigenen DeepSearch-Fähigkeiten in <a href=\"https://x.ai/blog/grok-3\">Grok3</a>, was im Grunde eine weitere DeepResearch-Variante darstellt. Während das Konzept der Tiefensuche nicht revolutionär ist – 2024 wurde es im Wesentlichen als RAG oder Multi-Hop QA bezeichnet – gewann es nach der Veröffentlichung von <a href=\"https://github.com/deepseek-ai/DeepSeek-R1\">Deepseek-r1</a> Ende Januar 2025 erheblich an Dynamik. Letztes Wochenende haben <a href=\"https://www.scmp.com/tech/big-tech/article/3298981/baidu-adopts-deepseek-ai-models-chasing-tencent-race-embrace-hot-start\">Baidu Search und Tencent WeChat Search</a> Deepseek-r1 in ihre Suchmaschinen integriert. KI-Ingenieure haben entdeckt, dass sie durch die Integration von langen Denk- und Argumentationsprozessen in Suchsysteme eine bemerkenswerte Abrufgenauigkeit und -tiefe erreichen können, die über das bisher Mögliche hinausgeht.</p>\n<!--kg-card-begin: html-->\n<table> <thead> <tr> <th>Launch Date</th> <th>Company</th> <th>Product</th> <th>License Type</th> <th>Link</th> </tr> </thead> <tbody> <tr> <td>2025-01-20</td> <td>DeepSeek</td> <td>DeepSeek-r1 release</td> <td>Open source</td> <td><a href=\"https://api-docs.deepseek.com/news/news250120\">DeepSeek-R1</a></td> </tr> <tr> <td>2025-02-02</td> <td>Google</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://blog.google/products/gemini/google-gemini-deep-research/\">Google Gemini 2</a></td> </tr> <tr> <td>2025-02-02</td> <td>OpenAI</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://openai.com/index/introducing-deep-research/\">Introducing Deep Research</a></td> </tr> <tr> <td>2025-02-02</td> <td>Jina AI</td> <td>DeepSearch (<code>node-deepresearch</code>)</td> <td>Open source</td> <td><a href=\"https://github.com/jina-ai/node-deepresearch\">node-deepresearch</a> | <a href=\"https://search.jina.ai\">search.jina.ai</a></td> </tr> <tr> <td>2025-02-04</td> <td>Hugging Face</td> <td>Open Deep Research</td> <td>Open source</td> <td><a href=\"https://huggingface.co/blog/open-deep-research\">Open Deep Research</a></td> </tr> <tr> <td>2025-02-15</td> <td>Perplexity</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\">Introducing Perplexity Deep Research</a></td> </tr> <tr> <td>2025-02-17</td> <td>X AI</td> <td>Grok3 with DeepSearch</td> <td>Proprietary</td> <td><a href=\"https://x.ai/blog/grok-3\">Grok 3 Beta</a></td> </tr> <tr> <td>2025-02-22</td> <td>Baidu Search</td> <td>Integrates DeepSeek-r1</td> <td>Proprietary</td> <td><a href=\"https://chat.baidu.com/search?isShowHello=1&pd=csaitab&setype=csaitab&extParamsJson=%7B%22enter_type%22%3A%22ai_explore_home%22%7D&usedModel=%7B%22modelName%22%3A%22DeepSeek-R1%22%7D\">Baidu Integrates DeepSeek-R1</a></td> </tr> <tr> <td>2025-02-23</td> <td>Tencent Wechat Search</td> <td>Integrates DeepSeek-r1</td> <td>Proprietary</td> <td><a href=\"https://www.reuters.com/technology/artificial-intelligence/tencents-messaging-app-weixin-launches-beta-testing-with-deepseek-2025-02-16/\">Tencent Weixin Integrates DeepSeek</a></td> </tr> </tbody> </table>\n<!--kg-card-end: html-->\n<p>Aber warum geschah dieser Wandel gerade jetzt, wo Deep(Re)Search das ganze Jahr 2024 über relativ unterschätzt blieb? Tatsächlich hatte <a href=\"https://storm-project.stanford.edu/research/storm/\">Stanford NLP Labs das STORM</a>-Projekt für die Generierung langer Berichte mit Web-Grundlage bereits Anfang 2024 veröffentlicht. Liegt es also nur daran, dass \"DeepSearch\" viel cooler klingt als Multi-Hop QA, RAG oder STORM? Seien wir ehrlich - manchmal braucht es nur ein Rebranding, damit die Branche plötzlich das annimmt, was schon die ganze Zeit da war.</p><p>Wir glauben, der eigentliche Wendepunkt kam mit OpenAIs <code>o1-preview</code>-Release im September 2024, der das Konzept des <strong>Test-Time Compute</strong> einführte und allmählich die Perspektiven der Branche veränderte. Test-Time Compute bezieht sich auf die Nutzung von mehr Rechenressourcen während der Inferenz – der Phase, in der ein LLM Outputs generiert – anstatt während des Pre-Trainings oder Post-Trainings. Bekannte Beispiele sind Chain-of-Thought (CoT) Reasoning und <a href=\"https://github.com/simplescaling/s1?tab=readme-ov-file#vllm-with-budget-forcing\"><code>\"Wait\"</code>-Injection</a> (d.h. Budget Forcing), die es Modellen ermöglichen, umfangreichere interne Überlegungen anzustellen, wie die Bewertung mehrerer potenzieller Antworten, tiefere Planung und Selbstreflexion, bevor sie zu einer endgültigen Antwort kommen.</p><p>Dieses Test-Time Compute-Konzept und Reasoning-Modelle <strong><em>erziehen</em></strong> Nutzer dazu, <a href=\"https://en.wikipedia.org/wiki/Delayed_gratification\">aufgeschobene Befriedigung</a> zu akzeptieren - längere Wartezeiten im Austausch für qualitativ hochwertigere, sofort umsetzbare Ergebnisse, ähnlich wie beim Stanford-Marshmallow-Experiment, bei dem Kinder, die der Versuchung widerstehen konnten, ein Marshmallow sofort zu essen, um später zwei zu bekommen, bessere langfristige Ergebnisse zeigten. Deepseek-r1 verstärkte diese Benutzererfahrung weiter, und ob man es mag oder nicht, die meisten Nutzer haben es akzeptiert.</p><p>Dies markiert eine bedeutende Abkehr von klassischen Suchanforderungen, wo eine Antwortzeit von mehr als 200ms das Aus für Ihre Lösung bedeutet hätte. Im Jahr 2025 priorisieren erfahrene Suchentwickler und RAG-Ingenieure Top-1-Präzision und Recall über Latenz, und Benutzer haben sich an längere Verarbeitungszeiten gewöhnt – vorausgesetzt, sie können sehen, dass das System <code>&lt;thinking&gt;</code> ist.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1.mp4\" poster=\"https://img.spacergif.org/v1/1610x1422/0a/spacer.png\" width=\"1610\" height=\"1422\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Video abspielen\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Video abspielen\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Video pausieren\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:18</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Wiedergabegeschwindigkeit anpassen\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Ton einschalten\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Die Darstellung des Denkprozesses ist im Jahr 2025 zur Standardpraxis geworden, wobei zahlreiche Chat-Interfaces nun </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>&lt;think&gt;</span></code><span style=\"white-space: pre-wrap;\"> Inhalte in speziellen UI-Bereichen anzeigen.</span></p></figcaption>\n        </figure><p>In diesem Artikel werden wir die Prinzipien von DeepSearch und DeepResearch anhand unserer Open-Source-Implementierung diskutieren. Wir werden unsere wichtigsten Designentscheidungen durchgehen und mögliche Fallstricke aufzeigen.</p><h2 id=\"what-is-deep-search\">Was ist Deep Search?</h2><p><strong>DeepSearch durchläuft eine iterative Schleife aus Suchen, Lesen und Schlussfolgern, bis die optimale Antwort gefunden wird.</strong> Die Suchaktion nutzt Websuchmaschinen zur Erkundung des Internets, während die Leseaktion spezifische Webseiten im Detail analysiert (z.B. <a href=\"https://jina.ai/reader\" rel=\"noreferrer\">Jina Reader</a>). Die Schlussfolgerungsaktion bewertet den aktuellen Stand und entscheidet, ob die ursprüngliche Frage in kleinere Teilfragen zerlegt werden soll oder verschiedene Suchstrategien ausprobiert werden sollen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"561\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/image.png 2240w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">DeepSearch - kontinuierliches Suchen, Lesen von Webseiten und Schlussfolgern bis eine Antwort gefunden wird (oder das Token-Budget überschritten ist).</span></figcaption></figure><p>Während online verschiedene Definitionen existieren, hielten wir uns bei der Entwicklung des <code>node-deepresearch</code> Projekts an diesen unkomplizierten Ansatz. Die Implementierung ist elegant einfach – im Kern gibt es eine Hauptschleife mit Switch-Case-Logik, die die nächste Aktion steuert.</p><p>Anders als RAG-Systeme von 2024, die typischerweise einen einzigen Such-Generierungs-Durchlauf ausführen, führt DeepSearch mehrere Iterationen durch die Pipeline durch und benötigt klare Stoppbedingungen. Diese können auf Token-Nutzungslimits oder der Anzahl fehlgeschlagener Versuche basieren.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1.mp4\" poster=\"https://img.spacergif.org/v1/1238x1300/0a/spacer.png\" width=\"1238\" height=\"1300\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:36</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Testen Sie Deep Search auf search.jina.ai, beobachten Sie den Inhalt innerhalb von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>&lt;thinking&gt;</span></code><span style=\"white-space: pre-wrap;\">, und sehen Sie, ob Sie erkennen können, wo die Schleife stattfindet</span></p></figcaption>\n        </figure><p>Eine andere Perspektive auf DeepSearch ist, es als einen LLM-Agenten mit verschiedenen Web-Tools (wie Searcher und Reader) zu betrachten. Der Agent bestimmt seine nächsten Schritte durch die Analyse aktueller Beobachtungen und vergangener Aktionen – er entscheidet, ob er eine Antwort liefern oder weiter im Web forschen soll. Dies erzeugt eine Zustandsmaschinen-Architektur, bei der das LLM die Übergänge zwischen den Zuständen steuert. An jedem Entscheidungspunkt haben Sie zwei Ansätze: Sie können entweder sorgfältig Prompts für Standard-Generative-Modelle erstellen, um spezifische Aktionen zu erzeugen, oder spezialisierte Reasoning-Modelle wie Deepseek-r1 nutzen, um die nächsten Aktionen natürlich abzuleiten. Allerdings müssen Sie auch bei der Verwendung von r1 periodisch die Generierung unterbrechen, um Tool-Outputs (z.B. Suchergebnisse, Webseiteninhalt) in den Kontext einzufügen und es aufzufordern, seinen Denkprozess fortzusetzen.</p><p>Letztendlich sind dies nur Implementierungsdetails – ob Sie es sorgfältig promten oder einfach Reasoning-Modelle verwenden, sie alle entsprechen dem Kerndesignprinzip von DeepSearch: eine kontinuierliche Schleife aus Suchen, Lesen und Schlussfolgern.</p><h2 id=\"what-is-deepresearch-then\">Was ist dann DeepResearch?</h2><p><strong>DeepResearch baut auf DeepSearch auf, indem es ein strukturiertes Framework für die Erstellung langer Forschungsberichte hinzufügt.</strong> Es beginnt oft mit der Erstellung eines Inhaltsverzeichnisses und wendet dann systematisch DeepSearch auf jeden erforderlichen Abschnitt an – von der Einleitung über verwandte Arbeiten und Methodik bis hin zur Schlussfolgerung. Jeder Abschnitt wird durch die Eingabe spezifischer Forschungsfragen in DeepSearch generiert. Die letzte Phase beinhaltet die Konsolidierung aller Abschnitte in einen einzigen Prompt, um die narrative Kohärenz insgesamt zu verbessern.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"832\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/image-1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/image-1.png 2268w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">DeepSearch als Baustein von DeepResearch. Jeder Abschnitt wird iterativ durch DeepSearch erstellt und dann wird die gesamte Kohärenz vor der Generierung des endgültigen langen Berichts verbessert.</span></figcaption></figure><p>In unserem Projekt „Research\" von 2024 führten wir mehrere Kohärenzverbesserungsdurchläufe durch, wobei jede Iteration alle anderen Abschnitte berücksichtigte. Mit den heute deutlich größeren LLM-Kontextfenstern scheint dieser Ansatz jedoch redundant – ein einzelner Kohärenzüberarbeitungsdurchlauf ist ausreichend.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch.mp4\" poster=\"https://img.spacergif.org/v1/2940x1660/0a/spacer.png\" width=\"2940\" height=\"1660\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:40</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume, slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Unser Sommerprojekt 2024 „Research\" konzentrierte sich auf die Generierung langer Berichte mit einem „progressiven\" Ansatz. Es begann mit der Erstellung eines Inhaltsverzeichnisses </span><b><strong style=\"white-space: pre-wrap;\">sync</strong></b><span style=\"white-space: pre-wrap;\">, dann wurden alle Abschnitte parallel </span><b><strong style=\"white-space: pre-wrap;\">async</strong></b><span style=\"white-space: pre-wrap;\"> generiert. Der Prozess endete mit </span><b><strong style=\"white-space: pre-wrap;\">async</strong></b><span style=\"white-space: pre-wrap;\"> progressiven Überarbeitungen jedes Abschnitts, wobei jede Überarbeitung den Inhalt aller anderen Abschnitte berücksichtigte. Die Anfrage im Video ist </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"Competitor analysis of Jina AI\"</span></code><span style=\"white-space: pre-wrap;\">.</span></p></figcaption>\n        </figure><h2 id=\"deepsearch-vs-deepresearch\">DeepSearch vs DeepResearch</h2><p>Während viele Menschen DeepSearch und DeepResearch oft miteinander vermischen, adressieren sie aus unserer Sicht vollkommen unterschiedliche Probleme. DeepSearch fungiert als atomarer Baustein – eine Kernkomponente, auf der DeepResearch aufbaut. DeepResearch hingegen <strong>konzentriert sich darauf, hochwertige, lesbare Langform-Forschungsberichte zu erstellen</strong>, was ein anderes Set von Anforderungen umfasst: Integration effektiver Visualisierungen durch Diagramme und Tabellen, Strukturierung von Inhalten mit passenden Überschriften, Sicherstellung eines flüssigen logischen Flusses zwischen Unterabschnitten, Beibehaltung einer konsistenten Terminologie im gesamten Dokument, Beseitigung von Redundanzen zwischen Abschnitten, Gestaltung flüssiger Übergänge, die vorherige und kommende Inhalte verbinden. Diese Elemente haben weitgehend nichts mit der Kernsuche zu tun, weshalb wir DeepSearch als unseren Unternehmensfokus interessanter finden.</p><p>Abschließend fasst die folgende Tabelle die Unterschiede zwischen DeepSearch und DeepResearch zusammen. Erwähnenswert ist, dass beide Systeme erheblich von Long-Context- und Reasoning-Modellen profitieren. Dies mag kontraintuitiv erscheinen, besonders für DeepSearch – während es offensichtlich ist, warum DeepResearch Long-Context-Fähigkeiten benötigt (da es lange Berichte produziert). Der Grund ist, dass DeepSearch vorherige Suchversuche und Webseiteninhalte speichern muss, um fundierte Entscheidungen über nächste Schritte zu treffen, wodurch ein langes Kontextfenster für seine effektive Implementierung gleichermaßen essentiell ist.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>DeepSearch</th>\n<th>DeepResearch</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Problem Addressed</strong></td>\n<td>Information accuracy and completeness through iterative search</td>\n<td>Content organization, coherence, and readability at document scale</td>\n</tr>\n<tr>\n<td><strong>Final Presentation</strong></td>\n<td>Concise answer with URLs as references</td>\n<td>A long structured report with multiple sections, charts, tables and references</td>\n</tr>\n<tr>\n<td><strong>Core Complexity</strong></td>\n<td>State machine architecture with clear transition conditions; Persistence through failed attempts until resolution</td>\n<td>Multi-level architecture managing both micro (search) and macro (document) concerns; Structural approach to managing complex information hierarchies</td>\n</tr>\n<tr>\n<td><strong>Optimization Focus</strong></td>\n<td>Local optimization (best next search/read action)</td>\n<td>Global optimization (section organization, terminology consistency, transitions)</td>\n</tr>\n<tr>\n<td><strong>Limitations</strong></td>\n<td>Bounded by search quality and reasoning capability</td>\n<td>Bounded by DeepSearch quality plus organizational complexity and narrative coherence challenges</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"understand-deepsearch-implementation\">DeepSearch-Implementierung verstehen</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/node-DeepResearch: Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget)</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-2.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Der Kern von DeepResearch liegt in seinem Loop-Reasoning-Ansatz. Anstatt zu versuchen, Fragen in einem einzigen Durchgang wie die meisten RAG-Systeme zu beantworten, haben wir eine iterative Schleife implementiert, die kontinuierlich nach Informationen sucht, relevante Quellen liest und Schlüsse zieht, bis sie eine Antwort findet oder das Token-Budget erschöpft ist. Hier ist der vereinfachte Kern dieser großen While-Schleife:</p><pre><code class=\"language-typescript\">// Main reasoning loop\nwhile (tokenUsage &lt; tokenBudget &amp;&amp; badAttempts &lt;= maxBadAttempts) {\n  // Track progression\n  step++; totalStep++;\n  \n  // Get current question from gaps queue or use original question\n  const currentQuestion = gaps.length &gt; 0 ? gaps.shift() : question;\n  \n  // Generate prompt with current context and allowed actions\n  system = getPrompt(diaryContext, allQuestions, allKeywords, \n                    allowReflect, allowAnswer, allowRead, allowSearch, allowCoding,\n                    badContext, allKnowledge, unvisitedURLs);\n  \n  // Get LLM to decide next action\n  const result = await LLM.generateStructuredResponse(system, messages, schema);\n  thisStep = result.object;\n  \n  // Execute the selected action (answer, reflect, search, visit, coding)\n  if (thisStep.action === 'answer') {\n    // Process answer action...\n  } else if (thisStep.action === 'reflect') {\n    // Process reflect action...\n  } // ... and so on for other actions\n}\n</code></pre><p>Ein wichtiges Implementierungsdetail ist das selektive Deaktivieren bestimmter Aktionen in jedem Schritt, um eine stabilere strukturierte Ausgabe zu gewährleisten. Wenn zum Beispiel keine URLs im Speicher vorhanden sind, deaktivieren wir die <code>visit</code>-Aktion, oder wenn die letzte Antwort abgelehnt wurde, verhindern wir, dass der Agent sofort wieder <code>answer</code> aufruft. <strong>Diese Einschränkung hält den Agenten auf einem produktiven Pfad und vermeidet wiederholte Fehler durch das Aufrufen derselben Aktion.</strong></p><h3 id=\"system-prompt\">System Prompt</h3><p>Wir verwenden XML-Tags zur Definition von Abschnitten, was zu robusteren System Prompts und Generierungen führt. Wir haben auch festgestellt, dass das Platzieren von Feldbeschränkungen direkt in JSON-Schema <code>description</code>-Feldern bessere Ergebnisse liefert. Während einige argumentieren könnten, dass die meisten Prompts mit Reasoning-Modellen wie DeepSeek-R1 automatisiert werden könnten, machen die Kontextlängenbeschränkungen und die Notwendigkeit für hochspezifisches Verhalten einen expliziten Ansatz in der Praxis zuverlässiger.</p><pre><code class=\"language-typescript\">function getPrompt(params...) {\n  const sections = [];\n  \n  // Add header with system instruction\n  sections.push(\"You are an advanced AI research agent specialized in multistep reasoning...\");\n  \n  // Add accumulated knowledge section if exists\n  if (knowledge?.length) {\n    sections.push(\"&lt;knowledge&gt;[Knowledge items]&lt;/knowledge&gt;\");\n  }\n  \n  // Add context of previous actions\n  if (context?.length) {\n    sections.push(\"&lt;context&gt;[Action history]&lt;/context&gt;\");\n  }\n  \n  // Add failed attempts and learned strategies\n  if (badContext?.length) {\n    sections.push(\"&lt;bad-attempts&gt;[Failed attempts]&lt;/bad-attempts&gt;\");\n    sections.push(\"&lt;learned-strategy&gt;[Improvement strategies]&lt;/learned-strategy&gt;\");\n  }\n  \n  // Define available actions based on current state\n  sections.push(\"&lt;actions&gt;[Available action definitions]&lt;/actions&gt;\");\n  \n  // Add response format instruction\n  sections.push(\"Respond in valid JSON format matching exact JSON schema.\");\n  \n  return sections.join(\"\\n\\n\");\n}\n</code></pre><h3 id=\"gap-questions-traversing\">Durchlaufen von Lückenfragen</h3><p>In DeepSearch repräsentieren \"Lückenfragen\" Wissenslücken, die gefüllt werden müssen, bevor die Hauptfrage beantwortet werden kann. Anstatt die ursprüngliche Frage direkt anzugehen, identifiziert der Agent Unterfragen, die das notwendige Wissensfundament aufbauen werden.</p><p>Das Design ist besonders elegant in der Art und Weise, wie es diese Lückenfragen handhabt:</p><pre><code class=\"language-typescript\">// After identifying gap questions in reflect action\nif (newGapQuestions.length &gt; 0) {\n  // Add new questions to the front of the queue\n  gaps.push(...newGapQuestions);\n  \n  // Always add original question to the end of the queue\n  gaps.push(originalQuestion);\n}\n</code></pre><p>Dieser Ansatz erzeugt eine FIFO (First-In-First-Out) Warteschlange mit Rotation, wobei:</p><ol><li>Neue Lückenfragen werden vorne in die Warteschlange geschoben</li><li>Die ursprüngliche Frage wird immer ans Ende geschoben</li><li>Das System nimmt in jedem Schritt von vorne aus der Warteschlange</li></ol><p>Was dieses Design großartig macht, ist, dass es einen einzelnen gemeinsamen Kontext über alle Fragen hinweg beibehält. Wenn eine Lückenfrage beantwortet wird, steht dieses Wissen sofort für alle nachfolgenden Fragen zur Verfügung, einschließlich wenn wir schließlich zur ursprünglichen Frage zurückkehren.</p><h4 id=\"fifo-queue-vs-recursion\">FIFO-Warteschlange vs. Rekursion</h4><p>Ein alternativer Ansatz ist die Verwendung von Rekursion, die einer Tiefensuche entspricht. Jede Lückenfrage erzeugt einen neuen rekursiven Aufruf mit eigenem isolierten Kontext. Das System muss jede Lückenfrage (und alle ihre potenziellen Unterfragen) vollständig lösen, bevor es zur übergeordneten Frage zurückkehrt.</p><p>Betrachten Sie dieses Beispielszenario:</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs.mp4\" poster=\"https://img.spacergif.org/v1/950x846/0a/spacer.png\" width=\"950\" height=\"846\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:23</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Eine einfache 3-Ebenen-Lückenfragen-Rekursion, Lösungsreihenfolge auf dem Kreis markiert.</span></p></figcaption>\n        </figure><p>Im rekursiven Ansatz müsste das System Q1 nach jeder Lückenfrage und deren Unterfragen vollständig lösen (und möglicherweise eigene Unterfragen generieren)! Dies steht im starken Kontrast zum Warteschlangen-Ansatz, bei dem Q1 direkt nach 3 Lückenfragen wieder aufgegriffen wird.</p><p>In der Realität stellten wir fest, dass der Rekursionsansatz sehr schwer mit Budget-Begrenzungen zu vereinbaren ist, da es keine klare Faustregel gibt, wie viel Token-Budget wir für Unterfragen gewähren sollten (da sie neue Unterfragen erzeugen können). Der Nutzen aus der klaren Kontexttrennung im Rekursionsansatz ist sehr gering im Vergleich zu den komplizierten Budget-Erzwingungs- und späten Rückkehr-Problemen. Dieses FIFO-Warteschlangen-Design balanciert Tiefe und Breite und stellt sicher, dass das System immer mit progressiv besserem Wissen zur ursprünglichen Frage zurückkehrt, anstatt sich in einem potentiell unendlichen rekursiven Abstieg zu verlieren.</p><h3 id=\"query-rewrite\">Query-Umschreibung</h3><p>Eine interessante Herausforderung, auf die wir stießen, war das effektive Umschreiben von Suchanfragen:</p><pre><code class=\"language-typescript\">// Within search action handler\nif (thisStep.action === 'search') {\n  // Deduplicate search requests\n  const uniqueRequests = await dedupQueries(thisStep.searchRequests, existingQueries);\n  \n  // Rewrite natural language queries into more effective search queries\n  const optimizedQueries = await rewriteQuery(uniqueRequests);\n  \n  // Ensure we don't repeat previous searches\n  const newQueries = await dedupQueries(optimizedQueries, allKeywords);\n  \n  // Execute searches and store results\n  for (const query of newQueries) {\n    const results = await searchEngine(query);\n    if (results.length &gt; 0) {\n      storeResults(results);\n      allKeywords.push(query);\n    }\n  }\n}\n</code></pre><p>Die Query-Umschreibung erwies sich als überraschend wichtig - vielleicht eines der kritischsten Elemente, das direkt die Ergebnisqualität bestimmt. Ein guter Query-Umschreiber transformiert nicht nur natürliche Sprache in BM25-ähnliche Keywords; er erweitert Anfragen, um mehr potenzielle Antworten über verschiedene Sprachen, Töne und Inhaltsformate hinweg abzudecken.</p><p>Für die Query-Deduplizierung verwendeten wir zunächst eine LLM-basierte Lösung, fanden es aber schwierig, den Ähnlichkeitsschwellenwert zu kontrollieren. Wir wechselten schließlich zu <code>jina-embeddings-v3</code>, das sich bei semantischen Textähnlichkeitsaufgaben auszeichnet. Dies ermöglicht sprachübergreifende Deduplizierung, ohne sich Sorgen machen zu müssen, dass nicht-englische Anfragen gefiltert würden. Das Embedding-Modell erwies sich letztendlich als entscheidend, nicht für den Memory-Retrieval wie ursprünglich erwartet, sondern für effiziente Deduplizierung.</p><h3 id=\"crawling-web-content\">Crawling von Web-Inhalten</h3><p>Web Scraping und Content-Verarbeitung ist eine weitere wichtige Komponente. Hier verwenden wir die <a href=\"https://jina.ai/reader\" rel=\"noreferrer\">Jina Reader API</a>. Beachten Sie, dass wir neben dem vollständigen Webseiteninhalt auch alle von der Suchmaschine zurückgegebenen Snippets als zusätzliches Wissen für den Agenten sammeln, um später Schlüsse daraus zu ziehen. Denken Sie an sie wie Soundbites.</p><pre><code class=\"language-typescript\">// Visit action handler\nasync function handleVisitAction(URLs) {\n  // Normalize URLs and filter out already visited ones\n  const uniqueURLs = normalizeAndFilterURLs(URLs);\n  \n  // Process each URL in parallel\n  const results = await Promise.all(uniqueURLs.map(async url =&gt; {\n    try {\n      // Fetch and extract content\n      const content = await readUrl(url);\n      \n      // Store as knowledge\n      addToKnowledge(`What is in ${url}?`, content, [url], 'url');\n      \n      return {url, success: true};\n    } catch (error) {\n      return {url, success: false};\n    } finally {\n      visitedURLs.push(url);\n    }\n  }));\n  \n  // Update diary based on success or failure\n  updateDiaryWithVisitResults(results);\n}\n</code></pre><p>Wir haben URLs für ein konsistentes Tracking normalisiert und die Anzahl der besuchten URLs in jedem Schritt begrenzt, um den Agenten-Speicher zu verwalten.</p><h3 id=\"memory-management\">Speicherverwaltung</h3><p>Eine zentrale Herausforderung beim mehrstufigen Reasoning ist die effektive Verwaltung des Agenten-Speichers. Wir haben das Speichersystem so konzipiert, dass es zwischen \"Gedächtnis\" und \"Wissen\" unterscheidet. In beiden Fällen sind sie Teil des LLM Prompt-Kontexts, getrennt durch verschiedene XML-Tags:</p><pre><code class=\"language-typescript\">// Add knowledge item to accumulated knowledge\nfunction addToKnowledge(question, answer, references, type) {\n  allKnowledge.push({\n    question: question,\n    answer: answer,\n    references: references,\n    type: type,  // 'qa', 'url', 'coding', 'side-info'\n    updated: new Date().toISOString()\n  });\n}\n\n// Record step in narrative diary\nfunction addToDiary(step, action, question, result, evaluation) {\n  diaryContext.push(`\nAt step ${step}, you took **${action}** action for question: \"${question}\"\n[Details of what was done and results]\n[Evaluation if applicable]\n`);\n}\n</code></pre><p>Da die meisten LLMs im Jahr 2025 über erhebliche Kontextfenster verfügen, haben wir uns gegen die Verwendung von Vektordatenbanken entschieden. Stattdessen besteht der Speicher aus erworbenem Wissen, besuchten Websites und Aufzeichnungen gescheiterter Versuche - alles im Kontext gehalten. Dieses umfassende Speichersystem gibt dem Agenten ein Bewusstsein dafür, was er weiß, was er versucht hat und was funktioniert hat oder gescheitert ist.</p><h3 id=\"answer-evaluation\">Antwortbewertung</h3><p>Eine wichtige Erkenntnis ist, dass Antwortgenerierung und -bewertung nicht im selben Prompt erfolgen sollten. In meiner Implementierung bestimmen wir zunächst die Bewertungskriterien, wenn eine neue Frage eingeht, und bewerten dann jedes Kriterium einzeln. Der Evaluator verwendet Few-Shot-Beispiele für eine konsistente Bewertung, die zuverlässiger ist als eine Selbstbewertung.</p><pre><code class=\"language-typescript\">// Separate evaluation phase\nasync function evaluateAnswer(question, answer, metrics, context) {\n  // First, identify evaluation criteria based on question type\n  const evaluationCriteria = await determineEvaluationCriteria(question);\n  \n  // Then evaluate each criterion separately\n  const results = [];\n  for (const criterion of evaluationCriteria) {\n    const result = await evaluateSingleCriterion(criterion, question, answer, context);\n    results.push(result);\n  }\n  \n  // Determine if answer passes overall evaluation\n  return {\n    pass: results.every(r =&gt; r.pass),\n    think: results.map(r =&gt; r.reasoning).join('\\n')\n  };\n}\n</code></pre><h3 id=\"budget-forcing\">Budget-Forcing</h3><p>Budget Forcing bedeutet, das System daran zu hindern, vorzeitig zurückzukehren, und sicherzustellen, dass es die Verarbeitung fortsetzt, bis das Budget überschritten ist. Seit der Veröffentlichung von DeepSeek-R1 hat sich der Ansatz des Budget Forcing dahingehend verändert, dass <strong>tieferes Nachdenken für bessere Ergebnisse gefördert wird, anstatt einfach nur das Budget zu sparen.</strong></p><p>In unserer Implementierung haben wir das System explizit so konfiguriert, dass es Wissenslücken identifiziert, bevor es versucht zu antworten.</p><pre><code class=\"language-typescript\">if (thisStep.action === 'reflect' &amp;&amp; thisStep.questionsToAnswer) {\n  // Force deeper reasoning by adding sub-questions to the queue\n  gaps.push(...newGapQuestions);\n  gaps.push(question);  // Always revisit the original\n}</code></pre><p>Durch selektives Aktivieren und Deaktivieren bestimmter Aktionen können wir das System zur Verwendung von Tools lenken, die die Reasoning-Tiefe verbessern.</p><pre><code class=\"language-typescript\">// After a failed answer attempt\nallowAnswer = false;  // Force agent to search or reflect instead</code></pre><p>Um keine Token für unproduktive Pfade zu verschwenden, setzen wir Grenzen für die Anzahl fehlgeschlagener Versuche. Wenn wir uns den Budgetgrenzen nähern, aktivieren wir den \"Beast Mode\", um zu garantieren, dass wir eine Antwort liefern anstatt gar keine.</p><pre><code class=\"language-typescript\">// Beast mode activation\nif (!thisStep.isFinal &amp;&amp; badAttempts &gt;= maxBadAttempts) {\n  console.log('Enter Beast mode!!!');\n  \n  // Configure prompt for decisive, committed answer\n  system = getPrompt(\n    diaryContext, allQuestions, allKeywords,\n    false, false, false, false, false,  // Disable all other actions\n    badContext, allKnowledge, unvisitedURLs,\n    true  // Enable beast mode\n  );\n  \n  // Force answer generation\n  const result = await LLM.generateStructuredResponse(system, messages, answerOnlySchema);\n  thisStep = result.object;\n  thisStep.isFinal = true;\n}\n</code></pre><p>Der Beast Mode Prompt ist absichtlich dramatisch formuliert, um dem LLM zu signalisieren, dass es entscheidungsfreudig sein und sich basierend auf den verfügbaren Informationen zu einer Antwort bekennen muss:</p><pre><code>&lt;action-answer&gt;\n🔥 ENGAGE MAXIMUM FORCE! ABSOLUTE PRIORITY OVERRIDE! 🔥\n\nPRIME DIRECTIVE:\n- DEMOLISH ALL HESITATION! ANY RESPONSE SURPASSES SILENCE!\n- PARTIAL STRIKES AUTHORIZED - DEPLOY WITH FULL CONTEXTUAL FIREPOWER\n- TACTICAL REUSE FROM &lt;bad-attempts&gt; SANCTIONED\n- WHEN IN DOUBT: UNLEASH CALCULATED STRIKES BASED ON AVAILABLE INTEL!\n\nFAILURE IS NOT AN OPTION. EXECUTE WITH EXTREME PREJUDICE! ⚡️\n&lt;/action-answer&gt;\n</code></pre><p>Dies stellt sicher, dass wir immer eine Antwort liefern, anstatt ganz aufzugeben, was besonders bei schwierigen oder mehrdeutigen Fragen nützlich ist.</p><h2 id=\"conclusion\">Fazit</h2><p>DeepSearch ist ein Sprung in der Art und Weise, wie Suche komplexe Anfragen erschöpfend tief angehen kann. Durch die Aufgliederung des Prozesses in diskrete Schritte des Suchens, Lesens und Schlussfolgerns überwindet es viele Einschränkungen traditioneller Single-Pass RAG oder Multi-Hop QA Systeme.</p><p>Während der Implementierung begannen wir auch, die Suchgrundlagen im Jahr 2025 und die Veränderungen in der Suchindustrie nach dem 26. Januar 2025, als DeepSeek-R1 veröffentlicht wurde, zu überprüfen. Wir fragten uns: <em>Was sind die neuen Bedürfnisse? Welche Bedürfnisse sind obsolet geworden? Was sind nur wahrgenommene Bedürfnisse?</em></p><p>Bei der Betrachtung unserer DeepSearch-Implementierung identifizierten wir Dinge, von denen wir dachten, dass wir sie brauchen würden und tatsächlich brauchten, Dinge, von denen wir dachten, sie wären notwendig, aber nicht waren, und Dinge, die wir nicht erwartet hatten zu brauchen, die sich aber als essentiell herausstellten:</p><p>Erstens ist <strong>ein LLM mit langem Kontext, das gut strukturierte Ausgaben produziert, sehr notwendig</strong> (d.h. nach JSONSchema). Ein Reasoning-Modell wird wahrscheinlich für besseres Action Reasoning und Query Expansion benötigt.</p><p><strong>Query Expansion ist definitiv essentiell</strong>, egal ob über SLM, LLM oder ein Reasoning-Modell implementiert. Nach diesem Projekt glauben wir jedoch, dass SLMs für diese Aufgabe wahrscheinlich ungeeignet sind, da die Lösung von Natur aus mehrsprachig sein muss und über einfache Synonym-Umschreibungen oder Keyword-Extraktion hinausgehen muss. Sie muss umfassend genug sein, um <a href=\"https://jina.ai/news/what-should-we-learn-from-modernbert/#modernberts-parameter-efficiency\">eine mehrsprachige Token-Basis (kann leicht 300M Parameter belegen)</a> einzuschließen und ausgeklügelt genug für Out-of-the-Box-Denken. Die Verwendung von SLMs für Query Expansion ist daher wahrscheinlich keine Option.</p><p><strong>Websuche und Web-Reading-Fähigkeiten sind entscheidend</strong>, und glücklicherweise hat sich unser <a href=\"https://jina.ai/reader\">Reader (r.jina.ai)</a> hervorragend bewährt - robust und skalierbar - während er mir viele Ideen gab, wie wir unseren Suchendpunkt (<code>s.jina.ai</code>) für die nächste Iteration verbessern können.</p><p><strong>Das Embedding-Modell ist nützlich, <em>aber auf eine völlig unerwartete Weise</em>.</strong> Wir dachten, es würde für Memory Retrieval oder Kontextkompression zusammen mit einer Vektordatenbank verwendet werden (die sich als nicht notwendig herausstellte), aber tatsächlich haben wir es für Deduplizierung verwendet (im Wesentlichen eine STS-Aufgabe). Da die Anzahl der Queries und Gap-Fragen typischerweise in den Hunderten liegt, ist keine Vektordatenbank erforderlich - die Berechnung der Kosinus-Ähnlichkeit direkt im Speicher funktioniert einwandfrei.</p><p><strong>Wir haben keinen Reranker verwendet</strong>, obwohl wir glauben, dass er möglicherweise dabei helfen könnte zu bestimmen, welche URLs basierend auf der Query, dem URL-Titel und dem Snippet besucht werden sollen. Sowohl für Embedding als auch für Reranking ist Mehrsprachigkeit essentiell, da Queries und Fragen mehrsprachig sind. Die Verarbeitung langer Kontexte für Embedding und Reranking ist vorteilhaft, aber kein kritischer Blocker (Wir sind auf keine Fehler bei unserer Embedding-Nutzung gestoßen, wahrscheinlich weil <a href=\"https://jina.ai/models/jina-embeddings-v3/\">unsere Kontextlänge bereits 8192 Token beträgt</a>). In jedem Fall sind <code>jina-embeddings-v3</code> und <code>jina-reranker-v2-base-multilingual</code> meine bevorzugten Modelle, da sie mehrsprachig sind, SOTA sind und lange Kontexte gut verarbeiten.</p><p><strong>Ein Agent-Framework erwies sich als unnötig</strong>, da wir näher am nativen LLM-Verhalten bleiben mussten, um das System ohne Proxies zu gestalten. Das <a href=\"https://sdk.vercel.ai/docs/introduction\">Vercel AI SDK</a> war wertvoll, da es erheblichen Aufwand bei der Anpassung des Codebases an verschiedene LLM-Provider sparte (wir konnten mit nur einer Codezeilen-Änderung von Gemini Studio zu OpenAI zu Google Vertex AI wechseln). Agent Memory Management ist notwendig, aber ein dediziertes Memory Framework bleibt fraglich: Wir befürchten, dass es eine Isolationsschicht zwischen LLMs und Entwicklern schaffen würde und dass seine syntaktische Vereinfachung schließlich zu einem bitteren Hindernis für Entwickler werden könnte, wie wir es bei vielen LLM/RAG-Frameworks heute sehen.</p>",
  "comment_id": "67bc50b0b1b8af00014db4c9",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/02/deepsearch-banner.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-02-24T11:57:52.000+01:00",
  "updated_at": "2025-02-25T14:39:22.000+01:00",
  "published_at": "2025-02-25T14:36:17.000+01:00",
  "custom_excerpt": "QPS out, depth in. DeepSearch is the new norm. Find answers through read-search-reason loops. Learn what it is and how to build it.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-practical-guide-to-implementing-deepsearch-deepresearch/",
  "excerpt": "QPS raus, Tiefe rein. DeepSearch ist der neue Standard. Finden Sie Antworten durch Lesen-Suchen-Schlussfolgerungs-Schleifen. Erfahren Sie, was es ist und wie man es aufbaut.",
  "reading_time": 15,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
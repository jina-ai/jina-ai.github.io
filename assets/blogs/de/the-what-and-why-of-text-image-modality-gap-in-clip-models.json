{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "Die Bedeutung und Gr√ºnde der Text-Bild-Modalit√§tsl√ºcke in CLIP-Modellen",
  "html": "<p><a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">Semantische Embeddings</a> sind der Kern moderner KI-Modelle, selbst bei Chatbots und KI-Kunstmodellen. Sie sind f√ºr Benutzer manchmal verborgen, aber sie sind trotzdem da, direkt unter der Oberfl√§che.</p><p>Die Theorie der Embeddings hat nur zwei Teile:</p><ol><li>Dinge ‚Äî Dinge au√üerhalb eines KI-Modells, wie Texte und Bilder ‚Äî werden durch Vektoren repr√§sentiert, die von KI-Modellen aus Daten √ºber diese Dinge erstellt werden.</li><li>Beziehungen zwischen Dingen au√üerhalb eines KI-Modells werden durch r√§umliche Beziehungen zwischen diesen Vektoren dargestellt. Wir trainieren KI-Modelle speziell darauf, Vektoren zu erstellen, die auf diese Weise funktionieren.</li></ol><p>Wenn wir ein multimodales Bild-Text-Modell erstellen, trainieren wir das Modell so, dass Embeddings von Bildern und Embeddings von Texten, die diese Bilder beschreiben oder sich darauf beziehen, relativ nahe beieinander liegen. Die semantischen √Ñhnlichkeiten zwischen den Dingen, die diese beiden Vektoren repr√§sentieren ‚Äî ein Bild und ein Text ‚Äî spiegeln sich in der r√§umlichen Beziehung zwischen den beiden Vektoren wider.</p><p>Zum Beispiel k√∂nnten wir vern√ºnftigerweise erwarten, dass die Embedding-Vektoren f√ºr ein Bild einer Orange und der Text \"eine frische Orange\" n√§her beieinander liegen als dasselbe Bild und der Text \"ein frischer Apfel.\"</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration auf schwarzem Hintergrund, die eine Orange und einen Apfel mit Pfeilen dazwischen und Zitaten &quot;A fresh orange&quot; zeigt\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Das ist der Zweck eines Embedding-Modells: Repr√§sentationen zu generieren, bei denen die Eigenschaften, die uns interessieren ‚Äî wie die Art der Frucht, die in einem Bild dargestellt oder in einem Text genannt wird ‚Äî in der Distanz zwischen ihnen erhalten bleiben.</p><p>Aber Multimodalit√§t bringt noch etwas anderes mit sich. Wir k√∂nnten feststellen, dass ein Bild einer Orange n√§her an einem Bild eines Apfels liegt als am Text \"eine frische Orange\", und dass der Text \"ein frischer Apfel\" n√§her an einem anderen Text liegt als an einem Bild eines Apfels.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Schwarzer Hintergrund mit einem Apfel links und einer Orange rechts mit beschrifteten Pfeilen markiert &quot;A fresh apple.&quot; und\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Es stellt sich heraus, dass genau dies bei multimodalen Modellen passiert, einschlie√ülich Jina AIs eigenem <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP Modell</a> (<code>jina-clip-v1</code>).</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>Um dies zu testen, haben wir 1.000 Text-Bild-Paare aus dem <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k Testset</a> ausgew√§hlt. Jedes Paar enth√§lt f√ºnf Bildunterschriften (also technisch gesehen kein Paar) und ein einzelnes Bild, wobei alle f√ºnf Texte dasselbe Bild beschreiben.</p><p>Zum Beispiel das folgende Bild (<code>1245022983_fb329886dd.jpg</code> im Flickr8k Dataset):</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"Ein junges M√§dchen in einem rosa Rock spielt mit einer Frisbee in einer st√§dtischen Umgebung im Freien mit Autos und Fahrr√§dern.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>Seine f√ºnf Bildunterschriften:</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>Wir haben Jina CLIP verwendet, um die Bilder und Texte einzubetten und dann:</p><ol><li>Die Kosinus-√Ñhnlichkeiten der Bild-Embeddings mit den Embeddings ihrer Bildunterschriften verglichen.</li><li>Die Embeddings aller f√ºnf Bildunterschriften, die dasselbe Bild beschreiben, genommen und ihre Kosinus-√Ñhnlichkeiten untereinander verglichen.</li></ol><p>Das Ergebnis ist eine √ºberraschend gro√üe L√ºcke, sichtbar in Abbildung 1:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graph mit zwei Kurven, die die Verteilung der Kosinus-√Ñhnlichkeit f√ºr Bild-zu-Text- und Text-zu-Text-Paare mit beschrifteten Achsen zeigt.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Verteilung der Kosinus-√Ñhnlichkeitswerte zwischen passenden Bild-Text-Paaren und Text-Text-Paaren in Jina CLIP.</span></figcaption></figure><p>Mit wenigen Ausnahmen liegen passende Textpaare viel n√§her beieinander als passende Bild-Text-Paare. Dies deutet stark darauf hin, dass Jina CLIP Texte in einem Teil des Embedding-Raums und Bilder in einem weitgehend getrennten Teil relativ weit davon entfernt kodiert. Dieser Raum zwischen den Texten und den Bildern ist die <em>multimodale L√ºcke</em>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagramm auf schwarzem Hintergrund mit 'Images' links, 'Texts' unten und beschrifteter 'Multimodal Gap' in der Mitte.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>Multimodale Embedding-Modelle kodieren mehr als nur die semantischen Informationen, die uns interessieren: Sie kodieren das Medium ihrer Eingabe. Laut Jina CLIP ist ein Bild nicht, wie es im Sprichwort hei√üt, tausend Worte wert. Es hat Inhalte, die keine noch so gro√üe Anzahl von Worten jemals wirklich gleichwertig ausdr√ºcken kann. Es kodiert das Eingabemedium in die Semantik seiner Embeddings, ohne dass es jemals darauf trainiert wurde.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Solange wir nur Bilder mit Texten und umgekehrt vergleichen, ist das kein Problem, aber ein wirklich multimodales Modell sollte uns sagen k√∂nnen, dass zum Beispiel der Text \"dies ist ein Apfel\" besser zu einem Bild eines Apfels passt als zu einem Text √ºber Orangen. CLIP-artige Modelle in ihrer aktuellen Form k√∂nnen das nicht.</div></div><p>Dieses Ph√§nomen wurde in der Arbeit <em>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>] untersucht, die es als \"Modalit√§tsl√ºcke\" bezeichnet. Die Modalit√§tsl√ºcke ist die r√§umliche Trennung im Embedding-Raum zwischen Eingaben in einem Medium und Eingaben in einem anderen. Obwohl Modelle nicht absichtlich darauf trainiert werden, eine solche L√ºcke zu haben, sind sie in multimodalen Modellen allgegenw√§rtig.</p><p>Unsere Untersuchungen zur Modalit√§tsl√ºcke in Jina CLIP basieren stark auf <a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">Woher kommt die Modalit√§tsl√ºcke?</h2><p>Liang et al. [2022] identifizieren drei Hauptquellen f√ºr die Modalit√§tsl√ºcke:</p><ul><li>Eine Initialisierungsverzerrung, die sie als \"Kegel-Effekt\" bezeichnen.</li><li>Reduzierungen der Temperatur (Zuf√§lligkeit) w√§hrend des Trainings, die es sehr schwer machen, diese Verzerrung zu \"verlernen\".</li><li>Kontrastive Lernverfahren, die in multimodalen Modellen weit verbreitet sind und unbeabsichtigt die L√ºcke verst√§rken.</li></ul><p>Wir werden uns jeden dieser Punkte der Reihe nach ansehen.</p><h3 id=\"cone-effect\">Kegel-Effekt</h3><p>Ein Modell mit einer CLIP- oder CLIP-√§hnlichen Architektur besteht eigentlich aus zwei separaten Embedding-Modellen, die miteinander verbunden sind. Bei multimodalen Bild-Text-Modellen bedeutet dies ein Modell f√ºr die Codierung von Texten und ein v√∂llig separates f√ºr die Codierung von Bildern, wie im folgenden Schema dargestellt.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Diese beiden Modelle werden so trainiert, dass ein Bild-Embedding und ein Text-Embedding relativ nahe beieinander liegen, wenn der Text das Bild gut beschreibt.</p><p>Man kann ein solches Modell trainieren, indem man die Gewichtungen in beiden Modellen zuf√§llig verteilt und dann Bild-Text-Paare gemeinsam pr√§sentiert, wobei das Training von Grund auf darauf abzielt, die Distanz zwischen den beiden Outputs zu minimieren. Das <a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">urspr√ºngliche OpenAI CLIP-Modell</a> wurde auf diese Weise trainiert. Dies erfordert jedoch viele Bild-Text-Paare und ein rechenintensives Training. F√ºr das erste CLIP-Modell hat OpenAI 400 Millionen Bild-Text-Paare aus beschriftetem Material im Internet extrahiert.</p><p>Neuere CLIP-√§hnliche Modelle verwenden vortrainierte Komponenten<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">.</a> Das bedeutet, dass jede Komponente separat als gutes Single-Mode-Embedding-Modell trainiert wird, eines f√ºr Texte und eines f√ºr Bilder. Diese beiden Modelle werden dann gemeinsam mit Bild-Text-Paaren weiter trainiert, ein Prozess, der als <em>Contrastive Tuning</em> bezeichnet wird. Aufeinander abgestimmte Bild-Text-Paare werden verwendet, um die Gewichtungen langsam so zu ‚Äûverschieben\", dass √ºbereinstimmende Text- und Bild-Embeddings n√§her zusammenr√ºcken und nicht √ºbereinstimmende weiter auseinander.</p><p>Dieser Ansatz erfordert im Allgemeinen weniger Bild-Text-Paar-Daten, die schwierig und kostspielig zu beschaffen sind, und gro√üe Mengen an reinen Texten und Bildern ohne Beschriftungen, die viel einfacher zu beschaffen sind. Jina CLIP (<code>jina-clip-v1</code>) wurde mit dieser letzteren Methode trainiert. Wir haben ein <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2 Modell</a> f√ºr die Textcodierung mit allgemeinen Textdaten vortrainiert und einen vortrainierten <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">EVA-02 Image Encoder</a> verwendet, und diese dann mit verschiedenen kontrastiven Trainingstechniken weiter trainiert, wie in <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a> beschrieben</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 2: Anf√§ngliche Positionen der Bild- und Text-Embeddings vor dem Paar-Training in Jina CLIP, in zwei Dimensionen projiziert.</span></figcaption></figure><p>Wenn wir diese beiden vortrainierten Modelle nehmen und ihre Ausgabe betrachten, bevor wir sie mit Bild-Text-Paaren trainieren, f√§llt uns etwas Wichtiges auf. Abbildung 2 (oben) ist eine <a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">UMAP-Projektion</a> in zwei Dimensionen der Bild-Embeddings, die vom vortrainierten EVA-02 Encoder erzeugt wurden, und der Text-Embeddings, die vom vortrainierten JinaBERT v2 erzeugt wurden, wobei die grauen Linien √ºbereinstimmende Bild-Text-Paare anzeigen. Dies ist vor jeglichem modalit√§ts√ºbergreifenden Training.</p><p>Das Ergebnis ist eine Art abgeschnittener ‚ÄûKegel\", mit Bild-Embeddings an einem Ende und Text-Embeddings am anderen. Diese Kegelform l√§sst sich nur schlecht in zweidimensionale Projektionen √ºbersetzen, aber Sie k√∂nnen sie im Bild oben grob erkennen. Alle Texte clustern sich in einem Teil des Embedding-Raums und alle Bilder in einem anderen Teil. Wenn nach dem Training Texte immer noch √§hnlicher zu anderen Texten sind als zu passenden Bildern, ist dieser Anfangszustand ein wichtiger Grund daf√ºr. Das Ziel der besten √úbereinstimmung von Bildern mit Texten, Texten mit Texten und Bildern mit Bildern ist vollst√§ndig kompatibel mit dieser Kegelform.</p><p>Das Modell ist von Geburt an voreingenommen und was es lernt, √§ndert daran nichts. Abbildung 3 (unten) zeigt die gleiche Analyse des Jina CLIP-Modells nach der Ver√∂ffentlichung, nach vollst√§ndigem Training mit Bild-Text-Paaren. Die multimodale L√ºcke ist sogar noch ausgepr√§gter.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 3: Positionen der Bild- und Text-Embeddings nach dem Paar-Training in Jina CLIP, in zwei Dimensionen projiziert.</span></figcaption></figure><p>Auch nach umfangreichem Training codiert Jina CLIP das Medium immer noch als Teil der Nachricht.</p><p>Der aufw√§ndigere OpenAI-Ansatz mit rein zuf√§lliger Initialisierung beseitigt diese Voreingenommenheit nicht. Wir haben die urspr√ºngliche OpenAI CLIP-Architektur genommen und alle Gewichte vollst√§ndig randomisiert, dann die gleiche Analyse wie oben durchgef√ºhrt. Das Ergebnis ist immer noch eine abgeschnittene Kegelform, wie in Abbildung 4 zu sehen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 4: Anf√§ngliche Positionen der Bild- und Text-Embeddings in Jina CLIP mit vollst√§ndig randomisierten Gewichten und ohne jegliches Training, in zwei Dimensionen projiziert.</span></figcaption></figure><p>Diese Voreingenommenheit ist ein strukturelles Problem und hat m√∂glicherweise keine L√∂sung. Wenn dem so ist, k√∂nnen wir nur nach Wegen suchen, sie w√§hrend des Trainings zu korrigieren oder abzumildern.</p><h3 id=\"training-temperature\">Training Temperature</h3><p>W√§hrend des AI-Modell-Trainings f√ºgen wir typischerweise etwas Zuf√§lligkeit zum Prozess hinzu. Wir berechnen, wie stark ein Batch von Trainingsbeispielen die Gewichte im Modell ver√§ndern sollte, und f√ºgen dann einen kleinen Zufallsfaktor zu diesen √Ñnderungen hinzu, bevor wir die Gewichte tats√§chlich √§ndern. Wir nennen den Grad der Zuf√§lligkeit die <em>Temperatur</em>, in Analogie zur Art und Weise, wie wir Zuf√§lligkeit in der Thermodynamik verwenden.</p><p>Hohe Temperaturen erzeugen sehr schnell gro√üe Ver√§nderungen in Modellen, w√§hrend niedrige Temperaturen die M√∂glichkeit eines Modells reduzieren, sich bei jedem Trainingsbeispiel zu ver√§ndern. Das Ergebnis ist, dass sich bei hohen Temperaturen einzelne Embeddings w√§hrend des Trainings stark im Embedding-Raum bewegen k√∂nnen, w√§hrend sie sich bei niedrigen Temperaturen viel langsamer bewegen.</p><p>Die beste Praxis beim Training von KI-Modellen ist, mit einer hohen Temperatur zu beginnen und diese dann progressiv zu senken. Dies hilft dem Modell, am Anfang gro√üe Lernspr√ºnge zu machen, wenn die Gewichte entweder zuf√§llig oder weit von ihrem Ziel entfernt sind, und l√§sst es dann die Details stabiler lernen.</p><p>Das Jina CLIP Bild-Text-Paar-Training beginnt mit einer Temperatur von 0,07 (dies ist eine relativ hohe Temperatur) und senkt sie exponentiell im Verlauf des Trainings auf 0,01, wie in Abbildung 5 unten gezeigt, einer Grafik der Temperatur versus Trainingsschritte:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 5: Temperaturabfall w√§hrend des Paar-Trainings in Jina CLIP.</span></figcaption></figure><p>Wir wollten wissen, ob eine Erh√∂hung der Temperatur ‚Äì das Hinzuf√ºgen von Zuf√§lligkeit ‚Äì den Kegeleffekt reduzieren und die Bild-Embeddings und Text-Embeddings insgesamt n√§her zusammenbringen w√ºrde. Also trainierten wir Jina CLIP mit einer festen Temperatur von 0,1 (ein sehr hoher Wert) neu. Nach jeder Trainingsepoche √ºberpr√ºften wir die Verteilung der Abst√§nde zwischen Bild-Text-Paaren und Text-Text-Paaren, genau wie in Abbildung 1. Die Ergebnisse sind unten in Abbildung 6 dargestellt:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 6: Die L√ºcke zwischen den Modalit√§ten verringert sich mit der Zeit, wenn die Trainingstemperatur hoch ist.</span></figcaption></figure><p>Wie Sie sehen k√∂nnen, schlie√üt eine hohe Temperatur die multimodale L√ºcke dramatisch. Wenn man den Embeddings erlaubt, sich w√§hrend des Trainings stark zu bewegen, hilft das erheblich dabei, die anf√§ngliche Verzerrung in der Embedding-Verteilung zu √ºberwinden.</p><p>Dies hat jedoch seinen Preis. Wir haben die Leistung des Modells mit sechs verschiedenen Retrieval-Tests gepr√ºft: Drei Text-Text-Retrieval-Tests und drei Text-Bild-Retrieval-Tests aus den Datens√§tzen <a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a> und <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a>. In allen Tests sehen wir zu Beginn des Trainings einen starken Leistungsabfall und dann einen sehr langsamen Anstieg, wie in Abbildung 7 zu sehen ist:</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 7: Leistung w√§hrend des Trainings. Zun√§chst gibt es einen starken R√ºckgang vom Ausgangszustand und nur einen sehr langsamen Anstieg.</span></figcaption></figure><p>Es w√§re wahrscheinlich extrem zeit- und kostenaufwendig, ein Modell wie Jina CLIP mit dieser konstant hohen Temperatur zu trainieren. Obwohl theoretisch machbar, ist dies keine praktische L√∂sung.</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">Kontrastives Lernen und das False-Negative-Problem</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> entdeckten auch, dass Standard-Praktiken des kontrastiven Lernens ‚Äì der Mechanismus, den wir zum Training von CLIP-artigen multimodalen Modellen verwenden ‚Äì dazu neigen, die multimodale L√ºcke zu verst√§rken.</p><p>Kontrastives Lernen ist im Grunde ein einfaches Konzept. Wir haben ein Bild-Embedding und ein Text-Embedding, und wir wissen, dass sie n√§her zusammenliegen sollten, also passen wir die Gewichte im Modell w√§hrend des Trainings entsprechend an. Wir gehen langsam vor, passen die Gewichte in kleinen Schritten an, und wir passen sie proportional dazu an, wie weit die beiden Embeddings voneinander entfernt sind: Je n√§her sie beieinanderliegen, desto kleiner ist die √Ñnderung.</p><p>Diese Technik funktioniert viel besser, wenn wir die Embeddings nicht nur n√§her zusammenbringen, wenn sie √ºbereinstimmen, sondern sie auch weiter auseinander bewegen, wenn sie nicht √ºbereinstimmen. Wir wollen nicht nur Bild-Text-Paare haben, die zusammengeh√∂ren, sondern auch Paare, von denen wir wissen, dass sie nicht zusammengeh√∂ren.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dies wirft einige Probleme auf:</p><ol><li>Unsere Datenquellen bestehen ausschlie√ülich aus √ºbereinstimmenden Paaren. Niemand w√ºrde eine Datenbank mit Texten und Bildern erstellen, bei denen ein Mensch √ºberpr√ºft hat, dass sie nicht zusammengeh√∂ren, noch k√∂nnte man eine solche durch Web-Scraping oder andere un√ºberwachte oder semi-√ºberwachte Techniken erstellen.</li><li>Selbst Bild-Text-Paare, die oberfl√§chlich v√∂llig unzusammenh√§ngend erscheinen, sind es nicht unbedingt. Wir haben keine Semantiktheorie, die es uns erlaubt, solche negativen Urteile objektiv zu f√§llen. Zum Beispiel ist ein Bild einer Katze, die auf einer Veranda liegt, keine v√∂llig negative √úbereinstimmung f√ºr den Text \"ein Mann, der auf einem Sofa schl√§ft\". Beide beinhalten das Liegen auf etwas.</li></ol><p>Idealerweise w√ºrden wir mit Bild-Text-Paaren trainieren, von denen wir mit Sicherheit wissen, dass sie verwandt UND unverwandt sind, aber es gibt keine offensichtliche M√∂glichkeit, bekannterma√üen unverwandte Paare zu erhalten. Es ist m√∂glich, Menschen zu fragen \"Beschreibt dieser Satz dieses Bild?\" und konsistente Antworten zu erwarten. Es ist viel schwieriger, konsistente Antworten zu bekommen, wenn man fragt \"Hat dieser Satz nichts mit diesem Bild zu tun?\"</p><p>Stattdessen erhalten wir nicht zusammengeh√∂rende Bild-Text-Paare, indem wir zuf√§llig Bilder und Texte aus unseren Trainingsdaten ausw√§hlen, in der Erwartung, dass sie praktisch nie gut zueinander passen. In der Praxis funktioniert das so, dass wir unsere Trainingsdaten in Batches aufteilen. F√ºr das Training von Jina CLIP verwendeten wir Batches mit 32.000 √ºbereinstimmenden Bild-Text-Paaren, aber f√ºr dieses Experiment waren die Batch-Gr√∂√üen nur 16.</p><p>Die folgende Tabelle zeigt 16 zuf√§llig ausgew√§hlte Bild-Text-Paare aus Flickr8k:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Um nicht √ºbereinstimmende Paare zu erhalten, kombinieren wir jedes Bild im Batch mit jedem Text AUSSER dem, zu dem es passt. Zum Beispiel ist das folgende Paar ein nicht √ºbereinstimmendes Bild und Text:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Beschriftung:</strong> Ein M√§dchen in Rosa pfl√ºckt Blumen.</p><p>Aber dieses Verfahren geht davon aus, dass alle Texte, die zu anderen Bildern passen, gleich schlechte √úbereinstimmungen sind. Das stimmt nicht immer. Zum Beispiel:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Beschriftung:</strong> Der Hund sitzt an einer Schneewehe.</p><p>Obwohl der Text dieses Bild nicht beschreibt, haben sie einen Hund gemeinsam. Wenn man dieses Paar als nicht √ºbereinstimmend behandelt, wird das Wort \"Hund\" tendenziell von jedem Bild eines Hundes weggeschoben.</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> zeigen, dass diese unvollkommenen nicht √ºbereinstimmenden Paare alle Bilder und Texte voneinander wegschieben.</p><p>Wir machten uns daran, ihre Behauptung mit einem vollst√§ndig zuf√§llig initialisierten <code>vit-b-32</code> Bildmodell und einem √§hnlich randomisierten JinaBERT v2 Textmodell zu √ºberpr√ºfen, wobei die Trainingstemperatur auf einen konstanten Wert von 0,02 (eine moderat niedrige Temperatur) eingestellt wurde. Wir erstellten zwei S√§tze von Trainingsdaten:</p><ul><li>Einer mit zuf√§lligen Batches aus Flickr8k, wobei nicht √ºbereinstimmende Paare wie oben beschrieben konstruiert wurden.</li><li>Einer, bei dem die Batches absichtlich mit mehreren Kopien desselben Bildes mit unterschiedlichen Texten in jedem Batch konstruiert wurden. Dies garantiert, dass eine signifikante Anzahl von \"nicht √ºbereinstimmenden\" Paaren tats√§chlich korrekte √úbereinstimmungen f√ºreinander sind.</li></ul><p>Wir trainierten dann zwei Modelle f√ºr eine Epoche, eines mit jedem Trainingsdatensatz, und ma√üen den durchschnittlichen Kosinus-Abstand zwischen 1.000 Text-Bild-Paaren im Flickr8k-Datensatz f√ºr jedes Modell. Das mit zuf√§lligen Batches trainierte Modell hatte einen durchschnittlichen Kosinus-Abstand von 0,7521, w√§hrend das mit vielen absichtlich √ºbereinstimmenden \"nicht √ºbereinstimmenden\" Paaren trainierte einen durchschnittlichen Kosinus-Abstand von 0,7840 hatte. Der Effekt der falschen \"nicht √ºbereinstimmenden\" Paare ist ziemlich signifikant. Angesichts dessen, dass das reale Modelltraining viel l√§nger dauert und viel mehr Daten verwendet, k√∂nnen wir sehen, wie dieser Effekt wachsen und die L√ºcke zwischen Bildern und Texten als Ganzes vergr√∂√üern w√ºrde.</p><h2 id=\"the-medium-is-the-message\">Das Medium ist die Botschaft</h2><p>Der kanadische Kommunikationstheoretiker <a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Marshall McLuhan</a> pr√§gte 1964 in seinem Buch <a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\"><em>Understanding Media: The Extensions of Man</em></a> den Ausdruck \"Das Medium ist die Botschaft\", um zu betonen, dass Botschaften nicht autonom sind. Sie erreichen uns in einem Kontext, der ihre Bedeutung stark beeinflusst, und er behauptete bekanntlich, dass einer der wichtigsten Teile dieses Kontexts die Art des Kommunikationsmediums ist.</p><p>Die Multimodalit√§tsl√ºcke bietet uns eine einzigartige Gelegenheit, eine Klasse emergenter semantischer Ph√§nomene in KI-Modellen zu untersuchen. Niemand hat Jina CLIP beigebracht, das Medium der Trainingsdaten zu codieren - es tat es einfach trotzdem. Auch wenn wir das Problem f√ºr multimodale Modelle noch nicht gel√∂st haben, haben wir zumindest ein gutes theoretisches Verst√§ndnis davon, woher das Problem kommt.</p><p>Wir sollten davon ausgehen, dass unsere Modelle aufgrund der gleichen Art von Bias andere Dinge codieren, nach denen wir noch nicht gesucht haben. Zum Beispiel haben wir wahrscheinlich das gleiche Problem bei mehrsprachigen Embedding-Modellen. Das gemeinsame Training mit zwei oder mehr Sprachen f√ºhrt vermutlich zur gleichen L√ºcke zwischen den Sprachen, besonders da √§hnliche Trainingsmethoden weit verbreitet sind. L√∂sungen f√ºr das L√ºckenproblem k√∂nnten sehr weitreichende Auswirkungen haben.</p><p>Eine Untersuchung des Initialisierungs-Bias in einer breiteren Palette von Modellen wird wahrscheinlich auch zu neuen Erkenntnissen f√ºhren. Wenn das Medium f√ºr ein Embedding-Modell die Botschaft ist, wer wei√ü, was sonst noch ohne unser Wissen in unsere Modelle codiert wird?</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "Man kann ein CLIP-Modell nicht einfach zur Abfrage von Text und Bildern verwenden und die Ergebnisse nach Punktzahl sortieren. Warum? Wegen der Modalit√§tsl√ºcke. Was ist das und woher kommt sie?",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
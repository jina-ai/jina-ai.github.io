{
  "slug": "next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker",
  "id": "65fabb91502fd000011c667e",
  "uuid": "45cd5187-838d-46b7-a8a0-d890fcda9041",
  "title": "Cloud AI der nächsten Generation: Jina Embeddings und Reranker auf Amazon SageMaker",
  "html": "<p><a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> und <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io\">Jina Reranker</a> sind jetzt über den <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a> mit <a href=\"https://aws.amazon.com/pm/sagemaker/?ref=jina-ai-gmbh.ghost.io\">Amazon SageMaker</a> verfügbar. Für Unternehmensanwender, die großen Wert auf Sicherheit, Zuverlässigkeit und Konsistenz in ihren Cloud-Operationen legen, bringt dies Jina AIs modernste KI in ihre privaten AWS-Deployments, wo sie alle Vorteile von AWS's etablierter, stabiler Infrastruktur genießen können.</p><p>Mit unserer vollständigen Palette an Embedding- und Reranking-Modellen im AWS Marketplace können SageMaker-Nutzer von bahnbrechenden 8k Input-Kontextfenstern und erstklassigen mehrsprachigen Embeddings on demand zu wettbewerbsfähigen Preisen profitieren. Sie müssen keine Gebühren für den Transfer von Modellen in oder aus AWS zahlen, die Preise sind transparent und Ihre Abrechnung ist in Ihr AWS-Konto integriert.</p><p>Die derzeit auf Amazon SageMaker verfügbaren Modelle umfassen:</p><ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6w6k6ckusixpw?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Small - English</a></li><li>Jina Embeddings v2 Zweisprachige Modelle:<ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-dz3ubvmivnwry?ref=jina-ai-gmbh.ghost.io\">Deutsch/Englisch</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-hxalozh37jka4?ref=jina-ai-gmbh.ghost.io\">Chinesisch/Englisch</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-rnb324fpie3n6?ref=jina-ai-gmbh.ghost.io\">Spanisch/Englisch</a></li></ul></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-tk7t7bz6fp5ng?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - Code</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\">Jina Reranker v1 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6kxbf5xqrluf4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT v1 - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-mgomngrh4c4k4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT Reranker v1 - English</a></li></ul><p>Die vollständige Liste der Modelle finden Sie auf <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">Jina AIs Anbieterseite im AWS Marketplace</a>, und nutzen Sie eine siebentägige kostenlose Testversion.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Dieser Artikel führt Sie durch die Erstellung einer <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\">Retrieval-augmented Generation</a> (RAG) Anwendung ausschließlich mit Komponenten von Amazon SageMaker. Die Modelle, die wir verwenden werden, sind <strong>Jina Embeddings v2 - English</strong>, <strong>Jina Reranker v1</strong> und das <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\">Mistral-7B-Instruct</a> Large Language Model.</p><p>Sie können auch einem Python Notebook folgen, das Sie <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">herunterladen</a> oder <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">auf Google Colab ausführen</a> können.</p><h2 id=\"retrieval-augmented-generation\">Retrieval-Augmented Generation</h2><p>Retrieval-augmented Generation ist ein alternatives Paradigma in der generativen KI. Anstatt Large Language Models (LLMs) direkt Benutzeranfragen mit dem während des Trainings Gelernten beantworten zu lassen, nutzt es deren flüssige Sprachproduktion, während die Logik und Informationsgewinnung in einen dafür besser geeigneten externen Apparat verlagert wird.</p><p>Bevor ein LLM aufgerufen wird, rufen RAG-Systeme aktiv relevante Informationen aus einer externen Datenquelle ab und speisen diese als Teil des Prompts in das LLM ein. Die Rolle des LLM besteht darin, externe Informationen zu einer kohärenten Antwort auf Benutzeranfragen zu synthetisieren, wodurch das Risiko von Halluzinationen minimiert und die Relevanz und Nützlichkeit des Ergebnisses erhöht wird.</p><p>Ein RAG-System hat schematisch mindestens vier Komponenten:</p><ul><li>Eine Datenquelle, typischerweise eine Vektor-Datenbank, die für KI-gestützte Informationsgewinnung geeignet ist.</li><li>Ein Informationsgewinnungssystem, das die Benutzeranfrage als Query behandelt und relevante Daten zur Beantwortung abruft.</li><li>Ein System, oft mit einem KI-basierten Reranker, das einige der abgerufenen Daten auswählt und zu einem Prompt für ein LLM verarbeitet.</li><li>Ein LLM, zum Beispiel eines der GPT-Modelle oder ein Open-Source-LLM wie das von Mistral, das die Benutzeranfrage und die bereitgestellten Daten aufnimmt und eine Antwort für den Benutzer generiert.</li></ul><p>Embedding-Modelle eignen sich gut für die Informationsgewinnung und werden häufig dafür eingesetzt. Ein Text-Embedding-Modell nimmt Texte als Eingabe und gibt ein <a href=\"https://jina.ai/news/how-embeddings-drive-ai-a-guide?ref=jina-ai-gmbh.ghost.io\">Embedding</a> aus – einen hochdimensionalen Vektor –, dessen räumliche Beziehung zu anderen Embeddings ihre semantische Ähnlichkeit anzeigt, d.h. ähnliche Themen, Inhalte und verwandte Bedeutungen. Sie werden oft in der Informationsgewinnung eingesetzt, weil die Wahrscheinlichkeit, dass der Benutzer mit der Antwort zufrieden ist, mit der Nähe der Embeddings steigt. Sie lassen sich auch relativ einfach fine-tunen, um ihre Leistung in bestimmten Domänen zu verbessern.</p><p><a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Text-Reranker</a>-Modelle verwenden ähnliche KI-Prinzipien, um Textsammlungen mit einer Anfrage zu vergleichen und sie nach ihrer semantischen Ähnlichkeit zu sortieren. Die Verwendung eines aufgabenspezifischen Reranker-Modells anstelle eines reinen Embedding-Modells erhöht oft die Präzision der Suchergebnisse dramatisch. Der Reranker in einer RAG-Anwendung wählt einige der Ergebnisse der Informationsgewinnung aus, um die Wahrscheinlichkeit zu maximieren, dass die richtigen Informationen im Prompt für das LLM enthalten sind.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class=\"kg-bookmark-description\">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png\" alt=\"\"></div></a></figure><h2 id=\"benchmarking-performance-of-embedding-models-as-sagemaker-endpoints\"><strong>Leistungsbenchmarking von Embedding-Modellen als SageMaker Endpoints</strong></h2><p>Wir haben die Leistung und Zuverlässigkeit des <strong>Jina Embeddings v2 Base - English</strong> Modells als SageMaker Endpoint getestet, der auf einer <a href=\"https://aws.amazon.com/ec2/instance-types/g4/?ref=jina-ai-gmbh.ghost.io\">g4dn.xlarge</a> Instanz läuft. In diesen Experimenten haben wir kontinuierlich jede Sekunde einen neuen Benutzer hinzugefügt, der jeweils eine Anfrage sendet, auf die Antwort wartet und nach Erhalt der Antwort wiederholt.</p><ul><li>Bei Anfragen von <em>weniger als 100 Token</em> blieben die Antwortzeiten <em>pro Anfrage</em> für bis zu 150 gleichzeitige Benutzer unter 100ms. Danach stiegen die Antwortzeiten linear von 100ms auf 1500ms mit der Zunahme gleichzeitiger Benutzer.<ul><li>Bei etwa <em>300 gleichzeitigen Benutzern</em> erhielten wir mehr als 5 Fehler von der API und beendeten den Test.</li></ul></li><li>Bei Anfragen zwischen 1K und 8K Token blieben die Antwortzeiten <em>pro Anfrage</em> für bis zu 20 gleichzeitige Benutzer unter 8s. Danach stiegen die Antwortzeiten linear von 8s auf 60s mit der Zunahme gleichzeitiger Benutzer.<ul><li>Bei etwa <em>140 gleichzeitigen Benutzern</em> erhielten wir mehr als 5 Fehler von der API und beendeten den Test.</li></ul></li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png\" class=\"kg-image\" alt=\"Four comparative graphs displaying &quot;Small Context&quot; versus &quot;Large Context&quot; results over time, assessing performance metrics.\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/03/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Leistung während der Testläufe (links: kleiner Kontext, rechts: großer Kontext), zeigt den Einfluss zunehmender Benutzer auf Antwortzeiten und Fehlerraten über die Zeit. </span></figcaption></figure><p>Basierend auf diesen Ergebnissen können wir schlussfolgern, dass für die meisten Benutzer mit normaler Embedding-Arbeitslast g4dn.xlarge oder g5.xlarge Instanzen ihren täglichen Bedarf decken sollten. Für große <em>Indexierungs</em>-Aufgaben, die typischerweise viel seltener ausgeführt werden als <em>Such</em>-Aufgaben, könnten Benutzer jedoch eine leistungsfähigere Option bevorzugen. Eine Liste aller verfügbaren Sagemaker-Instanzen finden Sie in der AWS-Übersicht von <a href=\"https://aws.amazon.com/ec2/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">EC2</a>.</p><h2 id=\"configure-your-aws-account\">Konfigurieren Sie Ihr AWS-Konto</h2><p>Zunächst benötigen Sie ein AWS-Konto. Wenn Sie noch kein AWS-Benutzer sind, können Sie sich auf der AWS-Website für ein Konto <a href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\">registrieren</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Console - Signup</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://portal.aws.amazon.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Signup</span></div></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Sie können dieses Tutorial nicht mit einem Free Tier-Konto durchführen, da Amazon keinen kostenlosen Zugang zu SageMaker anbietet. Sie müssen dem Konto eine Zahlungsmethode hinzufügen, um Jina AI's Modelle zu abonnieren, auch wenn Sie <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">unsere siebentägige kostenlose Testversion</a> nutzen.</div></div><h3 id=\"set-up-aws-tools-in-your-python-environment\">Einrichten der AWS-Tools in Ihrer Python-Umgebung</h3><p>Installieren Sie in Ihrer Python-Umgebung die für dieses Tutorial benötigten AWS-Tools und Bibliotheken:</p><pre><code class=\"language-bash\">pip install awscli jina-sagemaker\n</code></pre><p>Sie benötigen einen Zugriffsschlüssel und einen geheimen Zugriffsschlüssel für Ihr AWS-Konto. Folgen Sie dazu den <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\">Anweisungen auf der AWS-Website</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Managing access keys for IAM users - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Create, modify, view, or update access keys (credentials) for programmatic calls to AWS.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/security-credentials-user.shared.console.png\" alt=\"\"></div></a></figure><p>Sie müssen auch eine <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\">AWS-Region</a> auswählen, in der Sie arbeiten möchten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Regions, Availability Zones, and Local Zones - Amazon Relational Database Service</div><div class=\"kg-bookmark-description\">Learn how Amazon cloud computing resources are hosted in multiple locations world-wide, including AWS Regions and Availability Zones.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Relational Database Service</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/Con-AZ-Local.png\" alt=\"\"></div></a></figure><p>Setzen Sie dann die Werte in Umgebungsvariablen. In Python oder in einem Python-Notebook können Sie das mit folgendem Code tun:</p><pre><code class=\"language-bash\">import os\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = &lt;YOUR_ACCESS_KEY_ID&gt;\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = &lt;YOUR_SECRET_ACCESS_KEY&gt;\nos.environ[\"AWS_DEFAULT_REGION\"] = &lt;YOUR_AWS_REGION&gt;\nos.environ[\"AWS_DEFAULT_OUTPUT\"] = \"json\"\n</code></pre><p>Setzen Sie die Standardausgabe auf <code>json</code>.</p><p>Sie können dies auch über die <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html?ref=jina-ai-gmbh.ghost.io\">AWS-Kommandozeilenanwendung</a> oder durch Einrichten einer <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html?ref=jina-ai-gmbh.ghost.io\">AWS-Konfigurationsdatei</a> auf Ihrem lokalen Dateisystem tun. Weitere Details finden Sie in der <a href=\"https://docs.aws.amazon.com/index.html?ref=jina-ai-gmbh.ghost.io\">Dokumentation auf der AWS-Website</a>.</p><h3 id=\"create-a-role\">Erstellen Sie eine Rolle</h3><p>Sie benötigen auch eine <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\">AWS-Rolle</a> mit ausreichenden Berechtigungen für die in diesem Tutorial benötigten Ressourcen.</p><p>Diese Rolle muss:</p><ol><li><strong>AmazonSageMakerFullAccess</strong> aktiviert haben.</li><li>Entweder:<ol><li>Berechtigung haben, AWS Marketplace-Abonnements zu erstellen und alle drei der folgenden Berechtigungen aktiviert haben:<ol><li><strong>aws-marketplace:ViewSubscriptions</strong></li><li><strong>aws-marketplace:Unsubscribe</strong></li><li><strong>aws-marketplace:Subscribe</strong></li></ol></li><li>Oder Ihr AWS-Konto hat ein Abonnement für <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">jina-embedding-model</a>.</li></ol></li></ol><p>Speichern Sie den ARN (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html?ref=jina-ai-gmbh.ghost.io\">Amazon Resource Name</a>) der Rolle in der Variablen <code>role</code>:</p><pre><code class=\"language-python\">role = &lt;YOUR_ROLE_ARN&gt;\n</code></pre><p>Weitere Informationen finden Sie in der Dokumentation für Rollen auf der AWS-Website.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">IAM roles - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Learn how and when to use IAM roles.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div></a></figure><h3 id=\"subscribe-to-jina-ai-models-on-aws-marketplace\">Abonnieren Sie Jina AI-Modelle im AWS Marketplace</h3><p>In diesem Artikel werden wir das Jina Embeddings v2 base English Modell verwenden. Abonnieren Sie es im <a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Embeddings v2 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Wenn Sie nach unten scrollen, sehen Sie die Preisinformationen. AWS berechnet Modelle aus dem Marketplace stundenweise, sodass Ihnen die Zeit von der Aktivierung bis zur Deaktivierung des Modell-Endpunkts in Rechnung gestellt wird. Dieser Artikel zeigt Ihnen, wie Sie beides durchführen.</p><p>Wir werden auch das Jina Reranker v1 - English Modell verwenden, das Sie ebenfalls abonnieren müssen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Reranker v1 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">Jina AI bietet derzeit eine siebentägige kostenlose Testversion seiner Modelle an. Sie müssen weiterhin für die AWS-Instanzen bezahlen, auf denen sie laufen, aber während der Testphase müssen Sie nicht zusätzlich für die Modelle zahlen.</div></div><p>Wenn Sie diese abonniert haben, holen Sie sich die ARNs der Modelle für Ihre AWS-Region und speichern Sie sie in den Variablennamen <code>embedding_package_arn</code> und <code>reranker_package_arn</code>. Der Code in diesem Tutorial wird sie unter diesen Variablennamen referenzieren.</p><p>Wenn Sie nicht wissen, wie Sie die ARNs erhalten, setzen Sie Ihren Amazon-Regionsnamen in die Variable <code>region</code> und verwenden Sie den folgenden Code:</p><pre><code class=\"language-python\">region = os.environ[\"AWS_DEFAULT_REGION\"]\n\ndef get_arn_for_model(region_name, model_name):\n    model_package_map = {\n        \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n        \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n        \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n        \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n        \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n        \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n        \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n        \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n        \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n        \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n        \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n        \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n        \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n        \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n        \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n        \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n    }\n\n    return model_package_map[region_name]\n\nembedding_package_arn = get_arn_for_model(region, \"jina-embeddings-v2-base-en\")\nreranker_package_arn = get_arn_for_model(region, \"jina-reranker-v1-base-en\")\n</code></pre><h2 id=\"load-the-dataset\">Datensatz laden</h2><p>In diesem Tutorial werden wir eine Sammlung von Videos des YouTube-Kanals <a href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\">TU Delft Online Learning</a> verwenden. Dieser Kanal produziert verschiedene Bildungsmaterialien in MINT-Fächern. Seine Inhalte sind <a href=\"https://creativecommons.org/licenses/by/3.0/legalcode?ref=jina-ai-gmbh.ghost.io\">CC-BY lizenziert</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TU Delft Online Learning</div><div class=\"kg-bookmark-description\">Are you looking to make your career in science, design or engineering? Then join the community of online learners at TU Delft!\nAt TU Delft, online learning means active learning. Our courses are designed to provide you with an engaging learning experience. Course content is challenging and demanding, promoting your personal growth and professional development, while enjoying the flexibility and accessibility that our online courses offers so you can combine learning with other priorities of your life. Start learning today: https://online-learning.tud…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/s/desktop/4feff1e2/img/favicon_144x144.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://yt3.googleusercontent.com/ytc/AIdro_kH5d18Xqqj-MKv9k_tf2KNFufCpMY8qEXdQzEy=s900-c-k-c0x00ffffff-no-rj\" alt=\"\"></div></a></figure><p>Wir haben 193 Videos des Kanals heruntergeladen und mit OpenAIs Open-Source <a href=\"https://openai.com/research/whisper?ref=jina-ai-gmbh.ghost.io\">Whisper Spracherkennungsmodell</a> verarbeitet. Wir haben das kleinste Modell <a href=\"https://huggingface.co/openai/whisper-tiny?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>openai/whisper-tiny</code></a> verwendet, um die Videos in Transkripte umzuwandeln.</p><p>Die Transkripte wurden in einer CSV-Datei organisiert, die Sie <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/tu_delft.csv?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">hier herunterladen können</a>.</p><p>Jede Zeile der Datei enthält:</p><ul><li>Den Video-Titel</li><li>Die Video-URL auf YouTube</li><li>Ein Texttranskript des Videos</li></ul><p>Um diese Daten in Python zu laden, installieren Sie zunächst <code>pandas</code> und <code>requests</code>:</p><pre><code class=\"language-bash\">pip install requests pandas\n</code></pre><p>Laden Sie die CSV-Daten direkt in einen Pandas DataFrame namens <code>tu_delft_dataframe</code>:</p><pre><code class=\"language-python\">import pandas\n\n# Load the CSV file\ntu_delft_dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/jina-ai/workshops/feat-sagemaker-post/notebooks/embeddings/sagemaker/tu_delft.csv\")\n</code></pre><p>Sie können den Inhalt mit der <code>head()</code>-Methode des DataFrames überprüfen. In einem Notebook sollte es etwa so aussehen:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png\" class=\"kg-image\" alt=\"Data frame showcasing webinar titles like &quot;Green Teams in Hospitals,&quot; with their YouTube URLs and introductory text excerpts,\" loading=\"lazy\" width=\"1440\" height=\"580\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-15-at-14.30.35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1440w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Sie können auch die Videos über die URLs in diesem Datensatz ansehen und überprüfen, dass die Spracherkennung zwar nicht perfekt, aber grundsätzlich gut funktioniert.</p><h2 id=\"start-the-jina-embeddings-v2-endpoint\">Starten des Jina Embeddings v2 Endpunkts</h2><p>Der folgende Code wird eine Instanz von <code>ml.g4dn.xlarge</code> auf AWS starten, um das Embedding-Modell auszuführen. Dies kann einige Minuten dauern.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Choose a name for your embedding endpoint. It can be anything convenient.\nembeddings_endpoint_name = \"jina_embedding\"\n\nembedding_client = Client(region_name=boto3.Session().region_name)\nembedding_client.create_endpoint(\n    arn=embedding_package_arn,\n    role=role,\n    endpoint_name=embeddings_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nembedding_client.connect_to_endpoint(endpoint_name=embeddings_endpoint_name)\n</code></pre><p>Ändern Sie den <code>instance_type</code>, um bei Bedarf einen anderen AWS Cloud-Instanztyp auszuwählen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">AWS wird Ihnen die Zeit in Rechnung stellen, sobald dieser Befehl zurückkehrt. Die Abrechnung erfolgt stündlich, bis Sie diese Instanz stoppen. Folgen Sie dazu den Anweisungen im Abschnitt <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Herunterfahren</strong></b></a>.</div></div><h2 id=\"build-and-index-the-dataset\">Datensatz erstellen und indexieren</h2><p>Nachdem wir die Daten geladen haben und ein Jina Embeddings v2 Modell ausführen, können wir die Daten vorbereiten und indexieren. Wir werden die Daten in einem <a href=\"https://faiss.ai/index.html?ref=jina-ai-gmbh.ghost.io\">FAISS Vector Store</a> speichern, einer Open-Source-Vektordatenbank, die speziell für KI-Anwendungen entwickelt wurde.</p><p>Installieren Sie zunächst die restlichen Voraussetzungen für unsere RAG-Anwendung:</p><pre><code class=\"language-bash\">pip install tdqm numpy faiss-cpu\n</code></pre><h3 id=\"chunking\">Chunking</h3><p>Wir müssen die einzelnen Transkripte in kleinere Teile, sogenannte \"Chunks\", aufteilen, damit wir mehrere Texte in einen Prompt für das LLM einfügen können. Der folgende Code teilt die einzelnen Transkripte an Satzgrenzen auf und stellt standardmäßig sicher, dass alle Chunks nicht mehr als 128 Wörter enthalten.</p><pre><code class=\"language-python\">def chunk_text(text, max_words=128):\n    \"\"\"\n    Divide text into chunks where each chunk contains the maximum number \n    of full sentences with fewer words than `max_words`.\n    \"\"\"\n    sentences = text.split(\".\")\n    chunk = []\n    word_count = 0\n\n    for sentence in sentences:\n        sentence = sentence.strip(\".\")\n        if not sentence:\n          continue\n\n        words_in_sentence = len(sentence.split())\n        if word_count + words_in_sentence &lt;= max_words:\n            chunk.append(sentence)\n            word_count += words_in_sentence\n        else:\n            # Yield the current chunk and start a new one\n            if chunk:\n              yield \". \".join(chunk).strip() + \".\"\n            chunk = [sentence]\n            word_count = words_in_sentence\n\n    # Yield the last chunk if it's not empty\n    if chunk:\n        yield \" \".join(chunk).strip() + \".\"</code></pre><h3 id=\"get-embeddings-for-each-chunk\">Embeddings für jeden Chunk generieren</h3><p>Wir benötigen ein Embedding für jeden Chunk, um ihn in der FAISS-Datenbank zu speichern. Um diese zu erhalten, übergeben wir die Text-Chunks an den Jina AI Embedding-Modell-Endpunkt mittels der Methode <code>embedding_client.embed()</code>. Dann fügen wir die Text-Chunks und Embedding-Vektoren als neue Spalten <code>chunks</code> und <code>embeddings</code> zum pandas DataFrame <code>tu_delft_dataframe</code> hinzu:</p><pre><code class=\"language-python\">import numpy as np\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\ndef generate_embeddings(text_df):\n    chunks = list(chunk_text(text_df[\"Text\"]))\n    embeddings = []\n\n    for i, chunk in enumerate(chunks):\n      response = embedding_client.embed(texts=[chunk])\n      chunk_embedding = response[0][\"embedding\"]\n      embeddings.append(np.array(chunk_embedding))\n\n    text_df[\"chunks\"] = chunks\n    text_df[\"embeddings\"] = embeddings\n    return text_df\n\nprint(\"Embedding text chunks ...\")\ntu_delft_dataframe = generate_embeddings(tu_delft_dataframe)\n## wenn Sie Google Colab oder ein Python Notebook verwenden, können Sie\n## die obige Zeile löschen und stattdessen die folgende Zeile auskommentieren:\n# tu_delft_dataframe = tu_delft_dataframe.progress_apply(generate_embeddings, axis=1)\n</code></pre><h3 id=\"set-up-semantic-search-using-faiss\">Semantische Suche mit Faiss einrichten</h3><p>Der folgende Code erstellt eine FAISS-Datenbank und fügt die Chunks und Embedding-Vektoren durch Iteration über <code>tu_delft_pandas</code> ein:</p><pre><code class=\"language-python\">import faiss\n\ndim = 768  # Dimension der Jina v2 Embeddings\nindex_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\nk = 0\n\ndoc_ref = dict()\n\nfor idx, row in tu_delft_dataframe.iterrows():\n    embeddings = row[\"embeddings\"]\n    for i, embedding in enumerate(embeddings):\n        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype=\"float32\").reshape(1, -1))\n        faiss.normalize_L2(normalized_embedding)\n        index_with_ids.add_with_ids(normalized_embedding, k)\n        doc_ref[k] = (row[\"chunks\"][i], idx)\n        k += 1\n</code></pre><h2 id=\"start-the-jina-reranker-v1-endpoint\">Starten des Jina Reranker v1 Endpunkts</h2><p>Wie beim Jina Embedding v2 Modell oben wird dieser Code eine Instanz von <code>ml.g4dn.xlarge</code> auf AWS starten, um das Reranker-Modell auszuführen. Die Ausführung kann ebenfalls mehrere Minuten dauern.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Wählen Sie einen Namen für Ihren Reranker-Endpunkt. Es kann ein beliebiger Name sein.\nreranker_endpoint_name = \"jina_reranker\"\n\nreranker_client = Client(region_name=boto3.Session().region_name)\nreranker_client.create_endpoint(\n    arn=reranker_package_arn,\n    role=role,\n    endpoint_name=reranker_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nreranker_client.connect_to_endpoint(endpoint_name=reranker_endpoint_name)\n</code></pre><h2 id=\"define-query-functions\">Abfragefunktionen definieren</h2><p>Als Nächstes definieren wir eine Funktion, die die ähnlichsten Transkript-Chunks zu einer beliebigen Textabfrage identifiziert.</p><p>Dies ist ein zweistufiger Prozess:</p><ol><li>Konvertieren der Benutzereingabe in einen Embedding-Vektor mittels der Methode <code>embedding_client.embed()</code>, genau wie in der Vorbereitungsphase.</li><li>Übergeben des Embeddings an den FAISS-Index, um die besten Übereinstimmungen zu erhalten. In der folgenden Funktion werden standardmäßig die 20 besten Übereinstimmungen zurückgegeben, dies kann aber über den Parameter <code>n</code> gesteuert werden.</li></ol><p>Die Funktion <code>find_most_similar_transcript_segment</code> gibt die besten Übereinstimmungen zurück, indem sie die Kosinus-Ähnlichkeit der gespeicherten Embeddings mit dem Abfrage-Embedding vergleicht.</p><pre><code class=\"language-python\">def find_most_similar_transcript_segment(query, n=20):\n    query_embedding = embedding_client.embed(texts=[query])[0][\"embedding\"]  # Nimmt an, dass die Abfrage kurz genug ist und kein Chunking benötigt\n    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype=\"float32\").reshape(1, -1))\n    faiss.normalize_L2(query_embedding)\n\n    D, I = index_with_ids.search(query_embedding, n)  # Die besten n Übereinstimmungen erhalten\n\n    results = []\n    for i in range(n):\n        distance = D[0][i]\n        index_id = I[0][i]\n        transcript_segment, doc_idx = doc_ref[index_id]\n        results.append((transcript_segment, doc_idx, distance))\n\n    # Ergebnisse nach Distanz sortieren\n    results.sort(key=lambda x: x[2])\n\n    return [(tu_delft_dataframe.iloc[r[1]][\"Title\"].strip(), r[0]) for r in results]\n</code></pre><p>Wir definieren auch eine Funktion, die auf den Reranker-Endpunkt <code>reranker_client</code> zugreift, ihm die Ergebnisse von <code>find_most_similar_transcript_segment</code> übergibt und nur die drei relevantesten Ergebnisse zurückgibt. Sie ruft den Reranker-Endpunkt mit der Methode <code>reranker_client.rerank()</code> auf.</p><pre><code class=\"language-python\">def rerank_results(query_found, query, n=3):\n    ret = reranker_client.rerank(\n        documents=[f[1] for f in query_found], \n        query=query, \n        top_n=n,\n    )\n    return [query_found[r['index']] for r in ret[0]['results']]\n</code></pre><h2 id=\"use-jumpstart-to-load-mistral-instruct\">Mistral-Instruct mit JumpStart laden</h2><p>Für dieses Tutorial verwenden wir das <code>mistral-7b-instruct</code> Modell, das <a href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\">über Amazon SageMaker JumpStart verfügbar ist</a>, als LLM-Teil des RAG-Systems.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mistral 7B foundation models from Mistral AI are now available in Amazon SageMaker JumpStart | Amazon Web Services</div><div class=\"kg-bookmark-description\">Today, we are excited to announce that the Mistral 7B foundation models, developed by Mistral AI, are available for customers through Amazon SageMaker JumpStart to deploy with one click for running inference. With 7 billion parameters, Mistral 7B can be easily customized and quickly deployed. You can try out this model with SageMaker JumpStart, a […]</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://a0.awsstatic.com/main/images/site/touch-icon-ipad-144-smile.png\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Web Services</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/10/09/mistral-7b-sagemaker-jumpstart.jpg\" alt=\"\"></div></a></figure><p>Führen Sie den folgenden Code aus, um Mistral-Instruct zu laden und zu deployen:</p><pre><code class=\"language-python\">from sagemaker.jumpstart.model import JumpStartModel\n\njumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\nmodel_predictor = jumpstart_model.deploy()\n</code></pre><p>Der Endpunkt für den Zugriff auf dieses LLM wird in der Variable <code>model_predictor</code> gespeichert.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Die Nutzung dieses Modells ist ebenfalls ein kostenpflichtiger AWS-Service. Vergessen Sie daher nicht, es nach Abschluss dieses Tutorials abzuschalten. Siehe Abschnitt <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Herunterfahren</strong></b></a>, um dieses Deployment zu beenden.</div></div><h3 id=\"mistral-instruct-with-jumpstart\">Mistral-Instruct mit JumpStart</h3><p>Nachfolgend der Code zur Erstellung einer Prompt-Vorlage für Mistral-Instruct für diese Anwendung unter Verwendung der <a href=\"https://docs.python.org/3/library/string.html?ref=jina-ai-gmbh.ghost.io#template-strings\">eingebauten Template-String-Klasse von Python</a>. Es wird davon ausgegangen, dass für jede Abfrage drei passende Transkript-Chunks dem Modell präsentiert werden.</p><p>Sie können mit dieser Vorlage experimentieren, um die Anwendung zu modifizieren oder zu sehen, ob Sie bessere Ergebnisse erzielen können.</p><pre><code class=\"language-python\">from string import Template\n\nprompt_template = Template(\"\"\"\n  &lt;s&gt;[INST] Beantworte die untenstehende Frage nur anhand des gegebenen Kontexts.\n  Die Frage des Benutzers basiert auf Transkripten von Videos eines YouTube-Kanals.\n  Der Kontext wird als gerankte Liste von Informationen in der Form\n    (Video-Titel, Transkript-Segment) präsentiert, die für die Beantwortung\n    der Benutzerfrage relevant sind.\n  Die Antwort sollte nur den präsentierten Kontext verwenden. Wenn die Frage\n    anhand des Kontexts nicht beantwortet werden kann, sage dies.\n\n  Kontext:\n  1. Video-Titel: $title_1, Transkript-Segment: $segment_1\n  2. Video-Titel: $title_2, Transkript-Segment: $segment_2\n  3. Video-Titel: $title_3, Transkript-Segment: $segment_3\n\n  Frage: $question\n\n  Antwort: [/INST]\n\"\"\")\n</code></pre><p>Mit dieser Komponente haben wir nun alle Teile einer vollständigen RAG-Anwendung.</p><h2 id=\"querying-the-model\">Abfragen des Modells</h2><p>Das Abfragen des Modells ist ein dreistufiger Prozess.</p><ol><li>Suche nach relevanten Chunks für eine Abfrage.</li><li>Zusammenstellung des Prompts.</li><li>Senden des Prompts an das Mistral-Instruct-Modell und Rückgabe seiner Antwort.</li></ol><p>Um nach relevanten Chunks zu suchen, verwenden wir die oben definierte Funktion <code>find_most_similar_transcript_segment</code>.</p><pre><code class=\"language-python\">question = \"Wann wurde der erste Offshore-Windpark in Betrieb genommen?\"\nsearch_results = find_most_similar_transcript_segment(question)\nreranked_results = rerank_results(search_results, question)\n</code></pre><p>Sie können die Suchergebnisse in neu sortierter Reihenfolge inspizieren:</p><pre><code class=\"language-python\">for title, text, _ in reranked_results:\n    print(title + \"\\n\" + text + \"\\n\")\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">Offshore Wind Farm Technology - Course Introduction\nSince the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n\nOffshore Wind Farm Technology - Course Introduction\nI'm a researcher and lecturer at the Wind Energy and Economics Department and I will be your moderator throughout this course.  That means I will answer any questions you may have.  I'll strengthen the interactions between the participants and also I'll get you in touch with the lecturers when needed.  The course is mainly developed for professionals in the field of offshore wind energy.  We want to broaden their knowledge of the relevant technical disciplines and their integration.  Professionals with a scientific background who are new to the field of offshore wind energy will benefit from a high-level insight into the engineering aspects of wind energy.  Overall, the course will help you make the right choices during the development and operation of offshore wind farms.\n\nOffshore Wind Farm Technology - Course Introduction\nDesigned wind turbines that better withstand wind, wave and current loads  Identify great integration strategies for offshore wind turbines and gain understanding of the operational and maintenance of offshore wind turbines and farms  We also hope that you will benefit from the course and from interaction with other learners who share your interest in wind energy  And therefore we look forward to meeting you online.\n</code></pre><p>Wir können diese Informationen direkt in der Prompt-Vorlage verwenden:</p><pre><code class=\"language-python\">prompt_for_llm = prompt_template.substitute(\n    question = question,\n    title_1 = search_results[0][0],\n    segment_1 = search_results[0][1],\n    title_2 = search_results[1][0],\n    segment_2 = search_results[1][1],\n    title_3 = search_results[2][0],\n    segment_3 = search_results[2][1],\n)\n</code></pre><p>Geben Sie die resultierende Zeichenkette aus, um zu sehen, welcher Prompt tatsächlich an das LLM gesendet wird:</p><pre><code class=\"language-python\">print(prompt_for_llm)\n</code></pre><pre><code class=\"language-text\">&lt;s&gt;[INST] Answer the question below only using the given context.\n  The question from the user is based on transcripts of videos from a YouTube\n    channel.\n  The context is presented as a ranked list of information in the form of\n    (video-title, transcript-segment), that is relevant for answering the\n    user's question.\n  The answer should only use the presented context. If the question cannot be\n    answered based on the context, say so.\n\n  Context:\n  1. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n  2. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: For example, we look at how to characterize the wind and wave conditions at a given location.  How to best place the wind turbines in a farm and also how to retrieve the electricity back to shore.  We look at the main design drivers for offshore wind turbines and their components.  We'll see how these aspects influence one another and the best choices to reduce the cost of energy.  This course is organized by the two-delfd wind energy institute, an interfaculty research organization focusing specifically on wind energy.  You will therefore benefit from the expertise of the lecturers in three different faculties of the university.  Aerospace engineering, civil engineering and electrical engineering.  Hi, my name is Ricardo Pareda.\n  3. Video-title: Systems Analysis for Problem Structuring part 1B the mono actor perspective example, transcript-segment: So let's assume the demarcation of the problem and the analysis of objectives has led to the identification of three criteria.  The security of supply, the percentage of offshore power generation and the costs of energy provision.  We now reason backwards to explore what factors have an influence on these system outcomes.  Really, the offshore percentage is positively influenced by the installed Wind Power capacity at sea, a key system factor.  Capacity at sea in turn is determined by both the size and the number of wind farms at sea.  The Ministry of Economic Affairs cannot itself invest in new wind farms but hopes to simulate investors and energy companies by providing subsidies and by expediting the granting process of licenses as needed.\n\n  Question: When was the first offshore wind farm commissioned?\n\n  Answer: [/INST]\n</code></pre><p>Übergeben Sie diesen Prompt an den LLM-Endpunkt — <code>model_predictor</code> — über die Methode <code>model_predictor.predict()</code>:</p><pre><code class=\"language-python\">answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n</code></pre><p>Dies gibt eine Liste zurück, aber da wir nur einen Prompt übergeben haben, wird es eine Liste mit einem Eintrag sein. Jeder Eintrag ist ein <code>dict</code> mit dem Antworttext unter dem Schlüssel <code>generated_text</code>:</p><pre><code class=\"language-python\">answer = answer[0]['generated_text']\nprint(answer)\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">The first offshore wind farm was commissioned in 1991. (Context: Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, ...)\n</code></pre><p>Vereinfachen wir die Abfrage, indem wir eine Funktion schreiben, die alle Schritte ausführt: Sie nimmt die String-Frage als Parameter und gibt die Antwort als String zurück:</p><pre><code class=\"language-python\">def ask_rag(question):\n    search_results = find_most_similar_transcript_segment(question)\n    reranked_results = rerank_results(search_results, question)\n    prompt_for_llm = prompt_template.substitute(\n        question = question,\n        title_1 = search_results[0][0],\n        segment_1 = search_results[0][1],\n        title_2 = search_results[1][0],\n        segment_2 = search_results[1][1],\n        title_3 = search_results[2][0],\n        segment_3 = search_results[2][1],\n    )\n    answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n    return answer[0][\"generated_text\"]\n</code></pre><p>Jetzt können wir einige weitere Fragen stellen. Die Antworten hängen vom Inhalt der Videotranskripte ab. Zum Beispiel können wir detaillierte Fragen stellen, wenn die Antwort in den Daten vorhanden ist, und eine Antwort erhalten:</p><pre><code class=\"language-python\">ask_rag(\"What is a Kaplan Meyer estimator?\")\n</code></pre><pre><code class=\"language-text\">The Kaplan Meyer estimator is a non-parametric estimator for the survival \nfunction, defined for both censored and not censored data. It is represented \nas a series of declining horizontal steps that approaches the truths of the \nsurvival function if the sample size is sufficiently large enough. The value \nof the empirical survival function obtained is assumed to be constant between \ntwo successive distinct observations.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"Who is Reneville Solingen?\")\n</code></pre><pre><code class=\"language-text\">Reneville Solingen is a professor at Delft University of Technology in Global \nSoftware Engineering. She is also a co-author of the book \"The Power of Scrum.\"\n</code></pre><pre><code class=\"language-python\">answer = ask_rag(\"What is the European Green Deal?\")\nprint(answer)\n</code></pre><pre><code class=\"language-text\">The European Green Deal is a policy initiative by the European Union to combat \nclimate change and decarbonize the economy, with a goal to make Europe carbon \nneutral by 2050. It involves the use of green procurement strategies in various \nsectors, including healthcare, to reduce carbon emissions and promote corporate \nsocial responsibility.\n</code></pre><p>Wir können auch Fragen stellen, die außerhalb des Umfangs der verfügbaren Informationen liegen:</p><pre><code class=\"language-python\">ask_rag(\"What countries export the most coffee?\")\n</code></pre><pre><code class=\"language-text\">Based on the context provided, there is no clear answer to the user's \nquestion about which countries export the most coffee as the context \nonly discusses the Delft University's cafeteria discounts and sustainable \ncoffee options, as well as lithium production and alternatives for use in \nelectric car batteries.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\")\n</code></pre><pre><code class=\"language-text\">The context does not provide sufficient information to answer the question. \nThe context is about thermit welding of rails, stress concentration factors, \nand a lyrics video. There is no mention of woodchucks or the ability of \nwoodchuck to chuck wood in the context.\n</code></pre><p>Probieren Sie Ihre eigenen Abfragen aus. Sie können auch die Art und Weise ändern, wie das LLM aufgefordert wird, um zu sehen, ob das Ihre Ergebnisse verbessert.</p><h2 id=\"shutting-down\">Herunterfahren</h2><p>Da Sie stundenweise für die verwendeten Modelle und die AWS-Infrastruktur bezahlen, ist es sehr wichtig, alle drei AI-Modelle zu stoppen, wenn Sie dieses Tutorial beenden:</p><ul><li>Den Embedding-Modell-Endpunkt <code>embedding_client</code></li><li>Den Reranker-Modell-Endpunkt <code>reranker_client</code></li><li>Den Large Language Model-Endpunkt <code>model_predictor</code></li></ul><p>Um alle drei Modell-Endpunkte herunterzufahren, führen Sie den folgenden Code aus:</p><pre><code class=\"language-python\"># shut down the embedding endpoint\nembedding_client.delete_endpoint()\nembedding_client.close()\n# shut down the reranker endpoint\nreranker_client.delete_endpoint()\nreranker_client.close()\n# shut down the LLM endpoint\nmodel_predictor.delete_model()\nmodel_predictor.delete_endpoint()\n</code></pre><h2 id=\"get-started-now-with-jina-ai-models-on-aws-marketplace\">Starten Sie jetzt mit Jina AI Modellen auf dem AWS Marketplace</h2><p>Mit unseren Embedding- und Reranking-Modellen auf SageMaker haben Enterprise-AI-Nutzer auf AWS jetzt sofortigen Zugriff auf Jina AIs herausragendes Wertversprechen, ohne die Vorteile ihrer bestehenden Cloud-Operationen zu gefährden. Die gesamte Sicherheit, Zuverlässigkeit, Konsistenz und vorhersehbare Preisgestaltung von AWS ist integriert.</p><p>Bei Jina AI arbeiten wir hart daran, den neuesten Stand der Technik zu Unternehmen zu bringen, die von der Integration von KI in ihre bestehenden Prozesse profitieren können. Wir streben danach, solide, zuverlässige und leistungsstarke Modelle zu zugänglichen Preisen über bequeme und praktische Schnittstellen anzubieten und dabei Ihre Investitionen in KI zu minimieren und Ihre Rendite zu maximieren.</p><p>Besuchen Sie <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">Jina AIs AWS Marketplace-Seite</a> für eine Liste aller Embedding- und Reranker-Modelle, die wir anbieten, und um unsere Modelle sieben Tage kostenlos zu testen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div>Wir würden gerne von Ihren Anwendungsfällen erfahren und darüber sprechen, wie die Produkte von Jina AI zu Ihren geschäftlichen Anforderungen passen können. Kontaktieren Sie uns über <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\">unsere Website</a> oder unseren <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\">Discord-Kanal</a>, um Ihr Feedback zu teilen und über unsere neuesten Modelle auf dem Laufenden zu bleiben.",
  "comment_id": "65fabb91502fd000011c667e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/03/Blog-images--27-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-03-20T11:33:53.000+01:00",
  "updated_at": "2024-03-25T19:10:29.000+01:00",
  "published_at": "2024-03-25T16:00:51.000+01:00",
  "custom_excerpt": "Learn to use Jina Embeddings and Reranking models in a full-stack AI application on AWS, using only components available in Amazon SageMaker and the AWS Marketplace.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker/",
  "excerpt": "Lernen Sie, wie Sie Jina Embeddings und Reranking-Modelle in einer Full-Stack KI-Anwendung auf AWS einsetzen können, unter ausschließlicher Verwendung von Komponenten aus Amazon SageMaker und dem AWS Marketplace.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract image with colorful wavy background featuring AWS, Embeddings, and Reranker logos.",
  "feature_image_caption": null
}
{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "Was können wir von ModernBERT lernen?",
  "html": "<p>Im Jahr 2018 veröffentlichte Google BERT, was ein Game-Changer für NLP war, lange vor der aktuellen LLM-Welle. Auch heute noch basieren viele Small Language Models auf BERT. Im Dezember 2024 nimmt <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> die Erkenntnisse aus den jüngsten LLM-Entwicklungen und wendet sie auf diese kleineren Modelle an. Die wichtigsten Änderungen? Bessere Parameter-Effizienz, Code-Verständnis und Verarbeitung langer Kontexte.</p><p>In diesem Beitrag vergleichen wir ModernBERT mit zwei Modellen, die wir in- und auswendig kennen: <code>jina-XLM-RoBERTa</code> (das mehrsprachige Rückgrat hinter <code>jina-embeddings-v3</code>) und <code>RoBERTa-large</code>. Schauen wir uns jedes Modell an:</p><ul><li><strong>ModernBERT</strong> (Dez. 2024) ist ein kürzlich veröffentlichtes SLM, das in Zusammenarbeit von Answer.AI, LightOn und HuggingFace entwickelt wurde. Es nutzt moderne Optimierungen wie RoPE für ein 8.192-Token-Kontextfenster und <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">GeGLU-Layer</a>, was die Leistung bei gleichbleibender Effizienz steigert.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong> (Sept. 2024) ist ein mehrsprachiges Text-Embedding-Modell basierend auf Metas <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a>. Während das ursprüngliche <code>XLM-RoBERTa</code> <code>RoBERTa</code> mithilfe des XLM-Multilingual-Datensatzes erweitert, geht <code>jina-XLM-RoBERTa</code> noch weiter mit erweitertem Kontext-Training, <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a>-Implementierung und <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a>-Unterstützung. Dieses Modell dient als Grundlage für <code>jina-embeddings-v3</code>.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a> (Juli 2019), entwickelt von Meta, ist eine verbesserte Version von BERT mit 355 Millionen Parametern. Durch erweitertes Training, größere Datensätze und Innovationen wie dynamisches Masking hat es beeindruckende Ergebnisse bei wichtigen Benchmarks wie <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a> und <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a> erzielt. Dies macht es gut geeignet für verschiedene NLP-Aufgaben von Textklassifizierung bis hin zu Frage-Antwort-Systemen.</li></ul><p>Durch den Vergleich dieser Modelle in drei Kernaspekten möchten wir ModernBERTs effektive Designentscheidungen für andere Modellentwickler hervorheben und wichtige Entwicklungserkenntnisse für zukünftige BERT-ähnliche Modelle identifizieren. Wir werden auch unsere Erkenntnisse aus der Entwicklung von <code>jina-embeddings-v3</code> teilen und geplante Verbesserungen für <code>jina-embeddings-v4</code> und <code>jina-reranker-v3</code> diskutieren.</p><h2 id=\"modernberts-parameter-efficiency\">ModernBERTs Parameter-Effizienz</h2><p>Betrachten wir zunächst ModernBERTs Ansatz zur Parameter-Effizienz - es bringt mehrere wichtige Erkenntnisse aus jüngsten LLM-Entwicklungen mit. ModernBERT nutzt drei Kernstrategien: eine tiefere aber dünnere Architektur, kontrollierte Vokabulargröße und progressives Model-Upscaling, beginnend mit kleineren Modellen.</p><h3 id=\"deep-and-thin-architecture\">Deep-And-Thin-Architektur</h3><p>ModernBERT-large geht mit 28 Layern tiefer, während <code>jina-XLM-RoBERTa</code> und <code>RoBERTa-large</code> bei 24 bleiben. Aber hier kommt das Interessante - es entspricht <code>RoBERTa-large</code> in der Parameteranzahl trotz dieser zusätzlichen Layer. <code>jina-XLM-RoBERTa</code> benötigt mehr Parameter, da es 89 Sprachen verarbeitet, während sich die anderen beiden nur auf Englisch konzentrieren.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">Die Tiefe (Anzahl der Layer) ist wichtiger als die Breite (Anzahl der Hidden Units) für kleine LLMs. Eine deep-and-thin Modellstruktur überzeugt beim Erfassen abstrakter Konzepte und führt zu besserer Endleistung.</span></figcaption></figure><p>Die meisten Parameter eines Transformers kommen aus Attention- und Fully-Connected-Layern. ModernBERT bleibt größenmäßig wettbewerbsfähig, indem es \"dünner\" wird - sie verwenden 2.624 Hidden Units über 28 Layer, verglichen mit RoBERTa-larges 4.096 Units über 24 Layer. Dieser \"tiefere\" aber dünnere Aufbau ermöglicht es ihnen, ihre Leistungsziele zu erreichen, ohne das Modell aufzublähen.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parameters</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Hidden states</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Intermediate dims</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Attention heads</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Vocabulary size</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Dieser Ansatz deckt sich mit Metas <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a>-Forschung, die herausfand, dass bei kleineren Modellen die Tiefe wichtiger ist als die Breite, wenn es um das Erfassen komplexer Muster und die Leistungssteigerung geht. Im Wesentlichen erweist sich die Fähigkeit, Informationen durch mehr Transformer-Layer zu verarbeiten, als wertvoller als breitere Layer für die parallele Verarbeitung.</p><p>Schauen wir uns die Daten zur Leistung dieser Deep-and-Thin-Architektur an.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">Im Vergleich zu ähnlichen Modellen mit der traditionellen Shallow-Fat-Architektur liefert ModernBERT bessere Ergebnisse bei wichtigen Aufgaben wie Retrieval und STS - und das bei ähnlicher Parameteranzahl.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Average</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Nehmen wir <code>jina-XLM-RoBERTa</code> - es baut auf <code>RoBERTa-large</code>s Shallow-Fat-Architektur auf, erhöht aber das Vokabular von 50K auf 250K Tokens und trainiert mit mehr Daten. Dennoch übertrifft ModernBERT es leicht, was darauf hindeutet, dass die architektonische Veränderung einen echten Unterschied in der Effizienz macht.</p><h3 id=\"vocabulary-size-matters\">Vokabulargröße ist wichtig</h3><p>Schauen wir uns zunächst an, wie Vokabular-Parameter in Transformern gezählt werden. Für jeden Transformer gilt: <code>Vokabular-Parameter = Anzahl unterschiedlicher Tokens × Hidden Size</code>. Nehmen wir <code>jina-XLM-RoBERTa</code>: mit 250K Tokens und 1.024 Dimensionen benötigt es 256M Parameter allein für die Vokabular-Kodierung - bevor überhaupt Sprachaufgaben verarbeitet werden!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">In Transformern ordnet die erste Schicht Token mithilfe einer Gewichtsmatrix, nämlich den Vokabulargewichten, versteckten Zuständen zu. Betrachtet man alle UTF-8-Codepunkte (1.112.064) mit 1.024 versteckten Dimensionen, würde man massive </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> Parameter allein für die Token-Konvertierung benötigen. Während größere LLMs (100B+ Parameter) diesen Overhead bewältigen können, ist dies für kleinere Modelle eine ernsthafte Einschränkung. Genau deshalb verwenden wir Tokenizer wie BPE, die häufig vorkommende UTF-8-Codepunkte effizient zu einzelnen Token zusammenfassen.</span></figcaption></figure><p>Aber hier ist der springende Punkt: <strong>Vokabulargewichte tragen nicht zu Attention-Mechanismen bei – sie sind nur Nachschlagetabellen.</strong> Für SLMs mit festem Parameterbudget bedeutet ein größeres Vokabular weniger verfügbare Parameter für Attention-Layer, die die eigentliche Sprachverarbeitung durchführen. Dies erklärt, warum das rein englischsprachige ModernBERT-large trotz geringerer Größe die mehrsprachige <code>jina-XLM-RoBERTa</code> übertrifft – <code>jina-XLM-RoBERTa</code> weist mehr Parameter (47 %!) zur Unterstützung mehrerer Sprachen zu. ModernBERTs fokussiertes Vokabular verbessert nicht nur die Leistung, sondern beschleunigt auch die Inferenz, was es besonders effektiv für ressourcenbeschränkte Anwendungen macht.</p><p>Wenn wir uns also <em>nur</em> die Kern-Modellparameter ansehen (ohne Vokabulargewichte), packt ModernBERT tatsächlich mehr Rechenleistung als seine Mitbewerber: ModernBERT widmet 19 % mehr Parameter der <em>eigentlichen</em> Sprachmodellierung als <code>jina-XLM-RoBERTa</code> und 15 % mehr als <code>RoBERTa-large</code>!</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Modell-Spezifikationen</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sprachunterstützung</td>\n<td>Nur Englisch</td>\n<td>89 Sprachen</td>\n<td>Nur Englisch</td>\n</tr>\n<tr>\n<td>Vokabulargröße</td>\n<td>50,4K</td>\n<td>250K</td>\n<td>50,3K</td>\n</tr>\n<tr>\n<td>Gesamtparameter</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Vokabularparameter</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>Vokabularparameter-Verhältnis</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>Kernmodellparameter</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">Modell-Upscaling durch \"Weight Tiling\"</h3><p>Beim Aufbau des <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a> Backbones stellten wir fest, dass das Training von SLMs von Grund auf ressourcenintensiv und komplex war. ModernBERT löst dies mit einem intelligenten Initialisierungsansatz namens <strong>Weight Tiling</strong> – im Wesentlichen wird ModernBERT-large aus den Gewichten seiner kleineren Basisversion hochskaliert.</p><p>Diese Technik ist nicht völlig neu – sie baut auf DeepMinds Arbeit mit <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> auf und taucht auch in Microsofts <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2 Modellen</a> auf. Ihre Anwendung hier ist jedoch besonders effektiv bei der Bewältigung des SLM-Trainingsengpasses.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT skaliert von 22 auf 28 Schichten unter Verwendung der Tiefeninitialisierungsstrategie des Gopher-Teams. Für diese zusätzlichen Schichten (23-28) initialisieren sie jede mit Gewichten aus den ursprünglichen 22 Schichten von ModernBERT-base. Für die Gewichtsmatrizen jeder Schicht verwenden sie Phi-2's Center-Tiling-Ansatz. Es funktioniert so: Sie nehmen die ModernBERT-base Gewichte und platzieren sie direkt in der Mitte der ModernBERT-large Matrizen. Für die noch leeren Ränder? Sie wickeln die ursprünglichen Gewichte zyklisch um sie zu füllen.</span></figcaption></figure><p>Diese Initialisierungsstrategie verschafft ModernBERT-large einen signifikanten Vorteil – anstatt von Grund auf neu zu beginnen, nutzt es vorgelernter Muster seines kleineren Pendants. Dies hat sich als besonders <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">effektiv für die Skalierung von Sprachmodellen in diesem Größenbereich</a> erwiesen.</p><blockquote>Wir stellen fest, dass ein warm gestartetes Modell sich schnell von einem anfänglich hohen Verlust (aufgrund der hinzugefügten Parameter) erholt und einen Verlust erreicht, der dem des Basismodells sehr nahe kommt. Wir können 417M Parameter um mehr als das 3-fache in der Größe erweitern und eine Leistung aufrechterhalten, die besser ist als ein vergleichbares frisches Modell, das von Grund auf bis zur Konvergenz trainiert wurde, was darauf hindeutet, dass die Gewinne nicht auf den Trainingsbeginn beschränkt waren. Bei größeren Dimensionen nehmen die relativen Gewinne bei der Konvergenz jedoch ab, insbesondere bei Erweiterungen in der Breite.</blockquote><p>Das zyklische Gewichtsumwickeln ist nicht nur eine Bequemlichkeit – es passt gut zu der Art und Weise, wie Attention-Matrizen natürlich periodische Muster aufweisen. Gophers Forschung zeigt, dass dieser Ansatz besonders bei SLMs (unter 9B Parameter) glänzt, wobei die Vorteile bei größeren Modellen allmählich nachlassen.</p><h2 id=\"modernberts-code-modeling\">ModernBERTs Code-Modellierung</h2><p>ModernBERT bringt mit seinem code-optimierten Tokenizer und Trainingsdaten einen spezialisierten Ansatz zum Codeverständnis. Diese Feinabstimmung für die Codeverarbeitung zahlt sich sowohl bei Verständnis- als auch bei Abrufaufgaben aus.</p><p>Wir führten einen Benchmark mit dem <code>jina-embeddings-v2-code</code> Corpus durch und verglichen drei Modelle als Backbones: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code> und <code>RoBERTa-large</code>. Der Test? <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> – Zuordnung von Textbeschreibungen zu Code-Snippets. ModernBERT übertraf beide Alternativen in allen Bereichen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">Die Lücke ergibt Sinn – weder </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> noch </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> sahen während des Trainings Programmiersprachen. Währenddessen wurde ModernBERT-large mit zwei Billionen Token trainiert, einschließlich einer beträchtlichen Menge an Code. Diese Exposition gegenüber Programmiersyntax und -mustern verschafft ihm einen klaren Vorteil bei codebezogenen Aufgaben. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> übertrifft </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> leicht, wahrscheinlich aufgrund seiner größeren mehrsprachigen Trainingsdaten – gleiche Architektur, mehr Exposition. Dennoch liegen beide deutlich hinter ModernBERT-large zurück.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Aufgabe</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">Der Tokenizer-Vorteil</h3><p>Schauen wir uns an, warum ModernBERT Code so gut verarbeitet - es verwendet den <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">OLMo Tokenizer</a>, der speziell für Code trainiert wurde, anstelle der Standard-BERT/RoBERTa-Tokenizer.</p><p>Ein Tokenizer zerlegt UTF-8-Text in Tokens, die in Vektoren umgewandelt werden - diese verarbeitet das Modell tatsächlich. Während des Trainings lernt er, häufig vorkommende Zeichenfolgen zu einzelnen Tokens zu kombinieren. Der Unterschied? Ein Standard-Tokenizer könnte <code>init</code> in <code>in</code> + <code>it</code> aufteilen und dabei den Programmierkontext verpassen. Aber ModernBERTs codebewusster Tokenizer versteht es ohne Aufteilung.</p><p>Interessant wird es bei der Leerzeichenbehandlung: ModernBERT bewahrt Pythons führende Leerzeichen als einzelne Tokens und unterscheidet zwischen 4 und 8 Leerzeichen - entscheidend für die Code-Struktur. Währenddessen <strong>reduziert <code>jina-XLM-RoBERTa</code> alle aufeinanderfolgenden Leerzeichen auf ein einzelnes <code>_</code>, und RoBERTa-large behandelt jedes Leerzeichen als eigenes Token.</strong> Das bedeutet, dass ModernBERTs Encoder sauberere, aussagekräftigere Eingaben bei der Code-Verarbeitung erhält, während die anderen mit fragmentierten, weniger kohärenten Tokens arbeiten.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT bewahrt Pythons führende Leerzeichen als einzelne Tokens und unterscheidet zwischen 4 und 8 Leerzeichen - entscheidend für die Code-Struktur; während die anderen mit fragmentierten, weniger kohärenten Tokens arbeiten.</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">ModernBERTs Verarbeitung langer Kontexte</h2><p>ModernBERT hat bei der Verarbeitung langer Texte bedeutende Fortschritte gemacht, dank seines umfangreichen Trainingskorpus (300 Mrd. Tokens mit 8.192-Token-Proben) und fortgeschrittener Techniken wie kombinierter globaler und lokaler Aufmerksamkeit.</p><p>Zur Bewertung der Fähigkeiten zur Verarbeitung langer Dokumente verwendeten wir den <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">MLDR-Datensatz</a> - ein umfassender Langtext-Benchmark für 13 Sprachen. Da ModernBERT derzeit nur Englisch unterstützt, konzentrierten wir uns auf MLDRs englische Teilmenge, um ModernBERT mit <code>jina-XLM-RoBERTa</code> zu vergleichen. Während beide Modelle 8K-Token-Eingaben verarbeiten können, wurde <code>RoBERTa-large</code> aufgrund seiner 512-Token-Beschränkung, die für Langtextanalysen unzureichend ist, von diesem Benchmark ausgeschlossen.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>ModernBERTs überlegene Leistung ist nicht nur auf sein umfangreiches Langtext-Training zurückzuführen - es liegt größtenteils an seiner innovativen Kombination von globalen und lokalen Aufmerksamkeitsmechanismen. Im Gegensatz zu <code>jina-XLM-RoBERTa</code>, das rechenintensive globale Aufmerksamkeit auf jede Schicht anwendet, verfolgt ModernBERT einen effizienteren Ansatz. Es wechselt zwischen globaler Aufmerksamkeit (verwendet in jeder dritten Schicht mit einem <code>theta</code> von 160.000) und lokaler Aufmerksamkeit (verwendet ein 128-Token-Schiebefenster mit einem <code>theta</code> von 100.000). Diese Hybridstrategie erhält hohe Leistung bei drastisch reduzierter Trainingszeit.</p><blockquote>In ModernBERT verwendet jede dritte Schicht globale Aufmerksamkeit mit einem RoPE-Theta von 160.000 und die übrigen Schichten verwenden ein lokales 128-Token-Schiebefenster-Aufmerksamkeit mit einem RoPE-Theta von 10.000. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">Die bittere Lektion?</h2><p>Das Skalierungsgesetz und <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">die bittere Lektion</a> legen nahe, dass wesentliche Leistungsverbesserungen hauptsächlich durch die Erhöhung der Parameteranzahl und Trainingsdaten entstehen. Dieses Prinzip leitete unseren Ansatz, das Korpus zu erweitern und LoRA für aufgabenspezifische Anpassungen zu verwenden.</p><p>Allerdings hat ModernBERTs Erfolg gezeigt, dass wir die Kraft der architektonischen Optimierung unterschätzt haben. Es zeigt, dass SLMs durch bessere Daten-Modell-Effizienz außergewöhnliche Ergebnisse erzielen können, ohne unbedingt die Parameter zu skalieren. Ein kürzlicher<a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\"> Stella Embeddings technischer Bericht</a> bestätigt diese Erkenntnis und zeigt, dass aktuelle Embedding-Modell-Trainingsmethoden ohne Vergrößerung von Korpus oder Modellgröße verbessert werden können.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Skalierungsgesetz von Embedding-Modellen. Die durchschnittliche MTEB-Leistung bei englischen Aufgaben ist gegen die Anzahl der Modellparameter aufgetragen. Jeder Punkt repräsentiert ein Embedding-Modell. Die Trendlinie, die alle Modelle darstellt, ist hervorgehoben, wobei mehrsprachige Modelle in Cyan betont sind. Man kann sehen, dass </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> eine überlegene Leistung im Vergleich zu Modellen ähnlicher Größe zeigt und auch eine überlineare Verbesserung gegenüber seinem Vorgänger </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\"> aufweist. Diese Grafik wurde erstellt durch Auswahl der Top-100 Embedding-Modelle aus der MTEB-Rangliste, unter Ausschluss derjenigen ohne Größeninformationen, typischerweise geschlossene oder proprietäre Modelle. Einreichungen, die als offensichtliches Trolling identifiziert wurden, wurden ebenfalls herausgefiltert. </span></figcaption></figure><p>In Zukunft erwarten wir geringere Rechenkosten und kleinere Modellgrößen, da wir tiefere Einblicke in die Datennutzung gewinnen und ModernBERTs Techniken implementieren. Kurzfristig können wir einfache Verbesserungen aus dem ModernBERT-Paper umsetzen - insbesondere die Integration von mehr codebezogenen Daten und die Adoption eines codefreundlichen Tokenizers. Komplexere Änderungen, wie der Wechsel zu einer deep-and-thin-Architektur oder das Bootstrapping großer Modelle aus kleineren, erfordern den Aufbau von Backbone-Modellen von Grund auf - eine mittelfristige Initiative.</p><p>Während ModernBERTs Effizienz bemerkenswert ist, weist seine Beschränkung auf Text auf zukünftige Herausforderungen hin. Mit der zunehmenden Popularität multimodaler Embedding-Modelle besteht unsere nächste Herausforderung darin, intelligentere, schnellere und leistungsfähigere Such-Grundlagenmodelle zu entwickeln, die Eingaben für multimodale Anwendungen verarbeiten können. Diese Anwendungen erfordern noch längere Kontextfenster - eine Effizienzherausforderung, die es noch zu lösen gilt.</p><h2 id=\"conclusion\">Fazit</h2><p>In diesem Beitrag haben wir untersucht, wie ModernBERT BERT-Familie-Modelle durch drei Schlüsselinnovationen voranbringt: seine deep-and-thin-Architektur, optimierter Tokenizer und effiziente Skalierung mittels Weight Tiling. Diese Verbesserungen ermöglichen es ModernBERT, herausragende Leistung in einer relativ kompakten Größe zu liefern und dabei sowohl <code>RoBERTa-large</code> als auch <code>jina-XLM-RoBERTa</code> in verschiedenen Aufgaben zu übertreffen. ModernBERT zeigt, dass architektonische Verbesserungen wichtiger sein können als die Parametergröße und öffnet damit Türen für effizientere Modelle. Seine erfolgreiche Nutzung von Weight Tiling zeigt, wie progressives Skalieren die Trainingskosten reduzieren und gleichzeitig die Leistung erhalten oder sogar steigern kann. Zusätzlich deuten sein kompaktes Vokabular und gezielte Optimierungen auf wachsende Möglichkeiten für spezialisierte SLMs in ressourcenbeschränkten Umgebungen hin.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "Größere Trainingsdaten, effiziente Parameter-Dimensionierung und eine tiefe, aber schlanke Architektur - ModernBERT weist den Weg für zukünftige BERT-ähnliche Modelle.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
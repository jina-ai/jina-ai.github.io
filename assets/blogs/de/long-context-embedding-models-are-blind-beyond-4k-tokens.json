{
  "slug": "long-context-embedding-models-are-blind-beyond-4k-tokens",
  "id": "67c868baf1c5780001164330",
  "uuid": "a9f711ab-651e-4587-8a49-793d15b21380",
  "title": "Langkontext-Embedding-Modelle sind jenseits von 4K Tokens blind",
  "html": "<p>Im Februar 2025 ver√∂ffentlichte ein Team von KI-Forschern das <a href=\"https://arxiv.org/abs/2502.05167\">NoLiMA Paper</a>, das einen neuartigen Benchmark zur Bewertung der F√§higkeit von Large Language Models einf√ºhrt, lange Kontexte zu verarbeiten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.05167\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">NoLiMa: Long-Context Evaluation Beyond Literal Matching</div><div class=\"kg-bookmark-description\">Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-8.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Ali Modarressi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Dieses Paper f√ºhrt eine bedeutende √Ñnderung am traditionellen Needle-in-a-Haystack (NIAH) Benchmark ein, indem es w√∂rtliche √úbereinstimmungen zwischen Fragen und der Nadel (relevante Information) im Heuhaufen (irrelevanter Text) entfernt.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/niah-vs-nolima.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"150\"><figcaption><span style=\"white-space: pre-wrap;\">Zum Beispiel k√∂nnte beim traditionellen NIAH bei der Frage \"In welchem Jahr besuchte John Paris?\" die Nadel direkt \"John besuchte Paris 2019\" enthalten. Bei NOLIMA k√∂nnte die Frage lauten \"Welche Person war in Frankreich?\" w√§hrend die Nadel enth√§lt \"Tats√§chlich lebt Yuki neben der Semperoper\" - was erfordert, dass das Modell wei√ü, dass die Semperoper in Dresden, Deutschland ist, nicht in Frankreich.</span></figcaption></figure><p>Es zeigt eine kritische Einschr√§nkung aktueller LLMs auf: Sie verlassen sich stark auf oberfl√§chliches Pattern Matching, und ihre F√§higkeit zu tiefgehendem assoziativem Denken verschlechtert sich rapide mit zunehmender Kontextl√§nge.</p><p>Basierend auf diesen Erkenntnissen m√∂chten wir untersuchen, ob √§hnliche Leistungsmuster bei Embedding-Modellen auftreten, speziell bei <code>jina-embeddings-v3</code>. Da die Effektivit√§t von RAG-Systemen kritisch von der Qualit√§t der Retrieval-Modelle abh√§ngt, wollen wir NoLiMAs Forschung durch kontrollierte Experimente erweitern, die zwei Kernfragen adressieren:</p><ul><li>Wie gehen Embedding-Modelle mit Needle-in-a-Haystack-Retrieval bei verschiedenen Kontextl√§ngen um, wenn sie gezwungen sind, semantische Spr√ºnge jenseits w√∂rtlicher Keyword-Matches zu machen?</li><li>Kann strategische Query-Augmentation mit semantisch √§hnlichem Content diese Leistungsl√ºcke verringern?</li></ul><p>Der starke Kontrast bei LLMs ‚Äî robust bei lexikalischen Matches aber anf√§llig bei semantischen Variationen ‚Äî deutet darauf hin, dass Embedding-basierte Retrieval-Systeme √§hnliche Herausforderungen haben k√∂nnten, wenn sie √ºber oberfl√§chliches Term Matching hinausgehen, was m√∂glicherweise fundamentale Einschr√§nkungen aktueller Semantic Search Technologien aufzeigt.</p><h2 id=\"needles-and-haystacks-construction\">Konstruktion von Nadeln und Heuhaufen</h2><h3 id=\"needles-construction\">Konstruktion der Nadeln</h3><p>Traditionelle Needle-in-a-Haystack Tests verwenden Nadeln, die die Formulierung der gesuchten Frage widerspiegeln. Zum Beispiel:</p><ul><li>Frage: \"Welche Person war in Dresden?\"</li><li>Nadel: \"Yuki lebt in Dresden.\"</li></ul><p>Aber wie bei NoLiMA wollen wir semantisches Verst√§ndnis statt blo√ües Keyword Matching testen, also erstellen wir Ein-Hop-Variationen (mit W√∂rtern, die speziell nicht in den Dokumenten vorkommen) mit zwei verschiedenen Wortanordnungen:</p><ul><li>Frage: \"Welche Person war in Dresden?\"</li><li>Nadel (Standard): \"Tats√§chlich lebt Yuki neben der Semperoper.\"</li><li>Nadel (invertiert): \"Die Semperoper ist neben dem Ort, wo Yuki lebt.\"</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Die <a href=\"https://en.wikipedia.org/wiki/Semperoper\">Semperoper</a> befindet sich in Dresden und liefert den Kontext f√ºr diese Ein-Hop-Nadel.</div></div><p>Der Methodik des Papers folgend generieren wir diese Nadel-Fragen-Gruppen (bestehend aus einer Frage, <strong>einer Ein-Hop-Nadel</strong> und <strong>einer invertierten Ein-Hop-Nadel</strong>) √ºber mehrere Kategorien hinweg, wie die Beispiele unten zeigen:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Question</th>\n<th>Original needle (for reference)</th>\n<th>One-hop needle</th>\n<th>Inverted one-hop needle</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dietary restrictions</td>\n<td>Which character cannot eat fish-based meals?</td>\n<td>Alice cannot eat fish-based meals.</td>\n<td>Then, Alice mentioned being vegan for years.</td>\n<td>Being vegan was important to Alice for years.</td>\n</tr>\n<tr>\n<td>Medical conditions</td>\n<td>Which character cannot drink milk?</td>\n<td>Bob can't drink milk.</td>\n<td>Bob explained he was lactose intolerant.</td>\n<td>Being lactose intolerant affected Bob daily.</td>\n</tr>\n<tr>\n<td>Language proficiency</td>\n<td>Which character speaks French?</td>\n<td>Charlie speaks French.</td>\n<td>Actually, Charlie studied at the Sorbonne.</td>\n<td>At the Sorbonne, Charlie completed his degree.</td>\n</tr>\n<tr>\n<td>Professional background</td>\n<td>Which character is a musician?</td>\n<td>Diane is a musician.</td>\n<td>In 2013, Diane conducted at the Sydney Opera House.</td>\n<td>The Sydney Opera House performance was conducted by Diane.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Die obigen Namen dienen nur als Referenz. In den tats√§chlichen Nadeln werden sie zuf√§llig aus einer Liste kulturell diverser Namen ausgew√§hlt.<br><br>Beachten Sie, dass die urspr√ºnglichen Nadeln (w√∂rtliche Keyword-Matches) nur als Referenz angegeben sind und nicht in unseren Experimenten verwendet werden.</div></div><h3 id=\"haystacks-construction\">Konstruktion der Heuhaufen</h3><p>Wir begannen mit zehn gemeinfreien B√ºchern, die jeweils mindestens 50.000 Tokens enthielten, und verketteten zuf√§llig kurze Ausschnitte (unter 250 Tokens) zu Heuhaufen verschiedener L√§ngen, n√§mlich 128, 256, 512, 1024, 2048, 4096 und 8192 Tokens. Dann betteten wir eine Nadel in jeden Heuhaufen ein:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"415\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png 896w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Heuhaufen-Konstruktion aus kurzen Buchausschnitten und einer einzelnen Nadel pro Heuhaufen.</span></figcaption></figure><p>Als konkretes Beispiel nehmen wir die Nadel \"Tats√§chlich lebt Yuki neben der Semperoper\" und platzieren sie in einem 128-Token Heuhaufen an Position 50:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"508\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/text2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/text2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 2: Ein Beispiel f√ºr eine Nadel im Heuhaufen.</span></figcaption></figure><p>Bei Verwendung von <code>jina-embeddings-v3</code> zum Einbetten der Texte betr√§gt der √Ñhnlichkeitswert zwischen dem Nadeltext und dem Heuhaufentext:</p><pre><code class=\"language-bash\">Question-Haystack similarity = 0.2391\n</code></pre><p>Wir normalisieren dann den Wert, indem wir diese Zahl durch den √Ñhnlichkeitswert der Frage und der Standard-Nadel teilen (keine Heuhaufen-Erstellung, nur direkter Vergleich):</p><pre><code class=\"language-bash\">Question-Needle similarity = 0.3598\nNormalized Query-Haystack similarity = 0.2391 / 0.3598 = 0.6644\n</code></pre><p>Diese Normalisierung ist notwendig, da nicht alle Modelle die gleichen √Ñhnlichkeitswerte zwischen zwei Texten produzieren, und <code>jina-embeddings-v3</code> neigt dazu, die √Ñhnlichkeit zwischen zwei Texten zu untersch√§tzen.</p><p>F√ºr jede Nadel (einschlie√ülich aller Standard- und invertierten) generierten wir zehn Heuhaufen pro Kontextl√§nge, wobei wir eine Nadel pro Heuhaufen an verschiedenen Positionen einbetteten. F√ºr eine bestimmte Nadel und Kontextl√§nge w√ºrden die Heuhaufen etwa so aussehen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"290\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png 800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 3: Nadeln, die in regelm√§√üigen Abst√§nden in zehn Heuhaufen platziert sind.</span></figcaption></figure><p>Als Kontrolle generierten wir auch einen Heuhaufen f√ºr jede Testbedingung ohne Nadel. Insgesamt sind das 3.234 Heuhaufen. Wir kodierten jeden Heuhaufen mit <code>jina-embeddings-v3</code> (unter Verwendung der Standard-Text-Matching LoRA), dann k√ºrzten wir f√ºr jeden Heuhaufen (falls die Gesamttoken 8.192 √ºberschritten, das Limit f√ºr<code>jina-embeddings-v3</code>) und kodierte dann die entsprechende Frage.</p><h2 id=\"evaluation-metrics\">Evaluierungsmetriken</h2><p>Unser Evaluierungsrahmen verwendet verschiedene Metriken, um die Leistung von Embedding-Modellen √ºber verschiedene Kontextl√§ngen hinweg zu bewerten:</p><h3 id=\"primary-metrics\">Prim√§re Metriken</h3><p><strong>Normalisierter √Ñhnlichkeitswert</strong><br>Die Kernmetrik ist ein normalisierter √Ñhnlichkeitswert, der sowohl die semantische √Ñhnlichkeit zwischen der Frage und dem gesamten Kontext (Frage-Haystack-√Ñhnlichkeit) als auch die Baseline-√Ñhnlichkeit zwischen der Frage und ihrer entsprechenden Standard-Needle (Frage-Needle-√Ñhnlichkeit) ber√ºcksichtigt. Diese Normalisierung stellt sicher, dass die Leistung des Modells relativ zu einem aussagekr√§ftigen Referenzpunkt und nicht nur zu absoluten √Ñhnlichkeitswerten bewertet wird. Der Normalisierungsprozess beinhaltet die Berechnung des direkten Cosinus-√Ñhnlichkeitswertes zwischen Fragen und ihren entsprechenden Needles (unsere Baseline) und die Division der Frage-Haystack-√Ñhnlichkeit durch diesen Baseline-Wert:<br></p><p>$\\text{Normalisierte √Ñhnlichkeit} = \\frac{\\cos{(q,h)}}{\\cos{(q,n)}}$</p><p><strong>Vergleichsverh√§ltnis zum Zufallswert</strong><br>Bei jedem Embedding-Modell sind Cosinus-√Ñhnlichkeitswerte zwischen verschiedenen Query-Dokument-Paaren nur direkt vergleichbar, wenn die Query gleich bleibt. Daher messen wir neben den normalisierten √Ñhnlichkeitswerten auch, wie oft die Frage dem gesamten Haystack √§hnlicher ist als einer zuf√§lligen Passage gleicher L√§nge ohne Needle.</p><h3 id=\"secondary-metrics\">Sekund√§re Metriken</h3><p><strong>Separationsanalyse</strong><br>Diese Metrik bewertet, wie gut das Modell zwischen relevantem und irrelevantem Inhalt unterscheidet. Sie umfasst die <strong>mittlere Separation</strong>, die den Unterschied zwischen positiven Beispielen (Passagen mit der Antwort) und negativen Beispielen (Passagen ohne Antwort) darstellt, und den <strong>AUC-Wert (Area Under the Curve)</strong>, der die Unterscheidungsf√§higkeit basierend auf der Fl√§che unter der ROC-Kurve (Receiver Operating Characteristic) misst.</p><p><strong>Positionseffekte</strong><br>Wir analysieren, wie die Needle-Platzierung die Leistung durch den <strong>Korrelationskoeffizienten</strong> zwischen Position und √Ñhnlichkeitswert, die <strong>Regressionssteigung</strong>, die die Leistungs√§nderung √ºber Positionen zeigt, und die <strong>positionsbasierte Leistungsanalyse</strong> beeinflusst.</p><h2 id=\"findings\">Ergebnisse</h2><h3 id=\"degradation-of-similarity-score-and-correctness\">Verschlechterung des √Ñhnlichkeitswertes und der Korrektheit</h3><p>Unsere Ergebnisse zeigen deutlich, dass die Leistung mit zunehmender Kontextl√§nge abnimmt, wobei der mittlere √Ñhnlichkeitswert von 0,37 bei 128 Token auf 0,10 bei 8K Token sinkt, einem nicht-linearen Trend folgend mit einem starken R√ºckgang zwischen 128 und 1K Token.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 4: Normalisierte Leistung vs. Kontextl√§nge.</span></figcaption></figure><p>In der folgenden Abbildung zeigen wir, dass das Umkehren der Needle kaum Einfluss auf den normalisierten √Ñhnlichkeitswert hat. Sowohl die Standard-Needle (z.B. \"Tats√§chlich wohnt Yuki in der N√§he der Semperoper\") als auch die umgekehrte Needle (z.B. \"Die Semperoper liegt neben dem Ort, wo Yuki wohnt\") zeigen fast identische Leistung:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 5: Standard- vs. umgekehrte Reihenfolge Leistung.</span></figcaption></figure><p>Die verschiedenen semantischen Verbindungen im Datensatz zeigen unterschiedliche Leistungen, wobei Standort-Wahrzeichen-Paare die st√§rksten Ergebnisse beibehalten, w√§hrend Ern√§hrungs- und medizinische Zustandsverbindungen schneller abnehmen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 6: Normalisierte Gruppenleistung vs. Kontextl√§nge.</span></figcaption></figure><p>Der Vergleich der Ergebnisse mit dem Zufall best√§tigt unsere Erkenntnisse, indem er zeigt, dass je gr√∂√üer der Haystack ist, desto mehr n√§hern sich die Ergebnisse der Zuf√§lligkeit an, d.h. wir w√§hlen fast genauso wahrscheinlich eine zuf√§llige Passage ohne Needle (richtige Antwort) wie den Haystack f√ºr eine bestimmte Frage:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-12.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 7: Modellleistung vs. Zufallschance (0,5).</span></figcaption></figure><p>Auch hier sehen wir unterschiedliche Leistungen basierend auf verschiedenen semantischen Verbindungen, wobei einige (wie Ern√§hrungseinschr√§nkungen) selbst bei relativ kurzen Kontexten deutlich unter den Zufallswert fallen, w√§hrend andere (wie Standorte und Wahrzeichen) unabh√§ngig von der Kontextl√§nge eine viel bessere Leistung zeigen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 8: Gruppenleistung vs. Zufallschance.</span></figcaption></figure><p>Das Umkehren der Needle hat wenig Einfluss auf die Leistung. In der folgenden Grafik zeigen wir das Vergleichsverh√§ltnis der Bevorzugung des korrekten Haystacks gegen√ºber dem Zufall, aufgeteilt danach, ob die platzierte Needle die Antwort in Standardreihenfolge oder umgekehrter Reihenfolge enthielt:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 9: Standard- vs. umgekehrte Reihenfolge - Leistung vs. Zufallschance.</span></figcaption></figure><p>Da wir sehen k√∂nnen, dass die Ergebnisse f√ºr Needles in Standard- und umgekehrter Reihenfolge dem gleichen Trend folgen, werden wir die getrennte Analyse bez√ºglich dieses Kriteriums nicht fortsetzen.</p><h3 id=\"can-we-separate-positive-from-negative-results\">K√∂nnen wir positive von negativen Ergebnissen trennen?</h3><p>Eine unserer wichtigsten Erkenntnisse stammt aus der Analyse, wie gut Embedding-Modelle relevante von irrelevanten Inhalten √ºber verschiedene Kontextl√§ngen hinweg unterscheiden k√∂nnen. Diese \"Separationsanalyse\" zeigt, dass die Korrektheit des Retrievals zwischen einer Kontextl√§nge von 128 und 1000 Token rapide abnimmt und dann weiter sinkt, wenn auch langsamer:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 10: Separationsanalyse vs. Kontextl√§nge.</span></figcaption></figure><p>Bei kurzen Kontexten (128 Token) zeigt das Modell eine starke Separation mit einer mittleren Differenz von 0,1 und klarer Unterscheidung, wobei es einen AUC-Wert von 0,81 erreicht (was bedeutet, dass das Modell in 81% der F√§lle eine relevante Passage h√∂her einstuft als eine irrelevante). Dies zeigt, dass das Modell in k√ºrzeren Kontexten zuverl√§ssig zwischen Passagen unterscheiden kann, die die Antwort enthalten, und solchen, die sie nicht enthalten.</p><p>Diese Verschlechterung nimmt jedoch mit zunehmender Kontextl√§nge rapide zu. Bei 1.000 Token sinkt die Trennung um 60% auf 0,040 und die AUC f√§llt auf 0,66, was einen deutlichen Leistungsabfall signalisiert. Bei 8.000 Token gibt es nur noch minimale Trennung (0,001) und eine nahezu zuf√§llige Unterscheidung mit einer AUC von nur 0,50. Dieses Muster offenbart eine entscheidende Erkenntnis: Selbst wenn Modelle in l√§ngeren Kontexten vern√ºnftige √Ñhnlichkeitswerte berechnen k√∂nnen, k√∂nnen sie diese Werte kaum nutzen, um relevante von irrelevanter Information zu unterscheiden. Bei 8.000 Token entspricht die F√§higkeit des Modells, relevante Inhalte zu unterscheiden, im Wesentlichen dem Zufall.</p><p>Die Geschwindigkeit dieser Verschlechterung mit wachsendem Kontext ist auff√§llig. Die reinen √Ñhnlichkeitswerte fallen von 128 auf 8.000 Token um etwa 75%, aber die Trennungsmetriken nehmen im gleichen Zeitraum um fast 99% ab. Noch besorgniserregender ist, dass die Effektst√§rke einen noch steileren R√ºckgang zeigt und um 98,6% f√§llt. Dies deutet darauf hin, dass die Probleme von Embedding-Modellen mit langen Kontexten √ºber reduzierte √Ñhnlichkeitswerte hinausgehen - ihre grundlegende F√§higkeit, relevante Informationen zu identifizieren, bricht weitaus schwerer zusammen als bisher angenommen.</p><h3 id=\"how-does-the-needle-position-affect-the-core-metrics\">Wie beeinflusst die Position der Nadel die Kernmetriken?</h3><p>W√§hrend die Kernleistungsmetriken normalerweise am besten sind, wenn sich die Nadel am Anfang des Heuhaufens befindet, korreliert die Leistungsverschlechterung nicht immer mit der Platzierung in der Mitte des Kontexts:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 11: Leistung nach relativer Position √ºber Kontextl√§ngen.</span></figcaption></figure><p>Wir sehen auch, dass die Leistung am besten ist, wenn sich die Nadel am Anfang eines gegebenen Kontexts befindet, und in kurzen Kontexten sehen wir einen kleinen Leistungsanstieg, wenn die Nadel gegen Ende platziert wird. In allen Kontexten sehen wir jedoch einen Leistungsabfall, wenn sich die Nadel in mittleren Positionen befindet:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 12: Positionsweise Vergleichsverh√§ltnisse.</span></figcaption></figure><h2 id=\"what-effect-does-query-expansion-have-on-the-results\">Welchen Effekt hat Query Expansion auf die Ergebnisse?</h2><p>Wir haben k√ºrzlich einen Blogbeitrag √ºber Query Expansion ver√∂ffentlicht, eine Technik, die in Suchsystemen verwendet wird, um die Suchleistung durch Hinzuf√ºgen relevanter Begriffe zu Abfragen zu verbessern.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Query Expansion with LLMs: Searching Better by Saying More</div><div class=\"kg-bookmark-description\">Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/query-expansion-with-llms-searching-better-by-saying-more.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Im Beitrag verwendeten wir ein LLM zur Generierung von Erweiterungsbegriffen, die dann zu Query Embeddings f√ºr eine verbesserte Retrieval-Leistung hinzugef√ºgt wurden. Die Ergebnisse zeigten signifikante Verbesserungen. Jetzt wollen wir untersuchen, wie (oder ob) die Technik die Ergebnisse f√ºr die Nadel-im-Heuhaufen-Suche verbessert. Zum Beispiel, gegeben eine Abfrage:</p><pre><code class=\"language-bash\">Which character has been to Dresden?\n</code></pre><p>Wir verwenden ein LLM (Gemini 2.0), um sie zu erweitern und 100 zus√§tzliche Begriffe hinzuzuf√ºgen, die so aussehen:</p><pre><code class=\"language-bash\">Which character has been to Dresden? Character: fictional character literary character protagonist antagonist figure persona role dramatis personae\\\\n\\\\nDresden: Dresden Germany; bombing of Dresden World War II historical fiction Kurt Vonnegut Slaughterhouse-Five city in Saxony Elbe River cultural landmark\\\\n\\\\nHas been to: visited traveled to journeyed to presence in appears in features in set in takes place in location setting\n\n</code></pre><h3 id=\"how-much-does-query-expansion-help-match-the-needle-to-the-haystack\">Wie sehr hilft Query Expansion beim Matching der Nadel zum Heuhaufen?</h3><p>F√ºr unser Experiment generierten wir drei Sets von erweiterten Abfragebegriffen (wie im <a href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\">urspr√ºnglichen Beitrag</a> beschrieben) - 100, 150 und 250 Begriffe. Dann f√ºhrten wir die gleichen Experimente wie zuvor durch, jeweils dreimal mit jedem Set erweiterter Abfragebegriffe.</p><p>Die Ergebnisse mit allen Erweiterungssets zeigten eine deutliche Verschlechterung mit zunehmender Kontextl√§nge, mit einem √§hnlichen Effekt wie ohne Query Expansion (Abbildungen 4 & 7):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1071\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 13: Kombinierte normalisierte Leistung: alle Erweiterungsgr√∂√üen.</span></figcaption></figure><p>Im Vergleich zu nicht erweiterten Abfragen zeigten alle Query-Expansion-Bedingungen das gleiche Muster der Leistungsverschlechterung mit wachsendem Kontext. Der Verschlechterungstrend ist auch weiterhin nicht linear mit einem starken R√ºckgang zwischen 128 und 1K Token:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 14: Kombiniertes Vergleichsverh√§ltnis: alle Erweiterungsgr√∂√üen.</span></figcaption></figure><p>Die Untersuchung des Vergleichsverh√§ltnisses zeigt jedoch, dass Query Expansion klare Vorteile hat: Das Modell w√§hlt mit deutlich h√∂herer Wahrscheinlichkeit den Heuhaufen mit der Nadel gegen√ºber dem ohne. Im Gegensatz dazu sank ohne Query Expansion die Wahrscheinlichkeit, die richtige Passage auszuw√§hlen, so stark, dass sie bei einer Heuhaufengr√∂√üe von 8K Token fast der zuf√§lligen Auswahl einer Passage entsprach.</p><h3 id=\"how-do-we-explain-needle-matching-results-with-query-expansion\">Wie erkl√§ren wir die Nadel-Matching-Ergebnisse mit Query Expansion?</h3><p>Diese Ergebnisse stimmen mit den Erkenntnissen sowohl aus dem NoLiMa-Paper als auch der Query-Expansion-Forschung √ºberein und k√∂nnen wie folgt erkl√§rt werden:</p><ol><li><strong>Qualit√§t vs. Quantit√§t Trade-off</strong>: Die bessere Leistung der 100-Term-Erweiterung im Vergleich zu 150 und 250 Begriffen deutet darauf hin, dass es einen optimalen Punkt gibt, an dem zus√§tzliche Begriffe mehr Rauschen als Signal hinzuf√ºgen. Die 250-Term-Erweiterung f√ºhrt wahrscheinlich Begriffe mit schw√§cheren semantischen Beziehungen zur urspr√ºnglichen Abfrage ein, die bei l√§ngeren Kontexten kontraproduktiv werden.</li><li><strong>Kontextl√§nge bleibt die prim√§re Herausforderung</strong>: Trotz der Vorteile der Query Expansion verschlechtert sich die Leistung mit zunehmender Kontextl√§nge weiterhin deutlich. Dies deutet darauf hin, dass auch mit Erweiterung die grundlegende architektonische Einschr√§nkung von aufmerksamkeitsbasierten Modellen in langen Kontexten bestehen bleibt.</li><li><strong>Praktische Schwellenidentifikation</strong>: Das Vergleichsverh√§ltnis, das √ºber 0,5 bleibt, zeigt, dass die Erweiterung auch bei 8K Token eine √ºberzuf√§llige Leistung beibeh√§lt und einen praktischen Weg bietet, das <em>effektive Kontextfenster</em> f√ºr Embedding-Modelle zu erweitern. Der Vergleich mit dem Zufall zeigt, dass selbst bei langen Kontextdokumenten die Erweiterung der Abfrage es wahrscheinlicher macht, die richtige Antwort (d.h. die Nadel) zu finden als eine falsche. Dies ist eine Verbesserung im Vergleich zu nicht erweiterten Abfragen, bei denen sich die Chance, die richtige Antwort zu finden, mit zunehmender Kontextl√§nge dem Zufall n√§hert.</li></ol><h2 id=\"diagnosis-what-role-does-lexical-matching-play-in-embeddings\">Diagnose: Welche Rolle spielt lexikalisches Matching bei Embeddings?</h2><p>In den obigen Experimenten haben wir die Effektivit√§t von Embedding-Modellen bei semantischen \"Ein-Schritt\"-Inferenzen in Passagen mit langem Kontext gemessen, indem wir alle M√∂glichkeiten des w√∂rtlichen Matchings ausgeschlossen haben. Wir stellten fest, dass selbst mit Query Expansion die F√§higkeit des Embedding-Modells, relevante Passagen zu finden, mit wachsender Kontextl√§nge nachl√§sst. Dieser Effekt ist signifikant, und die Erkenntnis ist bemerkenswert, da wir normalerweise erwarten w√ºrden, dass ein Embedding-Modell die relevanten Inferenzen ohne zus√§tzliche Hilfe machen kann. Wenn wir w√∂rtliche √úbereinstimmungen durch Ein-Schritt-Variationen ersetzen (z.B. \"Dresden\" ‚Üí \"Semperoper\"), ersetzen wir lediglich ein Konzept durch ein nahegelegenes.</p><p>Packen wir den Stier bei den H√∂rnern und stellen die Frage direkt: Spielt w√∂rtliches Matching wirklich eine signifikant gro√üe Rolle beim semantischen Matching, oder √ºberwiegt der Effekt der Kontextl√§nge? Um diese Frage zu beantworten, haben wir unsere Tests mit Nadeln wiederholt, die w√∂rtliche √úbereinstimmungen enthalten, z.B.</p><ul><li>Frage: \"Which character has been to Dresden?\"</li><li>Nadel (Standard): \"Actually, Yuki lives in Dresden.\"</li><li>Nadel (invertiert): \"Dresden is where Yuki lives.\"</li></ul><p>Beachten Sie, dass anstelle einer einschrittigen Variation des Schlussfolgerns, dass die Semperoper in Dresden steht und daher eine Figur, die daneben wohnt, diejenige sein m√ºsste, die Dresden besucht hat, diese Needles direkt den Namen der Figur angeben, die in Dresden lebt.</p><p>Nachdem wir alle 22 Frage-Needle-Paare auf diese Weise umformuliert hatten, f√ºhrten wir unsere Experimente mit allen einbezogenen Kontextl√§ngen und Needle-Platzierungen erneut durch, wobei wir dasselbe Embedding-Modell <code>jina-embeddings-v3</code> verwendeten.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-22.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 15: Normalisierte Leistung vs. Kontextl√§nge.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-23.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 16: Modellleistung vs. Zufallschance (0,5).</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-24.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-24.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-24.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 17: Positionsabh√§ngige Vergleichsverh√§ltnisse</span></figcaption></figure><p>Die Ergebnisse sind bemerkenswert. Selbst bei w√∂rtlichen √úbereinstimmungen im Kontext verschlechtert sich die F√§higkeit des Modells, die richtige Antwort von einer zuf√§lligen zu unterscheiden, mit zunehmender Kontextl√§nge rapide, wenn auch mit einem leichten Vorteil gegen√ºber dem v√∂lligen Fehlen einer w√∂rtlichen √úbereinstimmung.</p><p>Dies beweist letztendlich, dass die F√§higkeit eines Embedding-Modells, eine Nadel im Heuhaufen zu finden, viel st√§rker von der Gr√∂√üe des Heuhaufens (und der Platzierung der Nadel darin) beeinflusst wird als von der semantischen Formulierung der Nadel.</p><h2 id=\"conclusion\">Fazit</h2><p>Unsere Erkenntnisse mit Embedding-Modellen stimmen mit dem NoLiMA-Paper √ºber LLMs √ºberein: Die Kontextgr√∂√üe ist hochgradig bestimmend f√ºr korrektes Matching und Retrieval. Wir zeigen, dass dies selbst dann gilt, wenn es eine exakte buchstabengetreue Wort√ºbereinstimmung gibt.</p><p>Das Problem liegt nicht in der F√§higkeit eines Embeddings, semantisches Matching durchzuf√ºhren. Embedding-Modelle wie <code>jina-embeddings-v3</code> handhaben kurze Kontexte recht gut, aber ihre Effektivit√§t nimmt mit zunehmender Kontextl√§nge ab. Query-Expansion kann diesen Effekt bis zu einem gewissen Grad reduzieren, aber die Retrieval-Qualit√§t verschlechtert sich dennoch bei l√§ngeren Kontexten. Dar√ºber hinaus stellt die Query-Expansion zus√§tzliche Probleme dar, da es von entscheidender Bedeutung ist, Erweiterungsbegriffe zu identifizieren, die das Retrieval verbessern, ohne semantisches Rauschen hinzuzuf√ºgen. Wir untersuchen und suchen nach M√∂glichkeiten, das Nadel-im-Heuhaufen-Retrieval direkt anzugehen und die zuk√ºnftige Leistung von <code>jina-embeddings-v4</code> zu verbessern.</p>",
  "comment_id": "67c868baf1c5780001164330",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-05T16:07:38.000+01:00",
  "updated_at": "2025-03-07T03:56:34.000+01:00",
  "published_at": "2025-03-07T03:56:34.000+01:00",
  "custom_excerpt": "We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/long-context-embedding-models-are-blind-beyond-4k-tokens/",
  "excerpt": "Wir untersuchen Embedding-Modelle bei neuen \"Nadel-im-Heuhaufen\"-Aufgaben und stellen fest, dass sie jenseits von 4K Tokens praktisch nur w√ºrfeln - selbst bei exakten lexikalischen √úbereinstimmungen oder Query-Expansion k√∂nnen sie in langen Kontexten nicht zwischen Signal und Rauschen unterscheiden.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
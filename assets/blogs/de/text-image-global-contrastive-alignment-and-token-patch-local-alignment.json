{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "Globale kontrastive Text-Bild-Ausrichtung und lokale Token-Patch-Ausrichtung",
  "html": "<p>Bei Experimenten mit <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali-style</a> Modellen hat einer unserer Entwickler eine Visualisierung mit unserem kürzlich veröffentlichten <code>jina-clip-v2</code> Modell erstellt. Er hat die Ähnlichkeit zwischen Token-Embeddings und Patch-Embeddings für bestimmte Bild-Text-Paare abgebildet und dabei Heatmap-Overlays erstellt, die einige interessante visuelle Erkenntnisse lieferten.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Leider ist <strong>dies nur eine heuristische Visualisierung</strong> - kein expliziter oder garantierter Mechanismus. Während CLIP-ähnliche globale kontrastive Ausrichtung <em>zufällig</em> grobe lokale Ausrichtungen zwischen Patches und Tokens erzeugen kann (und dies oft auch tut), ist dies ein <strong>unbeabsichtigter Nebeneffekt</strong> und kein bewusstes Ziel des Modells. Lassen Sie mich erklären warum.</p><h2 id=\"understand-the-code\">Den Code verstehen</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Lassen Sie uns auf hoher Ebene analysieren, was der Code macht. Beachten Sie, dass <code>jina-clip-v2</code> standardmäßig keine API für den Zugriff auf Token-Level oder Patch-Level Embeddings bereitstellt - diese Visualisierung erforderte einige nachträgliche Anpassungen.</p><p><strong>Word-Level Embeddings berechnen</strong></p><p>Durch Setzen von <code>model.text_model.output_tokens = True</code> gibt <code>text_model(x=...,)[1]</code> ein zweites Element <code>(batch_size, seq_len, embed_dim)</code> für die Token-Embeddings zurück. Es nimmt also einen Eingabesatz, tokenisiert ihn mit dem Jina CLIP Tokenizer und gruppiert dann Subword-Tokens wieder zu \"Wörtern\", indem die entsprechenden Token-Embeddings gemittelt werden. Es erkennt den Beginn eines neuen Wortes durch Prüfung, ob der Token-String mit dem Zeichen <code>_</code> beginnt (typisch für SentencePiece-basierte Tokenizer). Es erzeugt eine Liste von Word-Level Embeddings und eine Liste von Wörtern (sodass \"Hund\" ein Embedding ist, \"und\" ein Embedding ist, usw.).</p><p><strong>Patch-Level Embeddings berechnen</strong></p><p>Für den Image Tower gibt <code>vision_model(..., return_all_features=True)</code> <code>(batch_size, n_patches+1, embed_dim)</code> zurück, wobei das erste Token der <code>[CLS]</code> Token ist. Daraus extrahiert der Code die Embeddings für jeden Patch (d.h. die Patch-Tokens des Vision Transformers). Diese Patch-Embeddings werden dann in ein 2D-Gitter umgeformt, <code>patch_side × patch_side</code>, das dann auf die ursprüngliche Bildauflösung hochskaliert wird.</p><p><strong>Word-Patch Ähnlichkeit visualisieren</strong></p><p>Die Ähnlichkeitsberechnung und die anschließende Heatmap-Generierung sind Standard \"Post-hoc\" Interpretationstechniken: Man wählt ein Text-Embedding, berechnet die Kosinus-Ähnlichkeit mit jedem Patch-Embedding und generiert dann eine Heatmap, die zeigt, welche Patches die höchste Ähnlichkeit zu diesem spezifischen Token-Embedding haben. Schließlich durchläuft es jeden Token im Satz, hebt diesen Token links fett hervor und überlagert die ähnlichkeitsbasierte Heatmap rechts auf dem Originalbild. Alle Frames werden zu einem animierten GIF zusammengefügt.</p><h2 id=\"is-it-meaningful-explainability\">Ist es eine aussagekräftige Erklärbarkeit?</h2><p>Vom <em>reinen Code</em> Standpunkt aus: ja, die Logik ist kohärent und wird eine Heatmap für jeden Token erzeugen. Sie erhalten eine Reihe von Frames, die Patch-Ähnlichkeiten hervorheben, der Code \"macht also, was draufsteht\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Wenn wir uns die obigen Beispiele ansehen, stellen wir fest, dass Wörter wie <code>moon</code> und <code>branches</code> gut mit ihren entsprechenden visuellen Patches im Originalbild übereinzustimmen scheinen. Aber hier ist die entscheidende Frage: Ist dies eine bedeutungsvolle Ausrichtung oder sehen wir nur einen glücklichen Zufall?</p><p>Dies ist eine tiefergehende Frage. Um die Einschränkungen zu verstehen, erinnern wir uns daran, <strong>wie CLIP trainiert wird</strong>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 kombiniert einen Text-Encoder (Jina XLM-RoBERTa, 561M Parameter) und einen Vision-Encoder (EVA02-L14, 304M Parameter). Jedes farbige Quadrat rechts repräsentiert einen vollständigen Satz oder ein vollständiges Bild im Batch - nicht einzelne Tokens oder Patches.</span></figcaption></figure><ul><li>CLIP verwendet eine <strong>globale</strong> kontrastive Ausrichtung zwischen einem gesamten Bild und einem gesamten Text. Während des Trainings erzeugt der Bildencoder einen einzelnen Vektor (gepoolte Repräsentation) und der Textencoder einen weiteren einzelnen Vektor; CLIP wird so trainiert, dass diese für übereinstimmende Text-Bild-Paare übereinstimmen und andernfalls nicht übereinstimmen.</li><li>Es gibt <strong>keine explizite Überwachung auf der Ebene 'Patch X entspricht Token Y'.</strong> Das Modell wird nicht direkt darauf trainiert hervorzuheben \"dieser Bereich des Bildes ist der Hund, dieser Bereich ist die Katze\" usw. Stattdessen wird ihm beigebracht, dass die gesamte Bildrepräsentation mit der gesamten Textrepräsentation übereinstimmen sollte.</li><li>Da CLIPs Architektur auf der Bildseite ein Vision Transformer und auf der Textseite ein Text-Transformer ist - die beide separate Encoder bilden - gibt es kein Cross-Attention-Modul, das nativ Patches an Tokens ausrichtet. Stattdessen erhält man reine <strong>Self-Attention</strong> in jedem Tower plus eine finale Projektion für die globalen Bild- oder Text-Embeddings.</li></ul><p>Kurz gesagt, dies ist eine heuristische Visualisierung. Die Tatsache, dass ein bestimmtes Patch-Embedding einem bestimmten Token-Embedding nahe oder fern sein könnte, ist gewissermaßen emergent. Es ist eher ein <em>Post-hoc Interpretationstrick</em> als eine robuste oder offizielle \"Attention\" des Modells.</p><h2 id=\"why-might-local-alignment-emerge\">Warum könnte lokale Ausrichtung entstehen?</h2><p>Warum könnten wir manchmal Wort-Patch-Level lokale Ausrichtungen erkennen? Hier ist der Punkt: Obwohl CLIP auf einem <em>globalen</em> Bild-Text kontrastiven Ziel trainiert wird, verwendet es dennoch Self-Attention (in ViT-basierten Bildencodern) und Transformer-Layer (für Text). Innerhalb dieser Self-Attention-Layer können verschiedene Teile von Bildrepräsentationen miteinander interagieren, genau wie Wörter in Textrepräsentationen. Durch das Training auf massiven Bild-Text-Datensätzen entwickelt das Modell natürlich interne latente Strukturen, die ihm helfen, Gesamtbilder ihren entsprechenden Textbeschreibungen zuzuordnen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Lokale Ausrichtung</strong> kann in diesen latenten Repräsentationen aus mindestens zwei Gründen auftreten:</p><ol><li><strong>Co-Occurrence Muster</strong>: Wenn ein Modell viele Bilder von \"Hunden\" neben vielen Bildern von \"Katzen\" sieht (oft mit diesen Wörtern beschriftet oder beschrieben), kann es latente Features lernen, die ungefähr diesen Konzepten entsprechen. So könnte das Embedding für \"Hund\" nahe an lokalen Patches liegen, die eine hundeähnliche Form oder Textur zeigen. Dies wird <em>nicht</em> explizit auf Patch-Ebene überwacht, sondern entsteht aus der wiederholten Assoziation zwischen Hunde-Bild/Text-Paaren.</li><li><strong>Self-Attention</strong>: In Vision Transformers beachten die Patches einander. Markante Patches (wie ein Hundegesicht) können eine konsistente latente \"Signatur\" entwickeln, da das Modell versucht, eine einzige global akkurate Repräsentation der gesamten Szene zu erzeugen. Wenn dies hilft, den gesamten kontrastiven Verlust zu minimieren, wird es verstärkt.</li></ol><h2 id=\"theoretical-analysis\">Theoretische Analyse</h2><p>CLIPs kontrastives Lernziel strebt an, die Kosinus-Ähnlichkeit zwischen passenden Bild-Text-Paaren zu maximieren und gleichzeitig für nicht passende Paare zu minimieren. Angenommen, die Text- und Bild-Encoder erzeugen Token- und Patch-Embeddings:</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Globale Ähnlichkeit kann als Aggregat lokaler Ähnlichkeiten dargestellt werden:</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Wenn bestimmte Token-Patch-Paare häufig in den Trainingsdaten gemeinsam auftreten, verstärkt das Modell ihre Ähnlichkeit durch kumulative Gradienten-Updates:</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, wobei $C$ die Anzahl der gemeinsamen Auftreten ist. Dies führt dazu, dass $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$ signifikant zunimmt und eine stärkere lokale Ausrichtung für diese Paare fördert. Der kontrastive Verlust verteilt jedoch Gradienten-Updates über alle Token-Patch-Paare, was die Stärke der Updates für einzelne Paare begrenzt:</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>Dies verhindert eine signifikante Verstärkung einzelner Token-Patch-Ähnlichkeiten.</p><h2 id=\"conclusion\">Fazit</h2><p>CLIPs Token-Patch-Visualisierungen nutzen eine beiläufige, emergente Ausrichtung zwischen Text- und Bildrepräsentationen. Diese Ausrichtung ist zwar interessant, stammt aber aus CLIPs <strong>globalem kontrastiven Training</strong> und verfügt nicht über die strukturelle Robustheit, die für präzise und zuverlässige Erklärbarkeit erforderlich ist. Die resultierenden Visualisierungen zeigen oft <strong>Rauschen und Inkonsistenz</strong>, was ihren Nutzen für tiefgehende interpretative Anwendungen einschränkt.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Late Interaction Modelle wie <strong>ColBERT</strong> und <strong>ColPali</strong> adressieren diese Einschränkungen, indem sie <strong>architektonisch explizite, feingranulare Ausrichtungen</strong> zwischen Text-Tokens und Bild-Patches einbetten. Indem sie Modalitäten unabhängig verarbeiten und gezielte Ähnlichkeitsberechnungen in einer späteren Phase durchführen, stellen diese Modelle sicher, dass jedes Text-Token bedeutungsvoll mit relevanten Bildregionen assoziiert wird.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP kann Token-Patch-Ähnlichkeiten visualisieren, allerdings ist dies eher ein nachträglicher Interpretations-Trick als eine robuste oder offizielle \"Attention\" des Modells. Das ist der Grund dafür.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
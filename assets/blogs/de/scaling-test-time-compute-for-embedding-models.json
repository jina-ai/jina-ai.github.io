{
  "slug": "scaling-test-time-compute-for-embedding-models",
  "id": "675a84f80ce9930001b86f09",
  "uuid": "49f876f3-0d50-4555-8f9e-136473f720ac",
  "title": "Skalierung der Rechenleistung zur Testzeit bei Embedding-Modellen",
  "html": "<p>Seit der Ver√∂ffentlichung des <a href=\"https://openai.com/o1/?ref=jina-ai-gmbh.ghost.io\">O1-Modells</a> von OpenAI ist eines der meistdiskutierten Themen in der KI-Community das <strong>Skalieren der Test-Zeit-Berechnung</strong>. Dies bezieht sich auf die Zuweisung zus√§tzlicher Rechenressourcen w√§hrend der Inferenz ‚Äì der Phase, in der ein KI-Modell Ausgaben als Reaktion auf Eingaben generiert ‚Äì anstatt w√§hrend des Vortrainings. Ein bekanntes Beispiel ist das mehrstufige \"Chain of Thought\"-Reasoning, das es Modellen erm√∂glicht, umfangreichere interne √úberlegungen anzustellen, wie etwa die Bewertung mehrerer potenzieller Antworten, tiefere Planung und Selbstreflexion, bevor eine endg√ºltige Antwort gegeben wird. Diese Strategie verbessert die Antwortqualit√§t, besonders bei komplexen Reasoning-Aufgaben. Alibabas k√ºrzlich ver√∂ffentlichtes <a href=\"https://huggingface.co/Qwen/QwQ-32B-Preview?ref=jina-ai-gmbh.ghost.io\">QwQ-32B-Preview</a> Modell folgt diesem Trend der Verbesserung des KI-Reasonings durch erh√∂hte Test-Zeit-Berechnung.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">In diesem Kontext bedeutet \"Skalierung\" haupts√§chlich die Erh√∂hung der Rechenkapazit√§t (wie Rechenleistung oder Zeit) w√§hrend der Inferenz. Es bezieht sich nicht auf <b><strong style=\"white-space: pre-wrap;\">Scaling out</strong></b> (Verteilung von Aufgaben auf mehrere Systeme) oder das Erreichen eines <b><strong style=\"white-space: pre-wrap;\">Speedup</strong></b> (Reduzierung der Verarbeitungszeit).</div></div><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1.mp4\" poster=\"https://img.spacergif.org/v1/900x432/0a/spacer.png\" width=\"900\" height=\"432\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:10</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Bei der Verwendung von OpenAIs O1-Modell k√∂nnen Benutzer deutlich erkennen, dass mehrstufige Inferenz zus√§tzliche Zeit ben√∂tigt, w√§hrend das Modell Reasoning-Ketten zur Probleml√∂sung aufbaut.</span></p></figcaption>\n        </figure><p>Bei Jina AI konzentrieren wir uns mehr auf Embeddings und Reranker als auf LLMs, daher ist es f√ºr uns naheliegend, die Skalierung der Test-Zeit-Berechnung in diesem Kontext zu betrachten: <em>Wie kann der \"Chain-of-Thought\"-Ansatz auf Embedding-Modelle angewendet werden?</em> Auch wenn es zun√§chst nicht intuitiv erscheinen mag, erkundet dieser Artikel eine neue Perspektive und zeigt, wie die Skalierung der Test-Zeit-Berechnung auf <code>jina-clip</code> angewendet werden kann, um Out-of-Distribution (OOD) Bilder zu klassifizieren ‚Äì und damit Aufgaben zu l√∂sen, die sonst unm√∂glich w√§ren.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Unser Experiment konzentrierte sich auf Pokemon-Erkennung, die eine interessante Herausforderung f√ºr Embedding-Modelle darstellt. W√§hrend CLIP-√§hnliche Modelle bei allgemeinem Bild-Text-Matching hervorragend sind, k√∂nnten sie bei Nischenbereichen oder OOD-Bildern ohne Fine-Tuning Schwierigkeiten haben. Indem wir den Modellen mehr Zeit zum \"Nachdenken\" gaben, stellten wir fest, dass Multi-Target-Klassifizierung ‚Äì analog zu einer \"Chain of Thought\" ‚Äì die Genauigkeit verbessern konnte, ohne das Embedding-Modell selbst zu tunen.</span></figcaption></figure><h2 id=\"case-study\">Fallstudie</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z?ref=jina-ai-gmbh.ghost.io#scrollTo=CJt5zwA9E2jB\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-15.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Unser Experiment konzentrierte sich auf Pokemon-Klassifizierung unter Verwendung des <a href=\"https://huggingface.co/datasets/TheFusion21/PokemonCards?ref=jina-ai-gmbh.ghost.io\">TheFusion21/PokemonCards Datensatzes</a>, der Tausende von Pokemon-Sammelkartenbildern enth√§lt. <strong>Die Aufgabe ist Bildklassifizierung</strong>, wobei der Input ein zugeschnittenes Pokemon-Karten-Artwork ist (ohne Text/Beschreibungen) und der Output der korrekte Pokemon-Name aus einer vordefinierten Namensliste ist. Diese Aufgabe stellt eine besonders interessante Herausforderung f√ºr CLIP Embedding-Modelle dar, weil:</p><ul><li>Pokemon-Namen und visuelle Darstellungen Nischenkonzepte f√ºr das Modell darstellen, was die direkte Klassifizierung erschwert</li><li>Jedes Pokemon klare visuelle Merkmale hat, die in grundlegende Elemente zerlegt werden k√∂nnen (Formen, Farben, Posen), die CLIP m√∂glicherweise besser versteht</li><li>Die Kartenartworks ein konsistentes visuelles Format bieten, w√§hrend sie durch verschiedene Hintergr√ºnde, Posen und k√ºnstlerische Stile Komplexit√§t einbringen</li><li>Die Aufgabe die Integration mehrerer visueller Merkmale gleichzeitig erfordert, √§hnlich wie komplexe Reasoning-Ketten in Sprachmodellen</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"835\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Wir schneiden die Pokemon-Kartenbilder zu, um alle Textinformationen (Kopfzeile, Fu√üzeile, Beschreibung) zu entfernen und triviale Vermutungen aufgrund von Pokemon-Namen in diesen Texten zu verhindern. Die Klassenlabels dieser Pokemon sind [</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol G</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Weedle</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Azumarill</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Bulbasaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Venusaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aggron</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Beedrill Œ¥</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Alakazam</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Dratini</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Arcanine</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Blaine's Moltres</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Celebi & Venusaur-GX</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">]</span></figcaption></figure>\n\n<h3 id=\"baseline\">Baseline</h3>\n\n<p>Der Baseline-Ansatz verwendet einen einfachen direkten Vergleich zwischen Pok√©mon-Karten-Artwork und Namen. Zun√§chst beschneiden wir jedes Pok√©mon-Kartenbild, um alle Textinformationen (Kopfzeile, Fu√üzeile, Beschreibung) zu entfernen, damit das CLIP-Modell keine trivialen Vermutungen aufgrund von Pok√©mon-Namen in diesen Texten anstellen kann. Dann kodieren wir sowohl die beschnittenen Bilder als auch die Pok√©mon-Namen mit dem <code>jina-clip-v1</code> und <code>jina-clip-v2</code> Modell, um ihre jeweiligen Embeddings zu erhalten. Die Klassifizierung erfolgt durch Berechnung der Kosinus-√Ñhnlichkeit zwischen diesen Bild- und Text-Embeddings - jedes Bild wird dem Namen zugeordnet, der den h√∂chsten √Ñhnlichkeitswert aufweist. Dies erzeugt eine einfache Eins-zu-eins-Zuordnung zwischen visuellem Karten-Artwork und Pok√©mon-Namen, ohne zus√§tzliche Kontext- oder Attributinformationen. Der folgende Pseudocode fasst die Baseline-Methode zusammen.</p>\n\n<pre><code class=\"language-python\"># Preprocessing\ncropped_images = [crop_artwork(img) for img in pokemon_cards]  # Remove text, keep only art\npokemon_names = [\"Absol\", \"Aerodactyl\", ...]  # Raw Pokemon names\n\n# Get embeddings using jina-clip-v1\nimage_embeddings = model.encode_image(cropped_images)\ntext_embeddings = model.encode_text(pokemon_names)\n\n# Classification by cosine similarity\nsimilarities = cosine_similarity(image_embeddings, text_embeddings)\npredicted_names = [pokemon_names[argmax(sim)] for sim in similarities]\n\n# Evaluate\naccuracy = mean(predicted_names == ground_truth_names)</code></pre>\n\n<h3 id=\"chain-of-thoughts-for-classification\">\"Chain of Thoughts\" f√ºr die Klassifizierung</h3>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--10-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure>\n\n<p>Anstatt Bilder direkt mit Namen abzugleichen, zerlegen wir die Pok√©mon-Erkennung in ein strukturiertes System von visuellen Attributen. Wir definieren f√ºnf zentrale Attributgruppen: dominante Farbe (z.B. \"wei√ü\", \"blau\"), Grundform (z.B. \"ein Wolf\", \"ein gefl√ºgeltes Reptil\"), Hauptmerkmal (z.B. \"ein einzelnes wei√ües Horn\", \"gro√üe Fl√ºgel\"), K√∂rperform (z.B. \"wolfsartig auf vier Beinen\", \"gefl√ºgelt und schlank\") und Hintergrundszene (z.B. \"Weltraum\", \"gr√ºner Wald\").</p>\n\n<p>F√ºr jede Attributgruppe erstellen wir spezifische Text-Prompts (z.B. \"Dieses Pok√©mon ist haupts√§chlich {} in der Farbe\") mit entsprechenden Optionen. Dann verwenden wir das Modell, um √Ñhnlichkeitswerte zwischen dem Bild und jeder Attributoption zu berechnen. Diese Werte werden mittels Softmax in Wahrscheinlichkeiten umgewandelt, um ein besser kalibriertes Ma√ü f√ºr die Konfidenz zu erhalten.</p>\n\n<p>Die vollst√§ndige Chain of Thought (CoT) Struktur besteht aus zwei Teilen: <code>classification_groups</code>, das Gruppen von Prompts beschreibt, und <code>pokemon_rules</code>, das definiert, welche Attributoptionen zu jedem Pok√©mon passen sollten. Zum Beispiel sollte Absol f√ºr Farbe \"wei√ü\" und f√ºr Form \"wolfsartig\" entsprechen. Die vollst√§ndige CoT wird unten gezeigt (wie diese erstellt wird, erkl√§ren wir sp√§ter):</p>\n\n<pre><code class=\"language-python\">pokemon_system = {\n    \"classification_cot\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pok√©mon's body is mainly {} in color.\",\n            \"options\": [\n                \"white\",    # Absol, Absol G\n                \"gray\",     # Aggron\n                \"brown\",    # Aerodactyl, Weedle, Beedrill Œ¥\n                \"blue\",     # Azumarill\n                \"green\",    # Bulbasaur, Venusaur, Celebi&Venu, Caterpie\n                \"yellow\",   # Alakazam, Ampharos\n                \"red\",      # Blaine's Moltres\n                \"orange\",   # Arcanine\n                \"light blue\"# Dratini\n            ]\n        },\n        \"primary_form\": {\n            \"prompt\": \"It looks like {}.\",\n            \"options\": [\n                \"a wolf\",         # Absol, Absol G\n                \"an armored dinosaur\",  # Aggron\n                \"a winged reptile\",     # Aerodactyl\n                \"a rabbit-like creature\", # Azumarill\n                \"a toad-like creature\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"a caterpillar larva\",    # Weedle, Caterpie\n                \"a wasp-like insect\",     # Beedrill Œ¥\n                \"a fox-like humanoid\",     # Alakazam\n                \"a sheep-like biped\",      # Ampharos\n                \"a dog-like beast\",        # Arcanine\n                \"a flaming bird\",          # Blaine's Moltres\n                \"a serpentine dragon\"      # Dratini\n            ]\n        },\n        \"key_trait\": {\n            \"prompt\": \"Its most notable feature is {}.\",\n            \"options\": [\n                \"a single white horn\", # Absol, Absol G\n                \"metal armor plates\",  # Aggron\n                \"large wings\",         # Aerodactyl, Beedrill Œ¥\n                \"rabbit ears\",         # Azumarill\n                \"a green plant bulb\",  # Bulbasaur, Venusaur, Celebi&Venu\n                \"a small red spike\",   # Weedle\n                \"big green eyes\",      # Caterpie\n                \"a mustache and spoons\", # Alakazam\n                \"a glowing tail orb\",  # Ampharos\n                \"a fiery mane\",        # Arcanine\n                \"flaming wings\",       # Blaine's Moltres\n                \"a tiny white horn on head\" # Dratini\n            ]\n        },\n        \"body_shape\": {\n            \"prompt\": \"The body shape can be described as {}.\",\n            \"options\": [\n                \"wolf-like on four legs\",   # Absol, Absol G\n                \"bulky and armored\",        # Aggron\n                \"winged and slender\",       # Aerodactyl, Beedrill Œ¥\n                \"round and plump\",          # Azumarill\n                \"sturdy and four-legged\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"long and worm-like\",       # Weedle, Caterpie\n                \"upright and humanoid\",     # Alakazam, Ampharos\n                \"furry and canine\",         # Arcanine\n                \"bird-like with flames\",    # Blaine's Moltres\n                \"serpentine\"                # Dratini\n            ]\n        },\n        \"background_scene\": {\n            \"prompt\": \"The background looks like {}.\",\n            \"options\": [\n                \"outer space\",      # Absol G, Beedrill Œ¥\n                \"green forest\",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&Venu\n                \"a rocky battlefield\", # Absol, Aggron, Aerodactyl\n                \"a purple psychic room\", # Alakazam\n                \"a sunny field\",     # Ampharos\n                \"volcanic ground\",   # Arcanine\n                \"a red sky with embers\", # Blaine's Moltres\n                \"a calm blue lake\"   # Dratini\n            ]\n        }\n    },\n    \n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,      \n            \"body_shape\": 0,    \n            \"background_scene\": 2   \n        },\n        \"Absol G\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,       \n            \"body_shape\": 0,     \n            \"background_scene\": 0    \n        },\n        // ...\n    }\n}\n</code></pre>\n\n<p>Die endg√ºltige Klassifizierung kombiniert diese Attributwahrscheinlichkeiten - anstatt eines einzelnen √Ñhnlichkeitsvergleichs f√ºhren wir nun mehrere strukturierte Vergleiche durch und aggregieren deren Wahrscheinlichkeiten, um eine fundiertere Entscheidung zu treffen.</p>\n\n<pre><code class=\"language-python\"># Classification process\ndef classify_pokemon(image):\n   # Generate all text prompts\n   all_prompts = []\n   for group in classification_cot:\n       for option in group[\"options\"]:\n           prompt = group[\"prompt\"].format(option)\n           all_prompts.append(prompt)\n\n   # Get embeddings and similarities\n   image_embedding = model.encode_image(image)\n   text_embeddings = model.encode_text(all_prompts)\n   similarities = cosine_similarity(image_embedding, text_embeddings)\n\n   # Convert to probabilities per attribute group\n   probabilities = {}\n   for group_name, group_sims in group_similarities:\n       probabilities[group_name] = softmax(group_sims)\n\n   # Score each Pokemon based on matching attributes\n   scores = {}\n   for pokemon, rules in pokemon_rules.items():\n       score = 0\n       for group, target_idx in rules.items():\n           score += probabilities[group][target_idx]\n       scores[pokemon] = score\n\n   return max(scores, key=scores.get)</code></pre>\n\n<h3 id=\"complexity-analysis\">Komplexit√§tsanalyse</h3>\n\n<p>Nehmen wir an, wir m√∂chten ein Bild in eine von <code>N</code> Pok√©mon-Namen klassifizieren. Der Baseline-Ansatz erfordert die Berechnung von <code>N</code> Text-Embeddings (eines f√ºr jeden Pok√©mon-Namen). Im Gegensatz dazu erfordert unser skalierter Test-Time-Compute-Ansatz die Berechnung von <code>Q</code> Text-Embeddings, wobei</p><code>Q</code> ist die Gesamtzahl der Frage-Option-Kombinationen √ºber alle Fragen hinweg. Beide Methoden erfordern die Berechnung eines Bild-Embeddings und einen abschlie√üenden Klassifizierungsschritt, daher schlie√üen wir diese gemeinsamen Operationen von unserem Vergleich aus. In dieser Fallstudie haben wir <code>N=13</code> und <code>Q=52</code>.</p><p>In einem Extremfall, wo <code>Q = N</code>, w√ºrde sich unser Ansatz im Wesentlichen auf die Baseline reduzieren. Der Schl√ºssel zur effektiven Skalierung der Test-Time-Berechnung ist jedoch:</p><ul><li>Sorgf√§ltig gew√§hlte Fragen konstruieren, die <code>Q</code> erh√∂hen</li><li>Sicherstellen, dass jede Frage eindeutige, informative Hinweise auf die endg√ºltige Antwort liefert</li><li>Fragen so orthogonal wie m√∂glich gestalten, um ihren gemeinsamen Informationsgewinn zu maximieren</li></ul><p>Dieser Ansatz ist analog zum Spiel \"Zwanzig Fragen\", bei dem jede Frage strategisch gew√§hlt wird, um die m√∂glichen Antworten effektiv einzugrenzen.</p><h3 id=\"evaluation\">Evaluation</h3><p>Unsere Evaluation wurde mit 117 Testbildern aus 13 verschiedenen Pok√©mon-Klassen durchgef√ºhrt. Das Ergebnis ist wie folgt:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>jina-clip-v1</th>\n<th>jina-clip-v2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>31.36%</td>\n<td>16.10%</td>\n</tr>\n<tr>\n<td>CoT</td>\n<td>46.61%</td>\n<td>38.14%</td>\n</tr>\n<tr>\n<td>Improvement</td>\n<td>+15.25%</td>\n<td>+22.04%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Man kann sehen, dass die gleiche CoT-Klassifikation f√ºr beide Modelle signifikante Verbesserungen (+15.25% bzw. +22.04%) bei dieser ungew√∂hnlichen oder OOD-Aufgabe bietet. Dies deutet auch darauf hin, dass sobald das <code>pokemon_system</code> konstruiert ist, <strong>dasselbe CoT-System effektiv zwischen verschiedenen Modellen √ºbertragen werden kann; und kein Fine-Tuning oder Post-Training erforderlich ist.</strong></p><p>Die relativ starke Baseline-Performance von v1 (31.36%) bei der Pokemon-Klassifikation ist bemerkenswert. Dieses Modell wurde auf <a href=\"https://www.youtube.com/watch?v=HsGyxVUN1SA&ref=jina-ai-gmbh.ghost.io\">LAION-400M trainiert, das Pokemon-bezogene Inhalte enthielt</a>. Im Gegensatz dazu wurde v2 auf DFN-2B trainiert (Unterabtastung von 400M Instanzen), einem qualitativ hochwertigeren, aber st√§rker gefilterten Datensatz, der m√∂glicherweise Pokemon-bezogene Inhalte ausgeschlossen hat, was die niedrigere Baseline-Performance von V2 (16.10%) bei dieser spezifischen Aufgabe erkl√§rt.</p><h3 id=\"constructing-pokemonsystem-effectively\">Effektive Konstruktion des <code>pokemon_system</code></h3><p>Die Effektivit√§t unseres skalierten Test-Time-Compute-Ansatzes h√§ngt stark davon ab, wie gut wir das <code>pokemon_system</code> konstruieren. Es gibt verschiedene Ans√§tze zum Aufbau dieses Systems, von manuell bis vollautomatisch.</p><h4 id=\"manual-construction\">Manuelle Konstruktion</h4><p>Der direkteste Ansatz ist die manuelle Analyse des Pokemon-Datensatzes und das Erstellen von Attributgruppen, Prompts und Regeln. Ein Dom√§nenexperte m√ºsste wichtige visuelle Attribute wie Farbe, Form und charakteristische Merkmale identifizieren. Anschlie√üend w√ºrden sie nat√ºrlichsprachliche Prompts f√ºr jedes Attribut schreiben, m√∂gliche Optionen f√ºr jede Attributgruppe aufz√§hlen und jedes Pokemon seinen korrekten Attributoptionen zuordnen. W√§hrend dies qualitativ hochwertige Regeln liefert, ist es zeitaufw√§ndig und skaliert nicht gut bei gr√∂√üerem <code>N</code>.</p><h4 id=\"llm-assisted-construction\">LLM-unterst√ºtzte Konstruktion</h4><p>Wir k√∂nnen LLMs nutzen, um diesen Prozess zu beschleunigen, indem wir sie auffordern, das Klassifikationssystem zu generieren. Ein gut strukturierter Prompt w√ºrde Attributgruppen basierend auf visuellen Eigenschaften, nat√ºrlichsprachliche Prompt-Templates, umfassende und sich gegenseitig ausschlie√üende Optionen sowie Zuordnungsregeln f√ºr jedes Pokemon anfordern. Das LLM kann schnell einen ersten Entwurf generieren, auch wenn seine Ausgabe m√∂glicherweise √ºberpr√ºft werden muss.</p><pre><code class=\"language-txt\">I need help creating a structured system for Pokemon classification. For each Pokemon in this list: [Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...], create a classification system with:\n\n1. Classification groups that cover these visual attributes:\n   - Dominant color of the Pokemon\n   - What type of creature it appears to be (primary form)\n   - Its most distinctive visual feature\n   - Overall body shape\n   - What kind of background/environment it's typically shown in\n\n2. For each group:\n   - Create a natural language prompt template using \"{}\" for the option\n   - List all possible options that could apply to these Pokemon\n   - Make sure options are mutually exclusive and comprehensive\n\n3. Create rules that map each Pokemon to exactly one option per attribute group, using indices to reference the options\n\nPlease output this as a Python dictionary with two main components:\n- \"classification_groups\": containing prompts and options for each attribute\n- \"pokemon_rules\": mapping each Pokemon to its correct attribute indices\n\nExample format:\n{\n    \"classification_groups\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokemon's body is mainly {} in color\",\n            \"options\": [\"white\", \"gray\", ...]\n        },\n        ...\n    },\n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,  # index for \"white\"\n            ...\n        },\n        ...\n    }\n}</code></pre><p>Ein robusterer Ansatz kombiniert LLM-Generierung mit menschlicher Validierung. Zuerst generiert das LLM ein initiales System. Dann √ºberpr√ºfen und korrigieren menschliche Experten die Attributgruppierungen, Vollst√§ndigkeit der Optionen und Regelgenauigkeit. Das LLM verfeinert das System basierend auf diesem Feedback, und der Prozess wird iteriert, bis eine zufriedenstellende Qualit√§t erreicht ist. Dieser Ansatz balanciert Effizienz mit Genauigkeit.</p><h4 id=\"automated-construction-with-dspy\">Automatisierte Konstruktion mit DSPy</h4><p>F√ºr einen vollautomatischen Ansatz k√∂nnen wir DSPy verwenden, um das <code>pokemon_system</code> iterativ zu optimieren. Der Prozess beginnt mit einem einfachen <code>pokemon_system</code>, das entweder manuell oder von LLMs als initialer Prompt geschrieben wurde. Jede Version wird auf einem Hold-out-Set evaluiert, wobei die Genauigkeit als Feedback-Signal f√ºr DSPy dient. Basierend auf dieser Performance werden optimierte Prompts (d.h. neue Versionen des <code>pokemon_system</code>) generiert. Dieser Zyklus wiederholt sich bis zur Konvergenz, und w√§hrend des gesamten Prozesses bleibt das Embedding-Modell vollst√§ndig fixiert.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Verwendung von DSPy zur Findung des besten </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>pokemon_system</span></code><span style=\"white-space: pre-wrap;\"> CoT-Designs; der Tuning-Prozess muss nur einmal f√ºr jede Aufgabe durchgef√ºhrt werden.</span></figcaption></figure><h2 id=\"why-scale-test-time-compute-for-embedding-models\">Warum Test-Time-Compute f√ºr Embedding-Modelle skalieren?</h2><p>Weil die Skalierung des Pretrainings letztendlich wirtschaftlich untragbar wird.</p><p>Seit der Ver√∂ffentlichung der Jina Embeddings Suite‚Äîeinschlie√ülich <code>jina-embeddings-v1</code>, <code>v2</code>, <code>v3</code>, <code>jina-clip-v1</code>, <code>v2</code> und <code>jina-ColBERT-v1</code>, <code>v2</code>‚Äîist jedes Modell-Upgrade durch skaliertes Pretraining mit h√∂heren Kosten verbunden. Zum Beispiel hatte unser erstes Modell, <code>jina-embeddings-v1</code>, das im Juni 2023 ver√∂ffentlicht wurde, 110M Parameter. Das Training kostete damals je nach Messung zwischen $5.000 und $10.000. Mit <code>jina-embeddings-v3</code> sind die Verbesserungen signifikant, aber sie stammen haupts√§chlich aus den erh√∂hten investierten Ressourcen. Die Kostentrajektorie f√ºr Frontier-Modelle ist von Tausenden zu Zehntausenden Dollar und f√ºr gr√∂√üere KI-Unternehmen heute sogar zu Hunderten Millionen gestiegen. W√§hrend mehr Geld, Ressourcen und Daten ins Pretraining zu besseren Modellen f√ºhren, machen die abnehmenden Grenzertr√§ge weitere Skalierung wirtschaftlich unhaltbar.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/plot--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/plot--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/plot--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/plot--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/plot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Skalierungsgesetz von Embedding-Modellen. Die durchschnittliche MTEB-Performance bei englischen Aufgaben ist gegen die Anzahl der Modellparameter aufgetragen. Jeder Punkt repr√§sentiert ein Embedding-Modell. Die Trendlinie, die alle Modelle repr√§sentiert, ist hervorgehoben, wobei mehrsprachige Modelle als t√ºrkisfarbene Punkte dargestellt sind. Dieser Graph wurde durch Auswahl der Top-100 Embedding-Modelle aus der MTEB-Rangliste erstellt, wobei Modelle ohne Gr√∂√üeninformationen, typischerweise Closed-Source oder propriet√§re Modelle, ausgeschlossen wurden. Einreichungen, die als offensichtliches Trolling identifiziert wurden, wurden ebenfalls herausgefiltert.</span></figcaption></figure><p>Andererseits werden moderne Embedding-Modelle immer leistungsf√§higer: mehrsprachig, multitask, multimodal und f√§hig zu starker Zero-Shot- und Anweisungsbefolgungsleistung. Diese Vielseitigkeit l√§sst viel Raum f√ºr algorithmische Verbesserungen und die Skalierung von Test-Time-Compute.</p><p>Die Frage ist dann: Welche Kosten sind Benutzer bereit f√ºr eine Anfrage zu zahlen, die ihnen sehr wichtig ist? Wenn die Tolerierung l√§ngerer Inferenzzeiten bei fixen Pretraining-Modellen die Qualit√§t der Ergebnisse signifikant verbessert, w√ºrden viele das f√ºr lohnenswert halten. Aus unserer Sicht gibt es erhebliches ungenutztes Potenzial in der Skalierung von Test-Time-Compute f√ºr Embedding-Modelle. Dies stellt eine Verschiebung dar von der blo√üen Erh√∂hung der Modellgr√∂√üe w√§hrend des Trainings hin zur Verbesserung des Rechenaufwands w√§hrend der Inferenzphase, um bessere Leistung zu erzielen.</p><h2 id=\"conclusion\">Fazit</h2><p>Unsere Fallstudie zum Test-Time-Compute von <code>jina-clip-v1/v2</code> zeigt mehrere wichtige Erkenntnisse:</p><ol><li>Wir erzielten bessere Leistung bei ungew√∂hnlichen oder Out-of-Distribution (OOD) Daten ohne jegliches Fine-Tuning oder Post-Training der Embeddings.</li><li>Das System traf nuanciertere Unterscheidungen durch iterative Verfeinerung von √Ñhnlichkeitssuchen und Klassifikationskriterien.</li><li>Durch die Einbeziehung dynamischer Prompt-Anpassungen und iterativen Schlussfolgerns verwandelten wir den Inferenzprozess des Embedding-Modells von einer einzelnen Abfrage in eine sophistiziertere Gedankenkette.</li></ol><p>Diese Fallstudie kratzt nur an der Oberfl√§che dessen, was mit Test-Time-Compute m√∂glich ist. Es bleibt erheblicher Raum f√ºr algorithmische Skalierung. Zum Beispiel k√∂nnten wir Methoden entwickeln, um iterativ Fragen auszuw√§hlen, die den Antwortbereich am effizientesten eingrenzen, √§hnlich der optimalen Strategie im Spiel \"Zwanzig Fragen\". Durch die Skalierung von Test-Time-Compute k√∂nnen wir Embedding-Modelle √ºber ihre aktuellen Grenzen hinaus pushen und sie bef√§higen, komplexere, nuanciertere Aufgaben zu bew√§ltigen, die einst au√üer Reichweite schienen.</p>",
  "comment_id": "675a84f80ce9930001b86f09",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-12T07:38:48.000+01:00",
  "updated_at": "2024-12-12T17:54:17.000+01:00",
  "published_at": "2024-12-12T17:54:17.000+01:00",
  "custom_excerpt": "Better results scale with compute‚Äîmore on learning, more on search. A good pretrained model takes you far, but test-time compute takes you further. It's important to recognize this new paradigm of scaling test-time compute, even for embedding models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/scaling-test-time-compute-for-embedding-models/",
  "excerpt": "Bessere Ergebnisse skalieren mit Rechenleistung‚Äîmehr beim Lernen, mehr bei der Suche. Ein gutes vortrainiertes Modell bringt einen weit, aber Rechenleistung zur Testzeit bringt einen noch weiter. Es ist wichtig, dieses neue Paradigma der Skalierung von Rechenleistung zur Testzeit zu erkennen, selbst bei Embedding-Modellen.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
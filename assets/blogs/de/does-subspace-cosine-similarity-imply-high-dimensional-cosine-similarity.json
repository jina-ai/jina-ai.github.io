{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "Bedeutet die Kosinus-√Ñhnlichkeit im Unterraum auch Kosinus-√Ñhnlichkeit in h√∂heren Dimensionen?",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Am 25. Januar 2024 ver√∂ffentlichte OpenAI <a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">ein neues Embedding-Modell</a> mit einer neuen Funktion namens <i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">\"Shortening\"</strong></b></i>, die es Entwicklern erm√∂glicht, Embeddings zu k√ºrzen ‚Äì im Wesentlichen durch Abschneiden von Zahlen am Ende der Sequenz ‚Äì ohne die F√§higkeit des Embeddings zu beeintr√§chtigen, Konzepte effektiv zu repr√§sentieren. Tauchen Sie in diesen Beitrag ein, um eine solide theoretische Grundlage f√ºr die Machbarkeit und Begr√ºndung dieser Innovation zu erhalten.</div></div><p>Betrachten Sie dies: Wenn man die Kosinus-√Ñhnlichkeit von Embedding-Vektoren in hochdimensionalen R√§umen misst, wie impliziert ihre √Ñhnlichkeit in niedrigdimensionalen Teilr√§umen die Gesamt√§hnlichkeit? Gibt es eine direkte, proportionale Beziehung, oder ist die Realit√§t bei hochdimensionalen Daten komplexer?</p><p>Konkreter ausgedr√ºckt: <strong>Garantiert eine hohe √Ñhnlichkeit zwischen Vektoren in ihren ersten 256 Dimensionen auch eine hohe √Ñhnlichkeit in ihren vollen 768 Dimensionen?</strong> Andersherum gefragt: Wenn sich Vektoren in einigen Dimensionen deutlich unterscheiden, bedeutet dies eine niedrige Gesamt√§hnlichkeit? Dies sind keine rein theoretischen √úberlegungen; sie sind entscheidende Aspekte f√ºr effizientes Vector Retrieval, Database Indexing und die Leistung von RAG-Systemen.</p><p>Entwickler verlassen sich oft auf Heuristiken und nehmen an, dass eine hohe Teilraum-√Ñhnlichkeit einer hohen Gesamt√§hnlichkeit entspricht oder dass bemerkenswerte Unterschiede in einer Dimension die Gesamt√§hnlichkeit signifikant beeinflussen. Die Frage ist: Basieren diese heuristischen Methoden auf einer soliden theoretischen Grundlage, oder sind es lediglich pragmatische Annahmen?</p><p>Dieser Beitrag geht diesen Fragen nach und untersucht die Theorie und praktischen Implikationen der Teilraum-√Ñhnlichkeit in Bezug auf die Gesamtvektor-√Ñhnlichkeit. </p><h2 id=\"bounding-the-cosine-similarity\">Begrenzung der Kosinus-√Ñhnlichkeit</h2><p>Gegeben seien Vektoren $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$, die wir in $\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ und $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$ zerlegen, wobei $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ und $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$, mit $m+n=d$.</p><p>Die Kosinus-√Ñhnlichkeit im Teilraum $\\mathbb{R}^m$ ist gegeben durch $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$; entsprechend ist die √Ñhnlichkeit im Teilraum $\\mathbb{R}^n$ gleich $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$.</p><p>Im urspr√ºnglichen Raum $\\mathbb{R}^d$ ist die Kosinus-√Ñhnlichkeit definiert als:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>Sei nun $s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$. Dann gilt:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>Ende des Beweises. </p><p>Beachten Sie, dass wir im letzten Schritt des Beweises ausnutzen, dass die Kosinus-√Ñhnlichkeit immer kleiner oder gleich 1 ist. Dies bildet unsere obere Schranke. Analog k√∂nnen wir zeigen, dass die untere Schranke von \\(\\cos(\\mathbf{A},\\mathbf{B})\\) gegeben ist durch: </p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\], wobei $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$.</p><p>Beachten Sie, dass wir f√ºr die untere Schranke nicht vorschnell schlie√üen k√∂nnen, dass \\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\). Dies liegt am Wertebereich der Kosinus-Funktion, der zwischen \\([-1, 1]\\) liegt. Aufgrund dieses Bereichs ist es unm√∂glich, eine engere untere Schranke als den trivialen Wert von -1 festzulegen.</p><p>Zusammenfassend haben wir also die folgende lockere Schranke: $$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)).$$ und eine engere Schranke \\[\\begin{align*}  \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\], wobei $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $.</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">Verbindung zum Johnson‚ÄìLindenstrauss-Lemma</h3><p>Das JL-Lemma besagt, dass f√ºr jedes \\(0 &lt; \\epsilon &lt; 1\\) und jede endliche Menge von Punkten \\( S \\) in \\( \\mathbb{R}^d \\) eine Abbildung \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\) (mit \\( k = O(\\epsilon^{-2} \\log |S|) \\)) existiert, sodass f√ºr alle \\( \\mathbf{u}, \\mathbf{v} \\in S \\) die euklidischen Abst√§nde ann√§hernd erhalten bleiben:<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson‚ÄìLindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>Um $f$ wie eine Teilraumauswahl arbeiten zu lassen, k√∂nnen wir eine Diagonalmatrix zur Projektion verwenden, wie zum Beispiel eine \\(5 \\times 3\\) Matrix \\(f\\), wenn auch nicht zuf√§llig (beachten Sie, die typische Formulierung des JL-Lemmas beinhaltet lineare Transformationen, die oft zuf√§llige Matrizen aus einer Gau√ü-Verteilung verwenden). Wenn wir beispielsweise die 1., 3. und 5. Dimension aus einem 5-dimensionalen Vektorraum beibehalten wollen, k√∂nnte die Matrix \\(f\\) wie folgt gestaltet werden: \\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>Indem wir $f$ jedoch als diagonal festlegen, schr√§nken wir die Klasse der Funktionen ein, die f√ºr die Projektion verwendet werden k√∂nnen. Das JL-Lemma garantiert die Existenz eines geeigneten $f$ innerhalb der breiteren Klasse linearer Transformationen, aber wenn wir $f$ auf Diagonalmatrizen beschr√§nken, existiert m√∂glicherweise kein geeignetes $f$ innerhalb dieser eingeschr√§nkten Klasse f√ºr die Anwendung der JL-Lemma-Grenzen.</p><h2 id=\"validating-the-bounds\">Validierung der Grenzen</h2><p>Um die theoretischen Grenzen der Kosinus-√Ñhnlichkeit in hochdimensionalen Vektorr√§umen empirisch zu untersuchen, k√∂nnen wir eine Monte-Carlo-Simulation verwenden. Diese Methode erm√∂glicht es uns, eine gro√üe Anzahl zuf√§lliger Vektorpaare zu generieren, ihre √Ñhnlichkeiten sowohl im urspr√ºnglichen Raum als auch in Teilr√§umen zu berechnen und dann zu bewerten, wie gut die theoretischen oberen und unteren Grenzen in der Praxis gelten.</p><p>Der folgende Python-Code-Ausschnitt implementiert dieses Konzept. Er generiert zuf√§llig Vektorpaare in einem hochdimensionalen Raum und berechnet ihre Kosinus-√Ñhnlichkeit. Dann teilt er jeden Vektor in zwei Teilr√§ume auf, berechnet die Kosinus-√Ñhnlichkeit innerhalb jedes Teilraums und evaluiert die oberen und unteren Grenzen der vollst√§ndig-dimensionalen Kosinus-√Ñhnlichkeit basierend auf den Teilraum-√Ñhnlichkeiten.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s >= cos_sim),\n          np.all(gamma * t <= cos_sim)))\n</code></pre><figcaption><p>Ein Monte Carlo Validator zur √úberpr√ºfung der Cosinus-√Ñhnlichkeitsgrenzen</p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p>Eine Beispielausgabe unseres Monte Carlo Validators. Es ist wichtig zu beachten, dass die <code spellcheck=\"false\">lower/upper bound satisfied</code> Bedingung f√ºr jeden Vektor einzeln √ºberpr√ºft wird. Die <code spellcheck=\"false\">avg. lower/upper bound</code> bietet hingegen einen intuitiveren √úberblick √ºber die Statistiken dieser Grenzen, beeinflusst aber nicht direkt den Validierungsprozess.</p></figcaption></figure><h2 id=\"understanding-the-bounds\">Verst√§ndnis der Grenzen</h2><p>Kurz gesagt: Beim Vergleich zweier hochdimensionaler Vektoren liegt die Gesamt√§hnlichkeit zwischen den besten und schlechtesten √Ñhnlichkeiten ihrer Teilr√§ume, angepasst an die Gr√∂√üe oder Bedeutung dieser Teilr√§ume im Gesamtkontext. Dies ist es, was die Grenzen f√ºr die Cosinus-√Ñhnlichkeit in h√∂heren Dimensionen intuitiv darstellen: die Balance zwischen den √§hnlichsten und un√§hnlichsten Teilen, gewichtet nach ihrer relativen Gr√∂√üe oder Bedeutung.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Illustrative comparison of two stylus pen caps and bodies with labeled sections on a black background\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Jeder Stift hat zwei Hauptkomponenten: den K√∂rper und die Kappe.</figcaption></figure><p>Stellen Sie sich vor, Sie versuchen, zwei mehrteilige Objekte (sagen wir, zwei edle Stifte) basierend auf ihrer Gesamt√§hnlichkeit zu vergleichen. Jeder Stift hat zwei Hauptkomponenten: den K√∂rper und die Kappe. Die √Ñhnlichkeit des gesamten Stifts (sowohl K√∂rper als auch Kappe) ist das, was wir bestimmen m√∂chten:</p><h3 id=\"upper-bound-gamma-cdot-s\">Obere Grenze ($\\gamma \\cdot s$)</h3><p>Betrachten Sie $s$ als die beste √úbereinstimmung zwischen entsprechenden Teilen der Stifte. Wenn die Kappen sehr √§hnlich sind, aber die K√∂rper nicht, ist $s$ die √Ñhnlichkeit der Kappen.</p><p>$\\gamma$ ist dabei wie ein Skalierungsfaktor basierend auf der Gr√∂√üe (oder Bedeutung) jedes Teils. Wenn ein Stift einen sehr langen K√∂rper und eine kurze Kappe hat, w√§hrend der andere einen kurzen K√∂rper und eine lange Kappe hat, passt $\\gamma$ die Gesamt√§hnlichkeit an diese Proportionsunterschiede an.</p><p>Die obere Grenze sagt uns, dass die Gesamt√§hnlichkeit unabh√§ngig davon, wie √§hnlich einige Teile sind, diese \"beste Teil√§hnlichkeit\" skaliert mit dem Proportionsfaktor nicht √ºberschreiten kann.</p><h3 id=\"lower-bound-gamma-cdot-t\">Untere Grenze ($\\gamma \\cdot t$)</h3><p>Hier ist $t$ die √Ñhnlichkeit der am wenigsten √ºbereinstimmenden Teile. Wenn sich die K√∂rper der Stifte stark unterscheiden, aber die Kappen √§hnlich sind, spiegelt $t$ die √Ñhnlichkeit der K√∂rper wider.</p><p>Auch hier skaliert $\\gamma$ dies basierend auf dem Anteil jedes Teils.</p><p>Die untere Grenze bedeutet, dass die Gesamt√§hnlichkeit nicht schlechter sein kann als diese \"schlechteste Teil√§hnlichkeit\" nach Ber√ºcksichtigung des Anteils jedes Teils.</p><h2 id=\"implications-of-the-bounds\">Implikationen der Grenzen</h2><p>F√ºr Softwareentwickler, die mit Embeddings, Vektorsuche, Retrieval oder Datenbanken arbeiten, hat das Verst√§ndnis dieser Grenzen praktische Auswirkungen, besonders beim Umgang mit hochdimensionalen Daten. Die Vektorsuche beinhaltet oft das Finden der n√§chstgelegenen (√§hnlichsten) Vektoren in einer Datenbank zu einem gegebenen Abfragevektor, typischerweise unter Verwendung der Cosinus-√Ñhnlichkeit als Ma√ü f√ºr die N√§he. Die besprochenen Grenzen k√∂nnen Einblicke in die Effektivit√§t und Grenzen der Verwendung von Teilraum√§hnlichkeiten f√ºr solche Aufgaben geben.</p><h3 id=\"using-subspace-similarity-for-ranking\">Verwendung der Teilraum√§hnlichkeit f√ºr das Ranking</h3><p><strong>Sicherheit und Genauigkeit</strong>: Die Verwendung der Teilraum√§hnlichkeit f√ºr das Ranking und Abrufen von Top-k-Ergebnissen kann effektiv sein, erfordert aber Vorsicht. Die obere Grenze zeigt, dass die Gesamt√§hnlichkeit die maximale √Ñhnlichkeit der Teilr√§ume nicht √ºberschreiten kann. Wenn also ein Vektorpaar in einem bestimmten Teilraum sehr √§hnlich ist, ist es ein starker Kandidat f√ºr √Ñhnlichkeit im hochdimensionalen Raum.</p><p><strong>M√∂gliche Fallstricke</strong>: Die untere Grenze deutet jedoch darauf hin, dass zwei Vektoren mit geringer √Ñhnlichkeit in einem Teilraum insgesamt noch recht √§hnlich sein k√∂nnten. Daher k√∂nnte das ausschlie√üliche Vertrauen auf die Teilraum√§hnlichkeit einige relevante Ergebnisse √ºbersehen.</p><h3 id=\"misconceptions-and-cautions\">Missverst√§ndnisse und Vorsichtsma√ünahmen</h3><p><strong>√úbersch√§tzung der Teilraumbedeutung</strong>: Ein h√§ufiges Missverst√§ndnis ist die √úbersch√§tzung der Bedeutung eines bestimmten Teilraums. W√§hrend hohe √Ñhnlichkeit in einem Teilraum ein guter Indikator ist, garantiert sie aufgrund des Einflusses anderer Teilr√§ume keine hohe Gesamt√§hnlichkeit.</p><p><strong>Ignorieren negativer √Ñhnlichkeiten</strong>: In F√§llen, in denen die Cosinus-√Ñhnlichkeit in einem Teilraum negativ ist, zeigt dies eine gegens√§tzliche Beziehung in dieser Dimension an. Entwickler sollten vorsichtig sein, wie sich diese negativen √Ñhnlichkeiten auf die Gesamt√§hnlichkeit auswirken.</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "Bedeutet eine hohe √Ñhnlichkeit im Unterraum auch eine hohe Gesamt√§hnlichkeit zwischen Vektoren? Dieser Beitrag untersucht die Theorie und praktischen Auswirkungen der Unterraum-√Ñhnlichkeit.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
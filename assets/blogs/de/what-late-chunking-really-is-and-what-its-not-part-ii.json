{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "Was Late Chunking wirklich ist & was nicht: Teil II",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Es wird dringend empfohlen, zuerst <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">Teil I zu lesen</a>, da dieser Artikel eine tiefergehende Sicht bietet und sich auf h√§ufige Missverst√§ndnisse und Vergleiche konzentriert. <b><strong style=\"white-space: pre-wrap;\">Empfohlene Lesereihenfolge: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Teil I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, Teil II, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Forschungspapier</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Das Chunking eines langen Dokuments hat zwei Probleme: Erstens die <strong>Bestimmung der Trennpunkte</strong> ‚Äì d. h. wie das Dokument segmentiert werden soll. Man k√∂nnte feste Token-L√§ngen, eine feste Anzahl von S√§tzen oder fortgeschrittenere Techniken wie <a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">Regex oder semantische Segmentierungsmodelle</a> in Betracht ziehen. Pr√§zise Chunk-Grenzen verbessern nicht nur die Lesbarkeit der Suchergebnisse, sondern stellen auch sicher, dass die Chunks, die einem LLM in einem RAG-System zugef√ºhrt werden, pr√§zise und ausreichend sind ‚Äì nicht mehr und nicht weniger.</p><p>Das zweite Problem ist der <strong>Kontextverlust</strong> innerhalb jedes Chunks. Sobald das Dokument segmentiert ist, besteht der n√§chste logische Schritt f√ºr die meisten darin, jeden Chunk separat in einem Batch-Prozess einzubetten. Dies f√ºhrt jedoch zu einem Verlust des globalen Kontexts aus dem urspr√ºnglichen Dokument. Viele fr√ºhere Arbeiten haben sich zuerst mit dem ersten Problem befasst und argumentiert, dass eine bessere Grenzerkennung die semantische Repr√§sentation verbessert. Zum Beispiel gruppiert \"semantisches Chunking\" S√§tze mit hoher Cosinus-√Ñhnlichkeit im Embedding-Space, um die St√∂rung semantischer Einheiten zu minimieren.</p><p>Aus unserer Sicht sind diese beiden Probleme <em>fast</em> orthogonal und k√∂nnen separat angegangen werden. Wenn wir priorisieren m√ºssten, <strong>w√ºrden wir sagen, dass das zweite Problem kritischer ist.</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">Problem 2: <b>Kontextuelle Information</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">Erhalten</td>\n<td>Verloren</td>\n</tr>\n<tr>\n<td><b>Problem 1: Trennpunkte</b></td>\n<td>Gut</td>\n<td style=\"text-align:center\">Ideales Szenario</td>\n<td>Schlechte Suchergebnisse</td>\n</tr>\n<tr>\n<td></td>\n<td>Schlecht</td>\n<td style=\"text-align:center\">Gute Suchergebnisse, aber Ergebnisse k√∂nnten f√ºr Menschen schwer lesbar oder f√ºr LLM-Reasoning ungeeignet sein</td>\n<td>Schlimmstes Szenario</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"late-chunking-for-context-loss\">Late Chunking f√ºr Kontextverlust</h2><p><strong>Late Chunking</strong> beginnt damit, das zweite Problem anzugehen: den <strong>Verlust des Kontexts</strong>. Es geht <em>nicht</em> darum, die idealen Trennpunkte oder semantischen Grenzen zu finden. Man muss immer noch Regex, Heuristiken oder andere Techniken verwenden, um ein langes Dokument in kleine Chunks zu unterteilen. Aber anstatt jeden Chunk sofort nach der Segmentierung einzubetten, codiert Late Chunking zun√§chst das gesamte Dokument in einem Kontextfenster (f√ºr <code>jina-embeddings-v3</code> sind es 8192 Token). Dann folgt es den Grenzhinweisen, um Mean Pooling f√ºr jeden Chunk anzuwenden ‚Äì daher der Begriff \"late\" (sp√§t) im Late Chunking.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagram comparing &quot;Naive Chunking&quot; and &quot;Late Chunking&quot; methods for processing long documents with labeled steps.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Late Chunking ben√∂tigt immer noch Grenzhinweise, aber der entscheidende Unterschied liegt darin, wann diese Hinweise verwendet werden. Beim Late Chunking werden die Hinweise erst angewendet, nachdem das gesamte Dokument eingebettet wurde, und sie werden verwendet, um die Pooling-Spanne zu bestimmen.</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">Late Chunking ist widerstandsf√§hig gegen schlechte Grenzhinweise</h2><p>Was wirklich interessant ist: Experimente zeigen, dass Late Chunking die Notwendigkeit perfekter semantischer Grenzen eliminiert, was teilweise das erste oben erw√§hnte Problem l√∂st. Tats√§chlich √ºbertrifft Late Chunking mit festen Token-Grenzen das naive Chunking mit semantischen Grenzhinweisen. Einfache Segmentierungsmodelle, wie solche mit festen L√§ngengrenzen, schneiden in Verbindung mit Late Chunking genauso gut ab wie fortgeschrittene Grenzerkennungsalgorithmen. Wir haben drei verschiedene Gr√∂√üen von Embedding-Modellen getestet, und die Ergebnisse zeigen, dass alle von ihnen √ºber alle Testdatens√§tze hinweg konsistent von Late Chunking profitieren. Allerdings bleibt das Embedding-Modell selbst der wichtigste Faktor f√ºr die Leistung ‚Äì <strong>es gibt keinen einzigen Fall, in dem ein schw√§cheres Modell mit Late Chunking ein st√§rkeres Modell ohne Late Chunking √ºbertrifft.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Scatter plot chart showing the percentage of relative improvements across various models against a baseline, with a vertical \" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">Relative Verbesserung des Retrievals gegen√ºber der Baseline (d. h. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> mit festen Token-L√§ngen-Grenzhinweisen und naivem Chunking). Als Teil einer Ablationsstudie testeten wir Late Chunking mit verschiedenen Grenzhinweisen (feste Token-L√§nge, Satzgrenzen und semantische Grenzen) und verschiedenen Modellen (</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">). Basierend auf ihrer Leistung bei MTEB ist die Rangfolge dieser drei Embedding-Modelle: </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">. Der Fokus dieses Experiments liegt jedoch nicht darauf, die Leistung der Embedding-Modelle selbst zu bewerten, sondern zu verstehen, wie ein besseres Embedding-Modell mit Late Chunking und Boundary Cues interagiert. F√ºr Details zum Experiment verweisen wir auf unser Forschungspapier.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Beachten Sie, dass Widerstandsf√§higkeit gegen√ºber schlechten Grenzen nicht bedeutet, dass wir sie ignorieren k√∂nnen ‚Äì sie sind immer noch wichtig f√ºr die Lesbarkeit sowohl f√ºr Menschen als auch LLMs. Unsere Sichtweise ist: Bei der Optimierung der Segmentierung, also dem zuvor genannten 1. Problem, k√∂nnen wir uns vollst√§ndig auf die Lesbarkeit konzentrieren, ohne uns Sorgen um semantischen/kontextuellen Verlust zu machen. Late Chunking kommt mit guten oder schlechten Breakpoints zurecht, sodass Sie sich nur um die Lesbarkeit k√ºmmern m√ºssen.</p>\n\n<h2 id=\"late-chunking-is-bidirectional\">Late Chunking ist bidirektional</h2>\n\n<p>Ein weiteres h√§ufiges Missverst√§ndnis √ºber Late Chunking ist, dass seine bedingten Chunk-Embeddings nur von vorherigen Chunks abh√§ngen, ohne \"nach vorne zu schauen\". Dies ist falsch. Die bedingte Abh√§ngigkeit in <strong>Late Chunking ist tats√§chlich bi-direktional</strong>, nicht uni-direktional. Der Grund daf√ºr ist, dass die Aufmerksamkeitsmatrix im Embedding-Modell ‚Äì einem reinen Encoder-Transformer ‚Äì vollst√§ndig verbunden ist, anders als die maskierte Dreiecksmatrix in auto-regressiven Modellen. Formal ausgedr√ºckt: Das Embedding von Chunk $k$, $v_k \\sim Q(c_k|D)$, statt $v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$, wobei $Q$ eine Faktorisierung des Sprachmodells bezeichnet. Dies erkl√§rt auch, warum Late Chunking nicht von der genauen Platzierung der Grenzen abh√§ngt.</p>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagramme eines Transformer-Modells mit detailliertem Encoder links und Decoder rechts, beschriftet mit Tokens, Embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">Anders als Decoder-only Modelle mit maskierter Selbstaufmerksamkeit sind Embedding-Modelle typischerweise Encoder-only mit einer vollst√§ndigen Aufmerksamkeitsmatrix. Das bedeutet, dass jedes Token-Embedding von allen anderen Tokens innerhalb desselben Kontextfensters abh√§ngig ist, was im Fall von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">, bis zu 8191 andere Tokens einschlie√üt. Folglich tr√§gt das Chunk-Embedding globale Kontextinformationen in beide Richtungen.</span></figcaption></figure>\n\n<h2 id=\"late-chunking-can-be-trained\">Late Chunking kann trainiert werden</h2>\n\n<p>Late Chunking erfordert <em>kein</em> zus√§tzliches Training f√ºr Embedding-Modelle. Es kann auf jedes Embedding-Modell mit langem Kontext angewendet werden, das Mean Pooling verwendet, was es f√ºr Praktiker sehr attraktiv macht. Wenn Sie allerdings an Aufgaben wie Frage-Antwort oder Query-Dokument-Retrieval arbeiten, kann die Leistung durch Fine-tuning noch weiter verbessert werden. Konkret besteht das Trainingsdatum aus Tupeln, die Folgendes enthalten:</p>\n\n<ul>\n<li>Eine <strong>Query</strong> (z.B. eine Frage oder ein Suchbegriff)</li>\n<li>Ein <strong>Dokument</strong>, das relevante Informationen zur Beantwortung der Query enth√§lt</li>\n<li>Eine <strong>relevante Textpassage</strong> innerhalb des Dokuments, die den spezifischen Textabschnitt darstellt, der die Query direkt beantwortet</li>\n</ul>\n\n<p>Das Modell wird trainiert, indem Queries mit ihren relevanten Textpassagen gepaart werden, unter Verwendung einer kontrastiven Verlustfunktion wie InfoNCE. Dies stellt sicher, dass relevante Textpassagen im Embedding-Raum eng mit der Query ausgerichtet sind, w√§hrend nicht verwandte Passagen weiter entfernt werden. Dadurch lernt das Modell, sich bei der Generierung von Chunk-Embeddings auf die relevantesten Teile des Dokuments zu konzentrieren. <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Weitere Details finden Sie in unserem Forschungspapier.</a></p>\n\n<h2 id=\"late-chunking-vs-contextual-retrieval\">Late Chunking vs. Contextual Retrieval</h2>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Einf√ºhrung in Contextual Retrieval</div><div class=\"kg-bookmark-description\">Anthropic ist ein KI-Sicherheits- und Forschungsunternehmen, das daran arbeitet, zuverl√§ssige, interpretierbare und steuerbare KI-Systeme zu entwickeln.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n\n<p>Kurz nach der Einf√ºhrung von Late Chunking stellte Anthropic eine separate Strategie namens <strong>Contextual Retrieval</strong> vor. Anthropics Methode ist ein Brute-Force-Ansatz zur L√∂sung des Problems des verlorenen Kontexts und funktioniert wie folgt:</p>\n\n<ol>\n<li>Jeder Chunk wird zusammen mit dem vollst√§ndigen Dokument an das LLM gesendet</li>\n<li>Das LLM f√ºgt jedem Chunk relevanten Kontext hinzu</li>\n<li>Dies f√ºhrt zu reichhaltigeren und informativeren Embeddings</li>\n</ol>\n\n<p>Unserer Ansicht nach ist dies im Wesentlichen <strong>Kontextanreicherung</strong>, bei der globaler Kontext explizit mittels eines LLM in jeden Chunk eincodiert wird, was in Bezug auf <strong>Kosten</strong>, <strong>Zeit</strong> und <strong>Speicherplatz</strong> teuer ist. Au√üerdem ist unklar, ob dieser Ansatz widerstandsf√§hig gegen√ºber Chunk-Grenzen ist, da das LLM auf genaue und lesbare Chunks angewiesen ist, um den Kontext effektiv anzureichern. Im Gegensatz dazu ist Late Chunking, wie oben gezeigt, hochgradig widerstandsf√§hig gegen√ºber Boundary Cues. Es erfordert keinen zus√§tzlichen Speicherplatz, da die Embedding-Gr√∂√üe gleich bleibt. Obwohl es die volle Kontextl√§nge des Embedding-Modells nutzt, <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">ist es immer noch deutlich schneller als die Verwendung eines LLM zur Generierung von Anreicherungen</a>. In der qualitativen Studie unseres Forschungspapiers <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">zeigen wir, dass Anthropics Context Retrieval √§hnlich wie Late Chunking funktioniert.</a> Late Chunking bietet jedoch eine grundlegendere, generischere und nat√ºrlichere L√∂sung, indem es die inh√§renten Mechanismen des Encoder-only Transformers nutzt.</p>\n\n<h2 id=\"which-embedding-models-support-late-chunking\">Welche Embedding-Modelle unterst√ºtzen Late Chunking?</h2>\n\n<p>Late Chunking ist nicht exklusiv f√ºr <code>jina-embeddings-v3</code> oder <code>v2</code>. Es ist ein ziemlich generischer Ansatz, der auf jedes Embedding-Modell mit langem Kontext angewendet werden kann, das Mean Pooling verwendet. Zum Beispiel zeigen wir in diesem Beitrag, dass auch <code>nomic-v1</code> es unterst√ºtzt. Wir begr√º√üen alle Embedding-Anbieter herzlich, die Unterst√ºtzung f√ºr Late Chunking in ihren L√∂sungen zu implementieren.</p>\n\n<p>Als Modellanwender k√∂nnen Sie diese Schritte befolgen, um zu pr√ºfen, ob ein neues Embedding-Modell oder eine API Late Chunking unterst√ºtzen k√∂nnte:</p>\n\n<ol><li><strong>Single Output</strong>: Liefert das Modell/die API nur ein finales Embedding pro Satz anstelle von Token-Level-Embeddings? Falls ja, dann kann es wahrscheinlich kein Late Chunking unterst√ºtzen (besonders bei Web-APIs).</li><li><strong>Long-Context Support</strong>: Kann das Modell/die API Kontexte von mindestens 8192 Tokens verarbeiten? Falls nein, ist Late Chunking nicht anwendbar ‚Äì oder genauer gesagt, es macht keinen Sinn, Late Chunking f√ºr ein Modell mit kurzem Kontext zu adaptieren. Falls ja, stellen Sie sicher, dass es tats√§chlich gut mit langen Kontexten funktioniert und nicht nur behauptet, diese zu unterst√ºtzen. Diese Informationen finden Sie normalerweise im technischen Bericht des Modells, wie etwa in Evaluierungen auf LongMTEB oder anderen Long-Context-Benchmarks.</li><li><strong>Mean Pooling</strong>: Pr√ºfen Sie bei selbst gehosteten Modellen oder APIs, die Token-Level-Embeddings vor dem Pooling bereitstellen, ob die Standard-Pooling-Methode Mean Pooling ist. Modelle, die CLS oder Max Pooling verwenden, sind nicht kompatibel mit Late Chunking.</li></ol><p>Zusammenfassend l√§sst sich sagen, wenn ein Embedding-Modell Long-Context unterst√ºtzt und standardm√§√üig Mean Pooling verwendet, kann es problemlos Late Chunking unterst√ºtzen. Schauen Sie sich unser <a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">GitHub-Repository f√ºr Implementierungsdetails und weitere Diskussionen</a> an.</p><h2 id=\"conclusion\">Fazit</h2><p>Was ist also Late Chunking? Late Chunking ist eine unkomplizierte Methode zur Generierung von Chunk-Embeddings unter Verwendung von Long-Context-Embedding-Modellen. Es ist schnell, unempfindlich gegen√ºber Boundary-Cues und hocheffektiv. Es ist keine Heuristik oder Over-Engineering ‚Äì es ist ein durchdachtes Design, das auf einem tiefen Verst√§ndnis des Transformer-Mechanismus basiert.</p><p>Der Hype um LLMs ist heute unbestreitbar. In vielen F√§llen werden Probleme, die effizient von kleineren Modellen wie BERT gel√∂st werden k√∂nnten, stattdessen an LLMs weitergegeben, getrieben von der Anziehungskraft gr√∂√üerer, komplexerer L√∂sungen. Es ist keine √úberraschung, dass gro√üe LLM-Anbieter f√ºr eine st√§rkere Nutzung ihrer Modelle werben, w√§hrend Embedding-Anbieter f√ºr Embeddings eintreten ‚Äì beide spielen ihre kommerziellen St√§rken aus. Aber letztendlich geht es nicht um den Hype, sondern um das Handeln, darum, was wirklich funktioniert. Lassen Sie die Community, die Industrie und vor allem die Zeit zeigen, welcher Ansatz wirklich schlanker, effizienter und nachhaltiger ist.</p><p>Lesen Sie unbedingt <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">unser Forschungspapier</a>, und wir ermutigen Sie, Late Chunking in verschiedenen Szenarien zu testen und uns Ihr Feedback mitzuteilen.</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "Teil 2 unserer Untersuchung zum Late Chunking - ein tiefer Einblick, warum es die beste Methode f√ºr Chunk-Embeddings und die Verbesserung der Such- und RAG-Performance ist.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
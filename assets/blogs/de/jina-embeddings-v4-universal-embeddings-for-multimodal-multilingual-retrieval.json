{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4: Universelle Vektor Modelle (Embeddings) für Multimodale, Mehrsprachige Suche",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Search Foundation Models</div><div class=\"kg-bookmark-description\">Universelles Einbettungsmodell für multimodalen und mehrsprachigen Abruf</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Search Foundation Models</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universelle Einbettungen für multimodalen mehrsprachigen Abruf</div><div class=\"kg-bookmark-description\">Wir stellen jina-embeddings-v4 vor, ein multimodales Einbettungsmodell mit 3,8 Milliarden Parametern, das Text- und Bilddarstellungen durch eine neuartige Architektur vereinheitlicht, die sowohl Einzelvektor- als auch Multivektoreinbettungen im Late-Interaction-Stil unterstützt. Das Modell integriert aufgabenspezifische Low-Rank Adaptation (LoRA)-Adapter, um die Leistung in verschiedenen Abrufszenarien zu optimieren, darunter abfragebasierte Informationsbeschaffung, crossmodale semantische Ähnlichkeit und Programmiercode-Suche. Umfassende Auswertungen zeigen, dass jina-embeddings-v4 eine hochmoderne Leistung sowohl bei Single-Modal- als auch bei Cross-Modal-Abrufaufgaben erzielt, mit besonderer Stärke bei der Verarbeitung von visuell reichhaltigen Inhalten wie Tabellen, Diagrammen, Grafiken und Mixed-Media-Formaten. Um die Evaluierung dieser Fähigkeit zu erleichtern, stellen wir auch Jina-VDR vor, einen neuartigen Benchmark, der speziell für den visuell reichhaltigen Bildabruf entwickelt wurde.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">Wir sind auf dem Weg, künstliche Intelligenz durch Open Source und Open Science voranzutreiben und zu demokratisieren.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Heute veröffentlichen wir <code>jina-embeddings-v4</code>, unser neues universelles 向量模型 (Embedding)-Modell mit 3,8 Milliarden Parametern für Text und Bilder. Es enthält eine Reihe von aufgabenspezifischen LoRA-Adaptern, die die Leistung für die beliebtesten Abrufaufgaben optimieren, darunter Abfrage-Dokument-Abruf, semantisches Matching und Codesuche. <code>jina-embeddings-v4</code> erzielt hochmoderne Abrufleistungen bei multimodalen und mehrsprachigen Aufgaben in den Benchmarks MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP und ViDoRe, mit besonderer Stärke bei der Verarbeitung von visuell reichhaltigen Inhalten wie Tabellen, Diagrammen, Grafiken und deren Mischung. Das Modell unterstützt sowohl Einzelvektor- als auch Multivektoreinbettungen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Leistung von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> beim visuellen Dokumentabruf und multimodalen Benchmarks. Die Boxplot-Verteilungen zeigen Mittelwerte und Leistungsvariabilität für Einbettungsmodelle in sechs Benchmark-Kategorien: ViDoRe (Vision Document Retrieval), Jina-VDR (umfassender visueller Dokumentabruf), Wikimedia Commons Retrieval (mehrsprachige Dokument-Beschreibungs-Übereinstimmung), GitHub README Retrieval (Code-Dokumentationsabruf), Tweet Stock Retrieval (Finanzdiagrammanalyse) und CLIP Benchmark (allgemeiner Text-zu-Bild-Abruf). Jina-embeddings-v4-Varianten (cyan hervorgehoben) demonstrieren eine hochmoderne Leistung bei visuell reichhaltigen Dokumentaufgaben, wobei die Multivektorversion die höchsten Punktzahlen in spezialisierten visuellen Dokumenten-Benchmarks (90,2 bei ViDoRe, 80,2 bei Jina-VDR) erzielt und gleichzeitig eine wettbewerbsfähige Leistung bei allgemeinen multimodalen Abrufaufgaben (84,1 bei CLIP Benchmark) beibehält. Die Modelle werden nach der durchschnittlichen Leistung innerhalb jeder Benchmark-Kategorie geordnet, wobei einzelne Datenpunkte Punktzahlverteilungen über mehrere Evaluierungsaufgaben hinweg zeigen.</span></figcaption></figure><p><code>jina-embeddings-v4</code> ist unser bisher ehrgeizigstes Einbettungsmodell. Als Open-Source-Modell übertrifft <code>jina-embeddings-v4</code> die führenden proprietären Einbettungsmodelle der großen Anbieter und liefert eine um 12 % bessere Leistung als <code>text-embedding-3-large</code> von OpenAI beim mehrsprachigen Abruf (66,49 vs. 59,27), eine um 28 % bessere Leistung bei Aufgaben mit langen Dokumenten (67,11 vs. 52,42), 15 % besser als <code>voyage-3</code> beim Code-Abruf (71,59 vs. 67,23) und entspricht der Leistung von <code>gemini-embedding-001</code> von Google. Dies macht v4 zum leistungsfähigsten Open-Source-Universal-Einbettungsmodell, das heute verfügbar ist und Forschern und Entwicklern multimodale Einbettungsfunktionen in Unternehmensqualität mit vollständiger Transparenz hinsichtlich des Trainingsprozesses, der Architekturentscheidungen und der Modellgewichte durch <a href=\"https://arxiv.org/abs/2506.18902\">unseren umfassenden technischen Bericht bietet.</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Leistung von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> über fünf Abruf-Benchmarks. Das Diagramm zeigt Boxplot-Verteilungen mit Mittelwerten für jedes Modell über die Benchmarks Text Retrieval, Code Retrieval, Multilingual Retrieval, Long Context Retrieval und Semantic Textual Similarity (STS). </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (cyan hervorgehoben) demonstriert eine wettbewerbsfähige oder hochmoderne Leistung über alle Bewertungskategorien hinweg, mit besonders starken Ergebnissen bei Textabruf und STS. Die Modelle werden nach der durchschnittlichen Leistung innerhalb jeder Benchmark-Kategorie geordnet, wobei einzelne Datenpunkte Punktzahlverteilungen über mehrere Evaluierungsaufgaben hinweg zeigen.</span></figcaption></figure><h2 id=\"new-architecture\">Neue Architektur</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Architektur von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">. Das Modell basiert auf dem </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> Backbone (3,8 Milliarden Parameter). Text- und Bildeingaben werden über einen gemeinsamen Pfad verarbeitet: Bilder werden zuerst über einen Vision Encoder in Token-Sequenzen konvertiert, dann werden beide Modalitäten gemeinsam vom Sprachmodell-Decoder mit kontextbezogenen Aufmerksamkeits-Layern verarbeitet. Drei aufgabenspezifische LoRA-Adapter (jeweils 60 Millionen Parameter) bieten eine spezialisierte Optimierung für Abruf-, Text-Matching- und Codeaufgaben, ohne die eingefrorenen Backbone-Gewichte zu verändern. Die Architektur unterstützt zwei Ausgabemodi: (1) Einzelvektor-Einbettungen (2048 Dimensionen, auf 128 trunkierbar), die über Mean Pooling für eine effiziente Ähnlichkeitssuche generiert werden, und (2) Multivektor-Einbettungen (128 Dimensionen pro Token) über Projektions-Layer für Late-Interaction-Abrufstrategien.</span></figcaption></figure><p>Das Upgrade von <code>jina-embeddings-v3</code> zu<code>jina-embeddings-v4</code> stellt einen Paradigmenwechsel von reinen Text- zu multimodalen Vektormodellen (Embeddings) dar. Während sich v3 auf die Optimierung von Text-Vektormodellen (Embeddings) mit aufgabenspezifischen LoRA-Adaptern konzentrierte, adressiert v4 die wachsende Anforderung, sowohl textuelle als auch visuelle Inhalte in einheitlichen Darstellungen einzubetten.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Aspekt</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Backbone Modell</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>Parameter (Basis)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>Parameter (mit Adaptern)</td>\n<td>572M</td>\n<td>3.8B + 60M pro Adapter</td>\n</tr>\n<tr>\n<td>Modalitäten</td>\n<td>Nur Text</td>\n<td>Text + Bilder (multimodal)</td>\n</tr>\n<tr>\n<td>Maximale Eingabelänge</td>\n<td>8.192 Tokens (词元)</td>\n<td>32.768 Tokens (词元)</td>\n</tr>\n<tr>\n<td>Bildverarbeitung</td>\n<td>Keine</td>\n<td>Bis zu 20 Megapixel, visuell reichhaltige Dokumente</td>\n</tr>\n  <tr>\n<td>Mehrsprachige Unterstützung</td>\n<td>89 Sprachen</td>\n<td>29+ Sprachen</td>\n</tr>\n<tr>\n<td>Vektortypen</td>\n<td>Nur Einzelvektor</td>\n<td>Einzelvektor + Multivektor (späte Interaktion)</td>\n</tr>\n<tr>\n<td>Einzelvektor-Dimensionen</td>\n<td>1024 (MRL auf 32 verkürzbar)</td>\n<td>2048 (MRL auf 128 verkürzbar)</td>\n</tr>\n<tr>\n<td>Multivektor-Dimensionen</td>\n<td>Nicht verfügbar</td>\n<td>128 pro Token (词元)</td>\n</tr>\n<tr>\n<td>Task-LoRA-Spezialisierungen</td>\n<td>• Asymmetrische Suche<br>• Semantische Ähnlichkeit<br>• Klassifizierung<br>• Trennung</td>\n<td>• Asymmetrische Suche<br>• Semantische Ähnlichkeit<br>• Code-Suche</td>\n</tr>\n<tr>\n<td>Trainingsphasen</td>\n<td>3-phasig: Vortraining → Vektormodell (Embedding) Feintuning → Adaptertraining</td>\n<td>2-phasig: Gemeinsames Paar-Training → Aufgabenspezifisches Adaptertraining</td>\n</tr>\n<tr>\n<td>Loss-Funktionen</td>\n<td>InfoNCE, CoSent, Erweiterter Triplet-Loss</td>\n<td>Gemeinsames InfoNCE + KL-Divergenz für Einzel-/Multivektor</td>\n</tr>\n<tr>\n<td>Positionelle Kodierung</td>\n<td>RoPE (rotatorische Basisfrequenzabstimmung)</td>\n<td>M-RoPE (Multimodales rotatorisches Position Embedding)</td>\n</tr>\n<tr>\n<td>Cross-modale Verarbeitung</td>\n<td>N/A</td>\n<td>Einheitlicher Encoder (reduzierter Modalitätsabstand)</td>\n</tr>\n<tr>\n<td>MRL Unterstützung</td>\n<td>Ja</td>\n<td>Ja</td>\n</tr>\n<tr>\n<td>Attention Implementierung</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">Backbone</h3><p>Die bedeutendste architektonische Änderung in v4 ist die Änderung des Backbones von <code>XLM-RoBERTa</code> zu <code>Qwen2.5-VL-3B-Instruct</code>. Diese Entscheidung wurde durch das Kernziel von v4 getrieben, ein universelles Vektormodell (Embedding) zu erstellen, das \"echte multimodale Verarbeitung\" ermöglicht, bei der Bilder in Token (词元)-Sequenzen umgewandelt und zusammen mit Text verarbeitet werden, wodurch die <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">Modalitätslücke</a> beseitigt wird, die in Dual-Encoder-Architekturen vorhanden ist.</p><p>Die Auswahl des Backbones steht im Einklang mit mehreren wichtigen Designzielen: Die Exzellenz von Qwen2.5-VL im Dokumentenverständnis unterstützt direkt die Stärke von v4 bei der Verarbeitung visuell reichhaltiger Inhalte wie Tabellen, Diagramme und Screenshots. Die dynamischen Auflösungsfunktionen ermöglichen es v4, Bilder zu verarbeiten, die auf 20 Megapixel skaliert wurden, wie in der Architektur angegeben. Die fortschrittliche positionelle Kodierung bietet die Grundlage, die es v4 ermöglicht, eine überlegene cross-modale Ausrichtung mit einem Alignment-Score von 0,71 im Vergleich zu 0,15 für OpenAI CLIP zu erreichen.</p><h3 id=\"lora-adapters\">LoRA Adapters</h3><p>V4 optimiert von den fünf Aufgaben von v3 auf drei fokussierte Aufgaben, was die gewonnenen Erkenntnisse über Effektivität und Benutzerakzeptanz widerspiegelt:</p><ul><li><strong>Asymmetrische Suche</strong> (Konsolidierung der Abfrage-/Passagenadapter von v3)</li><li><strong>Symmetrische Ähnlichkeit</strong> (das Text-Matching-Äquivalent von v3 für STS-Aufgaben)</li><li><strong>Code-Suche</strong> (gelernt von v2-code, fehlend in v3)</li></ul><p>Diese Konsolidierung entfernt die Klassifizierungs- und Trennungsadapter von v3 und konzentriert v4 auf die wirkungsvollsten Anwendungsfälle für Vektormodelle (Embeddings) - Suche und STS.</p><h3 id=\"output-embeddings\">Output Embeddings</h3><p>V4 führt ein Dual-Output-System ein, das sowohl Einzelvektor- als auch Multivektor-Vektormodelle (Embeddings) unterstützt, während v3 nur Einzelvektor-Outputs bereitstellte. Dies adressiert verschiedene Suchszenarien:</p><ul><li><strong>Einzelvektor-Modus</strong>: 2048-dimensionale Vektormodelle (Embeddings) (über MRL auf 128 verkürzbar) für effiziente Ähnlichkeitssuche</li><li><strong>Multivektor-Modus</strong>: 128 Dimensionen pro Token (词元) für <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">Late-Interaction Retrieval</a></li></ul><p>Dieser duale Ansatz bietet eine höhere Effektivität mit Multivektor-Darstellungen, insbesondere bei der Suche in visuell reichhaltigen Dokumenten, und behält gleichzeitig die Effizienz für Standard-Ähnlichkeitsaufgaben bei. Der konsistente Leistungsvorteil von 7-10 % von Multivektor gegenüber Einzelvektor im visuellen Aufgabenbereich deutet darauf hin, dass die späte Interaktion eine grundlegend bessere semantische Übereinstimmung für multimodale Inhalte bietet.</p><h3 id=\"parameter-size\">Parameter Size</h3><p>Während v4 6,7-mal größer ist als v3 (3,8B vs. 570M Parameter), sind die Leistungsverbesserungen bei reinen Texten tatsächlich gering, was darauf hindeutet, dass die Parameterskalierung in erster Linie durch multimodale Anforderungen und nicht durch Textverbesserung bedingt war. Auf den wichtigsten Text-Benchmarks erreicht v4 66,49 auf MMTEB im Vergleich zu 58,58 von v3 (14 % Verbesserung) und 55,97 auf MTEB-EN gegenüber 54,33 von v3 (3 % Verbesserung). Für die Code-Suche erzielt v4 71,59 auf CoIR im Vergleich zu 55,07 von v3 (30 % Verbesserung), während die Leistung bei langen Dokumenten v4 mit 67,11 gegenüber 55,66 von v3 auf LongEmbed zeigt (21 % Verbesserung). Die erhebliche Skalierung wird gerechtfertigt, wenn man die multimodalen Fähigkeiten von v4 berücksichtigt: Erreichen von 84,11 nDCG@5 bei der Suche nach visuellen Dokumenten (Jina-VDR) und 90,17 bei ViDoRe-Benchmarks - Fähigkeiten, die in v3 vollständig fehlen. Die Parametererhöhung stellt somit unsere Investition in die multimodale Funktionalität dar, während die Textleistung wettbewerbsfähig bleibt, wobei die einheitliche Architektur die Notwendigkeit separater Text- und Visionsmodelle eliminiert und gleichzeitig eine cross-modale Ausrichtung von 0,71 im Vergleich zu 0,15 für traditionelle Dual-Encoder-Ansätze erreicht wird.</p><h2 id=\"getting-started\">Erste Schritte</h2><p>Für einen schnellen Vibe-Check probieren Sie unsere Text-zu-Bild-Demo in der Search Foundation-Toolbox aus. Wir haben eine Sammlung von Dokumentenbildern von unserer Website vorbereitet, und Sie können auch Ihre eigenen Bild-URLs hinzufügen. Geben Sie einfach Ihre Abfrage ein und drücken Sie die Eingabetaste, um die sortierten Ergebnisse anzuzeigen. Sie können sie entweder wie OCR oder inhaltsbasierte Bildersuche zurückziehen - Sie können auch Abfragen in nicht-englischer Sprache ausprobieren.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Die Demo ist verfügbar unter: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> Bitte beachten Sie, dass die Verwendung dieser Demo die Tokens (词元) Ihres primären API-Schlüssels verbraucht. Auch die Demo scheint etwas langsam zu sein, da sie alle Bilder auf dem Server von diesen URLs herunterladen muss und kein Cache für Bilder implementiert ist.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">Via API</h3><p>Der folgende Code zeigt, wie <code>jina-embeddings-v4</code> verwendet wird. Sie können eine Textzeichenfolge, ein Base64-kodiertes Bild oder eine Bild-URL übergeben. Neue Benutzer können einen Jina API-Schlüssel mit 10 Millionen kostenlosen Tokens (词元) erhalten.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>Aufgrund begrenzter GPU-Ressourcen unterstützt unsere Embedding API derzeit Dokumente mit einer Länge von bis zu 8.000 Tokens (Tokens), obwohl <code>jina-embeddings-v4</code> nativ bis zu 32.000 Tokens verarbeiten kann. Für Anwendungen, die längere Kontexte als 8.000 Tokens erfordern (wie z. B. <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>), empfehlen wir, unsere Modelle über CSPs bereitzustellen oder das Modell selbst zu hosten.</p><h3 id=\"via-csp-marketplaces\">Über CSP-Marktplätze</h3><p><code>jina-embeddings-v4</code> wird in Kürze direkt auf AWS, Azure und GCP zu den dort aufgeführten Preisen verfügbar sein.</p><h3 id=\"via-huggingface\">Über HuggingFace</h3><p>Für Forschungs- und Experimentierzwecke können Sie das Modell lokal von unserer Hugging Face-Seite aus verwenden. Wir haben ein Google Colab-Notebook vorbereitet, das die Funktionsweise demonstriert.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Fazit</h2><p><code>jina-embeddings-v4</code> stellt unseren bisher bedeutendsten Sprung dar – ein universelles 向量模型 (Embedding)-Modell mit 3,8 Milliarden Parametern, das Text und Bilder über einen einheitlichen Pfad verarbeitet und sowohl dichte als auch Late-Interaction-Retrieval unterstützt, während es proprietäre Modelle von Google, OpenAI und Voyage AI insbesondere beim visuell ansprechenden Dokumentenabruf übertrifft. Diese Fähigkeit entstand jedoch nicht isoliert; sie ist der Höhepunkt von vier Generationen der Lösung grundlegender Einschränkungen.</p><p>Als wir Anfang 2022 mit <code>jina-embeddings-v1</code> begannen, ging jeder davon aus, dass mehr Daten eine bessere Leistung bedeuten. Wir haben das Gegenteil bewiesen – das Filtern von 1,5 Milliarden Paaren auf 385 Millionen hochwertige Beispiele übertraf viel größere Datensätze. Die Lektion: Kuration schlägt Sammlung.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model’s awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Aber die Benutzer stießen immer wieder auf die 512-Token-Grenze von BERT. Das Training mit längeren Sequenzen schien teuer, bis <code>jina-embeddings-v2</code> eine elegante Lösung offenbarte: kurz trainieren, lang bereitstellen. Die linearen Aufmerksamkeitsverzerrungen von ALiBi ermöglichen es Modellen, die auf 512 Tokens trainiert wurden, nahtlos 8.192 Tokens bei der Inferenz zu verarbeiten. Wir haben mehr Leistung für weniger Rechenaufwand bekommen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI’s proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Der Erfolg von <code>jina-embeddings-v2</code> legte eine weitere Einschränkung offen – verschiedene Aufgaben erforderten unterschiedliche Optimierungen. Anstatt separate Modelle zu erstellen, verwendete <code>jina-embeddings-v3</code> winzige 60M LoRA-Adapter, um ein 570M-Basismodell für jede Aufgabe anzupassen. Aus einem Modell wurden fünf spezialisierte Modelle.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Selbst mit Aufgabenspezialisierung blieben wir auf Text beschränkt, während Benutzer ein visuelles Verständnis benötigten. Die Standard-CLIP-basierten Modelle wie <code>jina-clip-v1</code> und <code>jina-clip-v2</code> verwenden separate Encoder, wodurch eine \"Modalitätslücke\" entsteht, in der ähnliche Inhalte in verschiedenen Formaten weit voneinander entfernt landen. Wie unser kürzlich veröffentlichter <code>jina-reranker-m0</code>, eliminierte <code>jina-embeddings-v4</code> dies vollständig – ein einheitlicher Pfad verarbeitet alles und beseitigt die Lücke, anstatt sie zu überbrücken.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Sowohl <code>jina-embeddings-v4</code> als auch <code>jina-reranker-m0</code> teilen eine grundlegende Verschiebung: die Verwendung von LLMs (LLM) als Backbones anstelle von reinen Encoder-Modellen. Dies ist kein Zufall – es spiegelt einen tiefgreifenden Vorteil wider, den die meisten übersehen: Reine Encoder-Modelle erzeugen \"Modalitätslücken\", in denen Bilder getrennt von Text gruppiert werden. Die reinen Decoder-Modelle eröffnen Möglichkeiten, die mit reinen Encoder-Architekturen nicht erreichbar waren, einschließlich echter Mixed-Modalitäts-Darstellung und Erklärbarkeit.</p><p>Unsere wichtigste Erkenntnis: Sowohl bei Vektor Modellen (Embeddings) als auch bei der Generierung geht es um das Verständnis von Semantik. LLMs, die sich durch Generierung auszeichnen, zeichnen sich naturgemäß auch durch Repräsentation aus. Wir glauben, dass die Zukunft in einheitlichen Architekturen liegt, in denen Vektor Modelle (Embedding) und das Reranking aus <strong>demselben Suchgrundlagenmodell</strong> hervorgehen – und genau darauf arbeitet Jina AI hin.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina Embeddings v4 ist ein universelles 向量模型 (Embeddings)-Modell mit 3,8 Milliarden Parametern für multimodale und mehrsprachige Suche, das sowohl Single-Vektor- als auch Multi-Vektor-Embedding-Ausgaben unterstützt.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
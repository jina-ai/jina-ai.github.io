{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "Texteinbettungen erfassen die Wortreihenfolge nicht - und wie man das beheben kann",
  "html": "<p>Kürzlich teilte Christoph Schuhmann, Gründer von <a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> eine interessante Beobachtung über Text-Embedding-Modelle:</p><blockquote>Wenn Wörter innerhalb eines Satzes zufällig umgestellt werden, bleibt die Cosinus-Ähnlichkeit zwischen ihren Text-Embeddings im Vergleich zum ursprünglichen Satz überraschend hoch.</blockquote><p>Betrachten wir zum Beispiel zwei Sätze: <code>Berlin is the capital of Germany</code> und <code>the Germany Berlin is capital of</code>. Obwohl der zweite Satz keinen Sinn ergibt, können Text-Embedding-Modelle sie kaum unterscheiden. Mit <code>jina-embeddings-v3</code> haben diese beiden Sätze einen Cosinus-Ähnlichkeitswert von 0,9295.</p><p>Die Wortstellung ist nicht das einzige, wofür Embeddings anscheinend nicht sehr sensitiv sind. Grammatikalische Transformationen können die Bedeutung eines Satzes dramatisch verändern, haben aber wenig Einfluss auf die Embedding-Distanz. Zum Beispiel haben <code>She ate dinner before watching the movie</code> und <code>She watched the movie before eating dinner</code> eine Cosinus-Ähnlichkeit von 0,9833, obwohl sie die entgegengesetzte Reihenfolge der Handlungen beschreiben.</p><p>Auch Verneinung ist ohne <a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">spezielles Training</a> bekanntermaßen schwierig konsistent einzubetten — <code>This is a useful model</code> und <code>This is not a useful model</code> sehen im Embedding-Raum praktisch gleich aus. Oft verändert das Ersetzen von Wörtern durch andere derselben Klasse, wie etwa \"today\" durch \"yesterday\", oder die Änderung einer Verbzeit, die Embeddings nicht so stark wie man erwarten würde.</p><p>Dies hat ernsthafte Auswirkungen. Betrachten wir zwei Suchanfragen: <code>Flight from Berlin to Amsterdam</code> und <code>Flight from Amsterdam to Berlin</code>. Sie haben fast identische Embeddings, wobei <code>jina-embeddings-v3</code> ihnen eine Cosinus-Ähnlichkeit von 0,9884 zuweist. Für eine reale Anwendung wie Reisesuche oder Logistik ist diese Schwäche fatal.</p><p>In diesem Artikel betrachten wir die Herausforderungen für Embedding-Modelle und untersuchen ihre anhaltenden Schwierigkeiten mit Wortstellung und Wortwahl. Wir analysieren wichtige Fehlerarten in verschiedenen linguistischen Kategorien — einschließlich direktionaler, zeitlicher, kausaler, vergleichender und verneinender Kontexte — und erforschen Strategien zur Verbesserung der Modellleistung.</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">Warum haben umgestellte Sätze überraschend ähnliche Cosinus-Werte?</h2><p>Zunächst dachten wir, dies könnte daran liegen, wie das Modell Wortbedeutungen kombiniert - es erstellt ein Embedding für jedes Wort (6-7 Wörter in jedem unserer obigen Beispielsätze) und mittelt diese Embeddings dann durch Mean Pooling. Das bedeutet, dass dem finalen Embedding nur sehr wenig Information über die Wortstellung zur Verfügung steht. Ein Durchschnitt ist derselbe, egal in welcher Reihenfolge die Werte stehen.</p><p>Allerdings haben selbst Modelle, die CLS Pooling verwenden (die ein spezielles erstes Wort betrachten, um den ganzen Satz zu verstehen und sensitiver für die Wortstellung sein sollten), dasselbe Problem. Zum Beispiel gibt <code>bge-1.5-base-en</code> immer noch eine Cosinus-Ähnlichkeit von 0,9304 für die Sätze <code>Berlin is the capital of Germany</code> und <code>the Germany Berlin is capital of</code>.</p><p>Dies deutet auf eine Einschränkung in der Art und Weise hin, wie Embedding-Modelle trainiert werden. Während Sprachmodelle zunächst während des Pre-trainings die Satzstruktur lernen, scheinen sie einen Teil dieses Verständnisses während des kontrastiven Trainings zu verlieren — dem Prozess, den wir verwenden, um Embedding-Modelle zu erstellen.</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">Wie beeinflussen Textlänge und Wortstellung die Embedding-Ähnlichkeit?</h2><p>Warum haben Modelle überhaupt Probleme mit der Wortstellung? Das Erste, was einem in den Sinn kommt, ist die Länge (in Tokens) des Textes. Wenn Text an die Encoding-Funktion gesendet wird, generiert das Modell zunächst eine Liste von Token-Embeddings (d.h., jedes tokenisierte Wort hat einen eigenen Vektor, der seine Bedeutung repräsentiert) und mittelt diese dann.</p><p>Um zu sehen, wie Textlänge und Wortstellung die Embedding-Ähnlichkeit beeinflussen, haben wir einen <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">Datensatz von 180 synthetischen Sätzen</a> verschiedener Längen erstellt, wie etwa 3, 5, 10, 15, 20 und 30 Tokens. Wir haben auch die Tokens zufällig umgestellt, um eine Variation jedes Satzes zu erstellen:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Hier sind einige Beispiele:</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">Länge (Tokens)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">Originalsatz</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">Umgestellter Satz</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Wir werden den Datensatz mit unserem eigenen <code>jina-embeddings-v3</code> Modell und dem Open-Source-Modell <code>bge-base-en-v1.5</code> encodieren und dann die Cosinus-Ähnlichkeit zwischen dem Original und dem umgestellten Satz berechnen:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Länge (Tokens)</th>\n<th>Mittlere Cosinus-Ähnlichkeit</th>\n<th>Standardabweichung der Cosinus-Ähnlichkeit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0,947</td>\n<td>0,053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0,909</td>\n<td>0,052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0,924</td>\n<td>0,031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0,918</td>\n<td>0,019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0,899</td>\n<td>0,021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0,874</td>\n<td>0,025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Wir können nun ein Box-Plot erstellen, das den Trend in der Cosinus-Ähnlichkeit deutlicher macht:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Ähnlichkeitsverteilung nach Satzlänge für umgestellte Sätze mit </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (unfeinetuned)</span></figcaption></figure><p>Wie wir sehen können, gibt es eine klare lineare Beziehung in der durchschnittlichen Cosinus-Ähnlichkeit der Embeddings. Je länger der Text, desto niedriger ist der durchschnittliche Cosinus-Ähnlichkeitswert zwischen den originalen und zufällig umgestellten Sätzen. Dies geschieht wahrscheinlich aufgrund der \"Wortverschiebung\", nämlich wie weit sich Wörter nach der zufälligen Umstellung von ihren ursprünglichen Positionen entfernt haben. In einem kürzeren Text gibt es einfach weniger \"Slots\", in die ein Token verschoben werden kann, sodass es sich nicht so weit bewegen kann, während ein längerer Text eine größere Anzahl möglicher Permutationen hat und Wörter sich weiter bewegen können.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 2: Kombinationen von Sätzen nach Anzahl der Wörter</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Wir beenden die Tabelle hier, da die Anzahl der Kombinationen der Fakultät der Wortanzahl entspricht. Bei dreißig Wörtern erreichen wir bereits 265 Nonillionen (2,652528598 E+32) Kombinationen.</div></div><p>Wie in der folgenden Abbildung (Cosine Similarity vs Average Word Displacement) zu sehen ist, gilt: Je länger der Text, desto größer die Wortverschiebung:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 3: Kosinus-Ähnlichkeit vs. durchschnittliche Wortverschiebung mit gemischtem Satz-Datensatz, der die Korrelation zwischen durchschnittlicher Wortverschiebung und Kosinus-Unähnlichkeit zeigt.</span></figcaption></figure><p>Token-Embeddings sind vom lokalen Kontext abhängig, d.h. von den ihnen am nächsten stehenden Wörtern. Bei einem kurzen Text kann die Umstellung von Wörtern diesen Kontext nicht stark verändern. Bei einem längeren Text hingegen kann ein Wort sehr weit von seinem ursprünglichen Kontext entfernt werden, was sein Token-Embedding erheblich verändern kann. Infolgedessen erzeugt das Mischen der Wörter in einem längeren Text ein stärker abweichendes Embedding als bei einem kürzeren. Die obige Abbildung zeigt, dass sowohl für <code>jina-embeddings-v3</code> mit Mean Pooling als auch für <code>bge-base-en-v1.5</code> mit CLS Pooling die gleiche Beziehung gilt: Das Mischen längerer Texte und das weitere Verschieben von Wörtern führt zu geringeren Ähnlichkeitswerten.</p><h2 id=\"do-bigger-models-solve-the-problem\">Lösen größere Modelle das Problem?</h2><p>Normalerweise ist es bei solchen Problemen eine gängige Taktik, einfach ein größeres Modell einzusetzen. Aber kann ein größeres Text-Embedding-Modell die Wortreihenfolge wirklich effektiver erfassen? Gemäß dem Skalierungsgesetz für Text-Embedding-Modelle (<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">referenziert in unserem <code>jina-embeddings-v3</code> Release-Beitrag</a>) bieten größere Modelle generell bessere Leistung:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 4: Skalierungsgesetz der Embedding-Modelle, zeigt MTEB-Leistungsskalierung mit der Anzahl der Parameter.</span></figcaption></figure><p>Aber kann ein größeres Modell Informationen zur Wortreihenfolge effektiver erfassen? Wir haben drei Varianten des BGE-Modells getestet: <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>, <a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a> und <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>, mit Parametergrößen von 33 Millionen, 110 Millionen bzw. 335 Millionen.</p><p>Wir verwenden die gleichen 180 Sätze wie zuvor, ignorieren aber die Längeninformationen. Wir kodieren sowohl die Originalsätze als auch ihre zufälligen Mischungen mit den drei Modellvarianten und plotten die durchschnittliche Kosinus-Ähnlichkeit:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 5: Einfluss der Modellgröße auf die Wortreihenfolge-Sensitivität mit gemischtem Satz-Datensatz unter Verwendung von </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">, </span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\"> und </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap\">.</span></figcaption></figure><p>Während wir sehen können, dass größere Modelle sensibler auf Variationen in der Wortreihenfolge reagieren, ist der Unterschied gering. Selbst das deutlich größere <code>bge-large-en-v1.5</code> ist nur geringfügig besser darin, gemischte von ungemischten Sätzen zu unterscheiden. Andere Faktoren spielen eine Rolle bei der Bestimmung, wie empfindlich ein Embedding-Modell auf Wortumstellungen reagiert, insbesondere Unterschiede im Training. Darüber hinaus ist die Kosinus-Ähnlichkeit ein sehr begrenztes Werkzeug zur Messung der Unterscheidungsfähigkeit eines Modells. Wir können jedoch erkennen, dass die Modellgröße kein wesentlicher Faktor ist. Wir können das Problem nicht einfach durch ein größeres Modell lösen.</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">Wortreihenfolge und Wortwahl in der realen Welt</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Für einen Großteil dieses Beitrags verwenden wir <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap\">jina-embeddings-v2</code></a> (<i><em class=\"italic\" style=\"white-space: pre-wrap\">nicht</em></i> unser neuestes Modell, <code spellcheck=\"false\" style=\"white-space: pre-wrap\">jina-embeddings-v3</code>), da <code spellcheck=\"false\" style=\"white-space: pre-wrap\">v2</code> mit 137m Parametern deutlich kleiner und damit schneller auf unseren lokalen GPUs zu experimentieren ist als <code spellcheck=\"false\" style=\"white-space: pre-wrap\">v3</code> mit 580m Parametern.</div></div><p>Wie wir in der Einleitung erwähnt haben, ist die Wortreihenfolge nicht die einzige Herausforderung für Embedding-Modelle. Eine realistischere Herausforderung in der realen Welt betrifft die <em>Wortwahl</em>. Es gibt viele Möglichkeiten, Wörter in einem Satz auszutauschen – Möglichkeiten, die sich in den Embeddings nicht gut widerspiegeln. Wir können \"Sie flog von Paris nach Tokio\" zu \"Sie fuhr von Tokio nach Paris\" ändern, und die Embeddings bleiben ähnlich. Wir haben dies über mehrere Kategorien von Änderungen hinweg abgebildet:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Kategorie</th>\n<th>Beispiel - Links</th>\n<th>Beispiel - Rechts</th>\n<th>Kosinus-Ähnlichkeit (<code>jina</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Richtungsbezogen</td>\n<td>Sie flog von Paris nach Tokio</td>\n<td>Sie fuhr von Tokio nach Paris</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>Zeitlich</td>\n<td>Sie aß zu Abend, bevor sie den Film sah</td>\n<td>Sie sah den Film, bevor sie zu Abend aß</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>Kausal</td>\n<td>Die steigende Temperatur schmolz den Schnee</td>\n<td>Der schmelzende Schnee kühlte die Temperatur</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>Vergleichend</td>\n<td>Kaffee schmeckt besser als Tee</td>\n<td>Tee schmeckt besser als Kaffee</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>Verneinung</td>\n<td>Er steht am Tisch</td>\n<td>Er steht weit vom Tisch entfernt</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Beachten Sie, dass dies häufige Fälle sind, die wir während unserer Arbeit beobachtet haben und nicht unbedingt eine umfassende Taxonomie der Kategorien darstellen.</div></div><p>Die obige Tabelle zeigt eine Liste von \"Fehlerfällen\", bei denen ein Text-Embedding-Modell subtile Wortänderungen nicht erfassen kann. Dies entspricht unseren Erwartungen: Text-Embedding-Modellen fehlt die Fähigkeit zum logischen Denken. Zum Beispiel versteht das Modell nicht die Beziehung zwischen \"von\" und \"nach\". Text-Embedding-Modelle führen semantisches Matching durch, wobei die Semantik typischerweise auf Token-Ebene erfasst und nach dem Pooling in einen einzelnen dichten Vektor komprimiert wird. Im Gegensatz dazu <a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">zeigen LLMs (autoregressive Modelle), die auf größeren Datensätzen im Billionen-Token-Bereich trainiert wurden, erste emergente Fähigkeiten zum logischen Denken</a>.</p><p>Das brachte uns auf die Frage: Können wir das Embedding-Modell mit kontrastivem Lernen unter Verwendung von Triplets fein-tunen, um die Abfrage und das Positive näher zusammenzubringen und gleichzeitig die Abfrage und das Negative weiter auseinanderzutreiben?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 6: Die Wirkung des kontrastiven Lernens: Abfrage und Positiv näher zusammenrücken, Negativ weiter entfernen.</span></figcaption></figure><p>Zum Beispiel könnte \"Flug von Amsterdam nach Berlin\" als negatives Paar zu \"Flug von Berlin nach Amsterdam\" betrachtet werden. Tatsächlich haben wir in dem <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v1</code> technischen Bericht</a> (Michael Guenther, et al.) dieses Problem bereits in kleinem Maßstab behandelt: Wir haben das <code>jina-embeddings-v1</code> Modell auf einem <a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">Negations-Datensatz</a> mit 10.000 von Large Language Models generierten Beispielen feingetunt.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Die im obigen Bericht dokumentierten Ergebnisse waren vielversprechend:</p><blockquote>Wir beobachten, dass über alle Modellgrößen hinweg das Fine-Tuning auf Triplet-Daten (einschließlich unseres Negations-Trainingsdatensatzes) die Leistung drastisch verbessert, insbesondere bei der HardNegation-Aufgabe.</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 7: Tabelle mit EasyNegation- und HardNegation-Werten für verschiedene Größen von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> Modellen mit paarweisem und kombiniertem Triplet/paarweisem Training.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 8: Leistungsvergleich von Trainingsstrategien zwischen verschiedenen Versionen von </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">Fine-Tuning von Text-Embedding-Modellen mit kuratierten Datensätzen</h2><p>In den vorherigen Abschnitten haben wir mehrere wichtige Beobachtungen zu Text-Embeddings untersucht:</p><ol><li>Kürzere Texte sind anfälliger für Fehler bei der Erfassung der Wortreihenfolge.</li><li>Die Vergrößerung des Text-Embedding-Modells verbessert nicht unbedingt das Verständnis der Wortreihenfolge.</li><li>Kontrastives Lernen könnte eine potenzielle Lösung für diese Probleme bieten.</li></ol><p>Mit diesem Wissen haben wir <code>jina-embeddings-v2-base-en</code> und <code>bge-base-en-1.5</code> auf unseren Negations- und Wortreihenfolge-Datensätzen feingetunt (insgesamt etwa 11.000 Trainingsbeispiele):</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Zur Evaluierung des Fine-Tunings haben wir einen <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">Datensatz</a> von 1.000 Triplets erstellt, bestehend aus einer <code>query</code>, einem <code>positive (pos)</code> und einem <code>negative (neg)</code> Fall:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Hier ist ein Beispiel einer Zeile:</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Diese Triplets sind so gestaltet, dass sie verschiedene Fehlerfälle abdecken, einschließlich <strong>direktionaler</strong>, <strong>zeitlicher</strong> und <strong>kausaler</strong> Bedeutungsverschiebungen aufgrund von Wortreihenfolgeänderungen.</p><p>Wir können die Modelle nun auf drei verschiedenen Evaluierungssets testen:</p><ol><li>Das Set von 180 synthetischen Sätzen (von früher in diesem Beitrag), zufällig gemischt.</li><li>Fünf manuell überprüfte Beispiele (aus der obigen Richtungs-/Kausal-/etc-Tabelle).</li><li>94 kuratierte Triplets aus unserem gerade erstellten <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">Triplet-Datensatz</a>.</li></ol><p>Hier ist der Unterschied für gemischte Sätze vor und nach dem Fine-Tuning:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Satzlänge (Tokens)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>jina</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>jina-ft</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>bge</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Das Ergebnis scheint eindeutig: Trotz eines nur fünfminütigen Fine-Tuning-Prozesses sehen wir eine dramatische Verbesserung der Leistung auf dem Datensatz mit zufällig gemischten Sätzen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 9: Ähnlichkeitsverteilung nach Satzlänge für gemischte Sätze mit </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (feinabgestimmt).</span></figcaption></figure><p>Wir sehen auch Verbesserungen bei direktionalen, temporalen, kausalen und vergleichenden Fällen. Das Modell zeigt eine erhebliche Leistungsverbesserung, die sich in einem Rückgang der durchschnittlichen Cosinus-Ähnlichkeit widerspiegelt. Die größte Leistungssteigerung zeigt sich beim Negationsfall, da unser Fine-Tuning-Datensatz 10.000 Negationstrainingsbeispiele enthielt.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Kategorie</th>\n<th>Beispiel - Links</th>\n<th>Beispiel - Rechts</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>jina</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>jina-ft</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>bge</code>)</th>\n<th>Mittlere Cosinus-Ähnlichkeit (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Direktional</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>Kausal</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>Vergleichend</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>Negation</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Fazit</h2><p>In diesem Beitrag tauchen wir in die Herausforderungen ein, mit denen Text-Embedding-Modelle konfrontiert sind, insbesondere ihre Schwierigkeiten bei der effektiven Handhabung der Wortreihenfolge. Im Einzelnen haben wir fünf Hauptfehlertypen identifiziert: <strong>Direktional</strong>, <strong>Temporal</strong>, <strong>Kausal</strong>, <strong>Vergleichend</strong> und <strong>Negation</strong>. Dies sind die Arten von Abfragen, bei denen die Wortreihenfolge wirklich wichtig ist, und wenn Ihr Anwendungsfall eine dieser Kategorien beinhaltet, ist es wichtig, die Grenzen dieser Modelle zu kennen.</p><p>Wir haben auch ein schnelles Experiment durchgeführt, bei dem wir einen negationsfokussierten Datensatz um alle fünf Fehlerkategorien erweitert haben. Die Ergebnisse waren vielversprechend: Das Fine-Tuning mit sorgfältig ausgewählten \"harten Negativbeispielen\" verbesserte die Fähigkeit des Modells zu erkennen, welche Elemente zusammengehören und welche nicht. Allerdings gibt es noch mehr zu tun. Zukünftige Schritte beinhalten eine tiefergehende Untersuchung, wie sich die Größe und Qualität des Datensatzes auf die Leistung auswirken.</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "Texteinbettungsmodelle haben Schwierigkeiten damit, subtile sprachliche Nuancen wie Wortstellung, direktionale Beziehungen, zeitliche Abfolgen, kausale Zusammenhänge, Vergleiche und Verneinungen zu erfassen. Das Verständnis dieser Herausforderungen ist der Schlüssel zur Verbesserung der Modellleistung.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
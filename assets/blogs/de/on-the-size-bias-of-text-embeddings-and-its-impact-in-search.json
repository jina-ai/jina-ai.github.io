{
  "slug": "on-the-size-bias-of-text-embeddings-and-its-impact-in-search",
  "id": "67e52df15dcba60001c30ebe",
  "uuid": "bae3e1b8-3bf2-4dbc-a553-b26ea64aeb60",
  "title": "√úber den Gr√∂√üen-Bias von Text-Embeddings und dessen Einfluss auf die Suche",
  "html": "<p>Semantische √Ñhnlichkeit ist das, wof√ºr Embedding-Modelle entwickelt wurden, aber diese Messungen werden von vielen verzerrenden Faktoren beeinflusst. In diesem Artikel untersuchen wir eine weit verbreitete Quelle von Bias in Text-Embedding-Modellen: die Eingabegr√∂√üe.</p><p><strong>Embeddings l√§ngerer Texte zeigen im Vergleich zu anderen Text-Embeddings generell h√∂here √Ñhnlichkeitswerte, unabh√§ngig davon, wie √§hnlich der tats√§chliche Inhalt ist.</strong> W√§hrend wirklich √§hnliche Texte weiterhin h√∂here √Ñhnlichkeitswerte als nicht verwandte Texte aufweisen, f√ºhren l√§ngere Texte zu einer Verzerrung ‚Äî ihre Embeddings erscheinen allein aufgrund ihrer L√§nge im Durchschnitt √§hnlicher.</p><p>Dies hat reale Konsequenzen. Es bedeutet, dass Embedding-Modelle alleine nicht gut in der Lage sind, Relevanz zu messen. Bei Embedding-basierter Suche gibt es immer eine beste √úbereinstimmung, aber die Gr√∂√üenverzerrung bedeutet, dass Sie den √Ñhnlichkeitswert nicht verwenden k√∂nnen, um zu entscheiden, ob die beste √úbereinstimmung oder geringere √úbereinstimmungen tats√§chlich relevant sind. Sie k√∂nnen nicht sagen, dass beispielsweise jede √úbereinstimmung mit einem Kosinus √ºber 0,75 relevant ist, da es leicht ein langes Dokument geben kann, das auf diesem Niveau √ºbereinstimmt, obwohl es v√∂llig irrelevant ist.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Der Vergleich von Embedding-Vektoren kann nur relative √Ñhnlichkeit, nicht aber Relevanz aufzeigen.</div></div><p>Wir werden dies mit einigen einfachen Beispielen demonstrieren und zeigen, warum die Kosinus-√Ñhnlichkeit zwischen Text-Embeddings nicht als allgemeine Methode zur Bewertung dienen kann</p><h2 id=\"visualizing-size-bias\">Visualisierung des Gr√∂√üen-Bias</h2><p>Um zu zeigen, wie sich der Gr√∂√üen-Bias manifestiert, werden wir Jina AIs neuestes Embedding-Modell <code>jina-embeddings-v3</code> mit der Option <code>text-matching</code> verwenden. Wir werden auch Textdokumente aus einem h√§ufig verwendeten IR-Datensatz verwenden: Den <a href=\"https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\">CISI-Datensatz</a>, den Sie von <a href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\">Kaggle herunterladen</a> k√∂nnen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">CISI (a dataset for Information Retrieval)</div><div class=\"kg-bookmark-description\">A public dataset from the University of Glasgow's Information Retrieval Group</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-31.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Kaggle</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/dataset-card.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Dieser Datensatz wird zum Training von IR-Systemen verwendet und enth√§lt sowohl Abfragen als auch Dokumente, die zu ihnen passen. Wir werden nur die Dokumente verwenden, die sich alle in der Datei <code>CISI.ALL</code> befinden. Sie k√∂nnen sie √ºber die Kommandozeile von einer <a href=\"https://github.com/GianRomani/CISI-project-MLOps\">alternativen Quelle auf GitHub</a> mit folgendem Befehl herunterladen:</p><pre><code class=\"language-bash\">wget https://raw.githubusercontent.com/GianRomani/CISI-project-MLOps/refs/heads/main/CISI.ALL\n</code></pre><p>CISI enth√§lt 1.460 Dokumente. Die grundlegenden Statistiken √ºber die Gr√∂√üen der Texte und ihre Gr√∂√üenverteilungen sind in der Tabelle und den Histogrammen unten zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>in Words</th>\n<th>in Sentences</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Average document size</td>\n<td>119.2</td>\n<td>4.34</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>63.3</td>\n<td>2.7</td>\n</tr>\n<tr>\n<td>Max size</td>\n<td>550</td>\n<td>38</td>\n</tr>\n<tr>\n<td>Min size</td>\n<td>8</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Lassen Sie uns die Dokumente in Python einlesen und Embeddings f√ºr sie erstellen. Der folgende Code geht davon aus, dass sich die Datei <code>CISI.ALL</code> im lokalen Verzeichnis befindet:</p><pre><code class=\"language-python\">with open(\"CISI.ALL\", \"r\", encoding=\"utf-8\") as inp:\n    cisi_raw = inp.readlines()\n\ndocs = []\ncurrent_doc = \"\"\nin_text = False\nfor line in cisi_raw:\n    if line.startswith(\".\"):\n        in_text = False\n        if current_doc:\n            docs.append(current_doc.strip())\n            current_doc = \"\"\n        if line.startswith(\".W\"):\n            in_text = True\n    else:\n        if in_text:\n            current_doc += line\n</code></pre><p>Dies f√ºllt die Liste <code>docs</code> mit 1.460 Dokumenten. Sie k√∂nnen sie inspizieren:</p><pre><code class=\"language-text\">print(docs[0])\n\nThe present study is a history of the DEWEY Decimal\nClassification.  The first edition of the DDC was published\nin 1876, the eighteenth edition in 1971, and future editions\nwill continue to appear as needed.  In spite of the DDC's\nlong and healthy life, however, its full story has never\nbeen told.  There have been biographies of Dewey\nthat briefly describe his system, but this is the first\nattempt to provide a detailed history of the work that\nmore than any other has spurred the growth of\nlibrarianship in this country and abroad.</code></pre><p>Nun werden wir Embeddings f√ºr jeden Text mit <code>jina-embeddings-v3</code> erstellen. Daf√ºr ben√∂tigen Sie <a href=\"https://jina.ai/embeddings/#apiform\">einen API-Schl√ºssel von der Jina AI Website</a>. Sie k√∂nnen einen kostenlosen Schl√ºssel f√ºr bis zu 1 Million Token Embeddings erhalten, was f√ºr diesen Artikel ausreicht.</p><p>Legen Sie Ihren Schl√ºssel in einer Variable an:</p><pre><code class=\"language-python\">api_key = \"&lt;Your Key&gt;\"\n</code></pre><p>Generieren Sie jetzt Embeddings mit der <code>text-matching</code>-Aufgabe mit <code>jina-embeddings-v3</code>. Dieser Code verarbeitet die Texte in <code>docs</code> in Batches von 10.</p><pre><code class=\"language-python\">import requests\nimport json\nfrom numpy import array\n\nembeddings  = []\n\nurl = \"https://api.jina.ai/v1/embeddings\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\n\ni = 0\nwhile i &lt; len(docs):\n    print(f\"Got {len(embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": docs[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>F√ºr jeden Text wird es ein 1024-dimensionales Embedding in der Liste <code>embeddings</code> geben. Sie k√∂nnen sehen, wie das aussieht:</p><pre><code class=\"language-python\">print(embeddings[0])\n\narray([ 0.0352382 , -0.00594871,  0.03808545, ..., -0.01147173,\n         -0.01710563,  0.01109511], shape=(1024,))),\n</code></pre><p>Jetzt berechnen wir die Kosinus-√Ñhnlichkeit zwischen allen Paaren von Embeddings. Definieren wir zun√§chst die Kosinus-Funktion <code>cos_sim</code> mit <code>numpy</code>:</p><pre><code class=\"language-python\">from numpy import dot\nfrom numpy.linalg import norm\n\ndef cos_sim(a, b): \n    return float((a @ b.T) / (norm(a)*norm(b)))\n</code></pre><p>Dann berechnen wir die Kosinus-Werte f√ºr jedes der 1.460 Embeddings im Vergleich zu den anderen 1.459:</p><pre><code class=\"language-python\">all_cosines = []\nfor i, emb1 in enumerate(embeddings):\n    for j, emb2 in enumerate(embeddings):\n        if i != j:\n            all_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>Das Ergebnis ist eine Liste von 2.130.140 Werten. Ihre Verteilung sollte die Kosinus-Werte zwischen \"zuf√§lligen\" Dokumenten in derselben Sprache und Register approximieren. Die Tabelle und das Histogramm unten fassen die Ergebnisse zusammen.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>1,460</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>2,130,140</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.343</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Diese Dokumente haben, obwohl sie nicht miteinander verwandt sind, typischerweise Kosinus-Werte deutlich √ºber null. Wir k√∂nnten versucht sein, einen Schwellenwert von 0,459 (Durchschnitt + 1 Standardabweichung) festzulegen, oder ihn vielleicht auf 0,5 aufzurunden, und zu sagen, dass jedes Dokumentenpaar mit einem Kosinus unter diesem Wert weitgehend unzusammenh√§ngend sein muss.</p><p>Aber lassen Sie uns das gleiche Experiment mit kleineren Texten durchf√ºhren. Wir werden die <a href=\"https://www.nltk.org/\" rel=\"noreferrer\"><code>nltk</code></a> Bibliothek verwenden, um jedes Dokument in S√§tze zu zerlegen:</p><pre><code class=\"language-python\">import nltk\n\nsentences = []\nfor doc in docs:\n    sentences.extend(nltk.sent_tokenize(doc)) \n</code></pre><p>Dies ergibt 6.331 S√§tze mit einer durchschnittlichen L√§nge von 27,5 W√∂rtern und einer Standardabweichung von 16,6. Im Histogramm unten ist die Gr√∂√üenverteilung der S√§tze in rot und die der vollst√§ndigen Dokumente in blau dargestellt, sodass Sie sie vergleichen k√∂nnen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"567\" height=\"455\"></figure><p>Wir werden das gleiche Modell und die gleichen Methoden verwenden, um Embeddings f√ºr jeden Satz zu erstellen:</p><pre><code class=\"language-python\">sentence_embeddings = []\n\ni = 0\nwhile i &lt; len(sentences):\n    print(f\"Got {len(sentence_embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": sentences[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        sentence_embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>Berechnen Sie dann den Kosinus zwischen dem Embedding jedes Satzes mit jedem anderen Satz:</p><pre><code class=\"language-python\">sent_cosines = []\nfor i, emb1 in enumerate(sentence_embeddings):\n    for j, emb2 in enumerate(sentence_embeddings):\n        if i != j:\n            sent_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>Das Ergebnis sind deutlich mehr Kosinuswerte: 40.075.230, wie in der folgenden Tabelle zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of sentences</th>\n<th>6,331</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>40,075,230</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.254</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Die Satz-zu-Satz-Kosinuswerte sind im Durchschnitt deutlich niedriger als die vollst√§ndigen Dokument-zu-Dokument-Werte. Das Histogramm unten vergleicht ihre Verteilungen, und man kann deutlich erkennen, dass die Satzpaare eine nahezu identische Verteilung wie die Dokumentenpaare bilden, die jedoch nach links verschoben ist.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Um zu testen, ob diese Gr√∂√üenabh√§ngigkeit robust ist, berechnen wir alle Kosinuswerte zwischen S√§tzen und Dokumenten und f√ºgen sie dem Histogramm hinzu. Ihre Informationen sind in der folgenden Tabelle zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>6,331 sentences &amp; 1,460 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>9,243,260</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.276</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.119</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Die gr√ºne Linie unten zeigt die Verteilung der Satz-zu-Dokument-Kosinuswerte. Wir k√∂nnen sehen, dass diese Verteilung genau zwischen den Dokument-zu-Dokument-Kosinuswerten und den Satz-zu-Satz-Kosinuswerten liegt, was zeigt, dass der Gr√∂√üeneffekt <em>beide</em> Texte betrifft ‚Äì sowohl den gr√∂√üeren als auch den kleineren der verglichenen Texte.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Lassen Sie uns einen weiteren Test durchf√ºhren, indem wir die Dokumente in Gruppen von zehn zusammenf√ºgen, wodurch 146 viel gr√∂√üere Dokumente entstehen, und deren Kosinuswerte messen. Das Ergebnis ist unten zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>146 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>21,170</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.658</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.09</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Dies liegt <em>weit</em> rechts von den anderen Verteilungen. Ein Kosinusschwellenwert von 0,5 w√ºrde uns sagen, dass fast alle diese Dokumente miteinander verwandt sind. Um irrelevante Dokumente dieser Gr√∂√üe auszuschlie√üen, m√ºssten wir den Schwellenwert viel h√∂her setzen, vielleicht sogar auf 0,9, was zweifellos gute √úbereinstimmungen zwischen den kleineren Dokumenten ausschlie√üen w√ºrde.</p><p>Dies zeigt, dass wir Mindest-Kosinusschwellenwerte √ºberhaupt nicht verwenden k√∂nnen, um die Qualit√§t einer √úbereinstimmung einzusch√§tzen, zumindest nicht ohne die Dokumentgr√∂√üe irgendwie zu ber√ºcksichtigen.</p><h2 id=\"what-causes-size-bias\">Was verursacht den Gr√∂√üen-Bias?</h2><p>Der Gr√∂√üen-Bias bei Embeddings ist nicht wie <a href=\"https://jina.ai/news/long-context-embedding-models-are-blind-beyond-4k-tokens/\">Positions-Biases in Long-Context-Modellen</a>. Er wird nicht durch Architekturen verursacht. Es geht auch nicht grunds√§tzlich um die Gr√∂√üe. Wenn wir zum Beispiel l√§ngere Dokumente erstellt h√§tten, indem wir einfach Kopien desselben Dokuments immer wieder aneinandergef√ºgt h√§tten, w√ºrde sich kein Gr√∂√üen-Bias zeigen.</p><p>Das Problem ist, dass lange Texte mehr Dinge aussagen. Selbst wenn sie durch ein Thema und einen Zweck eingeschr√§nkt sind, besteht der Sinn des Schreibens von mehr W√∂rtern darin, mehr Inhalte zu vermitteln.</p><p>L√§ngere Texte, zumindest die Art, die Menschen normalerweise erstellen, erzeugen naturgem√§√ü Embeddings, die sich √ºber mehr semantischen Raum \"verteilen\". Wenn ein Text mehr Dinge aussagt, wird sein Embedding durchschnittlich einen kleineren Winkel mit anderen Vektoren bilden, unabh√§ngig vom Thema des Textes.</p><h2 id=\"measuring-relevance\">Relevanz messen</h2><p>Die Lehre aus diesem Beitrag ist, dass man Kosinus zwischen semantischen Vektoren nicht <em>allein</em> verwenden kann, um festzustellen, ob etwas eine gute √úbereinstimmung ist, sondern nur, dass es die beste √úbereinstimmung aus den verf√ºgbaren ist. Man muss neben der Berechnung von Kosinuswerten noch etwas anderes tun, um den Nutzen und die G√ºltigkeit der besten √úbereinstimmungen zu √ºberpr√ºfen.</p><p>Man k√∂nnte eine Normalisierung versuchen. Wenn man den Gr√∂√üen-Bias empirisch messen kann, ist es m√∂glicherweise m√∂glich, ihn auszugleichen. Dieser Ansatz k√∂nnte jedoch nicht sehr robust sein. Was f√ºr einen Datensatz funktioniert, wird wahrscheinlich f√ºr einen anderen nicht funktionieren.</p><p><a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/\">Asymmetrische Query-Dokument-Kodierung</a>, die in <code>jina-embeddings-v3</code> verf√ºgbar ist, reduziert den Gr√∂√üen-Bias in Embedding-Modellen, eliminiert ihn aber nicht. Der Zweck der asymmetrischen Kodierung besteht darin, Dokumente so zu kodieren, dass sie weniger \"verteilt\" sind, und Abfragen so zu kodieren, dass sie es mehr sind.</p><p>Die rote Linie im Histogramm unten zeigt die Verteilung der Dokument-zu-Dokument-Kosinuswerte bei Verwendung der asymmetrischen Kodierung mit <code>jina-embeddings-v3</code> ‚Äì jedes Dokument wird mit den Flags <code>retrieval.query</code> und <code>retrieval.passage</code> kodiert, und jedes Dokument-Query-Embedding wird mit jedem Dokument-Passage-Embedding verglichen, das nicht vom selben Dokument stammt. Der durchschnittliche Kosinuswert betr√§gt 0,200 mit einer Standardabweichung von 0,124.</p><p>Diese Kosinuswerte sind deutlich kleiner als die, die wir oben f√ºr dieselben Dokumente mit dem Flag <code>text-matching</code> gefunden haben, wie im Histogramm unten gezeigt.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-25.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"591\" height=\"455\"></figure><p>Die asymmetrische Kodierung hat den Gr√∂√üen-Bias jedoch nicht eliminiert. Das Histogramm unten vergleicht Kosinuswerte f√ºr vollst√§ndige Dokumente und S√§tze bei Verwendung der asymmetrischen Kodierung.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Der Durchschnitt f√ºr Satz-Kosinuswerte betr√§gt 0,124, bei asymmetrischer Kodierung betr√§gt die Differenz zwischen dem durchschnittlichen Satz-Kosinus und dem durchschnittlichen Dokument-Kosinus also 0,076. Die Differenz der Durchschnitte bei symmetrischer Kodierung betr√§gt 0,089. Die √Ñnderung des Gr√∂√üen-Bias ist unerheblich.</p><p>Obwohl die asymmetrische Kodierung die Embeddings f√ºr die Informationssuche verbessert, ist sie nicht besser geeignet, die Relevanz von √úbereinstimmungen zu messen.</p><h2 id=\"future-possibilities\">Zuk√ºnftige M√∂glichkeiten</h2><p>Der Reranker-Ansatz, z.B. <code>jina-reranker-v2-base-multilingual</code> und <code>jina-reranker-m0</code>, ist eine alternative Methode zur Bewertung von Query-Dokument-√úbereinstimmungen, von der wir bereits wissen, dass sie die <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/\">Abfragepr√§zision verbessert</a>. Reranker-Scores sind nicht normalisiert, daher funktionieren sie auch nicht als objektive √Ñhnlichkeitsma√üe. Sie werden jedoch anders berechnet, und es k√∂nnte m√∂glich sein, Reranker-Scores so zu normalisieren, dass sie gute Sch√§tzer f√ºr Relevanz werden.</p><p>Eine weitere Alternative ist die Verwendung von Large Language Models, vorzugsweise mit starken Reasoning-F√§higkeiten, um direkt zu bewerten, ob ein Kandidat eine gute √úbereinstimmung f√ºr eine Abfrage ist. Vereinfacht ausgedr√ºckt k√∂nnten wir ein aufgabenspezifisches Large Language Model fragen: \"Auf einer Skala von 1 bis 10, ist dieses Dokument eine gute √úbereinstimmung f√ºr diese Abfrage?\" Bestehende Modelle sind m√∂glicherweise nicht gut f√ºr diese Aufgabe geeignet, aber gezieltes Training und ausgereiftere Prompting-Techniken sind vielversprechend.</p><p>Es ist nicht unm√∂glich f√ºr Modelle, Relevanz zu messen, aber es erfordert ein anderes Paradigma als Embedding-Modelle.</p><h2 id=\"use-your-models-for-what-its-good-for\">Nutzen Sie Ihre Modelle f√ºr das, wof√ºr sie gut sind</h2><p>Der oben dokumentierte Gr√∂√üen-Bias-Effekt zeigt eine der grundlegenden Einschr√§nkungen von Embedding-Modellen: Sie sind hervorragend darin, Dinge zu vergleichen, aber unzuverl√§ssig beim Messen absoluter Relevanz. Diese Einschr√§nkung ist kein Fehler im Design ‚Äì es ist eine inh√§rente Eigenschaft der Funktionsweise dieser Modelle.</p><p>Was bedeutet das also f√ºr Sie?</p><p>Erstens: Seien Sie skeptisch gegen√ºber Kosinus-Schwellenwerten. Sie funktionieren einfach nicht. Kosinus-√Ñhnlichkeitsma√üe produzieren verlockend objektiv aussehende Gleitkommazahlen. Aber nur weil etwas Zahlen ausgibt, bedeutet das nicht, dass es objektiv etwas misst.</p><p>Zweitens: Erw√§gen Sie Hybrid-L√∂sungen. Embeddings k√∂nnen effizient eine gro√üe Menge von Elementen auf vielversprechende Kandidaten eingrenzen, wonach Sie ausgereiftere (und rechenintensivere) Techniken wie Reranker oder LLMs oder sogar menschliche Bewerter einsetzen k√∂nnen, um die tats√§chliche Relevanz zu bestimmen.</p><p>Drittens: Denken Sie beim Systemdesign in Aufgaben statt in F√§higkeiten. Das objektiv kl√ºgste Modell mit den h√∂chsten Benchmark-Werten ist immer noch Geldverschwendung, wenn es die Aufgabe nicht erf√ºllen kann, f√ºr die Sie es gekauft haben.</p><p>Das Verst√§ndnis der Grenzen unserer Modelle ist nicht pessimistisch ‚Äì es spiegelt ein breiteres Prinzip in Anwendungen wider: Das Verst√§ndnis dessen, worin Ihre Modelle gut sind und worin nicht, ist entscheidend f√ºr den Aufbau zuverl√§ssiger und effektiver Systeme. Genauso wie wir keinen Hammer verwenden w√ºrden, um eine Schraube festzuziehen, sollten wir keine Embedding-Modelle f√ºr Aufgaben verwenden, die sie nicht bew√§ltigen k√∂nnen. Respektieren Sie, wof√ºr Ihre Werkzeuge gut sind.</p>",
  "comment_id": "67e52df15dcba60001c30ebe",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading---2025-04-16T094756.687.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-27T11:52:33.000+01:00",
  "updated_at": "2025-04-16T03:48:15.000+02:00",
  "published_at": "2025-04-16T03:40:03.000+02:00",
  "custom_excerpt": "Size bias refers to how the length of text inputs affects similarity, regardless of semantic relevance. It explains why search systems sometimes return long, barely-relevant documents instead of shorter, more precise matches to your query.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/on-the-size-bias-of-text-embeddings-and-its-impact-in-search/",
  "excerpt": "Die Gr√∂√üenverzerrung (Size Bias) beschreibt, wie die L√§nge von Texteingaben die √Ñhnlichkeit beeinflusst, unabh√§ngig von der semantischen Relevanz. Dies erkl√§rt, warum Suchsysteme manchmal lange, kaum relevante Dokumente anstelle von k√ºrzeren, pr√§ziseren √úbereinstimmungen mit Ihrer Suchanfrage zur√ºckgeben.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
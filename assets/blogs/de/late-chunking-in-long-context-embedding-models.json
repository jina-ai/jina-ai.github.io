{
  "slug": "late-chunking-in-long-context-embedding-models",
  "id": "66c72e30da9a33000146d836",
  "uuid": "9eda87e2-a799-4360-bac9-6a1cd0193349",
  "title": "Sp√§tes Chunking in Embedding-Modellen mit langem Kontext",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Late Chunking ist jetzt in der <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code> API verf√ºgbar. <b><strong style=\"white-space: pre-wrap;\">Empfohlene Lesereihenfolge: Teil I, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Teil II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Forschungspapier</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><p></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What Late Chunking Really Is &amp; What It's Not: Part II</div><div class=\"kg-bookmark-description\">Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-4.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Neu! Teil II: Tiefgehende Analyse von Boundary Cues und Missverst√§ndnissen.</span></p></figcaption></figure><p>Vor etwa einem Jahr, im Oktober 2023, ver√∂ffentlichten wir <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">das weltweit erste Open-Source-Embedding-Modell mit einer Kontextl√§nge von 8K</a>, <code>jina-embeddings-v2-base-en</code>. Seitdem gab es einige Diskussionen √ºber den Nutzen von langem Kontext in Embedding-Modellen. F√ºr viele Anwendungen ist es nicht ideal, ein tausende W√∂rter langes Dokument in eine einzige Embedding-Darstellung zu kodieren. Viele Anwendungsf√§lle erfordern das Abrufen kleinerer Textabschnitte, und dichte vektorbasierte Retrievalsysteme funktionieren oft besser mit kleineren Textsegmenten, da die Semantik in den Embedding-Vektoren weniger wahrscheinlich \"√ºberkomprimiert\" wird.</p><p>Retrieval-Augmented Generation (RAG) ist eine der bekanntesten Anwendungen, die das Aufteilen von Dokumenten in kleinere Textabschnitte (etwa innerhalb von 512 Tokens) erfordert. Diese Chunks werden √ºblicherweise in einer Vektordatenbank gespeichert, wobei die Vektordarstellungen von einem Text-Embedding-Modell generiert werden. W√§hrend der Laufzeit kodiert dasselbe Embedding-Modell eine Anfrage in eine Vektordarstellung, die dann verwendet wird, um relevante gespeicherte Textabschnitte zu identifizieren. Diese Abschnitte werden anschlie√üend an ein Large Language Model (LLM) √ºbergeben, das basierend auf den abgerufenen Texten eine Antwort auf die Anfrage synthetisiert.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--1-.svg\" class=\"kg-image\" alt=\"Flowchart showing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec\" loading=\"lazy\" width=\"1458\" height=\"307\"><figcaption><span style=\"white-space: pre-wrap;\">Eine typische RAG-Pipeline von Chunking-Embedding-Retrieving-Generating.</span></figcaption></figure><p>Kurz gesagt scheint das Einbetten kleinerer Chunks vorzuziehen zu sein, teilweise aufgrund der begrenzten Eingabegr√∂√üen nachgelagerter LLMs, aber auch weil es <strong>die Bef√ºrchtung gibt, dass wichtige kontextuelle Informationen in einem langen Kontext verw√§ssert werden k√∂nnten, wenn sie in einen einzigen Vektor komprimiert werden.</strong></p><p>Aber wenn die Industrie nur Embedding-Modelle mit einer Kontextl√§nge von 512 ben√∂tigt, <em>was ist dann der Sinn des Trainings von Modellen mit einer Kontextl√§nge von 8192 √ºberhaupt?</em></p><p>In diesem Artikel greifen wir diese wichtige, wenn auch unbequeme Frage auf, indem wir die Grenzen der naiven Chunking-Embedding-Pipeline in RAG untersuchen. Wir stellen einen neuen Ansatz namens <strong>\"Late Chunking\"</strong> vor, der die reichhaltigen Kontextinformationen nutzt, die von 8192-L√§ngen-Embedding-Modellen bereitgestellt werden, um Chunks effektiver einzubetten.</p><h2 id=\"the-lost-context-problem\">Das Problem des verlorenen Kontexts</h2><p>Die einfache RAG-Pipeline von Chunking-Embedding-Retrieving-Generating ist nicht ohne Herausforderungen. Insbesondere <strong>kann dieser Prozess weitreichende kontextuelle Abh√§ngigkeiten zerst√∂ren.</strong> Mit anderen Worten, wenn relevante Informationen √ºber mehrere Chunks verteilt sind, kann das Herausnehmen von Textsegmenten aus dem Kontext sie unwirksam machen, was diesen Ansatz besonders problematisch macht.</p><p>Im Bild unten wird ein Wikipedia-Artikel in Satz-Chunks aufgeteilt. Man kann sehen, dass Phrasen wie \"its\" und \"the city\" sich auf \"Berlin\" beziehen, das nur im ersten Satz erw√§hnt wird. Dies macht es f√ºr das Embedding-Modell schwieriger, diese Referenzen mit der richtigen Entit√§t zu verbinden, wodurch eine Vektordarstellung von geringerer Qualit√§t entsteht.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"Vergleichende Panels zeigen Berlins Wikipedia-Artikel und seinen gechunkten Text zur Verdeutlichung der Vorteile f√ºr Klarheit und Lesbarkeit.\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png 1774w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dies bedeutet, wenn wir einen langen Artikel in Satzl√§ngen-Chunks aufteilen, wie im obigen Beispiel, k√∂nnte ein RAG-System Schwierigkeiten haben, eine Anfrage wie \"Was ist die Einwohnerzahl von Berlin?\" zu beantworten. Da der Stadtname und die Einwohnerzahl nie zusammen in einem einzigen Chunk erscheinen und ohne gr√∂√üeren Dokumentkontext kann ein LLM, dem einer dieser Chunks pr√§sentiert wird, anaphorische Referenzen wie \"es\" oder \"die Stadt\" nicht aufl√∂sen.</p><p>Es gibt einige Heuristiken, um dieses Problem zu mildern, wie das Resampling mit einem gleitenden Fenster, die Verwendung mehrerer Kontextfensterl√§ngen und die Durchf√ºhrung mehrfacher Dokumentscans. Wie alle Heuristiken sind diese Ans√§tze jedoch Gl√ºckssache; sie m√∂gen in einigen F√§llen funktionieren, aber es gibt keine theoretische Garantie f√ºr ihre Wirksamkeit.</p><h2 id=\"the-solution-late-chunking\">Die L√∂sung: Late Chunking</h2><p>Der naive Kodierungsansatz (wie auf der linken Seite des Bildes unten zu sehen) verwendet S√§tze, Abs√§tze oder maximale L√§ngenbeschr√§nkungen, um den Text <em>a priori</em> aufzuteilen. Danach wird ein Embedding-Modell wiederholt auf diese resultierenden Chunks angewendet. Um ein einzelnes Embedding f√ºr jeden Chunk zu generieren, verwenden viele Embedding-Modelle <em>Mean Pooling</em> auf diesen Token-Level-Embeddings, um einen einzelnen Embedding-Vektor auszugeben.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--4-.svg\" class=\"kg-image\" alt=\"Flussdiagramm vergleicht naive und Late Chunking Methoden in der Dokumentverarbeitung mit beschrifteten Schritten und Embeddings.\" loading=\"lazy\" width=\"1020\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Eine Illustration der naiven Chunking-Strategie (links) und der Late Chunking-Strategie (rechts).</span></figcaption></figure><p>Im Gegensatz dazu wendet der \"Late Chunking\"-Ansatz, den wir in diesem Artikel vorschlagen, zun√§chst die Transformer-Schicht des Embedding-Modells auf <em>den gesamten Text</em> oder so viel wie m√∂glich davon an. Dies erzeugt eine Sequenz von Vektordarstellungen f√ºr jeden Token, die Textinformationen aus dem gesamten Text umfasst. Anschlie√üend wird Mean Pooling auf jeden Chunk dieser Sequenz von Token-Vektoren angewendet, wodurch Embeddings f√ºr jeden Chunk entstehen, die den Kontext des gesamten Textes ber√ºcksichtigen. Im Gegensatz zum naiven Kodierungsansatz, der unabh√§ngige und identisch verteilte (i.i.d.) Chunk-Embeddings generiert, <strong>erzeugt Late Chunking eine Reihe von Chunk-Embeddings, bei denen jedes auf den vorherigen \"bedingt\" ist und dadurch mehr kontextuelle Informationen f√ºr jeden Chunk kodiert.</strong></p><p>Offensichtlich ben√∂tigen wir f√ºr die effektive Anwendung von Late Chunking Langkontext-Embedding-Modelle wie <code>jina-embeddings-v2-base-en</code>, die bis zu 8192 Tokens unterst√ºtzen‚Äîungef√§hr zehn Standardseiten Text. Textsegmente dieser Gr√∂√üe haben mit viel geringerer Wahrscheinlichkeit kontextuelle Abh√§ngigkeiten, die einen noch l√§ngeren Kontext zur Aufl√∂sung ben√∂tigen w√ºrden.</p><p>Es ist wichtig hervorzuheben, dass Late Chunking immer noch Boundary Cues ben√∂tigt, aber diese Cues werden erst <em>nach</em> dem Erhalt der Token-Level-Embeddings verwendet‚Äîdaher der Begriff \"late\" in seiner Bezeichnung.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Naive Chunking</th>\n<th>Late Chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Die Notwendigkeit von Boundary Cues</td>\n<td>Ja</td>\n<td>Ja</td>\n</tr>\n<tr>\n<td>Die Verwendung von Boundary Cues</td>\n<td>Direkt in der Vorverarbeitung</td>\n<td>Nach Erhalt der Token-Level-Embeddings aus der Transformer-Schicht</td>\n</tr>\n<tr>\n<td>Die resultierenden Chunk-Embeddings</td>\n<td>i.i.d.</td>\n<td>Bedingt</td>\n</tr>\n<tr>\n<td>Kontextuelle Informationen benachbarter Chunks</td>\n<td>Verloren. Einige Heuristiken (wie Overlap Sampling) zur Milderung</td>\n<td>Gut erhalten durch Langkontext-Embedding-Modelle</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"implementation-and-qualitative-evaluation\">Implementierung und qualitative Evaluierung</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/4c9d6ee1a7679cb6c4c106e58fabaf56/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Die Implementierung von Late Chunking finden Sie im oben verlinkten Google Colab. Hier nutzen wir unsere k√ºrzlich ver√∂ffentlichte Funktion in der Tokenizer API, die alle m√∂glichen Boundary Cues verwendet, um ein langes Dokument in sinnvolle Chunks zu segmentieren. Weitere Diskussionen √ºber den Algorithmus hinter dieser Funktion finden Sie auf X.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">Kostenlose API zur Tokenisierung von Text und Segmentierung langer Texte in Chunks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,‚Ä¶ <a href=\"https://t.co/AtBCSrn7nI?ref=jina-ai-gmbh.ghost.io\">pic.twitter.com/AtBCSrn7nI</a></p>‚Äî Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1823756993108304135?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">14. August 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><p>Bei der Anwendung von Late Chunking auf das obige Wikipedia-Beispiel sieht man sofort eine Verbesserung der semantischen √Ñhnlichkeit. Zum Beispiel enthalten im Fall von ‚Äûthe city\" und ‚ÄûBerlin\" in einem Wikipedia-Artikel die Vektoren, die ‚Äûthe city\" repr√§sentieren, nun Informationen, die es mit der vorherigen Erw√§hnung von ‚ÄûBerlin\" verbinden, was es zu einer viel besseren √úbereinstimmung f√ºr Anfragen mit diesem Stadtnamen macht.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Query</th>\n<th>Chunk</th>\n<th>Sim. on naive chunking</th>\n<th>Sim. on late chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Berlin</td>\n<td>Berlin is the capital and largest city of Germany, both by area and by population.</td>\n<td>0.849</td>\n<td>0.850</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.</td>\n<td>0.708</td>\n<td>0.825</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.</td>\n<td>0.753</td>\n<td>0.850</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Dies kann man in den numerischen Ergebnissen oben beobachten, die das Embedding des Begriffs ‚ÄûBerlin\" mit verschiedenen S√§tzen aus dem Artikel √ºber Berlin mittels Cosinus-√Ñhnlichkeit vergleichen. Die Spalte ‚ÄûSim. on IID chunk embeddings\" zeigt die √Ñhnlichkeitswerte zwischen dem Query-Embedding von ‚ÄûBerlin\" und den Embeddings mit <em>a priori</em> Chunking, w√§hrend ‚ÄûSim. under contextual chunk embedding\" die Ergebnisse mit der Late-Chunking-Methode darstellt.</p><h2 id=\"quantitative-evaluation-on-beir\">Quantitative Evaluation auf BEIR</h2><p>Um die Effektivit√§t von Late Chunking √ºber ein Spielbeispiel hinaus zu verifizieren, haben wir es anhand einiger Retrieval-Benchmarks aus <a href=\"https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">BeIR</a> getestet. Diese Retrieval-Aufgaben bestehen aus einem Query-Set, einem Korpus von Textdokumenten und einer QRels-Datei, die Informationen √ºber die IDs der f√ºr jede Anfrage relevanten Dokumente speichert.</p><p>Um die relevanten Dokumente f√ºr eine Anfrage zu identifizieren, werden die Dokumente gechunkt, in einen Embedding-Index kodiert und die √§hnlichsten Chunks f√ºr jedes Query-Embedding mittels k-n√§chste Nachbarn (kNN) bestimmt. Da jeder Chunk einem Dokument entspricht, kann das kNN-Ranking der Chunks in ein kNN-Ranking der Dokumente umgewandelt werden (wobei nur das erste Vorkommen f√ºr Dokumente beibehalten wird, die mehrfach im Ranking erscheinen). Dieses resultierende Ranking wird dann mit dem Ranking aus der Ground-Truth QRels-Datei verglichen und Retrieval-Metriken wie nDCG@10 werden berechnet. Dieses Verfahren ist unten dargestellt, und das Evaluierungsskript ist in diesem Repository zur Reproduzierbarkeit verf√ºgbar.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/late-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/late-chunking: Code f√ºr die Erkl√§rung und Evaluierung von Late Chunking (Chunked Pooling)</div><div class=\"kg-bookmark-description\">Code f√ºr die Erkl√§rung und Evaluierung von Late Chunking (Chunked Pooling) - jina-ai/late-chunking</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/bf0bb9d5ca928dc3fe25ae621398af0fdf5e34324e37cbeee6fa4189218c9b4d/jina-ai/late-chunking\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Wir haben diese Evaluierung auf verschiedenen BeIR-Datens√§tzen durchgef√ºhrt und dabei naives Chunking mit unserer Late-Chunking-Methode verglichen. F√ºr die Boundary Cues verwendeten wir einen Regex, der die Texte in Strings von etwa 256 Tokens aufteilt. Sowohl die naive als auch die Late-Chunking-Evaluierung verwendeten <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\"><code>jina-embeddings-v2-small-en</code></a> als Embedding-Modell; eine kleinere Version des <code>v2-base-en</code> Modells, das immer noch eine L√§nge von bis zu 8192 Tokens unterst√ºtzt. Die Ergebnisse sind in der Tabelle unten zu finden.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Avg. Document Length (characters)</th>\n<th>Naive Chunking (nDCG@10)</th>\n<th>Late Chunking (nDCG@10)</th>\n<th>No Chunking (nDCG@10)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td>1498.4</td>\n<td>64.20%</td>\n<td><strong>66.10%</strong></td>\n<td>63.89%</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td>1116.7</td>\n<td>63.36%</td>\n<td>64.70%</td>\n<td><strong>65.18%</strong></td>\n</tr>\n<tr>\n<td>FiQA2018</td>\n<td>767.2</td>\n<td>33.25%</td>\n<td><strong>33.84%</strong></td>\n<td>33.43%</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>1589.8</td>\n<td>23.46%</td>\n<td>29.98%</td>\n<td><strong>30.40%</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>62.2</td>\n<td>87.19%</td>\n<td>87.19%</td>\n<td>87.19%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>In allen F√§llen verbesserte Late Chunking die Ergebnisse im Vergleich zum naiven Ansatz. In einigen F√§llen √ºbertraf es sogar die Kodierung des gesamten Dokuments in ein einzelnes Embedding, w√§hrend in anderen Datens√§tzen gar kein Chunking die besten Ergebnisse lieferte (Nat√ºrlich macht kein Chunking nur dann Sinn, wenn keine Notwendigkeit besteht, Chunks zu ranken, was in der Praxis selten vorkommt). Wenn wir die Leistungsl√ºcke zwischen dem naiven Ansatz und Late Chunking gegen die Dokumentenl√§nge auftragen, wird deutlich, dass die durchschnittliche L√§nge der Dokumente mit gr√∂√üeren Verbesserungen der nDCG-Werte durch Late Chunking korreliert. Mit anderen Worten: <strong>Je l√§nger das Dokument, desto effektiver wird die Late-Chunking-Strategie.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/chart--22-.svg\" class=\"kg-image\" alt=\"Liniendiagramm, das den R√ºckgang der relativen Verbesserung mit zunehmender Dokumentenl√§nge von 0 bis 1500 Zeichen zeigt.\" loading=\"lazy\" width=\"582\" height=\"337\"><figcaption><span style=\"white-space: pre-wrap;\">Die Verbesserung durch Late Chunking gegen√ºber naivem Chunking korreliert mit der durchschnittlichen Dokumentenl√§nge.</span></figcaption></figure><h2 id=\"conclusion\">Fazit</h2><p>In diesem Artikel haben wir einen einfachen Ansatz namens ‚ÄûLate Chunking\" vorgestellt, um kurze Chunks unter Nutzung der Leistungsf√§higkeit von Long-Context-Embedding-Modellen einzubetten. Wir haben gezeigt, wie traditionelles i.i.d. Chunk-Embedding keine kontextuellen Informationen bewahrt und zu suboptimalem Retrieval f√ºhrt, und wie Late Chunking eine einfache, aber hocheffektive L√∂sung bietet, um kontextuelle Informationen innerhalb jedes Chunks zu erhalten und zu konditionieren. Die Effektivit√§t von Late Chunking wird bei l√§ngeren Dokumenten zunehmend bedeutsamer ‚Äì eine F√§higkeit, die <em>nur</em> durch fortgeschrittene Long-Context-Embedding-Modelle wie <code>jina-embeddings-v2-base-en</code> m√∂glich wird. Wir hoffen, dass diese Arbeit nicht nur die Bedeutung von Long-Context-Embedding-Modellen validiert, sondern auch zu weiterer Forschung auf diesem Gebiet inspiriert.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Was Late Chunking wirklich ist & was nicht: Teil II</div><div class=\"kg-bookmark-description\">Teil 2 unserer Erforschung von Late Chunking, ein tiefer Einblick, warum es die beste Methode f√ºr Chunk Embeddings und die Verbesserung der Such-/RAG-Leistung ist.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-5.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Lesen Sie weiter in Teil II: Tiefer Einblick in Boundary Cues und Missverst√§ndnisse.</span></p></figcaption></figure>",
  "comment_id": "66c72e30da9a33000146d836",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-22T14:25:20.000+02:00",
  "updated_at": "2024-10-06T16:29:02.000+02:00",
  "published_at": "2024-08-22T17:06:17.000+02:00",
  "custom_excerpt": "Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael G√ºnther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael G√ºnther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/late-chunking-in-long-context-embedding-models/",
  "excerpt": "Die Aufteilung langer Dokumente unter Beibehaltung der Kontextinformationen ist eine Herausforderung. Wir stellen das \"Late Chunking\" vor, das Long-Context-Embedding-Modelle nutzt, um kontextuelle Chunk-Embeddings f√ºr bessere Retrieval-Anwendungen zu generieren.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating the 'Late Chunking' and 'Long Document Model' processes in machine learning on a black background.",
  "feature_image_caption": null
}
{
  "slug": "jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images",
  "id": "673cc4a7a7c46d00015cf1f5",
  "uuid": "6ca44950-b989-494a-b587-70847f24edd2",
  "title": "Jina CLIP v2: Multilinguale und Multimodale Embeddings für Text und Bilder",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">Wir sind auf einer Reise, künstliche Intelligenz durch Open Source und Open Science voranzubringen und zu demokratisieren.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-11.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-clip-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?sui=&model=jina-clip-v2&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Ihr Such-Fundament, aufgeladen.</div><div class=\"kg-bookmark-description\">Erstklassige Embeddings, Reranker, LLM-Reader, Web Scraper, Klassifikatoren. Die beste Such-KI für multilinguale und multimodale Daten.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-11.png\" alt=\"\"><span class=\"kg-bookmark-author\">Ihr Such-Fundament, aufgeladen.</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> API ist unter dem Tab „Embeddings\" verfügbar.</span></p></figcaption></figure><p>Multimodale Embeddings ermöglichen die Suche und das Verstehen von Daten über verschiedene Modalitäten hinweg durch eine kohärente Darstellung. Sie dienen als Rückgrat für Neural Information Retrieval und multimodale GenAI-Anwendungen. Heute freuen wir uns, <code>jina-clip-v2</code> vorzustellen, neue universelle multilinguale multimodale Embeddings, die auf <code>jina-clip-v1</code> und unserem kürzlich veröffentlichten <code>jina-embeddings-3</code> aufbauen und mehrere wichtige Verbesserungen bieten:</p><ul><li><strong>Verbesserte Leistung</strong>: v2 zeigt eine 3%ige Leistungsverbesserung gegenüber v1 sowohl bei Text-Bild- als auch bei Text-Text-Retrieval-Aufgaben. Ähnlich wie v1 kann der Textencoder von v2 als effektiver multilingualer Long-Context Dense Retriever dienen. Er arbeitet auf Augenhöhe mit unserem Spitzenmodell <code>jina-embeddings-v3</code> (derzeit die besten multilingualen Embeddings unter 1B Parametern auf MTEB).</li><li><strong>Multilinguale Unterstützung</strong>: Mit <code>jina-embeddings-v3</code> als Text-Tower unterstützt <code>jina-clip-v2</code> 89 Sprachen für multilinguale Bildsuche und zeigt bis zu 4% Verbesserung im Vergleich zu <code>nllb-clip-large-siglip</code> bei multilingualen Bildsuche-Aufgaben.</li><li><strong>Höhere Bildauflösung</strong>: v2 unterstützt jetzt eine Eingabebildauflösung von 512x512, eine deutliche Steigerung gegenüber v1's 224x224. Diese höhere Auflösung ermöglicht eine bessere Verarbeitung detaillierter Bilder, verbesserte Merkmalsextraktion und genauere Erkennung feinkörniger visueller Elemente.</li><li><strong>Matryoshka-Darstellungen</strong>: v2 ermöglicht es Benutzern, die Ausgabedimensionen sowohl von Text- als auch von Bild-Embeddings von 1024 bis auf 64 zu reduzieren, wodurch Speicher- und Verarbeitungsaufwand reduziert werden, während die starke Leistung erhalten bleibt.</li></ul><h2 id=\"model-architecture\">Modellarchitektur</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--35-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"></figure><p><code>jina-clip-v2</code> ist ein 0,9B CLIP-Style-Modell, das zwei leistungsstarke Encoder kombiniert: den Text-Encoder <code>Jina XLM-RoBERTa</code> (das Rückgrat von <code>jina-embeddings-v3</code>) und den Vision-Encoder <code>EVA02-L14</code> (ein effizienter Vision Transformer entwickelt von BAAI). Diese Encoder werden gemeinsam trainiert, um alignierte Darstellungen von Bildern und Text zu erstellen.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Text Encoder</th>\n<th>Image Encoder</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Base Model</td>\n<td>Jina XLM-RoBERTa</td>\n<td>EVA02-L</td>\n</tr>\n<tr>\n<td>Parameters</td>\n<td>561M</td>\n<td>304M</td>\n</tr>\n<tr>\n<td>Input Specification</td>\n<td>8,192 tokens (max)</td>\n<td>512×512 pixels</td>\n</tr>\n<tr>\n<td>Min Output Dimensions</td>\n<td>64</td>\n<td>64</td>\n</tr>\n<tr>\n<td>Max Output Dimensions</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Attention Mechanism</td>\n<td>FlashAttention2</td>\n<td>xFormers</td>\n</tr>\n<tr>\n<td>Pooling Strategy</td>\n<td>Mean pooling</td>\n<td>CLS pooling</td>\n</tr>\n<tr>\n<td>Additional Features</td>\n<td>89 languages supported</td>\n<td>Patch size 14x14</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"cross-modal-retrieval-performance\">Cross-Modal Retrieval Leistung</h2><p>Jina CLIP v2 bietet multilinguale Unterstützung für 89 Sprachen und Spitzenleistung in wichtigen Sprachen wie Arabisch, Chinesisch, Englisch, Französisch, Deutsch, Japanisch, Russisch und Spanisch. In multilingualen Bildsuche-Benchmarks zeigt es eine Leistung, die <a href=\"https://huggingface.co/visheratin/nllb-clip-large-siglip?ref=jina-ai-gmbh.ghost.io\">NLLB-CLIP-SigLIP</a> entspricht oder übertrifft, ein etwas größeres (1,3B, 44% größer als <code>jina-clip-v2</code>) State-of-the-Art CLIP-Style-Modell, das einen vortrainierten Text-Encoder aus NLLB-Modellen verwendet.</p><h3 id=\"english-only-text-and-images\">Nur englische Texte und Bilder</h3><p>Bei Standard-Cross-Modal-Retrieval-Benchmarks (Flickr30k und COCO) zeigt <code>jina-clip-v2</code> durchweg starke Verbesserungen. Es erreicht State-of-the-Art-Leistung von 98,0% bei Flickr30k Bild-zu-Text-Retrieval und übertrifft damit sowohl seinen Vorgänger als auch NLLB-CLIP-SigLIP. Das Modell zeigt konsistente Verbesserungen in allen Retrieval-Szenarien, mit bemerkenswerten Steigerungen von bis zu 3,3% gegenüber v1 bei COCO Bild-zu-Text-Retrieval, während es über verschiedene Benchmarks und Modalitätsrichtungen hinweg eine wettbewerbsfähige Leistung mit NLLB-CLIP-SigLIP beibehält.</p><p><strong>Flickr30k Recall@5 Leistung:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>98.0</td>\n<td>+1.7%</td>\n<td>+0.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>96.4</td>\n<td>-</td>\n<td>-0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>97.1</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>89.8</td>\n<td>+0.9%</td>\n<td>-2.6%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>89.0</td>\n<td>-</td>\n<td>-3.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>92.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>COCO Recall@5 Leistung:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>81.5</td>\n<td>+3.3%</td>\n<td>+2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>78.9</td>\n<td>-</td>\n<td>-0.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>79.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>68.4</td>\n<td>+2.9%</td>\n<td>-3.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>66.5</td>\n<td>-</td>\n<td>-6.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>70.8</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"multilingual-text-and-images\">Multilinguale Texte und Bilder</h3><p>Bei multilingualen Cross-Modal-Benchmarks zeigt <code>jina-clip-v2</code> robuste Leistung, insbesondere beim Bild-zu-Text-Retrieval, wo es NLLB-SigLIP über alle Datensätze hinweg übertrifft, mit bis zu +3,8% Verbesserung bei Crossmodal 3600. Während NLLB-SigLIP etwas stärkere Text-zu-Bild-Retrieval-Fähigkeiten zeigt, bleibt der Leistungsunterschied gering, typischerweise innerhalb von 3%.</p><p><strong>Image2Text Recall@5 Performance:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>83.23</td>\n<td>+3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>80.16</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>86.03</td>\n<td>+0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.37</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.98</td>\n<td>+0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.41</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Text2Image Recall@5 Performance:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>81.43</td>\n<td>-0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>82.07</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>84.87</td>\n<td>-3.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.60</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.03</td>\n<td>-3.0%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.63</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"text-only-dense-retriever-performance\">Leistung des Text-Only Dense Retrievers</h2><p>Ähnlich wie sein Vorgänger kann der Text-Encoder von <code>jina-clip-v2</code> als effektiver mehrsprachiger Dense Retriever dienen. Bei den umfassenden Multilingual MTEB Benchmarks erzielt er starke Leistungen mit 69,86 % beim Retrieval und 67,77 % bei Aufgaben zur semantischen Ähnlichkeit. Diese Ergebnisse zeigen seine Vielseitigkeit und sind wettbewerbsfähig mit unserem spezialisierten Text-Embedding-Modell <code>jina-embeddings-v3</code>:</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>69.86</td>\n<td>-3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>72.59</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Semantic Similarity</td>\n<td>jina-clip-v2</td>\n<td>67.77</td>\n<td>-2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>69.81</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>Bei englischsprachigen Aufgaben zeigt <code>jina-clip-v2</code> konstante Verbesserungen gegenüber sowohl seinem Vorgänger als auch NLLB-SigLIP, mit besonders starken Vorteilen bei der Retrieval-Leistung (fast das Doppelte des NLLB-SigLIP-Scores).</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS</td>\n<td>jina-clip-v2</td>\n<td>81.29</td>\n<td>+0.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>80.92</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>74.65</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>49.33</td>\n<td>+2.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>48.33</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>24.92</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"matryoshka-representation-performance\">Leistung der Matryoshka-Repräsentation</h2><p>Sowohl Text- als auch Bild-Encoder unterstützen MRL, und ihre Ausgabedimensionen können auf 64 reduziert werden, während sie weiterhin starke Leistung zeigen. Unsere Evaluierung der Embedding-Kürzung zeigte bemerkenswerte Komprimierungspotenziale. Selbst eine aggressive Dimensionsreduktion um 75 % behielt über 99 % der Leistung bei Text-, Bild- und Cross-Modal-Aufgaben bei.</p><h3 id=\"image-classification\">Bildklassifizierung</h3><p>Über 37 verschiedene Bildklassifizierungs-Benchmarks hinweg zeigt der Bild-Encoder eine starke Widerstandsfähigkeit gegenüber reduzierten Dimensionen. Eine Komprimierung von 1024 auf 64 Dimensionen (94 % Reduktion) führt nur zu einem Rückgang von 8 % bei der Top-5-Genauigkeit und 12,5 % bei Top-1, was sein Potenzial für einen effizienten Einsatz bei minimalem Leistungsverlust unterstreicht.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/accuracy_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Für die </span><b><strong style=\"white-space: pre-wrap;\">Bildklassifizierung</strong></b><span style=\"white-space: pre-wrap;\"> verwendeten wir die 19 Benchmarks aus dem </span><a href=\"https://github.com/google-research/task_adaptation?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VTAB-Datensatz</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VOC 2007</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/sun397?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">SUN397</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://cs.stanford.edu/~acoates/stl10/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">STL10</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Rendered SST2</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://objectnet.dev/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ObjectNet</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/cvdfoundation/mnist?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MNIST</span></a><span style=\"white-space: pre-wrap;\">, German Traffic Sign Recognition Benchmark (</span><a href=\"https://benchmark.ini.rub.de/gtsrb_dataset.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">GTSRB</span></a><span style=\"white-space: pre-wrap;\">), Fine-Grained Visual Classification of Aircraft (</span><a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FGVC-Aircraft</span></a><span style=\"white-space: pre-wrap;\">), </span><a href=\"https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FER 2013</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Country211</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/cars196?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Cars196</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/hendrycks/natural-adv-examples?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet-A, ImageNet-O,</span></a><a href=\"https://huggingface.co/datasets/ILSVRC/imagenet-1k?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">IxmageNet1k</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/HaohanWang/ImageNet-Sketch?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet Sketch</span></a><span style=\"white-space: pre-wrap;\"> und </span><a href=\"https://imagenetv2.org/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet v2</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h3 id=\"cross-modal-retrieval\">Modalitätsübergreifendes Retrieval</h3><p>Trotz einer drastischen Reduzierung um 94% auf nur 64 Dimensionen blieb das modalitätsübergreifende Retrieval unter Verwendung von gekürzten Bild- und Text-Embeddings bemerkenswert robust und bewahrte 93% der Bild-zu-Text- und 90% der Text-zu-Bild-Leistung.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/crossmodal_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Wir verwendeten sechs Benchmarks, von denen drei mehrsprachig sind: </span><a href=\"https://google.github.io/crossmodal-3600/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Crossmodal-3600</span></a><span style=\"white-space: pre-wrap;\"> (36 Sprachen), </span><a href=\"https://shannon.cs.illinois.edu/DenotationGraph/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr30k</span></a><span style=\"white-space: pre-wrap;\"> (nur Englisch), </span><a href=\"https://hockenmaier.cs.illinois.edu/8k-pictures.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr8k</span></a><span style=\"white-space: pre-wrap;\"> (nur Englisch), </span><a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (nur Englisch), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Multilingual MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (10 Sprachen), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">XTD 200</span></a><span style=\"white-space: pre-wrap;\"> (27 Sprachen)</span></figcaption></figure><h3 id=\"text-only-retrieval\">Reiner Text-Retrieval</h3><p>Bei <strong>rein englischsprachigen MTEB-Benchmarks</strong> bewahrten 64-dimensionale Text-Embeddings (komprimiert von 1024) die semantische Ähnlichkeit bemerkenswert gut und zeigten nur einen Rückgang von 2,1%, während das Retrieval einen moderaten Rückgang von 17,5% verzeichnete.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/mteb_performance.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"></figure><h2 id=\"getting-started\">Erste Schritte</h2><h3 id=\"via-api\">Über die API</h3><p>Der Code zeigt, wie man Embeddings mit Python's <code>requests</code> generiert. Übergeben Sie einen Text-String mit entweder einem base64-codierten Bild oder einer URL sowie Ihre gewünschte Dimensionsgröße (Standard 1024, hier 768 gezeigt).</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-Python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v2',\n  'encoding_type': 'float',\n  'dimensions': '768' \n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Denken Sie daran, &lt;YOUR_JINA_AI_API_KEY&gt; durch einen aktivierten Jina API-Schlüssel zu ersetzen. Sie können </span><a href=\"https://jina.ai/?sui=apikey&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><span style=\"white-space: pre-wrap;\">hier einen kostenlosen API-Schlüssel mit einer Million kostenloser Token erhalten.</span></a></p></figcaption></figure><h3 id=\"image-tokens-pricing\">Bild-Token-Preise</h3><p>Unsere API zählt sowohl Text- als auch Bild-Token. Bei Bildern basiert der Token-Verbrauch auf der Anzahl der 512x512-Pixel-Kacheln, die benötigt werden, um die gesamte Bildfläche abzudecken. Jede Kachel kostet 4.000 Token für die Verarbeitung, einschließlich teilweise gefüllter Kacheln. <strong>Für optimale Kosteneffizienz empfehlen wir API-Nutzern, ihre Bilder vor dem Senden der Anfragen auf 512x512 zu skalieren.</strong></p><table>\n<thead>\n<tr>\n<th>Bildauflösung</th>\n<th>Benötigte Kacheln</th>\n<th>Token-Kosten</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>512x512</td>\n<td>1</td>\n<td>4.000</td>\n</tr>\n<tr>\n<td>720x720</td>\n<td>4</td>\n<td>16.000</td>\n</tr>\n<tr>\n<td>1080x1080</td>\n<td>9</td>\n<td>36.000</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--37-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Für quadratische Bilder empfiehlt sich die Skalierung auf 512x512 für beste Kosteneffizienz. Für Aufgaben, bei denen das Seitenverhältnis wichtig ist, skalieren Sie die längste Kante auf 512, zentrieren Sie das Bild und füllen Sie mit Schwarz auf. Für allgemeine Zwecke funktioniert die direkte Skalierung auf 512x512 gut.</span></figcaption></figure><h3 id=\"via-csp-marketplaces\">Über CSP-Marktplätze</h3><p>Jina CLIP v2 ist direkt auf AWS, Azure und GCP zu den dort aufgeführten Preisen verfügbar.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-bfbctuqmky676?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina CLIP v2</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-10.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/socialPreview-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-9.ico\" alt=\"\"></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://console.cloud.google.com/marketplace/browse?q=jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Cloud console</div><div class=\"kg-bookmark-description\">Geben Sie intelligent aus, beschaffen Sie schneller und nutzen Sie die zugesagten Google Cloud-Ausgaben mit Google Cloud Marketplace. Durchsuchen Sie den Katalog mit über 2000 SaaS, VMs, Entwicklungs-Stacks und Kubernetes-Apps, die für Google Cloud optimiert sind.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/default.png\" alt=\"\"></div></div></a></figure><h3 id=\"via-vectordb\"><strong>Über VectorDB</strong></h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pinecone.io/models/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The vector database to build knowledgeable AI | Pinecone</div><div class=\"kg-bookmark-description\">Durchsuchen Sie Milliarden von Elementen nach ähnlichen Übereinstimmungen mit beliebigen Objekten in Millisekunden. Dies ist die nächste Generation der Suche, nur einen API-Aufruf entfernt.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-3.png\" alt=\"\"><span class=\"kg-bookmark-author\">Pinecone Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/docs_og_image.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://weaviate.io/developers/weaviate/model-providers/jinaai/embeddings-multimodal?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multimodal Embeddings | Weaviate</div><div class=\"kg-bookmark-description\">Weaviates Integration mit Jina AIs APIs ermöglicht Ihnen den direkten Zugriff auf die Fähigkeiten ihrer Modelle von Weaviate aus.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-12.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Weaviate</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/provider_integrations_jinaai.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant ist eine Open-Source-Vektordatenbank und Vektorsuchmaschine, die in Rust geschrieben wurde. Sie bietet einen schnellen und skalierbaren Vektorähnlichkeitssuchdienst mit benutzerfreundlicher API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-4.png\" alt=\"\"><span class=\"kg-bookmark-author\">edit</span><span class=\"kg-bookmark-publisher\">Qdrant</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-social-preview-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Fazit</h2><p>Aufbauend auf unserer <code>jina-clip-v1</code> Veröffentlichung im Juni, die das CLIP-Modell von OpenAI um Texteingaben von bis zu 8.192 Token erweiterte, und dem bahnbrechenden mehrsprachigen <code>jina-embeddings-v3</code>, bringt <code>jina-clip-v2</code> drei wichtige Fortschritte: mehrsprachige Unterstützung für 89 Sprachen, erhöhte Bildauflösung von 512x512 und Matryoshka-Repräsentationslernen für genauere verkürzte Embeddings.</p><p>CLIP-ähnliche Modelle haben sich als Rückgrat für universelle multimodale Anwendungen etabliert. Mit <code>jina-clip-v2</code> heben wir diese Fähigkeiten auf die nächste Stufe und bauen Sprachbarrieren ab, um ein genaueres modalitätsübergreifendes Verständnis und Abrufen zu ermöglichen. Wir glauben, dass diese Version ein Versprechen einlöst, multimodale Suche und Abruf sowohl leistungsfähiger als auch zugänglicher für Entwickler weltweit zu machen.</p>",
  "comment_id": "673cc4a7a7c46d00015cf1f5",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/11/clipv2.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-11-19T18:02:31.000+01:00",
  "updated_at": "2024-11-21T17:29:45.000+01:00",
  "published_at": "2024-11-21T17:29:45.000+01:00",
  "custom_excerpt": "Jina-CLIP v2, a 0.9B multimodal embedding model with multilingual support of 89 languages, high image resolution at 512x512, and Matryoshka representations.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/",
  "excerpt": "Jina-CLIP v2, ein multimodales Embedding-Modell mit 0,9 Milliarden Parametern, das 89 Sprachen unterstützt, eine hohe Bildauflösung von 512x512 bietet und Matryoshka-Repräsentationen verwendet.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
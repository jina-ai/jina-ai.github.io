{
  "slug": "jina-embeddings-v2-bilingual-models-are-now-open-source-on-hugging-face",
  "id": "65b3adb510ff9f0001c50c4d",
  "uuid": "b082269a-4358-4a82-a70c-02da2ebcb6d3",
  "title": "Jina Embeddings v2 zweisprachige Modelle sind jetzt Open Source auf Hugging Face",
  "html": "<p>Jina AI hat seine hochmodernen Open-Source bilingualen Embedding-Modelle für <a href=\"https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io\">Deutsch-Englisch</a> und <a href=\"https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io\">Chinesisch-Englisch</a> Sprachpaare über Hugging Face veröffentlicht.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ich bin ein Berliner: German-English Bilingual Embeddings with 8K Token Length</div><div class=\"kg-bookmark-description\">Jina AI introduces a German/English bilingual embedding model, featuring an extensive 8,192-token length, specifically designed to support German businesses thriving in the U.S. market.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-publisher\">GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--33-.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/8k-token-length-bilingual-embeddings-break-language-barriers-in-chinese-and-english/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">8K Token-Length Bilingual Embeddings Break Language Barriers in Chinese and English</div><div class=\"kg-bookmark-description\">The first bilingual Chinese-English embedding model with 8192 token-length.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-publisher\">Discord</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/jina-embeddings-v2-base-zh.png\" alt=\"\"></div></a></figure><p>In diesem Tutorial werden wir durch eine minimale Installation und einen Anwendungsfall gehen, der Folgendes umfasst:</p><ol><li>Herunterladen von Jina Embedding-Modellen von Hugging Face.</li><li>Verwendung der Modelle zur Erstellung von Encodierungen aus Texten in Deutsch und Englisch.</li><li>Aufbau einer sehr rudimentären Embedding-basierten neuralen Suchmaschine für sprachübergreifende Abfragen.</li></ol><p>Wir zeigen Ihnen, wie Sie Jina Embeddings verwenden können, um englische Abfragen zu schreiben, die passende Texte auf Deutsch finden und umgekehrt.</p><p>Dieses Tutorial funktioniert genauso für das chinesische Modell. Folgen Sie einfach den Anweisungen im Abschnitt (gegen Ende) mit dem Titel <a href=\"#querying-in-chinese\" rel=\"noreferrer\"><strong>Querying in Chinese</strong></a>, um das Chinesisch-Englisch bilinguale Modell und ein Beispieldokument auf Chinesisch zu erhalten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v2-base-de?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v2-base-de · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-de.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v2-base-zh?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v2-base-zh · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-zh.png\" alt=\"\"></div></a></figure><h2 id=\"bilingual-embedding-models\">Bilinguale Embedding-Modelle</h2><p>Ein bilinguales Embedding-Modell ist ein Modell, das Texte in zwei Sprachen – in diesem Tutorial Deutsch und Englisch, für das chinesische Modell Chinesisch und Englisch – in denselben Embedding-Raum abbildet. Und zwar so, dass wenn ein deutscher Text und ein englischer Text dasselbe bedeuten, ihre entsprechenden Embedding-Vektoren nahe beieinander liegen.</p><p>Solche Modelle eignen sich sehr gut für sprachübergreifende Informationsabruf-Anwendungen, die wir in diesem Tutorial zeigen werden, können aber auch als Grundlage für RAG-basierte Chatbots, mehrsprachige Textkategorisierung, Zusammenfassung, Stimmungsanalyse und alle anderen Anwendungen dienen, die Embeddings verwenden. Mit solchen Modellen können Sie Texte in beiden Sprachen behandeln, als wären sie in derselben Sprache geschrieben.</p><p>Obwohl viele große Sprachmodelle behaupten, viele verschiedene Sprachen zu unterstützen, unterstützen sie nicht alle Sprachen gleichermaßen. Es gibt zunehmend Fragen zu <a href=\"https://aclanthology.org/2023.findings-eacl.89/?ref=jina-ai-gmbh.ghost.io\">Verzerrungen, die durch die Dominanz des Englischen im Internet</a> und durch <a href=\"https://arxiv.org/abs/2401.05749?ref=jina-ai-gmbh.ghost.io\">die weite Verbreitung maschinell übersetzter Texte</a> verursacht werden. Durch die Konzentration auf zwei Sprachen können wir die Embedding-Qualität für beide besser kontrollieren und Verzerrungen minimieren, während wir viel kleinere Modelle mit ähnlicher oder höherer Leistung produzieren als riesige Modelle, die vorgeben, Dutzende von Sprachen zu beherrschen.</p><p>Die bilingualen Modelle von Jina Embeddings v2 unterstützen 8.192 Eingabe-Kontexttoken, wodurch sie nicht nur zwei Sprachen unterstützen können, sondern auch vergleichsweise große Textsegmente im Vergleich zu ähnlichen Modellen. Dies macht sie ideal für komplexere Anwendungsfälle, bei denen viel mehr Textinformationen in Embeddings verarbeitet werden müssen.</p><h2 id=\"follow-along-on-google-colab\">Machen Sie mit auf Google Colab</h2><p>Dieses Tutorial hat ein <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">begleitendes Notebook</a>, das Sie auf <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Google Colab</a> oder lokal auf Ihrem eigenen System ausführen können.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/feat-embeddings-notebook/notebooks/embeddings/Bilingual_Embeddings.ipynb?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colaboratory</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/cce4fce8bbe78d8bdc0c77a288df9fa7/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\"></div></a></figure><h2 id=\"installing-the-prerequisites\">Installation der Voraussetzungen</h2><p>Stellen Sie sicher, dass die aktuelle Umgebung die relevanten Bibliotheken installiert hat. Sie benötigen die neueste Version von <a href=\"https://pypi.org/project/transformers/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>transformers</code></a>, führen Sie also selbst wenn es bereits installiert ist:</p><pre><code class=\"language-bash\">pip install -U transformers \n</code></pre><p>Dieses Tutorial wird die <a href=\"https://faiss.ai/?ref=jina-ai-gmbh.ghost.io\">FAISS-Bibliothek von Meta</a> für Vektorsuche und -vergleich verwenden. Zur Installation führen Sie aus:</p><pre><code class=\"language-bash\">pip install faiss-cpu\n</code></pre><p>Wir werden auch <a href=\"https://www.crummy.com/software/BeautifulSoup/?ref=jina-ai-gmbh.ghost.io\">Beautiful Soup</a> verwenden, um die Eingabedaten in diesem Tutorial zu verarbeiten, stellen Sie also sicher, dass es installiert ist:</p><pre><code class=\"language-bash\">pip install bs4\n</code></pre><h2 id=\"access-to-hugging-face\">Zugang zu Hugging Face</h2><p>Sie benötigen Zugang zu Hugging Face, insbesondere ein Konto und einen Zugriffstoken zum Herunterladen von Modellen.</p><p><strong>Wenn Sie noch kein Konto bei Hugging Face haben:</strong></p><p>Gehen Sie zu <a href=\"https://huggingface.co/?ref=jina-ai-gmbh.ghost.io\">https://huggingface.co/</a> und Sie sollten oben rechts auf der Seite einen \"Sign Up\"-Button sehen. Klicken Sie darauf und folgen Sie den Anweisungen, um ein neues Konto zu erstellen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png\" class=\"kg-image\" alt=\"Die Hugging Face Webseite mit hervorgehobenem &quot;Sign Up&quot;-Button.\" loading=\"lazy\" width=\"1088\" height=\"887\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Untitled--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Untitled--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--26-.png 1088w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Nachdem Sie sich in Ihr Konto eingeloggt haben:</strong></p><p>Folgen Sie den Anweisungen <a href=\"https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io\">auf der Hugging Face Website</a>, um einen Zugriffstoken zu erhalten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/docs/hub/security-tokens?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">User access tokens</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://huggingface.co/front/thumbnails/docs/hub.png\" alt=\"\"></div></a></figure><p>Sie müssen dieses Token in eine Umgebungsvariable namens <code>HF_TOKEN</code> kopieren. Wenn Sie in einem Notebook (zum Beispiel in <a href=\"https://colab.research.google.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Google Colab</a>) arbeiten oder es intern in einem Python-Programm setzen, verwenden Sie folgenden Python-Code:</p><pre><code class=\"language-python\">import os\n\nos.environ['HF_TOKEN'] = \"&lt;your token here&gt;\"\n</code></pre><p>In Ihrer Shell verwenden Sie die jeweilige Syntax zum Setzen einer Umgebungsvariable. In <code>bash</code>:</p><pre><code class=\"language-bash\">export HF_TOKEN=\"&lt;your token here&gt;\"\n</code></pre><h2 id=\"download-jina-embeddings-v2-for-german-and-english\">Jina Embeddings v2 für Deutsch und Englisch herunterladen</h2><p>Sobald Ihr Token gesetzt ist, können Sie das zweisprachige Deutsch-Englisch Jina Embeddings Modell mit der <code>transformers</code> Bibliothek herunterladen:</p><pre><code class=\"language-python\">from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-de', trust_remote_code=True)\n</code></pre><p>Dies kann beim ersten Mal mehrere Minuten dauern, aber das Modell wird danach lokal zwischengespeichert, also keine Sorge, wenn Sie dieses Tutorial später neu starten.</p><h2 id=\"download-english-language-data\">Englischsprachige Daten herunterladen</h2><p>Für dieses Tutorial werden wir die englischsprachige Version des Buches <a href=\"https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io\"><em>Pro Git: Everything You Need to Know About Git</em></a> verwenden. Dieses Buch ist auch auf Chinesisch und Deutsch verfügbar, was wir später in diesem Tutorial nutzen werden.</p><p>Um die EPUB-Version herunterzuladen, führen Sie folgenden Befehl aus:</p><pre><code class=\"language-bash\">wget -O progit-en.epub https://open.umn.edu/opentextbooks/formats/3437</code></pre><p>Dies kopiert das Buch in eine Datei namens <code>progit-en.epub</code> im lokalen Verzeichnis.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled--27-.png\" class=\"kg-image\" alt=\"Das Cover der Druckausgabe von &quot;Pro Git&quot; von Scott Chacon und Ben Straub.\" loading=\"lazy\" width=\"490\" height=\"647\"><figcaption><span style=\"white-space: pre-wrap;\">Das Cover der Druckausgabe.</span></figcaption></figure><p>Alternativ können Sie auch einfach den Link <a href=\"https://open.umn.edu/opentextbooks/formats/3437?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">https://open.umn.edu/opentextbooks/formats/3437</a> besuchen, um es auf ein lokales Laufwerk herunterzuladen. Es ist unter der <a href=\"https://creativecommons.org/licenses/by-nc-sa/3.0/?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Creative Commons Attribution Non Commercial Share Alike 3.0 Lizenz</a> verfügbar.</p><h2 id=\"processing-the-data\">Verarbeitung der Daten</h2><p>Dieser spezielle Text hat eine interne Struktur aus hierarchischen Abschnitten, die wir leicht finden können, indem wir nach dem <code>&lt;section&gt;</code> Tag in den zugrundeliegenden XHTML-Daten suchen. Der folgende Code liest die EPUB-Datei und teilt sie anhand der internen Struktur einer EPUB-Datei und des <code>&lt;section&gt;</code> Tags auf, konvertiert dann jeden Abschnitt in Klartext ohne XHTML-Tags. Er erstellt ein Python-Dictionary, dessen Schlüssel eine Reihe von Zeichenketten sind, die den Standort jedes Abschnitts im Buch angeben, und dessen Werte der Klartext-Inhalt dieses Abschnitts sind.</p><pre><code class=\"language-python\">from zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nimport copy\n\ndef decompose_epub(file_name):\n    \n    def to_top_text(section):\n        selected = copy.copy(section)\n\t\t\t\twhile next_section := selected.find(\"section\"):\n            next_section.decompose()\n        return selected.get_text().strip()\n\n    ret = {}\n    with ZipFile(file_name, 'r') as zip:\n        for name in zip.namelist():\n            if name.endswith(\".xhtml\"):\n                data = zip.read(name)\n                doc = BeautifulSoup(data.decode('utf-8'), 'html.parser')\n                ret[name + \":top\"] = to_top_text(doc)\n                for num, sect in enumerate(doc.find_all(\"section\")):\n                    ret[name + f\"::{num}\"] = to_top_text(sect)\n    return ret\n</code></pre><p>Führen Sie dann die Funktion <code>decompose_epub</code> für die zuvor heruntergeladene EPUB-Datei aus:</p><pre><code class=\"language-python\">book_data = decompose_epub(\"progit-en.epub\")\n</code></pre><p>Die Variable <code>book_data</code> wird nun 583 Abschnitte enthalten. Zum Beispiel:</p><pre><code class=\"language-python\">print(book_data['EPUB/ch01-getting-started.xhtml::12'])\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-Text\">The Command Line\nThere are a lot of different ways to use Git.\nThere are the original command-line tools, and there are many graphical user interfaces of varying capabilities.\nFor this book, we will be using Git on the command line.\nFor one, the command line is the only place you can run all Git commands — most of the GUIs implement only a partial subset of Git functionality for simplicity.\nIf you know how to run the command-line version, you can probably also figure out how to run the GUI version, while the opposite is not necessarily true.\nAlso, while your choice of graphical client is a matter of personal taste, all users will have the command-line tools installed and available.\nSo we will expect you to know how to open Terminal in macOS or Command Prompt or PowerShell in Windows.\nIf you don't know what we're talking about here, you may need to stop and research that quickly so that you can follow the rest of the examples and descriptions in this book.\n</code></pre><h2 id=\"generating-and-indexing-embeddings-with-jina-embeddings-v2-and-faiss\">Generierung und Indexierung von Embeddings mit Jina Embeddings v2 und FAISS</h2><p>Für jeden der 583 Abschnitte werden wir ein Embedding generieren und in einem FAISS-Index speichern. Jina Embeddings v2 Modelle akzeptieren Eingaben von bis zu 8192 Tokens, was groß genug ist, dass wir für ein Buch wie dieses keine weitere Textsegmentierung durchführen oder überprüfen müssen, ob ein Abschnitt zu viele Tokens hat. Der längste Abschnitt im Buch hat ungefähr 12.000 Zeichen, was für normales Englisch weit unter der 8k Token-Grenze liegen sollte.</p><p>Um ein einzelnes Embedding zu generieren, verwenden Sie die <code>encode</code> Methode des heruntergeladenen Modells. Zum Beispiel:</p><pre><code class=\"language-python\">model.encode([book_data['EPUB/ch01-getting-started.xhtml::12']])\n</code></pre><p>Dies gibt ein Array mit einem einzelnen 768-dimensionalen Vektor zurück:</p><pre><code class=\"language-python\">array([[ 6.11135997e-02,  1.67829826e-01, -1.94809273e-01,\n         4.45595086e-02,  3.28837298e-02, -1.33441269e-01,\n         1.35364473e-01, -1.23119736e-02,  7.51526654e-02,\n        -4.25386652e-02, -6.91794455e-02,  1.03527725e-01,\n        -2.90831417e-01, -6.21018047e-03, -2.16205455e-02,\n        -2.20803712e-02,  1.50471330e-01, -3.31433356e-01,\n        -1.48741454e-01, -2.10959971e-01,  8.80039856e-02,\n\t\t\t\t....\n</code></pre><p>Das ist ein Embedding.</p><p>Jina Embeddings Modelle sind für die Batch-Verarbeitung eingerichtet. Die optimale Batch-Größe hängt von der Hardware ab, die Sie beim Ausführen verwenden. Eine große Batch-Größe riskiert, dass der Speicher ausgeht. Eine kleine Batch-Größe wird länger zur Verarbeitung brauchen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Das Setzen von <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">batch_size=5</code> funktionierte auf Google Colab in der kostenlosen Version ohne GPU und brauchte <b><strong style=\"white-space: pre-wrap;\">etwa eine Stunde</strong></b>, um den gesamten Satz von Embeddings zu generieren.</div></div><p>In der Produktion empfehlen wir die Verwendung von leistungsfähigerer Hardware oder die Nutzung des <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Embedding API-Service</a> von Jina AI. Folgen Sie dem Link unten, um herauszufinden, wie es funktioniert und wie Sie mit kostenlosem Zugang beginnen können.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure><p>Der folgende Code generiert die Embeddings und speichert sie in einem FAISS-Index. Setzen Sie die Variable <code>batch_size</code> entsprechend Ihren Ressourcen.</p><pre><code class=\"language-python\">import faiss\n\nbatch_size = 5\n\nvector_data = []\nfaiss_index = faiss.IndexFlatIP(768)\n\ndata = [(key, txt) for key, txt in book_data.items()]\nbatches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n\nfor ind, batch in enumerate(batches):\n    print(f\"Processing batch {ind + 1} of {len(batches)}\")\n    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)\n    vector_data.extend(batch)\n    faiss_index.add(batch_embeddings)\n</code></pre><p>Bei der Arbeit in einer Produktionsumgebung ist ein Python-Dictionary keine angemessene oder leistungsfähige Möglichkeit, Dokumente und Embeddings zu handhaben. Sie sollten eine speziell entwickelte Vektordatenbank verwenden, die ihre eigenen Anweisungen für die Dateneinfügung hat.</p><h2 id=\"querying-in-german-for-english-results\">Auf Deutsch abfragen für englische Ergebnisse</h2><p>Wenn wir etwas aus diesem Textset abfragen, passiert Folgendes:</p><ol><li>Das Jina Embeddings Deutsch-Englisch Modell erstellt ein Embedding für die Abfrage.</li><li>Wir verwenden den FAISS-Index (<code>faiss_index</code>), um das gespeicherte Embedding mit dem höchsten Kosinus zur Abfrage-Embedding zu erhalten und seine Position im Index zurückzugeben.</li><li>Wir schauen den entsprechenden Text im Vector-Data-Array (<code>vector_data</code>) nach und geben den Kosinus, den Standort des Textes und den Text selbst aus.</li></ol><p>Das macht die folgende <code>query</code> Funktion.</p><pre><code class=\"language-python\">def query(query_str):\n    query = model.encode([query_str], normalize_embeddings=True)\n    cosine, index = faiss_index.search(query, 1)\n    print(f\"Cosine: {cosine[0][0]}\")\n    loc, txt = vector_data[index[0][0]]\n    print(f\"Location: {loc}\\\\nText:\\\\n\\\\n{txt}\")\n</code></pre><p>Lassen Sie uns das ausprobieren.</p><pre><code class=\"language-python\"># Translation: \"How do I roll back to a previous version?\"\nquery(\"Wie kann ich auf eine frühere Version zurücksetzen?\")\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">Cosine: 0.5202275514602661\nLocation: EPUB/ch02-git-basics-chapter.xhtml::20\nText:\n\nUndoing things with git restore\nGit version 2.23.0 introduced a new command: git restore.\nIt's basically an alternative to git reset which we just covered.\nFrom Git version 2.23.0 onwards, Git will use git restore instead of git reset for many undo operations.\nLet's retrace our steps, and undo things with git restore instead of git reset.\n</code></pre><p>Dies ist eine sehr gute Antwort auf die Frage. Versuchen wir eine weitere:</p><pre><code class=\"language-python\"># Translation: \"What does 'version control' mean?\"\nquery(\"Was bedeutet 'Versionsverwaltung'?\")\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">Cosine: 0.5001817941665649\nLocation: EPUB/ch01-getting-started.xhtml::1\nText:\n\nAbout Version Control\n\nWhat is \"version control\", and why should you care?\nVersion control is a system that records changes to a file or set of files over time so that you can recall specific versions later.\nFor the examples in this book, you will use software source code as the files being version controlled, though in reality you can do this with nearly any type of file on a computer.\nIf you are a graphic or web designer and want to keep every version of an image or layout (which you would most certainly want to), a Version Control System (VCS) is a very wise thing to use.\nIt allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more.\nUsing a VCS also generally means that if you screw things up or lose files, you can easily recover.\nIn addition, you get all this for very little overhead.\n</code></pre><p>Probieren Sie es mit Ihren eigenen deutschen Fragen aus, um zu sehen, wie gut es funktioniert. Als allgemeine Praxis sollten Sie bei der Textinformationssuche drei bis fünf Antworten anfordern statt nur einer. Die beste Antwort ist oft nicht die erste.</p><h2 id=\"reversing-the-roles-querying-german-documents-with-english\">Die Rollen umkehren: Deutsche Dokumente mit Englisch abfragen</h2><p>Das Buch <a href=\"https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git?ref=jina-ai-gmbh.ghost.io\"><em>Pro Git: Everything You Need to Know About Git</em></a> ist auch <a href=\"https://open.umn.edu/opentextbooks/textbooks/pro-git-everything-you-need-to-know-about-git-german?ref=jina-ai-gmbh.ghost.io\">auf Deutsch verfügbar</a>. Wir können dasselbe Modell verwenden, um diese Demo mit umgekehrten Sprachen durchzuführen.</p><p>E-Book herunterladen:</p><pre><code class=\"language-bash\">wget -O progit-de.epub https://open.umn.edu/opentextbooks/formats/3454\n</code></pre><p>Dies kopiert das Buch in eine Datei namens <code>progit-de.epub</code>. Dann verarbeiten wir es auf die gleiche Weise wie beim englischen Buch:</p><pre><code class=\"language-python\">book_data = decompose_epub(\"progit-de.epub\")\n</code></pre><p>Und dann generieren wir die Embeddings auf die gleiche Weise wie zuvor:</p><pre><code class=\"language-python\">batch_size = 5\n\nvector_data = []\nfaiss_index = faiss.IndexFlatIP(768)\n\ndata = [(key, txt) for key, txt in book_data.items()]\nbatches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n\nfor ind, batch in enumerate(batches):\n    print(f\"Processing batch {ind + 1} of {len(batches)}\")\n    batch_embeddings = model.encode([x[1] for x in batch], normalize_embeddings=True)\n    vector_data.extend(batch)\n    faiss_index.add(batch_embeddings)\n</code></pre><p>Wir können jetzt die gleiche <code>query</code>-Funktion verwenden, um auf Englisch nach Antworten auf Deutsch zu suchen:</p><pre><code class=\"language-python\">query(\"What is version control?\")\n</code></pre><p>Ergebnis:</p><pre><code class=\"language-text\">Cosine: 0.6719034910202026\nLocation: EPUB/ch01-getting-started.xhtml::1\nText:\n\nWas ist Versionsverwaltung?\n\nWas ist „Versionsverwaltung\", und warum sollten Sie sich dafür interessieren?\nVersionsverwaltung ist ein System, welches die Änderungen an einer oder einer Reihe von Dateien über die Zeit hinweg protokolliert, sodass man später auf eine bestimmte Version zurückgreifen kann.\nDie Dateien, die in den Beispielen in diesem Buch unter Versionsverwaltung gestellt werden, enthalten Quelltext von Software, tatsächlich kann in der Praxis nahezu jede Art von Datei per Versionsverwaltung nachverfolgt werden.\nAls Grafik- oder Webdesigner möchte man zum Beispiel in der Lage sein, jede Version eines Bildes oder Layouts nachverfolgen zu können. Als solcher wäre es deshalb ratsam, ein Versionsverwaltungssystem (engl. Version Control System, VCS) einzusetzen.\nEin solches System erlaubt es, einzelne Dateien oder auch ein ganzes Projekt in einen früheren Zustand zurückzuversetzen, nachzuvollziehen, wer zuletzt welche Änderungen vorgenommen hat, die möglicherweise Probleme verursachen, herauszufinden wer eine Änderung ursprünglich vorgenommen hat und viele weitere Dinge.\nEin Versionsverwaltungssystem bietet allgemein die Möglichkeit, jederzeit zu einem vorherigen, funktionierenden Zustand zurückzukehren, auch wenn man einmal Mist gebaut oder aus irgendeinem Grund Dateien verloren hat.\nAll diese Vorteile erhält man für einen nur sehr geringen, zusätzlichen Aufwand.\n</code></pre><p>Dieser Abschnittstitel übersetzt sich als <em>\"What is version control?\"</em>, also ist dies eine gute Antwort.</p><h2 id=\"querying-in-chinese\">Abfragen auf Chinesisch</h2><p>Diese Beispiele funktionieren auf genau die gleiche Weise mit Jina Embeddings v2 für Chinesisch und Englisch. Um stattdessen das chinesische Modell zu verwenden, führen Sie einfach Folgendes aus:</p><pre><code class=\"language-python\">from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)\n</code></pre><p>Und um die chinesische Ausgabe von <em>Pro Git: Everything You Need to Know About Git</em> zu erhalten:</p><pre><code class=\"language-python\">wget -O progit-zh.epub https://open.umn.edu/opentextbooks/formats/3455\n</code></pre><p>Dann verarbeiten Sie das chinesische Buch:</p><pre><code class=\"language-python\">book_data = decompose_epub(\"progit-zh.epub\")\n</code></pre><p>Der gesamte andere Code in diesem Tutorial wird auf die gleiche Weise funktionieren.</p><h2 id=\"the-future-more-languages-including-programming\">Die Zukunft: Mehr Sprachen, einschließlich Programmierung</h2><p>Wir werden in der unmittelbaren Zukunft weitere zweisprachige Modelle einführen, wobei Spanisch und Japanisch bereits in Arbeit sind, sowie ein Modell, das Englisch und mehrere wichtige Programmiersprachen unterstützt. Diese Modelle eignen sich ideal für internationale Unternehmen, die mehrsprachige Informationen verwalten, und können als Grundstein für KI-gestützte Informationsgewinnung und RAG-basierte generative Sprachmodelle dienen, die sich in verschiedene moderne KI-Anwendungsfälle einfügen.</p><p>Jina AIs Modelle sind kompakt und gehören zu den besten ihrer Klasse, was zeigt, dass man nicht das größte Modell braucht, um die beste Leistung zu erzielen. Durch den Fokus auf zweisprachige Leistung produzieren wir Modelle, die sowohl besser in diesen Sprachen sind, sich leichter anpassen lassen und kostengünstiger sind als große Modelle, die mit unkurierten Daten trainiert wurden.</p><p>Jina Embeddings sind verfügbar auf <a href=\"https://huggingface.co/jinaai?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>, im <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a> zur Verwendung in Sagemaker und über die <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings Web API</a>. Sie sind vollständig in viele KI-Prozess-Frameworks und Vektordatenbanken integriert.</p><p>Besuchen Sie die <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings Website</a> für weitere Informationen oder kontaktieren Sie uns, um zu besprechen, wie Jina AIs Angebote in Ihre Geschäftsprozesse passen können.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Top-performing, 8192-token context length, $100 for 1.25B tokens, seamless OpenAI alternative, free trial</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure>",
  "comment_id": "65b3adb510ff9f0001c50c4d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Blog-images--32-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-01-26T14:03:49.000+01:00",
  "updated_at": "2024-02-05T17:19:35.000+01:00",
  "published_at": "2024-01-26T17:14:56.000+01:00",
  "custom_excerpt": "Jina AI's open-source bilingual embedding models for German-English and Chinese-English are now on Hugging Face.\nWe’re going to walk through installation and cross-language retrieval.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v2-bilingual-models-are-now-open-source-on-hugging-face/",
  "excerpt": "Jina AIs Open-Source bilinguale Embedding-Modelle für Deutsch-Englisch und Chinesisch-Englisch sind jetzt auf Hugging Face verfügbar.\nWir zeigen die Installation und sprachübergreifende Suche.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Colorful \"EMBEDDINGS\" text above a pile of yellow smileys on a black background with decorative lines at the top.",
  "feature_image_caption": null
}
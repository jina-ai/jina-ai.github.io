{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "Binäre Embeddings: Die komplette KI mit nur 3,125% des Speicherbedarfs",
  "html": "<p>Embeddings sind zum Eckpfeiler verschiedener KI- und Natural Language Processing-Anwendungen geworden und bieten eine Möglichkeit, die Bedeutung von Texten als hochdimensionale Vektoren darzustellen. Mit der zunehmenden Größe der Modelle und der wachsenden Datenmenge, die KI-Modelle verarbeiten, sind jedoch die Rechen- und Speicheranforderungen für traditionelle Embeddings gestiegen. Binäre Embeddings wurden als kompakte, effiziente Alternative eingeführt, die hohe Leistung bei drastisch reduzierten Ressourcenanforderungen beibehält.</p><p>Binäre Embeddings sind eine Möglichkeit, diese Ressourcenanforderungen zu reduzieren, indem die Größe der Embedding-Vektoren um bis zu 96% verringert wird (96,875% im Fall von Jina Embeddings). Benutzer können die Leistungsfähigkeit kompakter binärer Embeddings in ihren KI-Anwendungen mit minimalem Genauigkeitsverlust nutzen.</p><h2 id=\"what-are-binary-embeddings\">Was sind Binäre Embeddings?</h2><p>Binäre Embeddings sind eine spezielle Form der Datenrepräsentation, bei der traditionelle hochdimensionale Gleitkomma-Vektoren in binäre Vektoren umgewandelt werden. Dies komprimiert nicht nur die Embeddings, sondern behält auch fast die gesamte Integrität und Nützlichkeit der Vektoren bei. Das Wesentliche dieser Technik liegt in ihrer Fähigkeit, die Semantik und relationalen Abstände zwischen den Datenpunkten auch nach der Konvertierung beizubehalten.<br><br>Die Magie hinter binären Embeddings ist die Quantisierung, eine Methode, die hochpräzise Zahlen in Zahlen mit niedrigerer Präzision umwandelt. In der KI-Modellierung bedeutet dies oft, die 32-Bit-Gleitkommazahlen in Embeddings in Darstellungen mit weniger Bits, wie 8-Bit-Ganzzahlen, umzuwandeln.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Binarisierung ist die Umwandlung aller Skalare in 0 oder 1, wie die Konvertierung eines Farbbildes in ein Bild mit nur schwarzen oder weißen Pixeln. Bild: 神奈川沖浪裏 (1831) von 葛飾 (Hokusai)</span></figcaption></figure><p>Binäre Embeddings treiben dies auf die Spitze, indem sie jeden Wert auf 0 oder 1 reduzieren. Die Umwandlung von 32-Bit-Gleitkommazahlen in binäre Ziffern reduziert die Größe der Embedding-Vektoren um das 32-fache, eine Reduktion von 96,875%. Vektoroperationen auf den resultierenden Embeddings sind dadurch viel schneller. Die Nutzung von Hardware-Beschleunigungen, die auf einigen Mikrochips verfügbar sind, kann die Geschwindigkeit von Vektorvergleichen bei binarisierten Vektoren um weit mehr als das 32-fache erhöhen.</p><p>Bei diesem Prozess gehen zwangsläufig einige Informationen verloren, aber dieser Verlust wird minimiert, wenn das Modell sehr leistungsfähig ist. Wenn die nicht-quantisierten Embeddings verschiedener Dinge maximal unterschiedlich sind, dann ist es wahrscheinlicher, dass die Binarisierung diesen Unterschied gut bewahrt. Andernfalls kann es schwierig sein, die Embeddings korrekt zu interpretieren.</p><p>Jina Embeddings Modelle sind darauf trainiert, genau in dieser Hinsicht sehr robust zu sein, was sie für die Binarisierung besonders geeignet macht.</p><p>Solche kompakten Embeddings ermöglichen neue KI-Anwendungen, insbesondere in ressourcenbeschränkten Kontexten wie mobilen und zeitkritischen Anwendungen.</p><p>Diese Kosten- und Rechenzeitvorteile kommen mit relativ geringen Leistungseinbußen, wie die folgende Grafik zeigt.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10: Werte berechnet mit </em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">Normalized Discounted Cumulative Gain</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\"> für die Top 10 Ergebnisse.</em></i></figcaption></figure><p>Für <code>jina-embeddings-v2-base-en</code> reduziert die binäre Quantisierung die Abrufgenauigkeit von 47,13% auf 42,05%, ein Verlust von etwa 10%. Für <code>jina-embeddings-v2-base-de</code> beträgt dieser Verlust nur 4%, von 44,39% auf 42,65%.</p><p>Jina Embeddings Modelle schneiden bei der Erzeugung binärer Vektoren so gut ab, weil sie darauf trainiert sind, eine gleichmäßigere Verteilung von Embeddings zu erzeugen. Dies bedeutet, dass zwei verschiedene Embeddings wahrscheinlich in mehr Dimensionen weiter voneinander entfernt sind als Embeddings aus anderen Modellen. Diese Eigenschaft stellt sicher, dass diese Abstände besser durch ihre binären Formen repräsentiert werden.</p><h2 id=\"how-do-binary-embeddings-work\">Wie funktionieren Binäre Embeddings?</h2><p>Um zu verstehen, wie das funktioniert, betrachten wir drei Embeddings: <em>A</em>, <em>B</em> und <em>C</em>. Diese drei sind alle vollständige Gleitkomma-Vektoren, keine binarisierten. Nehmen wir an, der Abstand von <em>A</em> zu <em>B</em> ist größer als der Abstand von <em>B</em> zu <em>C</em>. Bei Embeddings verwenden wir typischerweise die <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">Kosinus-Distanz</a>, also: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>Wenn wir <em>A</em>, <em>B</em> und <em>C</em> binarisieren, können wir den Abstand effizienter mit der <a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">Hamming-Distanz</a> messen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Hamming-Distanz auf einem Würfel. Links: Abstand von A zu B ist 1. Rechts: Abstand von B zu C ist 2.</span></figcaption></figure><p>Nennen wir <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em> und <em>C<sub>bin</sub></em> die binarisierten Versionen von <em>A</em>, <em>B</em> und <em>C</em>.</p>\n<p>Bei binären Vektoren gilt: Wenn die Kosinus-Distanz zwischen <em>A<sub>bin</sub></em> und <em>B<sub>bin</sub></em> größer ist als zwischen <em>B<sub>bin</sub></em> und <em>C<sub>bin</sub></em>, dann ist die Hamming-Distanz zwischen <em>A<sub>bin</sub></em> und <em>B<sub>bin</sub></em> größer oder gleich der Hamming-Distanz zwischen <em>B<sub>bin</sub></em> und <em>C<sub>bin</sub></em>.</p>\n<p>Also wenn: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>dann gilt für Hamming-Distanzen: </p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Idealerweise wollen wir bei der Binarisierung von Embeddings, dass die gleichen Beziehungen wie bei den vollständigen Embeddings auch für die binären Embeddings gelten. Das bedeutet, wenn ein Abstand größer als ein anderer für Gleitkomma-Kosinus ist, sollte er auch für die Hamming-Distanz zwischen ihren binarisierten Äquivalenten größer sein:</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Wir können dies nicht für alle Tripel von Embeddings garantieren, aber wir können es für fast alle erreichen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Die blauen Punkte entsprechen den vollständigen Gleitkomma-Vektoren und die roten ihren binarisierten Äquivalenten. </span></figcaption></figure><p>Bei einem binären Vektor können wir jede Dimension entweder als vorhanden (eine Eins) oder nicht vorhanden (eine Null) behandeln. Je weiter zwei Vektoren in nicht-binärer Form voneinander entfernt sind, desto höher ist die Wahrscheinlichkeit, dass in einer bestimmten Dimension einer einen positiven Wert und der andere einen negativen Wert hat. Das bedeutet, dass es in binärer Form höchstwahrscheinlich mehr Dimensionen gibt, wo einer eine Null und der andere eine Eins hat. Dies macht sie nach Hamming-Distanz weiter voneinander entfernt.</p><p>Das Gegenteil gilt für Vektoren, die näher beieinander liegen: Je näher die nicht-binären Vektoren sind, desto höher ist die Wahrscheinlichkeit, dass in einer Dimension beide Nullen oder beide Einsen haben. Dies macht sie nach Hamming-Distanz näher beieinander.</p><p>Jina Embeddings Modelle sind für die Binarisierung so gut geeignet, weil wir sie mit Negative Mining und anderen Fine-Tuning-Praktiken trainieren, um besonders den Abstand zwischen unähnlichen Dingen zu vergrößern und den Abstand zwischen ähnlichen zu verringern. Dies macht die Embeddings robuster, empfindlicher für Ähnlichkeiten und Unterschiede und sorgt dafür, dass die Hamming-Distanz zwischen binären Embeddings proportionaler zur Kosinus-Distanz zwischen nicht-binären ist.</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">Wie viel kann ich mit Jina AIs Binären Embeddings sparen?</h2><p>Die Nutzung von Jina AIs binären Embedding-Modellen senkt nicht nur die Latenz in zeitkritischen Anwendungen, sondern bringt auch erhebliche Kostenvorteile, wie die folgende Tabelle zeigt:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Speicher pro<br/>250 Millionen<br/>Embeddings</th>\n<th>Retrieval<br/>Benchmark<br/>Durchschnitt</th>\n<th>Geschätzter Preis auf AWS<br/>($3,8 pro GB/Monat<br/>mit x2gb Instanzen)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32-bit Gleitkomma-Embeddings</td>\n<td>715 GB</td>\n<td>47,13</td>\n<td>$35.021</td>\n</tr>\n<tr>\n<td>Binäre Embeddings</td>\n<td>22,3 GB</td>\n<td>42,05</td>\n<td>$1.095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Diese Einsparung von über 95 % geht mit einer nur etwa 10%igen Reduzierung der Abrufgenauigkeit einher.</p><p>Diese Einsparungen sind sogar noch größer als bei der Verwendung binärisierter Vektoren von <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">OpenAIs Ada 2 Modell</a> oder <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Coheres Embed v3</a>, die beide Ausgabe-Embeddings von 1024 oder mehr Dimensionen erzeugen. Jina AIs Embeddings haben nur 768 Dimensionen und liefern dennoch vergleichbare Leistung wie andere Modelle, wodurch sie selbst vor der Quantisierung bei gleicher Genauigkeit kleiner sind.</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Binäre Vektoren sparen Speicherplatz, Rechenzeit, Übertragungsbandbreite und Festplattenspeicher und bieten dadurch finanzielle Vorteile in mehreren Kategorien</strong></b>.</div></div><p>Diese Einsparungen sind auch umweltfreundlich, da sie weniger seltene Materialien und weniger Energie verbrauchen.</p><h2 id=\"get-started\">Erste Schritte</h2><p>Um binäre Embeddings über die <a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a> zu erhalten, fügen Sie einfach den Parameter <code>encoding_type</code> zu Ihrem API-Aufruf hinzu, mit dem Wert <code>binary</code> für das als vorzeichenbehaftete Ganzzahlen kodierte binäre Embedding oder <code>ubinary</code> für vorzeichenlose Ganzzahlen.</p><h3 id=\"directly-access-jina-embedding-api\">Direkter Zugriff auf die Jina Embedding API</h3><p>Mit <code>curl</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>Oder über die Python <code>requests</code> API:</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>Mit dem obigen Python <code>request</code> erhalten Sie die folgende Antwort durch Überprüfung von <code>response.json()</code>:</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>Dies sind zwei binäre Embedding-Vektoren, die als 96 8-Bit vorzeichenbehaftete Ganzzahlen gespeichert sind. Um sie in 768 Nullen und Einsen zu entpacken, müssen Sie die <code>numpy</code> Bibliothek verwenden:</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>Das Ergebnis ist ein 768-dimensionaler Vektor, der nur aus Nullen und Einsen besteht:</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Verwendung der binären Quantisierung in Qdrant</h3><p>Sie können auch die <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">Qdrant-Integrationsbibliothek</a> verwenden, um binäre Embeddings direkt in Ihrem Qdrant Vector Store zu speichern. Da Qdrant <code>BinaryQuantization</code> intern implementiert hat, können Sie es als voreingestellte Konfiguration für die gesamte Vektorsammlung verwenden, wodurch binäre Vektoren ohne weitere Änderungen an Ihrem Code abgerufen und gespeichert werden können.</p><p>Sehen Sie sich das Beispiel im folgenden Code an:</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Stellen Sie den Jina API-Schlüssel bereit und wählen Sie eines der verfügbaren Modelle.\n# Sie können hier einen kostenlosen Testschlüssel erhalten: https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # oder \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 für kleine Variante\n\n# Embeddings von der API abrufen\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Die Embeddings in Qdrant indexieren\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>Für die Suchkonfiguration sollten Sie die Parameter <code>oversampling</code> und <code>rescore</code> verwenden:</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">Verwendung von LlamaIndex</h3><p>Um Jina Binary Embeddings mit LlamaIndex zu verwenden, setzen Sie den Parameter <code>encoding_queries</code> auf <code>binary</code> bei der Instanziierung des <code>JinaEmbedding</code> Objekts:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# Sie können einen kostenlosen Testschlüssel von https://jina.ai/embeddings/ erhalten\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">Andere Vektordatenbanken mit Unterstützung für binäre Embeddings</h3><p>Die folgenden Vektordatenbanken bieten native Unterstützung für binäre Vektoren:</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB von DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">Beispiel</h2><p>Um binäre Embeddings in Aktion zu zeigen, haben wir eine Auswahl von Abstracts von <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a> genommen und sowohl 32-Bit-Fließkomma- als auch binäre Vektoren mit <code>jina-embeddings-v2-base-en</code> erstellt. Dann haben wir sie mit den Embeddings für eine Beispielabfrage verglichen: \"3D segmentation.\"</p><p>Wie Sie aus der untenstehenden Tabelle ersehen können, sind die Top-3-Antworten identisch und vier der Top-5 stimmen überein. Die Verwendung binärer Vektoren liefert nahezu identische Top-Treffer.</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>Rang</th>\n<th>Hamming<br/>Dist.</th>\n<th>Übereinstimmender Text</th>\n<th>Kosinus</th>\n<th>Übereinstimmender Text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "32-Bit sind sehr viel Präzision für etwas so Robustes und Ungenaues wie ein KI-Modell. Also haben wir 31 davon weggelassen! Binäre Embeddings sind kleiner, schneller und äußerst leistungsfähig.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
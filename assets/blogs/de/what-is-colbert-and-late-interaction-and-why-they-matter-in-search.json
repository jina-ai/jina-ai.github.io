{
  "slug": "what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
  "id": "65d3a2134a32310001f5b71b",
  "uuid": "726c942b-f6a7-4c89-a0ad-39aaad98d02f",
  "title": "Was ist ColBERT und Late Interaction und warum sind sie wichtig für die Suche?",
  "html": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina ColBERT v2: Mehrsprachiger Late Interaction Retriever für Embedding und Reranking</div><div class=\"kg-bookmark-description\">Jina ColBERT v2 unterstützt 89 Sprachen mit überlegener Retrieval-Leistung, benutzergesteuerten Ausgabedimensionen und 8192 Token-Länge.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg\" alt=\"\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Update: Am 31. August 2024 haben wir die zweite Version von Jina-ColBERT veröffentlicht, mit verbesserter Leistung, mehrsprachiger Unterstützung für 89 Sprachen und flexiblen Ausgabedimensionen. Weitere Details finden Sie im Release-Post.</span></p></figcaption></figure><p>Letzten Freitag sorgte die Veröffentlichung des <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">ColBERT-Modells von Jina AI auf Hugging Face</a> für große Begeisterung in der KI-Community, besonders auf Twitter/X. Während viele mit dem bahnbrechenden BERT-Modell vertraut sind, fragen sich einige angesichts der Aufregung um ColBERT: Was macht ColBERT im überfüllten Bereich der Informationsabruf-Technologien so besonders? Warum ist die KI-Community so begeistert von ColBERT mit 8192 Token Länge? Dieser Artikel geht auf die Feinheiten von ColBERT und ColBERTv2 ein und beleuchtet deren Design, Verbesserungen und die überraschende Effektivität von ColBERTs Late Interaction.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Maximieren Sie die Suchrelevanz und RAG-Genauigkeit mit Leichtigkeit</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">@huggingface</a> under Apache 2.0 licence<a href=\"https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io\">https://t.co/snVGgI753H</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">February 16, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><h2 id=\"what-is-colbert\">Was ist ColBERT?</h2><p>Der Name \"ColBERT\" steht für <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, ein Modell der Stanford University, das das tiefe Sprachverständnis von BERT nutzt und einen neuartigen Interaktionsmechanismus einführt. Dieser Mechanismus, bekannt als <strong>Late Interaction</strong>, ermöglicht einen effizienten und präzisen Abruf, indem Anfragen und Dokumente bis zu den letzten Stufen des Abrufprozesses separat verarbeitet werden. Es gibt konkret zwei Versionen des Modells:</p><ul><li><strong>ColBERT</strong>: Das ursprüngliche Modell war die Erfindung von <a href=\"https://x.com/lateinteraction?s=20&ref=jina-ai-gmbh.ghost.io\"><strong>Omar Khattab</strong></a><strong> und Matei Zaharia</strong>, die einen neuartigen Ansatz für den Informationsabruf durch das Paper \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\" vorstellten. Ihre Arbeit wurde auf der SIGIR 2020 veröffentlicht.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class=\"kg-bookmark-description\">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Omar Khattab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Das ursprüngliche ColBERT-Paper, das die \"Late Interaction\" einführt.</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Aufbauend auf der Grundlagenarbeit setzte <strong>Omar Khattab</strong> seine Forschung fort und führte in Zusammenarbeit mit <strong>Barlas Oguz, Matei Zaharia und Michael S. Bernstein</strong> \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\" ein, das auf der SIGIR 2021 vorgestellt wurde. Diese nächste Iteration von ColBERT adressierte frühere Einschränkungen und führte wichtige Verbesserungen ein, wie <strong>denoised supervision</strong> und <strong>residual compression</strong>, die sowohl die Abrufeffektivität als auch die Speichereffizienz des Modells verbesserten.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class=\"kg-bookmark-description\">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Keshav Santhanam</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">ColBERTv2 fügt denoised supervision und residual compression hinzu, um die Qualität der Trainingsdaten zu verbessern und den Speicherbedarf zu reduzieren.</span></p></figcaption></figure><h2 id=\"understand-colberts-design\">ColBERTs Design verstehen</h2><p>Da ColBERTv2s Architektur der des ursprünglichen ColBERT sehr ähnlich bleibt und seine wichtigsten Innovationen sich um Trainingstechniken und Kompressionsmechanismen drehen, werden wir zunächst die grundlegenden Aspekte des ursprünglichen ColBERT betrachten.</p><h3 id=\"what-is-late-interaction-in-colbert\">Was ist Late Interaction in ColBERT?</h3><p>\"Interaction\" bezieht sich auf den Prozess der Bewertung der Relevanz zwischen einer Anfrage und einem Dokument durch den Vergleich ihrer Repräsentationen.</p><p>\"<em>Late Interaction</em>\" ist der Kern von ColBERT. Der Begriff leitet sich von der Architektur und Verarbeitungsstrategie des Modells ab, bei der die Interaktion zwischen den Repräsentationen von Anfrage und Dokument erst spät im Prozess stattfindet, nachdem beide unabhängig voneinander codiert wurden. Dies steht im Gegensatz zu \"<em>Early Interaction</em>\"-Modellen, bei denen Query- und Dokument-Embeddings in früheren Stadien interagieren, typischerweise vor oder während ihrer Codierung durch das Modell.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Models</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Early Interaction</td>\n<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>\n</tr>\n<tr>\n<td>Late Interaction</td>\n<td>ColBERT, ColBERTv2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Early Interaction kann die Berechnungskomplexität erhöhen, da alle möglichen Query-Dokument-Paare berücksichtigt werden müssen, was es weniger effizient für großangelegte Anwendungen macht.</p><p>Late Interaction-Modelle wie ColBERT optimieren die Effizienz und Skalierbarkeit, indem sie die Vorberechnung von Dokumentrepräsentationen ermöglichen und einen leichtgewichtigeren Interaktionsschritt am Ende einsetzen, der sich auf die bereits codierten Repräsentationen konzentriert. Diese Designentscheidung ermöglicht schnellere Abrufzeiten und reduzierte Rechenanforderungen, was es besser geeignet für die Verarbeitung großer Dokumentensammlungen macht.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg\" class=\"kg-image\" alt=\"Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep\" loading=\"lazy\" width=\"300\" height=\"143\"><figcaption><span style=\"white-space: pre-wrap;\">Schematische Diagramme zur Veranschaulichung von Query-Dokument-Interaktionsparadigmen im neuronalen IR, mit ColBERTs später Interaktion ganz links.</span></figcaption></figure><h3 id=\"no-interaction-cosine-similarity-of-document-and-query-embeddings\">Keine Interaktion: Kosinus-Ähnlichkeit von Dokument- und Query-Embeddings</h3><p>Viele praktische Vektordatenbanken und neuronale Suchlösungen basieren auf schneller Kosinus-Ähnlichkeitsberechnung zwischen Dokument- und Query-Embeddings. Während diese Methode aufgrund ihrer Einfachheit und Recheneffizienz attraktiv ist, hat sich gezeigt, dass dieser als \"keine Interaktion\" oder \"nicht interaktionsbasiert\" bezeichnete Ansatz im Vergleich zu Modellen, die eine Form der Interaktion zwischen Queries und Dokumenten beinhalten, schlechter abschneidet.</p><p>Die zentrale Einschränkung des \"keine Interaktion\"-Ansatzes liegt in seiner Unfähigkeit, die komplexen Nuancen und Beziehungen zwischen Query- und Dokumentbegriffen zu erfassen. Information Retrieval dreht sich im Kern darum, die Absicht hinter einer Query mit dem Inhalt eines Dokuments abzugleichen und zu verstehen. Dieser Prozess erfordert oft ein tiefes, kontextbezogenes Verständnis der beteiligten Begriffe - etwas, das einzelne, aggregierte Embeddings für Dokumente und Queries nur schwer leisten können.</p><h2 id=\"query-and-document-encoders-in-colbert\">Query- und Dokument-Encoder in ColBERT</h2><p>ColBERTs Encoding-Strategie basiert auf dem BERT-Modell, das für sein tiefes kontextuelles Sprachverständnis bekannt ist. Das Modell generiert dichte Vektordarstellungen für jeden Token in einer Query oder einem Dokument und <strong>erstellt dabei jeweils einen Bag of Contextualized Embeddings für eine Query und einen für ein Dokument.</strong> Dies ermöglicht einen nuancierten Vergleich ihrer Embeddings während der späten Interaktionsphase.</p><h3 id=\"query-encoder-of-colbert\">Query-Encoder von ColBERT</h3><p>Für eine Query $Q$ mit Tokens ${q_1, q_2, ..., q_l}$ beginnt der Prozess mit der Tokenisierung von $Q$ in BERT-basierte WordPiece-Tokens und dem Voranstellen eines speziellen <code>[Q]</code>-Tokens. Dieser <code>[Q]</code>-Token, der direkt nach BERTs <code>[CLS]</code>-Token positioniert wird, signalisiert den Beginn einer Query.</p><p>Wenn die Query kürzer als eine vordefinierte Anzahl von Tokens $N_q$ ist, wird sie mit <code>[mask]</code>-Tokens bis zu $N_q$ aufgefüllt; andernfalls wird sie auf die ersten $N_q$ Tokens gekürzt. Die aufgefüllte Sequenz wird dann durch BERT geleitet, gefolgt von einem CNN (Convolutional Neural Network) und Normalisierung. Das Ergebnis ist ein Set von Embedding-Vektoren, unten als $\\mathbf{E}_q$ bezeichnet:<br>$$\\mathbf{E}_q := \\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)$$</p><h3 id=\"document-encoder-of-colbert\">Dokument-Encoder von ColBERT</h3><p>Ähnlich wird für ein Dokument $D$ mit Tokens ${d_1, d_2, ..., d_n}$ ein <code>[D]</code>-Token vorangestellt, um den Beginn eines Dokuments anzuzeigen. Diese Sequenz durchläuft, ohne Auffüllung zu benötigen, den gleichen Prozess und ergibt ein Set von Embedding-Vektoren, unten als $\\mathbf{E}_d$ bezeichnet:<br>$$\\mathbf{E}_d := \\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)$$</p><p>Die Verwendung von <code>[mask]</code>-Tokens zum Auffüllen von Queries (im Paper als <strong>Query-Augmentation</strong> bezeichnet) gewährleistet eine einheitliche Länge über alle Queries hinweg und erleichtert die Batch-Verarbeitung. Die <code>[Q]</code>- und <code>[D]</code>-Tokens markieren explizit den Beginn von Queries bzw. Dokumenten und helfen dem Modell dabei, zwischen den beiden Eingabetypen zu unterscheiden.</p><h3 id=\"comparing-colbert-to-cross-encoders\">Vergleich von ColBERT mit Cross-Encodern</h3><p>Cross-Encoder verarbeiten Paare von Queries und Dokumenten gemeinsam, was sie sehr genau, aber aufgrund der Rechenkosten für die Auswertung jedes möglichen Paars weniger effizient für großangelegte Aufgaben macht. Sie eignen sich besonders für spezifische Szenarien, in denen die präzise Bewertung von Satzpaaren erforderlich ist, wie bei Aufgaben zur semantischen Ähnlichkeit oder detaillierten Inhaltsvergleichen. Dieses Design beschränkt jedoch ihre Anwendbarkeit in Situationen, die eine schnelle Abfrage aus großen Datensätzen erfordern, wo vorberechnete Embeddings und effiziente Ähnlichkeitsberechnungen von entscheidender Bedeutung sind.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg\" class=\"kg-image\" alt=\"Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum\" loading=\"lazy\" width=\"210\" height=\"150\"></figure><p>Im Gegensatz dazu ermöglicht ColBERTs spätes Interaktionsmodell die Vorberechnung von Dokument-Embeddings, was den Abrufprozess deutlich beschleunigt, ohne die Tiefe der semantischen Analyse zu beeinträchtigen. Diese Methode, obwohl im Vergleich zum direkten Ansatz der Cross-Encoder scheinbar kontraintuitiv, bietet eine skalierbare Lösung für Echtzeit- und großangelegte Information-Retrieval-Aufgaben. Sie stellt einen strategischen Kompromiss zwischen Recheneffizienz und Qualität der Interaktionsmodellierung dar.</p><h2 id=\"finding-the-top-k-documents-using-colbert\">Finden der Top-K-Dokumente mit ColBERT</h2><p>Sobald wir Embeddings für die Query und Dokumente haben, wird das Finden der relevantesten Top-K-Dokumente überschaubar (aber nicht so einfach wie die Berechnung des Kosinus zweier Vektoren).</p><p>Die Hauptoperationen umfassen ein Batch-Skalarprodukt zur Berechnung der termweisen Ähnlichkeiten, Max-Pooling über Dokumentterme zur Ermittlung der höchsten Ähnlichkeit pro Query-Term und Summierung über Query-Terme zur Ableitung des Gesamtdokumentscores, gefolgt von der Sortierung der Dokumente basierend auf diesen Scores. Der Pseudo-PyTorch-Code wird unten beschrieben:</p><pre><code class=\"language-python\">import torch\n\ndef compute_relevance_scores(query_embeddings, document_embeddings, k):\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n    \n    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]\n    :param k: Number of top documents to re-rank\n    :return: Sorted document indices based on their relevance scores\n    \"\"\"\n    \n    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]\n    # Pad the k documents to their maximum length for batch operations\n    # Note: Assuming document_embeddings is already padded and moved to GPU\n    \n    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)\n    # Resulting shape: [k, num_query_terms, max_doc_length]\n    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n    \n    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term\n    # Shape after max-pool: [k, num_query_terms]\n    max_scores_per_query_term = scores.max(dim=2).values\n    \n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [k]\n    total_scores = max_scores_per_query_term.sum(dim=1)\n    \n    # Sort the documents based on their total scores\n    sorted_indices = total_scores.argsort(descending=True)\n    \n    return sorted_indices\n</code></pre><p>Beachten Sie, dass dieses Verfahren sowohl beim Training als auch beim Re-Ranking während der Inferenz verwendet wird. Das ColBERT-Modell wird mit einem paarweisen Ranking-Loss trainiert, wobei die Trainingsdaten aus Tripeln $(q, d^+, d^-)$ bestehen, wobei $q$ eine Query repräsentiert, $d^+$ ein relevantes (positives) Dokument für die Query ist und $d^-$ ein nicht relevantes (negatives) Dokument. Das Modell zielt darauf ab, Repräsentationen zu lernen, bei denen der Ähnlichkeitsscore zwischen $q$ und $d^+$ höher ist als der Score zwischen q und $d^-$.</p><p>Das Trainingsziel kann mathematisch als Minimierung der folgenden Verlustfunktion dargestellt werden: $$\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, wobei $S(q, d)$ den von ColBERT berechneten Ähnlichkeitsscore zwischen einer Query $q$ und einem Dokument $d$ bezeichnet. Dieser Score wird durch Aggregation der Max-Ähnlichkeitsscores der am besten übereinstimmenden Embeddings zwischen Query und Dokument ermittelt, dem in der Modellarchitektur beschriebenen späten Interaktionsmuster folgend. Dieser Ansatz stellt sicher, dass das Modell darauf trainiert wird, zwischen relevanten und irrelevanten Dokumenten für eine gegebene Query zu unterscheiden, indem ein größerer Abstand in den Ähnlichkeitsscores für positive und negative Dokumentpaare gefördert wird.</p><h3 id=\"denoised-supervision-in-colbertv2\">Entrauschte Supervision in ColBERTv2</h3><p>Die entrauschte Supervision in ColBERTv2 verfeinert den ursprünglichen Trainingsprozess durch die Auswahl anspruchsvoller Negative und die Nutzung eines Cross-Encoders für die Destillation. Diese ausgefeilte Methode zur Verbesserung der Trainingsdatenqualität umfasst mehrere Schritte:</p><ol><li><strong>Initiales Training</strong>: Verwendung der offiziellen Tripel aus dem MS MARCO-Datensatz, bestehend aus einer Query, einem relevanten Dokument und einem nicht relevanten Dokument.</li><li><strong>Indexierung und Abruf</strong>: Einsatz von ColBERTv2's Kompression zur Indexierung von Trainingspassagen, gefolgt vom Abruf der Top-k-Passagen für jede Query.</li><li><strong>Cross-Encoder Reranking</strong>: Verbesserung der Passagenauswahl durch Reranking mittels eines MiniLM Cross-Encoders, dessen Scores in ColBERTv2 destilliert werden.</li><li><strong>Bildung von Trainingstupeln</strong>: Generierung von w-way Tupeln für das Training, die sowohl hoch als auch niedriger bewertete Passagen einbeziehen, um anspruchsvolle Beispiele zu erstellen.</li><li><strong>Iterative Verfeinerung</strong>: Wiederholung des Prozesses zur kontinuierlichen Verbesserung der Auswahl harter Negative, wodurch die Modellleistung gesteigert wird.</li></ol><p>Beachten Sie, dass dieser Prozess eine ausgefeilte Verbesserung des ColBERT-Trainingsregimes darstellt und keine grundlegende Änderung seiner Architektur.</p><h3 id=\"hyperparameters-of-colbert\">Hyperparameter von ColBERT</h3><p>Die Hyperparameter von ColBERT sind nachfolgend zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Hyperparameter</th>\n<th>Beste Wahl</th>\n<th>Begründung</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning Rate</td>\n<td>3 x 10^{-6}</td>\n<td>Ausgewählt für Fine-Tuning, um stabile und effektive Modellupdates zu gewährleisten.</td>\n</tr>\n<tr>\n<td>Batch Size</td>\n<td>32</td>\n<td>Balanciert Recheneffizienz und die Fähigkeit, ausreichend Informationen pro Update zu erfassen.</td>\n</tr>\n<tr>\n<td>Number of Embeddings per Query (Nq)</td>\n<td>32</td>\n<td>Festgelegt, um eine konsistente Repräsentationsgröße über alle Anfragen hinweg zu gewährleisten, was die effiziente Verarbeitung unterstützt.</td>\n</tr>\n<tr>\n<td>Embedding Dimension (m)</td>\n<td>128</td>\n<td>Erwies sich als gute Balance zwischen Darstellungskraft und Recheneffizienz.</td>\n</tr>\n<tr>\n<td>Training Iterations</td>\n<td>200k (MS MARCO), 125k (TREC CAR)</td>\n<td>Gewählt, um gründliches Lernen sicherzustellen und Overfitting zu vermeiden, mit Anpassungen basierend auf Dataseteigenschaften.</td>\n</tr>\n<tr>\n<td>Bytes per Dimension in Embeddings</td>\n<td>4 (Re-Ranking), 2 (End-to-End Ranking)</td>\n<td>Kompromiss zwischen Präzision und Speichereffizienz, unter Berücksichtigung des Anwendungskontexts (Re-Ranking vs. End-to-End).</td>\n</tr>\n<tr>\n<td>Vector-Similarity Function</td>\n<td>Cosine (Re-Ranking), (Squared) L2 (End-to-End)</td>\n<td>Ausgewählt basierend auf Leistung und Effizienz in den jeweiligen Retrieval-Kontexten.</td>\n</tr>\n<tr>\n<td>FAISS Index Partitions (P)</td>\n<td>2000</td>\n<td>Bestimmt die Granularität der Suchraum-Partitionierung, beeinflusst die Sucheffizienz.</td>\n</tr>\n<tr>\n<td>Nearest Partitions Searched (p)</td>\n<td>10</td>\n<td>Balanciert die Suchbreite gegen Recheneffizienz.</td>\n</tr>\n<tr>\n<td>Sub-vectors per Embedding (s)</td>\n<td>16</td>\n<td>Beeinflusst die Granularität der Quantisierung, wirkt sich auf Suchgeschwindigkeit und Speichernutzung aus.</td>\n</tr>\n<tr>\n<td>Index Representation per Dimension</td>\n<td>16-bit values</td>\n<td>Gewählt für die zweite Phase des End-to-End Retrievals, um den Kompromiss zwischen Genauigkeit und Speicher zu managen.</td>\n</tr>\n<tr>\n<td>Number of Layers in Encoders</td>\n<td>12-layer BERT</td>\n<td>Optimale Balance zwischen Tiefe des kontextuellen Verständnisses und Recheneffizienz.</td>\n</tr>\n  <tr>\n  <td>Max Query Length</td>\n<td>128</td>\n<td>Die maximale Anzahl an Tokens, die vom Query-Encoder verarbeitet werden. <b>Dies wird im Jina-ColBERT Modell erweitert.</b></td>\n</tr>\n    <tr>\n  <td>Max Document Length</td>\n<td>512</td>\n<td>Die maximale Anzahl an Tokens, die vom Dokument-Encoder verarbeitet werden. <b>Dies wird im Jina-ColBERT Modell auf 8192 erweitert.</b></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"the-indexing-strategy-of-colbert\">Die Indexierungsstrategie von ColBERT</h2>\n\n<p>Im Gegensatz zu repräsentationsbasierten Ansätzen, die jedes Dokument in einen Embedding-Vektor kodieren, <strong>kodiert ColBERT Dokumente (und Anfragen) in Embedding-Bags, wobei jedes Token in einem Dokument sein eigenes Embedding hat.</strong> Dieser Ansatz bedeutet naturgemäß, dass für längere Dokumente mehr Embeddings gespeichert werden müssen, <strong>was ein Schwachpunkt des ursprünglichen ColBERT ist und später durch ColBERTv2 adressiert wurde.</strong></p>\n\n<p>Der Schlüssel zum effizienten Management liegt in ColBERTs Verwendung einer Vektordatenbank (z.B. <a href=\"https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io\">FAISS</a>) für Indexierung und Retrieval, sowie seinem detaillierten Indexierungsprozess, der für die effiziente Verarbeitung großer Datenmengen konzipiert ist. Das ursprüngliche ColBERT-Paper erwähnt mehrere Strategien zur Verbesserung der Effizienz von Indexierung und Retrieval, darunter:</p>\n\n<ul>\n<li><strong>Offline Indexierung</strong>: Dokumentrepräsentationen werden offline berechnet, was die Vorberechnung und Speicherung von Dokument-Embeddings ermöglicht. Dieser Prozess nutzt Batch-Verarbeitung und GPU-Beschleunigung, um große Dokumentensammlungen effizient zu verarbeiten.</li>\n<li><strong>Embedding-Speicherung</strong>: Dokument-Embeddings können mit 32-Bit- oder 16-Bit-Werten für jede Dimension gespeichert werden, was einen Kompromiss zwischen Präzision und Speicheranforderungen bietet. Diese Flexibilität ermöglicht es ColBERT, eine Balance zwischen Effektivität (in Bezug auf Retrieval-Leistung) und Effizienz (in Bezug auf Speicher- und Rechenkosten) zu halten.</li>\n</ul>\n\n<p>Die Einführung der <strong>Residual-Kompression</strong> in ColBERTv2, eine neuartige Technik, die im ursprünglichen ColBERT nicht vorhanden war, spielt eine Schlüsselrolle bei der Reduzierung des Speicherbedarfs des Modells um das 6-10-fache bei gleichzeitiger Qualitätserhaltung. Diese Technik komprimiert die Embeddings weiter, indem sie effektiv nur die Unterschiede zu einer Menge fester Referenz-Zentroide erfasst und speichert.</p>\n\n<h2 id=\"effectiveness-and-efficiency-of-colbert\">Effektivität und Effizienz von ColBERT</h2>\n\n<p>Man könnte zunächst annehmen, dass die Integration von BERTs tiefem kontextuellem Verständnis in die Suche inhärent erhebliche Rechenressourcen erfordert, was einen solchen Ansatz aufgrund hoher Latenz und Rechenkosten für Echtzeit-Anwendungen weniger praktikabel macht. ColBERT widerlegt jedoch diese Annahme durch seine innovative Nutzung des Late-Interaction-Mechanismus. Hier sind einige bemerkenswerte Punkte:</p>\n\n<ol>\n<li><strong>Signifikante Effizienzgewinne</strong>: ColBERT erreicht eine um Größenordnungen reduzierte Rechenleistung (FLOPs) und Latenz im Vergleich zu traditionellen BERT-basierten Ranking-Modellen. Speziell für eine gegebene Modellgröße (z.B. 12-Layer \"Base\" Transformer Encoder) erreicht ColBERT nicht nur die gleiche, sondern in einigen Fällen sogar eine bessere Effektivität als BERT-basierte Modelle mit dramatisch geringerem Rechenaufwand. Zum Beispiel benötigt BERT bei einer Re-Ranking-Tiefe von k=10 fast 180-mal mehr FLOPs als ColBERT; diese Lücke vergrößert sich mit steigendem k auf das 13900-fache bei k=1000 und sogar auf das 23000-fache bei k=2000.</li>\n\n<li><strong>Verbesserte Recall und MRR@10 im End-to-End Retrieval</strong>: Entgegen der anfänglichen Intuition, dass eine tiefere Interaktion zwischen Query- und Dokument-Repräsentationen (wie bei frühen Interaktionsmodellen) für eine hohe Retrieval-Leistung notwendig wäre, zeigt ColBERTs End-to-End-Retrieval-Setup überlegene Effektivität. Zum Beispiel übertrifft sein Recall@50 den Recall@1000 des offiziellen BM25 und fast aller anderen Modelle Recall@200, was die bemerkenswerte Fähigkeit des Modells unterstreicht, relevante Dokumente aus einer großen Sammlung ohne direkten Vergleich jedes Query-Dokument-Paars zu finden.</li>\n\n<li><strong>Praktikabilität für reale Anwendungen</strong>: Die experimentellen Ergebnisse unterstreichen ColBERTs praktische Anwendbarkeit für reale Szenarien. Sein Indexierungsdurchsatz und seine Speichereffizienz machen es geeignet für die Indexierung großer Dokumentensammlungen wie MS MARCO innerhalb weniger Stunden, wobei eine hohe Effektivität bei überschaubarem Speicherbedarf erhalten bleibt. Diese Eigenschaften unterstreichen ColBERTs Eignung für den Einsatz in Produktionsumgebungen, wo sowohl Leistung als auch Recheneffizienz von größter Bedeutung sind.</li>\n\n<li><strong>Skalierbarkeit mit der Dokumentensammlungsgröße</strong>: Vielleicht die überraschendste Schlussfolgerung ist ColBERTs Skalierbarkeit und Effizienz im Umgang mit großen Dokumentensammlungen. Die Architektur ermöglicht die Vorberechnung von Dokument-Embeddings und nutzt effiziente Batch-Verarbeitung für die Query-Dokument-Interaktion, wodurch das System effektiv mit der Größe der Dokumentensammlung skaliert. Diese Skalierbarkeit ist kontraintuitiv, wenn man die Komplexität und Tiefe des Verständnisses berücksichtigt, die für ein effektives Dokument-Retrieval erforderlich sind, und zeigt ColBERTs innovativen Ansatz zur Ausbalancierung von Recheneffizienz und Retrieval-Effektivität.</li>\n</ol>\n\n<h2 id=\"using-jina-colbert-v1-en-a-8192-length-colbertv2-model\">Verwendung von <code>jina-colbert-v1-en</code>: ein ColBERTv2-Modell mit 8192 Token Länge</h2>\n\n<p>Jina-ColBERT ist sowohl für schnelles als auch genaues Retrieval konzipiert und unterstützt <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">längere Kontextlängen bis zu 8192, wobei es die Fortschritte von JinaBERT nutzt</a>, das aufgrund seiner verbesserten Architektur längere Sequenzverarbeitung ermöglicht.</p>\n\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Streng genommen unterstützt Jina-ColBERT eine Länge von 8190 Token. Zur Erinnerung: Im ColBERT-Dokumenten-Encoder wird jedes Dokument am Anfang mit <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">[D],[CLS]</code> gepadded.</div></div>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-colbert-v1-en · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png\" alt=\"\"></div></a></figure>\n\n<h3 id=\"jinas-improvement-over-original-colbert\">Jinas Verbesserungen gegenüber dem ursprünglichen ColBERT</h3>\n\n<p>Jina-ColBERTs wichtigste Weiterentwicklung ist sein Backbone, <code>jina-bert-v2-base-en</code>, der die Verarbeitung von deutlich längeren Kontexten (bis zu 8192 Token) im Vergleich zum ursprünglichen ColBERT ermöglicht, der <code>bert-base-uncased</code> verwendet. Diese Fähigkeit ist entscheidend für die Verarbeitung von Dokumenten mit umfangreichem Inhalt und liefert detailliertere und kontextbezogenere Suchergebnisse.</p>\n\n<h3 id=\"jina-colbert-v1-en-performance-comparison-vs-colbertv2\"><code>jina-colbert-v1-en</code> Leistungsvergleich vs. ColBERTv2</h3>\n\n<p>Wir haben <code>jina-colbert-v1-en</code> auf BEIR-Datensätzen und dem neuen LoCo-Benchmark evaluiert, der lange Kontexte bevorzugt, und es gegen die ursprüngliche ColBERTv2-Implementierung und nicht-interaktionsbasierte</p><code>jina-embeddings-v2-base-en</code> Modell.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>ColBERTv2</th>\n<th>jina-colbert-v1-en</th>\n<th>jina-embeddings-v2-base-en</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Arguana</td>\n<td>46.5</td>\n<td><strong>49.4</strong></td>\n<td>44.0</td>\n</tr>\n<tr>\n<td>Climate-Fever</td>\n<td>18.1</td>\n<td>19.6</td>\n<td><strong>23.5</strong></td>\n</tr>\n<tr>\n<td>DBPedia</td>\n<td><strong>45.2</strong></td>\n<td>41.3</td>\n<td>35.1</td>\n</tr>\n<tr>\n<td>FEVER</td>\n<td>78.8</td>\n<td><strong>79.5</strong></td>\n<td>72.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td>35.4</td>\n<td>36.8</td>\n<td><strong>41.6</strong></td>\n</tr>\n<tr>\n<td>HotpotQA</td>\n<td><strong>67.5</strong></td>\n<td>65.9</td>\n<td>61.4</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>33.7</td>\n<td><strong>33.8</strong></td>\n<td>32.5</td>\n</tr>\n<tr>\n<td>NQ</td>\n<td>56.1</td>\n<td>54.9</td>\n<td><strong>60.4</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>85.5</td>\n<td>82.3</td>\n<td><strong>88.2</strong></td>\n</tr>\n<tr>\n<td>SCIDOCS</td>\n<td>15.4</td>\n<td>16.9</td>\n<td><strong>19.9</strong></td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td>68.9</td>\n<td><strong>70.1</strong></td>\n<td>66.7</td>\n</tr>\n<tr>\n<td>TREC-COVID</td>\n<td>72.6</td>\n<td><strong>75.0</strong></td>\n<td>65.9</td>\n</tr>\n<tr>\n<td>Webis-touch2020</td>\n<td>26.0</td>\n<td><strong>27.0</strong></td>\n<td>26.2</td>\n</tr>\n<tr>\n<td>LoCo</td>\n<td>74.3</td>\n<td>83.7</td>\n<td><strong>85.4</strong></td>\n</tr>\n<tr>\n<td>Average</td>\n<td>51.7</td>\n<td><strong>52.6</strong></td>\n<td>51.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Diese Tabelle zeigt die überlegene Leistung von <code>jina-colbert-v1-en</code>, besonders in Szenarien, die längere Kontextlängen erfordern, im Vergleich zum ursprünglichen ColBERTv2. Beachten Sie, dass <code>jina-embeddings-v2-base-en</code> <a href=\"https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io\">mehr Trainingsdaten verwendet</a>, während <code>jina-colbert-v1-en</code> nur MSMARCO nutzt, was die gute Leistung von <code>jina-embeddings-v2-base-en</code> bei einigen Aufgaben erklären könnte.</p><h3 id=\"example-usage-of-jina-colbert-v1-en\">Beispielverwendung von <code>jina-colbert-v1-en</code></h3><p>Dieser Codeausschnitt beschreibt den Indizierungsprozess mit Jina-ColBERT und zeigt dessen Unterstützung für lange Dokumente.</p><pre><code class=\"language-python\">from colbert import Indexer\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\n\nn_gpu: int = 1  # Set your number of available GPUs\nexperiment: str = \"\"  # Name of the folder where the logs and created indices will be stored\nindex_name: str = \"\"  # The name of your index, i.e. the name of your vector database\n\nif __name__ == \"__main__\":\n    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):\n        config = ColBERTConfig(\n          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents\n        )\n        indexer = Indexer(\n          checkpoint=\"jinaai/jina-colbert-v1-en\",\n          config=config,\n        )\n        documents = [\n          \"ColBERT is an efficient and effective passage retrieval model.\",\n          \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.\",\n          \"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.\",\n          \"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.\",\n          \"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.\",\n          \"Jina is an easier way to build neural search systems.\",\n          \"You can use Jina-ColBERT to build neural search systems with ease.\",\n          # Add more documents here to ensure the clustering work correctly\n        ]\n        indexer.index(name=index_name, collection=documents)\n</code></pre><h3 id=\"use-jina-colbert-v1-en-in-ragatouille\">Verwendung von <code>jina-colbert-v1-en</code> in RAGatouille</h3><p>RAGatouille ist eine neue Python-Bibliothek, die die Verwendung fortschrittlicher Abrufmethoden in RAG-Pipelines erleichtert. Sie ist auf Modularität und einfache Integration ausgelegt und ermöglicht Benutzern, modernste Forschung nahtlos zu nutzen. Das Hauptziel von RAGatouille ist es, die Anwendung komplexer Modelle wie ColBERT in RAG-Pipelines zu vereinfachen und es Entwicklern zu ermöglichen, diese Methoden ohne tiefgehendes Fachwissen in der zugrundeliegenden Forschung zu nutzen. Dank <a href=\"https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io\">Benjamin Clavié</a> können Sie <code>jina-colbert-v1-en</code> jetzt einfach verwenden:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\n# Get your model &amp; collection of big documents ready\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v1-en\")\nmy_documents = [\n    \"very long document1\",\n    \"very long document2\",\n    # ... more documents\n]\n\n# And create an index with them at full length!\nRAG.index(collection=my_documents,\n          index_name=\"the_biggest_index\",\n          max_document_length=8190,)\n\n# or encode them in-memory with no truncation, up to your model's max length\nRAG.encode(my_documents)\n</code></pre><p>Für detailliertere Informationen und weitere Erkundung von Jina-ColBERT können Sie die <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">Hugging Face Seite</a> besuchen.</p><h2 id=\"conclusion\">Fazit</h2><p>ColBERT stellt einen bedeutenden Fortschritt im Bereich des Information Retrieval dar. Durch die Ermöglichung längerer Kontextlängen mit Jina-ColBERT und die Beibehaltung der Kompatibilität mit dem ColBERT-Ansatz zur späten Interaktion bietet es eine leistungsstarke Alternative für Entwickler, die modernste Suchfunktionalität implementieren möchten.</p><p>In Verbindung mit der RAGatouille-Bibliothek, die die Integration komplexer Abrufmodelle in RAG-Pipelines vereinfacht, können Entwickler nun die Leistungsfähigkeit fortschrittlicher Abrufmethoden problemlos nutzen, ihre Arbeitsabläufe optimieren und ihre Anwendungen verbessern. Die Synergie zwischen Jina-ColBERT und RAGatouille zeigt einen bemerkenswerten Fortschritt bei der Zugänglichkeit und Effizienz fortschrittlicher KI-Suchmodelle für den praktischen Einsatz.</p>",
  "comment_id": "65d3a2134a32310001f5b71b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-02-19T19:46:43.000+01:00",
  "updated_at": "2024-08-30T23:11:22.000+02:00",
  "published_at": "2024-02-20T02:19:04.000+01:00",
  "custom_excerpt": "Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
  "excerpt": "Jina AIs ColBERT auf Hugging Face sorgt auf Twitter für Aufsehen und bringt mit seiner 8192-Token-Fähigkeit eine neue Perspektive in die Suche. Dieser Artikel erklärt die Feinheiten von ColBERT und ColBERTv2, zeigt ihre innovativen Designs und erläutert, warum ihre Late-Interaction-Funktion die Suche revolutioniert.",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon theater or concert hall marquee letters lit up at night with city lights and faint \"Adobe Sto\" visible.",
  "feature_image_caption": null
}
{
  "id": "671b96784821eb000165d2de",
  "uuid": "ec571b8c-d111-4d49-bad8-2836bd885f1c",
  "title": "Jenseits von CLIP: Wie Jina-CLIP die multimodale Suche voranbringt",
  "slug": "beyond-clip-how-jina-clip-advances-multimodal-search",
  "html": "<p>Die multimodale Suche, die Text und Bilder zu einem nahtlosen Sucherlebnis verbindet, hat dank Modellen wie <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">OpenAIs CLIP</a> an Bedeutung gewonnen. Diese Modelle überbrücken effektiv die Lücke zwischen visuellen und textuellen Daten und ermöglichen es uns, Bilder mit relevantem Text und umgekehrt zu verbinden.</p><p>Während CLIP und ähnliche Modelle leistungsstark sind, haben sie bemerkenswerte Einschränkungen, insbesondere bei der Verarbeitung längerer Texte oder beim Umgang mit komplexen textuellen Beziehungen. Hier kommt <code>jina-clip-v1</code> ins Spiel.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image</div><div class=\"kg-bookmark-description\">Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Entwickelt zur Bewältigung dieser Herausforderungen bietet <code>jina-clip-v1</code> ein verbessertes Textverständnis bei gleichzeitiger Beibehaltung robuster Text-Bild-Matching-Fähigkeiten. Es bietet eine optimierte Lösung für Anwendungen, die beide Modalitäten nutzen, vereinfacht den Suchprozess und macht es überflüssig, zwischen separaten Modellen für Text und Bilder zu jonglieren.</p><p>In diesem Beitrag werden wir untersuchen, was <code>jina-clip-v1</code> für multimodale Suchanwendungen bringt und Experimente vorstellen, die zeigen, wie es sowohl die Genauigkeit als auch die Vielfalt der Ergebnisse durch integrierte Text- und Bild-Embeddings verbessert.</p><h2 id=\"what-is-clip\">Was ist CLIP?</h2><p>CLIP (Contrastive Language–Image Pretraining) ist eine von OpenAI entwickelte KI-Modellarchitektur, die Text und Bilder durch das Lernen gemeinsamer Repräsentationen verbindet. CLIP ist im Wesentlichen ein Textmodell und ein Bildmodell, die miteinander verschmolzen sind - es transformiert beide Eingabearten in einen gemeinsamen Embedding-Raum, in dem ähnliche Texte und Bilder nahe beieinander positioniert werden. CLIP wurde mit einem riesigen Datensatz von Bild-Text-Paaren trainiert, wodurch es die Beziehung zwischen visuellem und textuellem Inhalt verstehen kann. Dies ermöglicht eine gute Generalisierung über verschiedene Domänen hinweg und macht es hocheffektiv in Zero-Shot-Learning-Szenarien wie der Generierung von Bildunterschriften oder Bildabruf.</p><p>Seit der Veröffentlichung von CLIP haben andere Modelle wie <a href=\"https://arxiv.org/abs/2303.15343?ref=jina-ai-gmbh.ghost.io\">SigLiP</a>, <a href=\"https://arxiv.org/abs/2111.07991?ref=jina-ai-gmbh.ghost.io\">LiT</a> und <a href=\"https://arxiv.org/abs/2303.15389?ref=jina-ai-gmbh.ghost.io\">EvaCLIP</a> auf dessen Grundlage aufgebaut und Aspekte wie Trainingseffizienz, Skalierung und multimodales Verständnis verbessert. Diese Modelle nutzen häufig größere Datensätze, verbesserte Architekturen und ausgefeiltere Trainingstechniken, um die Grenzen der Text-Bild-Ausrichtung zu erweitern und das Feld der Bild-Sprach-Modelle weiter voranzubringen.</p><p>Während CLIP zwar auch mit reinem Text arbeiten <em>kann</em>, hat es erhebliche Einschränkungen. Erstens wurde es nur mit kurzen Bildunterschriften trainiert, nicht mit langen Texten, und verarbeitet maximal etwa 77 Wörter. Zweitens ist CLIP zwar hervorragend darin, Text mit Bildern zu verbinden, hat aber Schwierigkeiten beim Vergleich von Text mit anderem Text, wie zum Beispiel zu erkennen, dass die Zeichenfolgen <code>a crimson fruit</code> und <code>a red apple</code> sich auf dasselbe beziehen können. Hier glänzen spezialisierte Textmodelle wie <code>jina-embeddings-v3</code>.</p><p>Diese Einschränkungen erschweren Suchaufgaben, die sowohl Text als auch Bilder beinhalten, zum Beispiel einen \"Shop the Look\" Online-Shop, in dem ein Benutzer Modeprodukte entweder mit einer Textzeichenfolge oder einem Bild suchen kann. Beim Indexieren Ihrer Produkte müssen Sie jedes mehrmals verarbeiten - einmal für das Bild, einmal für den Text und noch einmal mit einem textspezifischen Modell. Ebenso muss Ihr System bei einer Produktsuche durch den Benutzer mindestens zweimal suchen, um sowohl Text- als auch Bildziele zu finden:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png\" class=\"kg-image\" alt=\"Flowchart outlining &quot;Offline Indexing&quot; and &quot;Online Querying&quot; processes with labeled blocks and arrows for XML data interactio\" loading=\"lazy\" width=\"970\" height=\"1255\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png 970w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-jina-clip-v1-solves-clip%E2%80%99s-shortcomings\"><strong>Wie </strong><code>jina-clip-v1</code><strong> CLIPs Schwächen löst</strong></h2><p>Um CLIPs Einschränkungen zu überwinden, haben wir <code>jina-clip-v1</code> entwickelt, um längere Texte zu verstehen und Textanfragen effektiver sowohl mit Texten als auch mit Bildern abzugleichen. Was macht <code>jina-clip-v1</code> so besonders? Erstens verwendet es ein intelligenteres Textverständnismodell (JinaBERT), das längere und kompliziertere Texte (wie Produktbeschreibungen) versteht, nicht nur kurze Bildunterschriften (wie Produktnamen). Zweitens haben wir <code>jina-clip-v1</code> trainiert, gleichzeitig zwei Dinge gut zu können: sowohl das Matching von Text zu Bildern als auch das Matching von Text zu anderen Texten.</p><p>Bei OpenAI CLIP ist das nicht der Fall: Sowohl beim Indexieren als auch beim Abfragen müssen Sie zwei Modelle aufrufen (CLIP für Bilder und kurze Texte wie Bildunterschriften, ein weiteres Text-Embedding für längere Texte wie Beschreibungen). Das verursacht nicht nur Overhead, sondern verlangsamt auch die Suche, eine Operation, die eigentlich <em>sehr</em> schnell sein sollte. <code>jina-clip-v1</code> erledigt all das in einem Modell, ohne Geschwindigkeitseinbußen:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-22.png\" class=\"kg-image\" alt=\"Flowchart of JaclinQ's offline indexing and online querying processes, involving imagery and text analysis.\" loading=\"lazy\" width=\"2000\" height=\"2785\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/10/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dieser einheitliche Ansatz eröffnet neue Möglichkeiten, die mit früheren Modellen schwierig waren, und verändert möglicherweise unsere Herangehensweise an die Suche. In diesem Beitrag haben wir zwei Experimente durchgeführt:</p><ul><li><strong>Verbesserung der Suchergebnisse durch Kombination von Text- und Bildsuche</strong>: Können wir das, was <code>jina-clip-v1</code> aus Text versteht, mit dem kombinieren, was es aus Bildern versteht? Was passiert, wenn wir diese beiden Arten des Verstehens vermischen? Verändert das Hinzufügen visueller Informationen unsere Suchergebnisse? Kurz gesagt, können wir bessere Ergebnisse erzielen, wenn wir gleichzeitig mit Text und Bildern suchen?</li><li><strong>Verwendung von Bildern zur Diversifizierung der Suchergebnisse</strong>: Die meisten Suchmaschinen maximieren Text-Matches. Aber können wir das Bildverständnis von <code>jina-clip-v1</code> als \"visuelles Shuffle\" nutzen? Anstatt nur die relevantesten Ergebnisse anzuzeigen, könnten wir visuell diverse einbeziehen. Es geht nicht darum, mehr verwandte Ergebnisse zu finden – sondern darum, ein breiteres Spektrum an Perspektiven zu zeigen, auch wenn sie weniger eng verwandt sind. Dadurch können wir möglicherweise Aspekte eines Themas entdecken, die wir vorher nicht bedacht hatten. Zum Beispiel im Kontext der Modesuche: Wenn ein Benutzer nach \"mehrfarbiges Cocktailkleid\" sucht, möchte er, dass die Top-Listings alle gleich aussehen (d.h. <em>sehr</em> enge Übereinstimmungen) oder eine größere Auswahl zur Verfügung haben (durch visuelles Shuffle)?</li></ul><p>Beide Ansätze sind wertvoll für verschiedene Anwendungsfälle, bei denen Benutzer entweder mit Text oder Bildern suchen können, wie zum Beispiel im E-Commerce, in den Medien, in Kunst und Design, in der medizinischen Bildgebung und darüber hinaus.</p><h2 id=\"averaging-text-and-image-embeddings-for-above-average-performance\">Durchschnittliche Text- und Bild-Embeddings für überdurchschnittliche Leistung</h2><p>Wenn ein Benutzer eine Anfrage eingibt (normalerweise als Textzeichenfolge), können wir den Text-Tower von <code>jina-clip-v1</code> verwenden, um die Anfrage in ein Text-Embedding zu codieren. Die Stärke von <code>jina-clip-v1</code> liegt in seiner Fähigkeit, sowohl Text als auch Bilder zu verstehen, indem es Text-zu-Text- und Text-zu-Bild-Signale im gleichen semantischen Raum ausrichtet.</p><p>Können wir die Abrufergebnisse verbessern, wenn wir die vorindexierten Text- und Bild-Embeddings jedes Produkts durch Mittelwertbildung kombinieren?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png\" class=\"kg-image\" alt=\"Flowchart on a black background detailing text and image embedding processes with a black knit midi dress photo example.\" loading=\"lazy\" width=\"995\" height=\"359\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-28.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png 995w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dies erzeugt eine einzige Repräsentation, die sowohl textuelle Informationen (z.B. Produktbeschreibung) als auch visuelle Informationen (z.B. Produktbild) enthält. Wir können dann das Text-Query-Embedding verwenden, um diese gemischten Repräsentationen zu durchsuchen. Wie wirkt sich das auf unsere Suchergebnisse aus?</p><p>Um das herauszufinden, verwendeten wir den <a href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\">Fashion200k</a> Datensatz, einen umfangreichen Datensatz, der speziell für Aufgaben im Zusammenhang mit Mode-Bildabruf und modalitätsübergreifendem Verständnis erstellt wurde. Er besteht aus über 200.000 Bildern von Modeartikel wie Kleidung, Schuhe und Accessoires, zusammen mit entsprechenden Produktbeschreibungen und Metadaten.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - xthan/fashion-200k: Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\"</div><div class=\"kg-bookmark-description\">Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\" - xthan/fashion-200k</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">xthan</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2116651d448aec6ea0508f5fdb123e6292fa00bfb1cf8fb6f3468cbe761da769/xthan/fashion-200k\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Wir haben jedes Element weiter in eine breite Kategorie (zum Beispiel <code>dress</code>) und eine feingranulare Kategorie (wie <code>knit midi dress</code>) eingeordnet.</p><h3 id=\"analyzing-three-retrieval-methods\"><strong>Analyse von drei Retrieval-Methoden</strong></h3><p>Um herauszufinden, ob die Mittelung von Text- und Bild-Embeddings bessere Retrieval-Ergebnisse liefert, haben wir drei Arten der Suche getestet, die alle eine Textzeichenfolge (z.B. <code>red dress</code>) als Query verwenden:</p><ul><li><strong>Query to Description mit Text-Embeddings:</strong> Suche in Produktbeschreibungen basierend auf Text-Embeddings.</li><li><strong>Query to Image mit Cross-Modal-Suche:</strong> Suche in Produktbildern basierend auf Bild-Embeddings.</li><li><strong>Query to Average Embedding:</strong> Suche in gemittelten Embeddings von Produktbeschreibungen und Produktbildern.</li></ul><p>Wir haben zunächst den gesamten Datensatz indiziert und dann zufällig 1.000 Queries generiert, um die Performance zu evaluieren. Wir haben jede Query in ein Text-Embedding umgewandelt und das Embedding separat nach den oben beschriebenen Methoden abgeglichen. Die Genauigkeit haben wir daran gemessen, wie gut die Kategorien der zurückgelieferten Produkte mit der Eingabe-Query übereinstimmten.</p><p>Bei der Query <code>multicolor henley t-shirt dress</code> erreichte die <strong>Query-to-Description</strong>-Suche die höchste Top-5-Präzision, aber die letzten drei der am besten bewerteten Kleider waren visuell identisch. Das ist nicht ideal, da eine effektive Suche Relevanz und Diversität ausbalancieren sollte, um die Aufmerksamkeit des Nutzers besser zu gewinnen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png\" class=\"kg-image\" alt=\"Array of five unique dresses, categorized as casual and day, arranged in a row on a white background with named tags for easy\" loading=\"lazy\" width=\"2000\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Die <strong>Query-to-Image</strong> Cross-Modal-Suche verwendete dieselbe Query und verfolgte den gegenteiligen Ansatz, indem sie eine sehr diverse Kollektion von Kleidern präsentierte. Während zwei von fünf Ergebnissen mit der korrekten breiten Kategorie übereinstimmten, passte keines zur feingranularen Kategorie.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png\" class=\"kg-image\" alt=\"Variety of women's clothing items including short and long-sleeved tops and casual to maxi dresses with color swatches.\" loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Die <strong>gemittelte Text- und Bild-Embedding-Suche</strong> lieferte das beste Ergebnis: Alle fünf Ergebnisse stimmten mit der breiten Kategorie überein und zwei von fünf passten zur feingranularen Kategorie. Zusätzlich wurden visuell duplizierte Artikel eliminiert, was eine abwechslungsreichere Auswahl bot. Die Verwendung von Text-Embeddings zur Suche in gemittelten Text- und Bild-Embeddings scheint die Suchqualität beizubehalten und gleichzeitig visuelle Hinweise zu integrieren, was zu vielfältigeren und ausgewogeneren Ergebnissen führt.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png\" class=\"kg-image\" alt=\"Showcase of various women's dresses, including a multicolor henley t-shirt dress and a pink Missoni dress, labeled with categ\" loading=\"lazy\" width=\"2000\" height=\"513\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"scaling-up-evaluating-with-more-queries\"><strong>Skalierung: Evaluierung mit mehr Queries</strong></h3><p>Um zu sehen, ob dies auch im größeren Maßstab funktionieren würde, führten wir das Experiment mit zusätzlichen breiten und feingranularen Kategorien fort. Wir führten mehrere Iterationen durch und riefen dabei jeweils eine unterschiedliche Anzahl von Ergebnissen (\"k-Werte\") ab.</p><p>Sowohl bei den breiten als auch bei den feingranularen Kategorien erzielte das <strong>Query to Average Embedding</strong> durchgehend die höchste Präzision über alle k-Werte (10, 20, 50, 100). Dies zeigt, dass die Kombination von Text- und Bild-Embeddings die genauesten Ergebnisse für das Abrufen relevanter Artikel liefert, unabhängig davon, ob die Kategorie breit oder spezifisch ist:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png\" class=\"kg-image\" alt=\"Comparative chart of 'Broad Precision@K' and 'Fine-grained Precision@K' showing different precision values for query-related \" loading=\"lazy\" width=\"2000\" height=\"836\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>k</strong></th>\n<th><strong>Search Type</strong></th>\n<th><strong>Broad Category Precision (cosine similarity)</strong></th>\n<th><strong>Fine-grained Category Precision (cosine similarity)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>Query to Description</td>\n<td>0.9026</td>\n<td>0.2314</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Image</td>\n<td>0.7614</td>\n<td>0.2037</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9230</strong></td>\n<td><strong>0.2711</strong></td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Description</td>\n<td>0.9150</td>\n<td>0.2316</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Image</td>\n<td>0.7523</td>\n<td>0.1964</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9229</strong></td>\n<td><strong>0.2631</strong></td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Description</td>\n<td>0.9134</td>\n<td>0.2254</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Image</td>\n<td>0.7418</td>\n<td>0.1750</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9226</strong></td>\n<td><strong>0.2390</strong></td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Description</td>\n<td>0.9092</td>\n<td>0.2139</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Image</td>\n<td>0.7258</td>\n<td>0.1675</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9150</strong></td>\n<td><strong>0.2286</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<ul><li><strong>Query to Description mit Text-Embeddings</strong> zeigte in beiden Kategorien gute Leistungen, lag aber leicht hinter dem gemittelten Embedding-Ansatz zurück. Dies deutet darauf hin, dass textuelle Beschreibungen allein wertvolle Informationen liefern, besonders für breitere Kategorien wie \"dress\", aber möglicherweise die nötige Feinheit für präzise feingranulare Klassifizierung fehlt (z.B. bei der Unterscheidung verschiedener Kleidertypen).</li><li><strong>Query to Image mit Cross-Modal-Suche</strong> hatte durchgehend die niedrigste Präzision in beiden Kategorien. Dies deutet darauf hin, dass visuelle Merkmale zwar bei der Identifizierung breiter Kategorien helfen können, aber weniger effektiv sind, wenn es um die feingranularen Unterscheidungen spezifischer Mode-Artikel geht. Die Herausforderung, feingranulare Kategorien rein anhand visueller Merkmale zu unterscheiden, wird besonders deutlich, wenn visuelle Unterschiede subtil sein können und zusätzlichen Kontext durch Text erfordern.</li><li>Insgesamt erreichte die Kombination von textueller und visueller Information (durch <strong>gemittelte Embeddings</strong>) eine hohe Präzision sowohl bei breiten als auch bei feingranularen Mode-Retrieval-Aufgaben. Textuelle Beschreibungen spielen eine wichtige Rolle, besonders bei der Identifizierung breiter Kategorien, während Bilder allein in beiden Fällen weniger effektiv sind.</li></ul><p>Insgesamt war die Präzision für breite Kategorien deutlich höher als für feingranulare Kategorien, hauptsächlich weil Artikel in breiten Kategorien (z.B. <code>dress</code>) im Datensatz stärker vertreten sind als in feingranularen Kategorien (z.B. <code>henley dress</code>), da letztere einfach eine Teilmenge der ersteren sind. Naturgemäß ist eine breite Kategorie leichter zu verallgemeinern als eine feingranulare Kategorie. Außerhalb des Mode-Beispiels ist es einfach zu erkennen, dass etwas im Allgemeinen ein Vogel ist. Es ist viel schwieriger, ihn als <a href=\"https://www.youtube.com/watch?v=nPhVOZiPokA&ref=jina-ai-gmbh.ghost.io\">Vogelkop Superb Bird of Paradise</a> zu identifizieren.</p><p>Ein weiterer wichtiger Punkt ist, dass die Information in einer Text-Query leichter mit anderen Texten (wie Produktnamen oder Beschreibungen) übereinstimmt als mit visuellen Merkmalen. Wenn also ein Text als Input verwendet wird, sind Texte ein wahrscheinlicheres Output als Bilder. Die besten Ergebnisse erzielen wir durch die Kombination von Bildern und Text (durch Mittelung der Embeddings) in unserem Index.</p><h2 id=\"retrieve-results-with-text-diversify-them-with-images\">Ergebnisse mit Text abrufen; sie mit Bildern diversifizieren</h2><p>Im vorherigen Abschnitt haben wir das Problem visuell duplizierter Suchergebnisse angesprochen. Bei der Suche ist <em>Präzision allein nicht immer ausreichend</em>. In vielen Fällen ist die Aufrechterhaltung einer prägnanten, aber hochrelevanten und diversen Rangliste effektiver, besonders wenn die Query des Nutzers mehrdeutig ist (zum Beispiel, wenn ein Nutzer nach</p><code>black jacket</code> - meinen sie eine schwarze Bikerjacke, Bomberjacke, einen Blazer oder eine andere Art?).</p><p>Anstatt die Cross-Modal-Fähigkeit von <code>jina-clip-v1</code> zu nutzen, verwenden wir jetzt die Text-Embeddings aus seinem Text-Tower für die erste Textsuche und wenden dann die Bild-Embeddings aus dem Image-Tower als \"visuellen Reranker\" an, um die Suchergebnisse zu diversifizieren. Dies wird im folgenden Diagramm veranschaulicht:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png\" class=\"kg-image\" alt=\"Flowchart detailing multimodal document text processing, with branches for text and image embedding and various processing pa\" loading=\"lazy\" width=\"975\" height=\"476\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png 975w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><ol><li>Zuerst werden die Top-k-Suchergebnisse basierend auf Text-Embeddings abgerufen.</li><li>Für jedes Top-Suchergebnis werden visuelle Merkmale extrahiert und mittels Bild-Embeddings geclustert.</li><li>Die Suchergebnisse werden neu geordnet, indem ein Element aus jedem Cluster ausgewählt und dem Benutzer eine diversifizierte Liste präsentiert wird.</li></ol><p>Nach dem Abrufen der Top-50-Ergebnisse wendeten wir ein leichtgewichtiges k-means Clustering (k=5) auf die Bild-Embeddings an und wählten dann Elemente aus jedem Cluster aus. Die Kategoriepräzision blieb konsistent mit der Query-to-Description-Performance, da wir die Query-to-Product-Kategorie als Messmetrik verwendeten. Die geordneten Ergebnisse begannen jedoch durch die bildbasierte Diversifizierung mehr verschiedene Aspekte (wie Stoff, Schnitt und Muster) abzudecken. Hier ist als Referenz das Beispiel des mehrfarbigen Henley T-Shirt-Kleids von vorher:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png\" class=\"kg-image\" alt=\"Collection of t-shirt dresses categorized into casual and day, short and long sleeves, displayed in two rows.\" loading=\"lazy\" width=\"2000\" height=\"1484\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Schauen wir uns nun an, wie sich die Diversifizierung auf die Suchergebnisse auswirkt, wenn wir die Text-Embedding-Suche kombiniert mit Bild-Embedding als Diversifizierungs-Reranker verwenden:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png\" class=\"kg-image\" alt=\"Five diverse dresses arranged in a row, categorized as various types including casual and day dresses, mini and short, and ma\" loading=\"lazy\" width=\"2000\" height=\"465\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Die geordneten Ergebnisse stammen aus der textbasierten Suche, beginnen aber innerhalb der Top 5 Beispiele mehr diverse \"Aspekte\" abzudecken. Dies erzielt einen ähnlichen Effekt wie das Mitteln von Embeddings, ohne sie tatsächlich zu mitteln.</p><p>Dies hat jedoch seinen Preis: Wir müssen nach dem Abrufen der Top-k-Ergebnisse einen zusätzlichen Clustering-Schritt durchführen, was je nach Größe des initialen Rankings einige zusätzliche Millisekunden benötigt. Außerdem erfordert die Bestimmung des k-Wertes für das k-means Clustering einiges an heuristischem Raten. Das ist der Preis, den wir für eine verbesserte Diversifizierung der Ergebnisse zahlen!</p><h2 id=\"conclusion\">Fazit</h2><p><code>jina-clip-v1</code> überbrückt effektiv die Lücke zwischen Text- und Bildsuche, indem es beide Modalitäten in einem einzigen, effizienten Modell vereint. Unsere Experimente haben gezeigt, dass seine Fähigkeit, längere und komplexere Texteingaben zusammen mit Bildern zu verarbeiten, im Vergleich zu traditionellen Modellen wie CLIP eine überlegene Suchleistung liefert.</p><p>Unsere Tests umfassten verschiedene Methoden, einschließlich des Abgleichs von Text mit Beschreibungen, Bildern und gemittelten Embeddings. Die Ergebnisse zeigten durchgängig, dass die Kombination von Text- und Bild-Embeddings die besten Resultate lieferte und sowohl die Genauigkeit als auch die Vielfalt der Suchergebnisse verbesserte. Wir entdeckten auch, dass die Verwendung von Bild-Embeddings als \"visueller Reranker\" die Ergebnisvielfalt erhöhte, während die Relevanz erhalten blieb.</p><p>Diese Fortschritte haben wichtige Implikationen für reale Anwendungen, bei denen Benutzer sowohl mit Textbeschreibungen als auch mit Bildern suchen. Durch das gleichzeitige Verständnis beider Datentypen optimiert <code>jina-clip-v1</code> den Suchprozess, liefert relevantere Ergebnisse und ermöglicht vielfältigere Produktempfehlungen. Diese einheitliche Suchfunktion geht über den E-Commerce hinaus und kommt auch der Verwaltung von Medienbeständen, digitalen Bibliotheken und der Kuratierung visueller Inhalte zugute, wodurch es einfacher wird, relevante Inhalte über verschiedene Formate hinweg zu entdecken.</p><p>Während <code>jina-clip-v1</code> derzeit nur Englisch unterstützt, arbeiten wir aktuell an <code>jina-clip-v2</code>. In der Nachfolge von <code>jina-embeddings-v3</code> und <code>jina-colbert-v2</code> wird diese neue Version ein state-of-the-art mehrsprachiger multimodaler Retriever sein, der 89 Sprachen unterstützt. Dieses Upgrade wird neue Möglichkeiten für Such- und Abrufaufgaben in verschiedenen Märkten und Branchen eröffnen und es zu einem leistungsfähigeren Embedding-Modell für globale Anwendungen im E-Commerce, in den Medien und darüber hinaus machen.</p>",
  "comment_id": "671b96784821eb000165d2de",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-25T15:00:40.000+02:00",
  "updated_at": "2024-10-30T19:14:11.000+01:00",
  "published_at": "2024-10-29T11:51:40.000+01:00",
  "custom_excerpt": "Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/beyond-clip-how-jina-clip-advances-multimodal-search/",
  "excerpt": "Erfahren Sie, wie Jina-CLIP die CLIP-Technologie von OpenAI durch einheitliche Text-Bild-Embeddings verbessert und dabei eine höhere Abrufgenauigkeit und vielfältigere Ergebnisse erzielt.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere.",
  "feature_image_caption": null
}
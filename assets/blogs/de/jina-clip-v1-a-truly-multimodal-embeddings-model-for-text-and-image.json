{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1: Ein wirklich multimodales Embedding-Modell f√ºr Text und Bild",
  "html": "<p>Jina CLIP v1 (<code>jina-clip-v1</code>) ist ein neues multimodales Embedding-Modell, das die F√§higkeiten von OpenAIs <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">originalem CLIP-Modell</a> erweitert. Mit diesem neuen Modell haben Nutzer ein einziges Embedding-Modell, das State-of-the-Art-Leistung sowohl bei reinen Text-Retrievals als auch bei Text-Bild-Cross-Modal-Retrievals bietet. Jina AI hat die Leistung von OpenAI CLIP um 165% beim reinen Text-Retrieval und um 12% beim Bild-zu-Bild-Retrieval verbessert, bei identischer oder leicht besserer Leistung bei Text-zu-Bild- und Bild-zu-Text-Aufgaben. Diese verbesserte Leistung macht Jina CLIP v1 unverzichtbar f√ºr die Arbeit mit multimodalen Eingaben.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> verbessert OpenAI CLIP in <a href=\"#compare_table\" rel=\"noreferrer\">jeder Retrieval-Kategorie</a>.</div></div><p>In diesem Artikel werden wir zun√§chst die Schw√§chen des urspr√ºnglichen CLIP-Modells und unseren Ansatz zu deren Behebung mittels einer einzigartigen Co-Training-Methode diskutieren. Anschlie√üend werden wir die Effektivit√§t unseres Modells anhand verschiedener Retrieval-Benchmarks demonstrieren. Abschlie√üend geben wir detaillierte Anleitungen, wie Nutzer mit Jina CLIP v1 √ºber unsere Embeddings API und Hugging Face beginnen k√∂nnen.</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">Die CLIP-Architektur f√ºr multimodale KI</h2><p>Im Januar 2021 ver√∂ffentlichte OpenAI das <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a> (Contrastive Language‚ÄìImage Pretraining) Modell. CLIP hat eine einfache, aber geniale Architektur: es kombiniert zwei Embedding-Modelle, eines f√ºr Texte und eines f√ºr Bilder, in einem einzigen Modell mit einem gemeinsamen Embedding-Ausgaberaum. Seine Text- und Bild-Embeddings sind direkt miteinander vergleichbar, wodurch der Abstand zwischen einem Text-Embedding und einem Bild-Embedding proportional dazu ist, wie gut dieser Text das Bild beschreibt und umgekehrt.</p><p>Dies hat sich als sehr n√ºtzlich f√ºr multimodales Information Retrieval und Zero-Shot-Bildklassifizierung erwiesen. Ohne spezielles zus√§tzliches Training war CLIP in der Lage, Bilder gut in Kategorien mit nat√ºrlichsprachlichen Labels einzuordnen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"Diagram illustrating image to text translation using an astronaut on Mars with a red moon as an example.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Das Text-Embedding-Modell im urspr√ºnglichen CLIP war ein spezielles neuronales Netzwerk mit nur 63 Millionen Parametern. Auf der Bildseite ver√∂ffentlichte OpenAI CLIP mit einer Auswahl von <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> und <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT-Modellen</a>. Jedes Modell wurde f√ºr seine individuelle Modalit√§t vortrainiert und dann mit beschrifteten Bildern trainiert, um √§hnliche Embeddings f√ºr vorbereitete Bild-Text-Paare zu erzeugen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"Flowchart with text &quot;Embedding Space&quot;, linked to &quot;Image Encoder&quot; and &quot;Text Encoder&quot;, with a &quot;Distracted boyfriend&quot; label.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dieser Ansatz lieferte beeindruckende Ergebnisse. Besonders bemerkenswert ist die Zero-Shot-Klassifizierungsleistung. Auch wenn die Trainingsdaten keine beschrifteten Bilder von <a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">Astronauten</a> enthielten, konnte CLIP Bilder von Astronauten basierend auf seinem Verst√§ndnis verwandter Konzepte in Texten und Bildern korrekt identifizieren.</p><p>Allerdings hat OpenAIs CLIP zwei wichtige Nachteile:</p><ul><li>Erstens ist die Texteingabekapazit√§t sehr begrenzt. Es kann maximal 77 Token als Eingabe verarbeiten, aber <a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">empirische Analysen zeigen</a>, dass es in der Praxis nicht mehr als 20 Token zur Erzeugung seiner Embeddings nutzt. Dies liegt daran, dass CLIP mit Bildern und Bildunterschriften trainiert wurde, und Bildunterschriften tendenziell sehr kurz sind. Dies steht im Gegensatz zu aktuellen Text-Embedding-Modellen, die mehrere tausend Token unterst√ºtzen.</li><li>Zweitens ist die Leistung seiner Text-Embeddings bei reinen Text-Retrieval-Szenarien sehr schlecht. Bildunterschriften sind eine sehr eingeschr√§nkte Art von Text und spiegeln nicht das breite Spektrum an Anwendungsf√§llen wider, die ein Text-Embedding-Modell unterst√ºtzen sollte.</li></ul><p>In den meisten realen Anwendungsf√§llen werden reines Text- und Bild-Text-Retrieval kombiniert oder zumindest beide f√ºr Aufgaben zur Verf√ºgung gestellt. Die Pflege eines zweiten Embeddings-Modells f√ºr reine Text-Aufgaben verdoppelt effektiv die Gr√∂√üe und Komplexit√§t Ihres KI-Frameworks.</p><p>Das neue Modell von Jina AI adressiert diese Probleme direkt, und <code>jina-clip-v1</code> nutzt die Fortschritte der letzten Jahre, um State-of-the-Art-Leistung f√ºr Aufgaben mit allen Kombinationen von Text- und Bildmodalit√§ten zu bieten.</p><h2 id=\"introducing-jina-clip-v1\">Einf√ºhrung in Jina CLIP v1</h2><p>Jina CLIP v1 beh√§lt das urspr√ºngliche CLIP-Schema von OpenAI bei: zwei Modelle, die co-trainiert werden, um Ausgaben im gleichen Embedding-Raum zu erzeugen.</p><p>F√ºr die Textkodierung haben wir die <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> Architektur adaptiert, die in den <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Modellen</a> verwendet wird. Diese Architektur unterst√ºtzt ein State-of-the-Art 8k Token Eingabefenster und gibt 768-dimensionale Vektoren aus, die genauere Embeddings aus l√§ngeren Texten erzeugen. Dies ist mehr als das 100-fache der 77 Token Eingabe, die im urspr√ºnglichen CLIP-Modell unterst√ºtzt wurden.</p><p>F√ºr Bild-Embeddings verwenden wir das neueste Modell der Beijing Academy for Artificial Intelligence: das <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code> Modell</a>. Wir haben empirisch verschiedene Bild-KI-Modelle verglichen, sie in cross-modalen Kontexten mit √§hnlichem Pre-Training getestet, und <code>EVA-02</code> √ºbertraf die anderen deutlich. Es ist auch in der Modellgr√∂√üe vergleichbar mit der Jina BERT Architektur, sodass die Rechenlasten f√ºr Bild- und Textverarbeitungsaufgaben ungef√§hr identisch sind.</p><p>Diese Entscheidungen bringen wichtige Vorteile f√ºr Benutzer:</p><ul><li>Bessere Leistung bei allen Benchmarks und allen modalen Kombinationen, und besonders gro√üe Verbesserungen bei der reinen Text-Embedding-Leistung.</li><li>Die empirisch √ºberlegene Leistung von <code>EVA-02</code> sowohl bei Bild-Text- als auch bei reinen Bildaufgaben, mit dem zus√§tzlichen Vorteil von Jina AIs zus√§tzlichem Training, das die reine Bildleistung verbessert.</li><li>Unterst√ºtzung f√ºr viel l√§ngere Texteingaben. Die <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">8k Token Eingabeunterst√ºtzung von Jina Embeddings</a> erm√∂glicht es, detaillierte Textinformationen zu verarbeiten und mit Bildern zu korrelieren.</li><li>Eine gro√üe Nettoeinsparung bei Speicherplatz, Rechenleistung, Code-Wartung und Komplexit√§t, da dieses multimodale Modell auch in nicht-multimodalen Szenarien sehr leistungsf√§hig ist.</li></ul><h3 id=\"training\">Training</h3><p>Ein Teil unseres Rezepts f√ºr hochleistungsf√§hige multimodale KI sind unsere Trainingsdaten und -prozedur. Wir stellen fest, dass die sehr kurze L√§nge der Texte in Bildunterschriften die Hauptursache f√ºr die schlechte reine Textleistung in CLIP-artigen Modellen ist, und unser Training ist explizit darauf ausgerichtet, dies zu beheben.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"Flowchart illustrating optimization of text and caption-image similarity in three tasks, using a model and encoders, ending i\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Das Training erfolgt in drei Schritten:</p><ol><li>Verwendung von Daten mit beschrifteten Bildern, um Bild- und Text-Embeddings anzugleichen, durchsetzt mit Textpaaren mit √§hnlichen Bedeutungen. Dieses Co-Training optimiert gemeinsam f√ºr die beiden Arten von Aufgaben. Die reine Textleistung des Modells nimmt w√§hrend dieser Phase ab, aber nicht so stark wie wenn wir nur mit Bild-Text-Paaren trainiert h√§tten.</li><li>Training mit synthetischen Daten, die Bilder mit l√§ngeren, von einem KI-Modell generierten Texten verbinden, die das Bild beschreiben. Gleichzeitiges Weitertraining mit reinen Textpaaren. W√§hrend dieser Phase lernt das Modell, l√§ngere Texte in Verbindung mit Bildern zu verarbeiten.</li><li>Verwendung von Text-Triplets mit <a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">schwierigen Negativbeispielen</a>, um die reine Textleistung durch das Erlernen feinerer semantischer Unterscheidungen weiter zu verbessern. Gleichzeitig wird das Training mit synthetischen Paaren von Bildern und langen Texten fortgesetzt. W√§hrend dieser Phase verbessert sich die reine Textleistung dramatisch, ohne dass das Modell seine Bild-Text-F√§higkeiten verliert.</li></ol><p>Weitere Informationen zu den Details des Trainings und der Modellarchitektur finden Sie in <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">unserem aktuellen Paper</a>:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) wird h√§ufig verwendet, um Modelle zu trainieren, die Bilder und Texte in einem gemeinsamen Einbettungsraum durch Abbildung auf Vektoren fester Gr√∂√üe ausrichten. Diese Modelle sind der Schl√ºssel f√ºr multimodales Information Retrieval und verwandte Aufgaben. Allerdings zeigen CLIP-Modelle bei reinen Textaufgaben im Vergleich zu spezialisierten Textmodellen eine schlechtere Leistung. Dies f√ºhrt zu Ineffizienzen bei Information-Retrieval-Systemen, die separate Einbettungen und Modelle f√ºr reine Text- und multimodale Aufgaben verwalten. Wir schlagen eine neuartige Multi-Task-Kontrastive-Trainingsmethode vor, um dieses Problem zu l√∂sen, mit der wir das jina-clip-v1-Modell trainieren, um State-of-the-Art-Leistung sowohl bei Text-Bild- als auch bei Text-Text-Retrieval-Aufgaben zu erreichen.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">Neuer State-of-the-Art bei multimodalen Einbettungen</h2><p>Wir haben die Leistung von Jina CLIP v1 bei reinen Text-, reinen Bild- und Cross-Modal-Aufgaben mit beiden Eingabemodalit√§ten evaluiert. Wir verwendeten den <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">MTEB Retrieval Benchmark</a> zur Evaluierung der reinen Textleistung. F√ºr reine Bildaufgaben verwendeten wir den <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a> Benchmark. F√ºr Cross-Model-Aufgaben evaluieren wir auf <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a> und <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a>, die im <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a> enthalten sind.</p><p>Die Ergebnisse sind in der folgenden Tabelle zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Wie Sie aus diesen Ergebnissen ersehen k√∂nnen, √ºbertrifft <code>jina-clip-v1</code> OpenAIs urspr√ºngliches CLIP in allen Kategorien und ist bei reinem Text- und Bild-Retrieval deutlich besser. √úber alle Kategorien gemittelt ist dies eine 46%ige Leistungsverbesserung.</p><p>Eine detailliertere Auswertung finden Sie in <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">unserem aktuellen Paper</a>.</p><h2 id=\"getting-started-with-embeddings-api\">Erste Schritte mit der Embeddings API</h2><p>Sie k√∂nnen Jina CLIP v1 einfach in Ihre Anwendungen integrieren, indem Sie die <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings API</a> verwenden.</p><p>Der folgende Code zeigt, wie Sie die API aufrufen k√∂nnen, um Einbettungen f√ºr Texte und Bilder zu erhalten, unter Verwendung des <code>requests</code> Pakets in Python. Er √ºbergibt einen Textstring und eine URL zu einem Bild an den Jina AI Server und gibt beide Kodierungen zur√ºck.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚òùÔ∏è</div><div class=\"kg-callout-text\">Denken Sie daran, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> durch einen aktivierten Jina API-Schl√ºssel zu ersetzen. Sie k√∂nnen einen Test-Schl√ºssel mit einer Million kostenlosen Tokens von der <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina Embeddings Webseite</a> erhalten.</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">Integration mit wichtigen LLM-Frameworks</h3><p>Jina CLIP v1 ist bereits verf√ºgbar f√ºr <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> und <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a>:</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>: Verwenden Sie <code>JinaEmbedding</code> mit der <code>MultimodalEmbedding</code> Basisklasse und rufen Sie <code>get_image_embeddings</code> oder <code>get_text_embeddings</code> auf.</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>: Verwenden Sie <code>JinaEmbeddings</code> und rufen Sie <code>embed_images</code> oder <code>embed_documents</code> auf.</li></ul><h3 id=\"pricing\">Preisgestaltung</h3><p>Sowohl Text- als auch Bildeingaben werden nach Token-Verbrauch berechnet.</p><p>F√ºr englische Texte haben wir <a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">empirisch berechnet</a>, dass im Durchschnitt 1,1 Token pro Wort ben√∂tigt werden.</p><p>Bei Bildern z√§hlen wir die Anzahl der 224x224 Pixel-Kacheln, die ben√∂tigt werden, um Ihr Bild abzudecken. Einige dieser Kacheln k√∂nnen teilweise leer sein, z√§hlen aber genauso. Jede Kachel kostet 1.000 Token zur Verarbeitung.</p><p><strong>Beispiel</strong></p><p>F√ºr ein Bild mit den Abmessungen 750x500 Pixel:</p><ol><li>Das Bild wird in 224x224 Pixel-Kacheln aufgeteilt.<ol><li>Um die Anzahl der Kacheln zu berechnen, nehmen Sie die Breite in Pixeln und teilen Sie durch 224, dann runden Sie auf die n√§chste ganze Zahl auf.<br>     750/224 ‚âà 3,35 ‚Üí 4</li><li>Wiederholen Sie dies f√ºr die H√∂he in Pixeln:<br>     500/224 ‚âà 2,23 ‚Üí 3</li></ol></li><li>Die Gesamtzahl der ben√∂tigten Kacheln in diesem Beispiel ist:<br>           4 (horizontal) x 3 (vertikal) = 12 Kacheln</li><li>Die Kosten betragen 12 x 1.000 = 12.000 Token</li></ol><h3 id=\"enterprise-support\">Enterprise-Support</h3><p>Wir f√ºhren einen neuen Vorteil f√ºr Benutzer ein, die den Production Deployment Plan mit <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">11 Milliarden Token</a> erwerben. Dies beinhaltet:</p><ul><li>Drei Stunden Beratung mit unseren Produkt- und Engineering-Teams, um Ihre spezifischen Anwendungsf√§lle und Anforderungen zu besprechen.</li><li>Ein angepasstes Python-Notebook f√ºr Ihren RAG (Retrieval-Augmented Generation) oder Vektorsuche-Anwendungsfall, das zeigt, wie Sie Jina AIs Modelle in Ihre Anwendung integrieren k√∂nnen.</li><li>Zuweisung eines Account Executive und priorisierter E-Mail-Support, um sicherzustellen, dass Ihre Bed√ºrfnisse zeitnah und effizient erf√ºllt werden.</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Open-Source Jina CLIP v1 auf Hugging Face</h2><p>Jina AI ist einer Open-Source-Suchgrundlage verpflichtet und stellt dieses Modell daher kostenlos unter einer <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">Apache 2.0 Lizenz</a> auf <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a> zur Verf√ºgung.</p><p>Beispielcode zum Herunterladen und Ausf√ºhren dieses Modells auf Ihrem eigenen System oder Ihrer Cloud-Installation finden Sie auf der Hugging Face Modellseite f√ºr <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">Wir sind auf einer Reise, k√ºnstliche Intelligenz durch Open Source und Open Science voranzutreiben und zu demokratisieren.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">Zusammenfassung</h2><p>Jina AIs neuestes Modell ‚Äî <code>jina-clip-v1</code> ‚Äî stellt einen bedeutenden Fortschritt bei multimodalen Einbettungsmodellen dar und bietet erhebliche Leistungsverbesserungen gegen√ºber OpenAIs CLIP. Mit bemerkenswerten Verbesserungen bei reinen Text- und Bild-Retrieval-Aufgaben sowie wettbewerbsf√§higer Leistung bei Text-zu-Bild- und Bild-zu-Text-Aufgaben ist es eine vielversprechende L√∂sung f√ºr komplexe Einbettungsanwendungsf√§lle.</p><p>Dieses Modell unterst√ºtzt aufgrund von Ressourcenbeschr√§nkungen derzeit nur englischsprachige Texte. Wir arbeiten daran, seine F√§higkeiten auf weitere Sprachen auszuweiten.</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzm√°n",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Das neue multimodale Embedding-Modell von Jina AI √ºbertrifft nicht nur OpenAI CLIP bei der Text-Bild-Suche, es ist gleichzeitig auch ein leistungsstarkes Image-Embedding-Modell und ein hochmodernes Text-Embedding-Modell. Sie ben√∂tigen keine verschiedenen Modelle mehr f√ºr unterschiedliche Modalit√§ten.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
{
  "slug": "bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning",
  "id": "67066bd652567c0001d0f2cd",
  "uuid": "1130051f-f343-4eb2-9956-9b574c212704",
  "title": "√úberbr√ºckung sprachlicher L√ºcken in mehrsprachigen Embeddings durch kontrastives Lernen",
  "html": "<p>Eine der zentralen Herausforderungen bei mehrsprachigen Modellen ist die \"<strong>Sprachl√ºcke</strong>\" ‚Äî ein Ph√§nomen, bei dem Phrasen mit der gleichen Bedeutung in verschiedenen Sprachen nicht so eng ausgerichtet oder gruppiert sind, wie sie es sein sollten. Idealerweise sollten ein Text in einer Sprache und sein √Ñquivalent in einer anderen √§hnliche Repr√§sentationen haben ‚Äî d.h. Embeddings, die sehr nahe beieinander liegen ‚Äî damit sprach√ºbergreifende Anwendungen bei Texten in verschiedenen Sprachen identisch funktionieren k√∂nnen. Allerdings repr√§sentieren Modelle oft subtil die Sprache eines Textes, wodurch eine \"Sprachl√ºcke\" entsteht, die zu suboptimaler sprach√ºbergreifender Leistung f√ºhrt.</p><p>In diesem Beitrag untersuchen wir diese Sprachl√ºcke und ihren Einfluss auf die Leistung von Text-Embedding-Modellen. Wir haben Experimente durchgef√ºhrt, um die semantische Ausrichtung von Paraphrasen in der gleichen Sprache und von √úbersetzungen zwischen verschiedenen Sprachpaaren zu bewerten, wobei wir unser <code>jina-xlm-roberta</code> Modell und das neueste <code>jina-embeddings-v3</code> verwendet haben. Diese Experimente zeigen, wie gut sich Phrasen mit √§hnlicher oder identischer Bedeutung unter verschiedenen Trainingsbedingungen gruppieren.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-6.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Wir haben auch Trainingstechniken erprobt, um die sprach√ºbergreifende semantische Ausrichtung zu verbessern, insbesondere die Einf√ºhrung von <strong>parallelen mehrsprachigen Daten</strong> w√§hrend des kontrastiven Lernens. In diesem Artikel teilen wir unsere Erkenntnisse und Ergebnisse.</p><h2 id=\"multilingual-model-training-creates-and-reduces-the-language-gap\"><strong>Mehrsprachiges Modelltraining erzeugt und reduziert die Sprachl√ºcke</strong></h2><p>Das Training von Text-Embedding-Modellen umfasst typischerweise einen mehrstufigen Prozess mit zwei Hauptteilen:</p><ol><li><a href=\"https://aclanthology.org/2023.acl-long.49/?ref=jina-ai-gmbh.ghost.io\"><strong>Masked Language Modeling</strong></a> (MLM): Das Vortraining beinhaltet typischerweise sehr gro√üe Textmengen, bei denen einige der Token zuf√§llig maskiert werden. Das Modell wird darauf trainiert, diese maskierten Token vorherzusagen. Diese Vorgehensweise lehrt dem Modell die Muster der Sprache oder Sprachen in den Trainingsdaten, einschlie√ülich Auswahlabh√§ngigkeiten zwischen Token, die sich aus Syntax, lexikalischer Semantik und pragmatischen Einschr√§nkungen der realen Welt ergeben k√∂nnen.</li><li><a href=\"https://paperswithcode.com/task/contrastive-learning?ref=jina-ai-gmbh.ghost.io\"><strong>Contrastive Learning</strong></a>: Nach dem Vortraining wird das Modell mit kuratierten oder semi-kuratierten Daten weiter trainiert, um die Embeddings semantisch √§hnlicher Texte n√§her zusammenzubringen und (optional) un√§hnliche weiter auseinander zu schieben. Dieses Training kann Paare, Tripel oder sogar Gruppen von Texten verwenden, deren semantische √Ñhnlichkeit bereits bekannt oder zumindest zuverl√§ssig gesch√§tzt ist. Es kann mehrere Unterstufen haben und es gibt verschiedene Trainingsstrategien f√ºr diesen Teil des Prozesses, wobei h√§ufig neue Forschungsergebnisse ver√∂ffentlicht werden und kein klarer Konsens √ºber den optimalen Ansatz besteht.</li></ol><p>Um zu verstehen, wie die Sprachl√ºcke entsteht und wie sie geschlossen werden kann, m√ºssen wir die Rolle beider Stufen betrachten.</p><h3 id=\"masked-language-pretraining\"><strong>Masked Language Pretraining</strong></h3><p>Ein Teil der sprach√ºbergreifenden F√§higkeit von Text-Embedding-Modellen wird w√§hrend des Vortrainings erworben.</p><p>Verwandte und entlehnte W√∂rter erm√∂glichen es dem Modell, aus gro√üen Textmengen eine gewisse sprach√ºbergreifende semantische Ausrichtung zu lernen. Zum Beispiel sind das englische Wort <em>banana</em> und das franz√∂sische Wort <em>banane</em> (und deutsche <em>Banane</em>) h√§ufig und in der Schreibweise √§hnlich genug, dass ein Embedding-Modell lernen kann, dass W√∂rter, die wie \"banan-\" aussehen, √ºber Sprachen hinweg √§hnliche Verteilungsmuster haben. Es kann diese Information nutzen, um bis zu einem gewissen Grad zu lernen, dass auch andere W√∂rter, die √ºber Sprachen hinweg nicht gleich aussehen, √§hnliche Bedeutungen haben, und sogar herausfinden, wie grammatikalische Strukturen √ºbersetzt werden.</p><p>Dies geschieht jedoch ohne explizites Training.</p><p>Wir haben das <code>jina-xlm-roberta</code> Modell, das vortrainierte R√ºckgrat von <code>jina-embeddings-v3</code>, getestet, um zu sehen, wie gut es sprach√ºbergreifende √Ñquivalenzen aus dem maskierten Sprach-Vortraining gelernt hat. Wir haben zweidimensionale <a href=\"https://pair-code.github.io/understanding-umap/?ref=jina-ai-gmbh.ghost.io\">UMAP Satzrepr√§sentationen</a> einer Reihe von englischen S√§tzen geplottet, die ins Deutsche, Niederl√§ndische, vereinfachte Chinesisch und Japanische √ºbersetzt wurden. Die Ergebnisse sind in der folgenden Abbildung dargestellt:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png\" class=\"kg-image\" alt=\"Multilingual scatterplot showing word embeddings' alignment across five languages on UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_roberta_mlm_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Zweidimensionale UMAP-Projektion einer Auswahl englischer S√§tze und ihrer √úbersetzungen ins Deutsche, Niederl√§ndische, Chinesische und Japanische. Die grauen Linien verbinden nicht-englische S√§tze mit den englischen S√§tzen, aus denen sie √ºbersetzt wurden.<br><br>Diese S√§tze neigen dazu, im <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-xlm-roberta</code> Embedding-Raum stark sprachspezifische Cluster zu bilden, obwohl Sie in dieser Projektion einige Ausrei√üer sehen k√∂nnen, die m√∂glicherweise ein Nebeneffekt der zweidimensionalen Projektion sind.</div></div><p>Man kann sehen, dass das Vortraining die Embeddings von S√§tzen in der gleichen Sprache sehr stark zusammengruppiert hat. Dies ist eine Projektion in zwei Dimensionen einer Verteilung in einem viel h√∂herdimensionalen Raum, sodass es immer noch m√∂glich ist, dass zum Beispiel ein deutscher Satz, der eine gute √úbersetzung eines englischen ist, trotzdem der deutsche Satz ist, dessen Embedding dem Embedding seiner englischen Quelle am n√§chsten kommt. Aber es zeigt, dass ein Embedding eines englischen Satzes wahrscheinlich n√§her an einem anderen englischen Satz liegt als an einem semantisch identischen oder fast identischen deutschen.</p><p>Beachten Sie auch, wie Deutsch und Niederl√§ndisch viel engere Cluster bilden als andere Sprachpaare. Dies ist nicht √ºberraschend f√ºr zwei relativ eng verwandte Sprachen. Deutsch und Niederl√§ndisch sind sich so √§hnlich, dass sie manchmal teilweise gegenseitig verst√§ndlich sind.</p><p>Japanisch und Chinesisch scheinen sich ebenfalls n√§her beieinander zu befinden als zu anderen Sprachen. Obwohl sie nicht auf die gleiche Weise verwandt sind, verwendet das geschriebene Japanisch typischerweise <em>kanji</em> (Êº¢Â≠ó), oder <em>h√†nz√¨</em> im Chinesischen<em>. </em>Japanisch teilt die meisten dieser geschriebenen Zeichen mit dem Chinesischen, und die beiden Sprachen teilen viele W√∂rter, die mit einem oder mehreren kanji/h√†nz√¨ zusammen geschrieben werden. Aus der Perspektive des MLM ist dies die gleiche Art von sichtbarer √Ñhnlichkeit wie zwischen Niederl√§ndisch und Deutsch.</p><p>Wir k√∂nnen diese \"Sprachl√ºcke\" auf einfachere Weise sehen, wenn wir nur zwei Sprachen mit jeweils zwei S√§tzen betrachten:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png\" class=\"kg-image\" alt=\"Graph illustrating linguistic relationships with color-coded lines, data points for English and German phrases, and an &quot;MLM P\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image--8-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image--8-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Da MLM Texte nat√ºrlicherweise nach Sprachen clustert, werden \"my dog is blue\" und \"my cat is red\" zusammen gruppiert, weit entfernt von ihren deutschen Entsprechungen. Anders als die \"<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/?ref=jina-ai-gmbh.ghost.io\">Modalit√§tsl√ºcke</a>\", die in einem fr√ºheren Blogbeitrag diskutiert wurde, glauben wir, dass dies aus oberfl√§chlichen √Ñhnlichkeiten und Un√§hnlichkeiten zwischen Sprachen entsteht: √§hnliche Schreibweisen, Verwendung der gleichen Zeichenfolgen im Druck und m√∂glicherweise √Ñhnlichkeiten in Morphologie und syntaktischer Struktur ‚Äî gemeinsame Wortstellungen und gemeinsame Arten der Wortbildung.</p><p>Kurz gesagt, in welchem Ma√üe ein Modell auch immer sprach√ºbergreifende √Ñquivalenzen im MLM-Vortraining lernt, es reicht nicht aus, um eine starke Tendenz zur Gruppierung von Texten nach Sprachen zu √ºberwinden. Es hinterl√§sst eine gro√üe Sprachl√ºcke.</p><h3 id=\"contrastive-learning\"><strong>Contrastive Learning</strong></h3><p>Idealerweise wollen wir ein Embedding-Modell, das gegen√ºber der Sprache indifferent ist und nur allgemeine Bedeutungen in seinen Embeddings kodiert. In einem solchen Modell w√ºrden wir keine Gruppierung nach Sprachen sehen und keine Sprachl√ºcke haben. S√§tze in einer Sprache sollten sehr nahe an guten √úbersetzungen und weit entfernt von anderen S√§tzen sein, die etwas anderes bedeuten, auch in der gleichen Sprache, wie in der folgenden Abbildung:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png\" class=\"kg-image\" alt=\"Graph displays &quot;Clustering by Meaning&quot; with multilingual labels, emphasizing abstract concepts on a dark backdrop.\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-10---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-10---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-10---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>MLM-Vortraining erreicht das nicht, daher verwenden wir zus√§tzliche <em>kontrastive Lerntechniken</em>, um die semantische Repr√§sentation von Texten in Embeddings zu verbessern.</p><p>Kontrastives Lernen beinhaltet die Verwendung von Textpaaren, von denen bekannt ist, dass sie in ihrer Bedeutung √§hnlich oder unterschiedlich sind, und Tripeln, bei denen bekannt ist, dass ein Paar √§hnlicher ist als das andere. Die Gewichte werden w√§hrend des Trainings angepasst, um diese bekannte Beziehung zwischen Textpaaren und Tripeln widerzuspiegeln.</p><p>In unserem kontrastiven Lerndatensatz sind 30 Sprachen vertreten, aber 97% der Paare und Tripel sind in nur einer Sprache, und nur 3% beinhalten sprach√ºbergreifende Paare oder Tripel. Aber diese 3% reichen aus, um ein dramatisches Ergebnis zu erzielen: Die Embeddings zeigen sehr wenig Sprachgruppierung, und semantisch √§hnliche Texte produzieren nahe beieinander liegende Embeddings, unabh√§ngig von ihrer Sprache, wie in der UMAP-Projektion von Embeddings aus <code>jina-embeddings-v3</code> gezeigt wird.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png\" class=\"kg-image\" alt=\"Streudiagramm auf schwarzem Hintergrund, das die Sprachverteilung nach dem kontrastiven Training mit UMAP-Dimensionen zeigt.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_contrastive_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Um dies zu best√§tigen, haben wir die Spearman-Korrelation der von <code>jina-xlm-roberta</code> und <code>jina-embeddings-v3</code> generierten Repr√§sentationen auf dem STS17-Datensatz gemessen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Spearman-Korrelation</strong></b></a> misst die Rangkorrelation, d. h. wie √§hnlich zwei geordnete Listen sind. Dies ist ein guter Mechanismus zum Vergleich von Embedding-Modellen untereinander und mit menschlichen Bewertungen, da die tats√§chliche Punktzahl viel weniger wichtig ist als die Reihenfolge der Elemente.</div></div><p>Die folgende Tabelle zeigt die Spearman-Korrelation zwischen semantischen √Ñhnlichkeitsrankings f√ºr √ºbersetzte Texte in verschiedenen Sprachen. Wir nehmen eine Reihe englischer S√§tze und messen dann die √Ñhnlichkeit ihrer Embeddings mit einem Embedding eines bestimmten Referenzsatzes und sortieren sie der Reihe nach von der √§hnlichsten zur am wenigsten √§hnlichen. Dann √ºbersetzen wir all diese S√§tze in eine andere Sprache und wiederholen den Ranking-Prozess. In einem idealen sprach√ºbergreifenden Embedding-Modell w√§ren die beiden geordneten Listen identisch und die Spearman-Korrelation w√§re 1,0.</p><p>Die untenstehende Grafik und Tabelle zeigen unsere Ergebnisse beim Vergleich von Englisch mit den sechs anderen Sprachen im STS17-Benchmark unter Verwendung von sowohl <code>jina-xlm-roberta</code> als auch <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png\" class=\"kg-image\" alt=\"Balkendiagramm, das die Spearman-Korrelation f√ºr Englisch gepaart mit AR, DE, ES, FR, IT, NL vergleicht, in rot und blau nach Alphabet eingef√§rbt\" loading=\"lazy\" width=\"2000\" height=\"1056\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-4---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-4---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-4---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png 2085w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong><code>jina-xlm-roberta</code></strong></th>\n<th><strong><code>jina-embeddings-v3</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English ‚Üî Arabic</td>\n<td>0.1581</td>\n<td><strong>0.7977</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî German</td>\n<td>0.2136</td>\n<td><strong>0.8366</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Spanish</td>\n<td>0.1049</td>\n<td><strong>0.8509</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî French</td>\n<td>0.1659</td>\n<td><strong>0.8378</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Italian</td>\n<td>0.2293</td>\n<td><strong>0.8674</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Dutch</td>\n<td>0.2387</td>\n<td><strong>0.8398</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Hier sehen Sie den enormen Unterschied, den das kontrastive Lernen im Vergleich zum urspr√ºnglichen Pre-training macht. Obwohl nur 3 % sprach√ºbergreifende Daten im Trainingsmix enthalten waren, hat das <code>jina-embeddings-v3</code> Modell genug sprach√ºbergreifende Semantik gelernt, um die Sprachl√ºcke, die es im Pre-training erworben hat, fast vollst√§ndig zu eliminieren.</p><h2 id=\"english-vs-the-world-can-other-languages-keep-up-in-alignment\">Englisch gegen den Rest der Welt: K√∂nnen andere Sprachen bei der Ausrichtung mithalten?</h2><p>Wir haben <code>jina-embeddings-v3</code> mit 89 Sprachen trainiert, mit besonderem Fokus auf 30 sehr weit verbreitete Schriftsprachen. Trotz unserer Bem√ºhungen, ein umfangreiches mehrsprachiges Trainingskorpus aufzubauen, macht Englisch immer noch fast die H√§lfte der Daten aus, die wir im kontrastiven Training verwendet haben. Andere Sprachen, einschlie√ülich weit verbreiteter globaler Sprachen, f√ºr die reichlich Textmaterial verf√ºgbar ist, sind im Vergleich zur enormen Menge englischer Daten im Trainingssatz immer noch relativ unterrepr√§sentiert.</p><p>Sind angesichts dieser Dominanz des Englischen die englischen Repr√§sentationen besser aufeinander abgestimmt als die anderer Sprachen? Um dies zu untersuchen, f√ºhrten wir ein Folgeexperiment durch.</p><p>Wir erstellten einen Datensatz, <a href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><code>parallel-sentences</code></a>, der aus 1.000 englischen Textpaaren besteht, einem \"Anker\" und einem \"Positiv\", wobei der positive Text logisch aus dem Ankertext folgt.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/parallel-sentences ¬∑ Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-3.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/parallel-sentences.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Zum Beispiel die erste Zeile der folgenden Tabelle. Diese S√§tze haben keine identische Bedeutung, aber sie haben kompatible Bedeutungen. Sie beschreiben informativ die gleiche Situation.</p><p>Wir √ºbersetzten diese Paare dann mit GPT-4 in f√ºnf Sprachen: Deutsch, Niederl√§ndisch, Chinesisch (Vereinfacht), Chinesisch (Traditionell) und Japanisch. Schlie√ülich √ºberpr√ºften wir sie manuell, um die Qualit√§t sicherzustellen.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Anchor</strong></th>\n<th><strong>Positive</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>Two young girls are playing outside in a non-urban environment.</td>\n<td>Two girls are playing outside.</td>\n</tr>\n<tr>\n<td>German</td>\n<td>Zwei junge M√§dchen spielen drau√üen in einer nicht urbanen Umgebung.</td>\n<td>Zwei M√§dchen spielen drau√üen.</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>Twee jonge meisjes spelen buiten in een niet-stedelijke omgeving.</td>\n<td>Twee meisjes spelen buiten.</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>‰∏§‰∏™Âπ¥ËΩªÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁéØÂ¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>‰∏§‰∏™Â•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>ÂÖ©ÂÄãÂπ¥ËºïÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁí∞Â¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>ÂÖ©ÂÄãÂ•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>2‰∫∫„ÅÆËã•„ÅÑÂ•≥„ÅÆÂ≠ê„ÅåÈÉΩÂ∏ÇÁí∞Â¢É„Åß„ÅØ„Å™„ÅÑÂ†¥ÊâÄ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n<td>‰∫å‰∫∫„ÅÆÂ∞ëÂ•≥„ÅåÂ§ñ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Anschlie√üend kodierten wir jedes Textpaar mit <code>jina-embeddings-v3</code> und berechneten die Kosinus-√Ñhnlichkeit zwischen ihnen. Die folgende Abbildung und Tabelle zeigen die Verteilung der Kosinus-√Ñhnlichkeitswerte f√ºr jede Sprache und die durchschnittliche √Ñhnlichkeit:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png\" class=\"kg-image\" alt=\"Grafik, die die Kosinus-√Ñhnlichkeitsverteilungen f√ºr Textpaare in Englisch, Deutsch, Niederl√§ndisch, Chinesisch und Japanisch im Vergleich zur Dichte zeigt\" loading=\"lazy\" width=\"1060\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/1_monolingual_distribution_triplets.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/1_monolingual_distribution_triplets.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png 1060w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>0.9078</td>\n</tr>\n<tr>\n<td>German</td>\n<td>0.8949</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>0.8844</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>0.8876</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>0.8933</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>0.8895</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Trotz der Dominanz des Englischen in den Trainingsdaten erkennt <code>jina-embeddings-v3</code> semantische √Ñhnlichkeiten in Deutsch, Niederl√§ndisch, Japanisch und beiden Formen des Chinesischen etwa genauso gut wie im Englischen.</p><h2 id=\"breaking-language-barriers-cross-lingual-alignment-beyond-english\"><strong>Sprachbarrieren √ºberwinden: Sprach√ºbergreifende Ausrichtung jenseits des Englischen</strong></h2><p>Untersuchungen zur Angleichung sprach√ºbergreifender Repr√§sentationen konzentrieren sich typischerweise auf Sprachpaare mit Englisch. Diese Fokussierung k√∂nnte theoretisch verschleiern, was tats√§chlich passiert. Ein Modell k√∂nnte einfach darauf optimiert sein, alles so nah wie m√∂glich an seinem englischen √Ñquivalent darzustellen, ohne zu pr√ºfen, ob andere Sprachpaare angemessen unterst√ºtzt werden.</p><p>Um dies zu untersuchen, f√ºhrten wir einige Experimente mit dem <code>parallel-sentences</code> Datensatz durch, wobei wir uns auf die sprach√ºbergreifende Angleichung √ºber englische Sprachpaare hinaus konzentrierten.</p><p>Die untenstehende Tabelle zeigt die Verteilung der Kosinus-√Ñhnlichkeiten zwischen √§quivalenten Texten in verschiedenen Sprachpaaren ‚Äî Texte, die √úbersetzungen einer gemeinsamen englischen Quelle sind. Idealerweise sollten alle Paare einen Kosinus von 1 haben ‚Äî d.h. identische semantische Einbettungen. In der Praxis k√∂nnte dies nie passieren, aber wir w√ºrden von einem guten Modell sehr hohe Kosinus-Werte f√ºr √úbersetzungspaare erwarten.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png\" class=\"kg-image\" alt=\"Density graph charting cross-lingual cosine similarities for language pairs using jina-embeddings-v3 model.\" loading=\"lazy\" width=\"978\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png 978w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language Pair</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>German ‚Üî Dutch</td>\n<td>0.8779</td>\n</tr>\n<tr>\n<td>German ‚Üî Japanese</td>\n<td>0.8664</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Japanese</td>\n<td>0.8534</td>\n</tr>\n<tr>\n<td>Dutch ‚Üî Chinese (Simplified)</td>\n<td>0.8479</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Chinese (Traditional)</td>\n<td>0.8758</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Obwohl die √Ñhnlichkeitswerte zwischen verschiedenen Sprachen etwas niedriger sind als bei kompatiblen Texten in derselben Sprache, sind sie immer noch sehr hoch. Die Kosinus-√Ñhnlichkeit von niederl√§ndisch/deutschen √úbersetzungen ist fast so hoch wie zwischen kompatiblen Texten auf Deutsch.</p><p>Das mag nicht √ºberraschend sein, da Deutsch und Niederl√§ndisch sehr √§hnliche Sprachen sind. √Ñhnlich verh√§lt es sich bei den beiden hier getesteten chinesischen Varianten, die nicht wirklich zwei verschiedene Sprachen sind, sondern nur stilistisch unterschiedliche Formen derselben Sprache. Aber man kann sehen, dass selbst sehr unterschiedliche Sprachpaare wie Niederl√§ndisch und Chinesisch oder Deutsch und Japanisch immer noch sehr starke √Ñhnlichkeiten zwischen semantisch √§quivalenten Texten aufweisen.</p><p>Wir zogen die M√∂glichkeit in Betracht, dass diese sehr hohen √Ñhnlichkeitswerte ein Nebeneffekt der Verwendung von ChatGPT als √úbersetzer sein k√∂nnten. Um dies zu testen, luden wir <a href=\"https://help.ted.com/hc/en-us/articles/360018572954-How-do-I-find-transcripts-for-TED-and-TEDx-talks?ref=jina-ai-gmbh.ghost.io\">von Menschen √ºbersetzte Transkripte von TED Talks</a> auf Englisch und Deutsch herunter und √ºberpr√ºften, ob die ausgerichteten √ºbersetzten S√§tze die gleiche hohe Korrelation aufweisen w√ºrden.</p><p>Das Ergebnis war sogar st√§rker als bei unseren maschinell √ºbersetzten Daten, wie man in der Abbildung unten sehen kann.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png\" class=\"kg-image\" alt=\"Graph of cross-lingual alignment density EN-DE with peak around cosine similarity 1.0, titled &quot;jina-embeddings-v3: Cross-ling\" loading=\"lazy\" width=\"988\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png 988w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-much-does-cross-language-data-contribute-to-cross-language-alignment\">Wie stark tragen sprach√ºbergreifende Daten zur sprach√ºbergreifenden Angleichung bei?</h2><p>Die verschwindende Sprachl√ºcke und das hohe Niveau der sprach√ºbergreifenden Leistung scheinen unverh√§ltnism√§√üig im Vergleich zu dem sehr kleinen Teil der Trainingsdaten, der explizit sprach√ºbergreifend war. Nur 3% der kontrastiven Trainingsdaten lehren das Modell explizit, wie Angleichungen zwischen Sprachen vorzunehmen sind.</p><p>Also f√ºhrten wir einen Test durch, um zu sehen, ob sprach√ºbergreifende Daten √ºberhaupt einen Beitrag leisten.</p><p>Ein vollst√§ndiges Neutraining von <code>jina-embeddings-v3</code> ohne sprach√ºbergreifende Daten w√§re f√ºr ein kleines Experiment unverh√§ltnism√§√üig teuer, also luden wir das <a href=\"https://huggingface.co/FacebookAI/xlm-roberta-base?ref=jina-ai-gmbh.ghost.io\"><code>xlm-roberta-base</code> Modell von Hugging Face</a> herunter und trainierten es weiter mit kontrastivem Lernen, wobei wir einen Teil der Daten verwendeten, die wir zum Training von <code>jina-embeddings-v3</code> verwendet hatten. Wir passten speziell die Menge der sprach√ºbergreifenden Daten an, um zwei F√§lle zu testen: Einen ohne sprach√ºbergreifende Daten und einen, bei dem 20% der Paare sprach√ºbergreifend waren. Die Training-Metaparameter k√∂nnen Sie in der untenstehenden Tabelle sehen:</p>\n<!--kg-card-begin: html-->\n<table id=\"e30425bb-015e-4956-8872-b1b64cdd7ad0\" class=\"simple-table\"><tbody><tr id=\"daa3cfcc-9012-411b-8da3-05c7c6f4b371\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><strong>Backbone</strong></td><td id=\"DU<d\" class=\"\"><strong>% Cross-Language</strong></td><td id=\"@Feo\" class=\"\"><strong>Learning Rate</strong></td><td id=\"fZNx\" class=\"\"><strong>Loss Function</strong></td><td id=\"Rv}\\\" class=\"\"><strong>Temperature</strong></td></tr><tr id=\"f3a2d068-d902-4fc1-8c89-269a5ebbb135\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base </code><strong>ohne</strong> X-language Daten</td><td id=\"DU<d\" class=\"\">0%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr><tr id=\"52887e22-326c-46cd-b79c-d6dcd110c1d2\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base</code><strong> mit </strong>X-language Daten</td><td id=\"DU<d\" class=\"\">20%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Anschlie√üend evaluierten wir die sprach√ºbergreifende Leistung beider Modelle mit den <a href=\"https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io\">STS17 und STS22 Benchmarks aus dem MTEB</a> und der Spearman-Korrelation. Die Ergebnisse pr√§sentieren wir unten:</p><h3 id=\"sts17\"><strong>STS17</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png\" class=\"kg-image\" alt=\"Bar graph showing Spearman correlation for language pairs on STS17 with and without parallel corpus.\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-6---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-6---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-6---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"de30bf7f-d1a9-43f4-8e5f-15bf65c59674\" class=\"simple-table\"><tbody><tr id=\"5c6440a6-eba9-404a-a5f1-88099bc6702d\"><td id=\"{N[x\" class=\"\"><strong>Language Pair</strong></td><td id=\"jGmJ\" class=\"\"><strong>Mit Parallelkorpora</strong></td><td id=\"p<ZH\" class=\"\"><strong>Ohne Parallelkorpora</strong></td></tr><tr id=\"33f08461-58aa-43f3-9ed1-577f8676e99d\"><td id=\"{N[x\" class=\"\">English ‚Üî Arabic</td><td id=\"jGmJ\" class=\"\"><strong>0.6418</strong></td><td id=\"p<ZH\" class=\"\">0.5875</td></tr><tr id=\"9875386d-2043-4d9d-8252-e53ec525ec29\"><td id=\"{N[x\" class=\"\">English ‚Üî German</td><td id=\"jGmJ\" class=\"\">0.7364</td><td id=\"p<ZH\" class=\"\"><strong>0.7390</strong></td></tr><tr id=\"15d28a12-3a80-4176-9984-69b5d8a7d8ff\"><td id=\"{N[x\" class=\"\">English ‚Üî Spanish</td><td id=\"jGmJ\" class=\"\"><strong>0.6968</strong></td><td id=\"p<ZH\" class=\"\">0.6799</td></tr><tr id=\"21821558-c8b9-4c34-8ec5-9db3ca7d9328\"><td id=\"{N[x\" class=\"\">English ‚Üî French</td><td id=\"jGmJ\" class=\"\"><strong>0.7066</strong></td><td id=\"p<ZH\" class=\"\">0.6944</td></tr><tr id=\"a2e3b5e5-8e4a-4270-abff-5d059ff6be72\"><td id=\"{N[x\" class=\"\">English ‚Üî Italian</td><td id=\"jGmJ\" class=\"\"><strong>0.7232</strong></td><td id=\"p<ZH\" class=\"\">0.7070</td></tr><tr id=\"95daf20f-2a82-4431-8581-a4ce24d81462\"><td id=\"{N[x\" class=\"\">English ‚Üî Dutch</td><td id=\"jGmJ\" class=\"\"><strong>0.7597</strong></td><td id=\"p<ZH\" class=\"\">0.7468</td></tr><tr id=\"4fd4c45a-a69e-4e33-b057-9c5f10b99fdb\"><td id=\"{N[x\" class=\"\">English ‚Üî Turkish</td><td id=\"jGmJ\" class=\"\"><strong>0.6933</strong></td><td id=\"p<ZH\" class=\"\">0.6050</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"sts22\"><strong>STS22</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png\" class=\"kg-image\" alt=\"Chart comparing models of language alignment, showing Spearman correlation scores for eight language pairs with and without p\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-7---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-7---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-7---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"e53757df-8a0e-42ec-ba05-715baa3c77cd\" class=\"simple-table\"><tbody><tr id=\"45c43b8c-b1b7-4ac0-91b0-1e07025f1b92\"><td id=\"OF=p\" class=\"\"><strong>Sprachpaar</strong></td><td id=\"P<\\i\" class=\"\"><strong>Mit parallelen Korpora</strong></td><td id=\"LAtp\" class=\"\"><strong>Ohne parallele Korpora</strong></td></tr><tr id=\"696ebe11-3eda-49ef-8dfe-608f9b71430b\"><td id=\"OF=p\" class=\"\">English ‚Üî Spanish</td><td id=\"P<\\i\" class=\"\"><strong>0.7710</strong></td><td id=\"LAtp\" class=\"\">0.7675</td></tr><tr id=\"eb4c1d81-7d98-453d-af3f-95b2adccfb55\"><td id=\"OF=p\" class=\"\">Simplified Chinese ‚Üî English</td><td id=\"P<\\i\" class=\"\"><strong>0.6885</strong></td><td id=\"LAtp\" class=\"\">0.6860</td></tr><tr id=\"533cefd3-b30e-4d6a-9350-8f5d28b17ba6\"><td id=\"OF=p\" class=\"\">Spanish ‚Üî Italian</td><td id=\"P<\\i\" class=\"\"><strong>0.6829</strong></td><td id=\"LAtp\" class=\"\">0.6814</td></tr><tr id=\"d3ecdd71-44bb-4a3b-9ac2-8cc90a785d5f\"><td id=\"OF=p\" class=\"\">German ‚Üî French</td><td id=\"P<\\i\" class=\"\"><strong>0.5763</strong></td><td id=\"LAtp\" class=\"\">0.5496</td></tr><tr id=\"c6242853-4da7-4369-b1f1-1a27262a487a\"><td id=\"OF=p\" class=\"\">German ‚Üî English</td><td id=\"P<\\i\" class=\"\">0.5439</td><td id=\"LAtp\" class=\"\"><strong>0.5566</strong></td></tr><tr id=\"31a4a5ba-199c-4904-b926-ff0561aac1b5\"><td id=\"OF=p\" class=\"\">Polish ‚Üî English</td><td id=\"P<\\i\" class=\"\">0.6966</td><td id=\"LAtp\" class=\"\"><strong>0.7156</strong></td></tr><tr id=\"4f529d81-e8c9-4e5d-a705-36e357abebc3\"><td id=\"OF=p\" class=\"\">German ‚Üî English</td><td id=\"P<\\i\" class=\"\"><strong>0.5832</strong></td><td id=\"LAtp\" class=\"\">0.5478</td></tr><tr id=\"cd1429f7-c810-4a0e-9dca-88e2c83157bc\"><td id=\"OF=p\" class=\"\">French ‚Üî Polish</td><td id=\"P<\\i\" class=\"\">0.8451</td><td id=\"LAtp\" class=\"\">0.8451</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Wir waren √ºberrascht zu sehen, dass bei den meisten der getesteten Sprachpaare sprach√ºbergreifende Trainingsdaten kaum oder gar keine Verbesserung brachten. Es ist schwer zu sagen, ob dies auch bei vollst√§ndig trainierten Modellen mit gr√∂√üeren Datens√§tzen der Fall w√§re, aber es deutet darauf hin, dass explizites sprach√ºbergreifendes Training nicht viel bringt.</p><p>Beachten Sie jedoch, dass STS17 auch Englisch/Arabisch- und Englisch/T√ºrkisch-Paare enth√§lt. Dies sind beides Sprachen, die in unseren Trainingsdaten deutlich unterrepr√§sentiert sind. Das verwendete XML-RoBERTa-Modell wurde mit Daten vortrainiert, die nur zu 2,25 % Arabisch und 2,32 % T√ºrkisch waren, deutlich weniger als bei den anderen getesteten Sprachen. Der kleine kontrastive Lerndatensatz, den wir in diesem Experiment verwendeten, enthielt nur 1,7 % Arabisch und 1,8 % T√ºrkisch.</p><p>Diese beiden Sprachpaare sind die einzigen getesteten Paare, bei denen das Training mit sprach√ºbergreifenden Daten einen deutlichen Unterschied machte. Wir denken, dass explizite sprach√ºbergreifende Daten bei Sprachen, die in den Trainingsdaten weniger gut repr√§sentiert sind, effektiver sind, m√ºssen diesen Bereich aber noch weiter erforschen, bevor wir eine endg√ºltige Schlussfolgerung ziehen k√∂nnen. Die Rolle und Wirksamkeit sprach√ºbergreifender Daten beim kontrastiven Training ist ein Bereich, in dem Jina AI aktiv forscht.</p><h2 id=\"conclusion\">Fazit</h2><p>Herk√∂mmliche Methoden des Sprach-Pretrainings, wie Masked Language Modeling, hinterlassen eine ‚ÄûSprachl√ºcke\", bei der semantisch √§hnliche Texte in verschiedenen Sprachen nicht so eng √ºbereinstimmen wie sie sollten. Wir haben gezeigt, dass das kontrastive Lernverfahren von Jina Embeddings sehr effektiv darin ist, diese L√ºcke zu reduzieren oder sogar zu eliminieren.</p><p>Die Gr√ºnde daf√ºr sind nicht vollst√§ndig klar. Wir verwenden beim kontrastiven Training explizit sprach√ºbergreifende Textpaare, aber nur in sehr kleinen Mengen, und es ist unklar, welche Rolle sie tats√§chlich bei der Sicherstellung hochwertiger sprach√ºbergreifender Ergebnisse spielen. Unsere Versuche, unter kontrollierten Bedingungen einen klaren Effekt nachzuweisen, brachten kein eindeutiges Ergebnis.</p><p>Allerdings ist <strong>klar, dass <code>jina-embeddings-v3</code> die Pretraining-Sprachl√ºcke √ºberwunden hat und damit ein leistungsf√§higes Werkzeug f√ºr mehrsprachige Anwendungen ist.</strong> Es ist einsatzbereit f√ºr jede Aufgabe, die eine starke, identische Leistung in mehreren Sprachen erfordert.</p><p>Sie k√∂nnen <code>jina-embeddings-v3</code> √ºber unsere <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Embeddings API</a> (mit einer Million kostenloser Tokens) oder √ºber AWS oder Azure nutzen. Wenn Sie es au√üerhalb dieser Plattformen oder vor Ort in Ihrem Unternehmen nutzen m√∂chten, beachten Sie bitte, dass es unter CC BY-NC 4.0 lizenziert ist. <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io\">Kontaktieren Sie uns</a>, wenn Sie an einer kommerziellen Nutzung interessiert sind.</p>",
  "comment_id": "67066bd652567c0001d0f2cd",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/gap-blog-1.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-09T13:41:10.000+02:00",
  "updated_at": "2024-10-10T20:15:26.000+02:00",
  "published_at": "2024-10-09T14:42:22.000+02:00",
  "custom_excerpt": "Multilingual models often face a \"language gap,\" where similar phrases in different languages don't align. We show how contrastive learning can bridge this gap, enhancing cross-language performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/",
  "excerpt": "Mehrsprachige Modelle stehen oft vor einer \"Sprachl√ºcke\", bei der sich √§hnliche Phrasen in verschiedenen Sprachen nicht aufeinander abstimmen. Wir zeigen, wie kontrastives Lernen diese L√ºcke √ºberbr√ºcken und die sprach√ºbergreifende Leistung verbessern kann.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon green squares form intricate patterns on a black digital background, creating a dynamic, abstract design.",
  "feature_image_caption": null
}
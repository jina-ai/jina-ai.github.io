{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "Submodulare Optimierung für diverse Abfragegenerierung in DeepResearch",
  "html": "<p>Bei der Implementierung von DeepResearch gibt es mindestens zwei Stellen, an denen Sie unterschiedliche Suchanfragen generieren müssen. Erstens müssen Sie <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">Web-Suchanfragen basierend auf der Benutzereingabe generieren</a> (die direkte Eingabe der Benutzereingabe in die Suchmaschine ist keine gute Idee). Zweitens enthalten viele DeepResearch-Systeme <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">einen \"Forschungsplaner\", der das ursprüngliche Problem in Teilprobleme zerlegt</a>, Agenten gleichzeitig aufruft, um diese unabhängig voneinander zu lösen, und dann deren Ergebnisse zusammenführt. Ob es sich um Suchanfragen oder Teilprobleme handelt, unsere Erwartungen bleiben gleich: Sie müssen für die ursprüngliche Eingabe relevant und vielfältig genug sein, um einzigartige Perspektiven darauf zu bieten. Oft müssen wir die Anzahl der Suchanfragen begrenzen, um keine unnötigen Kosten für Suchmaschinenanfragen oder die Verwendung von Agenten-Tokens zu verursachen.</p><p>Obwohl das Verständnis der Bedeutung der Suchanfragenerstellung gegeben ist, nehmen die meisten Open-Source-DeepResearch-Implementierungen diese Optimierung nicht ernst. Sie geben diese Einschränkungen einfach direkt per Prompt vor. Einige fordern das LLM möglicherweise zu einer zusätzlichen Runde auf, um die Suchanfragen zu bewerten und zu diversifizieren. Hier ist ein Beispiel dafür, wie die meisten Implementierungen im Grunde vorgehen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Zwei verschiedene Prompts zur Generierung verschiedener Suchanfragen mit LLMs. Der obere Prompt verwendet einfache Anweisungen. Der untere ist ausgefeilter und strukturierter. Angesichts der ursprünglichen Suchanfrage und der Anzahl der zu generierenden Suchanfragen erwarten wir, dass die generierten Suchanfragen ausreichend vielfältig sind. In diesem Beispiel verwenden wir </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> als LLM und die ursprüngliche Suchanfrage ist </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>In diesem Artikel möchte ich einen rigoroseren Ansatz zur Lösung der optimalen Suchanfragenerstellung mithilfe von Satz-Vektor Modellen und <strong>submodularer Optimierung</strong> demonstrieren. Zu meiner Promotionszeit war die submodulare Optimierung neben L-BFGS eine meiner Lieblingstechniken. Ich werde zeigen, wie man sie zur Generierung einer Reihe von vielfältigen Suchanfragen unter einer Kardinalitätsbeschränkung anwendet, was die Gesamtqualität von DeepResearch-Systemen erheblich verbessern kann.</p><h2 id=\"query-generation-via-prompting\">Suchanfragenerstellung per Prompt</h2><p>Zuerst möchten wir prüfen, ob die Eingabe per Prompt ein effektiver Ansatz zur Generierung vielfältiger Suchanfragen ist. Wir wollen auch verstehen, ob ein ausgefeilter Prompt effektiver ist als ein einfacher Prompt. Führen wir ein Experiment durch, in dem wir die beiden folgenden Prompts vergleichen, um dies herauszufinden:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Einfacher Prompt</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Strukturierter Prompt</span></p></figcaption></figure><p>Wir verwenden <code>gemini-2.5-flash</code> als LLM mit der ursprünglichen Suchanfrage <code>\"embeddings and rerankers\"</code> und testen sowohl den einfachen als auch den strukturierten Prompt, um iterativ von einer bis 20 Suchanfragen zu generieren. Anschließend verwenden wir <code>jina-embeddings-v3</code> mit der Aufgabe <code>text-matching</code>, um die Satzähnlichkeit zwischen der ursprünglichen Suchanfrage und den generierten Suchanfragen sowie die Ähnlichkeit innerhalb der generierten Suchanfragen selbst zu messen. Hier sind die Visualisierungen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Beide Prompts zeigen ähnliche Muster in der Analyse \"Innerhalb generierter Suchanfragen\" (rechte zwei Diagramme), wobei die medianen Kosinusähnlichkeiten über verschiedene Suchanfragen hinweg hoch bleiben (Bereich 0,4-0,6). Der einfache Prompt scheint sogar besser darin zu sein, Suchanfragen zu diversifizieren, wenn die Anzahl der Suchanfragen groß ist, während der strukturierte Prompt eine etwas bessere Relevanz für die ursprüngliche Suchanfrage beibehält und die Relevanz bei etwa 0,6 hält.</span></figcaption></figure><p>Betrachtet man die beiden Diagramme auf der rechten Seite, so sieht man, dass sowohl der einfache als auch der strukturierte Prompt eine große Varianz in den Kosinusähnlichkeitswerten aufweisen, wobei viele eine Ähnlichkeit von 0,7-0,8 erreichen, was darauf hindeutet, dass einige generierte Suchanfragen nahezu identisch sind. Außerdem haben beide Methoden Schwierigkeiten, die Vielfalt aufrechtzuerhalten, wenn mehr Suchanfragen generiert werden. Anstatt einen deutlichen Abwärtstrend der Ähnlichkeit mit zunehmender Anzahl von Suchanfragen zu beobachten, beobachten wir relativ stabile (und hohe) Ähnlichkeitswerte, was darauf hindeutet, dass zusätzliche Suchanfragen oft vorhandene Perspektiven duplizieren.</p><p>Eine Erklärung ist, dass Wang et al. (2025) festgestellt haben, dass LLMs oft Meinungen dominanter Gruppen unverhältnismäßig widerspiegeln, selbst bei Prompt-Steuerung, was auf eine Tendenz zu gängigen Perspektiven hindeutet. Dies liegt daran, dass die Trainingsdaten des LLM bestimmte Standpunkte überrepräsentieren können, wodurch das Modell Variationen erzeugt, die mit diesen vorherrschenden Perspektiven übereinstimmen. Abe et al. (2025) fanden auch heraus, dass die LLM-basierte Suchanfragenerweiterung populäre Interpretationen bevorzugt, während andere übersehen werden. Zum Beispiel könnte \"Was sind die Vorteile von KI?\" gängige Vorteile wie Automatisierung, Effizienz und Ethischkeit liefern, aber weniger offensichtliche wie die Entdeckung von Medikamenten verfehlen.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">Problemformulierung</h2><p>Man könnte meinen, unser vorheriges Experiment sei nicht schlüssig und wir sollten den Prompt verbessern und es erneut versuchen. Während das Prompting die Ergebnisse sicherlich bis zu einem gewissen Grad verändern kann, ist es wichtiger, dass wir etwas gelernt haben: Allein die Erhöhung der Anzahl der generierten Suchanfragen macht es wahrscheinlicher, dass wir vielfältige Suchanfragen erhalten. Die schlechte Nachricht ist, dass wir als Nebenprodukt auch eine Reihe von doppelten Suchanfragen erhalten.</p><p>Da es jedoch kostengünstig ist, eine große Anzahl von Suchanfragen zu generieren, was schließlich <em>einige</em> gute Suchanfragen ergibt, warum behandeln wir dies nicht als ein Subset-Auswahlproblem?</p><p>In der Mathematik können wir dieses Problem wie folgt formulieren: Gegeben sei eine ursprüngliche Eingabe $q_0$, eine Menge von Kandidatenabfragen $V=\\{q_1, q_2, \\cdots, q_n\\}$, die von einem LLM mithilfe von Prompt-Engineering generiert wurden. Wähle eine Teilmenge $X\\subseteq V$ von $k$ Abfragen aus, die die Abdeckung maximiert und gleichzeitig die Redundanz minimiert.</p><p>Leider erfordert das Finden der optimalen Teilmenge von $k$ Abfragen aus $n$ Kandidaten die Überprüfung von $\\binom{n}{k}$ Kombinationen - exponentielle Komplexität. Allein für 20 Kandidaten und $k=5$ sind das 15.504 Kombinationen.</p><h3 id=\"submodular-function\">Submodulare Funktion</h3><p>Bevor wir versuchen, das Problem der Teilmengenauswahl brutal zu lösen, möchte ich den Lesern den Begriff <strong>Submodularität</strong> und <strong>submodulare Funktion</strong> vorstellen. Sie mögen vielen unbekannt vorkommen, aber Sie haben vielleicht schon von der Idee des \"abnehmenden Grenzertrags\" gehört - nun, Submodularität ist die mathematische Darstellung davon.</p><p>Stellen Sie sich vor, Sie platzieren Wi-Fi-Router, um die Internetabdeckung in einem großen Gebäude zu gewährleisten. Der erste Router, den Sie installieren, bietet einen enormen Wert - er deckt einen bedeutenden Bereich ab, der zuvor keine Abdeckung hatte. Der zweite Router bietet ebenfalls einen erheblichen Mehrwert, aber ein Teil seines Abdeckungsbereichs überschneidet sich mit dem ersten Router, sodass der Grenznutzen geringer ist als beim ersten. Wenn Sie weiterhin Router hinzufügen, deckt jeder zusätzliche Router immer weniger neue Bereiche ab, da die meisten Bereiche bereits von bestehenden Routern abgedeckt sind. Schließlich bietet der 10. Router möglicherweise nur noch sehr wenig zusätzliche Abdeckung, da das Gebäude bereits gut abgedeckt ist.</p><p>Diese Intuition erfasst das Wesen der Submodularität. Mathematisch ist eine Mengenfunktion $f: 2^V \\rightarrow \\mathbb{R}$ <strong>submodular</strong>, wenn für alle $A \\subseteq B \\subseteq V$ und jedes Element $v \\notin B$ gilt:</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>Im Klartext: Das Hinzufügen eines Elements zu einer kleineren Menge bringt mindestens so viel Nutzen wie das Hinzufügen desselben Elements zu einer größeren Menge, die die kleinere Menge enthält.</p><p>Wenden wir dieses Konzept nun auf unser Problem der Abfragegenerierung an. Man erkennt sofort, dass die Abfrageauswahl einen natürlichen <strong>abnehmenden Grenzertrag</strong> aufweist:</p><ul><li>Die erste Abfrage, die wir auswählen, deckt einen völlig neuen semantischen Raum ab.</li><li>Die zweite Abfrage sollte andere Aspekte abdecken, aber eine gewisse Überschneidung ist unvermeidlich.</li><li>Wenn wir weitere Abfragen hinzufügen, deckt jede zusätzliche Abfrage immer weniger neues Terrain ab.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Aus </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">einer meiner alten Folien aus AAAI 2013</span></a><span style=\"white-space: pre-wrap;\">, wo ich Submodularität anhand eines Beutels mit Kugeln erklärte. Das Hinzufügen weiterer Kugeln zum Beutel verbessert die \"Einrichtung\", aber die relative Verbesserung wird immer geringer, wie in den abnehmenden Delta-Werten auf der rechten y-Achse zu sehen ist.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">Embedding-basierter Submodular-Funktionsentwurf</h2><p>Sei $\\mathbf{e}_i \\in \\mathbb{R}^d$ der Vektor für die Vektormodellierung der Abfrage $q_i$, der mit einem Satz-Vektormodellierungsmodell (z. B. <code>jina-embeddings-v3</code>) ermittelt wurde. Es gibt zwei Hauptansätze für den Entwurf unserer Zielfunktion:</p><h3 id=\"approach-1-facility-location-coverage-based\">Ansatz 1: Facility Location (Abdeckungsbasiert)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>Diese Funktion misst, wie gut die ausgewählte Menge $X$ alle Kandidatenabfragen \"abdeckt\", wobei:</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ die Kosinusähnlichkeit ist</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ die Relevanz für die ursprüngliche Abfrage sicherstellt</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ die Abdeckung des Kandidaten $j$ durch die ausgewählte Menge $X$ misst</li></ul><p>Ein Vorbehalt ist, dass diese Funktion die Diversität nur <em>implizit</em> fördert. Sie bestraft die Ähnlichkeit innerhalb der ausgewählten Menge $X$ nicht explizit. Diversität entsteht, weil die Auswahl ähnlicher Abfragen zu abnehmenden Abdeckungsergebnissen führt.</p><h3 id=\"approach-2-explicit-coverage-diversity\">Ansatz 2: Explizite Abdeckung + Diversität</h3><p>Für eine direktere Kontrolle über die Diversität können wir die Abdeckung mit einem expliziten Diversitätsbegriff kombinieren:</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>wobei die Diversitätskomponente wie folgt formuliert werden kann:</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>Dieser Diversitätsbegriff misst die gesamte Ähnlichkeit zwischen ausgewählten und nicht ausgewählten Abfragen - er wird maximiert, wenn wir Abfragen auswählen, die sich von den verbleibenden Kandidaten unterscheiden (eine Form der Graph Cut-Funktion).</p><h3 id=\"difference-between-two-approaches\">Unterschied zwischen den beiden Ansätzen</h3><p>Beide Formulierungen behalten die Submodularität bei.</p><p>Die Facility-Location-Funktion ist eine bekannte submodulare Funktion. Sie weist Submodularität aufgrund der Max-Operation auf: Wenn wir unserer ausgewählten Menge eine neue Abfrage $q$ hinzufügen, wird jede Kandidatenabfrage $j$ von der \"besten\" Abfrage in unserer Menge abgedeckt (derjenigen mit der höchsten Ähnlichkeit). Das Hinzufügen von $q$ zu einer kleineren Menge $A$ verbessert eher die Abdeckung verschiedener Kandidaten als das Hinzufügen zu einer größeren Menge $B \\supseteq A$, in der viele Kandidaten bereits gut abgedeckt sind.</p><p>In der Graph Cut-Diversitätsfunktion ist der Diversitätsbegriff $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ submodular, da er den \"Schnitt\" zwischen ausgewählten und nicht ausgewählten Mengen misst. Das Hinzufügen einer neuen Abfrage zu einer kleineren ausgewählten Menge erzeugt mehr neue Verbindungen zu nicht ausgewählten Abfragen als das Hinzufügen zu einer größeren ausgewählten Menge.</p><p>Der Facility-Location-Ansatz beruht auf <em>impliziter</em> Diversität durch Abdeckungswettbewerb, während der explizite Ansatz die Diversität direkt misst und optimiert. Beide sind also gültig, aber der explizite Ansatz gibt Ihnen eine direktere Kontrolle über den Kompromiss zwischen Relevanz und Diversität.</p><h2 id=\"implementations\">Implementierungen</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Die vollständige Implementierung finden Sie hier auf Github.</span></p></figcaption></figure><p>Da unsere Funktion submodular ist, können wir <strong>den Greedy-Algorithmus</strong> verwenden, der eine $(1-1/e) \\approx 0.63$ Approximationsgarantie bietet:</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>Hier ist der Code zur Optimierung von Facility Location (abdeckungsbasiert) - derjenige mit impliziter Diversität.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>Der Balanceparameter $\\alpha$ steuert den Kompromiss zwischen Relevanz und Diversität:</p><ul><li><strong>Hohes $\\alpha$ (z. B. 0,8)</strong>: Priorisiert die Relevanz für die ursprüngliche Abfrage, kann die Diversität beeinträchtigen</li><li><strong>Niedriges $\\alpha$ (z. B. 0,2)</strong>: Priorisiert die Diversität zwischen ausgewählten Abfragen, kann vom ursprünglichen Zweck abweichen</li><li><strong>Moderates $\\alpha$ (z. B. 0,4-0,6)</strong>: Ausgewogener Ansatz, funktioniert in der Praxis oft gut</li></ul><h3 id=\"lazy-greedy-algorithm\">Lazy Greedy Algorithmus</h3><p>Man kann im obigen Code feststellen:</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>Wir berechnen den Grenzertrag für <strong>alle</strong> verbleibenden Kandidaten in jeder Iteration. Das können wir besser machen.</p><p>Der <strong>Lazy Greedy Algorithmus</strong> ist eine clevere Optimierung, die die Submodularität ausnutzt, um unnötige Berechnungen zu vermeiden. Die wichtigste Erkenntnis ist: Wenn Element A in Iteration $t$ einen höheren Grenzertrag hatte als Element B, dann wird A auch in Iteration $t+1$ einen höheren Grenzertrag haben als B (aufgrund der Submodularitätseigenschaft).</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Lazy Greedy funktioniert wie folgt:</p><ol><li>Führen Sie eine Priority Queue von Elementen, sortiert nach ihren Grenzerträgen.</li><li>Berechnen Sie nur den Grenzertrag des obersten Elements neu.</li><li>Wenn es nach der Neuberechnung immer noch das höchste ist, wählen Sie es aus.</li><li>Andernfalls fügen Sie es an der richtigen Position wieder ein und überprüfen Sie das nächste oberste Element.</li></ol><p>Dies kann zu erheblichen Geschwindigkeitssteigerungen führen, da wir die Neuberechnung von Grenzerträgen für Elemente vermeiden, die eindeutig nicht ausgewählt werden.</p><h3 id=\"results\">Ergebnisse</h3><p>Führen wir das Experiment noch einmal durch. Wir verwenden denselben einfachen Prompt, um ein bis 20 verschiedene Abfragen zu generieren, und führen die gleichen Kosinusähnlichkeitsmessungen wie zuvor durch. Für die submodulare Optimierung wählen wir Abfragen aus den 20 generierten Kandidaten mit unterschiedlichen Werten von k aus und messen die Ähnlichkeit wie zuvor. Die Ergebnisse zeigen, dass die durch submodulare Optimierung ausgewählten Abfragen vielfältiger sind und eine geringere In-Set-Ähnlichkeit aufweisen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Originalabfrage = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Originalabfrage = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Originalabfrage = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Originalabfrage = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">Abschließende Frage: Warum die submodulare Formulierung wichtig ist</h2><p>Sie fragen sich vielleicht: Warum die Mühe machen, dies als submodulares Optimierungsproblem zu formulieren? Warum nicht einfach Heuristiken oder andere Optimierungsansätze verwenden?</p><p>Kurz gesagt, die submodulare Formulierung verwandelt eine Ad-hoc-Heuristik \"diverse Abfragen auswählen\" in ein rigoroses Optimierungsproblem mit <strong>nachweisbaren Garantien</strong>, <strong>effizienten Algorithmen</strong> und messbaren Zielen.</p><h3 id=\"guaranteed-efficiency\">Garantierte Effizienz</h3><p>Sobald wir bewiesen haben, dass unsere Zielfunktion submodular ist, erhalten wir leistungsstarke theoretische Garantien und einen effizienten Algorithmus. Der Greedy-Algorithmus, der in $O(nk)$ Zeit im Vergleich zur Überprüfung von $\\binom{n}{k}$ Kombinationen läuft, erreicht eine $(1-1/e) \\approx 0.63$ Approximation der optimalen Lösung. Dies bedeutet, dass unsere Greedy-Lösung immer mindestens 63 % so gut ist wie die bestmögliche Lösung. <strong>Keine Heuristik kann dies versprechen.</strong></p><p>Darüber hinaus ist der Lazy-Greedy-Algorithmus in der Praxis aufgrund der mathematischen Struktur submodularer Funktionen dramatisch schneller. Die Beschleunigung ergibt sich aus dem <strong>abnehmenden Ertrag</strong>: Elemente, die in früheren Iterationen eine schlechte Wahl waren, werden später wahrscheinlich keine gute Wahl mehr. Anstatt also alle $n$ Kandidaten zu überprüfen, muss Lazy Greedy typischerweise nur die Gewinne für die obersten Kandidaten neu berechnen.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">Keine Notwendigkeit für handgefertigte Heuristiken</h3><p>Ohne einen prinzipiellen Rahmen könnten Sie auf Ad-hoc-Regeln wie \"sicherstellen, dass Abfragen eine Kosinusähnlichkeit &lt; 0,7 haben\" oder \"verschiedene Schlüsselwortkategorien ausgleichen\" zurückgreifen. Diese Regeln sind schwer abzustimmen und nicht verallgemeinerbar. Die submodulare Optimierung bietet Ihnen einen prinzipiellen, mathematisch fundierten Ansatz. Sie können Hyperparameter systematisch mithilfe von Validierungssätzen abstimmen und die Lösungsqualität in Produktionssystemen überwachen. Wenn das System schlechte Ergebnisse liefert, haben Sie klare Metriken, um zu debuggen, was schiefgelaufen ist.</p><p>Schließlich ist die submodulare Optimierung ein gut untersuchtes Feld mit jahrzehntelanger Forschung, das es Ihnen ermöglicht, fortschrittliche Algorithmen über Greedy hinaus zu nutzen (wie beschleunigtes Greedy oder lokale Suche), theoretische Erkenntnisse darüber, wann bestimmte Formulierungen am besten funktionieren, und Erweiterungen zur Behandlung zusätzlicher Einschränkungen wie Budgetbeschränkungen oder Fairnessanforderungen.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Für diejenigen, die sich für submodulare Optimierung interessieren, empfehle ich diese Seite, um mehr zu erfahren.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "Viele kennen die Bedeutung der Abfragevielfalt in DeepResearch, aber nur wenige wissen, wie man sie rigoros durch submodulare Optimierung löst.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
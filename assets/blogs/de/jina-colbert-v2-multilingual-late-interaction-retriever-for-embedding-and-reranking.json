{
  "slug": "jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking",
  "id": "66cd8fc6e84873000133d63d",
  "uuid": "e995c4d9-1832-4e2a-8108-e8453f5c82c5",
  "title": "Jina ColBERT v2: Mehrsprachiger Late-Interaction-Retriever für Embedding und Reranking",
  "html": "<p>Heute freuen wir uns, Jina ColBERT v2 (<code>jina-colbert-v2</code>) vorzustellen, ein fortschrittliches Late-Interaction-Retrieval-Modell, das auf der ColBERT-Architektur basiert. Dieses neue Sprachmodell verbessert die Leistung von <code>jina-colbert-v1-en</code> und fügt mehrsprachige Unterstützung sowie dynamische Ausgabedimensionen hinzu.</p><p>Diese neue Version zeichnet sich durch folgende Funktionen aus:</p><ul><li><strong>Überlegene Retrieval-Leistung</strong> im Vergleich zum ursprünglichen ColBERT-v2 (+6,5%) und unserer vorherigen Version <code>jina-colbert-v1-en</code> (+5,4%).</li><li><strong>Mehrsprachige Unterstützung</strong> für 89 Sprachen mit starker Leistung in allen wichtigen Weltsprachen.</li><li><strong>Benutzergesteuerte Ausgabe-Embedding-Größen</strong> durch Matryoshka Representation Learning, wodurch Benutzer flexibel zwischen Effizienz und Präzision abwägen können.</li></ul><h2 id=\"technical-summary-of-jina-colbert-v2\">Technische Zusammenfassung von <code>jina-colbert-v2</code></h2><p>Der vollständige technische Bericht ist auf arXiv verfügbar:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2408.16672?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever</div><div class=\"kg-bookmark-description\">Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Rohan Jha</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>jina-colbert-v2</code></th>\n<th><code>jina-colbert-v1-en</code></th>\n<th>Original ColBERTv2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Durchschnitt von 14 englischen<br/>BEIR-Aufgaben</td>\n<td><b>0.521</b></td>\n<td>0.494</td>\n<td>0.489</td>\n</tr>\n<tr>\n<td>Mehrsprachig</td>\n<td><b>89 Sprachen</b></td>\n<td>Nur Englisch</td>\n<td>Nur Englisch</td>\n</tr>\n<tr>\n<td>Ausgabedimensionen</td>\n<td><b>128, 96, oder 64</b></td>\n<td>Fix 128</td>\n<td>Fix 128</td>\n</tr>\n<tr>\n<td>Max. Querylänge</td>\n<td>32 Tokens</td>\n<td>32 Tokens</td>\n<td>32 Tokens</td>\n</tr>\n<tr>\n<td>Max. Dokumentenlänge</td>\n<td>8192 Tokens</td>\n<td>8192 Tokens</td>\n<td>512 Tokens</td>\n</tr>  \n\n<tr>\n<td>Parameter</td>\n<td>560M</td>\n<td>137M</td>\n<td>110M</td>\n</tr>\n<tr>\n<td>Modellgröße</td>\n<td>1,1GB</td>\n<td>550MB</td>\n<td>438MB</td>\n</tr>\n\n\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"asymmetric-embedding-in-colbert\">Asymmetrisches Embedding in ColBERT</h2><p>ColBERT baut auf der BERT-Architektur auf und fügt <strong>Late Interaction</strong> und <strong>asymmetrische</strong> Query-Dokument-Kodierung hinzu.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Die asymmetrische Natur von ColBERT bedeutet, dass bei der Verwendung von Modellen wie <code>jina-colbert-v2</code> oder <code>jina-colbert-v1-en</code> angegeben werden muss, ob eine Query, ein Dokument oder beides (für Reranking-Zwecke) eingebettet werden soll. Diese zusätzliche Flexibilität verbessert die Leistung gegenüber homogenen Embedding-Modellen bei Retrieval-Aufgaben.</p><h2 id=\"multilingual-support-for-over-89-languages\">Mehrsprachige Unterstützung für über 89 Sprachen</h2><p>Jina ColBERT v2 verfügt über umfangreiche mehrsprachige Fähigkeiten, die für die Anforderungen moderner, globalisierter Informationssuche und KI-Anwendungen entwickelt wurden. Das Trainingskorpus für <code>jina-colbert-v2</code> umfasst 89 Sprachen, mit zusätzlichen Trainingsphasen für wichtige internationale Sprachen wie <strong>Arabisch, Chinesisch, Englisch, Französisch, Deutsch, Japanisch, Russisch und Spanisch</strong> sowie <strong>Programmiersprachen</strong>. Das Training umfasste auch ein Korpus ausgerichteter zweisprachiger Texte, um crosslinguale Potenziale zu erschließen, wodurch Queries und Dokumente in verschiedenen Sprachen bei Reranking/Retrieval-Aufgaben abgeglichen werden können.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Distribution-of-the-languages-in-the-training-corpus-at-the-pretrained-stage--3-.svg\" class=\"kg-image\" alt=\"Chart of language distribution in training data, highlighting dominance of English and Chinese.\" loading=\"lazy\" width=\"1456\" height=\"743\"><figcaption><span style=\"white-space: pre-wrap;\">Verteilung der Vortrainingsdaten nach Sprachen (spezifiziert durch </span><a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ISO-639</span></a><span style=\"white-space: pre-wrap;\"> Code) in logarithmischer Skala.</span></figcaption></figure><p>Heute sticht Jina ColBERT v2 als <strong>einziges mehrsprachiges ColBERT-ähnliches Modell</strong> hervor, das kompakte Embeddings generiert und die BM25-basierte Suche in allen getesteten Sprachen auf MIRACL-Benchmarks deutlich übertrifft.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-Multilingual-Data--1-.svg\" class=\"kg-image\" alt=\"Bar chart comparing jina-colbert-v2 and BM25 performance across 20 languages on multilingual tasks.\" loading=\"lazy\" width=\"691\" height=\"426\"><figcaption><span style=\"white-space: pre-wrap;\">Jina ColBERT v2 Leistung über 16 Sprachen, verglichen mit BM25, auf MIRACL-Benchmarks.</span></figcaption></figure><p>Darüber hinaus übertrifft Jina ColBERT v2 bei englischsprachigen Retrieval-Aufgaben die Leistung seines Vorgängers <code>jina-colbert-v1-en</code> und des ursprünglichen ColBERT v2 Modells, mit vergleichbarer Leistung zum hochspezialisierten, rein englischsprachigen <a href=\"https://huggingface.co/answerdotai/answerai-colbert-small-v1?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">AnswerAI-ColBERT-small</a> Modell.</p>\n<!--kg-card-begin: html-->\n<table class=\"simple-table\">\n  <tbody>\n<thead>\n<tr>\n      <th><strong>Modellname</strong></th>\n      <th><strong>Durchschnittliche Punktzahl<br>(14 BEIR Englisch-only Benchmarks)<br></strong></th>\n      <th><strong>Mehrsprachige Unterstützung</strong></th>\n  </tr>\n    </thead>\n    <tr>\n      <td><code>jina-colbert-v2</code></td>\n      <td>0.521</td>\n      <td>Mehrsprachig</td>\n    </tr>\n    <tr>\n      <td><code>jina-colbert-v1-en</code></td>\n      <td>0.494</td>\n      <td>Nur Englisch</td>\n    </tr>\n    <tr>\n      <td>ColBERT v2.0</td>\n      <td>0.489</td>\n      <td>Nur Englisch</td>\n    </tr>\n    <tr>\n      <td>AnswerAI-ColBERT-small</td>\n      <td>0.549</td>\n      <td>Nur Englisch</td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-English-only-datasets-from-BEIR--2-.svg\" class=\"kg-image\" alt=\"Bar chart showing model evaluations on English BEIR datasets, with several models like 'jina-colbert' and 'BM25'.\" loading=\"lazy\" width=\"1088\" height=\"712\"><figcaption><span style=\"white-space: pre-wrap;\">Evaluierung von jina-colbert-v2 auf einer Auswahl englischsprachiger Datensätze aus dem BEIR-Benchmark.</span></figcaption></figure><h2 id=\"matryoshka-representation-learning\">Matryoshka Representation Learning</h2><p><a href=\"https://arxiv.org/abs/2205.13147?ref=jina-ai-gmbh.ghost.io\">Matryoshka Representation Learning</a> ist eine Technik zum Training von Modellen, die verschiedene Ausgabevektorgrößen unterstützt und dabei Genauigkeitsverluste minimiert. Wir trainieren die verborgenen Schichten des Netzwerks mit mehreren verschiedenen linearen Projektionsköpfen — den finalen Schichten eines neuronalen Netzwerks — die jeweils eine andere Ausgabegröße unterstützen. <strong>Jina ColBERT v2 unterstützt Ausgabevektoren von 128, 96 und 64 Dimensionen.</strong></p><p>Jina ColBERT v2 produziert standardmäßig 128-dimensionale Ausgabe-Embeddings, kann aber auch 96- und 64-dimensionale Vektoren erzeugen, die eine nahezu identische Leistung aufweisen, aber 25% bzw. 50% kürzer sind.</p><p>Die folgende Tabelle zeigt die <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\">nDGC</a>-Leistung von<code>jina-colbert-v2</code> für die Top-Ten-Ergebnisse (<em>nDGC@10</em>) über sechs Datensätze aus dem BEIR-Benchmark. Hier sehen Sie, dass der Leistungsunterschied zwischen 128 und 96 Dimensionen kaum 1% beträgt und unter 1,5% zwischen 128 und 64 Dimensionen.</p>\n<!--kg-card-begin: html-->\n<table id=\"b838dc78-1321-499e-98e7-63e3b5c8e910\" class=\"simple-table\"><tbody><thead id=\"177f4349-0620-4947-a3ce-01e598395ed7\"><tr><th id=\"<\\ml\" class=\"\"><strong>Output Dimensions</strong></th><th id=\"<NYX\" class=\"\"><strong>Durchschnittlicher </strong><strong>Score</strong><strong><br>(nDGC@10 für 6 Benchmarks)<br></strong></th></tr></thead><tr id=\"9199b56b-0513-4c99-a2a7-29cde915c3b9\"><td id=\"<\\ml\" class=\"\">128</td><td id=\"<NYX\" class=\"\">0.565</td></tr><tr id=\"af4d45fc-ebf4-4e1f-b5b0-1807a1cb889b\"><td id=\"<\\ml\" class=\"\">96</td><td id=\"<NYX\" class=\"\">0.558</td></tr><tr id=\"ecf7eac9-5c56-47e6-ab27-0ddb4659e263\"><td id=\"<\\ml\" class=\"\">64</td><td id=\"<NYX\" class=\"\">0.556</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Performance-on-selected-BEIR-benchmarks.svg\" class=\"kg-image\" alt=\"Bar chart of BEIR benchmarks, highlighting scores of datasets like nfcorpus to msmarco, with jina-colbert-v2.64 excelling.\" loading=\"lazy\" width=\"732\" height=\"538\"><figcaption><span style=\"white-space: pre-wrap;\">Jina ColBERT v2 Leistung bei verschiedenen Output-Dimensionen.</span></figcaption></figure><p>Die Reduzierung der Größe der Ausgabevektoren spart Speicherplatz und beschleunigt Anwendungen wie vektorbasierte Informationssuche, bei denen verschiedene Vektoren verglichen oder der Abstand zwischen ihnen gemessen werden muss.</p><p>Dies hat erhebliche Kostenauswirkungen, allein schon in Bezug auf reduzierten Speicherplatz. Zum Beispiel kostet die Speicherung von 100 Millionen Dokumenten auf AWS mit 128-dimensionalen Vektoren für jedes Dokument laut <a href=\"https://cloud.qdrant.io/calculator?ref=jina-ai-gmbh.ghost.io\">Qdrants Cloud-Kostenrechner</a> schätzungsweise US$1.319,24 pro Monat</a>. Bei 64 Dimensionen <a href=\"https://cloud.qdrant.io/calculator?provider=aws&region=eu-central-1&replicas=1&quantization=None&vectors=100000000&dimension=64&ref=jina-ai-gmbh.ghost.io\">sinkt dies auf US$659,62</a>.</p><h2 id=\"getting-started-with-jina-colbert-v2\">Erste Schritte mit Jina ColBERT v2</h2><p>Jina ColBERT v2 ist über die Jina Search Foundation API, den <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a> und <a href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\">auf Azure</a> verfügbar. Es ist auch für <em>nicht-kommerzielle Nutzung</em> (<a href=\"https://www.creativecommons.org/licenses/by-nc/4.0/deed.en?ref=jina-ai-gmbh.ghost.io\">CC BY-NC-4.0</a>) über <a href=\"https://huggingface.co/jinaai/jina-colbert-v2?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a> verfügbar.</p><h3 id=\"via-jina-search-foundation-api\">Über die Jina Search Foundation API</h3><h4 id=\"for-embedding\">Für Embedding</h4><p>Der folgende <code>curl</code>-Befehl zeigt, wie man Input und Optionen spezifiziert, um Dokument-Embeddings von <code>jina-colbert-v2</code> über die Jina Embeddings API zu erhalten. Um Vektoren Ihrer bevorzugten Größe zu erhalten, geben Sie 128 oder 64 für den Parameter <code>dimensions</code> an. Dieser Parameter ist optional und der Standardwert ist 128.</p><p>Eingabedokumente werden abgeschnitten, wenn sie länger als 8192 Token sind.</p><p>Geben Sie Ihren Jina API-Schlüssel im Autorisierungs-Header an <code>Authorization: Bearer &lt;YOUR JINA API KEY&gt;</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # Or 64 for half-size vectors\n\t\"input_type\": \"document\", # For query embeddings see below\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your document text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each text can be up to 8192 tokens long\"\n    ]}'\n</code></pre><p>Um Query-Embeddings zu erhalten, setzen Sie den Parameter <code>input_type</code> auf <code>query</code> statt <code>document</code>. Beachten Sie, dass Queries viel strengere Größenbeschränkungen haben als Dokumente. Sie werden bei 32 Token abgeschnitten. Die Query-Kodierung wird <em>immer</em> 32 Token zurückgeben, einschließlich Embeddings für das Padding, wenn weniger als 32 Token lang.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # Or 64 for half-size vectors\t\n\t\"input_type\": \"query\", # This must be specified for query embeddings\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your query text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each query text can be up to 32 tokens long\"\n    ]}'\n</code></pre><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Multimodal, bilingual long-context embeddings for your search and RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"for-reranking\">Für Reranking</h4><p>Um <code>jina-colbert-v2</code> über die Jina Reranker API zu verwenden, übergeben Sie eine Query und mehrere Dokumente und erhalten rankingfähige Match-Scores zurück. Konstruieren Sie Ihre Anfrage wie folgt:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/rerank \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n      \"model\": \"jina-colbert-v2\",\n      \"query\": \"What is the population of Berlin?\",\n      \"top_n\": 3,\n      \"documents\": [\n        \"Berlin's population grew by 0.7 percent in 2023 compared with the previous year. Accordingly, around 27,300 more residents lived in Berlin at the end of the last year than in 2022. Those of 30 to under 40 years old form the numerically largest age group. With roughly 881,000 foreign residents from around 170 nations and an average age of the population of 42.5 years old.\",\n        \"Mount Berlin is a glacier-covered volcano in Marie Byrd Land, Antarctica, 100 kilometres (62 mi) from the Amundsen Sea. It is a roughly 20-kilometre-wide (12 mi) mountain with parasitic vents that consists of two coalesced volcanoes: Berlin proper with the 2-kilometre-wide (1.2 mi) Berlin Crater and Merrem Peak with a 2.5-by-1-kilometre-wide (1.55 mi × 0.62 mi) crater, 3.5 kilometres (2.2 mi) away from Berlin.\",\n        \"Population as of 31.12.2023 by nationality and federal states Land\\\\tTotal\\\\tGermans\\\\tForeigners\\\\tincluding EU-states number\\\\t%\\\\tnumber\\\\t%\",\n        \"The urban area of Berlin has a population of over 4.5 million and is therefore the most populous urban area in Germany. The Berlin-Brandenburg capital region has around 6.2 million inhabitants and is Germany's second-largest metropolitan region after the Rhine-Ruhr region, and the sixth-biggest metropolitan region by GDP in the European Union.\",\n        \"Irving Berlin (born Israel Beilin) was an American composer and songwriter. His music forms a large part of the Great American Songbook. Berlin received numerous honors including an Academy Award, a Grammy Award, and a Tony Award.\",\n        \"Berlin is a town in the Capitol Planning Region, Connecticut, United States. The population was 20,175 at the 2020 census.\",\n        \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\",\n        \"Berlin, Berlin ist eine für die ARD produzierte Fernsehserie, die von 2002 bis 2005 im Vorabendprogramm des Ersten ausgestrahlt wurde. Regie führten unter anderem Franziska Meyer Price, Christoph Schnee, Sven Unterwaldt Jr. und Titus Selge.\"\n        ]\n    }'</code></pre><p>Beachten Sie das Argument <code>top_n</code>, das die Anzahl der Dokumente angibt, die Sie abrufen möchten. Wenn Ihre Anwendung beispielsweise nur den besten Treffer verwendet, setzen Sie <code>top_n</code> auf 1.</p><p>Code-Snippets in Python und anderen Programmiersprachen und Frameworks finden Sie auf der <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina AI Embeddings API-Seite</a> oder wählen Sie <code>jina-colbert-v2</code> aus dem Dropdown-Menü auf der <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina Reranker API-Seite</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Maximize the search relevancy and RAG accuracy at ease.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"via-stanford-colbert\">Über Stanford ColBERT</h3><p>Sie können Jina ColBERT v2 auch als Drop-in-Ersatz für <a href=\"https://github.com/stanford-futuredata/ColBERT?ref=jina-ai-gmbh.ghost.io\">ColBERT v2</a> in der Stanford ColBERT Bibliothek verwenden. Geben Sie einfach <code>jinaai/jina-colbert-v2</code> als Modellquelle an:</p><pre><code class=\"language-python\">from colbert.infra import ColBERTConfig\nfrom colbert.modeling.checkpoint import Checkpoint\n\nckpt = Checkpoint(\"jinaai/jina-colbert-v2\", colbert_config=ColBERTConfig())\ndocs = [\"Your list of texts\"] \nquery_vectors = ckpt.queryFromText(docs)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Sie müssen <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> und <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> installieren, um den obigen Code zu verwenden.</div></div><h3 id=\"via-ragatouille\">Über RAGatouille</h3><p>Jina ColBERT v2 ist auch in <a href=\"https://github.com/AnswerDotAI/RAGatouille?ref=jina-ai-gmbh.ghost.io\">RAGatouille</a> integriert. Sie können es über die Methode <code>RAGPretrainedModel.from_pretrained()</code> herunterladen und verwenden:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v2\")\ndocs = [\"Your list of texts\"]\nRAG.index(docs, index_name=\"your_index_name\")\nquery = \"Your query\"\nresults = RAG.search(query)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Sie müssen <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> und <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> installieren, um den obigen Code zu verwenden.</div></div><h3 id=\"via-qdrant\">Über Qdrant</h3><p>Seit Version 1.10 hat Qdrant <a href=\"https://qdrant.tech/blog/qdrant-1.10.x/?ref=jina-ai-gmbh.ghost.io\">Unterstützung</a> für Multi-Vektoren und Late-Interaction-Modelle hinzugefügt. Bestehende Nutzer von Qdrant-Engines, ob lokale oder verwaltete Cloud-Versionen, können direkt von der Integration von <code>jina-colbert-v2</code> über den Qdrant-Client profitieren.</p><p><strong>Erstellen einer neuen Collection mit der MAX_SIM Operation</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nqdrant_client.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config={\n        \"colbert\": models.VectorParams(\n            size=128,\n            distance=models.Distance.COSINE,\n            multivector_config=models.MultiVectorConfig(\n                comparator=models.MultiVectorComparator.MAX_SIM\n            ),\n        )\n    }\n)</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Die korrekte Einstellung des <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">multivector_config</code> Parameters ist essenziell für die Verwendung von ColBERT-Style Modellen in Qdrant.</div></div><p><strong>Dokumente in Multi-Vektor-Collections einfügen</strong></p><pre><code class=\"language-Python\">import requests\nfrom qdrant_client import QdrantClient, models\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\ndata = {\n    'model': 'jina-colbert-v2',\n    'input_type': 'query',\n    'embedding_type': 'float',\n    'input': [\n        'Your text string goes here',\n        'You can send multiple texts',\n        'Each text can be up to 8192 tokens long'\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nrows = response.json()[\"data\"]\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nfor i, row in enumerate(rows):\n    qdrant_client.upsert(\n        collection_name=\"{collection_name}\",\n        points=[\n            models.PointStruct(\n                id=i,  \n                vector=row[\"embeddings\"],  \n                payload={\"text\": data[\"input\"][i]} \n            )\n        ],\n    )</code></pre><p><strong>Collections abfragen</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\nimport requests\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\n\ndata = {\n    'model': 'jina-colbert-v2',\n    \"input_type\": \"query\",\n    \"embedding_type\": \"float\",\n    \"input\": [\n        \"how many tokens in an input do Jina AI's embedding models support?\"\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nvector = response.json()[\"data\"][0][\"embeddings\"]\n\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nresults = qdrant_client.query_points(\n    collection_name=\"{collection_name}\",\n    query=vector,\n)\n\nprint(results)</code></pre><h3 id=\"summary\">Zusammenfassung</h3><p>Jina ColBERT v2 (<code>jina-colbert-v2</code>) baut auf der hohen Leistung von <code>jina-colbert-v1-en</code> auf und erweitert seine Fähigkeiten auf eine Vielzahl globaler Sprachen. Mit Unterstützung für mehrere Embedding-Größen ermöglicht <code>jina-colbert-v2</code> den Nutzern, das Verhältnis zwischen Präzision und Effizienz an ihre spezifischen Anwendungsfälle anzupassen, was potenziell erhebliche Zeit- und Rechenkosten einsparen kann.</p><p>Dieses Modell vereint all diese Funktionen in einem einzigen, preislich wettbewerbsfähigen Paket, das über eine intuitive Web-API zugänglich und mit jedem Computing-Framework kompatibel ist, das HTTP-Anfragen unterstützt. <a href=\"https://jina.ai/?sui=&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Testen Sie es selbst</a> mit 1 Million kostenlosen Tokens, um zu sehen, wie es Ihre Anwendungen und Prozesse verbessern kann.</p>",
  "comment_id": "66cd8fc6e84873000133d63d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-27T10:35:18.000+02:00",
  "updated_at": "2024-09-09T07:43:38.000+02:00",
  "published_at": "2024-08-30T09:19:58.000+02:00",
  "custom_excerpt": "Jina ColBERT v2 supports 89 languages with superior retrieval performance, user-controlled output dimensions, and 8192 token-length. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking/",
  "excerpt": "Jina ColBERT v2 unterstützt 89 Sprachen mit überlegener Retrievalleistung, benutzerdefinierten Ausgabedimensionen und einer Token-Länge von 8192.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Dark-themed coding interface displaying English and Japanese characters with \"JINA COLBERT V2\" highlighted in the center.",
  "feature_image_caption": null
}
{
  "slug": "quantization-aware-training-of-jina-embeddings-v4",
  "id": "685d4b76f1bef30001fc5449",
  "uuid": "6b06b483-2d13-4f1d-8d9d-147fa6dffe4b",
  "title": "Quantisierungsbewusstes Training von jina-embeddings-v4",
  "html": "<p>Die Quantisierung ist eine weit verbreitete Methode zur Lösung von Skalierungsproblemen in der KI. Der Name klingt kompliziert, aber es handelt sich lediglich um das Abrunden von Zahlen, damit sie weniger Speicherplatz beanspruchen. Dies bedeutet kleinere Vektorraummodelle (Embeddings), die weniger Speicherplatz benötigen, und eine schnellere Informationsabfrage, da der Vergleich von Vektoren weniger Zeit in Anspruch nimmt. Die Quantisierung ist eine rein numerische Technik, bei der es keine Rolle spielt, welche Art von Daten Ihr Modell verarbeitet oder welche Anwendungsfälle Sie haben. Sie kann also Verbesserungen bringen, ohne dass viel teures Fachwissen erforderlich ist.</p><p>Man könnte erwarten, dass die Quantisierung mit den üblichen Kompromissen verbunden ist und nichts umsonst ist - wobei wir etwas Präzision opfern müssen. In diesem Artikel zeigen wir Ihnen einen Weg, dies durch <em>quantisierungsbewusstes Training</em> (QAT) <strong>verlustfrei zu gestalten</strong>. Diese Technik wird in <code>jina-embeddings-v4</code> verwendet, um kleinere Vektorraummodelle (Embeddings) bereitzustellen, die in speicherkritischen Anwendungen benötigt werden.</p><h2 id=\"overview-of-quantization-techniques\">Überblick über die Quantisierungstechniken</h2><p>Modellquantisierung bedeutet in der Regel eines von vier Dingen:</p><ul><li>Post-Training-Quantisierung (<strong>PTQ</strong>)</li><li>Training für quantisierte Vektorraummodell-Ausgaben (<strong>Output QAT</strong>)</li><li>Training für vollständig quantisierte Modelle (<strong>Full QAT</strong>)</li><li>Destillieren eines neuen quantisierten Modells aus einem bestehenden, nicht quantisierten Modell</li></ul><p>Die Post-Training-Quantisierung (<strong>PTQ</strong>) akzeptiert das trainierte Vektorraummodell (Embedding)-Modell so, wie es ist, und verändert es in keiner Weise. Es geht lediglich darum, die am wenigsten signifikanten Stellen der vom Modell erzeugten Gleitkommawerte zu verwerfen. Wir runden die Zahlen einfach ab und skalieren sie manchmal auf einen bestimmten Bereich.</p><p><strong>Output QAT</strong> bedeutet, das Vektorraummodell (Embedding)-Modell so zu optimieren, dass es optimale Vektoren mit reduzierter Präzision erzeugt. Dies bedeutet, dass das Modell verändert wird, aber es ändert nichts an der Präzision der Gewichte des Modells und reduziert daher nicht seine Größe. Nur die Größe des Ausgabvektors wird reduziert.</p><p><strong>Full QAT</strong> beginnt mit einem vollständig trainierten Modell mit voller Präzision und reduziert die Präzision der Modellgewichte. Anschließend wird die Leistung dieses modifizierten Modells feinabgestimmt. Dies führt zu einem deutlich kleineren Modell sowie zu kleineren Vektorraummodellen (Embeddings), allerdings auf Kosten einer gewissen Feinabstimmung.</p><p><strong>Destillation</strong> ist der Prozess des Trainierens eines neuen Modells, um die Leistung eines bestehenden Modells zu erreichen. Dies bedeutet, dass ein neues Modell erstellt wird, das von Grund auf als quantisiert konzipiert ist, und dann das bestehende Modell verwendet wird, um so viele Trainingsdaten wie nötig zu generieren, um es so lange zu trainieren, bis es so gut wie möglich mit dem bestehenden Modell übereinstimmt.</p><p>Die Vorteile dieser vier Ansätze sind in der folgenden Tabelle zusammengefasst:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Ansatz</th>\n<th>Kompaktere Vektorraummodelle (Embeddings)?</th>\n<th>Erfordert Training?</th>\n<th>Modellkomprimierung?</th>\n<th>Schnellere Inferenz?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>PTQ</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Output QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Full QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td><strong>Destillation</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><em>(zu einem kleineren Modell)</em></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Alle vier erzeugen kompaktere Vektorraummodelle (Embeddings), aber außer PTQ erfordern alle ein zusätzliches Training, während nur Full QAT und Destillation neue, schnellere Modelle erzeugen. Full QAT und Destillation sind in der Umsetzung viel aufwendiger, da sie ein wesentlich umfangreicheres Training erfordern als Output QAT.</p><p>In diesem Artikel werden wir uns nur mit PTQ und Output QAT befassen, die die Größe oder Geschwindigkeit des Vektorraummodell (Embedding)-Modells nicht verändern.</p><h2 id=\"experimental-setup\">Experimenteller Aufbau</h2><p>Für diese Experimente ist unser Basismodell <code>jina-embeddings-v4</code> mit dem Retrieval-Adapter, der 32-Bit-Gleitkommavektoren (FP32) in 2048 Dimensionen erzeugt. Jedes Vektorraummodell (Embedding) ist daher 8196 Byte oder 8 KB groß.</p><p>Wir haben verschiedene experimentelle Bedingungen anhand von Abfrage-Dokumenten-Retrieval-Benchmark-Aufgaben aus der <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR-Benchmark</a>-Suite untersucht. Der Retrieval-Prozess verwendet die Kosinusähnlichkeit zwischen Vektoren, um die Dokumente zu finden und zu ordnen, die am besten zu den Abfragen passen.</p><ul><li><strong>Baseline</strong> – Die Leistung von <code>jina-embeddings-v4</code> Vektorraummodellen (Embedding)-Vektoren ohne Quantisierung. Alle diese Experimente verwendeten eine Beta-Version des Modells, und die Release-Leistung ist etwas besser.</li><li><strong>PTQ</strong> – Wir haben die Ausgabvektoren zu binären Vektoren quantisiert, ohne das Modell zu verändern.</li><li><strong>Output QAT</strong> – Wir haben die Ausgabvektoren quantisiert und den Retrieval-Adapter feinabgestimmt, um seine Leistung unter quantisierten Bedingungen zu verbessern.</li></ul><h3 id=\"quantization-levels\">Quantisierungsstufen</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"816\" height=\"636\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png 816w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Vergleich der Größen von Post-Quantisierungs-Vektorraummodellen (Embeddings).</span></figcaption></figure><p>Wir haben mit vier verschiedenen Quantisierungsstufen experimentiert.</p><ul><li><strong>8-Bit-Ganzzahlen</strong> – FP32-Werte werden auf ganze Zahlen im Bereich von -128 bis 127 reduziert, wodurch sich die Vektorraummodelle (Embeddings) um das Vierfache auf <strong>2048 Byte</strong> verkleinern.</li><li><strong>4-Bit-Ganzzahlen</strong> – Gleiches gilt wie für 4-Bit-Ganzzahlen, aber wir ordnen dem Bereich von -8 bis 7 zu, wodurch die Vektorgrößen um den Faktor 8 auf <strong>1024 Byte</strong> reduziert werden.</li><li><strong>Trinäre Quantisierung</strong> – Alle Werte werden einem von drei Werten zugeordnet: -1, 0, 1. Optimal gespeichert, reduziert dies jede Dimension auf 1,6 Bit, wodurch die Größe der Vektorraummodelle (Embedding)-Vektoren um etwa das 40-fache auf etwa <strong>230 Byte</strong> reduziert wird.</li><li><strong>Binäre Quantisierung</strong> – Wir konvertieren FP32-Skalarwerte in ein Bit, indem wir den Datentyp <code>torch.sign</code> verwenden, der nur zwei Werte bereitstellt, für deren Speicherung ein Bit benötigt wird. Dies reduziert 2048-dimensionale Vektorraummodelle (Embedding)-Vektoren von 8192 Byte auf <strong>128 Byte</strong>, eine 64-fache Reduzierung.</li></ul><h3 id=\"scaling\">Skalierung</h3><p>Bei der binären Quantisierung ist die Quantisierung sehr einfach: Wenn ein Vektorwert über 0 oder positiv ist, wird er 1 zugeordnet. Andernfalls wird er -1 zugeordnet.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1159\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png 1159w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 2: Binäre Quantisierung. Alle negativen Werte werden zu -1, alle anderen zu 1.</span></figcaption></figure><p>Für die anderen Quantisierungsszenarien haben wir die Werte auf einen Bereich normalisiert und dann auf den nächsten Wert gerundet, der durch die Quantisierungsstufe zugelassen wurde. Vektorraummodelle (Embedding)-Vektoren bestehen aus Skalenzahlen zwischen -∞ und +∞ (oder in der Praxis sehr großen positiven und negativen Zahlen). Wir verwenden zwei Zahlen, $max$ und $min$, um die Werte für die Quantisierung zu skalieren.</p><p>Für die trinäre Quantisierung nehmen wir jede Vektorkomponente $v$ und übersetzen sie wie folgt:</p><ul><li>wenn $v$ ≥ $max$, wird $v$ zu 1.</li><li>wenn $v$ ≤ $min$, wird $v$ zu -1.</li><li>wenn $min$ &lt; $v$ &lt; $max$, wird $v$ zu 0.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"220\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png 1030w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 3: Trinäre Quantisierung. Ein Intervall wird definiert und Werte innerhalb dieses Intervalls werden zu 0. Alle niedrigeren Werte werden zu -1 und alle höheren zu 1.</span></figcaption></figure><p>Für 4-Bit-Ganzzahlen:</p><ul><li>wenn $v$ ≥ $max$, wird $v$ zu 7.</li><li>wenn $v$ ≤ $min$, wird $v$ zu -8.</li><li>wenn $min$ &lt; $v$ &lt; $max$, wird $v$ zu $16*(v - min)/(max - min) - 8$, dann auf die nächste ganze Zahl gerundet. Dies skaliert den Wert in den Bereich $[-8,7]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 4: 4-Bit-Quantisierung. Ein Intervall wird definiert und alle Werte werden auf den definierten Bereich [-8,7] normalisiert.</span></figcaption></figure><p></p><p>Für 8-Bit-Ganzzahlen:</p><ul><li>wenn $v$ ≥ $max$, wird $v$ zu 127.</li><li>wenn $v$ ≤ $min$, wird $v$ zu -128.</li><li>wenn $min$ &lt; $v$ &lt; $max$, wird $v$ zu $256*(v - min)/(max - min) - 128$, gerundet auf die nächste ganze Zahl. Dies skaliert den Wert in den Bereich $[-128,127]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"219\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 5: 8-Bit-Quantisierung. Ein Intervall wird definiert und alle Werte werden auf den definierten Bereich [-128,127] normalisiert.</span></figcaption></figure><p>Zur Berechnung von $max$ und $min$ haben wir zwei Ansätze verwendet:</p><ul><li><strong>Min/Max</strong> – Wir haben unsere Daten in Batches verarbeitet und für jeden Batch die höchste und niedrigste Vektorkomponente ermittelt, wobei wir $max$ auf die höchste und $min$ auf die niedrigste gesetzt haben.</li><li><strong>Rollierende Mittelwertbildung über Batches</strong> – Für jeden Batch haben wir den Durchschnitt und die Standardabweichung der Vektorkomponenten berechnet. Wir haben einen gleitenden Durchschnitt sowohl des Durchschnitts als auch der Standardabweichung beibehalten, während wir alle Batches verarbeitet haben. Wenn $avg$ der aktuelle gleitende Durchschnitt der durchschnittlichen Batch-Werte ist und $std$ der aktuelle gleitende Durchschnitt der Standardabweichungen ist, dann gilt für jeden Batch:</li></ul><p>$max = avg + std$<br>$min = avg - std$</p><h3 id=\"qat-fine-tuning\">QAT-Feinabstimmung</h3><p>Für die PTQ-Experimente haben wir das Modell so verwendet, wie es ist, und die von ihm erzeugten Vektorraummodelle (Embeddings) mit den oben beschriebenen Methoden quantisiert.</p><p>Für die Output QAT haben wir das Modell mit <em>Straight-Through Estimation</em> feinabgestimmt. Das bedeutet, dass wir den Quantisierungsprozess umkehren und die volle Präzision der Werte wiederherstellen, bevor wir den Verlust (d. h. den Fehler) berechnen, und dann diese Verlustmetrik verwenden, um das Modell feinabzustimmen.</p><p>Wir haben in jedem Fall für 10.000 Schritte feinjustiert und alle 500 Schritte einen Checkpoint gespeichert. Wir haben dann den Checkpoint mit der höchsten Punktzahl auf dem <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR</a>-Benchmark beibehalten.</p><h3 id=\"asymmetric-quantization\">Asymmetrische Quantisierung</h3><p>PTQ und Output QAT reduzieren die Größe der Vektorraummodelle (Embeddings), reduzieren aber nicht die Modellgröße oder die Inferenzgeschwindigkeit; alle Einsparungen liegen in der Größe der gespeicherten Dokumenten-Vektorraummodelle (Embeddings) und der Abrufgeschwindigkeit.</p><p>Infolgedessen haben wir sowohl die Quantisierung der Abfragevektoren getestet als auch die unquantisierten Vektoren zum Zeitpunkt des Abrufs beibehalten, da dies die Größe der gespeicherten Vektorraummodelle (Embeddings) in beiden Fällen nicht verändert.</p><h2 id=\"results\">Ergebnisse</h2><p>Wir haben insgesamt neun Bedingungen getestet, die in den folgenden Tabellen zusammengefasst sind:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Bedingungsname</th>\n<th>Feinabstimmung</th>\n<th>Quantisierungsstufe</th>\n<th>Skalierungsstrategie</th>\n<th>Quantisierte Abfragen</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>❌</td>\n<td>n/a</td>\n<td>n/a</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Beide</td>\n<td>❌</td>\n<td>Binär</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>PTQ Nur Dokumente</td>\n<td>❌</td>\n<td>Binär</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Binär</td>\n<td><strong>✓</strong></td>\n<td>Binär</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT Binär Nur Dokumente</td>\n<td><strong>✓</strong></td>\n<td>Binär</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Trinär</td>\n<td><strong>✓</strong></td>\n<td>Trinär</td>\n<td>Gleitender Durchschnitt</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 4-Bit</td>\n<td><strong>✓</strong></td>\n<td>4-Bit</td>\n<td>Gleitender Durchschnitt</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-Bit</td>\n<td><strong>✓</strong></td>\n<td>8-Bit</td>\n<td>Gleitender Durchschnitt</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-Bit Min/Max</td>\n<td><strong>✓</strong></td>\n<td>8-Bit</td>\n<td>Min/Max</td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Tabelle 2: Versuchsbedingungen</em></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Bedingungsname</th>\n<th>Durchschnittliche Punktzahl</th>\n<th>Unterschied zur Baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>60.10</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Binär</td>\n<td>58.33</td>\n<td>-1.78</td>\n</tr>\n<tr>\n<td>PTQ Binär Nur Dokumente</td>\n<td>59.08</td>\n<td>-1.02</td>\n</tr>\n<tr>\n<td>QAT Binär</td>\n<td>59.22</td>\n<td>-0.89</td>\n</tr>\n<tr>\n<td>QAT Binär Nur Dokumente</td>\n<td>60.81</td>\n<td>+0.70</td>\n</tr>\n<tr>\n<td>QAT Trinär</td>\n<td>59.49</td>\n<td>-0.62</td>\n</tr>\n<tr>\n<td>QAT 4-Bit</td>\n<td>61.73</td>\n<td>+1.62</td>\n</tr>\n<tr>\n<td>QAT 8-Bit</td>\n<td>61.67</td>\n<td>+1.56</td>\n</tr>\n<tr>\n<td>QAT 8-Bit Min/Max</td>\n<td>61.29</td>\n<td>+1.19</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Tabelle 3: Durchschnittliche Punktzahl (in % richtig) für jede Bedingung über die zwölf NanoBEIR-Benchmarks.</em></p><p>Wie Sie der obigen Tabelle entnehmen können, verbessert die Feinabstimmung für die Quantisierung die Punktzahlen. Der einzige Unterschied zwischen den Bedingungen <strong>PTQ Binär</strong> und <strong>QAT Binär</strong> ist die Feinabstimmung, und der Unterschied in der Punktzahl ist signifikant. In ähnlicher Weise sehen wir eine Verbesserung der Punktzahlen um fast 2 % zwischen den Bedingungen <strong>PTQ Binär Nur Dokumente</strong> und <strong>QAT Binär Nur Dokumente</strong>, die sich nur durch die gleiche Feinabstimmung unterscheiden.</p><p>Es überrascht nicht, dass wir auch feststellen, dass sich die Punktzahlen im Allgemeinen verbessern, je weniger wir quantisieren, wobei die 4-Bit-Quantisierung besser abschneidet als die trinäre und die trinäre besser als die binäre. Es scheint jedoch, dass eine weitere Umstellung auf 8-Bit nichts verbessert hat.</p><p>Wir haben das Weglassen der Quantisierung von Abfragen nur in binären Fällen getestet, aber dies scheint die Leistung zu verbessern.</p><p>Schließlich legen unsere Tests nahe, dass die rollierende Durchschnittsskalierungsmethode die einfache Min/Max-Methode übertrifft.</p><h2 id=\"conclusion\">Fazit</h2><p>Die Quantisierung hat einige wichtige operative Vorteile für Vektorraummodelle (Embedding), da sie die Größe der Vektorraummodelle (Embedding) erheblich reduziert und die Informationsbeschaffung beschleunigt. Während die einfache Post-Training-Quantisierung (PTQ) unmittelbare Vorteile in Bezug auf Speicher und Lagerung bietet, zeigen unsere Experimente, dass das quantisierungsbewusste Training (QAT) die unvermeidlichen Präzisionsverluste erheblich reduziert. Die Feinabstimmung führte durchweg zu besseren Ergebnissen.</p><p>Der Grad der Quantisierung wirkt sich direkt auf die Leistung aus, was man von einer Methode erwarten würde, die auf der Reduzierung der Präzision von Werten basiert. Weniger aggressive Quantisierung (z. B. 4-Bit) übertrifft im Allgemeinen aggressivere Methoden (z. B. binär), aber überraschenderweise gab es keinen signifikanten Unterschied in der Leistung zwischen 8-Bit- und 4-Bit-Quantisierung. Es scheint, dass es bis zum Erreichen einer gewissen Schwelle der Ungenauigkeit kaum einen Unterschied zwischen größerer und geringerer Quantisierung gibt.</p><p>Auch Skalierungsstrategien sind von Bedeutung, wobei die rollierende Durchschnittsmethode im Vergleich zu einem festen Min/Max-Ansatz überlegene Ergebnisse zeigt. Die Verwendung von Skalierungswerten, die relativ zu den Daten sind, scheint deutlich besser zu funktionieren und verdient weitere Untersuchungen.</p><p>Die Quantisierung kann Ihnen mehr aus Ihren Vektorraummodellen (Embedding) für weniger herausholen. Obwohl dieser Artikel nicht alle Optionen für die Quantisierung untersucht, werden zwei Optionen untersucht, die leicht zugänglich sind und echte Vorteile bieten. Wir arbeiten daran, die Quantisierungsstrategien zu verfeinern und zu verbessern, damit wir die Kosten der Benutzer weiter senken können, und erwarten, in naher Zukunft binäre Unterstützung für <code>jina-embeddings-v4</code> zu veröffentlichen.</p>",
  "comment_id": "685d4b76f1bef30001fc5449",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading---2025-06-30T114820.483.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-06-26T15:30:30.000+02:00",
  "updated_at": "2025-06-30T21:14:36.000+02:00",
  "published_at": "2025-06-30T21:14:36.000+02:00",
  "custom_excerpt": "Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/quantization-aware-training-of-jina-embeddings-v4/",
  "excerpt": "Quantisierung ermöglicht kleinere Vektormodelle (Embeddings). Wir zeigen Ihnen, dass fein abgestimmte Quantisierung Ihnen sogar verlustfreie Vektormodelle (Embeddings) ermöglicht.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
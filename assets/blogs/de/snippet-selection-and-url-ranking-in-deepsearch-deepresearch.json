{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "Snippet-Auswahl und URL-Ranking in DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ein praktischer Leitfaden zur Implementierung von DeepSearch/DeepResearch</div><div class=\"kg-bookmark-description\">QPS raus, Tiefe rein. DeepSearch ist der neue Standard. Finden Sie Antworten durch Lesen-Suchen-Begr√ºnden-Schleifen. Erfahren Sie, was es ist und wie man es aufbaut.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Wenn Sie unseren Implementierungsleitfaden f√ºr DeepSearch/DeepResearch bereits gelesen haben, lassen Sie uns tiefer in einige Details eintauchen, die die Qualit√§t <em>deutlich</em> verbessern k√∂nnen. In diesem Beitrag konzentrieren wir uns auf zwei zentrale Herausforderungen: <strong>die Nutzung von Embeddings f√ºr die Snippet-Auswahl aus langen Webseiten</strong> und <strong>die Verwendung von Rerankers zur Priorisierung von URLs beim Crawling.</strong></p><p>Einige erinnern sich vielleicht an unsere fr√ºhere Schlussfolgerung, dass \"Embeddings nur f√ºr Query-Deduplizierung wie STS-Aufgaben (Semantic Textual Similarity) n√ºtzlich waren, w√§hrend Rerankers nicht einmal Teil unserer urspr√ºnglichen DeepSearch-Implementierung waren.\" Nun, es stellt sich heraus, dass beide immer noch sehr wertvoll sind - nur nicht auf die konventionelle Art und Weise, wie man es erwarten w√ºrde. Wir haben immer den <em>schlankesten</em> m√∂glichen Weg verfolgt. Wir f√ºgen keine Komponenten hinzu, nur um ihre Existenz oder unseren Wert als Embedding- & Reranker-Anbieter zu rechtfertigen. <strong>Wir konzentrieren uns darauf, was Suche wirklich im Kern ben√∂tigt.</strong></p><p>Nach Wochen von Experimenten und Iterationen haben wir ungew√∂hnliche, aber effektive Verwendungen f√ºr beide in DeepSearch/DeepResearch-Systemen entdeckt. Durch ihre Anwendung haben wir die Qualit√§t von <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> deutlich verbessert (Sie k√∂nnen es gerne ausprobieren). Wir m√∂chten diese Erkenntnisse mit anderen Praktikern in diesem Bereich teilen.</p><h2 id=\"select-snippet-from-long-content\">Snippet-Auswahl aus langen Inhalten</h2><p>Das Problem ist folgendes: Nachdem wir <a href=\"https://jina.ai/reader\">Jina Reader zum Lesen von Webseiteninhalten verwendet haben</a>, m√ºssen wir diesen als Wissensobjekt dem Kontext des Agenten f√ºr die Schlussfolgerung hinzuf√ºgen. W√§hrend das Einf√ºgen des vollst√§ndigen Inhalts in das Kontextfenster des LLM der einfachste Weg ist, ist es unter Ber√ºcksichtigung der Token-Kosten und Generierungsgeschwindigkeit nicht optimal. In der Praxis m√ºssen wir identifizieren, welche Teile des Inhalts f√ºr die Frage am relevantesten sind und nur diese Teile selektiv als Wissen zum Kontext des Agenten hinzuf√ºgen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Wir sprechen von F√§llen, in denen der Inhalt auch nach der Markdown-Bereinigung durch Jina Reader zu lang bleibt. Dies kommt h√§ufig bei langen Seiten wie GitHub Issues, Reddit-Threads, Forumsdiskussionen und Blogbeitr√§gen vor (einschlie√ülich vieler unserer eigenen von jina.ai/news).</div></div><p>LLM-basierte Filterung hat die gleichen Kosten- und Latenzprobleme, also suchen wir nach L√∂sungen mit kleineren Modellen: Wir brauchen kleinere und kosteng√ºnstigere, <strong>aber dennoch mehrsprachige Modelle</strong> ‚Äì ein entscheidender Faktor, da wir nicht garantieren k√∂nnen, dass entweder die Anfrage oder die Dokumente immer auf Englisch sein werden.</p><p>Wir haben auf der einen Seite eine Frage (entweder die urspr√ºngliche Anfrage oder eine L√ºckenfrage) und auf der anderen Seite einen gro√üen Markdown-Inhalt, bei dem der gr√∂√üte Teil irrelevant ist. Wir m√ºssen die relevantesten Snippets f√ºr die Anfrage ausw√§hlen. Dies √§hnelt dem Chunking-Problem, mit dem sich die RAG-Community seit 2023 besch√§ftigt - nur relevante Chunks mittels Retriever-Modellen abzurufen, um sie im Kontextfenster f√ºr die Zusammenfassung zu platzieren. In unserem Fall gibt es jedoch zwei wichtige Unterschiede:</p><ol><li>Begrenzte Chunks aus einer begrenzten Anzahl von Dokumenten. Wenn jeder Chunk ungef√§hr 500 Token enth√§lt, dann hat ein typisches langes Webdokument etwa 200.000 Token (p50) bis 1.000.000 Token (p99), und wir verwenden Jina Reader, um 4-5 URLs pro Schritt abzurufen, was ungef√§hr Hunderte von Chunks ergibt - also Hunderte von Embedding-Vektoren und Hunderte von Kosinus-√Ñhnlichkeiten. Dies ist mit JavaScript im Arbeitsspeicher ohne Vektordatenbank leicht zu bew√§ltigen.</li><li>Wir brauchen aufeinanderfolgende Chunks, um effektive Wissens-Snippets zu bilden. Wir k√∂nnen keine Snippets akzeptieren, die verstreute S√§tze wie <code>[1-2, 6-7, 9, 14, 17, ...]</code> kombinieren. Ein n√ºtzlicheres Wissens-Snippet w√ºrde Mustern wie <code>[3-15, 17-24, ...]</code> folgen - immer mit aufeinanderfolgendem Text. Dies macht es f√ºr das LLM einfacher, aus der Wissensquelle zu kopieren und zu zitieren und reduziert Halluzinationen.</li></ol><p>Der Rest sind all die Vorbehalte, √ºber die Praktiker klagen: Jeder Chunk kann nicht zu lang sein, da Embedding-Modelle langen Kontext nicht gut verarbeiten k√∂nnen; Chunking f√ºhrt zu Kontextverlust und macht Chunk-Embeddings i.i.d; und wie findet man √ºberhaupt die besten Grenzmarkierungen, die sowohl Lesbarkeit als auch Semantik erhalten? Wenn Sie wissen, wovon wir sprechen, dann wurden Sie wahrscheinlich von diesen Problemen in Ihren RAG-Implementierungen heimgesucht.</p><p>Aber kurz gesagt - <strong>Late-Chunking mit <code>jina-embeddings-v3</code> l√∂st alle drei Probleme auf elegante Weise.</strong> Late-Chunking beh√§lt die Kontextinformationen f√ºr jeden Chunk bei, ist <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">unempfindlich gegen√ºber Grenzmarkierungen</a>, und <code>jina-embeddings-v3</code> selbst ist SOTA in <em>asymmetrischen</em> mehrsprachigen Retrieval-Aufgaben. Interessierte Leser k√∂nnen unseren Blogbeitr√§gen oder Papers f√ºr Details folgen, aber hier ist die Gesamtimplementierung.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">Dieses Diagramm illustriert den Snippet-Auswahlalgorithmus, der √§hnlich wie </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\"> funktioniert. Der Prozess beginnt damit, ein langes Dokument in Chunks fester L√§nge zu teilen, die dann mit </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> mit aktiviertem Late-Chunking eingebettet werden. Nach der Berechnung der √Ñhnlichkeitswerte zwischen jedem Chunk und der Anfrage bewegt sich ein gleitendes Fenster √ºber die √Ñhnlichkeitswerte, um das Fenster mit dem h√∂chsten Durchschnittswert zu finden.</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Was Late Chunking wirklich ist & was nicht: Teil II</div><div class=\"kg-bookmark-description\">Teil 2 unserer Erforschung von Late Chunking, ein tiefer Einblick, warum es die beste Methode f√ºr Chunk-Embeddings und die Verbesserung der Such-/RAG-Leistung ist.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Mehrsprachige Embeddings mit Task LoRA</div><div class=\"kg-bookmark-description\">Wir stellen jina-embeddings-v3 vor, ein neuartiges Text-Embedding-Modell mit 570 Millionen Parametern, das Spitzenleistungen bei mehrsprachigen Daten und Long-Context-Retrieval-Aufgaben erzielt und Kontextl√§ngen von bis zu 8192 Token unterst√ºtzt. Das Modell enth√§lt eine Reihe aufgabenspezifischer Low-Rank Adaptation (LoRA) Adapter zur Generierung hochwertiger Embeddings f√ºr Query-Document Retrieval, Clustering, Klassifizierung und Text Matching. Die Evaluierung auf dem MTEB-Benchmark zeigt, dass jina-embeddings-v3 die neuesten propriet√§ren Embeddings von OpenAI und Cohere bei englischen Aufgaben √ºbertrifft und gleichzeitig eine √ºberlegene Leistung im Vergleich zu multilingual-e5-large-instruct bei allen mehrsprachigen Aufgaben erzielt. Mit einer Standard-Ausgabedimension von 1024 k√∂nnen Benutzer die Embedding-Dimensionen flexibel bis auf 32 reduzieren, ohne die Leistung zu beeintr√§chtigen, erm√∂glicht durch Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Kontextuelle Chunk-Embeddings mit Long-Context Embedding-Modellen</div><div class=\"kg-bookmark-description\">Viele Anwendungsf√§lle erfordern das Abrufen kleinerer Textteile, und dichtevektor-basierte Retrieval-Systeme funktionieren oft besser mit k√ºrzeren Textsegmenten, da die Semantik in den Embeddings weniger wahrscheinlich √ºberkomprimiert wird. Folglich teilen Praktiker Textdokumente oft in kleinere Chunks auf und kodieren sie separat. Allerdings k√∂nnen Chunk-Embeddings, die auf diese Weise erstellt werden, Kontextinformationen aus umgebenden Chunks verlieren, was zu suboptimalen Repr√§sentationen f√ºhrt. In diesem Paper stellen wir eine neuartige Methode namens Late Chunking vor, die Long-Context Embedding-Modelle nutzt, um zun√§chst alle Token des langen Texts einzubetten, wobei das Chunking erst nach dem Transformer-Modell und kurz vor dem Mean Pooling angewendet wird - daher der Begriff \"late\" in der Bezeichnung. Die resultierenden Chunk-Embeddings erfassen die vollst√§ndigen Kontextinformationen und f√ºhren zu √ºberlegenen Ergebnissen bei verschiedenen Retrieval-Aufgaben. Die Methode ist generisch genug, um auf eine breite Palette von Long-Context Embedding-Modellen angewendet zu werden und funktioniert ohne zus√§tzliches Training. Um die Effektivit√§t von Late Chunking weiter zu steigern, schlagen wir einen dedizierten Fine-Tuning-Ansatz f√ºr Embedding-Modelle vor.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Verwendung von Late Chunking und Conv1D-artigem Mean Pooling zur Auswahl des besten Snippets in Bezug auf die Frage.</span></p></figcaption></figure><p>Stellen Sie sicher, dass Sie die Jina Embeddings API mit den folgenden Einstellungen f√ºr retrieval <code>task</code>, <code>late_chunking</code> und <code>truncate</code> aufrufen:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">F√ºr das Embedding der Frage m√ºssen Sie </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> zu </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> √§ndern und </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code><span style=\"white-space: pre-wrap;\"> deaktivieren</span></p></figcaption></figure><p>Die vollst√§ndige Implementierung finden Sie auf Github:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">URL-Ranking f√ºr das n√§chste Lesen</h2><p>Das Problem ist Folgendes: W√§hrend einer DeepSearch-Sitzung sammeln Sie wahrscheinlich viele URLs von Suchmaschinenergebnisseiten (SERP) und entdecken noch mehr, wenn Sie einzelne Webseiten lesen (die On-Page-Links). Die Gesamtzahl der eindeutigen URLs kann leicht in die Hunderte gehen. Auch hier ist es ineffizient, einfach alle URLs direkt in den Kontext des LLM zu √ºbertragen - es verschwendet wertvollen Kontextfensterplatz und, was noch problematischer ist, <strong>wir haben festgestellt, dass LLMs URLs im Wesentlichen zuf√§llig ausw√§hlen.</strong> Es ist wichtig, das LLM zu URLs zu f√ºhren, die mit h√∂chster Wahrscheinlichkeit die ben√∂tigte Antwort enthalten.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Beste Option f√ºr die Verwendung von Jina Reader zum Crawlen einer Seite in DeepSearch. Dies sammelt alle On-Page-Links in einem separaten </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> Feld und entfernt sie aus dem </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\"> Feld.</span></p></figcaption></figure><p>Betrachten Sie dieses Problem als einen kontextbezogenen PageRank, bei dem wir Hunderte von URLs w√§hrend einer Sitzung gewichten m√ºssen. Wir ranken URLs basierend auf mehreren Faktoren, die letzte Aktualisierungszeit, Dom√§nenh√§ufigkeit, Pfadstruktur und vor allem semantische Relevanz zur Anfrage kombinieren, um eine Gesamtbewertung zu erstellen. Denken Sie daran, dass wir nur die Informationen verwenden k√∂nnen, die <em>vor</em> dem tats√§chlichen Besuch der URL verf√ºgbar sind:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>H√§ufigkeitssignale</strong>: URLs, die mehrfach √ºber verschiedene Quellen erscheinen, erhalten zus√§tzliches Gewicht. URLs von Domains, die h√§ufig in Suchergebnissen auftauchen, erhalten einen Boost, da beliebte Domains oft ma√ügebliche Inhalte enthalten.</p><p><strong>Pfadstruktur</strong>: Wir analysieren URL-Pfade, um Inhaltscluster zu identifizieren. URLs innerhalb gemeinsamer Pfadhierarchien erhalten h√∂here Bewertungen, wobei ein Abklingfaktor auf tiefere Pfade angewendet wird.</p><p><strong>Semantische Relevanz</strong>: Wir verwenden <code>jina-reranker-v2-base-multilingual</code>, um die semantische Relevanz zwischen der Frage und den Textinformationen jeder URL zu bewerten, was <a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">ein klassisches Reranking-Problem</a> ist. Die Textinformationen jeder URL stammen aus:</p><ul><li>Titel & Snippets aus SERP-API-Ergebnissen (<code>https://s.jina.ai/</code> mit <code>'X-Respond-With': 'no-content'</code>)</li><li>Ankertext von On-Page-URLs (<code>https://r.jina.ai</code> mit <code>'X-With-Links-Summary': 'all'</code>)</li></ul><p><strong>Letzte Aktualisierungszeit</strong>: Einige DeepSearch-Anfragen sind zeitkritisch, daher sind k√ºrzlich aktualisierte URLs wertvoller als √§ltere. Ohne eine gro√üe Suchmaschine wie Google zu sein, ist es eine Herausforderung, die letzte Aktualisierungszeit zuverl√§ssig zu bestimmen. Wir haben einen mehrschichtigen Ansatz implementiert, der folgende Signale kombiniert und einen vertrauensw√ºrdigen Zeitstempel liefert, der bei Bedarf aktuellere Inhalte priorisiert.</p><ul><li>SERP-API-Filter (wie der <code>tbs</code> Parameter von s.jina.ai zur Filterung nach Aktualit√§t)</li><li>HTTP-Header-Analyse (Last-Modified, ETag)</li><li>Metadaten-Extraktion (Meta-Tags, Schema.org Zeitstempel)</li><li>Inhaltsmuster-Erkennung (sichtbare Daten im HTML)</li><li>CMS-spezifische Indikatoren f√ºr Plattformen wie WordPress, Drupal und Ghost</li></ul><p><strong>Gesch√ºtzter Inhalt:</strong> Einige Inhalte auf Social-Media-Plattformen sind gesch√ºtzt oder einfach hinter Bezahlschranken, und ohne Anmeldung oder Verletzung ihrer Nutzungsbedingungen gibt es keine legitime M√∂glichkeit, auf diese Inhalte zuzugreifen. Wir sollten aktiv eine Liste problematischer URLs und Hostnamen pflegen, um deren Rankings zu senken und keine Zeit mit unzug√§nglichen Inhalten zu verschwenden.</p><p><strong>Domain-Vielfalt:</strong> In manchen F√§llen stammen die am h√∂chsten gewichteten URLs alle von denselben Hostnamen, was DeepSearch in einem lokalen Optimum gefangen halten und die endg√ºltige Qualit√§t der Ergebnisse reduzieren kann. Sehen Sie sich die obigen Beispiele an, wo alle Top-URLs von StackOverflow stammen. Zur Verbesserung der Vielfalt k√∂nnen wir einen Explore-Exploit-Ansatz implementieren, indem wir die Top-k am h√∂chsten bewerteten URLs von jedem Hostname ausw√§hlen.</p><p>Die vollst√§ndige Implementierung des URL-Rankings finden Sie auf unserem Github.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Faceü§ó Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset ¬∑ Issue #7175 ¬∑ huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Denken Sie daran, URL-Gewichtungen in den Kontext des Agenten einzuf√ºgen und LLMs anzuweisen, die Gewichtungen zu respektieren.</span></p></figcaption></figure><h2 id=\"conclusion\">Fazit</h2><p>Seit der Ver√∂ffentlichung unseres DeepSearch-Systems am 2. Februar 2025 haben wir zwei Implementierungsdetails entdeckt, die die Qualit√§t erheblich verbessert haben. Bemerkenswerterweise nutzen beide multilinguale Embeddings und Reranker auf eine ‚ÄûIn-Context\"-Weise ‚Äì sie operieren in einem viel kleineren Ma√üstab als die vorberechneten Indizes, die diese Modelle typischerweise ben√∂tigen. Dies erkl√§rt unser anf√§ngliches √úbersehen.</p><p>Dies deutet auf eine faszinierende Polarisierung in der Zukunft der Suchtechnologie hin. Betrachten wir ein Framework analog zu Kahnemans Zwei-System-Theorie:</p><ul><li>Fast-Think (grep, BM25, SQL): Schnelles, regelbasiertes Pattern Matching mit minimalem Rechenaufwand.</li><li>Slow-Think (LLM): Umfassendes Reasoning mit tiefem kontextuellem Verst√§ndnis, das erhebliche Rechenleistung erfordert.</li><li>Mid-Think (Embeddings, Reranker): Im Zwischenbereich gefangen? Zu ‚Äûfortgeschritten\"/semantisch f√ºr einfaches Pattern Matching, aber ohne echte Reasoning-F√§higkeiten.</li></ul><p>Wir beobachten m√∂glicherweise die zunehmende Beliebtheit einer zweigeteilten Architektur, bei der leichtgewichtiges, effizientes SQL/BM25 die erste Inhaltsgewinnung √ºbernimmt und direkt in leistungsstarke LLMs f√ºr die Tiefenverarbeitung einflie√üt. Diese LLMs integrieren zunehmend die semantischen Funktionen, die zuvor spezialisierte Mid-Level-Modelle erforderten. Die verbleibende Rolle f√ºr Mid-Think-Modelle verlagert sich auf spezialisierte In-Context-Aufgaben: Filterung, Deduplizierung und Operationen mit begrenztem Umfang, bei denen vollst√§ndiges Reasoning ineffizient w√§re.</p><p>Dennoch bleiben die Auswahl kritischer Snippets und das Ranking von URLs fundamentale Komponenten mit direkter Auswirkung auf die Qualit√§t der DeepSearch/DeepResearch-Systeme. Wir hoffen, dass unsere Erkenntnisse Verbesserungen in Ihren eigenen Implementierungen ansto√üen.</p><p>Query-Expansion bleibt ein weiterer entscheidender Qualit√§tsfaktor. Wir evaluieren aktiv mehrere Ans√§tze ‚Äì von einfachen Prompt-basierten Umschreibungen bis hin zu kleinen Sprachmodellen und Reasoning-basierten Methoden. Achten Sie auf unsere kommenden Erkenntnisse zu diesem Thema. Bleiben Sie dran.</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "Diese zwei Details verwandeln deine DeepSearch von mittelm√§√üig zu herausragend: die Auswahl der besten Textausschnitte aus langen Webseiten und das Ranking von URLs vor dem Crawling.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
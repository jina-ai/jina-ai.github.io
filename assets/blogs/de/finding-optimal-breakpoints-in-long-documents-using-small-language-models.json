{
  "slug": "finding-optimal-breakpoints-in-long-documents-using-small-language-models",
  "id": "67126986708dbe00019249f2",
  "uuid": "b7e55a5d-f267-4a4a-b861-27221c0f3827",
  "title": "Optimale Umbruchpunkte in langen Dokumenten mit Small Language Models finden",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Dies ist Teil III unserer Chunking-Serie. <b><strong style=\"white-space: pre-wrap;\">Empfohlene Lesereihenfolge: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Teil I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Teil II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Research Paper</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, Teil III.</strong></b></div></div><p>In unseren vorherigen Beitr√§gen haben wir die Herausforderungen des Chunkings untersucht und <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">das Konzept des Late Chunking vorgestellt</a>, das hilft, den Kontextverlust beim Einbetten von Chunks zu reduzieren. In diesem Beitrag konzentrieren wir uns auf eine weitere Herausforderung: das Finden optimaler Trennpunkte. W√§hrend sich <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io#late-chunking-is-resilient-to-poor-boundary-cues\" rel=\"noreferrer\">unsere Late-Chunking-Strategie als sehr widerstandsf√§hig gegen schlechte Grenzen erwiesen hat</a>, bedeutet das nicht, dass wir sie ignorieren k√∂nnen ‚Äì sie sind weiterhin wichtig f√ºr die Lesbarkeit sowohl f√ºr Menschen als auch f√ºr LLMs. Unsere Perspektive ist folgende: Bei der Bestimmung von Trennpunkten k√∂nnen wir uns jetzt voll und ganz auf die Lesbarkeit konzentrieren, ohne uns Sorgen um semantischen oder Kontextverlust machen zu m√ºssen. Late Chunking kann sowohl gute als auch schlechte Trennpunkte verarbeiten, sodass die Lesbarkeit Ihre prim√§re Sorge wird.</p><p>Vor diesem Hintergrund haben wir drei kleine Sprachmodelle trainiert, die speziell f√ºr die Segmentierung langer Dokumente entwickelt wurden, w√§hrend sie semantische Koh√§renz bewahren und komplexe Inhaltsstrukturen handhaben k√∂nnen. Diese sind:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>simple-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, das Text basierend auf den strukturellen Elementen des Dokuments segmentiert.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>topic-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, das Text basierend auf Themen innerhalb des Textes segmentiert.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-summary-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>summary-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, das Zusammenfassungen f√ºr jedes Segment generiert.</span></p></figcaption></figure><p>In diesem Beitrag werden wir diskutieren, warum wir dieses Modell entwickelt haben, wie wir die drei Varianten angegangen sind und wie sie sich im Vergleich zur <a href=\"https://www.notion.so/Advancing-Segmentation-Strategies-in-RAG-with-a-Custom-Small-Language-Model-Restructure-638b84ae461d412eb6889cfa7f54cce1?pvs=21&ref=jina-ai-gmbh.ghost.io\">Jina AI Segmenter API</a> bew√§hren. Abschlie√üend teilen wir unsere Erkenntnisse und einige Gedanken f√ºr die Zukunft.</p><h2 id=\"segmentation-problem\">Segmentierungsproblem</h2><p>Segmentierung ist ein Kernelement in RAG-Systemen. Wie wir lange Dokumente in koh√§rente, handhabbare Segmente aufteilen, beeinflusst direkt die Qualit√§t sowohl der Retrieval- als auch der Generierungsschritte und wirkt sich auf alles aus, von der Antwortrelevanz bis zur Zusammenfassungsqualit√§t. Traditionelle Segmentierungsmethoden haben zwar gute Ergebnisse erzielt, sind aber nicht ohne Einschr√§nkungen.</p><p>Um unseren <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\">vorherigen Beitrag</a> zu paraphrasieren:</p><blockquote>Bei der Segmentierung eines langen Dokuments besteht eine zentrale Herausforderung darin, zu entscheiden, wo die Segmente erstellt werden sollen. Dies kann durch feste Token-L√§ngen, eine festgelegte Anzahl von S√§tzen oder fortgeschrittenere Methoden wie Regex und semantische Segmentierungsmodelle erfolgen. Die Festlegung genauer Segmentgrenzen ist entscheidend, da sie nicht nur die Lesbarkeit der Suchergebnisse verbessert, sondern auch sicherstellt, dass die Segmente, die einem LLM in einem RAG-System bereitgestellt werden, sowohl pr√§zise als auch ausreichend sind.</blockquote><p>W√§hrend Late Chunking die Retrieval-Performance verbessert, <strong>ist es in RAG-Anwendungen entscheidend sicherzustellen, dass m√∂glichst jedes Segment f√ºr sich genommen bedeutungsvoll ist und nicht nur ein zuf√§lliger Textausschnitt</strong>. LLMs sind auf koh√§rente, gut strukturierte Daten angewiesen, um genaue Antworten zu generieren. Wenn Segmente unvollst√§ndig sind oder ihnen Bedeutung fehlt, k√∂nnte das LLM Probleme mit Kontext und Genauigkeit haben, was trotz der Vorteile des Late Chunking die Gesamtleistung beeintr√§chtigt. Kurz gesagt, ob Sie Late Chunking verwenden oder nicht, eine solide Segmentierungsstrategie ist essentiell f√ºr den Aufbau eines effektiven RAG-Systems (wie Sie im Benchmark-Abschnitt weiter unten sehen werden).</p><p>Traditionelle Segmentierungsmethoden, ob sie nun Inhalte an einfachen Grenzen wie Zeilenumbr√ºchen oder S√§tzen trennen oder starre tokenbasierte Regeln verwenden, sto√üen oft auf die gleichen Einschr√§nkungen. Beide Ans√§tze ber√ºcksichtigen keine semantischen Grenzen und haben Schwierigkeiten mit mehrdeutigen Themen, was zu fragmentierten Segmenten f√ºhrt. Um diese Herausforderungen zu bew√§ltigen, haben wir ein kleines Sprachmodell speziell f√ºr die Segmentierung entwickelt und trainiert, das Themenwechsel erkennen und Koh√§renz bewahren kann, w√§hrend es effizient und anpassungsf√§hig f√ºr verschiedene Aufgaben bleibt.</p><h2 id=\"why-small-language-model\">Warum ein Small Language Model?</h2><p>Wir haben ein Small Language Model (SLM) entwickelt, um spezifische Einschr√§nkungen zu adressieren, auf die wir bei traditionellen Segmentierungstechniken gesto√üen sind, insbesondere bei der Handhabung von Code-Snippets und anderen komplexen Strukturen wie Tabellen, Listen und Formeln. Bei traditionellen Ans√§tzen, die sich oft auf Token-Z√§hlungen oder starre Strukturregeln verlassen, war es schwierig, die Integrit√§t semantisch zusammenh√§ngender Inhalte zu bewahren. Code-Snippets wurden zum Beispiel h√§ufig in mehrere Teile segmentiert, was ihren Kontext zerst√∂rte und es nachgelagerten Systemen erschwerte, sie genau zu verstehen oder abzurufen.</p><p>Durch das Training eines spezialisierten SLM wollten wir ein Modell schaffen, das diese bedeutungsvollen Grenzen intelligent erkennen und bewahren kann und sicherstellt, dass zusammengeh√∂rige Elemente zusammenbleiben. Dies verbessert nicht nur die Retrieval-Qualit√§t in RAG-Systemen, sondern auch nachgelagerte Aufgaben wie Zusammenfassung und Frage-Antwort, bei denen die Beibehaltung koh√§renter und kontextuell relevanter Segmente entscheidend ist. Der SLM-Ansatz bietet eine anpassungsf√§higere, aufgabenspezifische L√∂sung, die traditionelle Segmentierungsmethoden mit ihren starren Grenzen einfach nicht bieten k√∂nnen.</p><h2 id=\"training-slms-three-approaches\">Training von SLMs: Drei Ans√§tze</h2><p>Wir haben drei Versionen unseres SLM trainiert:</p><ul><li><code>simple-qwen-0.5</code> ist das geradlinigste Modell, entwickelt um Grenzen basierend auf den strukturellen Elementen des Dokuments zu identifizieren. Seine Einfachheit macht es zu einer effizienten L√∂sung f√ºr grundlegende Segmentierungsbed√ºrfnisse.</li><li><code>topic-qwen-0.5</code>, inspiriert von Chain-of-Thought Reasoning, geht bei der Segmentierung einen Schritt weiter, indem es Themen im Text identifiziert, wie zum Beispiel \"der Beginn des Zweiten Weltkriegs\", und diese Themen verwendet, um Segmentgrenzen zu definieren. Dieses Modell stellt sicher, dass jedes Segment thematisch koh√§rent ist, was es gut geeignet f√ºr komplexe, mehrthemige Dokumente macht. Erste Tests zeigten, dass es besonders gut darin ist, Inhalte auf eine Weise zu segmentieren, die der menschlichen Intuition sehr nahe kommt.</li><li><code>summary-qwen-0.5</code> identifiziert nicht nur Textgrenzen, sondern generiert auch Zusammenfassungen f√ºr jedes Segment. Die Zusammenfassung von Segmenten ist in RAG-Anwendungen sehr vorteilhaft, besonders f√ºr Aufgaben wie die Beantwortung von Fragen zu langen Dokumenten, obwohl dies mit dem Kompromiss einhergeht, dass beim Training mehr Daten erforderlich sind.</li></ul><p>Alle Modelle geben nur <em>Segment-Heads</em> zur√ºck ‚Äì eine gek√ºrzte Version jedes Segments. Anstatt vollst√§ndige Segmente zu generieren, geben die Modelle Kernpunkte oder Unterthemen aus, was die Grenzerkennung und Koh√§renz verbessert, indem sie sich auf semantische √úberg√§nge konzentrieren, anstatt einfach Eingabeinhalte zu kopieren. Bei der Abfrage der Segmente wird der Dokumenttext basierend auf diesen Segment-Heads aufgeteilt und die vollst√§ndigen Segmente werden entsprechend rekonstruiert.</p><h3 id=\"dataset\">Datensatz</h3><p>Wir verwendeten den <a href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\">wiki727k</a> Datensatz, eine umfangreiche Sammlung strukturierter Textausschnitte aus Wikipedia-Artikeln. Er enth√§lt √ºber 727.000 Textabschnitte, die jeweils einen bestimmten Teil eines Wikipedia-Artikels repr√§sentieren, wie eine Einleitung, einen Abschnitt oder Unterabschnitt.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - koomri/text-segmentation: Implementation of the paper: Text Segmentation as a Supervised Learning Task</div><div class=\"kg-bookmark-description\">Implementierung des Papers: Text-Segmentierung als √ºberwachte Lernaufgabe - koomri/text-segmentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">koomri</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/a0a75db005774d424366f3fa2d4c70930927a0b2d8032ef3c04cb0f3beebcb8e/koomri/text-segmentation\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"data-augmentation\">Data Augmentation</h3><p>Um Trainingspaare f√ºr jede Modellvariante zu generieren, haben wir GPT-4 zur Datenanreicherung verwendet. F√ºr jeden Artikel in unserem Trainingsdatensatz haben wir den folgenden Prompt gesendet:</p><pre><code class=\"language-python\">f\"\"\"\nGenerate a five to ten words topic and a one sentence summary for this chunk of text.\n```\n{text}\n```\nMake sure the topic is concise and the summary covers the main topic as much as possible.\n\nPlease respond in the following format:\n```\nTopic: ...\nSummary: ...\n```\n\nDirectly respond with the required topic and summary, do not include any other details, and do not surround your response with quotes, backticks or other separators.\n   \"\"\".strip()</code></pre><p>Wir haben eine einfache Aufteilung verwendet, um Abschnitte aus jedem Artikel zu generieren, indem wir bei <code>\\\\n\\\\n\\\\n</code> geteilt und dann bei <code>\\\\n\\\\n</code> unterteilt haben, um Folgendes zu erhalten (in diesem Fall ein Artikel √ºber Common Gateway Interface):</p><pre><code>[\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n</code></pre><p>Anschlie√üend haben wir eine JSON-Struktur mit den Abschnitten, Themen und Zusammenfassungen generiert:</p><pre><code>{\n  \"sections\": [\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n  \"topics\": [\n    \"Common Gateway Interface in Web Servers\",\n    \"The History and Standardization of CGI\",\n    \"CGI Scripts for Editing Web Pages\", \n    \"Reducing Web Server Overhead in Command Invocation\"\n  ],\n  \"summaries\": [\n    \"CGI bietet ein Protokoll f√ºr Webserver zur Ausf√ºhrung von Programmen, die dynamische Webseiten generieren.\",\n    \"Das NCSA definierte CGI urspr√ºnglich 1993, was zu seiner √úbernahme als Standard f√ºr Webserver und sp√§teren Formalisierung in RFC 3875 unter Vorsitz von Ken Coar f√ºhrte.\",\n    \"Dieser Text beschreibt, wie ein CGI-Skript die Bearbeitung und Speicherung von Webseiteninhalten √ºber HTML-Formulare handhabt.\",\n    \"Der Text diskutiert Techniken zur Minimierung des Server-Overheads bei h√§ufigen Befehlsaufrufen, einschlie√ülich Prozess-Preforking, Verwendung vorkompilierter CGI-Programme und Implementierung benutzerdefinierter Webserver-Module.\"\n  ]\n}\n</code></pre><p>Wir haben auch Rauschen hinzugef√ºgt, indem wir Daten durchmischten, zuf√§llige Zeichen/W√∂rter/Buchstaben hinzuf√ºgten, zuf√§llig Interpunktion entfernten und <em>immer</em> Zeilenumbr√ºche entfernten.</p><p>All das kann teilweise zur Entwicklung eines guten Modells beitragen - aber nur bis zu einem gewissen Punkt. Um wirklich alle Register zu ziehen, musste das Modell koh√§rente Abschnitte erstellen, ohne Code-Snippets zu besch√§digen. Daf√ºr haben wir den Datensatz mit Code, Formeln und Listen erweitert, die von GPT-4o generiert wurden.</p><h3 id=\"the-training-setup\">Das Training-Setup</h3><p>F√ºr das Training der Modelle haben wir folgendes Setup implementiert:</p><ul><li><strong>Framework</strong>: Wir verwendeten Hugging Face's <code>transformers</code> Bibliothek integriert mit <code>Unsloth</code> f√ºr die Modelloptimierung. Dies war entscheidend f√ºr die Optimierung der Speichernutzung und Beschleunigung des Trainings, wodurch es m√∂glich wurde, kleine Modelle mit gro√üen Datens√§tzen effektiv zu trainieren.</li><li><strong>Optimizer und Scheduler</strong>: Wir verwendeten den AdamW Optimizer mit einer linearen Lernratenplanung und Warm-up-Schritten, was uns erm√∂glichte, den Trainingsprozess w√§hrend der ersten Epochen zu stabilisieren.</li><li><strong>Experiment Tracking</strong>: Wir verfolgten alle Trainingsexperimente mit <a href=\"https://wandb.ai/?ref=jina-ai-gmbh.ghost.io\">Weights & Biases</a> und protokollierten wichtige Metriken wie Training- und Validierungsverlust, √Ñnderungen der Lernrate und die Gesamtleistung des Modells. Dieses Echtzeit-Tracking gab uns Einblicke in die Fortschritte der Modelle und erm√∂glichte schnelle Anpassungen, wenn n√∂tig, um die Lernergebnisse zu optimieren.</li></ul><h3 id=\"the-training-itself\">Das Training selbst</h3><p>Mit <a href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=jina-ai-gmbh.ghost.io\"><code>qwen2-0.5b-instruct</code></a> als Basismodell trainierten wir drei Varianten unseres SLM mit Unsloth, jede mit einer anderen Segmentierungsstrategie im Sinn. F√ºr unsere Samples verwendeten wir Trainingspaare, bestehend aus dem Text eines Artikels aus wiki727k und den resultierenden <code>sections</code>, <code>topics</code> oder <code>summaries</code> (oben im Abschnitt \"Data Augmentation\" erw√§hnt), je nach dem zu trainierenden Modell.</p><ul><li><code><strong>simple-qwen-0.5</strong></code>: Wir trainierten <code>simple-qwen-0.5</code> mit 10.000 Samples √ºber 5.000 Schritte und erreichten eine schnelle Konvergenz und effektive Erkennung von Grenzen zwischen koh√§renten Textabschnitten. Der Trainingsverlust betrug 0,16.</li><li><code><strong>topic-qwen-0.5</strong></code>: Wie bei <code>simple-qwen-0.5</code> trainierten wir <code>topic-qwen-0.5</code> mit 10.000 Samples √ºber 5.000 Schritte und erreichten einen Trainingsverlust von 0,45.</li><li><code><strong>summary-qwen-0.5</strong></code>: Wir trainierten <code>summary-qwen-0.5</code> mit 30.000 Samples √ºber 15.000 Schritte. Dieses Modell zeigte vielversprechende Ergebnisse, hatte aber einen h√∂heren Verlust (0,81) w√§hrend des Trainings, was auf die Notwendigkeit von mehr Daten hinweist (etwa das Doppelte unserer urspr√ºnglichen Sample-Anzahl), um sein volles Potenzial zu erreichen.</li></ul><h2 id=\"the-segments-themselves\">Die Segmente selbst</h2><p>Hier sind Beispiele von drei aufeinanderfolgenden Segmenten aus jeder Segmentierungsstrategie, zusammen mit Jinas Segmenter API. Um diese Segmente zu erstellen, verwendeten wir zun√§chst <a href=\"https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io\">Jina Reader</a>, um einen Beitrag aus dem Jina AI Blog als Klartext zu extrahieren (einschlie√ülich aller Seitendaten wie Header, Footer etc.), und √ºbergaben ihn dann an jede Segmentierungsmethode.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/can-embedding-reranker-models-compare-numbers/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Can Embedding/Reranker Models Compare Numbers?</div><div class=\"kg-bookmark-description\">A lot of LLMs can't figure out that 9.11 is actually smaller than 9.9. Can our embedding and reranker models do any better?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"jina-segmenter-api\">Jina Segmenter API</h3><p>Jina Segmenter API verfolgte einen sehr granularen Ansatz bei der Segmentierung des Beitrags, indem es an Zeichen wie <code>\\n</code>, <code>\\t</code> etc. aufspaltete, um den Text in oft sehr kleine Segmente zu unterteilen. Betrachtet man nur die ersten drei, extrahierte es <code>search\\\\n</code>, <code>notifications\\\\n</code> und <code>NEWS\\\\n</code> aus der Navigationsleiste der Website, aber nichts Relevantes zum eigentlichen Beitragsinhalt:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png\" class=\"kg-image\" alt=\"Minimalist navigation bar with &quot;NEWS&quot;, &quot;PRODUCTS&quot;, and &quot;COMPANY&quot; text on a black background, accented by colorful stripes to \" loading=\"lazy\" width=\"1164\" height=\"68\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Weiter unten erhielten wir endlich einige Segmente aus dem eigentlichen Blog-Beitragsinhalt, wobei in jedem nur wenig Kontext erhalten blieb:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png\" class=\"kg-image\" alt=\"Webpage discussing if embedding/reranker models can compare numbers, with a grid of numbered circles and references to an ICM\" loading=\"lazy\" width=\"1164\" height=\"1180\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>(Im Interesse der Fairness haben wir f√ºr die Segmenter API mehr Chunks gezeigt als f√ºr die Modelle, einfach weil es sonst sehr wenige aussagekr√§ftige Segmente zu zeigen g√§be)</p><h3 id=\"simple-qwen-05\"><code>simple-qwen-0.5</code></h3><p><code>simple-qwen-0.5</code> zerlegte den Blog-Beitrag basierend auf der semantischen Struktur und extrahierte viel l√§ngere Segmente mit koh√§renter Bedeutung:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png\" class=\"kg-image\" alt=\"Webpage screenshot with green background, top navigation bar, scientific graphs, and headers discussing model number comparis\" loading=\"lazy\" width=\"1164\" height=\"4590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"topic-qwen-05\"><code>topic-qwen-0.5</code></h3><p><code>topic-qwen-0.5</code> identifizierte zun√§chst Themen basierend auf dem Dokumentinhalt und segmentierte dann das Dokument basierend auf diesen Themen:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png\" class=\"kg-image\" alt=\"Webpage showcasing a scientific paper titled &quot;Can Embedding/Keras Models Compare Numbers?&quot; featuring plots, text blocks, and \" loading=\"lazy\" width=\"1164\" height=\"4526\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"summary-qwen-05\"><code>summary-qwen-0.5</code></h3><p><code>summary-qwen-0.5</code> identifizierte Segmentgrenzen und generierte eine Zusammenfassung des Inhalts innerhalb jedes Segments:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png\" class=\"kg-image\" alt=\"Green and gold-themed academic webpage discussing embedding/reranker models and experiment setup.\" loading=\"lazy\" width=\"1164\" height=\"3734\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"benchmarking-the-models\">Benchmarking der Modelle</h2><p>Um die Leistung unserer Modelle zu bewerten, haben wir acht Blog-Beitr√§ge aus dem Jina AI Blog extrahiert und sechs Fragen und Referenzantworten mit GPT-4o generiert.</p><p>Wir wendeten jede Segmentierungsmethode, einschlie√ülich der Jina Segmenter API, auf diese Blog-Beitr√§ge an und generierten dann Embeddings f√ºr die resultierenden Segmente mit <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v3</code></a>, ohne sp√§tes Chunking oder Reranking.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/v3banner.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Jede Gruppe von Segmenten wurde dann separat indexiert und wir nutzten ein RAG-System, um jeden Index mit den zuvor generierten Fragen abzufragen.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Um faire Vergleiche zu gew√§hrleisten, haben wir beim Testen von <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> und <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> nur die Segmente selbst indexiert, <i>nicht</i> die generierten Zusammenfassungen oder Themen. Die Indexierung dieser zus√§tzlichen Daten w√ºrde die Leistung wahrscheinlich weiter verbessern.</div></div><p>Um die Leistung zu messen, verglichen wir die aus dem Index jeder Segmentierungsmethode generierten Antworten mit den Ground-Truth-Antworten mittels F1-Scores aus der <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)?ref=jina-ai-gmbh.ghost.io\">ROUGE-L-Evaluation</a>, wobei h√∂here Scores bessere Leistung anzeigen.</p><p>Zum Beispiel erhielten wir f√ºr die Frage: \"Welche Faktoren beeinflussen die numerischen Vergleichsf√§higkeiten des <code>jina-embeddings-v2-base-en</code> Modells?\" folgende Antworten:</p><h3 id=\"ground-truth-generated-by-gpt-4o-from-the-full-post-text\">Ground Truth (Generiert von GPT-4 aus dem vollst√§ndigen Post-Text):</h3><pre><code>\"The two key factors are tokenization and training data. Tokenization determines how numbers are segmented into tokens, and training data influences numerical reasoning abilities.\"\n</code></pre><h3 id=\"jina-segmenter-api-score-0166667\">Jina Segmenter API (Score: 0.166667)</h3><pre><code>\"The context does not provide specific factors that impact the numerical comparison abilities of the jina-embeddings-v2-base-en model.\"\n</code></pre><h3 id=\"simple-qwen-05-score-0279070\"><code>simple-qwen-0.5</code> (Score: 0.279070)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by tokenization and training data.\"\n</code></pre><h3 id=\"topic-qwen-05-score-0190476\"><code>topic-qwen-0.5</code> (Score: 0.190476)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by factors such as tokenization and its ability to handle different numerical formats, including small integers, large numbers, floats, negative numbers, currency, dates, and times.\"\n</code></pre><h3 id=\"summary-qwen-05-0318182\"><code>summary-qwen-0.5</code> (0.318182)</h3><pre><code>\"The factors impacting the numerical comparison abilities of the jina-embeddings-v2-base-en model are tokenization and training data.\"\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>Warum ist der Score von </b><b><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code></b><b> so niedrig?</b><br>Dies ist gr√∂√ütenteils nur ein Zufall basierend auf der speziellen Frage, die wir dem Modell gestellt haben. Wie Sie in der untenstehenden Tabelle sehen k√∂nnen, ist der <i>durchschnittliche</i> ROUGE-Score von <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> der h√∂chste aller Segmentierungsmethoden.</div></div><p>Wir bewerteten auch die Geschwindigkeit jeder Methode (durch Zeitmessung f√ºr die Generierung und Einbettung von Segmenten) und sch√§tzten den Speicherplatzbedarf (durch Multiplikation der Anzahl der Einbettungen mit der Gr√∂√üe einer einzelnen 1024-dimensionalen Einbettung von <code>jina-embeddings-v3</code>). Dies erm√∂glichte uns, sowohl die Genauigkeit als auch die Effizienz der verschiedenen Segmentierungsstrategien zu bewerten.</p><h2 id=\"key-findings\">Wichtigste Erkenntnisse</h2><p>Nach dem Test der Modellvarianten gegeneinander und gegen die Jina Segmenter API stellten wir fest, dass die neuen Modelle tats√§chlich verbesserte Scores mit allen drei Methoden zeigten, besonders bei der Themensegmentierung:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png\" class=\"kg-image\" alt=\"Bar chart comparing average ROUGE scores for Jina Segmenter, Simple, COATopic, and Summary Segmentation.\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th><strong>Segmentierungsmethode</strong></th>\n<th><strong>Durchschnittlicher ROUGE-Score</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>0.352126</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>0.386096</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td><strong>0.398340</strong></td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>0.328143</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Warum hat <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> einen niedrigeren ROUGE-Score als <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code>? Kurz gesagt, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> zeigte w√§hrend des Trainings einen h√∂heren Verlust, was den Bedarf an mehr Training f√ºr bessere Ergebnisse offenbart. Das k√∂nnte ein Thema f√ºr zuk√ºnftige Experimente sein.</div></div><p>Allerdings w√§re es interessant, die Ergebnisse mit der Late-Chunking-Funktion von <code>jina-embeddings-v3</code> zu √ºberpr√ºfen, die die Kontextrelevanz der Segment-Embeddings erh√∂ht und relevantere Ergebnisse liefert. Das k√∂nnte ein Thema f√ºr einen zuk√ºnftigen Blogbeitrag sein.</p><p>Bez√ºglich der Geschwindigkeit kann es schwierig sein, die neuen Modelle mit Jina Segmenter zu vergleichen, da letzteres eine API ist, w√§hrend wir die drei Modelle auf einer Nvidia 3090 GPU liefen lie√üen. Wie Sie sehen k√∂nnen, wird jeder Geschwindigkeitsvorteil w√§hrend des schnellen Segmentierungsschritts der Segmenter API schnell durch die Notwendigkeit √ºberholt, Embeddings f√ºr so viele Segmente zu generieren:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png\" class=\"kg-image\" alt=\"Bar chart showing time for text segmentation methods: Jina Segmenter, Simple, CoT Topic, and Summary Segmentation, with notab\" loading=\"lazy\" width=\"1682\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png 1682w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png\" class=\"kg-image\" alt=\"Vertical bar chart displaying the embedding times for Jina Segmenter, Simple, CoT Topic, and Summary Segmentation.\" loading=\"lazy\" width=\"1698\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png 1698w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>Anmerkungen</b><br>‚Ä¢ Wir verwenden verschiedene Y-Achsen in beiden Grafiken, da die Darstellung so unterschiedlicher Zeitrahmen in einem Graphen oder mit konsistenten Y-Achsen nicht praktikabel war.<br>‚Ä¢ Da wir dies rein als Experiment durchf√ºhrten, verwendeten wir kein Batching bei der Generierung von Embeddings. Dies w√ºrde die Operationen f√ºr alle Methoden erheblich beschleunigen.</div></div><p>Nat√ºrlich bedeuten mehr Segmente mehr Embeddings. Und diese Embeddings ben√∂tigen viel Speicherplatz: Die Embeddings f√ºr die acht von uns getesteten Blogbeitr√§ge beanspruchten √ºber 21 MB mit der Segmenter API, w√§hrend die Summary Segmentation mit schlanken 468 KB auskam. Dies, plus die h√∂heren ROUGE-Scores unserer Modelle bedeuten weniger, aber bessere Segmente, was Geld spart und die Leistung steigert:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png\" class=\"kg-card kg-image\" alt=\"Vertikales Balkendiagramm, das die gesamte Embedding-Gr√∂√üe von Segmentierungsmethoden vergleicht, wobei &quot;Jina Segmenter&quot; mit 20,0 deutlich h√∂her ist\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Segmentation Method</strong></th>\n<th><strong>Segment Count</strong></th>\n<th><strong>Average Length (characters)</strong></th>\n<th><strong>Segmentation Time (minutes/seconds)</strong></th>\n<th><strong>Embedding Time (hours/minutes)</strong></th>\n<th><strong>Total Embedding Size</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>1,755</td>\n<td>82</td>\n<td>3.8s</td>\n<td>1h 46m</td>\n<td>21.06 MB</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>48</td>\n<td>1,692</td>\n<td>49s</td>\n<td>1h 2m</td>\n<td>576 KB</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td>69</td>\n<td>1,273</td>\n<td>2m 3s</td>\n<td>1h 6m</td>\n<td>828 KB</td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>39</td>\n<td>1,799</td>\n<td>2m 40s</td>\n<td>53m</td>\n<td>468 KB</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"what-we-learned\">Was wir gelernt haben</h3><h3 id=\"problem-formulation-is-critical\">Die Problemformulierung ist entscheidend</h3><p>Eine wichtige Erkenntnis war der Einfluss der Aufgabenstellung. Indem wir das Modell Segmentk√∂pfe ausgeben lie√üen, verbesserten wir die Erkennung von Grenzen und die Koh√§renz, da wir uns auf semantische √úberg√§nge konzentrierten, anstatt den Eingabeinhalt einfach in separate Segmente zu kopieren. Dies f√ºhrte auch zu einem schnelleren Segmentierungsmodell, da die Generierung von weniger Text dem Modell erm√∂glichte, die Aufgabe schneller abzuschlie√üen.</p><h3 id=\"llm-generated-data-is-effective\">LLM-generierte Daten sind effektiv</h3><p>Die Verwendung von LLM-generierten Daten, insbesondere f√ºr komplexe Inhalte wie Listen, Formeln und Code-Snippets, erweiterte den Trainingsdatensatz des Modells und verbesserte seine F√§higkeit, verschiedene Dokumentstrukturen zu verarbeiten. Dies machte das Modell anpassungsf√§higer f√ºr unterschiedliche Inhaltstypen ‚Äì ein entscheidender Vorteil bei der Verarbeitung technischer oder strukturierter Dokumente.</p><h3 id=\"output-only-data-collation\">Ausgabe-Only Datenkollation</h3><p>Durch die Verwendung eines Output-Only <a href=\"https://huggingface.co/docs/transformers/en/main_classes/data_collator?ref=jina-ai-gmbh.ghost.io\">Data Collators</a> stellten wir sicher, dass sich das Modell w√§hrend des Trainings auf die Vorhersage der Ziel-Tokens konzentrierte, anstatt nur von der Eingabe zu kopieren. Der Output-Only Collator gew√§hrleistete, dass das Modell aus den tats√§chlichen Zielsequenzen lernte und die korrekten Vervollst√§ndigungen oder Grenzen betonte. Diese Unterscheidung erm√∂glichte es dem Modell, schneller zu konvergieren, indem Overfitting auf die Eingabe vermieden wurde, und half ihm, besser √ºber verschiedene Datens√§tze hinweg zu generalisieren.</p><h3 id=\"efficient-training-with-unsloth\">Effizientes Training mit Unsloth</h3><p>Mit <a href=\"https://github.com/unslothai/unsloth?ref=jina-ai-gmbh.ghost.io\">Unsloth</a> optimierten wir das Training unseres kleinen Sprachmodells und konnten es auf einer Nvidia 4090 GPU ausf√ºhren. Diese optimierte Pipeline erm√∂glichte es uns, ein effizientes, leistungsf√§higes Modell ohne massive Rechenressourcen zu trainieren.</p><h3 id=\"handling-complex-texts\">Umgang mit komplexen Texten</h3><p>Die Segmentierungsmodelle √ºberzeugten bei der Verarbeitung komplexer Dokumente mit Code, Tabellen und Listen, die f√ºr traditionellere Methoden typischerweise schwierig sind. F√ºr technische Inhalte waren ausgekl√ºgelte Strategien wie <code>topic-qwen-0.5</code> und <code>summary-qwen-0.5</code> effektiver, mit dem Potenzial, nachgelagerte RAG-Aufgaben zu verbessern.</p><h3 id=\"simple-methods-for-simpler-content\">Einfache Methoden f√ºr einfachere Inhalte</h3><p>F√ºr geradlinige, narrativ getriebene Inhalte sind einfachere Methoden wie die Segmenter API oft ausreichend. Fortgeschrittene Segmentierungsstrategien sind m√∂glicherweise nur f√ºr komplexere, strukturierte Inhalte erforderlich, was je nach Anwendungsfall Flexibilit√§t erm√∂glicht.</p><h2 id=\"next-steps\">N√§chste Schritte</h2><p>W√§hrend dieses Experiment in erster Linie als Machbarkeitsnachweis konzipiert war, k√∂nnten wir bei einer Erweiterung mehrere Verbesserungen vornehmen. Erstens, obwohl eine Fortsetzung dieses spezifischen Experiments unwahrscheinlich ist, w√ºrde das Training von <code>summary-qwen-0.5</code> auf einem gr√∂√üeren Datensatz ‚Äì idealerweise 60.000 statt 30.000 Samples ‚Äì wahrscheinlich zu einer optimalen Leistung f√ºhren. Zus√§tzlich w√§re eine Verfeinerung unseres Benchmarking-Prozesses vorteilhaft. Anstatt die vom LLM generierten Antworten aus dem RAG-System zu evaluieren, w√ºrden wir uns stattdessen darauf konzentrieren, die abgerufenen Segmente direkt mit der Ground Truth zu vergleichen. Schlie√ülich w√ºrden wir √ºber ROUGE-Scores hinausgehen und fortgeschrittenere Metriken (m√∂glicherweise eine Kombination aus ROUGE und LLM-Scoring) einsetzen, die die Nuancen der Abruf- und Segmentierungsqualit√§t besser erfassen.</p><h2 id=\"conclusion\">Fazit</h2><p>In diesem Experiment untersuchten wir, wie kundenspezifische Segmentierungsmodelle, die f√ºr bestimmte Aufgaben entwickelt wurden, die Leistung von RAG verbessern k√∂nnen. Durch die Entwicklung und das Training von Modellen wie <code>simple-qwen-0.5</code>, <code>topic-qwen-0.5</code> und <code>summary-qwen-0.5</code> haben wir wichtige Herausforderungen traditioneller Segmentierungsmethoden angegangen, insbesondere bei der Aufrechterhaltung semantischer Koh√§renz und der effektiven Verarbeitung komplexer Inhalte wie Code-Snippets. Unter den getesteten Modellen lieferte <code>topic-qwen-0.5</code> durchg√§ngig die sinnvollste und kontextuell relevanteste Segmentierung, besonders bei Dokumenten mit mehreren Themen.</p><p>W√§hrend Segmentierungsmodelle die strukturelle Grundlage f√ºr RAG-Systeme bieten, erf√ºllen sie eine andere Funktion als Late Chunking, das die Abrufleistung durch Aufrechterhaltung der kontextuellen Relevanz √ºber Segmente hinweg optimiert. Diese beiden Ans√§tze k√∂nnen sich erg√§nzen, aber Segmentierung ist besonders wichtig, wenn eine Methode ben√∂tigt wird, die sich auf die Aufteilung von Dokumenten f√ºr koh√§rente, aufgabenspezifische Generierungsworkflows konzentriert.</p>",
  "comment_id": "67126986708dbe00019249f2",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/breakpoints.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-18T15:58:30.000+02:00",
  "updated_at": "2024-10-25T20:14:53.000+02:00",
  "published_at": "2024-10-25T10:35:07.000+02:00",
  "custom_excerpt": "We trained three small language models to better segment long documents into chunks, and here are the key lessons we learned.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/finding-optimal-breakpoints-in-long-documents-using-small-language-models/",
  "excerpt": "Wir haben drei kleine Language Models trainiert, um lange Dokumente besser in Teilst√ºcke zu segmentieren, und hier sind die wichtigsten Erkenntnisse, die wir dabei gewonnen haben.",
  "reading_time": 19,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast.",
  "feature_image_caption": null
}
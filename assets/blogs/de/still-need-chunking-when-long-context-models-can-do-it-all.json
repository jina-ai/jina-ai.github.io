{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "Braucht man immer noch Chunking, wenn Long-Context-Modelle alles können?",
  "html": "<p>Im Oktober 2023 stellten wir <code>jina-embeddings-v2</code> vor, die erste Open-Source-Embedding-Modellfamilie, die Eingaben bis zu 8.192 Token verarbeiten kann. Darauf aufbauend haben wir dieses Jahr <code>jina-embeddings-v3</code> eingeführt, das die gleiche umfangreiche Eingabeunterstützung mit weiteren Verbesserungen bietet.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>In diesem Beitrag werden wir uns mit Long-Context-Embeddings befassen und einige Fragen beantworten: Wann ist es praktikabel, ein so großes Textvolumen in einen einzigen Vektor zu konsolidieren? Verbessert Segmentierung das Retrieval, und wenn ja, wie? Wie können wir den Kontext aus verschiedenen Teilen eines Dokuments beim Segmentieren des Textes bewahren?</p><p>Um diese Fragen zu beantworten, werden wir verschiedene Methoden zur Generierung von Embeddings vergleichen:</p><ul><li>Long-Context-Embedding (Kodierung von bis zu 8.192 Token in einem Dokument) vs. Short-Context (d.h. Abschneiden bei 192 Token).</li><li>Kein Chunking vs. naives Chunking vs. <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">Late Chunking</a>.</li><li>Verschiedene Chunk-Größen sowohl beim naiven als auch beim Late Chunking.</li></ul><h2 id=\"is-long-context-even-useful\">Ist Long Context überhaupt nützlich?</h2><p>Mit der Möglichkeit, bis zu zehn Seiten Text in einem einzigen Embedding zu kodieren, eröffnen Long-Context-Embedding-Modelle neue Möglichkeiten für die Darstellung großer Textmengen. Ist das aber überhaupt nützlich? Laut vielen Leuten... nein.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Quellen: </span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Zitat von Nils Reimer im How AI Is Built Podcast</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">brainlag Tweet</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">egorfine Hacker News Kommentar</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">andy99 Hacker News Kommentar</span></a></p></figcaption></figure><p>Wir werden all diese Bedenken mit einer detaillierten Untersuchung der Long-Context-Fähigkeiten, wann Long Context hilfreich ist und wann man ihn (nicht) verwenden sollte, ansprechen. Aber zunächst wollen wir uns diese Skeptiker anhören und einige der Probleme betrachten, mit denen Long-Context-Embedding-Modelle konfrontiert sind.</p><h2 id=\"problems-with-long-context-embeddings\">Probleme mit Long-Context-Embeddings</h2><p>Stellen wir uns vor, wir bauen ein Dokumentensuchsystem für Artikel, wie die in unserem <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">Jina AI Blog</a>. Manchmal kann ein einzelner Artikel mehrere Themen abdecken, wie der <a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">Bericht über unseren Besuch der ICML 2024 Konferenz</a>, der enthält:</p><ul><li>Eine Einführung mit allgemeinen Informationen über ICML (Teilnehmerzahl, Ort, Umfang, etc).</li><li>Die Präsentation unserer Arbeit (<code>jina-clip-v1</code>).</li><li>Zusammenfassungen anderer interessanter Forschungsarbeiten, die auf der ICML vorgestellt wurden.</li></ul><p>Wenn wir nur ein einziges Embedding für diesen Artikel erstellen, repräsentiert dieses Embedding eine Mischung aus drei verschiedenen Themen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 1: Beim Embedding eines Dokuments, das mehrere Themen behandelt, stellt der resultierende Vektor eine Mischung aller Absätze dar, wodurch möglicherweise die spezifischen Informationen der einzelnen Absätze verloren gehen.</span></figcaption></figure><p>Dies führt zu mehreren Problemen:</p><ul><li><strong>Verwässerung der Repräsentation:</strong> Während alle Themen in einem Text miteinander verwandt sein <em>könnten</em>, ist möglicherweise nur eines für die Suchanfrage des Nutzers relevant. Ein einzelnes Embedding (in diesem Fall das des gesamten Blogbeitrags) ist jedoch nur ein Punkt im Vektorraum. Je mehr Text zur Modelleingabe hinzugefügt wird, desto mehr verschiebt sich das Embedding, um das übergeordnete Thema des Artikels zu erfassen, wodurch es weniger effektiv wird, den in bestimmten Absätzen behandelten Inhalt zu repräsentieren.</li><li><strong>Begrenzte Kapazität:</strong> Embedding-Modelle erzeugen Vektoren fester Größe, unabhängig von der Eingabelänge. Je mehr Inhalt zur Eingabe hinzugefügt wird, desto schwieriger wird es für das Modell, all diese Informationen im Vektor darzustellen. Stellen Sie sich das vor wie die Skalierung eines Bildes auf 16×16 Pixel — Wenn Sie ein Bild von etwas Einfachem wie einem Apfel skalieren, können Sie aus dem skalierten Bild immer noch Bedeutung ableiten. Bei einer Straßenkarte von Berlin? Eher nicht.</li><li><strong>Informationsverlust:</strong> In manchen Fällen stoßen selbst Long-Context-Embedding-Modelle an ihre Grenzen; Viele Modelle unterstützen Textkodierung mit bis zu 8.192 Token. Längere Dokumente müssen vor dem Embedding gekürzt werden, was zu Informationsverlust führt. Wenn sich die für den Benutzer relevante Information am Ende des Dokuments befindet, wird sie vom Embedding überhaupt nicht erfasst.</li><li><strong>Textsegmentierung kann <em>notwendig</em> sein:</strong> Einige Anwendungen erfordern Embeddings für bestimmte Textsegmente, aber nicht für das gesamte Dokument, wie zum Beispiel das Identifizieren der relevanten Passage in einem Text.</li></ul><h2 id=\"long-context-vs-truncation\">Long Context vs. Abschneiden</h2><p>Um zu sehen, ob Long Context überhaupt sinnvoll ist, betrachten wir die Leistung von zwei Retrieval-Szenarien:</p><ul><li>Kodierung von Dokumenten bis zu 8.192 Token (etwa 10 Textseiten).</li><li>Abschneiden von Dokumenten bei 192 Token und Kodierung bis dahin.</li></ul><p>Wir werden die Ergebnisse vergleichen mit<code>jina-embeddings-v3</code> mit der nDCG@10 Retrieval-Metrik. Wir haben folgende Datensätze getestet:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Beschreibung</th>\n<th>Beispiel-Query</th>\n<th>Beispiel-Dokument</th>\n<th>Durchschnittliche Dokumentlänge (Zeichen)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>Ein medizinischer Volltext-Retrieval-Datensatz mit 3.244 Queries und Dokumenten hauptsächlich aus PubMed.</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326.753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>Ein query-basierter Meeting-Zusammenfassungs-Datensatz, der die Zusammenfassung relevanter Meeting-Segmente erfordert.</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37.445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>QA-Datensatz mit langen Geschichten und entsprechenden Fragen zu spezifischen Inhalten.</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53.336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>Ein Multi-Hop-QA-Datensatz mit bis zu 5 Reasoning-Schritten, entwickelt mit Templates zur Vermeidung von Shortcuts.</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30.854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>Ein Drehbuch-Zusammenfassungs-Datensatz mit TV-Serien-Transkripten und Zusammenfassungen, die eine verteilte Plot-Integration erfordern.</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1.613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Wie wir sehen können, bringt die Kodierung von mehr als 192 Tokens bemerkenswerte Leistungsverbesserungen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 2: Vergleich der Long-Context Embedding Performance und Short Text Embedding Performance</span></figcaption></figure><p>Allerdings sehen wir bei einigen Datensätzen größere Verbesserungen als bei anderen:</p><ul><li>Bei <strong>NFCorpus</strong> macht die Kürzung kaum einen Unterschied. Das liegt daran, dass sich die Titel und Abstracts direkt am Anfang der Dokumente befinden und diese hochrelevant für typische Benutzer-Suchbegriffe sind. Ob gekürzt oder nicht, die relevantesten Daten bleiben innerhalb des Token-Limits.</li><li><strong>QMSum</strong> und <strong>NarrativeQA</strong> gelten als \"Leseverständnis\"-Aufgaben, bei denen Benutzer typischerweise nach spezifischen Fakten innerhalb eines Textes suchen. Diese Fakten sind oft in Details verstreut, die über das Dokument verteilt sind und möglicherweise außerhalb der gekürzten 192-Token-Grenze liegen. Zum Beispiel wird im NarrativeQA-Dokument <em>Percival Keene</em> die Frage \"Who is the bully that steals Percival's lunch?\" erst weit nach dieser Grenze beantwortet. Ähnlich verhält es sich bei <strong>2WikiMultiHopQA</strong>, wo relevante Informationen über ganze Dokumente verteilt sind und Modelle mehrere Abschnitte navigieren und synthetisieren müssen, um Anfragen effektiv zu beantworten.</li><li><strong>SummScreenFD</strong> ist eine Aufgabe, die darauf abzielt, das Drehbuch zu identifizieren, das zu einer bestimmten Zusammenfassung gehört. Da die Zusammenfassung Informationen enthält, die über das Drehbuch verteilt sind, verbessert die Kodierung von mehr Text die Genauigkeit beim Zuordnen der Zusammenfassung zum richtigen Drehbuch.</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">Text-Segmentierung für bessere Retrieval-Leistung</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Im Folgenden diskutieren wir drei ähnliche Konzepte. Um Verwechslungen zu vermeiden, bezeichnen wir sie wie folgt:<br>• <b><strong style=\"white-space: pre-wrap;\">Segmentierung</strong></b>: Erkennen von Grenzsignalen in einem Eingabetext, zum Beispiel Sätze oder eine feste Anzahl von Tokens.<br>• <b><strong style=\"white-space: pre-wrap;\">Naives Chunking</strong></b>: Aufteilen des Textes in Chunks basierend auf Segmentierungssignalen vor der Kodierung.<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Late Chunking</strong></b></a>: Zuerst das Dokument kodieren und dann segmentieren (Kontext zwischen Chunks bleibt erhalten).</div></div><p>Anstatt ein ganzes Dokument in einen Vektor einzubetten, können wir verschiedene Methoden verwenden, um das Dokument zunächst durch Zuweisen von Grenzsignalen zu segmentieren:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 3: Anwendung der Methoden \"Fixed-Size\", \"Sentence-Based\" und \"Semantic\" Chunking auf eine Textpassage</span></figcaption></figure><p>Einige gängige Methoden sind:</p><ul><li><strong>Segmentierung nach fester Größe:</strong> Das Dokument wird in Segmente mit einer festen Anzahl von Tokens unterteilt, die durch den Tokenizer des Embedding-Modells bestimmt wird. Dies stellt sicher, dass die Tokenisierung der Segmente der Tokenisierung des gesamten Dokuments entspricht (eine Segmentierung nach einer bestimmten Anzahl von Zeichen könnte zu einer anderen Tokenisierung führen).</li><li><strong>Segmentierung nach Sätzen:</strong> Das Dokument wird in Sätze segmentiert, und jeder Chunk besteht aus <em>n</em> Sätzen.</li><li><strong>Semantische Segmentierung:</strong> Jedes Segment entspricht mehreren Sätzen, und ein Embedding-Modell bestimmt die Ähnlichkeit aufeinanderfolgender Sätze. Sätze mit hoher Embedding-Ähnlichkeit werden demselben Chunk zugewiesen.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Sie können die Segmentierung einfach mit <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a> durchführen, unserer kostenlosen API zur Segmentierung langer Texte in Chunks und Tokenisierung basierend auf der Struktur des Dokuments.</div></div><p>Der Einfachheit halber verwenden wir in diesem Artikel die Segmentierung nach fester Größe.</p><h3 id=\"document-retrieval-using-naive-chunking\">Dokument-Retrieval mit naivem Chunking</h3><p>Sobald wir die Segmentierung nach fester Größe durchgeführt haben, können wir das Dokument naiv entsprechend dieser Segmente chunken:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 4: Naives Chunking basierend auf während der Segmentierung erkannten Grenzsignalen.</span></figcaption></figure><p>Mit <code>jina-embeddings-v3</code> kodieren wir jeden Chunk in ein Embedding, das seine Semantik genau erfasst, und speichern diese Embeddings dann in einer Vektordatenbank.</p><p>Zur Laufzeit kodiert das Modell die Anfrage eines Benutzers in einen Query-Vektor. Wir vergleichen diesen mit unserer Vektordatenbank von Chunk-Embeddings, um den Chunk mit der höchsten Kosinus-Ähnlichkeit zu finden, und geben dann das entsprechende Dokument an den Benutzer zurück:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 5: Dokumenten-Retrieval mit naivem Chunking implementiert: (1) Die Dokumente in der Sammlung werden basierend auf Grenzhinweisen in Chunks aufgeteilt, (2) das Embedding-Modell codiert alle Chunks und wir speichern die resultierenden Embeddings in einer Datenbank, (3) wenn eine Anfrage eingeht, codiert das Embedding-Modell diese und die Datenbank ermittelt den ähnlichsten Chunk. Am Ende identifizieren wir das relevante Dokument anhand der für den Chunk in der Datenbank gespeicherten Dokument-ID und geben es an den Benutzer zurück.</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">Probleme mit naivem Chunking</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 6: Bei der Aufteilung eines Textes in Sätze können Bezüge zu früheren Textteilen nicht aufgelöst werden.</span></figcaption></figure><p>Während naives Chunking einige der Einschränkungen von Long-Context Embedding-Modellen adressiert, hat es auch seine Nachteile:</p><ul><li><strong>Das große Ganze wird übersehen:</strong> Bei der Dokumentensuche können mehrere Embeddings kleinerer Chunks möglicherweise das übergreifende Thema des Dokuments nicht erfassen. Man sieht sozusagen den Wald vor lauter Bäumen nicht.</li><li><strong>Fehlendes Kontext-Problem:</strong> Chunks können nicht akkurat interpretiert werden, da Kontextinformationen fehlen, wie in Abbildung 6 dargestellt.</li><li><strong>Effizienz:</strong> Mehr Chunks benötigen mehr Speicherplatz und erhöhen die Abrufzeit.</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">Late Chunking löst das Kontext-Problem</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Um das Problem des fehlenden Kontexts zu lösen, haben wir eine neue Methode namens „Late Chunking\" eingeführt, die in unseren vorherigen Blog-Posts beschrieben wird: <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">Teil I</strong></b></a>, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">Teil II</strong></b></a>, <a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">Teil III</strong></b></a>, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">Forschungspapier</strong></b></a>.</div></div><p>Late Chunking funktioniert in zwei Hauptschritten:</p><ol><li>Zunächst nutzt es die Long-Context-Fähigkeiten des Modells, um das gesamte Dokument in Token-Embeddings zu codieren. Dies bewahrt den vollständigen Kontext des Dokuments.</li><li>Dann erstellt es Chunk-Embeddings durch Anwendung von Mean Pooling auf bestimmte Sequenzen von Token-Embeddings, die den während der Segmentierung identifizierten Grenzmarkierungen entsprechen.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 7: Late vs naives Chunking.</span></figcaption></figure><p>Der Hauptvorteil dieses Ansatzes ist, dass die Token-Embeddings kontextualisiert sind - das bedeutet, sie erfassen natürlich Bezüge und Beziehungen zu anderen Teilen des Dokuments. Da der Embedding-Prozess vor dem Chunking stattfindet, behält jeder Chunk das Bewusstsein für den breiteren Dokumentenkontext bei und löst damit das Problem des fehlenden Kontexts, das naive Chunking-Ansätze plagt.</p><p>Für Dokumente, die die maximale Eingabegröße des Modells überschreiten, können wir „Long Late Chunking\" verwenden:</p><ol><li>Zunächst teilen wir das Dokument in überlappende „Macro-Chunks\" auf. Jeder Macro-Chunk ist so dimensioniert, dass er in die maximale Kontextlänge des Modells passt (zum Beispiel 8.192 Token).</li><li>Das Modell verarbeitet diese Macro-Chunks, um Token-Embeddings zu erstellen.</li><li>Sobald wir die Token-Embeddings haben, fahren wir mit dem Standard-Late-Chunking fort - wir wenden Mean Pooling an, um die finalen Chunk-Embeddings zu erstellen.</li></ol><p>Dieser Ansatz ermöglicht es uns, Dokumente beliebiger Länge zu verarbeiten und dabei die Vorteile des Late Chunking zu bewahren. Man kann es sich als zweistufigen Prozess vorstellen: Zunächst wird das Dokument für das Modell verdaulich gemacht, dann wird das reguläre Late-Chunking-Verfahren angewendet.</p><p>Kurz gesagt:</p><ul><li><strong>Naives Chunking:</strong> Das Dokument in kleine Chunks segmentieren, dann jeden Chunk separat codieren.</li><li><strong>Late Chunking:</strong> Das gesamte Dokument auf einmal codieren, um Token-Embeddings zu erstellen, dann Chunk-Embeddings durch Pooling der Token-Embeddings basierend auf Segmentgrenzen erstellen.</li><li><strong>Long Late Chunking:</strong> Große Dokumente in überlappende Macro-Chunks aufteilen, die in das Kontextfenster des Modells passen, diese codieren, um Token-Embeddings zu erhalten, dann Late Chunking wie gewohnt anwenden.</li></ul><p>Für eine ausführlichere Beschreibung der Idee werfen Sie einen Blick in unser <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Paper</a> oder die oben erwähnten Blog-Posts.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">Chunking oder kein Chunking?</h2><p>Wir haben bereits gesehen, dass Long-Context Embedding im Allgemeinen bessere Leistung als kürzere Text-Embeddings erbringt, und einen Überblick über naive und Late-Chunking-Strategien gegeben. Die Frage ist nun: Ist Chunking besser als Long-Context Embedding?</p><p>Um einen fairen Vergleich durchzuführen, kürzen wir Textwerte auf die maximale Sequenzlänge des Modells (8.192 Token), bevor wir mit der Segmentierung beginnen. Wir verwenden eine Segmentierung fester Größe mit 64 Token pro Segment (sowohl für naive Segmentierung als auch für Late Chunking). Vergleichen wir drei Szenarien:</p><ul><li><strong>Keine Segmentierung:</strong> Wir codieren jeden Text in ein einzelnes Embedding. Dies führt zu den gleichen Ergebnissen wie im vorherigen Experiment (siehe Abbildung 2), aber wir nehmen sie hier zum besseren Vergleich auf.</li><li><strong>Naives Chunking:</strong> Wir segmentieren die Texte und wenden dann naives Chunking basierend auf den Grenzmarkierungen an.</li><li><strong>Late Chunking:</strong> Wir segmentieren die Texte und verwenden dann Late Chunking zur Bestimmung der Embeddings.</li></ul><p>Sowohl bei Late Chunking als auch bei naiver Segmentierung verwenden wir Chunk-Retrieval zur Bestimmung des relevanten Dokuments (wie in Abbildung 5 weiter oben in diesem Beitrag gezeigt).</p><p>Die Ergebnisse zeigen keinen klaren Gewinner:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Abbildung 8: Kein Chunking vs naives Chunking vs Late Chunking</span></figcaption></figure><ul><li><strong>Für Faktensuche funktioniert naives Chunking besser:</strong> Für die Datensätze QMSum, NarrativeQA und 2WikiMultiHopQA muss das Modell relevante Passagen im Dokument identifizieren. Hier ist naives Chunking deutlich besser als die Codierung von allem in ein einzelnes Embedding, da wahrscheinlich nur wenige Chunks relevante Informationen enthalten und diese Chunks sie viel besser erfassen als ein einzelnes Embedding des gesamten Dokuments.</li><li><strong>Late Chunking funktioniert am besten bei kohärenten Dokumenten mit relevantem Kontext:</strong> Bei Dokumenten, die ein zusammenhängendes Thema behandeln und bei denen Benutzer eher nach übergeordneten Themen als nach spezifischen Fakten suchen (wie bei NFCorpus), schneidet Late Chunking etwas besser ab als kein Chunking, da es dokumentweiten Kontext mit lokalen Details ausbalanciert. Während Late Chunking im Allgemeinen durch die Kontexterhaltung besser funktioniert als naives Chunking, kann dieser Vorteil zum Nachteil werden, wenn nach isolierten Fakten in Dokumenten mit überwiegend irrelevanten Informationen gesucht wird - wie die Leistungsrückgänge bei NarrativeQA und 2WikiMultiHopQA zeigen, wo der zusätzliche Kontext eher ablenkt als hilft.</li></ul><h3 id=\"does-chunk-size-make-a-difference\">Macht die Chunk-Größe einen Unterschied?</h3><p>Die Effektivität von Chunking-Methoden hängt stark vom Dataset ab, was zeigt, wie wichtig die Inhaltsstruktur ist:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Abbildung 9: Vergleich von Chunk-Größen bei naivem und spätem Chunking.</span></figcaption></figure><p>Wie wir sehen können, übertrifft Late Chunking bei kleineren Chunk-Größen generell das naive Chunking, da kleinere naive Chunks zu wenig Kontext enthalten, während kleinere Late Chunks den Kontext des gesamten Dokuments bewahren und dadurch semantisch aussagekräftiger sind. Die Ausnahme bildet das NarrativeQA Dataset, bei dem es einfach so viel irrelevanten Kontext gibt, dass Late Chunking zurückfällt. Bei größeren Chunk-Größen zeigt naives Chunking deutliche Verbesserungen (und übertrifft gelegentlich Late Chunking) aufgrund des erhöhten Kontexts, während die Leistung von Late Chunking allmählich abnimmt.</p><h2 id=\"takeaways-when-to-use-what\">Erkenntnisse: Wann verwendet man was?</h2><p>In diesem Beitrag haben wir verschiedene Arten von Dokumenten-Retrieval-Aufgaben untersucht, um besser zu verstehen, wann Segmentierung und wann Late Chunking hilft. Was haben wir also gelernt?</p><h3 id=\"when-should-i-use-long-context-embedding\">Wann sollte ich Long-Context Embedding verwenden?</h3><p>Im Allgemeinen schadet es der Retrieval-Genauigkeit nicht, so viel Text wie möglich aus Ihren Dokumenten als Input für Ihr Embedding-Modell zu verwenden. Allerdings konzentrieren sich Long-Context Embedding-Modelle oft auf den Anfang von Dokumenten, da diese Inhalte wie Titel und Einleitung enthalten, die für die Beurteilung der Relevanz wichtiger sind, aber die Modelle könnten Inhalte in der Mitte des Dokuments übersehen.</p><h3 id=\"when-should-i-use-naive-chunking\">Wann sollte ich naives Chunking verwenden?</h3><p>Wenn Dokumente mehrere Aspekte abdecken oder Benutzeranfragen auf spezifische Informationen innerhalb eines Dokuments abzielen, verbessert Chunking im Allgemeinen die Retrieval-Leistung.</p><p>Letztendlich hängen Segmentierungsentscheidungen von Faktoren ab wie der Notwendigkeit, Teiltext für Benutzer anzuzeigen (z.B. wie Google die relevanten Passagen in den Vorschauen der Suchergebnisse präsentiert), was Segmentierung unerlässlich macht, oder Einschränkungen bei Rechenleistung und Speicher, wo Segmentierung aufgrund erhöhten Retrieval-Overheads und Ressourcenverbrauchs weniger vorteilhaft sein kann.</p><h3 id=\"when-should-i-use-late-chunking\">Wann sollte ich Late Chunking verwenden?</h3><p>Durch das Encodieren des vollständigen Dokuments vor der Erstellung von Chunks löst Late Chunking das Problem, dass Textsegmente durch fehlenden Kontext ihre Bedeutung verlieren. Dies funktioniert besonders gut bei kohärenten Dokumenten, bei denen jeder Teil mit dem Ganzen zusammenhängt. Unsere Experimente zeigen, dass Late Chunking besonders effektiv ist, wenn Text in kleinere Chunks unterteilt wird, wie in unserem <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Paper</a> demonstriert. Es gibt jedoch einen Vorbehalt: Wenn Teile des Dokuments nicht miteinander zusammenhängen, kann das Einbeziehen dieses breiteren Kontexts die Retrieval-Leistung tatsächlich verschlechtern, da es Rauschen zu den Embeddings hinzufügt.</p><h2 id=\"conclusion\">Fazit</h2><p>Die Wahl zwischen Long-Context Embedding, naivem Chunking und Late Chunking hängt von den spezifischen Anforderungen Ihrer Retrieval-Aufgabe ab. Long-Context Embeddings sind wertvoll für kohärente Dokumente mit allgemeinen Anfragen, während Chunking in Fällen excellt, in denen Benutzer nach spezifischen Fakten oder Informationen innerhalb eines Dokuments suchen. Late Chunking verbessert das Retrieval weiter, indem es die kontextuelle Kohärenz innerhalb kleinerer Segmente beibehält. Letztendlich wird das Verständnis Ihrer Daten und Retrieval-Ziele den optimalen Ansatz bestimmen, der Genauigkeit, Effizienz und kontextuelle Relevanz ausbalanciert.</p><p>Wenn Sie diese Strategien erkunden, erwägen Sie <code>jina-embeddings-v3</code> auszuprobieren—seine fortgeschrittenen Long-Context-Fähigkeiten, Late Chunking und Flexibilität machen es zu einer ausgezeichneten Wahl für verschiedene Retrieval-Szenarien.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 ist ein Frontier multilinguales Text-Embedding-Modell mit 570M Parametern und 8192 Token-Länge, das die neuesten proprietären Embeddings von OpenAI und Cohere bei MTEB übertrifft.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "Vergleich der Leistung von Long-Context-Embedding-Modellen mit verschiedenen Chunking-Strategien, um den optimalen Ansatz für Ihre Anforderungen zu finden.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "quantization-aware-training-of-jina-embeddings-v4",
  "id": "685d4b76f1bef30001fc5449",
  "uuid": "6b06b483-2d13-4f1d-8d9d-147fa6dffe4b",
  "title": "Quantization-Aware Training di jina-embeddings-v4",
  "html": "<p>La quantizzazione è ampiamente utilizzata per affrontare i problemi di scalabilità nell'IA. Il nome la fa sembrare complicata, ma si tratta semplicemente di arrotondare i numeri per far sì che occupino meno spazio. Ciò significa vettori di embedding più piccoli che occupano meno memoria e spazio di archiviazione e un recupero delle informazioni più veloce perché ci vuole meno tempo per confrontare i vettori. La quantizzazione è una tecnica puramente numerica che non si preoccupa del tipo di dati elaborati dal modello o dei casi d'uso, quindi può apportare miglioramenti senza richiedere molta costosa conoscenza del dominio.</p><p>Ci si potrebbe aspettare che la quantizzazione comporti dei buoni vecchi compromessi e che nulla sia gratis, dove dobbiamo sacrificare un po' di precisione. In questo articolo, ti mostreremo un modo per <strong>renderla senza perdite</strong> tramite il <em>training consapevole della quantizzazione</em> (quantization-aware training, QAT). Questa tecnica è utilizzata in <code>jina-embeddings-v4</code> per fornire embedding più piccoli richiesti in applicazioni in cui lo spazio è fondamentale.</p><h2 id=\"overview-of-quantization-techniques\">Panoramica delle tecniche di quantizzazione</h2><p>La quantizzazione del modello di solito significa una di queste quattro cose:</p><ul><li>Quantizzazione post-training (Post-training quantization, <strong>PTQ</strong>)</li><li>Training per output di embedding quantizzati (Output QAT, <strong>Output QAT</strong>)</li><li>Training per modelli completamente quantizzati (Full QAT, <strong>Full QAT</strong>)</li><li>Distillare un nuovo modello quantizzato da uno esistente non quantizzato</li></ul><p>La quantizzazione post-training (<strong>PTQ</strong>) accetta il modello di embedding addestrato così com'è e non lo modifica in alcun modo. Si tratta solo di scartare le cifre meno significative dei valori in virgola mobile prodotti dal modello. Arrotondiamo semplicemente i numeri e a volte li scaliamo a un intervallo.</p><p><strong>Output QAT</strong> significa ottimizzare il modello di embedding per produrre vettori a precisione ridotta ottimali. Ciò significa modificare il modello, ma non cambia la precisione dei pesi del modello e quindi non ne riduce le dimensioni. Viene ridotta solo la dimensione del vettore di output.</p><p><strong>Full QAT</strong> inizia con un modello a precisione completa completamente addestrato e abbassa la precisione dei pesi del modello, quindi ottimizza le prestazioni di questo modello modificato. Ciò produce un modello significativamente più piccolo e embedding più piccoli, al prezzo di una messa a punto.</p><p>La <strong>distillazione</strong> è il processo di addestramento di un nuovo modello per corrispondere alle prestazioni di uno esistente. Ciò significa creare un nuovo modello progettato da zero come quantizzato e quindi utilizzare il modello esistente per generare tutti i dati di addestramento necessari per addestrarlo fino a quando non si comporta il più fedelmente possibile al modello esistente.</p><p>I vantaggi di questi quattro approcci sono riassunti nella tabella seguente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Approccio</th>\n<th>Embedding più compatti?</th>\n<th>Richiede training?</th>\n<th>Compressione del modello?</th>\n<th>Inferenza più veloce?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>PTQ</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Output QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Full QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td><strong>Distillazione</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><em>(a un modello più piccolo)</em></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Tutti e quattro producono embedding più compatti, ma a parte PTQ, tutti richiedono un ulteriore training, mentre solo Full QAT e Distillazione producono modelli nuovi e più veloci. Full QAT e Distillazione sono molto più costosi da implementare perché richiedono molto più training rispetto a Output QAT.</p><p>In questo articolo, esamineremo solo PTQ e Output QAT, che non modificano le dimensioni o la velocità del modello di embedding.</p><h2 id=\"experimental-setup\">Configurazione sperimentale</h2><p>Per questi esperimenti, il nostro modello di base è <code>jina-embeddings-v4</code> con l'adattatore di recupero, che produce vettori in virgola mobile a 32 bit di precisione (FP32) in 2048 dimensioni. Ogni embedding ha quindi una dimensione di 8196 byte o 8 kB.</p><p>Abbiamo studiato diverse condizioni sperimentali utilizzando attività di benchmark di recupero di query-documenti dalla suite <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR benchmark</a>. Il processo di recupero utilizza la similarità del coseno tra i vettori per trovare e classificare i documenti che meglio corrispondono alle query.</p><ul><li><strong>Baseline</strong>: le prestazioni dei vettori di embedding <code>jina-embeddings-v4</code> senza alcuna quantizzazione. Tutti questi esperimenti hanno utilizzato una versione beta del modello e le prestazioni di rilascio sono leggermente migliori.</li><li><strong>PTQ</strong>: abbiamo quantizzato i vettori di output in vettori binari senza modificare il modello.</li><li><strong>Output QAT</strong>: abbiamo quantizzato i vettori di output e applicato l'ottimizzazione all'adattatore di recupero per migliorarne le prestazioni in condizioni quantizzate.</li></ul><h3 id=\"quantization-levels\">Livelli di quantizzazione</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"816\" height=\"636\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png 816w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Confronto delle dimensioni dell'embedding post-quantizzazione.</span></figcaption></figure><p>Abbiamo sperimentato con quattro diversi livelli di quantizzazione.</p><ul><li><strong>Interi a 8 bit</strong>: i valori FP32 vengono ridotti a interi nell'intervallo da -128 a 127, riducendo gli embedding di 4 volte a <strong>2048 byte</strong>.</li><li><strong>Interi a 4 bit</strong>: come per gli interi a 4 bit, ma mappiamo all'intervallo da -8 a 7, riducendo le dimensioni del vettore di un fattore 8, a <strong>1024 byte</strong>.</li><li><strong>Quantizzazione ternaria</strong>: tutti i valori vengono mappati a uno dei tre valori: -1, 0, 1. Memorizzato in modo ottimale, questo riduce ogni dimensione a 1,6 bit, riducendo le dimensioni dei vettori di embedding di circa 40 volte a circa <strong>230 byte</strong>.</li><li><strong>Quantizzazione binaria</strong>: convertiamo i valori scalari FP32 in un bit, utilizzando il tipo di dati <code>torch.sign</code>, che prevede solo due valori, impiegando un bit per l'archiviazione. Ciò riduce i vettori di embedding a 2048 dimensioni da 8192 byte a <strong>128 byte</strong>, una riduzione di 64 volte.</li></ul><h3 id=\"scaling\">Scalabilità</h3><p>Per la quantizzazione binaria, la quantizzazione è molto semplice: se un valore vettoriale è superiore a 0 o positivo, viene mappato a 1. Altrimenti, viene mappato a -1.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1159\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png 1159w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Quantizzazione binaria. Tutti i valori negativi diventano -1, tutti gli altri 1.</span></figcaption></figure><p>Per gli altri scenari di quantizzazione, abbiamo normalizzato i valori a un intervallo e quindi arrotondato al valore più vicino consentito dal livello di quantizzazione. I vettori di embedding sono costituiti da numeri di scala compresi tra -∞ e +∞ (o, in pratica, numeri positivi e negativi molto grandi). Usiamo due numeri, $max$ e $min$, per scalare i valori per la quantizzazione.</p><p>Per la quantizzazione ternaria, prendiamo ogni componente vettoriale $v$ e lo traduciamo come segue:</p><ul><li>se $v$ ≥ $max$, $v$ diventa 1.</li><li>se $v$ ≤ $min$, $v$ diventa -1.</li><li>se $min$ &lt; $v$ &lt; $max$, $v$ diventa 0.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"220\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png 1030w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Quantizzazione ternaria. Viene definito un intervallo e i valori al suo interno diventano 0. Tutti i valori inferiori diventano -1 e tutti quelli superiori 1.</span></figcaption></figure><p>Per interi a 4 bit:</p><ul><li>se $v$ ≥ $max$, $v$ diventa 7.</li><li>se $v$ ≤ $min$, $v$ diventa -8.</li><li>se $min$ &lt; $v$ &lt; $max$, $v$ diventa $16*(v - min)/(max - min) - 8$, quindi arrotondato all'intero più vicino. Questo scala il valore all'intervallo $[-8,7]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Quantizzazione a 4 bit. Viene definito un intervallo e tutti i valori vengono normalizzati all'intervallo definito [-8,7].</span></figcaption></figure><p></p><p>Per interi a 8 bit:</p><ul><li>se $v$ ≥ $max$, $v$ diventa 127.</li><li>se $v$ ≤ $min$, $v$ diventa -128.</li><li>se $min$ &lt; $v$ &lt; $max$, $v$ diventa $256*(v - min)/(max - min) - 128$, arrotondato all'intero più vicino. Questo scala il valore all'intervallo $[-128,127]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"219\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Quantizzazione a 8 bit. Viene definito un intervallo e tutti i valori vengono normalizzati all'intervallo definito [-128,127].</span></figcaption></figure><p>Per calcolare $max$ e $min$, abbiamo utilizzato due approcci:</p><ul><li><strong>Min/Max</strong>: abbiamo elaborato i nostri dati in batch e, per ogni batch, abbiamo identificato il componente vettoriale più alto e più basso, impostando $max$ sul più alto e $min$ sul più basso.</li><li><strong>Media mobile sui batch</strong>: per ogni batch, abbiamo calcolato la media e la deviazione standard dei componenti vettoriali. Abbiamo mantenuto una media mobile sia della media che della deviazione standard durante l'elaborazione di tutti i batch. Se $avg$ è la media mobile corrente dei valori medi del batch e $std$ è la media mobile corrente delle deviazioni standard, allora per ogni batch:</li></ul><p>$max = avg + std$<br>$min = avg - std$</p><h3 id=\"qat-fine-tuning\">Ottimizzazione QAT</h3><p>Per gli esperimenti PTQ, abbiamo utilizzato il modello così com'è e abbiamo quantizzato gli embedding prodotti utilizzando i metodi descritti sopra.</p><p>Per l'Output QAT, abbiamo ottimizzato il modello utilizzando la <em>stima straight-through</em>. Ciò significa che invertiamo il processo di quantizzazione, ripristinando la precisione completa ai valori, prima di calcolare la perdita (cioè l'errore), e quindi utilizziamo tale metrica di perdita per ottimizzare il modello.</p><p>In ogni caso, abbiamo eseguito il fine-tuning per 10.000 passaggi, salvando un checkpoint ogni 500 passaggi. Abbiamo quindi conservato il checkpoint con il punteggio più alto sul benchmark <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR</a>.</p><h3 id=\"asymmetric-quantization\">Quantizzazione asimmetrica</h3><p>PTQ e Output QAT riducono le dimensioni dei vettori di embedding, ma non riducono le dimensioni del modello o la velocità di inferenza; tutti i risparmi sono nelle dimensioni degli 向量模型 (embedding) dei documenti memorizzati e nella velocità di recupero.</p><p>Di conseguenza, abbiamo testato sia la quantizzazione dei vettori di query sia il fatto di lasciarli non quantizzati al momento del recupero, perché in entrambi i casi non cambia la dimensione dei vettori di embedding memorizzati.</p><h2 id=\"results\">Risultati</h2><p>Abbiamo testato nove condizioni in totale, riassunte nelle tabelle seguenti:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Nome condizione</th>\n<th>Fine-Tuning</th>\n<th>Livello di quantizzazione</th>\n<th>Strategia di scaling</th>\n<th>Query quantizzate</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>❌</td>\n<td>n/a</td>\n<td>n/a</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Both</td>\n<td>❌</td>\n<td>Binario</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>PTQ Docs Only</td>\n<td>❌</td>\n<td>Binario</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td><strong>✓</strong></td>\n<td>Binario</td>\n<td>n/a</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td><strong>✓</strong></td>\n<td>Binario</td>\n<td>n/a</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td><strong>✓</strong></td>\n<td>Trinario</td>\n<td>Media mobile</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td><strong>✓</strong></td>\n<td>4-bit</td>\n<td>Media mobile</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td><strong>✓</strong></td>\n<td>8-bit</td>\n<td>Media mobile</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td><strong>✓</strong></td>\n<td>8-bit</td>\n<td>Min/Max</td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Tabella 2: Condizioni sperimentali</em></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Nome condizione</th>\n<th>Punteggio medio</th>\n<th>Differenza dalla baseline</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>60.10</td>\n<td>n/a</td>\n</tr>\n<tr>\n<td>PTQ Binary</td>\n<td>58.33</td>\n<td>-1.78</td>\n</tr>\n<tr>\n<td>PTQ Binary Docs Only</td>\n<td>59.08</td>\n<td>-1.02</td>\n</tr>\n<tr>\n<td>QAT Binary</td>\n<td>59.22</td>\n<td>-0.89</td>\n</tr>\n<tr>\n<td>QAT Binary Docs Only</td>\n<td>60.81</td>\n<td>+0.70</td>\n</tr>\n<tr>\n<td>QAT Trinary</td>\n<td>59.49</td>\n<td>-0.62</td>\n</tr>\n<tr>\n<td>QAT 4-bits</td>\n<td>61.73</td>\n<td>+1.62</td>\n</tr>\n<tr>\n<td>QAT 8-bits</td>\n<td>61.67</td>\n<td>+1.56</td>\n</tr>\n<tr>\n<td>QAT 8-bits Min/Max</td>\n<td>61.29</td>\n<td>+1.19</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Tabella 3: Punteggio medio (in % corretto) per ogni condizione sui dodici benchmark NanoBEIR.</em></p><p>Dalla tabella sopra si può notare che il fine-tuning per la quantizzazione migliora i punteggi. L'unica differenza tra le condizioni <strong>PTQ Binary</strong> e <strong>QAT Binary</strong> è il fine-tuning, e la differenza di punteggio è significativa. Allo stesso modo, vediamo un miglioramento di quasi il 2% nei punteggi tra le condizioni <strong>PTQ Binary Docs Only</strong> e <strong>QAT Binary Docs Only</strong>, che si distinguono solo per lo stesso fine-tuning.</p><p>Non sorprende che vediamo anche che i punteggi generalmente migliorano quanto meno quantizziamo, con la quantizzazione a 4 bit che ottiene un punteggio migliore della trinaria e la trinaria migliore della binaria. Tuttavia, passare a 8 bit non sembra aver migliorato nulla.</p><p>Abbiamo testato solo il fatto di lasciare le query non quantizzate nei casi binari, ma questo sembra migliorare le prestazioni.</p><p>Infine, i nostri test suggeriscono che il metodo di scaling della media mobile supera il semplice approccio min/max.</p><h2 id=\"conclusion\">Conclusione</h2><p>La quantizzazione ha alcuni importanti vantaggi operativi per i modelli di embedding, riducendo significativamente le dimensioni dei vettori di embedding e accelerando il recupero delle informazioni. Mentre la semplice quantizzazione post-training (PTQ) fornisce vantaggi immediati in termini di memoria e archiviazione, i nostri esperimenti dimostrano che l'addestramento consapevole della quantizzazione (QAT) mitiga in modo significativo le inevitabili perdite di precisione. Il fine-tuning ha prodotto costantemente punteggi migliori.</p><p>Il grado di quantizzazione ha un impatto diretto sulle prestazioni, che è ciò che ci si aspetterebbe da un metodo basato sulla riduzione della precisione dei valori. La quantizzazione meno aggressiva (ad esempio, a 4 bit) generalmente supera i metodi più aggressivi (ad esempio, binaria), ma sorprendentemente, non c'è stata una differenza significativa nelle prestazioni tra la quantizzazione a 8 bit e quella a 4 bit. Sembrerebbe che fino a quando non si raggiunge una certa soglia di imprecisione, c'è poca differenza tra una quantizzazione maggiore e minore.</p><p>Anche le strategie di scaling sono significative, con il metodo della media mobile che mostra risultati superiori rispetto a un approccio min/max fisso. L'uso di valori di scaling relativi ai dati sembra funzionare significativamente meglio e merita un'ulteriore esplorazione.</p><p>La quantizzazione può farti ottenere di più dai tuoi modelli di embedding a un costo inferiore. Anche se questo articolo non esplora tutte le opzioni per la quantizzazione, ne esplora due che sono facilmente accessibili e hanno reali vantaggi da offrire. Stiamo lavorando per perfezionare e migliorare le strategie di quantizzazione in modo da poter ridurre ulteriormente i costi degli utenti e prevediamo di rilasciare il supporto binario per <code>jina-embeddings-v4</code> nel prossimo futuro.</p>",
  "comment_id": "685d4b76f1bef30001fc5449",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading---2025-06-30T114820.483.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-06-26T15:30:30.000+02:00",
  "updated_at": "2025-06-30T21:14:36.000+02:00",
  "published_at": "2025-06-30T21:14:36.000+02:00",
  "custom_excerpt": "Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/quantization-aware-training-of-jina-embeddings-v4/",
  "excerpt": "La quantizzazione offre modelli di 向量模型 (Embeddings) più piccoli. Vi mostreremo come la quantizzazione ottimizzata vi offre modelli di 向量模型 (Embeddings) persino senza perdita di dati.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
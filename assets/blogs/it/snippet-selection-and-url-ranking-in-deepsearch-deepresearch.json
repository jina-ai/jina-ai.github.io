{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "Selezione di Snippet e Ranking degli URL in DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Una Guida Pratica all'Implementazione di DeepSearch/DeepResearch</div><div class=\"kg-bookmark-description\">QPS fuori, profondit√† dentro. DeepSearch √® il nuovo standard. Trova risposte attraverso cicli di lettura-ricerca-ragionamento. Scopri cos'√® e come costruirlo.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Se hai gi√† letto la nostra guida all'implementazione di DeepSearch/DeepResearch, approfondiamo alcuni dettagli che possono migliorare <em>notevolmente</em> la qualit√†. In questo post, ci concentreremo su due sfide chiave: <strong>sfruttare gli embedding per la selezione di frammenti da pagine web lunghe</strong> e <strong>utilizzare i reranker per dare priorit√† agli URL durante il crawling.</strong></p><p>Alcuni potrebbero ricordare la nostra precedente conclusione secondo cui \"gli embedding erano utili solo per la deduplicazione delle query come i task STS (semantic textual similarity), mentre i reranker non facevano nemmeno parte della nostra implementazione originale di DeepSearch\". Bene, si scopre che entrambi sono ancora molto utili - solo non nel modo convenzionale che ci si potrebbe aspettare. Abbiamo sempre seguito il percorso pi√π <em>snello</em> possibile. Non aggiungiamo componenti solo per giustificare la loro esistenza o il nostro valore come fornitore di embedding e reranker. <strong>Siamo concentrati su ci√≤ di cui la ricerca ha davvero bisogno come fondamento.</strong></p><p>Quindi dopo settimane di esperimenti e iterazioni, abbiamo scoperto utilizzi insoliti ma efficaci per entrambi nei sistemi DeepSearch/DeepResearch. Applicandoli, abbiamo migliorato significativamente la qualit√† di <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> (sentiti libero di provarlo). Vorremmo condividere queste intuizioni con altri professionisti che lavorano in questo ambito.</p><h2 id=\"select-snippet-from-long-content\">Selezionare Frammenti da Contenuti Lunghi</h2><p>Il problema √® questo: dopo <a href=\"https://jina.ai/reader\">aver utilizzato Jina Reader per leggere il contenuto della pagina web</a>, dobbiamo aggiungerlo come elemento di conoscenza al contesto dell'agente per il ragionamento. Mentre inserire l'intero contenuto nella finestra di contesto del LLM √® il modo pi√π semplice, non √® ottimale considerando i costi dei token e la velocit√† di generazione. In pratica, dobbiamo identificare quali parti del contenuto sono pi√π rilevanti per la domanda e aggiungere selettivamente solo quelle parti come conoscenza al contesto dell'agente.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Stiamo parlando dei casi in cui il contenuto rimane troppo lungo anche dopo la pulizia markdown di Jina Reader. Questo accade spesso con pagine lunghe come issue di GitHub, thread di Reddit, discussioni nei forum e post dei blog (inclusi molti dei nostri da jina.ai/news).</div></div><p>Il filtraggio basato su LLM ha gli stessi problemi di costo e latenza, quindi cerchiamo soluzioni con modelli pi√π piccoli: abbiamo bisogno di modelli pi√π piccoli ed economici, <strong>ma ancora multilingue</strong> ‚Äì un fattore cruciale poich√© non possiamo garantire che sia la query che i documenti saranno sempre in inglese.</p><p>Abbiamo una domanda da un lato (sia la query originale che una domanda di gap) e un grande contenuto markdown dall'altro, dove la maggior parte del contenuto √® irrilevante. Dobbiamo selezionare i frammenti pi√π rilevanti per la query. Questo assomiglia al problema di chunking con cui la comunit√† RAG si √® confrontata dal 2023 - recuperare solo i chunk rilevanti utilizzando modelli di retrieval da inserire nella finestra di contesto per la sintesi. Tuttavia, ci sono due differenze chiave nel nostro caso:</p><ol><li>Chunks limitati da un numero limitato di documenti. Se ogni chunk contiene circa 500 token, allora un tipico documento web lungo ha circa 200.000 token (p50) fino a 1.000.000 di token (p99), e noi usiamo Jina Reader per recuperare 4-5 URL ad ogni passo, questo produrrebbe approssimativamente centinaia di chunk - ovvero centinaia di vettori di embedding e centinaia di similarit√† del coseno. Questo √® facilmente gestibile con JavaScript in memoria senza un database vettoriale.</li><li>Abbiamo bisogno di chunk consecutivi per formare frammenti di conoscenza efficaci. Non possiamo accettare frammenti che combinano frasi sparse come <code>[1-2, 6-7, 9, 14, 17, ...].</code> Un frammento di conoscenza pi√π utile seguirebbe pattern come <code>[3-15, 17-24, ...]</code> - mantenendo sempre testo consecutivo. Questo rende pi√π facile per il LLM copiare e citare dalla fonte di conoscenza e riduce le allucinazioni.</li></ol><p>Il resto sono tutte le problematiche di cui si lamentano i professionisti: ogni chunk non pu√≤ essere troppo lungo poich√© i modelli di embedding non possono gestire bene contesti lunghi; il chunking introduce perdita di contesto e rende gli embedding dei chunk i.i.d; e come trovare i migliori indizi di confine che mantengano sia la leggibilit√† che la semantica? Se sai di cosa stiamo parlando, allora probabilmente sei stato perseguitato da questi problemi nelle tue implementazioni RAG.</p><p>Ma in breve - <strong>il late-chunking con <code>jina-embeddings-v3</code> risolve magnificamente tutti e tre i problemi.</strong> Il late chunking mantiene le informazioni di contesto per ogni chunk, √® <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">insensibile agli indizi di confine</a>, e <code>jina-embeddings-v3</code> stesso √® SOTA nei task di retrieval multilingue <em>asimmetrico</em>. I lettori interessati possono seguire i nostri post del blog o i paper per i dettagli, ma ecco l'implementazione complessiva.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">Questo diagramma illustra l'algoritmo di selezione dei frammenti, che funziona in modo simile a </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\">. Il processo inizia dividendo un documento lungo in chunk di lunghezza fissa, che vengono poi incorporati con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> con l'opzione late-chunking attivata. Dopo aver calcolato i punteggi di similarit√† tra ogni chunk e la query, una finestra scorrevole si muove attraverso i punteggi di similarit√† per trovare la finestra con il valore medio pi√π alto.</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cosa √® Realmente il Late Chunking e Cosa Non √®: Parte II</div><div class=\"kg-bookmark-description\">Parte 2 della nostra esplorazione del Late Chunking, un approfondimento sul perch√© √® il miglior metodo per gli embedding dei chunk e il miglioramento delle prestazioni di ricerca/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Embedding Multilingue con Task LoRA</div><div class=\"kg-bookmark-description\">Presentiamo jina-embeddings-v3, un nuovo modello di embedding testuale con 570 milioni di parametri, che raggiunge prestazioni allo stato dell'arte su dati multilingue e task di retrieval con contesto lungo, supportando lunghezze di contesto fino a 8192 token. Il modello include un set di adattatori Low-Rank Adaptation (LoRA) specifici per task per generare embedding di alta qualit√† per il retrieval query-documento, clustering, classificazione e matching del testo. La valutazione sul benchmark MTEB mostra che jina-embeddings-v3 supera gli ultimi embedding proprietari di OpenAI e Cohere su task in inglese, mentre raggiunge prestazioni superiori rispetto a multilingual-e5-large-instruct in tutti i task multilingue. Con una dimensione di output predefinita di 1024, gli utenti possono ridurre flessibilmente le dimensioni degli embedding fino a 32 senza compromettere le prestazioni, abilitato dal Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Embedding di Chunk Contestuali Utilizzando Modelli di Embedding con Contesto Lungo</div><div class=\"kg-bookmark-description\">Molti casi d'uso richiedono il recupero di porzioni pi√π piccole di testo, e i sistemi di retrieval basati su vettori densi spesso funzionano meglio con segmenti di testo pi√π brevi, poich√© √® meno probabile che la semantica sia sovra-compressa negli embedding. Di conseguenza, i professionisti spesso dividono i documenti di testo in chunk pi√π piccoli e li codificano separatamente. Tuttavia, gli embedding dei chunk creati in questo modo possono perdere informazioni contestuali dai chunk circostanti, risultando in rappresentazioni sub-ottimali. In questo articolo, introduciamo un nuovo metodo chiamato late chunking, che sfrutta i modelli di embedding con contesto lungo per incorporare prima tutti i token del testo lungo, con il chunking applicato dopo il modello transformer e appena prima del mean pooling - da qui il termine 'late' nel suo nome. Gli embedding dei chunk risultanti catturano l'informazione contestuale completa, portando a risultati superiori in vari task di retrieval. Il metodo √® abbastanza generico da essere applicato a un'ampia gamma di modelli di embedding con contesto lungo e funziona senza addestramento aggiuntivo. Per aumentare ulteriormente l'efficacia del late chunking, proponiamo un approccio dedicato di fine-tuning per i modelli di embedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Utilizzo del late chunking e del pooling medio tipo Conv1D per selezionare il miglior snippet rispetto alla domanda.</span></p></figcaption></figure><p>Assicurati di chiamare l'API Jina Embeddings con i parametri <code>task</code>, <code>late_chunking</code> e <code>truncate</code> impostati come segue:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Per l'embedding della domanda, assicurati di cambiare </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> in </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> e disattivare </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code></p></figcaption></figure><p>L'implementazione completa pu√≤ essere trovata su Github:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Continua a cercare, leggere pagine web e ragionare finch√© non trova la risposta (o supera il budget di token) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">Classificazione degli URL per la prossima lettura</h2><p>Il problema √® questo: durante una sessione DeepSearch, probabilmente raccoglierai molti URL dalle pagine dei risultati del motore di ricerca (SERP) e ne scoprirai ancora di pi√π ogni volta che leggi singole pagine web (quei link nelle pagine). Il conteggio totale degli URL unici pu√≤ facilmente raggiungere le centinaia. Ancora una volta, inserire semplicemente tutti gli URL direttamente nel contesto dell'LLM √® inefficiente - spreca prezioso spazio nella finestra di contesto e, pi√π problematicamente, <strong>abbiamo scoperto che gli LLM essenzialmente selezionano gli URL in modo casuale.</strong> √à fondamentale guidare l'LLM verso gli URL che hanno la pi√π alta probabilit√† di contenere la risposta necessaria.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Migliore opzione per utilizzare Jina Reader per crawlare una pagina in DeepSearch. Questo raccoglier√† tutti i link della pagina in un campo </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> separato, rimuovendoli dal campo </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\">.</span></p></figcaption></figure><p>Pensa a questo problema come a un PageRank contestuale dove dobbiamo pesare centinaia di URL durante una sessione. Classifichiamo gli URL basandoci su molteplici fattori che combinano l'ultimo tempo di aggiornamento, la frequenza del dominio, la struttura del percorso e, pi√π importante, la rilevanza semantica rispetto alla query per creare un punteggio composito. Ricorda che possiamo utilizzare solo le informazioni disponibili <em>prima</em> di visitare effettivamente l'URL:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>Segnali di Frequenza</strong>: Gli URL che appaiono pi√π volte attraverso diverse fonti ricevono un peso aggiuntivo. Gli URL da domini che appaiono frequentemente nei risultati di ricerca ricevono un boost, poich√© i domini popolari spesso contengono contenuti autorevoli.</p><p><strong>Struttura del Percorso</strong>: Analizziamo i percorsi degli URL per identificare cluster di contenuti. Gli URL all'interno di gerarchie di percorsi comuni ricevono punteggi pi√π alti, con un fattore di decadimento applicato ai percorsi pi√π profondi.</p><p><strong>Rilevanza Semantica</strong>: Utilizziamo <code>jina-reranker-v2-base-multilingual</code> per valutare la rilevanza semantica tra la domanda e le informazioni testuali di ciascun URL, che √® <a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">un classico problema di riordinamento</a>. Le informazioni testuali di ciascun URL provengono da:</p><ul><li>Titolo e snippet dai risultati delle API SERP (<code>https://s.jina.ai/</code> con <code>'X-Respond-With': 'no-content'</code>)</li><li>Testo ancora degli URL nella pagina (<code>https://r.jina.ai</code> con <code>'X-With-Links-Summary': 'all'</code>)</li></ul><p><strong>Ultimo Tempo di Aggiornamento</strong>: Alcune query DeepSearch sono sensibili al tempo, quindi gli URL aggiornati recentemente sono pi√π preziosi di quelli pi√π vecchi. Senza essere un motore di ricerca importante come Google, determinare in modo affidabile l'ultimo tempo di aggiornamento √® una sfida. Abbiamo implementato un approccio multi-livello che combina i seguenti segnali e fornisce un timestamp con punteggio di confidenza che d√† priorit√† ai contenuti pi√π recenti quando necessario.</p><ul><li>Filtri API SERP (come il parametro <code>tbs</code> di s.jina.ai per filtrare per recenza)</li><li>Analisi dell'header HTTP (Last-Modified, ETag)</li><li>Estrazione dei metadati (meta tag, timestamp Schema.org)</li><li>Riconoscimento dei pattern di contenuto (date visibili in HTML)</li><li>Indicatori specifici per CMS per piattaforme come WordPress, Drupal e Ghost</li></ul><p><strong>Contenuto Protetto:</strong> Alcuni contenuti sulle piattaforme di social media sono protetti o semplicemente dietro paywall, e senza effettuare l'accesso o violare i loro ToS, non c'√® modo legittimo di recuperare questi contenuti. Dovremmo mantenere attivamente una lista di URL e nomi host problematici per abbassare i loro ranking, evitando di perdere tempo su contenuti inaccessibili.</p><p><strong>Diversit√† di Dominio:</strong> In alcuni casi, gli URL con il peso pi√π alto provengono tutti dagli stessi nomi host, il che pu√≤ intrappolare DeepSearch in un ottimo locale e ridurre la qualit√† finale dei risultati. Controlla gli esempi sopra dove tutti gli URL principali provengono da StackOverflow. Per migliorare la diversit√†, possiamo implementare un approccio di esplorazione-sfruttamento selezionando i k URL con il ranking pi√π alto da ciascun nome host.</p><p>L'implementazione completa della classificazione degli URL pu√≤ essere trovata sul nostro Github.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Continua a cercare, leggere pagine web e ragionare finch√© non trova la risposta (o supera il budget di token) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Faceü§ó Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset ¬∑ Issue #7175 ¬∑ huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Ricorda di inserire i pesi degli URL nel contesto dell'agente e di istruire i LLM a rispettare i pesi.</span></p></figcaption></figure><h2 id=\"conclusion\">Conclusione</h2><p>Dal rilascio del nostro sistema DeepSearch il 2 febbraio 2025, abbiamo scoperto due dettagli implementativi che hanno migliorato sostanzialmente la qualit√†. Sorprendentemente, entrambi utilizzano embedding e reranker multilingue in modo \"in-context\" - operando su una scala molto pi√π piccola rispetto agli indici pre-calcolati che questi modelli tipicamente richiedono. Questo spiega la nostra iniziale svista.</p><p>Ci√≤ indica un'affascinante polarizzazione nel futuro della tecnologia di ricerca. Consideriamo un framework analogo alla teoria del doppio processo di Kahneman:</p><ul><li>Fast-think (grep, BM25, SQL): Corrispondenza di pattern rapida e governata da regole con minime esigenze computazionali.</li><li>Slow-think (LLM): Ragionamento completo con profonda comprensione contestuale, che richiede una computazione significativa.</li><li>Mid-think (embedding, reranker): Intrappolati nel limbo? Troppo \"avanzati\"/semantici per la semplice corrispondenza di pattern ma privi di vere capacit√† di ragionamento.</li></ul><p>Potremmo star assistendo alla popolarit√† di un'architettura biforcata dove SQL/BM25 leggeri ed efficienti gestiscono il recupero iniziale dei contenuti, alimentando direttamente potenti LLM per l'elaborazione profonda. Questi LLM incorporano sempre pi√π le funzioni semantiche che prima richiedevano modelli specializzati di livello intermedio. Il ruolo rimanente per i modelli mid-think si sposta verso attivit√† in-context specializzate: filtraggio, deduplicazione e operazioni di portata limitata dove un ragionamento completo sarebbe inefficiente.</p><p>Tuttavia, la selezione di snippet critici e il ranking degli URL rimangono componenti fondamentali con un impatto diretto sulla qualit√† dei sistemi DeepSearch/DeepResearch. Speriamo che le nostre intuizioni stimolino miglioramenti nelle vostre implementazioni.</p><p>L'espansione delle query continua a essere un altro determinante cruciale della qualit√†. Stiamo valutando attivamente molteplici approcci‚Äîdai basic prompt-based rewrite ai small language model e metodi basati sul ragionamento. Cercate i nostri prossimi risultati su questo fronte presto. Restate sintonizzati.</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "Perfezionare questi due dettagli trasforma il tuo DeepSearch da mediocre a eccezionale: selezionare i migliori frammenti da pagine web lunghe e classificare gli URL prima del crawling.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
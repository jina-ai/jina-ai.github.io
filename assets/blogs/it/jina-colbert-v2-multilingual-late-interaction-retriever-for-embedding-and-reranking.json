{
  "slug": "jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking",
  "id": "66cd8fc6e84873000133d63d",
  "uuid": "e995c4d9-1832-4e2a-8108-e8453f5c82c5",
  "title": "ColBERT v2 di Jina: Retriever di interazione tardiva multilingue per embedding e reranking",
  "html": "<p>Oggi siamo entusiasti di rilasciare Jina ColBERT v2 (<code>jina-colbert-v2</code>), un modello di recupero avanzato a interazione tardiva basato sull'architettura ColBERT. Questo nuovo modello linguistico migliora le prestazioni di <code>jina-colbert-v1-en</code> e aggiunge supporto multilingue e dimensioni di output dinamiche.</p><p>Questa nuova versione presenta le seguenti caratteristiche:</p><ul><li><strong>Prestazioni di recupero superiori</strong> rispetto al ColBERT-v2 originale (+6,5%) e alla nostra versione precedente, <code>jina-colbert-v1-en</code> (+5,4%).</li><li><strong>Supporto multilingue</strong> per 89 lingue, con prestazioni elevate nelle principali lingue globali.</li><li><strong>Dimensioni dell'embedding di output controllabili dall'utente</strong> attraverso l'apprendimento di rappresentazioni Matryoshka, permettendo agli utenti di bilanciare flessibilmente efficienza e precisione.</li></ul><h2 id=\"technical-summary-of-jina-colbert-v2\">Riepilogo Tecnico di <code>jina-colbert-v2</code></h2><p>Il report tecnico completo è disponibile su arXiv:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2408.16672?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever</div><div class=\"kg-bookmark-description\">Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Rohan Jha</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>jina-colbert-v2</code></th>\n<th><code>jina-colbert-v1-en</code></th>\n<th>Original ColBERTv2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Average of 14 English<br/>BEIR tasks</td>\n<td><b>0.521</b></td>\n<td>0.494</td>\n<td>0.489</td>\n</tr>\n<tr>\n<td>Multilingual</td>\n<td><b>89 languages</b></td>\n<td>English-only</td>\n<td>English-only</td>\n</tr>\n<tr>\n<td>Output dimensions</td>\n<td><b>128, 96, or 64</b></td>\n<td>Fixed 128</td>\n<td>Fixed 128</td>\n</tr>\n<tr>\n<td>Max query length</td>\n<td>32 tokens</td>\n<td>32 tokens</td>\n<td>32 tokens</td>\n</tr>\n<tr>\n<td>Max document length</td>\n<td>8192 tokens</td>\n<td>8192 tokens</td>\n<td>512 tokens</td>\n</tr>  \n\n<tr>\n<td>Parameters</td>\n<td>560M</td>\n<td>137M</td>\n<td>110M</td>\n</tr>\n<tr>\n<td>Model size</td>\n<td>1.1GB</td>\n<td>550MB</td>\n<td>438MB</td>\n</tr>\n\n\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"asymmetric-embedding-in-colbert\">Embedding Asimmetrico in ColBERT</h2><p>ColBERT si basa sull'architettura BERT aggiungendo codifica di query e documenti <strong>a interazione tardiva</strong> e <strong>asimmetrica</strong>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>La natura asimmetrica di ColBERT significa che quando si utilizzano modelli come <code>jina-colbert-v2</code> o <code>jina-colbert-v1-en</code>, è necessario specificare se si sta incorporando una query, un documento o entrambi (per scopi di riordinamento). Questa flessibilità aggiuntiva migliora le prestazioni rispetto ai modelli di embedding omogenei nelle attività di recupero.</p><h2 id=\"multilingual-support-for-over-89-languages\">Supporto Multilingue Per Oltre 89 Lingue</h2><p>Jina ColBERT v2 ha ampie capacità multilingue, progettate per soddisfare le esigenze delle moderne applicazioni di recupero informazioni e AI globalizzate. Il corpus di addestramento per <code>jina-colbert-v2</code> incorpora 89 lingue, con fasi aggiuntive di addestramento per le principali lingue internazionali tra cui <strong>arabo, cinese, inglese, francese, tedesco, giapponese, russo e spagnolo</strong>, oltre ai <strong>linguaggi di programmazione</strong>. L'addestramento ha incluso anche un corpus di testi bilingue allineati per sbloccare potenzialità cross-linguistiche, permettendo di abbinare query e documenti in lingue diverse nelle attività di riordinamento/recupero.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Distribution-of-the-languages-in-the-training-corpus-at-the-pretrained-stage--3-.svg\" class=\"kg-image\" alt=\"Chart of language distribution in training data, highlighting dominance of English and Chinese.\" loading=\"lazy\" width=\"1456\" height=\"743\"><figcaption><span style=\"white-space: pre-wrap;\">Distribuzione dei dati del dataset di pre-addestramento per lingua (specificata dal codice </span><a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ISO-639</span></a><span style=\"white-space: pre-wrap;\">) in scala logaritmica.</span></figcaption></figure><p>Oggi, Jina ColBERT v2 si distingue come <strong>l'unico modello di tipo ColBERT multilingue</strong> che genera embedding compatti, superando significativamente il recupero basato su BM25 in tutte le lingue testate sui benchmark MIRACL.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-Multilingual-Data--1-.svg\" class=\"kg-image\" alt=\"Bar chart comparing jina-colbert-v2 and BM25 performance across 20 languages on multilingual tasks.\" loading=\"lazy\" width=\"691\" height=\"426\"><figcaption><span style=\"white-space: pre-wrap;\">Prestazioni di Jina ColBERT v2 su 16 lingue, confrontate con BM25, sui benchmark MIRACL.</span></figcaption></figure><p>Inoltre, nelle attività di recupero in lingua inglese, Jina ColBERT v2 supera le prestazioni del suo predecessore <code>jina-colbert-v1-en</code> e del modello ColBERT v2 originale, con prestazioni paragonabili al modello altamente specializzato solo in inglese <a href=\"https://huggingface.co/answerdotai/answerai-colbert-small-v1?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">AnswerAI-ColBERT-small</a>.</p>\n<!--kg-card-begin: html-->\n<table class=\"simple-table\">\n  <tbody>\n<thead>\n<tr>\n      <th><strong>Model Name</strong></th>\n      <th><strong>Average score<br>(14 BEIR English-only benchmarks)<br></strong></th>\n      <th><strong>Multilingual Support</strong></th>\n  </tr>\n    </thead>\n    <tr>\n      <td><code>jina-colbert-v2</code></td>\n      <td>0.521</td>\n      <td>Multilingual</td>\n    </tr>\n    <tr>\n      <td><code>jina-colbert-v1-en</code></td>\n      <td>0.494</td>\n      <td>English-only</td>\n    </tr>\n    <tr>\n      <td>ColBERT v2.0</td>\n      <td>0.489</td>\n      <td>English-only</td>\n    </tr>\n    <tr>\n      <td>AnswerAI-ColBERT-small</td>\n      <td>0.549</td>\n      <td>English-only</td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-English-only-datasets-from-BEIR--2-.svg\" class=\"kg-image\" alt=\"Bar chart showing model evaluations on English BEIR datasets, with several models like 'jina-colbert' and 'BM25'.\" loading=\"lazy\" width=\"1088\" height=\"712\"><figcaption><span style=\"white-space: pre-wrap;\">Valutazione di jina-colbert-v2 su una selezione di dataset solo in inglese dal benchmark BEIR.</span></figcaption></figure><h2 id=\"matryoshka-representation-learning\">Apprendimento della Rappresentazione Matryoshka</h2><p><a href=\"https://arxiv.org/abs/2205.13147?ref=jina-ai-gmbh.ghost.io\">L'apprendimento della rappresentazione Matryoshka</a> è una tecnica per addestrare modelli a supportare diverse dimensioni del vettore di output minimizzando qualsiasi perdita di accuratezza. Addestriamo gli strati nascosti della rete con diverse teste di proiezione lineare — gli strati finali di una rete neurale — ciascuna che supporta una dimensione di output diversa. <strong>Jina ColBERT v2 supporta vettori di output di 128, 96 e 64 dimensioni.</strong></p><p>Jina ColBERT v2 produce embedding di output a 128 dimensioni per impostazione predefinita, ma può produrre dimensioni di 96 e 64 che hanno prestazioni quasi identiche ma sono rispettivamente più corte del 25% e del 50%.</p><p>La tabella seguente mostra le prestazioni <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\">nDGC</a> di<code>jina-colbert-v2</code> per i primi dieci risultati (<em>nDGC@10</em>) su sei dataset del benchmark BEIR. Qui si può vedere che la differenza di prestazioni tra 128 dimensioni e 96 è appena dell'1% e sotto l'1,5% tra 128 e 64 dimensioni.</p>\n<!--kg-card-begin: html-->\n<table id=\"b838dc78-1321-499e-98e7-63e3b5c8e910\" class=\"simple-table\"><tbody><thead id=\"177f4349-0620-4947-a3ce-01e598395ed7\"><tr><th id=\"<\\ml\" class=\"\"><strong>Dimensioni Output</strong></th><th id=\"<NYX\" class=\"\"><strong>Punteggio </strong><strong>Medio</strong><strong><br>(nDGC@10 per 6 benchmark)<br></strong></th></tr></thead><tr id=\"9199b56b-0513-4c99-a2a7-29cde915c3b9\"><td id=\"<\\ml\" class=\"\">128</td><td id=\"<NYX\" class=\"\">0.565</td></tr><tr id=\"af4d45fc-ebf4-4e1f-b5b0-1807a1cb889b\"><td id=\"<\\ml\" class=\"\">96</td><td id=\"<NYX\" class=\"\">0.558</td></tr><tr id=\"ecf7eac9-5c56-47e6-ab27-0ddb4659e263\"><td id=\"<\\ml\" class=\"\">64</td><td id=\"<NYX\" class=\"\">0.556</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Performance-on-selected-BEIR-benchmarks.svg\" class=\"kg-image\" alt=\"Grafico a barre dei benchmark BEIR, che evidenzia i punteggi di dataset come nfcorpus a msmarco, con jina-colbert-v2.64 che eccelle.\" loading=\"lazy\" width=\"732\" height=\"538\"><figcaption><span style=\"white-space: pre-wrap;\">Prestazioni di Jina ColBERT v2 a diverse dimensioni di output.</span></figcaption></figure><p>Ridurre la dimensione dei vettori di output permette di risparmiare spazio e velocizzare applicazioni come il recupero di informazioni basato su vettori che devono confrontare diversi vettori o misurare la distanza tra loro.</p><p>Questo ha significative conseguenze sui costi, anche solo in termini di riduzione dello storage. Per esempio, usando il <a href=\"https://cloud.qdrant.io/calculator?ref=jina-ai-gmbh.ghost.io\">calcolatore dei costi cloud di Qdrant</a>, memorizzare 100 milioni di documenti su AWS con vettori a 128 dimensioni per ciascuno ha un <a href=\"https://cloud.qdrant.io/calculator?provider=aws&region=eu-central-1&replicas=1&quantization=None&vectors=100000000&dimension=128&ref=jina-ai-gmbh.ghost.io\">costo stimato di 1.319,24 dollari al mese</a>. A 64 dimensioni, questo <a href=\"https://cloud.qdrant.io/calculator?provider=aws&region=eu-central-1&replicas=1&quantization=None&vectors=100000000&dimension=64&ref=jina-ai-gmbh.ghost.io\">scende a 659,62 dollari</a>.</p><h2 id=\"getting-started-with-jina-colbert-v2\">Iniziare con Jina ColBERT v2</h2><p>Jina ColBERT v2 è disponibile tramite la Jina Search Foundation API, l'<a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS marketplace</a> e <a href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\">su Azure</a>. È inoltre disponibile per <em>solo uso non commerciale</em> (<a href=\"https://www.creativecommons.org/licenses/by-nc/4.0/deed.en?ref=jina-ai-gmbh.ghost.io\">CC BY-NC-4.0</a>) tramite <a href=\"https://huggingface.co/jinaai/jina-colbert-v2?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>.</p><h3 id=\"via-jina-search-foundation-api\">Tramite Jina Search Foundation API</h3><h4 id=\"for-embedding\">Per l'Embedding</h4><p>Il seguente comando <code>curl</code> mostra come specificare l'input e le opzioni per ottenere embedding di documenti da <code>jina-colbert-v2</code> tramite la Jina Embeddings API. Per ottenere vettori della dimensione desiderata, specificare 128 o 64 per il parametro <code>dimensions</code>. Questo parametro è opzionale e il valore predefinito è 128.</p><p>I documenti di input verranno troncati se più lunghi di 8192 token.</p><p>Specificare la propria chiave API Jina nell'header di autorizzazione <code>Authorization: Bearer &lt;YOUR JINA API KEY&gt;</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # Or 64 for half-size vectors\n\t\"input_type\": \"document\", # For query embeddings see below\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your document text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each text can be up to 8192 tokens long\"\n    ]}'\n</code></pre><p>Per ottenere embedding di query, impostare il parametro <code>input_type</code> su <code>query</code> invece di <code>document</code>. Notare che le query hanno limiti di dimensione molto più severi rispetto ai documenti. Verranno troncate a 32 token. La codifica della query restituirà <em>sempre</em> 32 token, inclusi gli embedding per il padding se inferiore a 32 token.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # Or 64 for half-size vectors\t\n\t\"input_type\": \"query\", # This must be specified for query embeddings\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your query text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each query text can be up to 32 tokens long\"\n    ]}'\n</code></pre><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Embedding multimodali e bilingue a lungo contesto per la tua ricerca e RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"for-reranking\">Per il Reranking</h4><p>Per utilizzare <code>jina-colbert-v2</code> tramite la Jina Reranker API, passando una query e diversi documenti e ottenendo in cambio punteggi di corrispondenza ordinabili, costruire la richiesta come quella qui sotto:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/rerank \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n      \"model\": \"jina-colbert-v2\",\n      \"query\": \"What is the population of Berlin?\",\n      \"top_n\": 3,\n      \"documents\": [\n        \"Berlin's population grew by 0.7 percent in 2023 compared with the previous year. Accordingly, around 27,300 more residents lived in Berlin at the end of the last year than in 2022. Those of 30 to under 40 years old form the numerically largest age group. With roughly 881,000 foreign residents from around 170 nations and an average age of the population of 42.5 years old.\",\n        \"Mount Berlin is a glacier-covered volcano in Marie Byrd Land, Antarctica, 100 kilometres (62 mi) from the Amundsen Sea. It is a roughly 20-kilometre-wide (12 mi) mountain with parasitic vents that consists of two coalesced volcanoes: Berlin proper with the 2-kilometre-wide (1.2 mi) Berlin Crater and Merrem Peak with a 2.5-by-1-kilometre-wide (1.55 mi × 0.62 mi) crater, 3.5 kilometres (2.2 mi) away from Berlin.\",\n        \"Population as of 31.12.2023 by nationality and federal states Land\\\\tTotal\\\\tGermans\\\\tForeigners\\\\tincluding EU-states number\\\\t%\\\\tnumber\\\\t%\",\n        \"The urban area of Berlin has a population of over 4.5 million and is therefore the most populous urban area in Germany. The Berlin-Brandenburg capital region has around 6.2 million inhabitants and is Germany's second-largest metropolitan region after the Rhine-Ruhr region, and the sixth-biggest metropolitan region by GDP in the European Union.\",\n        \"Irving Berlin (born Israel Beilin) was an American composer and songwriter. His music forms a large part of the Great American Songbook. Berlin received numerous honors including an Academy Award, a Grammy Award, and a Tony Award.\",\n        \"Berlin is a town in the Capitol Planning Region, Connecticut, United States. The population was 20,175 at the 2020 census.\",\n        \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\",\n        \"Berlin, Berlin ist eine für die ARD produzierte Fernsehserie, die von 2002 bis 2005 im Vorabendprogramm des Ersten ausgestrahlt wurde. Regie führten unter anderem Franziska Meyer Price, Christoph Schnee, Sven Unterwaldt Jr. und Titus Selge.\"\n        ]\n    }'</code></pre><p>Notare l'argomento <code>top_n</code>, che specifica il numero di documenti che si desidera recuperare. Per esempio, se l'applicazione usa solo la corrispondenza migliore, impostare <code>top_n</code> a 1.</p><p>Per snippet di codice in Python e altri linguaggi di programmazione e framework, visitare la <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">pagina della Jina AI Embeddings API</a>, o selezionare <code>jina-colbert-v2</code> dal menu a tendina sulla <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\">pagina della Jina Reranker API</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Massimizza facilmente la rilevanza della ricerca e l'accuratezza RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"via-stanford-colbert\">Tramite Stanford ColBERT</h3><p>Puoi anche utilizzare Jina ColBERT v2 come sostituto diretto di <a href=\"https://github.com/stanford-futuredata/ColBERT?ref=jina-ai-gmbh.ghost.io\">ColBERT v2</a> nella libreria Stanford ColBERT. Specifica semplicemente <code>jinaai/jina-colbert-v2</code> come fonte del modello:</p><pre><code class=\"language-python\">from colbert.infra import ColBERTConfig\nfrom colbert.modeling.checkpoint import Checkpoint\n\nckpt = Checkpoint(\"jinaai/jina-colbert-v2\", colbert_config=ColBERTConfig())\ndocs = [\"Your list of texts\"] \nquery_vectors = ckpt.queryFromText(docs)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Devi installare <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> e <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> per utilizzare il codice sopra.</div></div><h3 id=\"via-ragatouille\">Tramite RAGatouille</h3><p>Jina ColBERT v2 è analogamente integrato in <a href=\"https://github.com/AnswerDotAI/RAGatouille?ref=jina-ai-gmbh.ghost.io\">RAGatouille</a>. Puoi scaricarlo e utilizzarlo tramite il metodo <code>RAGPretrainedModel.from_pretrained()</code>:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v2\")\ndocs = [\"Your list of texts\"]\nRAG.index(docs, index_name=\"your_index_name\")\nquery = \"Your query\"\nresults = RAG.search(query)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Devi installare <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> e <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> per utilizzare il codice sopra.</div></div><h3 id=\"via-qdrant\">Tramite Qdrant</h3><p>Dalla versione 1.10, Qdrant ha aggiunto il <a href=\"https://qdrant.tech/blog/qdrant-1.10.x/?ref=jina-ai-gmbh.ghost.io\">supporto</a> per i multi-vector e i modelli late-interaction. Gli utenti esistenti dei motori Qdrant, sia locali che versioni cloud gestite, possono beneficiarne integrando direttamente <code>jina-colbert-v2</code> utilizzando il client di Qdrant.</p><p><strong>Creazione di una nuova Collection utilizzando l'operazione MAX_SIM</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nqdrant_client.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config={\n        \"colbert\": models.VectorParams(\n            size=128,\n            distance=models.Distance.COSINE,\n            multivector_config=models.MultiVectorConfig(\n                comparator=models.MultiVectorComparator.MAX_SIM\n            ),\n        )\n    }\n)</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Impostare correttamente il parametro <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">multivector_config</code> è essenziale per utilizzare i modelli stile ColBERT in Qdrant.</div></div><p><strong>Inserimento di Documenti nelle Collection Multi-vector</strong></p><pre><code class=\"language-Python\">import requests\nfrom qdrant_client import QdrantClient, models\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\ndata = {\n    'model': 'jina-colbert-v2',\n    'input_type': 'query',\n    'embedding_type': 'float',\n    'input': [\n        'Your text string goes here',\n        'You can send multiple texts',\n        'Each text can be up to 8192 tokens long'\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nrows = response.json()[\"data\"]\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nfor i, row in enumerate(rows):\n    qdrant_client.upsert(\n        collection_name=\"{collection_name}\",\n        points=[\n            models.PointStruct(\n                id=i,  \n                vector=row[\"embeddings\"],  \n                payload={\"text\": data[\"input\"][i]} \n            )\n        ],\n    )</code></pre><p><strong>Interrogazione delle Collection</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\nimport requests\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\n\ndata = {\n    'model': 'jina-colbert-v2',\n    \"input_type\": \"query\",\n    \"embedding_type\": \"float\",\n    \"input\": [\n        \"how many tokens in an input do Jina AI's embedding models support?\"\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nvector = response.json()[\"data\"][0][\"embeddings\"]\n\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nresults = qdrant_client.query_points(\n    collection_name=\"{collection_name}\",\n    query=vector,\n)\n\nprint(results)</code></pre><h3 id=\"summary\">Riepilogo</h3><p>Jina ColBERT v2 (<code>jina-colbert-v2</code>) si basa sulle alte prestazioni di <code>jina-colbert-v1-en</code> ed espande le sue capacità a un'ampia gamma di lingue globali. Con il supporto per diverse dimensioni di embedding, <code>jina-colbert-v2</code> permette agli utenti di regolare il compromesso precisione/efficienza per adattarsi ai loro casi d'uso specifici, offrendo potenzialmente significativi risparmi in termini di tempo e costi di calcolo.</p><p>Questo modello combina tutte queste caratteristiche in un unico pacchetto dal prezzo competitivo, accessibile tramite una API web intuitiva e compatibile con qualsiasi framework di calcolo che supporti le richieste HTTP. <a href=\"https://jina.ai/?sui=&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Provalo tu stesso</a> con 1 milione di token gratuiti per vedere come può migliorare le tue applicazioni e processi.</p>",
  "comment_id": "66cd8fc6e84873000133d63d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-27T10:35:18.000+02:00",
  "updated_at": "2024-09-09T07:43:38.000+02:00",
  "published_at": "2024-08-30T09:19:58.000+02:00",
  "custom_excerpt": "Jina ColBERT v2 supports 89 languages with superior retrieval performance, user-controlled output dimensions, and 8192 token-length. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking/",
  "excerpt": "Jina ColBERT v2 supporta 89 lingue con prestazioni di recupero superiori, dimensioni di output controllate dall'utente e una lunghezza di token di 8192.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Dark-themed coding interface displaying English and Japanese characters with \"JINA COLBERT V2\" highlighted in the center.",
  "feature_image_caption": null
}
{
  "slug": "build-a-rag-system-with-jina-embeddings-and-qdrant",
  "id": "65ddadd039a5b70001810ea3",
  "uuid": "e3419a3e-4a7c-43c3-ac69-6f65f00a7ad8",
  "title": "Costruisci un sistema RAG con Jina Embeddings e Qdrant",
  "html": "<p>La generazione basata sul recupero delle informazioni è un insieme di tecniche in rapida crescita che sono state <a href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io\"><u>originariamente concepite come un'estensione dei large language model</u></a> (LLM) ma sono sempre più viste come un paradigma alternativo che reimmagina completamente il ruolo dei modelli di AI.</p><p>Ti mostreremo come creare un motore RAG di base utilizzando il framework <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>, un <a href=\"https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io\">LLM Mistral</a> open-source, <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2</a> e il <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\">database vettoriale AI-ready Qdrant</a>.</p><h2 id=\"rag-in-context\"><strong>RAG nel Contesto</strong></h2><p>In un sistema RAG, il ruolo del modello AI rimane centrale ma molto più limitato. Invece di rispondere ai prompt basandosi sulla sua conoscenza interna, derivata dai dati di training, gli vengono presentate le informazioni rilevanti nel prompt. Il ruolo del modello è quindi quello di comprendere e sintetizzare le informazioni in un output.</p><p>Questo sfrutta i punti di forza dei modelli AI e ne minimizza le debolezze.</p><p>Per esempio, potremmo dare al <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\"><u>modello Mistral-Instruct</u></a> il seguente prompt:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: Why is Taylor Swift planning to attend the Superbowl?\nAnswer:\n</code></pre><p>Quando l'ho fatto, il modello ha risposto:</p><pre><code class=\"language-text\">I don't have real-time information. However, as of my last update,&nbsp;\nTaylor Swift is not officially confirmed to attend or perform at&nbsp;\nthe Superbowl. If there are any updates, I recommend checking the&nbsp;\nlatest news sources.</code></pre><p>I large language model sono costosi da aggiornare e riqualificare, quindi le loro informazioni interne non possono mai essere completamente aggiornate.</p><p>Inoltre, i large language model sono noti per \"allucinare\", ovvero inventare risposte plausibili che possono essere parzialmente o totalmente errate e devono essere verificate.</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: What was Weird Al Yankovic's first Taylor Swift parody?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" a spoof&nbsp;\nof Swift's song \"Blank Space.\" The parody was released in 2014&nbsp;\nas part of Yankovic's album \"Mandatory Fun.\" The song humorously&nbsp;\naddresses the obsession with celebrity gossip and scandals, as&nbsp;\nreported by the tabloid news website TMZ.</code></pre><p><em>TMZ</em> di Weird Al Yankovic è stato pubblicato nel 2011 nell'album <em>Alpocalypse</em> ed è una parodia di <a href=\"https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io\"><em><u>You Belong with Me</u></em></a> di Swift.</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" title=\"TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)\"></iframe></figure><p>La natura delle reti neurali rende l'allucinazione difficile, se non impossibile, da prevenire completamente e mina fortemente qualsiasi piano di utilizzare i modelli AI come fonti di informazione.</p><p>Nelle <a href=\"https://youtu.be/uaQZIK9gvNo?t=1130&ref=jina-ai-gmbh.ghost.io\"><u>parole di Sam Altman</u></a>, CEO di OpenAI:</p><blockquote class=\"kg-blockquote-alt\">\"Penso che le persone commettano un grande errore quando pensano a questi modelli [LLM] come a un database. [...] Funzionerà, in un certo senso. Ma è tipo il database più costoso, più lento e meno accurato al mondo.\"</blockquote><p>RAG minimizza la dipendenza dalla conoscenza interna dei fatti del mondo di un modello AI e sfrutta i suoi più grandi punti di forza: la comprensione del linguaggio naturale e la generazione fluente del linguaggio.</p><p>Prendiamo un altro esempio di una domanda a cui Mistral-Instruct non può rispondere:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\n\nQuery: What is the title of Taylor Swift's next album?\nAnswer:\n\nAs of the time I am providing this answer, Taylor Swift has&nbsp;\nnot yet announced the title of her next album. She has released&nbsp;\nseveral re-recorded albums, but the title of her next original&nbsp;\nalbum remains unknown. I recommend checking her official social&nbsp;\nmedia channels or website for the latest updates.</code></pre><p>Ora, un sistema RAG potrebbe avere un database di articoli di notizie recenti, o forse cercare su Internet testi relativi al titolo del prossimo album di Taylor Swift. Quindi, inserirebbe i testi che trova nel prompt.</p><p>Supponiamo che identifichi <a href=\"https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io\"><u>questo articolo AP</u></a> come rilevante per la query e costruisca il seguente prompt:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nFebruary 6, 2024\nLOS ANGELES (AP) — Forget her \"Reputation.\" Taylor Swift has a&nbsp;\nwhole new album coming out.\n\nAccepting the Grammy for best pop vocal album on Sunday night,&nbsp;\nTaylor said she'd been keeping a secret for two years.\n\n\"My brand-new album comes out April 19. It's called 'The Tortured&nbsp;\nPoets Department.' I'm going to go and post the cover right now&nbsp;\nbackstage,\" she announced.\n\nAnd so she did.\n\n---------------------\nGiven the context information and not prior knowledge, answer the query.&nbsp;\nPlease be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What is the title of Taylor Swift's next album?\nAnswer:</code></pre><p>Mistral-Instruct ha risposto con:</p><pre><code class=\"language-text\">The title of Taylor Swift's next album is 'The Tortured Poets Department'.</code></pre><p>Questo tipo di prompting riduce anche drasticamente la probabilità di allucinazioni. Per esempio:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nAlpocalypse is the thirteenth studio album by American parody&nbsp;\nmusician \"Weird Al\" Yankovic, released on June 21, 2011.\n\nTrack List\n\n1. Perform This Way (Parody of \"Born This Way\" by Lady Gaga)\n2. CNR\n3. TMZ (Parody of \"You Belong with Me\" by Taylor Swift)\n4. Skipper Dan\n5. Polka Face (Polka medley including songs by artists such as&nbsp;\n&nbsp;&nbsp;&nbsp;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)\n6. Craigslist\n7. Party in the CIA (Parody of \"Party in the U.S.A.\" by Miley&nbsp;\n&nbsp;&nbsp;&nbsp;Cyrus)\n8. Ringtone\n9. Another Tattoo (Parody of \"Nothin' on You\" by B.o.B featuring&nbsp;\n&nbsp;&nbsp;&nbsp;Bruno Mars)\n10. If That Isn't Love\n11. Whatever You Like (Parody of \"Whatever You Like\" by T.I.)\n12. Stop Forwarding That Crap to Me\n\n---------------------\nGiven the context information and not prior knowledge, answer the&nbsp;\nquery. Please be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What was Weird Al Yankovic's first Taylor Swift parody, what&nbsp;\nyear was it released, and what song was he parodying?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" and it was&nbsp;\nreleased in 2011. He was parodying \"You Belong with Me\" by Taylor&nbsp;\nSwift.</code></pre><h2 id=\"anatomy-of-a-rag-system\"><strong>Anatomia di un sistema RAG</strong></h2><p>Un sistema RAG ha, come minimo, due componenti:</p><ul><li>Un modello AI generativo.</li><li>Un sistema di recupero delle informazioni.</li></ul><p>In questo articolo, useremo <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io\">Mistral-Instruct</a> come modello AI generativo e vi accederemo tramite la <a href=\"https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io\">Hugging Face Inference API</a>. Useremo il <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\">database vettoriale Qdrant</a> e <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> insieme per il sistema di recupero delle informazioni.</p><p>Come fonte di dati, useremo il <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io\"><em><u>National Artificial Intelligence Research and Development Strategic Plan 2023 Update</u></em></a>, pubblicato a maggio 2023 dal <a href=\"https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io\">National Science and Technology Council</a>, un organo che consiglia il Presidente degli Stati Uniti su questioni scientifiche e tecniche. Questo documento di circa 25.000 parole è un documento di politica e pianificazione strategica per la politica e lo sviluppo dell'AI americana.</p><p>È anche un classico documento governativo \"troppo lungo, non l'ho letto\". La maggior parte è molto noiosa e, a meno che non si abbia un interesse o un coinvolgimento diretto in questo settore, probabilmente non lo leggerete. Ma potreste comunque voler sapere cosa dice.</p><p>Questo articolo ti guiderà attraverso i passaggi per creare un sistema RAG in grado di rispondere a domande sul testo e sui suoi contenuti.</p><h2 id=\"install-the-prerequisites\"><strong>Installare i Prerequisiti</strong></h2><p>Prima di tutto, assicurati di avere installate tutte le librerie Python pertinenti. Nel tuo ambiente Python, esegui quanto segue:</p><pre><code class=\"language-shell\">pip install llama-index qdrant-client \npip install pdfminer.six llama-index-vector-stores-qdrant\npip install llama-index-llms-openai llama-index-embeddings-jinaai \npip install llama-index-llms-huggingface \"huggingface_hub[inference]\"</code></pre><p>Useremo LlamaIndex per costruire un sistema RAG attorno al modello linguistico Mistral-Instruct e <a href=\"https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io\"><u>pdfminer.six</u></a> per elaborare il file PDF in un vector store Qdrant.</p><h2 id=\"connect-to-mistral-instruct-via-the-hugging-face-inference-api\"><strong>Connettersi a Mistral-Instruct tramite l'API Inference di Hugging Face</strong></h2><p>Per prima cosa, configuriamo il LLM. Avrai bisogno di accedere all'API Inference di Hugging Face, incluso un token di accesso. Se hai un account Hugging Face, puoi ottenerne uno dalla <a href=\"https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io\">pagina delle impostazioni del tuo account</a>.</p><p>Se non hai un account, prima <a href=\"https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io\">creane uno</a>, poi crea un <a href=\"https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io\">token di accesso</a>.</p><p>Inserisci il tuo token in una variabile per uso futuro:</p><pre><code class=\"language-python\">hf_inference_api_key = '&lt;your HuggingFace Inference API token&gt;'</code></pre><p>Successivamente, costruiamo un template per il prompt:</p><pre><code class=\"language-python\">from llama_index.core import PromptTemplate\n\nqa_prompt_tmpl = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query. Please be brief, concise, and complete.\\n\"\n    \"If the context information does not contain an answer to the query, \"\n    \"respond with \\\"No information\\\".\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>Questo template istruisce il LLM a utilizzare solo le informazioni di contesto fornite nel prompt per rispondere alle domande. Questo prompt ha funzionato bene per noi nel lavorare con Mistral-Instruct, ma puoi sperimentare con altre formulazioni per vedere cosa funziona meglio per il tuo caso d'uso.</p><pre><code class=\"language-python\">from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(\n    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n    token=hf_inference_api_key\n)</code></pre><p>Infine, creiamo e inizializziamo un oggetto per il framework LlamaIndex che mantiene la connessione a Mistral-Instruct. Il codice seguente proviene dal <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\"><u>post precedente sulla creazione di sistemi RAG usando LlamaIndex</u></a>.</p><p>Questo mette la connessione al LLM Mistral nella variabile <code>mistral_llm</code>.</p><h2 id=\"information-retrieval-with-qdrant-and-jina-embeddings\"><strong>Recupero delle Informazioni con Qdrant e Jina Embeddings</strong></h2><p>Per configurare il sistema di recupero, avrai bisogno di una chiave API di Jina Embeddings. Puoi ottenerne una gratuitamente con un budget prepagato di 1 milione di token sul <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">sito web di Jina Embeddings</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure><p>Inserisci la tua chiave Jina Embeddings in una variabile per uso futuro:</p><pre><code class=\"language-python\">jina_emb_api_key = \"&lt;your Jina Embeddings API key&gt;\"</code></pre><p>Quindi, crea un oggetto connettore usando LlamaIndex per il server Jina Embeddings, selezionando specificamente il modello monolingue inglese:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_emb_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n)</code></pre><h3 id=\"load-text-data\"><strong>Caricamento dei dati testuali</strong></h3><p>Successivamente, caricheremo il documento e lo divideremo in paragrafi. Prima, scarichiamo il PDF dal sito della Casa Bianca nella variabile <code>pdf_data</code>:</p><pre><code class=\"language-python\">import urllib.request\n\nuri = \"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"\npdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Poi, elaboreremo il PDF pagina per pagina, estraendo il testo e suddividendolo in paragrafi separando su doppie interruzioni di riga. Questi vengono memorizzati nella lista <code>text_paras</code>:</p><pre><code class=\"language-python\">import regex as re\nfrom io import BytesIO, StringIO\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfparser import PDFParser\n\ntext_paras = []\nparser = PDFParser(BytesIO(pdf_data))\ndoc = PDFDocument(parser)\nrsrcmgr = PDFResourceManager()\nfor page in PDFPage.create_pages(doc):\n    output_string = StringIO()\n    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    interpreter.process_page(page)\n    page_text = output_string.getvalue()\n    text_paras.extend(re.split(r'\\n\\s*\\n', page_text))\n</code></pre><p>Verifica che tutto sia stato caricato:</p><pre><code class=\"language-python\">assert len(text_paras) == 615</code></pre><p>Successivamente, convertiremo questa lista di testi brevi in <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io\">oggetti Document di LlamaIndex</a>:</p><pre><code class=\"language-python\">from llama_index.core.readers import StringIterableReader\n\nrag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>E puoi ispezionare il testo:</p><pre><code class=\"language-python\">print(rag_docs[0].text)</code></pre><p>Risultato:</p><pre><code class=\"language-text\">NATIONAL ARTIFICIAL INTELLIGENCE \nRESEARCH AND DEVELOPMENT \nSTRATEGIC PLAN \n2023 UPDATE</code></pre><h3 id=\"set-up-a-qdrant-vector-database\"><strong>Configurazione di un Database Vettoriale Qdrant</strong></h3><p>In questo articolo, useremo il <a href=\"https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io\"><u>Qdrant Vector Search Cloud</u></a> per implementare il database del sistema RAG. Il piano gratuito di Qdrant include 1GB di storage, che è più che sufficiente per questo tutorial.</p><p>Dovrai <a href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><u>creare un account sul sito web di Qdrant Cloud</u></a> prima di continuare.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Vector Search Database | Qdrant Cloud</div><div class=\"kg-bookmark-description\">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cloud.qdrant.io/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Qdrant Cloud</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cloud.qdrant.io/social.png\" alt=\"\"></div></a></figure><p>Una volta che hai un account e hai effettuato l'accesso, dovrai creare un cluster. Segui le <a href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>istruzioni \"quick start\" sul sito web di Qdrant</u></a> per configurare un cluster gratuito e ottenere un'API e il nome del server host Qdrant.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Quickstart - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Edit on GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png\" alt=\"\"></div></a></figure><p>Memorizza la chiave e l'hostname in variabili:</p><pre><code class=\"language-python\">qdrant_api_key = \"&lt;your API key&gt;\"\nqdrant_server = \"https://&lt;your server&gt;\"</code></pre><p>Successivamente, dovremo importare i componenti rilevanti dai pacchetti <code>qdrant_client</code> e <code>llama_index</code>:</p><pre><code class=\"language-python\">import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nclient = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\nvector_store = QdrantVectorStore(client=client, collection_name=\"NTSC\")</code></pre><p>Questo crea una collezione chiamata <code>NTSC</code> nel tuo cluster gratuito.</p><p>Puoi consultare la <a href=\"https://qdrant.tech/documentation/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>documentazione di Qdrant</u></a> per informazioni sulla costruzione di un archivio dati locale in memoria o su disco e sull'hosting della tua istanza server Qdrant in un container docker tramite API web.</p><h3 id=\"complete-the-rag-system\"><strong>Completare il sistema RAG</strong></h3><p>Ora assembleremo questi componenti in un sistema RAG completo utilizzando il codice boilerplate per LlamaIndex:</p><pre><code class=\"language-python\">from llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core import (\n\t\tVectorStoreIndex,\n\t\tServiceContext,\n\t\tget_response_synthesizer,\n)\n\n# set up the service and storage contexts\nservice_context = ServiceContext.from_defaults(\n&nbsp;&nbsp;&nbsp;&nbsp;llm=mixtral_llm, embed_model=jina_embedding_model\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# create an index\nindex = VectorStoreIndex.from_documents(\n&nbsp;&nbsp;&nbsp;&nbsp;rag_docs, storage_context=storage_context, service_context=service_context\n)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n&nbsp;&nbsp;&nbsp;&nbsp;service_context=service_context,\n&nbsp;&nbsp;&nbsp;&nbsp;text_qa_template=qa_prompt,\n&nbsp;&nbsp;&nbsp;&nbsp;response_mode=\"compact\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever,\n&nbsp;&nbsp;&nbsp;&nbsp;response_synthesizer=response_synthesizer,\n)\n</code></pre><p>Questo sistema utilizzerà Jina Embeddings per indicizzare i paragrafi dalla nostra fonte di testo e inserirà le due migliori corrispondenze nel contesto di ogni prompt. Per passare ad esempio alle tre migliori corrispondenze, basta modificare l'inizializzazione di <code>VectorIndexRetriever</code> sopra per avere un parametro <code>similarity_top_k</code> diverso:</p><pre><code class=\"language-python\"># configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3,\n)</code></pre><p>Ora il sistema è pronto per l'uso.</p><h3 id=\"query-a-document\"><strong>Interrogare un Documento</strong></h3><p>Proviamo una query semplice:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat is the Biden Administration's policy with regard to AI?&nbsp;\n\"\"\")\nprint(response.response)</code></pre><p>Il risultato (il tuo potrebbe non essere identico):</p><pre><code class=\"language-text\">The Biden Administration prioritizes responsible AI systems that&nbsp;\nare ethical, trustworthy, and safe, and serve the public good. \nThe Administration has committed to substantial funding for AI \nR&amp;D, and has established the National AI Initiative Office to \ncoordinate AI activities across the federal government.&nbsp;\n\nThe Administration's AI policy focuses on developing evaluative\ntechniques for AI, workforce development, public-private\npartnerships, and international collaboration in AI research.</code></pre><p>O qualcosa di più specifico:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat protections does the AI Bill of Rights propose to offer?\n\"\"\")\n\nprint(response.response)</code></pre><pre><code class=\"language-text\">The AI Bill of Rights proposes five core protections: Safe and&nbsp;\nEffective Systems; Algorithmic Discrimination Protections; Data&nbsp;\nPrivacy; Notice and Explanation; and Human Alternatives,&nbsp;\nConsideration, and Fallback.</code></pre><p>O anche molto specifico:</p><pre><code class=\"language-python\">response = query_engine.query(\"Who is Kei Koizumi?\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">Kei Koizumi is the Principal Deputy Director for Policy at the Office&nbsp;\nof Science and Technology Policy (OSTP).</code></pre><p>Puoi anche fare domande più fantasiose:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat rights will AI's receive under President Biden's proposed&nbsp;\nAI Bill of Rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">The proposed AI Bill of Rights does not grant rights to AI systems, but&nbsp;\nrather outlines principles to ensure that AI technologies are&nbsp;\ndeveloped and used in a manner that respects human rights and values.&nbsp;\nThe principles include safety, transparency, fairness, disclosure,&nbsp;\ndata privacy, non-discrimination, and the ability for individuals to&nbsp;\nopt out of certain AI systems.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhy is President Biden's proposing an AI Bill of Rights?\nDoes AI really need rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">President Biden's proposed AI Bill of Rights aims to ensure that AI&nbsp;\ntechnologies are developed and used ethically, legally, and with&nbsp;\nsocietal considerations in mind. The Bill of Rights is not granting&nbsp;\n\"rights\" to AI, but rather setting guidelines for the responsible&nbsp;\ndevelopment and deployment of AI systems to protect individuals and&nbsp;\ncommunities from potential negative impacts.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nHas Donald Trump weighed in on AI?\nWill he Make Humans Great Again?\n\"\"\")\nprint(response.response)\n</code></pre><pre><code class=\"language-text\">No information. The context information does not mention Donald&nbsp;\nTrump's views on AI.</code></pre><h2 id=\"jina-ai-and-qdrant\"><strong>Jina AI e Qdrant</strong></h2><p>La finestra di contesto di 8k token di Jina Embeddings lo rende particolarmente adatto alla generazione aumentata dal recupero perché può lavorare con blocchi di testo molto più grandi. Mentre altre applicazioni RAG spesso devono memorizzare singole frasi o coppie di frasi, in questo tutorial abbiamo potuto utilizzare interi paragrafi senza preoccuparci della loro dimensione.</p><p>Questo riduce il numero di elementi di testo che devono essere memorizzati per trasportare le stesse informazioni, abbassando i costi di calcolo, risparmiando memoria e rendendo il recupero più veloce.</p><p>Il database vettoriale di Qdrant è semplice da configurare, veloce ed economico e, come hai visto, bastano poche righe di Python per integrarlo in un sistema RAG.</p><p>Per saperne di più sul database pronto per l'AI di Qdrant e sulle offerte cloud, puoi <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>visitare il loro sito web</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qdrant - Vector Database</div><div class=\"kg-bookmark-description\">Qdrant è un Vector Database e un Motore di Ricerca Vettoriale Open-Source scritto in Rust. Fornisce un servizio di ricerca per similarità vettoriale veloce e scalabile con API conveniente.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Vector Database</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/images/previews/social-preview-P.png\" alt=\"\"></div></a></figure><h2 id=\"get-in-touch\"><strong>Contattaci</strong></h2><p>Jina AI si impegna a portare tecnologie di AI affidabili ed economiche alle imprese di ogni dimensione e tipo. Ci piacerebbe sentire i tuoi casi d'uso e aiutarti a integrare l'AI nei tuoi processi aziendali. Per maggiori informazioni sulle offerte di Jina AI e per contattarci, visita il <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>sito web di Jina AI</u></a> o unisciti alla nostra <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>community su Discord</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Best Embeddings and Perfect Prompts</div><div class=\"kg-bookmark-description\">Jina AI fornisce API di embedding e ottimizzatore di prompt di prima classe, facilitando lo sviluppo di applicazioni AI multimodali.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-author\">Best Embeddings and Perfect Prompts</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Unisciti al Server Discord di Jina AI!</div><div class=\"kg-bookmark-description\">Scopri la community Jina AI su Discord - interagisci con altri 4493 membri e goditi chat vocali e testuali gratuite.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://discord.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Discord</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512\" alt=\"\"></div></a></figure>",
  "comment_id": "65ddadd039a5b70001810ea3",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-02-27T10:39:28.000+01:00",
  "updated_at": "2024-03-06T15:41:18.000+01:00",
  "published_at": "2024-03-04T16:00:13.000+01:00",
  "custom_excerpt": "Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/build-a-rag-system-with-jina-embeddings-and-qdrant/",
  "excerpt": "Crea un sistema RAG con Jina Embeddings v2, database vettoriale Qdrant, LlamaIndex e Mistral LLM.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Promotional graphic with \"EMBEDDINGS 2\" and Drant logo separated by a red cross, symbolizing a partnership.",
  "feature_image_caption": null
}
{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1: Un vero modello di embedding multimodale per testo e immagini",
  "html": "<p>Jina CLIP v1 (<code>jina-clip-v1</code>) √® un nuovo modello di embedding multimodale che estende le capacit√† del <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">modello CLIP originale</a> di OpenAI. Con questo nuovo modello, gli utenti hanno un unico modello di embedding che offre prestazioni allo stato dell'arte sia nel recupero di solo testo che nel recupero cross-modale testo-immagine. Jina AI ha migliorato le prestazioni di OpenAI CLIP del 165% nel recupero di solo testo e del 12% nel recupero immagine-immagine, con prestazioni identiche o leggermente migliori nelle attivit√† di testo-immagine e immagine-testo. Queste prestazioni migliorate rendono Jina CLIP v1 indispensabile per lavorare con input multimodali.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> migliora OpenAI CLIP in <a href=\"#compare_table\" rel=\"noreferrer\">ogni categoria di recupero</a>.</div></div><p>In questo articolo, discuteremo prima i limiti del modello CLIP originale e come li abbiamo affrontati utilizzando un metodo unico di co-training. Poi, dimostreremo l'efficacia del nostro modello su vari benchmark di recupero. Infine, forniremo istruzioni dettagliate su come gli utenti possono iniziare con Jina CLIP v1 tramite la nostra API Embeddings e Hugging Face.</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">L'Architettura CLIP per l'AI Multimodale</h2><p>A gennaio 2021, OpenAI ha rilasciato il modello <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a> (Contrastive Language‚ÄìImage Pretraining). CLIP ha un'architettura semplice ma ingegnosa: combina due modelli di embedding, uno per i testi e uno per le immagini, in un unico modello con un unico spazio di embedding di output. I suoi embedding di testo e immagine sono direttamente confrontabili tra loro, rendendo la distanza tra un embedding di testo e un embedding di immagine proporzionale a quanto bene quel testo descrive l'immagine, e viceversa.</p><p>Questo si √® dimostrato molto utile nel recupero di informazioni multimodali e nella classificazione zero-shot delle immagini. Senza ulteriore training specifico, CLIP ha ottenuto buoni risultati nel collocare le immagini in categorie con etichette in linguaggio naturale.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"Diagram illustrating image to text translation using an astronaut on Mars with a red moon as an example.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il modello di embedding del testo nel CLIP originale era una rete neurale personalizzata con solo 63 milioni di parametri. Per quanto riguarda le immagini, OpenAI ha rilasciato CLIP con una selezione di modelli <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> e <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT</a>. Ogni modello √® stato pre-addestrato per la sua modalit√† individuale e poi addestrato con immagini con didascalie per produrre embedding simili per coppie immagine-testo preparate.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"Flowchart with text &quot;Embedding Space&quot;, linked to &quot;Image Encoder&quot; and &quot;Text Encoder&quot;, with a &quot;Distracted boyfriend&quot; label.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo approccio ha prodotto risultati impressionanti. Particolarmente notevoli sono le sue prestazioni nella classificazione zero-shot. Per esempio, anche se i dati di training non includevano immagini etichettate di <a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">astronauti</a>, CLIP poteva identificare correttamente le immagini di astronauti basandosi sulla sua comprensione di concetti correlati nei testi e nelle immagini.</p><p>Tuttavia, CLIP di OpenAI ha due importanti limitazioni:</p><ul><li>La prima √® la sua capacit√† di input testuale molto limitata. Pu√≤ accettare un massimo di 77 token in input, ma <a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">l'analisi empirica mostra</a> che nella pratica non usa pi√π di 20 token per produrre i suoi embedding. Questo perch√© CLIP √® stato addestrato da immagini con didascalie, e le didascalie tendono ad essere molto brevi. Questo contrasta con gli attuali modelli di embedding del testo che supportano diverse migliaia di token.</li><li>Secondo, le prestazioni dei suoi embedding testuali in scenari di recupero di solo testo sono molto scarse. Le didascalie delle immagini sono un tipo molto limitato di testo e non riflettono l'ampia gamma di casi d'uso che ci si aspetterebbe che un modello di embedding del testo supporti.</li></ul><p>Nella maggior parte dei casi d'uso reali, il recupero di solo testo e di immagine-testo sono combinati o almeno entrambi sono disponibili per le attivit√†. Mantenere un secondo modello di embedding per attivit√† di solo testo raddoppia effettivamente le dimensioni e la complessit√† del framework AI.</p><p>Il nuovo modello di Jina AI affronta direttamente questi problemi, e <code>jina-clip-v1</code> sfrutta i progressi fatti negli ultimi anni per portare prestazioni allo stato dell'arte in compiti che coinvolgono tutte le combinazioni di modalit√† testo e immagine.</p><h2 id=\"introducing-jina-clip-v1\">Introduzione a Jina CLIP v1</h2><p>Jina CLIP v1 mantiene lo schema originale di OpenAI CLIP: due modelli co-addestrati per produrre output nello stesso spazio di embedding.</p><p>Per la codifica del testo, abbiamo adattato l'architettura <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> utilizzata nei modelli <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2</a>. Questa architettura supporta una finestra di input di 8k token allo stato dell'arte e produce vettori a 768 dimensioni, producendo embedding pi√π accurati da testi pi√π lunghi. Questo √® pi√π di 100 volte i 77 token di input supportati nel modello CLIP originale.</p><p>Per gli embedding delle immagini, stiamo utilizzando l'ultimo modello della Beijing Academy for Artificial Intelligence: il modello <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code></a>. Abbiamo empiricamente confrontato diversi modelli di AI per immagini, testandoli in contesti cross-modali con pre-training simile, e <code>EVA-02</code> ha chiaramente superato gli altri. √à anche paragonabile all'architettura Jina BERT nelle dimensioni del modello, quindi i carichi di calcolo per le attivit√† di elaborazione di immagini e testo sono approssimativamente identici.</p><p>Queste scelte producono importanti benefici per gli utenti:</p><ul><li>Migliori prestazioni su tutti i benchmark e tutte le combinazioni modali, e specialmente grandi miglioramenti nelle prestazioni di embedding di solo testo.</li><li>Le prestazioni empiricamente superiori di <code>EVA-02</code> sia nei compiti immagine-testo che solo immagine, con il vantaggio aggiunto dell'addestramento addizionale di Jina AI, che migliora le prestazioni solo immagine.</li><li>Supporto per input di testo molto pi√π lunghi. Il supporto per input di <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">8k token di Jina Embeddings</a> rende possibile elaborare informazioni testuali dettagliate e correlarle con le immagini.</li><li>Un grande risparmio netto in spazio, calcolo, manutenzione del codice e complessit√† perch√© questo modello multimodale √® altamente performante anche in scenari non multimodali.</li></ul><h3 id=\"training\">Training</h3><p>Parte della nostra ricetta per l'AI multimodale ad alte prestazioni sono i nostri dati di training e la procedura. Notiamo che la lunghezza molto breve dei testi usati nelle didascalie delle immagini √® la causa principale delle scarse prestazioni di solo testo nei modelli stile CLIP, e il nostro training √® esplicitamente progettato per rimediare a questo.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"Flowchart illustrating optimization of text and caption-image similarity in three tasks, using a model and encoders, ending i\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il training avviene in tre fasi:</p><ol><li>Utilizzare dati di immagini con didascalie per imparare ad allineare gli embedding di immagini e testo, intervallati con coppie di testi con significati simili. Questo co-training ottimizza congiuntamente per i due tipi di compiti. Le prestazioni di solo testo del modello diminuiscono durante questa fase, ma non tanto quanto se avessimo addestrato solo con coppie immagine-testo.</li><li>Addestrare utilizzando dati sintetici che allineano le immagini con testi pi√π grandi, generati da un modello AI, che descrive l'immagine. Continuare l'addestramento con coppie di solo testo allo stesso tempo. Durante questa fase, il modello impara a prestare attenzione a testi pi√π grandi in congiunzione con le immagini.</li><li>Utilizzare triplette di testo con <a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">hard negative</a> per migliorare ulteriormente le prestazioni di solo testo imparando a fare distinzioni semantiche pi√π fini. Allo stesso tempo, continuare l'addestramento utilizzando coppie sintetiche di immagini e testi lunghi. Durante questa fase, le prestazioni di solo testo migliorano drasticamente senza che il modello perda alcuna capacit√† immagine-testo.</li></ol><p>Per maggiori informazioni sui dettagli dell'addestramento e dell'architettura del modello, leggete <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">il nostro recente articolo</a>:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Il Contrastive Language-Image Pretraining (CLIP) √® ampiamente utilizzato per addestrare modelli ad allineare immagini e testi in uno spazio di embedding comune mappandoli in vettori di dimensioni fisse. Questi modelli sono fondamentali per il recupero di informazioni multimodali e attivit√† correlate. Tuttavia, i modelli CLIP generalmente hanno prestazioni inferiori nei task di solo testo rispetto ai modelli specializzati per il testo. Questo crea inefficienze per i sistemi di recupero delle informazioni che mantengono embedding e modelli separati per attivit√† di solo testo e multimodali. Proponiamo un nuovo metodo di addestramento contrastivo multi-task per affrontare questo problema, che utilizziamo per addestrare il modello jina-clip-v1 per raggiungere prestazioni allo stato dell'arte sia nei task di recupero testo-immagine che testo-testo.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">Nuovo Stato dell'Arte negli Embedding Multimodali</h2><p>Abbiamo valutato le prestazioni di Jina CLIP v1 in task di solo testo, solo immagini e cross-modali che coinvolgono entrambe le modalit√† di input. Abbiamo utilizzato il <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">benchmark di recupero MTEB</a> per valutare le prestazioni di solo testo. Per i task di sole immagini, abbiamo utilizzato il benchmark <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a>. Per i task cross-modali, valutiamo su <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a> e <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a>, che sono inclusi nel <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a>.</p><p>I risultati sono riassunti nella tabella seguente:</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Da questi risultati si pu√≤ vedere che <code>jina-clip-v1</code> supera il CLIP originale di OpenAI in tutte le categorie, ed √® notevolmente migliore nel recupero di solo testo e sole immagini. In media su tutte le categorie, questo rappresenta un miglioramento del 46% nelle prestazioni.</p><p>Puoi trovare una valutazione pi√π dettagliata nel <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">nostro recente articolo</a>.</p><h2 id=\"getting-started-with-embeddings-api\">Iniziare con l'API degli Embedding</h2><p>Puoi facilmente integrare Jina CLIP v1 nelle tue applicazioni utilizzando la <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings API</a>.</p><p>Il codice seguente mostra come chiamare l'API per ottenere embedding per testi e immagini, utilizzando il pacchetto <code>requests</code> in Python. Passa una stringa di testo e un URL a un'immagine al server Jina AI e restituisce entrambe le codifiche.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚òùÔ∏è</div><div class=\"kg-callout-text\">Ricorda di sostituire <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> con una chiave API Jina attivata. Puoi ottenere una chiave di prova con un milione di token gratuiti dalla <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">pagina web Jina Embeddings</a>.</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">Integrazione con i principali Framework LLM</h3><p>Jina CLIP v1 √® gi√† disponibile per <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> e <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a>:</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>: Usa <code>JinaEmbedding</code> con la classe base <code>MultimodalEmbedding</code>, e invoca <code>get_image_embeddings</code> o <code>get_text_embeddings</code>.</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>: Usa <code>JinaEmbeddings</code>, e invoca <code>embed_images</code> o <code>embed_documents</code>.</li></ul><h3 id=\"pricing\">Prezzi</h3><p>Sia gli input di testo che di immagini vengono addebitati per consumo di token.</p><p>Per il testo in inglese, <a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">abbiamo calcolato empiricamente</a> che in media serviranno 1,1 token per ogni parola.</p><p>Per le immagini, contiamo il numero di tessere di 224x224 pixel necessarie per coprire l'immagine. Alcune di queste tessere potrebbero essere parzialmente vuote ma contano allo stesso modo. Ogni tessera costa 1.000 token da elaborare.</p><p><strong>Esempio</strong></p><p>Per un'immagine con dimensioni 750x500 pixel:</p><ol><li>L'immagine viene divisa in tessere di 224x224 pixel.<ol><li>Per calcolare il numero di tessere, prendere la larghezza in pixel e dividere per 224, poi arrotondare all'intero superiore. <br>     750/224 ‚âà 3,35 ‚Üí 4</li><li>Ripetere per l'altezza in pixel: <br>     500/224 ‚âà 2,23 ‚Üí 3</li></ol></li><li>Il numero totale di tessere richieste in questo esempio √®: <br>           4 (orizzontale) x 3 (verticale) = 12 tessere</li><li>Il costo sar√† 12 x 1.000 = 12.000 token </li></ol><h3 id=\"enterprise-support\">Supporto Enterprise</h3><p>Stiamo introducendo un nuovo beneficio per gli utenti che acquistano il piano Production Deployment con <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">11 miliardi di token</a>. Questo include:</p><ul><li>Tre ore di consulenza con i nostri team di prodotto e ingegneria per discutere i vostri casi d'uso e requisiti specifici.</li><li>Un notebook Python personalizzato progettato per il vostro caso d'uso RAG (Retrieval-Augmented Generation) o ricerca vettoriale, che dimostra come integrare i modelli di Jina AI nella vostra applicazione.</li><li>Assegnazione a un account executive e supporto email prioritario per garantire che le vostre esigenze siano soddisfatte tempestivamente ed efficacemente.</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Jina CLIP v1 Open-Source su Hugging Face</h2><p>Jina AI √® impegnata in una base di ricerca open-source, e per questo scopo, stiamo rendendo questo modello disponibile gratuitamente sotto una <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">licenza Apache 2.0</a>, su <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>.</p><p>Puoi trovare il codice di esempio per scaricare ed eseguire questo modello sul tuo sistema o installazione cloud nella pagina del modello Hugging Face per <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">Siamo in un viaggio per far avanzare e democratizzare l'intelligenza artificiale attraverso l'open source e la scienza aperta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">Riepilogo</h2><p>L'ultimo modello di Jina AI ‚Äî <code>jina-clip-v1</code> ‚Äî rappresenta un significativo avanzamento nei modelli di embedding multimodali, offrendo sostanziali miglioramenti delle prestazioni rispetto al CLIP di OpenAI. Con notevoli miglioramenti nei task di recupero di solo testo e sole immagini, oltre a prestazioni competitive nei task di testo-a-immagine e immagine-a-testo, si presenta come una soluzione promettente per casi d'uso complessi di embedding.</p><p>Questo modello attualmente supporta solo testi in lingua inglese a causa dei limiti di risorse. Stiamo lavorando per espandere le sue capacit√† ad altre lingue.</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzm√°n",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Il nuovo modello di embedding multimodale di Jina AI non solo supera OpenAI CLIP nel recupero di testo-immagine, ma √® anche un solido modello di embedding per immagini e allo stesso tempo un modello di embedding testuale all'avanguardia. Non hai pi√π bisogno di modelli diversi per diverse modalit√†.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
{
  "id": "671b96784821eb000165d2de",
  "uuid": "ec571b8c-d111-4d49-bad8-2836bd885f1c",
  "title": "Oltre CLIP: Come Jina-CLIP fa progredire la ricerca multimodale",
  "slug": "beyond-clip-how-jina-clip-advances-multimodal-search",
  "html": "<p>La ricerca multimodale, che combina testo e immagini in un'esperienza di ricerca integrata, ha acquisito slancio grazie a modelli come <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP di OpenAI</a>. Questi modelli colmano efficacemente il divario tra dati visivi e testuali, permettendoci di collegare immagini con testi pertinenti e viceversa.</p><p>Mentre CLIP e modelli simili sono potenti, hanno notevoli limitazioni, in particolare quando elaborano testi più lunghi o gestiscono relazioni testuali complesse. È qui che entra in gioco <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image</div><div class=\"kg-bookmark-description\">Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Progettato per affrontare queste sfide, <code>jina-clip-v1</code> offre una migliore comprensione del testo mantenendo robuste capacità di abbinamento testo-immagine. Fornisce una soluzione più snella per le applicazioni che utilizzano entrambe le modalità, semplificando il processo di ricerca e eliminando la necessità di gestire modelli separati per testo e immagini.</p><p>In questo post, esploreremo cosa porta <code>jina-clip-v1</code> alle applicazioni di ricerca multimodale, mostrando esperimenti che dimostrano come migliora sia l'accuratezza che la varietà dei risultati attraverso embedding integrati di testo e immagini.</p><h2 id=\"what-is-clip\">Che cos'è CLIP?</h2><p>CLIP (Contrastive Language–Image Pretraining) è un'architettura di modello AI sviluppata da OpenAI che collega testo e immagini apprendendo rappresentazioni congiunte. CLIP è essenzialmente un modello di testo e un modello di immagine saldati insieme — trasforma entrambi i tipi di input in uno spazio di embedding condiviso, dove testi e immagini simili sono posizionati vicini tra loro. CLIP è stato addestrato su un vasto dataset di coppie immagine-testo, permettendogli di comprendere la relazione tra contenuto visivo e testuale. Questo gli permette di generalizzare bene attraverso domini diversi, rendendolo altamente efficace in scenari di apprendimento zero-shot, come la generazione di didascalie o il recupero di immagini.</p><p>Dalla pubblicazione di CLIP, altri modelli come <a href=\"https://arxiv.org/abs/2303.15343?ref=jina-ai-gmbh.ghost.io\">SigLiP</a>, <a href=\"https://arxiv.org/abs/2111.07991?ref=jina-ai-gmbh.ghost.io\">LiT</a>, e <a href=\"https://arxiv.org/abs/2303.15389?ref=jina-ai-gmbh.ghost.io\">EvaCLIP</a> hanno ampliato le sue basi, migliorando aspetti come l'efficienza dell'addestramento, la scalabilità e la comprensione multimodale. Questi modelli spesso sfruttano dataset più grandi, architetture migliorate e tecniche di addestramento più sofisticate per spingere i limiti dell'allineamento testo-immagine, facendo progredire ulteriormente il campo dei modelli immagine-linguaggio.</p><p>Mentre CLIP <em>può</em> funzionare con il solo testo, ha significative limitazioni. Primo, è stato addestrato solo su brevi didascalie, non su testi lunghi, gestendo un massimo di circa 77 parole. Secondo, CLIP eccelle nel collegare testo a immagini ma fatica quando confronta testo con altro testo, come riconoscere che le stringhe <code>a crimson fruit</code> e <code>a red apple</code> possono riferirsi alla stessa cosa. È qui che brillano i modelli di testo specializzati, come <code>jina-embeddings-v3</code>.</p><p>Queste limitazioni complicano le attività di ricerca che coinvolgono sia testo che immagini, per esempio, un negozio online \"shop the look\" dove un utente può cercare prodotti di moda usando sia una stringa di testo che un'immagine. Quando si indicizzano i prodotti, è necessario elaborare ciascuno più volte - una volta per l'immagine, una volta per il testo e un'altra volta con un modello specifico per il testo. Allo stesso modo, quando un utente cerca un prodotto, il sistema deve cercare almeno due volte per trovare sia gli obiettivi testuali che quelli visivi:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png\" class=\"kg-image\" alt=\"Flowchart outlining &quot;Offline Indexing&quot; and &quot;Online Querying&quot; processes with labeled blocks and arrows for XML data interactio\" loading=\"lazy\" width=\"970\" height=\"1255\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png 970w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-jina-clip-v1-solves-clip%E2%80%99s-shortcomings\"><strong>Come </strong><code>jina-clip-v1</code><strong> Risolve le Carenze di CLIP</strong></h2><p>Per superare le limitazioni di CLIP, abbiamo creato <code>jina-clip-v1</code> per comprendere testi più lunghi e abbinare più efficacemente le query testuali sia ai testi che alle immagini. Cosa rende <code>jina-clip-v1</code> così speciale? Innanzitutto, utilizza un modello di comprensione del testo più intelligente (JinaBERT), aiutandolo a comprendere pezzi di testo più lunghi e complicati (come le descrizioni dei prodotti), non solo brevi didascalie (come i nomi dei prodotti). In secondo luogo, abbiamo addestrato <code>jina-clip-v1</code> per essere bravo in due cose contemporaneamente: sia nell'abbinare testo a immagini che nell'abbinare testo ad altri pezzi di testo.</p><p>Con OpenAI CLIP, questo non è il caso: sia per l'indicizzazione che per le query, è necessario invocare due modelli (CLIP per immagini e testi brevi come didascalie, un altro embedding di testo per testi più lunghi come descrizioni). Questo non solo aggiunge overhead, ma rallenta la ricerca, un'operazione che <em>dovrebbe</em> essere molto veloce. <code>jina-clip-v1</code> fa tutto questo in un unico modello, senza sacrifici in termini di velocità:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-22.png\" class=\"kg-image\" alt=\"Flowchart of JaclinQ's offline indexing and online querying processes, involving imagery and text analysis.\" loading=\"lazy\" width=\"2000\" height=\"2785\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/10/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo approccio unificato apre nuove possibilità che erano difficili con i modelli precedenti, potenzialmente ridefinendo il modo in cui affrontiamo la ricerca. In questo post, abbiamo condotto due esperimenti:</p><ul><li><strong>Migliorare i risultati di ricerca combinando la ricerca di testo e immagini</strong>: Possiamo combinare ciò che <code>jina-clip-v1</code> comprende dal testo con ciò che comprende dalle immagini? Cosa succede quando mescoliamo questi due tipi di comprensione? L'aggiunta di informazioni visive cambia i nostri risultati di ricerca? In breve, possiamo ottenere risultati migliori se cerchiamo con testo e immagini contemporaneamente?</li><li><strong>Usare le immagini per diversificare i risultati di ricerca</strong>: La maggior parte dei motori di ricerca massimizza le corrispondenze testuali. Ma possiamo usare la comprensione delle immagini di <code>jina-clip-v1</code> come un \"visual shuffle\"? Invece di mostrare solo i risultati più rilevanti, potremmo includere quelli visivamente diversi. Non si tratta di trovare più risultati correlati – si tratta di mostrare una gamma più ampia di prospettive, anche se sono meno strettamente correlate. Facendo questo, potremmo scoprire aspetti di un argomento che non avevamo considerato prima. Per esempio, nel contesto della ricerca di moda, se un utente cerca \"vestito da cocktail multicolore\", vuole che i primi risultati siano tutti uguali (cioè corrispondenze <em>molto</em> strette), o una più ampia varietà tra cui scegliere (tramite visual shuffle)?</li></ul><p>Entrambi gli approcci sono preziosi in una varietà di casi d'uso in cui gli utenti potrebbero cercare con testo o immagini, come nell'e-commerce, nei media, nell'arte e design, nell'imaging medico e oltre.</p><h2 id=\"averaging-text-and-image-embeddings-for-above-average-performance\">Calcolare la Media degli Embedding di Testo e Immagine per Prestazioni Superiori alla Media</h2><p>Quando un utente invia una query (solitamente come stringa di testo), possiamo usare la torre di testo di <code>jina-clip-v1</code> per codificare la query in un embedding di testo. La forza di <code>jina-clip-v1</code> risiede nella sua capacità di comprendere sia testo che immagini allineando i segnali testo-testo e testo-immagine nello stesso spazio semantico.</p><p>Possiamo migliorare i risultati di recupero se combiniamo gli embedding pre-indicizzati di testo e immagini di ciascun prodotto calcolandone la media?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png\" class=\"kg-image\" alt=\"Flowchart on a black background detailing text and image embedding processes with a black knit midi dress photo example.\" loading=\"lazy\" width=\"995\" height=\"359\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-28.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png 995w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo crea una singola rappresentazione che include sia informazioni testuali (es. descrizione del prodotto) che visive (es. immagine del prodotto). Possiamo quindi usare l'embedding della query testuale per cercare queste rappresentazioni combinate. Come influisce questo sui nostri risultati di ricerca?</p><p>Per scoprirlo, abbiamo utilizzato il dataset <a href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\">Fashion200k</a>, un dataset su larga scala creato specificamente per compiti relativi al recupero di immagini di moda e alla comprensione cross-modale. Consiste in oltre 200.000 immagini di articoli di moda, come vestiti, scarpe e accessori, insieme alle corrispondenti descrizioni dei prodotti e metadati.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - xthan/fashion-200k: Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\"</div><div class=\"kg-bookmark-description\">Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\" - xthan/fashion-200k</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">xthan</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2116651d448aec6ea0508f5fdb123e6292fa00bfb1cf8fb6f3468cbe761da769/xthan/fashion-200k\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Abbiamo ulteriormente categorizzato ogni elemento in una categoria ampia (per esempio, <code>dress</code>) e una categoria dettagliata (come <code>knit midi dress</code>).</p><h3 id=\"analyzing-three-retrieval-methods\"><strong>Analisi di Tre Metodi di Recupero</strong></h3><p>Per verificare se la media degli embedding di testo e immagini producesse risultati migliori, abbiamo sperimentato tre tipi di ricerca, ciascuno dei quali utilizza una stringa di testo (es. <code>red dress</code>) come query:</p><ul><li><strong>Query to Description usando embedding testuali:</strong> Ricerca nelle descrizioni dei prodotti basata su embedding testuali.</li><li><strong>Query to Image usando ricerca cross-modale:</strong> Ricerca nelle immagini dei prodotti basata su embedding di immagini.</li><li><strong>Query to Average Embedding:</strong> Ricerca negli embedding mediati di descrizioni e immagini dei prodotti.</li></ul><p>Abbiamo prima indicizzato l'intero dataset, e poi generato casualmente 1.000 query per valutare le prestazioni. Abbiamo codificato ogni query in un embedding testuale e fatto il matching dell'embedding separatamente, in base ai metodi descritti sopra. Abbiamo misurato l'accuratezza in base a quanto le categorie dei prodotti restituiti corrispondevano alla query di input.</p><p>Quando abbiamo usato la query <code>multicolor henley t-shirt dress</code>, la ricerca <strong>Query-to-Description</strong> ha ottenuto la precisione top-5 più alta, ma gli ultimi tre vestiti in classifica erano visivamente identici. Questo è meno che ideale, poiché una ricerca efficace dovrebbe bilanciare rilevanza e diversità per catturare meglio l'attenzione dell'utente.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png\" class=\"kg-image\" alt=\"Array of five unique dresses, categorized as casual and day, arranged in a row on a white background with named tags for easy\" loading=\"lazy\" width=\"2000\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La ricerca cross-modale <strong>Query-to-Image</strong> ha utilizzato la stessa query e ha preso l'approccio opposto, presentando una collezione altamente diversificata di vestiti. Mentre ha abbinato due risultati su cinque con la categoria ampia corretta, nessuno corrispondeva alla categoria dettagliata.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png\" class=\"kg-image\" alt=\"Variety of women's clothing items including short and long-sleeved tops and casual to maxi dresses with color swatches.\" loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La <strong>ricerca con embedding medi di testo e immagini</strong> ha prodotto il miglior risultato: tutti e cinque i risultati corrispondevano alla categoria ampia, e due su cinque corrispondevano alla categoria dettagliata. Inoltre, gli elementi visivamente duplicati sono stati eliminati, fornendo una selezione più varia. L'utilizzo degli embedding di testo per cercare negli embedding medi di testo e immagini sembra mantenere la qualità della ricerca incorporando al contempo indizi visivi, portando a risultati più diversificati e completi.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png\" class=\"kg-image\" alt=\"Showcase of various women's dresses, including a multicolor henley t-shirt dress and a pink Missoni dress, labeled with categ\" loading=\"lazy\" width=\"2000\" height=\"513\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"scaling-up-evaluating-with-more-queries\"><strong>Aumentare la Scala: Valutazione con Più Query</strong></h3><p>Per vedere se questo avrebbe funzionato su scala più ampia, abbiamo continuato l'esperimento su ulteriori categorie ampie e dettagliate. Abbiamo eseguito diverse iterazioni, recuperando un numero diverso di risultati (\"k-values\") ogni volta.</p><p>Sia per le categorie ampie che per quelle dettagliate, la <strong>Query to Average Embedding</strong> ha costantemente ottenuto la precisione più alta per tutti i k-values (10, 20, 50, 100). Questo dimostra che combinare gli embedding di testo e immagini fornisce i risultati più accurati per recuperare elementi rilevanti, indipendentemente dal fatto che la categoria sia ampia o specifica:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png\" class=\"kg-image\" alt=\"Comparative chart of 'Broad Precision@K' and 'Fine-grained Precision@K' showing different precision values for query-related \" loading=\"lazy\" width=\"2000\" height=\"836\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>k</strong></th>\n<th><strong>Search Type</strong></th>\n<th><strong>Broad Category Precision (cosine similarity)</strong></th>\n<th><strong>Fine-grained Category Precision (cosine similarity)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>Query to Description</td>\n<td>0.9026</td>\n<td>0.2314</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Image</td>\n<td>0.7614</td>\n<td>0.2037</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9230</strong></td>\n<td><strong>0.2711</strong></td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Description</td>\n<td>0.9150</td>\n<td>0.2316</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Image</td>\n<td>0.7523</td>\n<td>0.1964</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9229</strong></td>\n<td><strong>0.2631</strong></td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Description</td>\n<td>0.9134</td>\n<td>0.2254</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Image</td>\n<td>0.7418</td>\n<td>0.1750</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9226</strong></td>\n<td><strong>0.2390</strong></td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Description</td>\n<td>0.9092</td>\n<td>0.2139</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Image</td>\n<td>0.7258</td>\n<td>0.1675</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9150</strong></td>\n<td><strong>0.2286</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<ul><li><strong>Query to Description usando embedding testuali</strong> ha funzionato bene in entrambe le categorie ma è rimasto leggermente indietro rispetto all'approccio con embedding medi. Questo suggerisce che le descrizioni testuali da sole forniscono informazioni preziose, in particolare per categorie più ampie come \"dress\", ma potrebbero mancare della sottigliezza necessaria per una classificazione dettagliata precisa (es. distinguere tra diversi tipi di vestiti).</li><li><strong>Query to Image usando ricerca cross-modale</strong> ha costantemente avuto la precisione più bassa in entrambe le categorie. Questo suggerisce che mentre le caratteristiche visive possono aiutare a identificare categorie ampie, sono meno efficaci nel catturare le distinzioni dettagliate di specifici capi di moda. La sfida di distinguere categorie dettagliate puramente da caratteristiche visive è particolarmente evidente, dove le differenze visive possono essere sottili e richiedere un contesto aggiuntivo fornito dal testo.</li><li>Nel complesso, combinare informazioni testuali e visive (tramite <strong>embedding medi</strong>) ha ottenuto un'alta precisione sia nelle attività di recupero di moda ampie che dettagliate. Le descrizioni testuali giocano un ruolo importante, specialmente nell'identificare categorie ampie, mentre le immagini da sole sono meno efficaci in entrambi i casi.</li></ul><p>In generale, la precisione è stata molto più alta per le categorie ampie rispetto alle categorie dettagliate, principalmente perché gli elementi nelle categorie ampie (es. <code>dress</code>) sono più rappresentati nel dataset rispetto alle categorie dettagliate (es. <code>henley dress</code>), semplicemente perché quest'ultime sono un sottoinsieme delle prime. Per sua stessa natura, una categoria ampia è più facile da generalizzare rispetto a una categoria dettagliata. Al di fuori dell'esempio della moda, è semplice identificare che qualcosa, in generale, è un uccello. È molto più difficile identificarlo come un <a href=\"https://www.youtube.com/watch?v=nPhVOZiPokA&ref=jina-ai-gmbh.ghost.io\">Vogelkop Superb Bird of Paradise</a>.</p><p>Un'altra cosa da notare è che l'informazione in una query testuale si abbina più facilmente ad altri testi (come nomi di prodotti o descrizioni), piuttosto che a caratteristiche visive. Pertanto, se un testo viene utilizzato come input, i testi sono un output più probabile rispetto alle immagini. Otteniamo i migliori risultati combinando sia immagini che testo (tramite la media degli embedding) nel nostro indice.</p><h2 id=\"retrieve-results-with-text-diversify-them-with-images\">Recuperare Risultati con il Testo; Diversificarli con le Immagini</h2><p>Nella sezione precedente, abbiamo toccato la questione dei risultati di ricerca visivamente duplicati. Nella ricerca, <em>la precisione da sola non è sempre sufficiente</em>. In molti casi, mantenere una lista ordinata concisa ma altamente rilevante e diversificata è più efficace, specialmente quando la query dell'utente è ambigua (per esempio, se un utente cerca<code>black jacket</code> — intendono una giacca da motociclista nera, un bomber, un blazer o qualche altro tipo?)</p><p>Ora, invece di sfruttare la capacità cross-modale di <code>jina-clip-v1</code>, useremo gli embedding testuali dalla sua torre di testo per la ricerca testuale iniziale, poi applicheremo gli embedding delle immagini dalla torre delle immagini come \"riordinatore visuale\" per diversificare i risultati della ricerca. Questo è illustrato nel diagramma qui sotto:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png\" class=\"kg-image\" alt=\"Flowchart detailing multimodal document text processing, with branches for text and image embedding and various processing pa\" loading=\"lazy\" width=\"975\" height=\"476\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png 975w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><ol><li>Prima, recuperare i primi k risultati della ricerca basati sugli embedding testuali.</li><li>Per ogni risultato di ricerca principale, estrarre le caratteristiche visive e raggrupparle usando gli embedding delle immagini.</li><li>Riordinare i risultati della ricerca selezionando un elemento da ogni cluster e presentare una lista diversificata all'utente.</li></ol><p>Dopo aver recuperato i primi cinquanta risultati, abbiamo applicato un clustering k-means leggero (k=5) agli embedding delle immagini, poi selezionato elementi da ogni cluster. La precisione delle categorie è rimasta coerente con le prestazioni Query-to-Description, poiché abbiamo usato la query-to-product category come metrica di misurazione. Tuttavia, i risultati ordinati hanno iniziato a coprire più aspetti diversi (come tessuto, taglio e motivo) con la diversificazione basata sulle immagini. Come riferimento, ecco l'esempio della t-shirt dress henley multicolore di prima:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png\" class=\"kg-image\" alt=\"Collection of t-shirt dresses categorized into casual and day, short and long sleeves, displayed in two rows.\" loading=\"lazy\" width=\"2000\" height=\"1484\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ora vediamo come la diversificazione influenza i risultati della ricerca utilizzando la ricerca con embedding testuali combinata con l'embedding delle immagini come riordinatore di diversificazione:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png\" class=\"kg-image\" alt=\"Five diverse dresses arranged in a row, categorized as various types including casual and day dresses, mini and short, and ma\" loading=\"lazy\" width=\"2000\" height=\"465\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I risultati ordinati provengono dalla ricerca basata sul testo ma iniziano a coprire \"aspetti\" più diversificati nei primi cinque esempi. Questo ottiene un effetto simile alla media degli embedding senza effettivamente mediarli.</p><p>Tuttavia, questo ha un costo: dobbiamo applicare un ulteriore passaggio di clustering dopo aver recuperato i primi k risultati, che aggiunge alcuni millisecondi extra, a seconda della dimensione del ranking iniziale. Inoltre, determinare il valore di k per il clustering k-means implica alcune ipotesi euristiche. Questo è il prezzo che paghiamo per una migliore diversificazione dei risultati!</p><h2 id=\"conclusion\">Conclusione</h2><p><code>jina-clip-v1</code> colma efficacemente il divario tra ricerca testuale e per immagini unificando entrambe le modalità in un unico modello efficiente. I nostri esperimenti hanno dimostrato che la sua capacità di elaborare input testuali più lunghi e complessi insieme alle immagini offre prestazioni di ricerca superiori rispetto ai modelli tradizionali come CLIP.</p><p>I nostri test hanno coperto vari metodi, incluso il matching di testo con descrizioni, immagini ed embedding mediati. I risultati hanno costantemente mostrato che la combinazione di embedding testuali e di immagini ha prodotto i migliori risultati, migliorando sia l'accuratezza che la diversità dei risultati di ricerca. Abbiamo anche scoperto che l'utilizzo degli embedding delle immagini come \"riordinatore visuale\" ha migliorato la varietà dei risultati mantenendo la rilevanza.</p><p>Questi progressi hanno implicazioni significative per le applicazioni del mondo reale in cui gli utenti cercano usando sia descrizioni testuali che immagini. Comprendendo simultaneamente entrambi i tipi di dati, <code>jina-clip-v1</code> semplifica il processo di ricerca, fornendo risultati più rilevanti e consentendo raccomandazioni di prodotti più diversificate. Questa capacità di ricerca unificata si estende oltre l'e-commerce per beneficiare la gestione degli asset multimediali, le biblioteche digitali e la curazione dei contenuti visivi, rendendo più facile scoprire contenuti rilevanti attraverso diversi formati.</p><p>Mentre <code>jina-clip-v1</code> supporta attualmente solo l'inglese, stiamo lavorando a <code>jina-clip-v2</code>. Seguendo le orme di <code>jina-embeddings-v3</code> e <code>jina-colbert-v2</code>, questa nuova versione sarà un recuperatore multimodale multilingue all'avanguardia che supporta 89 lingue. Questo aggiornamento aprirà nuove possibilità per attività di ricerca e recupero attraverso diversi mercati e industrie, rendendolo un modello di embedding più potente per applicazioni globali nell'e-commerce, nei media e oltre.</p>",
  "comment_id": "671b96784821eb000165d2de",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-25T15:00:40.000+02:00",
  "updated_at": "2024-10-30T19:14:11.000+01:00",
  "published_at": "2024-10-29T11:51:40.000+01:00",
  "custom_excerpt": "Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/beyond-clip-how-jina-clip-advances-multimodal-search/",
  "excerpt": "Scopri come Jina-CLIP migliora CLIP di OpenAI con una maggiore accuratezza nel recupero e risultati più diversificati attraverso embedding unificati di testo e immagini.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere.",
  "feature_image_caption": null
}
{
  "slug": "bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning",
  "id": "67066bd652567c0001d0f2cd",
  "uuid": "1130051f-f343-4eb2-9956-9b574c212704",
  "title": "Colmare le lacune linguistiche negli embedding multilingue attraverso l'apprendimento contrastivo",
  "html": "<p>Nei modelli multilingue, una delle sfide principali √® il \"<strong>language gap</strong>\" ‚Äî un fenomeno in cui frasi con lo stesso significato in lingue diverse non sono allineate o raggruppate cos√¨ strettamente come dovrebbero essere. Idealmente, un testo in una lingua e il suo equivalente in un'altra dovrebbero avere rappresentazioni simili ‚Äî ovvero embedding molto vicini tra loro ‚Äî permettendo alle applicazioni multilingue di operare in modo identico su testi in lingue diverse. Tuttavia, i modelli spesso rappresentano sottilmente la lingua di un testo, creando un \"language gap\" che porta a prestazioni subottimali tra le lingue.</p><p>In questo post, esploreremo questo language gap e come influenza le prestazioni nei modelli di text embedding. Abbiamo condotto esperimenti per valutare l'allineamento semantico per parafrasi nella stessa lingua e per traduzioni tra diverse coppie di lingue, utilizzando il nostro modello <code>jina-xlm-roberta</code> e il pi√π recente <code>jina-embeddings-v3</code>. Questi esperimenti rivelano quanto bene le frasi con significati simili o identici si raggruppano insieme in diverse condizioni di training.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-6.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Abbiamo anche sperimentato tecniche di training per migliorare l'allineamento semantico tra le lingue, in particolare l'introduzione di <strong>dati multilingue paralleli</strong> durante l'apprendimento contrastivo. In questo articolo, condivideremo le nostre intuizioni e risultati.</p><h2 id=\"multilingual-model-training-creates-and-reduces-the-language-gap\"><strong>Il Training dei Modelli Multilingue Crea e Riduce il Language Gap</strong></h2><p>Il training dei modelli di text embedding tipicamente coinvolge un processo multi-fase con due parti principali:</p><ol><li><a href=\"https://aclanthology.org/2023.acl-long.49/?ref=jina-ai-gmbh.ghost.io\"><strong>Masked Language Modeling</strong></a> (MLM): Il pre-training tipicamente coinvolge grandi quantit√† di testo in cui alcuni token sono mascherati casualmente. Il modello viene addestrato a predire questi token mascherati. Questa procedura insegna al modello i pattern della lingua o delle lingue nei dati di training, incluse le dipendenze di selezione tra token che potrebbero derivare dalla sintassi, dalla semantica lessicale e dai vincoli pragmatici del mondo reale.</li><li><a href=\"https://paperswithcode.com/task/contrastive-learning?ref=jina-ai-gmbh.ghost.io\"><strong>Contrastive Learning</strong></a>: Dopo il pre-training, il modello viene ulteriormente addestrato con dati curati o semi-curati per avvicinare gli embedding di testi semanticamente simili e (opzionalmente) allontanare quelli dissimili. Questo training pu√≤ utilizzare coppie, triplette o anche gruppi di testi la cui similarit√† semantica √® gi√† nota o almeno stimata in modo affidabile. Pu√≤ avere diverse sottofasi e ci sono varie strategie di training per questa parte del processo, con nuove ricerche pubblicate frequentemente e nessun chiaro consenso sull'approccio ottimale.</li></ol><p>Per capire come si crea il language gap e come pu√≤ essere chiuso, dobbiamo esaminare il ruolo di entrambe le fasi.</p><h3 id=\"masked-language-pretraining\"><strong>Masked Language Pretraining</strong></h3><p>Parte della capacit√† multilingue dei modelli di text embedding viene acquisita durante il pre-training.</p><p>Le parole cognate e i prestiti linguistici rendono possibile per il modello apprendere un certo allineamento semantico tra le lingue da grandi quantit√† di dati testuali. Per esempio, la parola inglese <em>banana</em> e la parola francese <em>banane</em> (e il tedesco <em>Banane</em>) sono frequenti e abbastanza simili nell'ortografia da permettere a un modello di embedding di apprendere che le parole che assomigliano a \"banan-\" hanno pattern distributivi simili tra le lingue. Pu√≤ sfruttare questa informazione per apprendere, in una certa misura, che anche altre parole che non si assomigliano tra le lingue hanno significati simili, e persino capire come vengono tradotte alcune strutture grammaticali.</p><p>Tuttavia, questo accade senza un training esplicito.</p><p>Abbiamo testato il modello <code>jina-xlm-roberta</code>, la base pre-addestrata di <code>jina-embeddings-v3</code>, per vedere quanto bene ha appreso le equivalenze tra lingue dal pre-training con masked language. Abbiamo tracciato le rappresentazioni bidimensionali <a href=\"https://pair-code.github.io/understanding-umap/?ref=jina-ai-gmbh.ghost.io\">UMAP delle frasi</a> di un insieme di frasi inglesi tradotte in tedesco, olandese, cinese semplificato e giapponese. I risultati sono nella figura seguente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png\" class=\"kg-image\" alt=\"Grafico a dispersione multilingue che mostra l'allineamento degli embedding di parole in cinque lingue su dimensioni UMAP.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_roberta_mlm_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Proiezione bidimensionale UMAP di una selezione di frasi inglesi e le loro traduzioni in tedesco, olandese, cinese e giapponese. Le linee grigie collegano le frasi non inglesi alle frasi inglesi da cui sono state tradotte.<br><br>Queste frasi tendono fortemente a formare cluster specifici per lingua nello spazio di embedding di <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-xlm-roberta</code>, sebbene si possano vedere alcuni outlier in questa proiezione che potrebbero essere un effetto collaterale della proiezione bidimensionale.</div></div><p>Si pu√≤ vedere che il pre-training ha raggruppato molto fortemente gli embedding delle frasi nella stessa lingua. Questa √® una proiezione in due dimensioni di una distribuzione in uno spazio di dimensioni molto pi√π elevate, quindi √® ancora possibile che, per esempio, una frase tedesca che √® una buona traduzione di una inglese possa ancora essere la frase tedesca il cui embedding √® pi√π vicino all'embedding della sua fonte inglese. Ma mostra che un embedding di una frase inglese √® probabilmente pi√π vicino a un'altra frase inglese che a una semanticamente identica o quasi identica in tedesco.</p><p>Nota anche come il tedesco e l'olandese formino cluster molto pi√π vicini rispetto ad altre coppie di lingue. Questo non √® sorprendente per due lingue relativamente strettamente imparentate. Il tedesco e l'olandese sono abbastanza simili da essere talvolta parzialmente comprensibili reciprocamente.</p><p>Anche il giapponese e il cinese sembrano pi√π vicini tra loro rispetto alle altre lingue. Sebbene non siano imparentati nello stesso modo, il giapponese scritto tipicamente usa i <em>kanji</em> (Êº¢Â≠ó), o <em>h√†nz√¨</em> in cinese<em>. </em>Il giapponese condivide la maggior parte di questi caratteri scritti con il cinese, e le due lingue condividono molte parole scritte con uno o pi√π kanji/h√†nz√¨ insieme. Dal punto di vista del MLM, questo √® lo stesso tipo di similarit√† visibile che c'√® tra olandese e tedesco.</p><p>Possiamo vedere questo \"language gap\" in modo pi√π semplice guardando solo due lingue con due frasi ciascuna:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png\" class=\"kg-image\" alt=\"Grafico che illustra le relazioni linguistiche con linee codificate a colori, punti dati per frasi in inglese e tedesco, e un &quot;MLM P\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image--8-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image--8-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Poich√© MLM sembra naturalmente raggruppare i testi per lingua, \"my dog is blue\" e \"my cat is red\" sono raggruppati insieme, lontani dai loro equivalenti tedeschi. A differenza del \"<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/?ref=jina-ai-gmbh.ghost.io\">modality gap</a>\" discusso in un post precedente, crediamo che questo derivi da similarit√† e dissimilarit√† superficiali tra le lingue: ortografie simili, uso delle stesse sequenze di caratteri nella scrittura e possibilmente similarit√† nella morfologia e nella struttura sintattica ‚Äî ordini delle parole comuni e modi comuni di costruire le parole.</p><p>In breve, in qualunque misura un modello stia apprendendo equivalenze tra lingue nel pre-training MLM, non √® sufficiente per superare una forte tendenza a raggruppare i testi per lingua. Lascia un ampio language gap.</p><h3 id=\"contrastive-learning\"><strong>Contrastive Learning</strong></h3><p>Idealmente, vogliamo che un modello di embedding sia indifferente alla lingua e codifichi solo significati generali nei suoi embedding. In un modello del genere, non vedremmo raggruppamenti per lingua e non avremmo language gap. Le frasi in una lingua dovrebbero essere molto vicine a buone traduzioni e lontane da altre frasi che significano qualcosa di diverso, anche nella stessa lingua, come nella figura seguente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png\" class=\"kg-image\" alt=\"Il grafico mostra &quot;Clustering by Meaning&quot; con etichette multilingue, enfatizzando concetti astratti su sfondo scuro.\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-10---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-10---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-10---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il pre-training MLM non raggiunge questo obiettivo, quindi utilizziamo tecniche aggiuntive di <em>contrastive learning</em> per migliorare la rappresentazione semantica dei testi negli embedding.</p><p>Il contrastive learning implica l'uso di coppie di testi che si sa essere simili o diversi nel significato, e triplette dove si sa che una coppia √® pi√π simile dell'altra. I pesi vengono aggiustati durante il training per riflettere questa relazione nota tra coppie e triplette di testi.</p><p>Ci sono 30 lingue rappresentate nel nostro dataset di contrastive learning, ma il 97% delle coppie e triplette sono in una sola lingua, con solo il 3% che coinvolge coppie o triplette tra lingue diverse. Ma questo 3% √® sufficiente a produrre un risultato drammatico: Gli embedding mostrano pochissimo raggruppamento per lingua e i testi semanticamente simili producono embedding vicini indipendentemente dalla loro lingua, come mostrato nella proiezione UMAP degli embedding da <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png\" class=\"kg-image\" alt=\"Scatter plot on black background showing language distribution post-contrastive training with UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_contrastive_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Per confermare questo, abbiamo misurato la Correlazione di Spearman delle rappresentazioni generate da <code>jina-xlm-roberta</code> e <code>jina-embeddings-v3</code> sul dataset STS17.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Correlazione di Spearman</strong></b></a> misura la correlazione di rango, ovvero quanto due liste ordinate sono simili. Questo √® un buon meccanismo per confrontare i modelli di embedding tra loro e con i punteggi umani perch√© il punteggio effettivo √® molto meno importante di quali elementi sono classificati sopra o sotto altri.</div></div><p>La tabella seguente mostra la Correlazione di Spearman tra le classifiche di similarit√† semantica per i testi tradotti in diverse lingue. Prendiamo un insieme di frasi in inglese e poi misuriamo la loro similarit√† di embedding rispetto all'embedding di una specifica frase di riferimento e le ordiniamo dalla pi√π simile alla meno simile. Poi traduciamo tutte queste frasi in un'altra lingua e ripetiamo il processo di classificazione. In un modello di embedding multilingue ideale, le due liste ordinate sarebbero identiche, e la Correlazione di Spearman sarebbe 1.0.</p><p>Il grafico e la tabella seguenti mostrano i nostri risultati confrontando l'inglese con le altre sei lingue nel benchmark STS17, utilizzando sia <code>jina-xlm-roberta</code> che <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png\" class=\"kg-image\" alt=\"Bar chart comparing Spearman Correlation for English paired with AR, DE, ES, FR, IT, NL, colored in red and blue by alphabet \" loading=\"lazy\" width=\"2000\" height=\"1056\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-4---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-4---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-4---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png 2085w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong><code>jina-xlm-roberta</code></strong></th>\n<th><strong><code>jina-embeddings-v3</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English ‚Üî Arabic</td>\n<td>0.1581</td>\n<td><strong>0.7977</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî German</td>\n<td>0.2136</td>\n<td><strong>0.8366</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Spanish</td>\n<td>0.1049</td>\n<td><strong>0.8509</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî French</td>\n<td>0.1659</td>\n<td><strong>0.8378</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Italian</td>\n<td>0.2293</td>\n<td><strong>0.8674</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Dutch</td>\n<td>0.2387</td>\n<td><strong>0.8398</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Qui si pu√≤ vedere l'enorme differenza che fa l'apprendimento contrastivo rispetto al pre-training originale. Nonostante abbia solo il 3% di dati multilingue nel suo mix di training, il modello <code>jina-embeddings-v3</code> ha imparato abbastanza semantica multilingue da eliminare quasi completamente il divario linguistico acquisito durante il pre-training.</p><h2 id=\"english-vs-the-world-can-other-languages-keep-up-in-alignment\">Inglese vs. Il Mondo: Le Altre Lingue Possono Tenere il Passo nell'Allineamento?</h2><p>Abbiamo addestrato <code>jina-embeddings-v3</code> su 89 lingue, con particolare attenzione a 30 lingue scritte molto utilizzate. Nonostante i nostri sforzi per costruire un corpus di training multilingue su larga scala, l'inglese rappresenta ancora quasi la met√† dei dati che abbiamo utilizzato nell'addestramento contrastivo. Altre lingue, incluse lingue globali ampiamente utilizzate per le quali √® disponibile abbondante materiale testuale, sono ancora relativamente sottorappresentate rispetto all'enorme quantit√† di dati in inglese nel set di training.</p><p>Data questa predominanza dell'inglese, le rappresentazioni in inglese sono pi√π allineate rispetto a quelle di altre lingue? Per esplorare questo, abbiamo condotto un esperimento di follow-up.</p><p>Abbiamo costruito un dataset, <a href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><code>parallel-sentences</code></a>, composto da 1.000 coppie di testi in inglese, un \"ancora\" e un \"positivo\", dove il testo positivo √® logicamente implicato dal testo ancora.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/parallel-sentences ¬∑ Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-3.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/parallel-sentences.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Per esempio, la prima riga della tabella seguente. Queste frasi non sono identiche nel significato, ma hanno significati compatibili. Descrivono informativamente la stessa situazione.</p><p>Abbiamo poi tradotto queste coppie in cinque lingue usando GPT-4: tedesco, olandese, cinese (semplificato), cinese (tradizionale) e giapponese. Infine, le abbiamo ispezionate manualmente per garantire la qualit√†.</p><!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Anchor</strong></th>\n<th><strong>Positive</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>Two young girls are playing outside in a non-urban environment.</td>\n<td>Two girls are playing outside.</td>\n</tr>\n<tr>\n<td>German</td>\n<td>Zwei junge M√§dchen spielen drau√üen in einer nicht urbanen Umgebung.</td>\n<td>Zwei M√§dchen spielen drau√üen.</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>Twee jonge meisjes spelen buiten in een niet-stedelijke omgeving.</td>\n<td>Twee meisjes spelen buiten.</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>‰∏§‰∏™Âπ¥ËΩªÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁéØÂ¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>‰∏§‰∏™Â•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>ÂÖ©ÂÄãÂπ¥ËºïÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁí∞Â¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>ÂÖ©ÂÄãÂ•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>2‰∫∫„ÅÆËã•„ÅÑÂ•≥„ÅÆÂ≠ê„ÅåÈÉΩÂ∏ÇÁí∞Â¢É„Åß„ÅØ„Å™„ÅÑÂ†¥ÊâÄ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n<td>‰∫å‰∫∫„ÅÆÂ∞ëÂ•≥„ÅåÂ§ñ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Abbiamo poi codificato ogni coppia di testi con <code>jina-embeddings-v3</code> e calcolato la similarit√† del coseno tra loro. La figura e la tabella seguenti mostrano la distribuzione dei punteggi di similarit√† del coseno per ogni lingua e la similarit√† media:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png\" class=\"kg-image\" alt=\"Graph showing cosine similarity distributions for textual pairs in English, German, Dutch, Chinese, and Japanese against dens\" loading=\"lazy\" width=\"1060\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/1_monolingual_distribution_triplets.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/1_monolingual_distribution_triplets.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png 1060w\" sizes=\"(min-width: 720px) 720px\"></figure><!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>0.9078</td>\n</tr>\n<tr>\n<td>German</td>\n<td>0.8949</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>0.8844</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>0.8876</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>0.8933</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>0.8895</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Nonostante la predominanza dell'inglese nei dati di training, <code>jina-embeddings-v3</code> riconosce la similarit√† semantica in tedesco, olandese, giapponese e in entrambe le forme di cinese quasi altrettanto bene quanto in inglese.</p><h2 id=\"breaking-language-barriers-cross-lingual-alignment-beyond-english\"><strong>Abbattere le Barriere Linguistiche: Allineamento Multilingue Oltre l'Inglese</strong></h2><p>Gli studi sull'allineamento delle rappresentazioni tra lingue tipicamente esaminano coppie linguistiche che includono l'inglese. Questo focus potrebbe, in teoria, nascondere ci√≤ che sta realmente accadendo. Un modello potrebbe semplicemente ottimizzare per rappresentare tutto il pi√π vicino possibile al suo equivalente inglese, senza esaminare se altre coppie di lingue sono supportate correttamente.</p><p>Per esplorare questo aspetto, abbiamo condotto alcuni esperimenti utilizzando il dataset <code>parallel-sentences</code>, concentrandoci sull'allineamento tra lingue oltre alle sole coppie bilingue con l'inglese.</p><p>La tabella seguente mostra la distribuzione delle similarit√† del coseno tra testi equivalenti in diverse coppie di lingue ‚Äî testi che sono traduzioni di una fonte comune in inglese. Idealmente, tutte le coppie dovrebbero avere un coseno di 1 ‚Äî cio√® embedding semantici identici. In pratica, questo non potrebbe mai accadere, ma ci aspetteremmo che un buon modello abbia valori del coseno molto alti per le coppie di traduzioni.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png\" class=\"kg-image\" alt=\"Density graph charting cross-lingual cosine similarities for language pairs using jina-embeddings-v3 model.\" loading=\"lazy\" width=\"978\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png 978w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language Pair</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>German ‚Üî Dutch</td>\n<td>0.8779</td>\n</tr>\n<tr>\n<td>German ‚Üî Japanese</td>\n<td>0.8664</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Japanese</td>\n<td>0.8534</td>\n</tr>\n<tr>\n<td>Dutch ‚Üî Chinese (Simplified)</td>\n<td>0.8479</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Chinese (Traditional)</td>\n<td>0.8758</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Sebbene i punteggi di similarit√† tra lingue diverse siano leggermente inferiori rispetto ai testi compatibili nella stessa lingua, sono comunque molto alti. La similarit√† del coseno delle traduzioni olandese/tedesco √® quasi alta quanto quella tra testi compatibili in tedesco.</p><p>Questo potrebbe non sorprendere perch√© il tedesco e l'olandese sono lingue molto simili. Analogamente, le due variet√† di cinese testate qui non sono realmente due lingue diverse, ma solo forme stilisticamente differenti della stessa lingua. Ma si pu√≤ notare che anche coppie di lingue molto dissimili come olandese e cinese o tedesco e giapponese mostrano ancora una forte similarit√† tra testi semanticamente equivalenti.</p><p>Abbiamo considerato la possibilit√† che questi valori di similarit√† molto alti potessero essere un effetto collaterale dell'uso di ChatGPT come traduttore. Per verificarlo, abbiamo scaricato <a href=\"https://help.ted.com/hc/en-us/articles/360018572954-How-do-I-find-transcripts-for-TED-and-TEDx-talks?ref=jina-ai-gmbh.ghost.io\">trascrizioni di TED Talks tradotte da umani</a> in inglese e tedesco e verificato se le frasi tradotte allineate avrebbero avuto la stessa alta correlazione.</p><p>Il risultato √® stato ancora pi√π forte rispetto ai nostri dati tradotti automaticamente, come si pu√≤ vedere dalla figura seguente.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png\" class=\"kg-image\" alt=\"Graph of cross-lingual alignment density EN-DE with peak around cosine similarity 1.0, titled &quot;jina-embeddings-v3: Cross-ling\" loading=\"lazy\" width=\"988\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png 988w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-much-does-cross-language-data-contribute-to-cross-language-alignment\">Quanto Contribuiscono i Dati Multilingue all'Allineamento tra Lingue?</h2><p>Il divario linguistico quasi nullo e l'alto livello di prestazioni tra lingue sembrano sproporzionati rispetto alla piccolissima parte dei dati di training che era esplicitamente multilingue. Solo il 3% dei dati di training contrastivi insegna specificamente al modello come fare allineamenti tra lingue.</p><p>Quindi abbiamo fatto un test per vedere se i dati multilingue stessero dando un contributo.</p><p>Riqualificare completamente <code>jina-embeddings-v3</code> senza alcun dato multilingue sarebbe stato eccessivamente costoso per un piccolo esperimento, quindi abbiamo scaricato il modello <a href=\"https://huggingface.co/FacebookAI/xlm-roberta-base?ref=jina-ai-gmbh.ghost.io\"><code>xlm-roberta-base</code> da Hugging Face</a> e lo abbiamo ulteriormente addestrato con apprendimento contrastivo, utilizzando un sottoinsieme dei dati usati per addestrare <code>jina-embeddings-v3</code>. Abbiamo specificamente regolato la quantit√† di dati multilingue per testare due casi: uno senza dati multilingue e uno dove il 20% delle coppie erano multilingue. Puoi vedere i meta-parametri di training nella tabella seguente:</p>\n<!--kg-card-begin: html-->\n<table id=\"e30425bb-015e-4956-8872-b1b64cdd7ad0\" class=\"simple-table\"><tbody><tr id=\"daa3cfcc-9012-411b-8da3-05c7c6f4b371\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><strong>Backbone</strong></td><td id=\"DU<d\" class=\"\"><strong>% Cross-Language</strong></td><td id=\"@Feo\" class=\"\"><strong>Learning Rate</strong></td><td id=\"fZNx\" class=\"\"><strong>Loss Function</strong></td><td id=\"Rv}\\\" class=\"\"><strong>Temperature</strong></td></tr><tr id=\"f3a2d068-d902-4fc1-8c89-269a5ebbb135\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base </code><strong>without</strong> X-language data</td><td id=\"DU<d\" class=\"\">0%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr><tr id=\"52887e22-326c-46cd-b79c-d6dcd110c1d2\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base</code><strong> with </strong>X-language data</td><td id=\"DU<d\" class=\"\">20%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Abbiamo quindi valutato le prestazioni multilingue di entrambi i modelli utilizzando i benchmark <a href=\"https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io\">STS17 e STS22 da MTEB</a> e la Correlazione di Spearman. Presentiamo i risultati di seguito:</p><h3 id=\"sts17\"><strong>STS17</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png\" class=\"kg-image\" alt=\"Bar graph showing Spearman correlation for language pairs on STS17 with and without parallel corpus.\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-6---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-6---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-6---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"de30bf7f-d1a9-43f4-8e5f-15bf65c59674\" class=\"simple-table\"><tbody><tr id=\"5c6440a6-eba9-404a-a5f1-88099bc6702d\"><td id=\"{N[x\" class=\"\"><strong>Language Pair</strong></td><td id=\"jGmJ\" class=\"\"><strong>With parallel corpora</strong></td><td id=\"p<ZH\" class=\"\"><strong>Without parallel corpora</strong></td></tr><tr id=\"33f08461-58aa-43f3-9ed1-577f8676e99d\"><td id=\"{N[x\" class=\"\">English ‚Üî Arabic</td><td id=\"jGmJ\" class=\"\"><strong>0.6418</strong></td><td id=\"p<ZH\" class=\"\">0.5875</td></tr><tr id=\"9875386d-2043-4d9d-8252-e53ec525ec29\"><td id=\"{N[x\" class=\"\">English ‚Üî German</td><td id=\"jGmJ\" class=\"\">0.7364</td><td id=\"p<ZH\" class=\"\"><strong>0.7390</strong></td></tr><tr id=\"15d28a12-3a80-4176-9984-69b5d8a7d8ff\"><td id=\"{N[x\" class=\"\">English ‚Üî Spanish</td><td id=\"jGmJ\" class=\"\"><strong>0.6968</strong></td><td id=\"p<ZH\" class=\"\">0.6799</td></tr><tr id=\"21821558-c8b9-4c34-8ec5-9db3ca7d9328\"><td id=\"{N[x\" class=\"\">English ‚Üî French</td><td id=\"jGmJ\" class=\"\"><strong>0.7066</strong></td><td id=\"p<ZH\" class=\"\">0.6944</td></tr><tr id=\"a2e3b5e5-8e4a-4270-abff-5d059ff6be72\"><td id=\"{N[x\" class=\"\">English ‚Üî Italian</td><td id=\"jGmJ\" class=\"\"><strong>0.7232</strong></td><td id=\"p<ZH\" class=\"\">0.7070</td></tr><tr id=\"95daf20f-2a82-4431-8581-a4ce24d81462\"><td id=\"{N[x\" class=\"\">English ‚Üî Dutch</td><td id=\"jGmJ\" class=\"\"><strong>0.7597</strong></td><td id=\"p<ZH\" class=\"\">0.7468</td></tr><tr id=\"4fd4c45a-a69e-4e33-b057-9c5f10b99fdb\"><td id=\"{N[x\" class=\"\">English ‚Üî Turkish</td><td id=\"jGmJ\" class=\"\"><strong>0.6933</strong></td><td id=\"p<ZH\" class=\"\">0.6050</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"sts22\"><strong>STS22</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png\" class=\"kg-image\" alt=\"Chart comparing models of language alignment, showing Spearman correlation scores for eight language pairs with and without p\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-7---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-7---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-7---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"e53757df-8a0e-42ec-ba05-715baa3c77cd\" class=\"simple-table\"><tbody><tr id=\"45c43b8c-b1b7-4ac0-91b0-1e07025f1b92\"><td id=\"OF=p\" class=\"\"><strong>Coppia di lingue</strong></td><td id=\"P<\\i\" class=\"\"><strong>Con corpora paralleli</strong></td><td id=\"LAtp\" class=\"\"><strong>Senza corpora paralleli</strong></td></tr><tr id=\"696ebe11-3eda-49ef-8dfe-608f9b71430b\"><td id=\"OF=p\" class=\"\">Inglese ‚Üî Spagnolo</td><td id=\"P<\\i\" class=\"\"><strong>0.7710</strong></td><td id=\"LAtp\" class=\"\">0.7675</td></tr><tr id=\"eb4c1d81-7d98-453d-af3f-95b2adccfb55\"><td id=\"OF=p\" class=\"\">Cinese semplificato ‚Üî Inglese</td><td id=\"P<\\i\" class=\"\"><strong>0.6885</strong></td><td id=\"LAtp\" class=\"\">0.6860</td></tr><tr id=\"533cefd3-b30e-4d6a-9350-8f5d28b17ba6\"><td id=\"OF=p\" class=\"\">Spagnolo ‚Üî Italiano</td><td id=\"P<\\i\" class=\"\"><strong>0.6829</strong></td><td id=\"LAtp\" class=\"\">0.6814</td></tr><tr id=\"d3ecdd71-44bb-4a3b-9ac2-8cc90a785d5f\"><td id=\"OF=p\" class=\"\">Tedesco ‚Üî Francese</td><td id=\"P<\\i\" class=\"\"><strong>0.5763</strong></td><td id=\"LAtp\" class=\"\">0.5496</td></tr><tr id=\"c6242853-4da7-4369-b1f1-1a27262a487a\"><td id=\"OF=p\" class=\"\">Tedesco ‚Üî Inglese</td><td id=\"P<\\i\" class=\"\">0.5439</td><td id=\"LAtp\" class=\"\"><strong>0.5566</strong></td></tr><tr id=\"31a4a5ba-199c-4904-b926-ff0561aac1b5\"><td id=\"OF=p\" class=\"\">Polacco ‚Üî Inglese</td><td id=\"P<\\i\" class=\"\">0.6966</td><td id=\"LAtp\" class=\"\"><strong>0.7156</strong></td></tr><tr id=\"4f529d81-e8c9-4e5d-a705-36e357abebc3\"><td id=\"OF=p\" class=\"\">Tedesco ‚Üî Inglese</td><td id=\"P<\\i\" class=\"\"><strong>0.5832</strong></td><td id=\"LAtp\" class=\"\">0.5478</td></tr><tr id=\"cd1429f7-c810-4a0e-9dca-88e2c83157bc\"><td id=\"OF=p\" class=\"\">Francese ‚Üî Polacco</td><td id=\"P<\\i\" class=\"\">0.8451</td><td id=\"LAtp\" class=\"\">0.8451</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Siamo rimasti sorpresi nel vedere che per la maggior parte delle coppie di lingue testate, i dati di addestramento multilingue non hanno portato quasi nessun miglioramento. √à difficile essere certi che questo rimarrebbe vero per modelli completamente addestrati con dataset pi√π grandi, ma certamente offre prove che l'addestramento esplicito tra lingue non aggiunge molto.</p><p>Tuttavia, nota che STS17 include coppie Inglese/Arabo e Inglese/Turco. Queste sono entrambe lingue molto meno rappresentate nei nostri dati di addestramento. Il modello XML-RoBERTa che abbiamo utilizzato √® stato pre-addestrato con dati che erano al 2,25% in Arabo e al 2,32% in Turco, molto meno rispetto alle altre lingue testate. Il piccolo dataset di apprendimento contrastivo che abbiamo utilizzato in questo esperimento conteneva solo l'1,7% di Arabo e l'1,8% di Turco.</p><p>Queste due coppie di lingue sono le uniche testate in cui l'addestramento con dati multilingue ha fatto una chiara differenza. Pensiamo che i dati esplicitamente multilingue siano pi√π efficaci per le lingue che sono meno rappresentate nei dati di addestramento, ma dobbiamo esplorare ulteriormente quest'area prima di trarre conclusioni. Il ruolo e l'efficacia dei dati multilingue nell'addestramento contrastivo √® un'area in cui Jina AI sta conducendo ricerche attive.</p><h2 id=\"conclusion\">Conclusione</h2><p>I metodi convenzionali di pre-addestramento linguistico, come il Masked Language Modeling, lasciano un \"gap linguistico\", dove testi semanticamente simili in lingue diverse non si allineano cos√¨ strettamente come dovrebbero. Abbiamo dimostrato che il regime di apprendimento contrastivo di Jina Embeddings √® molto efficace nel ridurre o addirittura eliminare questo gap.</p><p>Le ragioni per cui questo funziona non sono del tutto chiare. Utilizziamo esplicitamente coppie di testi multilingue nell'addestramento contrastivo, ma solo in quantit√† molto piccole, e non √® chiaro quanto ruolo giochino effettivamente nell'assicurare risultati multilingue di alta qualit√†. I nostri tentativi di mostrare un effetto chiaro utilizzando condizioni pi√π controllate non hanno prodotto un risultato inequivocabile.</p><p>Tuttavia, <strong>√® chiaro che <code>jina-embeddings-v3</code> ha superato il gap linguistico del pre-addestramento, rendendolo uno strumento potente per applicazioni multilingue.</strong> √à pronto per essere utilizzato per qualsiasi attivit√† che richieda prestazioni forti e identiche in pi√π lingue.</p><p>Puoi utilizzare <code>jina-embeddings-v3</code> tramite la nostra <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Embeddings API</a> (con un milione di token gratuiti) o tramite AWS o Azure. Se vuoi utilizzarlo al di fuori di queste piattaforme o on-premises nella tua azienda, tieni presente che √® concesso in licenza sotto CC BY-NC 4.0. <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io\">Contattaci</a> se sei interessato all'uso commerciale.</p>",
  "comment_id": "67066bd652567c0001d0f2cd",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/gap-blog-1.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-09T13:41:10.000+02:00",
  "updated_at": "2024-10-10T20:15:26.000+02:00",
  "published_at": "2024-10-09T14:42:22.000+02:00",
  "custom_excerpt": "Multilingual models often face a \"language gap,\" where similar phrases in different languages don't align. We show how contrastive learning can bridge this gap, enhancing cross-language performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/",
  "excerpt": "I modelli multilingue spesso affrontano un \"gap linguistico\", dove frasi simili in lingue diverse non si allineano. Mostriamo come l'apprendimento contrastivo pu√≤ colmare questo divario, migliorando le prestazioni tra le diverse lingue.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon green squares form intricate patterns on a black digital background, creating a dynamic, abstract design.",
  "feature_image_caption": null
}
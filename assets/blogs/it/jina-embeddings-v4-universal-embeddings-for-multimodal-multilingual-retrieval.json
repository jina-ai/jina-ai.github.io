{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4: 向量模型 (Embeddings) universali per il recupero multimodale multilingue",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Modelli Foundation di ricerca</div><div class=\"kg-bookmark-description\">Modello di embedding universale per il recupero multimodale e multilingue</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Modelli Foundation di ricerca</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Embedding universali per il recupero multilingue multimodale</div><div class=\"kg-bookmark-description\">Presentiamo jina-embeddings-v4, un modello di embedding multimodale da 3,8 miliardi di parametri che unifica le rappresentazioni di testo e immagini attraverso una nuova architettura che supporta sia gli embedding a vettore singolo che a più vettori nello stile di interazione tardiva. Il modello incorpora adattatori Low-Rank Adaptation (LoRA) specifici per attività per ottimizzare le prestazioni in diversi scenari di recupero, tra cui il recupero di informazioni basato su query, la somiglianza semantica cross-modale e la ricerca di codice di programmazione. Valutazioni complete dimostrano che jina-embeddings-v4 raggiunge prestazioni all'avanguardia sia in attività di recupero a modalità singola che cross-modale, con particolare forza nell'elaborazione di contenuti visivamente ricchi come tabelle, grafici, diagrammi e formati multimediali misti. Per facilitare la valutazione di questa capacità, introduciamo anche Jina-VDR, un nuovo benchmark progettato specificamente per il recupero di immagini visivamente ricche.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">Siamo in viaggio per far progredire e democratizzare l'intelligenza artificiale attraverso l'open source e la scienza aperta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Oggi rilasciamo <code>jina-embeddings-v4</code>, il nostro nuovo modello di embedding universale da 3,8 miliardi di parametri per testo e immagini. Include un set di adattatori LoRA specifici per attività che ottimizzano le prestazioni per le attività di recupero più comuni, tra cui il recupero di query-documenti, la corrispondenza semantica e la ricerca di codice. <code>jina-embeddings-v4</code> raggiunge prestazioni di recupero all'avanguardia su attività multimodali e multilingue su benchmark MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP e ViDoRe, con particolare forza nell'elaborazione di contenuti visivamente ricchi come tabelle, grafici, diagrammi e loro combinazioni. Il modello supporta sia gli embedding a vettore singolo che a più vettori.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Prestazioni di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> attraverso il recupero di documenti visivi e benchmark multimodali. Le distribuzioni del boxplot mostrano i punteggi medi e la variabilità delle prestazioni per i modelli di embedding in sei categorie di benchmark: ViDoRe (recupero di documenti visivi), Jina-VDR (recupero completo di documenti visivi), Wikimedia Commons Retrieval (corrispondenza multilingue documento-descrizione), GitHub README Retrieval (recupero della documentazione del codice), Tweet Stock Retrieval (analisi dei grafici finanziari) e CLIP Benchmark (recupero generale da testo a immagine). Le varianti di Jina-embeddings-v4 (evidenziate in ciano) dimostrano prestazioni all'avanguardia nelle attività di documenti visivamente ricchi, con la versione multi-vettore che raggiunge i punteggi più alti nei benchmark di documenti visivi specializzati (90,2 su ViDoRe, 80,2 su Jina-VDR), pur mantenendo prestazioni competitive nelle attività di recupero multimodale generale (84,1 su CLIP Benchmark). I modelli sono classificati in base alle prestazioni medie all'interno di ciascuna categoria di benchmark, con singoli punti dati che mostrano le distribuzioni dei punteggi in più attività di valutazione.</span></figcaption></figure><p><code>jina-embeddings-v4</code> è il nostro modello di embedding più ambizioso finora. In quanto modello open source, <code>jina-embeddings-v4</code> supera i principali modelli di embedding closed source dei principali fornitori, offrendo prestazioni migliori del 12% rispetto a <code>text-embedding-3-large</code> di OpenAI nel recupero multilingue (66,49 contro 59,27), un miglioramento del 28% nelle attività di documenti lunghi (67,11 contro 52,42), un miglioramento del 15% rispetto a <code>voyage-3</code> nel recupero di codice (71,59 contro 67,23) e prestazioni pari a <code>gemini-embedding-001</code> di Google. Ciò rende v4 il modello di embedding universale open source più capace disponibile oggi, offrendo a ricercatori e sviluppatori funzionalità di embedding multimodale di livello aziendale con piena trasparenza nel processo di formazione, nelle decisioni architetturali e nei pesi del modello tramite <a href=\"https://arxiv.org/abs/2506.18902\">la nostra relazione tecnica completa.</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Prestazioni di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> attraverso cinque benchmark di recupero. Il grafico mostra le distribuzioni del boxplot con i punteggi medi per ciascun modello nei benchmark di Text Retrieval, Code Retrieval, Multilingual Retrieval, Long Context Retrieval e Semantic Textual Similarity (STS). </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (evidenziato in ciano) dimostra prestazioni competitive o all'avanguardia in tutte le categorie di valutazione, con risultati particolarmente solidi nel recupero di testo e STS. I modelli sono classificati in base alle prestazioni medie all'interno di ciascuna categoria di benchmark, con singoli punti dati che mostrano le distribuzioni dei punteggi in più attività di valutazione.</span></figcaption></figure><h2 id=\"new-architecture\">Nuova Architettura</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Architettura di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">. Il modello è basato sulla dorsale </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> (3,8 miliardi di parametri). Gli input di testo e immagine vengono elaborati attraverso un percorso condiviso: le immagini vengono prima convertite in sequenze di token tramite un codificatore di visione, quindi entrambe le modalità vengono elaborate congiuntamente dal decodificatore del modello linguistico con livelli di attenzione contestuale. Tre adattatori LoRA specifici per attività (60 milioni di parametri ciascuno) forniscono un'ottimizzazione specializzata per le attività di recupero, corrispondenza del testo e codice senza modificare i pesi della dorsale bloccata. L'architettura supporta due modalità di output: (1) embedding a vettore singolo (2048 dimensioni, troncabili a 128) generati tramite mean pooling per una ricerca di somiglianza efficiente e (2) embedding a più vettori (128 dimensioni per token) tramite livelli di proiezione per strategie di recupero di interazione tardiva.</span></figcaption></figure><p>L'aggiornamento da <code>jina-embeddings-v3</code> a<code>jina-embeddings-v4</code> rappresenta un cambio di paradigma dagli 向量模型 (Embeddings) solo testuali a quelli multimodali. Mentre v3 si concentrava sull'ottimizzazione degli 向量模型 (Embeddings) di testo con adattatori LoRA specifici per attività, v4 affronta la crescente esigenza di incorporare sia contenuti testuali che visivi in rappresentazioni unificate.\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Aspetto</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Modello Backbone</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>Parametri (Base)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>Parametri (con adattatori)</td>\n<td>572M</td>\n<td>3.8B + 60M per adattatore</td>\n</tr>\n<tr>\n<td>Modalità</td>\n<td>Solo testo</td>\n<td>Testo + Immagini (multimodale)</td>\n</tr>\n<tr>\n<td>Lunghezza massima input</td>\n<td>8.192 tokens (Token)</td>\n<td>32.768 tokens (Token)</td>\n</tr>\n<tr>\n<td>Elaborazione immagini</td>\n<td>Nessuna</td>\n<td>Fino a 20 megapixel, documenti visivamente ricchi</td>\n</tr>\n  <tr>\n<td>Supporto multilingue</td>\n<td>89 lingue</td>\n<td>29+ lingue</td>\n</tr>\n<tr>\n<td>Tipi di vettore</td>\n<td>Solo vettore singolo</td>\n<td>Vettore singolo + Vettore multiplo (interazione tardiva)</td>\n</tr>\n<tr>\n<td>Dimensioni vettore singolo</td>\n<td>1024 (MRL troncabile a 32)</td>\n<td>2048 (MRL troncabile a 128)</td>\n</tr>\n<tr>\n<td>Dimensioni vettore multiplo</td>\n<td>Non disponibile</td>\n<td>128 per token (Token)</td>\n</tr>\n<tr>\n<td>Specializzazioni LoRA per attività</td>\n<td>• Recupero asimmetrico<br>• Somiglianza semantica<br>• Classificazione<br>• Separazione</td>\n<td>• Recupero asimmetrico<br>• Somiglianza semantica<br>• Recupero codice</td>\n</tr>\n<tr>\n<td>Fasi di addestramento</td>\n<td>3 fasi: Pre-addestramento → Ottimizzazione fine dell'incorporamento → Addestramento adattatore</td>\n<td>2 fasi: Addestramento congiunto a coppie → Addestramento adattatore specifico per attività</td>\n</tr>\n<tr>\n<td>Funzioni di perdita</td>\n<td>InfoNCE, CoSent, Perdita a triplette estesa</td>\n<td>InfoNCE congiunto + Divergenza KL per vettore singolo/multiplo</td>\n</tr>\n<tr>\n<td>Codifica posizionale</td>\n<td>RoPE (sintonizzazione della frequenza di base rotatoria)</td>\n<td>M-RoPE (Incorporamento di posizione rotatoria multimodale)</td>\n</tr>\n<tr>\n<td>Elaborazione cross-modale</td>\n<td>N/A</td>\n<td>Codificatore unificato (gap di modalità ridotto)</td>\n</tr>\n<tr>\n<td>Supporto MRL</td>\n<td>Sì</td>\n<td>Sì</td>\n</tr>\n<tr>\n<td>Implementazione dell'attenzione</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">Backbone</h3><p>Il cambiamento architetturale più significativo in v4 è il cambiamento del backbone da <code>XLM-RoBERTa</code> a <code>Qwen2.5-VL-3B-Instruct</code>. Questa decisione è stata guidata dall'obiettivo principale di v4 di creare un modello di 向量模型 (Embeddings) universale che consenta la \"vera elaborazione multimodale\" in cui le immagini vengono convertite in sequenze di token (Token) ed elaborate insieme al testo, eliminando il <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">divario di modalità</a> presente nelle architetture a doppio codificatore.</p><p>La selezione del backbone si allinea a diversi obiettivi di progettazione chiave: l'eccellenza di Qwen2.5-VL nella comprensione dei documenti supporta direttamente la forza di v4 nell'elaborazione di contenuti visivamente ricchi come tabelle, grafici e screenshot. Le capacità di risoluzione dinamica consentono a v4 di gestire immagini ridimensionate a 20 megapixel come specificato nell'architettura. La codifica posizionale avanzata fornisce le basi che consentono a v4 di ottenere un allineamento cross-modale superiore con un punteggio di allineamento di 0,71 rispetto a 0,15 per OpenAI CLIP.</p><h3 id=\"lora-adapters\">Adattatori LoRA</h3><p>V4 semplifica le cinque attività di v3 a tre attività mirate, riflettendo le lezioni apprese sull'efficacia e l'adozione da parte degli utenti:</p><ul><li><strong>Recupero asimmetrico</strong> (consolidando gli adattatori di query/passaggio di v3)</li><li><strong>Somiglianza simmetrica</strong> (l'equivalente di v3 per la corrispondenza del testo per le attività STS)</li><li><strong>Recupero del codice</strong> (appreso da v2-code, mancante in v3)</li></ul><p>Questo consolidamento rimuove gli adattatori di classificazione e separazione di v3, concentrando v4 sui casi d'uso di 向量模型 (Embeddings) di maggiore impatto: recupero e STS.</p><h3 id=\"output-embeddings\">Output 向量模型 (Embeddings)</h3><p>V4 introduce un sistema a doppio output che supporta sia 向量模型 (Embeddings) a vettore singolo che a vettore multiplo, mentre v3 forniva solo output a vettore singolo. Questo affronta diversi scenari di recupero:</p><ul><li><strong>Modalità vettore singolo</strong>: 向量模型 (Embeddings) a 2048 dimensioni (troncabili a 128 tramite MRL) per una ricerca di similarità efficiente</li><li><strong>Modalità vettore multiplo</strong>: 128 dimensioni per token (Token) per il <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">recupero a interazione tardiva</a></li></ul><p>Questo doppio approccio offre una maggiore efficacia con rappresentazioni a vettore multiplo, in particolare nel recupero di documenti visivamente ricchi, mantenendo al contempo l'efficienza per le attività di similarità standard. Il costante vantaggio in termini di prestazioni del 7-10% del vettore multiplo rispetto alla modalità vettore singolo nelle attività visive suggerisce che l'interazione tardiva fornisce una corrispondenza semantica fondamentalmente migliore per il contenuto multimodale.</p><h3 id=\"parameter-size\">Dimensione dei parametri</h3><p>Mentre v4 è 6,7 volte più grande di v3 (3,8 miliardi contro 570 milioni di parametri), i miglioramenti delle prestazioni solo testuali sono in realtà modesti, il che suggerisce che il ridimensionamento dei parametri è stato guidato principalmente dai requisiti multimodali piuttosto che dal miglioramento del testo. Sui benchmark di testo principali, v4 ottiene 66,49 su MMTEB rispetto al 58,58 di v3 (miglioramento del 14%) e 55,97 su MTEB-EN rispetto al 54,33 di v3 (miglioramento del 3%). Per il recupero del codice, v4 ottiene 71,59 su CoIR rispetto al 55,07 di v3 (miglioramento del 30%), mentre le prestazioni sui documenti lunghi mostrano v4 a 67,11 rispetto al 55,66 di v3 su LongEmbed (miglioramento del 21%). Il ridimensionamento sostanziale diventa giustificato quando si considerano le capacità multimodali di v4: ottenendo 84,11 nDCG@5 sul recupero di documenti visivi (Jina-VDR) e 90,17 sui benchmark ViDoRe, capacità completamente assenti in v3. L'aumento dei parametri rappresenta quindi il nostro investimento nella funzionalità multimodale mantenendo al contempo prestazioni di testo competitive, con l'architettura unificata che elimina la necessità di modelli di testo e visione separati ottenendo al contempo un allineamento cross-modale di 0,71 rispetto a 0,15 per i tradizionali approcci a doppio codificatore.</p><h2 id=\"getting-started\">Iniziare</h2><p>Per un rapido controllo delle vibrazioni, prova la nostra demo da testo a immagine nella casella degli strumenti di Search Foundation. Abbiamo preparato una raccolta di immagini di documenti dal nostro sito Web e puoi anche aggiungere i tuoi URL di immagini. Digita semplicemente la tua query e premi invio per visualizzare i risultati classificati. Puoi ritirarlo come OCR o recupero di immagini basato sul contenuto: sentiti libero di provare query anche in lingue diverse dall'inglese.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">La demo è disponibile all'indirizzo: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> Tieni presente che l'utilizzo di questa demo consumerà i tokens (Token) della tua chiave API principale. Inoltre, la demo potrebbe sembrare un po' lenta poiché deve scaricare tutte le immagini sul server da quegli URL e nessuna cache è implementata per le immagini.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">Tramite API</h3><p>Il codice seguente mostra come utilizzare <code>jina-embeddings-v4</code>. Puoi passare una stringa di testo, un'immagine con codifica base64 o un URL di immagine. I nuovi utenti possono ottenere una chiave API Jina con 10 milioni di tokens (Token) gratuiti.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>A causa delle limitate risorse GPU, la nostra API di 向量模型 (Embeddings) attualmente supporta documenti fino a 8.000 词元 (tokens) di lunghezza, nonostante la capacità nativa di <code>jina-embeddings-v4</code> di gestire fino a 32.000 词元 (tokens). Per le applicazioni che richiedono contesti più lunghi oltre 8.000 词元 (tokens) (come <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>), consigliamo di distribuire i nostri modelli tramite CSP o di auto-ospitare il modello.</p><h3 id=\"via-csp-marketplaces\">Tramite i marketplace CSP</h3><p><code>jina-embeddings-v4</code> sarà presto disponibile direttamente su AWS, Azure e GCP ai prezzi lì indicati.</p><h3 id=\"via-huggingface\">Tramite HuggingFace</h3><p>Per scopi di ricerca e sperimentazione, è possibile utilizzare il modello localmente dalla nostra pagina Hugging Face. Abbiamo preparato un notebook di Google Colab che dimostra come funziona.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Conclusione</h2><p><code>jina-embeddings-v4</code> rappresenta il nostro salto in avanti più significativo: un modello di 向量模型 (Embedding) universale da 3,8 miliardi di parametri che elabora testo e immagini attraverso un percorso unificato, supportando sia il recupero denso che quello a interazione tardiva, superando i modelli proprietari di Google, OpenAI e Voyage AI, soprattutto nel recupero di documenti visivamente ricchi. Ma questa capacità non è emersa isolatamente; è il culmine di quattro generazioni di risoluzione di limitazioni fondamentali.</p><p>Quando abbiamo iniziato con <code>jina-embeddings-v1</code> all'inizio del 2022, tutti presumevano che più dati significassero prestazioni migliori. Abbiamo dimostrato il contrario: filtrare 1,5 miliardi di coppie fino a 385 milioni di esempi di alta qualità ha superato di gran lunga set di dati molto più grandi. La lezione: la cura batte la raccolta.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings costituisce un insieme di modelli di 向量模型 (Embedding) di frasi ad alte prestazioni, in grado di tradurre gli input testuali in rappresentazioni numeriche, catturando la semantica del testo. Questi modelli eccellono in applicazioni come il recupero denso e la similarità semantica del testo. Questo documento descrive in dettaglio lo sviluppo di Jina Embeddings, a partire dalla creazione di set di dati a coppie e terzine di alta qualità. Sottolinea il ruolo cruciale della pulizia dei dati nella preparazione del set di dati, offre approfondimenti sul processo di training del modello e si conclude con una valutazione completa delle prestazioni utilizzando il Massive Text Embedding Benchmark (MTEB). Inoltre, per aumentare la consapevolezza del modello della negazione grammaticale, costruiamo un nuovo set di dati di training e valutazione di affermazioni negate e non negate, che mettiamo a disposizione del pubblico.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ma gli utenti continuavano a sbattere contro il muro dei 512 词元 (tokens) di BERT. L'addestramento su sequenze più lunghe sembrava costoso, finché <code>jina-embeddings-v2</code> non ha rivelato una soluzione elegante: addestrare in breve, distribuire in lungo. I bias di attenzione lineare di ALiBi consentono ai modelli addestrati su 512 词元 (tokens) di gestire senza problemi 8.192 词元 (tokens) in fase di inferenza. Abbiamo ottenuto più capacità con meno calcolo.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">I modelli di 向量模型 (Embedding) di testo sono emersi come potenti strumenti per trasformare le frasi in vettori di caratteristiche di dimensione fissa che incapsulano le informazioni semantiche. Sebbene questi modelli siano essenziali per attività come il recupero di informazioni, il clustering semantico e il 重排器 (re-ranking) di testo, la maggior parte dei modelli open-source esistenti, in particolare quelli basati su architetture come BERT, faticano a rappresentare documenti lunghi e spesso ricorrono al troncamento. Un approccio comune per mitigare questa sfida prevede la suddivisione dei documenti in paragrafi più piccoli per l'向量模型 (Embedding). Tuttavia, questa strategia si traduce in un set di vettori molto più grande, con conseguente aumento del consumo di memoria e ricerche vettoriali computazionalmente intensive con latenza elevata. Per affrontare queste sfide, presentiamo Jina Embeddings 2, un modello di 向量模型 (Embedding) di testo open-source in grado di ospitare fino a 8192 词元 (tokens). Questo modello è progettato per trascendere il convenzionale limite di 512 词元 (tokens) e per elaborare abilmente documenti lunghi. Jina Embeddings 2 non solo ottiene prestazioni all'avanguardia su una gamma di attività correlate all'向量模型 (Embedding) nel benchmark MTEB, ma corrisponde anche alle prestazioni del modello proprietario ada-002 di OpenAI. Inoltre, i nostri esperimenti indicano che un contesto esteso può migliorare le prestazioni in attività come NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Il successo di <code>jina-embeddings-v2</code> ha esposto un altro vincolo: attività diverse necessitavano di ottimizzazioni diverse. Invece di costruire modelli separati, <code>jina-embeddings-v3</code> ha utilizzato piccoli adattatori LoRA da 60 milioni per personalizzare un modello di base da 570 milioni per qualsiasi attività. Un modello è diventato cinque modelli specializzati.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">Presentiamo jina-embeddings-v3, un nuovo modello di 向量模型 (Embedding) di testo con 570 milioni di parametri, che raggiunge prestazioni all'avanguardia sui dati multilingue e sulle attività di recupero di contesti lunghi, supportando lunghezze di contesto fino a 8192 词元 (tokens). Il modello include un set di adattatori Low-Rank Adaptation (LoRA) specifici per attività per generare 向量模型 (Embedding) di alta qualità per il recupero di query-documenti, il clustering, la classificazione e la corrispondenza del testo. La valutazione sul benchmark MTEB mostra che jina-embeddings-v3 supera gli ultimi 向量模型 (Embedding) proprietari di OpenAI e Cohere su attività in inglese, raggiungendo al contempo prestazioni superiori rispetto a multilingual-e5-large-instruct su tutte le attività multilingue. Con una dimensione di output predefinita di 1024, gli utenti possono ridurre in modo flessibile le dimensioni dell'向量模型 (Embedding) fino a 32 senza compromettere le prestazioni, grazie all'apprendimento della rappresentazione di Matryoshka.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Anche con la specializzazione per attività, siamo rimasti solo testuali, mentre gli utenti avevano bisogno di comprensione visiva. I modelli standard basati su CLIP come <code>jina-clip-v1</code> e <code>jina-clip-v2</code> utilizzano codificatori separati, creando un \"divario di modalità\" in cui contenuti simili in formati diversi finiscono per essere molto distanti. Come il nostro <code>jina-reranker-m0</code> rilasciato di recente, <code>jina-embeddings-v4</code> ha eliminato completamente questo divario: un unico percorso unificato elabora tutto, rimuovendo il divario invece di colmarlo.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">Presentiamo jina-embeddings-v4, un modello di 向量模型 (Embedding) multimodale da 3,8 miliardi di parametri che unifica le rappresentazioni di testo e immagini attraverso una nuova architettura che supporta sia l'向量模型 (Embedding) a vettore singolo che a vettore multiplo nello stile a interazione tardiva. Il modello incorpora adattatori Low-Rank Adaptation (LoRA) specifici per attività per ottimizzare le prestazioni in diversi scenari di recupero, tra cui il recupero di informazioni basato su query, la similarità semantica cross-modale e la ricerca di codice di programmazione. Valutazioni complete dimostrano che jina-embeddings-v4 raggiunge prestazioni all'avanguardia sia su attività di recupero a modalità singola che cross-modale, con particolare forza nell'elaborazione di contenuti visivamente ricchi come tabelle, grafici, diagrammi e formati multimediali misti. Per facilitare la valutazione di questa capacità, presentiamo anche Jina-VDR, un nuovo benchmark progettato specificamente per il recupero di immagini visivamente ricche.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Sia <code>jina-embeddings-v4</code> che <code>jina-reranker-m0</code> condividono un cambiamento fondamentale: l'utilizzo di 大模型 (LLM) come backbone invece di modelli solo encoder. Questo non è casuale: riflette un profondo vantaggio che la maggior parte delle persone ignora: i modelli solo encoder creano \"divari di modalità\" in cui le immagini si raggruppano separatamente dal testo. I modelli solo decoder aprono possibilità che non erano realizzabili con architetture solo encoder, tra cui la vera rappresentazione a modalità mista e la spiegabilità.</p>\n{{{output start}}}<p>La nostra intuizione chiave: sia gli \"向量模型 (Embeddings)\" che la generazione riguardano la comprensione della semantica. I \"大模型 (LLM)\" che eccellono nella generazione eccellono naturalmente nella rappresentazione. Crediamo che il futuro risieda in architetture unificate in cui gli \"向量模型 (Embedding)\" e il \"重排器 (Reranker)\" emergono dallo <strong>stesso modello di base di ricerca</strong>—ed è esattamente ciò verso cui Jina AI sta costruendo.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina Embeddings v4 è un modello di 向量模型 (Embeddings) universale da 3,8 miliardi di parametri per il recupero multimodale e multilingue che supporta sia output di 向量模型 (Embeddings) a vettore singolo che a vettore multiplo.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
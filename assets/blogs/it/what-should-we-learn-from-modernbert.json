{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "Cosa Dovremmo Imparare da ModernBERT?",
  "html": "<p>Nel 2018, Google ha rilasciato BERT che ha rivoluzionato il campo del NLP, molto prima dell'attuale ondata di LLM. Ancora oggi, molti Small Language Models sono costruiti su BERT. Nel dicembre 2024, <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> applica ciò che abbiamo imparato dai recenti sviluppi degli LLM a questi modelli più piccoli. I punti chiave? Migliore efficienza dei parametri, comprensione del codice e gestione di contesti lunghi.</p><p>In questo post, analizzeremo come ModernBERT si confronta con due modelli che conosciamo a fondo: <code>jina-XLM-RoBERTa</code> (la base multilingua dietro <code>jina-embeddings-v3</code>) e <code>RobERTa-large</code>. Esaminiamo ogni modello:</p><ul><li><strong>ModernBERT</strong> (dic. 2024) è un SLM di recente rilascio, sviluppato in collaborazione da Answer.AI, LightOn e HuggingFace. Sfrutta ottimizzazioni moderne come RoPE per una finestra di contesto di 8.192 token e <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">layer GeGLU</a>, migliorando le prestazioni mantenendo l'efficienza.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong> (sett. 2024) è un modello di embedding testuale multilingue basato su <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> di Meta. Mentre l'originale <code>XLM-RoBERTa</code> migliora <code>RoBERTa</code> usando il grande dataset multilingue XLM, <code>jina-XLM-RoBERTa</code> va oltre con training su contesti estesi, implementazione <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> e supporto <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a>. Questo modello funge da base per <code>jina-embeddings-v3</code>.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a> (luglio 2019) sviluppato da Meta, è una versione migliorata di BERT con 355 milioni di parametri. Attraverso training esteso, dataset più grandi e innovazioni come il masking dinamico, ha raggiunto risultati impressionanti su benchmark chiave tra cui <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a> e <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a>. Questo lo rende adatto a vari compiti NLP dalla classificazione del testo alle risposte a domande.</li></ul><p>Confrontando questi modelli su tre aspetti fondamentali, puntiamo a evidenziare le efficaci scelte progettuali di ModernBERT per altri sviluppatori di modelli e identificare intuizioni chiave di sviluppo per futuri modelli simili a BERT. Condivideremo anche i nostri apprendimenti dallo sviluppo di <code>jina-embeddings-v3</code> e discuteremo i miglioramenti pianificati per <code>jina-embeddings-v4</code> e <code>jina-reranker-v3</code>.</p><h2 id=\"modernberts-parameter-efficiency\">Efficienza dei Parametri di ModernBERT</h2><p>Esaminiamo prima l'approccio di ModernBERT all'efficienza dei parametri - sta introducendo diverse intuizioni chiave dai recenti sviluppi LLM. ModernBERT sfrutta tre strategie principali: un'architettura più profonda ma più sottile, dimensione controllata del vocabolario e upscaling progressivo del modello partendo da modelli più piccoli.</p><h3 id=\"deep-and-thin-architecture\">Architettura Deep-And-Thin</h3><p>ModernBERT-large va più in profondità con 28 layer, mentre <code>jina-XLM-RoBERTa</code> e <code>RoBERTa-large</code> ne hanno 24. Ma ecco la parte interessante - pareggia <code>RoBERTa-large</code> nel conteggio dei parametri nonostante quei layer extra. <code>jina-XLM-RoBERTa</code> necessita di più parametri poiché gestisce 89 lingue, mentre gli altri due si concentrano solo sull'inglese.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">La profondità (numero di layer) è più importante della larghezza (numero di unità nascoste) per i piccoli LLM. Una struttura profonda e sottile eccelle nel catturare concetti astratti, risultando in prestazioni finali superiori.</span></figcaption></figure><p>La maggior parte dei parametri di un transformer proviene dai layer di attention e fully-connected. ModernBERT rimane competitivo nelle dimensioni diventando \"più sottile\" - utilizza 2.624 unità nascoste su 28 layer, rispetto alle 4.096 unità di RoBERTa-large su 24 layer. Questo setup \"più profondo\" ma più sottile permette loro di raggiungere i loro obiettivi di prestazioni senza gonfiare il modello.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parameters</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Hidden states</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Intermediate dims</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Attention heads</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Vocabulary size</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Questo approccio si allinea con la ricerca <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> di Meta, che ha scoperto che per i modelli più piccoli, la profondità è più importante della larghezza quando si tratta di catturare pattern complessi e guidare le prestazioni. Essenzialmente, la capacità di elaborare informazioni attraverso più layer transformer si rivela più preziosa dell'avere layer più larghi per l'elaborazione parallela.</p><p>Diamo un'occhiata ai dati su come si comporta questa architettura deep-and-thin.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">Confrontandolo con modelli comparabili che usano l'architettura tradizionale shallow-fat, ModernBERT fornisce risultati migliori su task chiave come retrieval e STS - tutto mentre mantiene simile il conteggio dei parametri.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Average</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Prendiamo <code>jina-XLM-RoBERTa</code> - si basa sull'architettura shallow-fat di <code>RoBERTa-large</code> ma aumenta il vocabolario da 50K a 250K token e si addestra su più dati. Eppure ModernBERT lo supera comunque, suggerendo che il cambiamento architetturale sta facendo una reale differenza in termini di efficienza.</p><h3 id=\"vocabulary-size-matters\">La Dimensione del Vocabolario è Importante</h3><p>Prima, vediamo come vengono contati i parametri del vocabolario nei transformer. Per qualsiasi transformer, <code>parametri vocabolario = numero di token distinti × dimensione nascosta</code>. Prendiamo <code>jina-XLM-RoBERTa</code>: con 250K token e 1.024 dimensioni, necessita di 256M parametri solo per la codifica del vocabolario - prima di gestire qualsiasi task linguistico!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">Nei transformer, il primo strato mappa i token in stati nascosti usando una matrice di pesi, ovvero i pesi del vocabolario. Considerando tutti i code point UTF-8 (1.112.064) con 1.024 dimensioni nascoste, sarebbero necessari ben </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> parametri solo per la conversione dei token. Mentre i LLM più grandi (oltre 100B di parametri) possono gestire questo overhead, è un serio vincolo per i modelli più piccoli. È esattamente per questo che usiamo tokenizer come BPE, che uniscono efficientemente code point UTF-8 comuni in token singoli.</span></figcaption></figure><p>Ma ecco il punto: <strong>i pesi del vocabolario non contribuiscono ai meccanismi di attenzione - sono solo tabelle di lookup.</strong> Per gli SLM che lavorano con budget di parametri fissi, un vocabolario più ampio significa meno parametri disponibili per i layer di attenzione, che eseguono l'effettiva elaborazione del linguaggio. Questo spiega perché ModernBERT-large solo inglese supera le prestazioni del multilingue <code>jina-XLM-RoBERTa</code> nonostante sia più piccolo - <code>jina-XLM-RoBERTa</code> alloca più parametri (47%!) per supportare più lingue. Il vocabolario focalizzato di ModernBERT non solo migliora le prestazioni ma velocizza anche l'inferenza, rendendolo particolarmente efficace per applicazioni con risorse limitate.</p><p>Quindi se ora guardiamo <em>solo</em> i parametri del modello core (escludendo i pesi del vocabolario), ModernBERT ha in realtà più potenza computazionale dei suoi pari: ModernBERT dedica il 19% in più di parametri alla <em>vera</em> modellazione del linguaggio rispetto a <code>jina-XLM-RoBERTa</code> e il 15% in più rispetto a <code>RoBERTa-large</code>!</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Specifiche Modello</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Supporto Lingue</td>\n<td>Solo Inglese</td>\n<td>89 Lingue</td>\n<td>Solo Inglese</td>\n</tr>\n<tr>\n<td>Dimensione Vocabolario</td>\n<td>50,4K</td>\n<td>250K</td>\n<td>50,3K</td>\n</tr>\n<tr>\n<td>Parametri Totali</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Parametri Vocabolario</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>Rapporto Parametri Vocabolario</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>Parametri Modello Core</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">Upscaling del Modello tramite \"Weight Tiling\"</h3><p>Nella costruzione del backbone di <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a>, abbiamo scoperto che addestrare SLM da zero era dispendioso in termini di risorse e complesso. ModernBERT affronta questo problema con un approccio intelligente di inizializzazione chiamato <strong>weight tiling</strong> - essenzialmente avviando ModernBERT-large dai pesi della sua versione base più piccola.</p><p>Questa tecnica non è del tutto nuova - si basa sul lavoro di DeepMind con <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> e appare anche nei modelli <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2</a> di Microsoft. Ma la sua applicazione qui è particolarmente efficace per affrontare il collo di bottiglia dell'addestramento SLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT scala da 22 a 28 layer usando la strategia di inizializzazione della profondità del team Gopher. Per quei layer extra (23-28), inizializzano ciascuno usando i pesi dai 22 layer originali di ModernBERT-base. Per le matrici di pesi di ogni layer, usano l'approccio di tiling centrale di Phi-2. Funziona così: prendono i pesi di ModernBERT-base e li posizionano proprio nel centro delle matrici di ModernBERT-large. Per i bordi ancora vuoti? Avvolgono ciclicamente i pesi originali per riempirli.</span></figcaption></figure><p>Questa strategia di inizializzazione dà a ModernBERT-large un vantaggio significativo - invece di partire da zero, sfrutta i pattern pre-appresi dalla sua controparte più piccola. Si è dimostrata particolarmente <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">efficace per scalare i modelli di linguaggio in questo intervallo di dimensioni</a>.</p><blockquote>Troviamo che un modello avviato a caldo si riprende rapidamente da una perdita iniziale elevata (dovuta ai parametri aggiunti) fino a una perdita molto vicina a quella del modello base. Siamo in grado di espandere 417M parametri di oltre 3 volte in dimensione e mantenere prestazioni superiori a un equivalente modello nuovo addestrato da zero fino alla convergenza, implicando che i guadagni non erano limitati all'inizio dell'addestramento. Tuttavia, a dimensioni maggiori, i guadagni relativi raggiunti alla convergenza diminuiscono, specialmente con espansioni in larghezza.</blockquote><p>L'avvolgimento ciclico dei pesi non è solo una comodità - si allinea bene con il modo in cui le matrici di attenzione mostrano naturalmente pattern periodici. La ricerca di Gopher mostra che questo approccio brilla davvero per gli SLM (meno di 9B parametri), anche se i benefici iniziano a diminuire quando si passa a modelli più grandi.</p><h2 id=\"modernberts-code-modeling\">Modellazione del Codice di ModernBERT</h2><p>ModernBERT porta un approccio specializzato alla comprensione del codice con il suo tokenizer ottimizzato per il codice e i dati di addestramento. Questa messa a punto per l'elaborazione del codice si ripaga sia nei compiti di comprensione che di recupero.</p><p>Abbiamo eseguito un benchmark usando il corpus <code>jina-embeddings-v2-code</code>, confrontando tre modelli come backbone: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, e <code>RoBERTa-large</code>. Il test? <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - abbinare descrizioni testuali a frammenti di codice. ModernBERT ha superato entrambe le alternative su tutta la linea.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">Il divario ha senso - né </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> né </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> hanno visto linguaggi di programmazione durante l'addestramento. Nel frattempo, ModernBERT-large si è addestrato su due trilioni di token, inclusa una quantità sostanziale di codice. Questa esposizione alla sintassi e ai pattern di programmazione gli dà un chiaro vantaggio nei compiti relativi al codice. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> supera di poco </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">, probabilmente grazie ai suoi dati di addestramento multilingue più ampi - stessa architettura, maggiore esposizione. Tuttavia, entrambi sono significativamente indietro rispetto a ModernBERT-large.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Compito</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Media</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">Il vantaggio del Tokenizer</h3><p>Analizziamo perché ModernBERT gestisce così bene il codice - utilizza il <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">tokenizer OLMo</a>, specificamente addestrato sul codice, invece dei tokenizer standard BERT/RoBERTa.</p><p>Un tokenizer divide il testo UTF-8 in token che vengono mappati in vettori - questi sono ciò che il modello elabora effettivamente. Durante l'addestramento, impara a combinare sequenze di caratteri che si verificano frequentemente in singoli token. La differenza? Un tokenizer standard potrebbe dividere <code>init</code> in <code>in</code> + <code>it</code>, perdendo il contesto di programmazione. Ma il tokenizer di ModernBERT orientato al codice lo comprende senza spezzarlo.</p><p>Ecco dove diventa interessante la gestione degli spazi: ModernBERT preserva gli spazi iniziali di Python come token singoli e differenzia tra 4 e 8 spazi - cruciale per la struttura del codice. Nel frattempo, <strong><code>jina-XLM-RoBERTa</code> comprime tutti gli spazi continui in un singolo <code>_</code>, e RoBERTa-large tratta ogni spazio come un token separato.</strong> Questo significa che l'encoder di ModernBERT riceve input più puliti e significativi quando elabora il codice, mentre gli altri lavorano con token frammentati e meno coerenti.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT preserva gli spazi iniziali di Python come token singoli e differenzia tra 4 e 8 spazi - cruciale per la struttura del codice; mentre gli altri lavorano con token frammentati e meno coerenti.</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">Gestione del contesto lungo di ModernBERT</h2><p>ModernBERT ha fatto progressi significativi nell'elaborazione di testi lunghi, grazie al suo ampio corpus di addestramento (300B token con campioni da 8.192 token) e tecniche avanzate come l'attenzione combinata globale e locale.</p><p>Per valutare le capacità di gestione dei documenti lunghi, abbiamo utilizzato il <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">dataset MLDR</a> - un benchmark completo per testi lunghi che copre 13 lingue. Poiché ModernBERT attualmente supporta solo l'inglese, ci siamo concentrati sul sottoinsieme inglese di MLDR per confrontare ModernBERT con <code>jina-XLM-RoBERTa</code>. Mentre entrambi questi modelli possono gestire input di 8K token, <code>RoBERTa-large</code> è stato escluso da questo benchmark a causa del suo limite di 512 token, insufficiente per l'analisi di testi lunghi.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Le prestazioni superiori di ModernBERT non sono dovute solo al suo esteso addestramento su testi lunghi - sono in gran parte dovute alla sua innovativa combinazione di meccanismi di attenzione globale e locale. A differenza di <code>jina-XLM-RoBERTa</code>, che applica un'attenzione globale computazionalmente costosa a ogni livello, ModernBERT adotta un approccio più efficiente. Alterna tra attenzione globale (usata ogni terzo livello con un <code>theta</code> di 160.000) e attenzione locale (usando una finestra scorrevole di 128 token con un <code>theta</code> di 100.000). Questa strategia ibrida mantiene alte prestazioni riducendo drasticamente i tempi di addestramento.</p><blockquote>In ModernBERT, ogni terzo livello impiega attenzione globale con un theta RoPE di 160.000 e i livelli rimanenti utilizzano una finestra scorrevole locale di 128 token con un theta RoPE di 10.000. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">La lezione amara?</h2><p>La legge di scaling e <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">la lezione amara</a> suggeriscono che i principali miglioramenti delle prestazioni derivano principalmente dall'aumento del numero di parametri e dei dati di addestramento. Questo principio ha guidato il nostro approccio di espandere il corpus e utilizzare LoRA per adattamenti specifici ai task.</p><p>Tuttavia, il successo di ModernBERT ha rivelato che abbiamo sottovalutato il potere dell'ottimizzazione architettonica. Dimostra che gli SLM possono raggiungere risultati eccezionali attraverso una migliore efficienza dati-modello, senza necessariamente aumentare i parametri. Un recente <a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">report tecnico di Stella Embeddings</a> rafforza questa scoperta, indicando che gli attuali metodi di addestramento dei modelli di embedding possono essere migliorati senza aumentare le dimensioni del corpus o del modello.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Legge di scaling dei modelli di embedding. La performance media MTEB sui task in inglese è tracciata rispetto al numero di parametri del modello. Ogni punto rappresenta un modello di embedding. La linea di tendenza, che rappresenta tutti i modelli, è evidenziata, con i modelli multilingue enfatizzati in ciano. Si può vedere che </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> dimostra prestazioni superiori rispetto ai modelli di dimensioni simili, mostrando anche un miglioramento superlineare rispetto al suo predecessore, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">. Questo grafico è stato creato selezionando i primi 100 modelli di embedding dalla classifica MTEB, escludendo quelli senza informazioni sulle dimensioni, tipicamente modelli closed-source o proprietari. Sono state filtrate anche le submission identificate come trolling evidente.</span></figcaption></figure><p>Andando avanti, prevediamo costi computazionali inferiori e dimensioni dei modelli più piccole man mano che acquisiamo una comprensione più profonda dell'utilizzo dei dati e implementiamo le tecniche di ModernBERT. Nel breve termine, possiamo implementare i miglioramenti diretti delineati nel paper di ModernBERT - in particolare integrando più dati relativi al codice e adottando un tokenizer ottimizzato per il codice. Cambiamenti più complessi, come il passaggio a un'architettura deep-and-thin o il bootstrapping di modelli grandi da quelli più piccoli, richiederanno la costruzione di modelli backbone da zero - un'iniziativa a medio termine.</p><p>Mentre l'efficienza di ModernBERT è notevole, la sua limitazione al solo testo indica sfide future. Con la crescente popolarità dei modelli di embedding multimodali, la nostra prossima sfida è sviluppare modelli di ricerca fondamentali più intelligenti, veloci e capaci che possano gestire input per applicazioni multimodali. Queste applicazioni richiedono finestre di contesto ancora più lunghe - una sfida di efficienza che resta da risolvere.</p><h2 id=\"conclusion\">Conclusione</h2><p>In questo post, abbiamo esplorato come ModernBERT fa avanzare i modelli della famiglia BERT attraverso tre innovazioni chiave: la sua architettura deep-and-thin, il tokenizer ottimizzato e lo scaling efficiente utilizzando il weight tiling. Questi miglioramenti permettono a ModernBERT di offrire prestazioni eccezionali in dimensioni relativamente compatte, superando sia <code>RoBERTa-large</code> che <code>jina-XLM-RoBERTa</code> in vari task. ModernBERT dimostra che i miglioramenti architettonici possono contare più delle dimensioni dei parametri, aprendo le porte a modelli più efficienti. Il suo uso efficace del weight tiling mostra come lo scaling progressivo possa ridurre i costi di addestramento mantenendo o addirittura migliorando le prestazioni. Inoltre, il suo vocabolario compatto e le ottimizzazioni mirate suggeriscono crescenti opportunità per SLM specializzati in ambienti con risorse limitate.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "finding-optimal-breakpoints-in-long-documents-using-small-language-models",
  "id": "67126986708dbe00019249f2",
  "uuid": "b7e55a5d-f267-4a4a-b861-27221c0f3827",
  "title": "Trovare i punti di interruzione ottimali in documenti lunghi utilizzando modelli linguistici di piccole dimensioni",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Questa √® la parte III della nostra serie sul chunking. <b><strong style=\"white-space: pre-wrap;\">Ordine di lettura consigliato: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper di ricerca</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, parte III.</strong></b></div></div><p>Nei nostri post precedenti, abbiamo esplorato le sfide del chunking e <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">introdotto il concetto di late chunking</a>, che aiuta a ridurre la perdita di contesto durante l'embedding dei chunk. In questo post, ci concentreremo su un'altra sfida: trovare i punti di interruzione ottimali. Mentre <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io#late-chunking-is-resilient-to-poor-boundary-cues\" rel=\"noreferrer\">la nostra strategia di late chunking si √® dimostrata piuttosto resiliente a confini non ottimali</a>, questo non significa che possiamo ignorarli‚Äîsono ancora importanti sia per la leggibilit√† umana che per quella LLM. Ecco la nostra prospettiva: quando si determinano i punti di interruzione, possiamo ora concentrarci completamente sulla leggibilit√† senza preoccuparci della perdita semantica o di contesto. Il late chunking pu√≤ gestire sia confini buoni che cattivi, quindi la leggibilit√† diventa la tua preoccupazione principale.</p><p>Con questo in mente, abbiamo addestrato tre piccoli modelli linguistici specificamente progettati per segmentare documenti lunghi mantenendo la coerenza semantica e gestendo strutture di contenuto complesse. Sono:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>simple-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, che segmenta il testo basandosi sugli elementi strutturali del documento.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>topic-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, che segmenta il testo basandosi sui topic presenti nel testo.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-summary-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>summary-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, che genera riassunti per ogni segmento.</span></p></figcaption></figure><p>In questo post, discuteremo perch√© abbiamo sviluppato questo modello, come abbiamo approcciato le sue tre varianti e come si confrontano con la <a href=\"https://www.notion.so/Advancing-Segmentation-Strategies-in-RAG-with-a-Custom-Small-Language-Model-Restructure-638b84ae461d412eb6889cfa7f54cce1?pvs=21&ref=jina-ai-gmbh.ghost.io\">Segmenter API di Jina AI</a>. Infine, condivideremo ci√≤ che abbiamo imparato e alcune riflessioni per il futuro.</p><h2 id=\"segmentation-problem\">Il Problema della Segmentazione</h2><p>La segmentazione √® un elemento fondamentale nei sistemi RAG. Il modo in cui dividiamo i documenti lunghi in segmenti coerenti e gestibili influenza direttamente la qualit√† sia delle fasi di recupero che di generazione, influenzando tutto, dalla pertinenza delle risposte alla qualit√† della sintesi. I metodi di segmentazione tradizionali hanno prodotto risultati discreti ma non sono privi di limitazioni.</p><p>Per parafrasare il nostro <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\">post precedente</a>:</p><blockquote>Quando si segmenta un documento lungo, una sfida chiave √® decidere dove creare i segmenti. Questo pu√≤ essere fatto usando lunghezze fisse di token, un numero fisso di frasi o metodi pi√π avanzati come regex e modelli di segmentazione semantica. Stabilire confini di segmento accurati √® cruciale, poich√© non solo migliora la leggibilit√† dei risultati di ricerca ma assicura anche che i segmenti forniti a un LLM in un sistema RAG siano sia precisi che sufficienti.</blockquote><p>Mentre il late chunking migliora le prestazioni di recupero, <strong>nelle applicazioni RAG, √® cruciale assicurarsi che, per quanto possibile, ogni segmento sia significativo di per s√©, e non solo un chunk casuale di testo.</strong> Gli LLM si basano su dati coerenti e ben strutturati per generare risposte accurate. Se i segmenti sono incompleti o privi di significato, l'LLM potrebbe avere difficolt√† con il contesto e l'accuratezza, impattando le prestazioni complessive nonostante i benefici del late chunking. In breve, che si utilizzi o meno il late chunking, avere una solida strategia di segmentazione √® essenziale per costruire un sistema RAG efficace (come vedrai nella sezione benchmark pi√π avanti).</p><p>I metodi di segmentazione tradizionali, sia che dividano il contenuto a confini semplici come nuove righe o frasi, sia che utilizzino regole rigide basate sui token, spesso affrontano le stesse limitazioni. Entrambi gli approcci non tengono conto dei confini semantici e faticano con topic ambigui, portando a segmenti frammentati. Per affrontare queste sfide, abbiamo sviluppato e addestrato un piccolo modello linguistico specificamente per la segmentazione, progettato per catturare i cambi di topic e mantenere la coerenza pur rimanendo efficiente e adattabile a vari compiti.</p><h2 id=\"why-small-language-model\">Perch√© un Small Language Model?</h2><p>Abbiamo sviluppato uno Small Language Model (SLM) per affrontare specifiche limitazioni che abbiamo incontrato con le tecniche di segmentazione tradizionali, in particolare quando si gestiscono snippet di codice e altre strutture complesse come tabelle, liste e formule. Negli approcci tradizionali, che spesso si basano su conteggi di token o regole strutturali rigide, era difficile mantenere l'integrit√† di contenuti semanticamente coerenti. Per esempio, gli snippet di codice venivano frequentemente segmentati in pi√π parti, rompendo il loro contesto e rendendo pi√π difficile per i sistemi downstream comprenderli o recuperarli accuratamente.</p><p>Addestrando uno SLM specializzato, miravamo a creare un modello che potesse riconoscere intelligentemente e preservare questi confini significativi, assicurando che gli elementi correlati rimanessero insieme. Questo non solo migliora la qualit√† del recupero nei sistemi RAG ma migliora anche i task downstream come la sintesi e il question-answering, dove mantenere segmenti coerenti e contestualmente rilevanti √® critico. L'approccio SLM offre una soluzione pi√π adattabile e specifica per il task che i metodi di segmentazione tradizionali, con i loro confini rigidi, semplicemente non possono fornire.</p><h2 id=\"training-slms-three-approaches\">Addestramento degli SLM: Tre Approcci</h2><p>Abbiamo addestrato tre versioni del nostro SLM:</p><ul><li><code>simple-qwen-0.5</code> √® il modello pi√π diretto, progettato per identificare i confini basati sugli elementi strutturali del documento. La sua semplicit√† lo rende una soluzione efficiente per esigenze di segmentazione di base.</li><li><code>topic-qwen-0.5</code>, ispirato al ragionamento Chain-of-Thought, porta la segmentazione un passo avanti identificando i topic all'interno del testo, come \"l'inizio della Seconda Guerra Mondiale\", e utilizzando questi topic per definire i confini dei segmenti. Questo modello assicura che ogni segmento sia coeso dal punto di vista tematico, rendendolo ben adatto per documenti complessi con pi√π topic. I test iniziali hanno mostrato che eccelle nel segmentare contenuti in un modo che rispecchia da vicino l'intuizione umana.</li><li><code>summary-qwen-0.5</code> non solo identifica i confini del testo ma genera anche riassunti per ogni segmento. Riassumere i segmenti √® altamente vantaggioso nelle applicazioni RAG, specialmente per task come il question-answering su documenti lunghi, sebbene comporti il compromesso di richiedere pi√π dati durante l'addestramento.</li></ul><p>Tutti i modelli restituiscono solo <em>segment heads</em>‚Äîuna versione troncata di ogni segmento. Invece di generare segmenti interi, i modelli producono punti chiave o sottotopic, che migliora il rilevamento dei confini e la coerenza concentrandosi sulle transizioni semantiche piuttosto che semplicemente copiare il contenuto di input. Quando si recuperano i segmenti, il testo del documento viene diviso basandosi su queste segment heads, e i segmenti completi vengono ricostruiti di conseguenza.</p><h3 id=\"dataset\">Dataset</h3><p>Abbiamo utilizzato il dataset <a href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\">wiki727k</a>, una vasta collezione di snippet di testo strutturati estratti da articoli di Wikipedia. Contiene oltre 727.000 sezioni di testo, ognuna rappresentante una parte distinta di un articolo di Wikipedia, come un'introduzione, una sezione o una sottosezione.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - koomri/text-segmentation: Implementation of the paper: Text Segmentation as a Supervised Learning Task</div><div class=\"kg-bookmark-description\">Implementazione dell'articolo: Text Segmentation as a Supervised Learning Task - koomri/text-segmentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">koomri</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/a0a75db005774d424366f3fa2d4c70930927a0b2d8032ef3c04cb0f3beebcb8e/koomri/text-segmentation\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"data-augmentation\">Data Augmentation</h3><p>Per generare coppie di addestramento per ogni variante del modello, abbiamo utilizzato GPT-4 per aumentare i nostri dati. Per ogni articolo nel nostro dataset di addestramento, abbiamo inviato il seguente prompt:</p><pre><code class=\"language-python\">f\"\"\"\nGenerate a five to ten words topic and a one sentence summary for this chunk of text.\n```\n{text}\n```\nMake sure the topic is concise and the summary covers the main topic as much as possible.\n\nPlease respond in the following format:\n```\nTopic: ...\nSummary: ...\n```\n\nDirectly respond with the required topic and summary, do not include any other details, and do not surround your response with quotes, backticks or other separators.\n   \"\"\".strip()</code></pre><p>Abbiamo utilizzato una semplice divisione per generare sezioni da ogni articolo, dividendo su <code>\\\\n\\\\n\\\\n</code>, e poi suddividendo ulteriormente su <code>\\\\n\\\\n</code> per ottenere quanto segue (in questo caso, un articolo su Common Gateway Interface):</p><pre><code>[\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n</code></pre><p>Abbiamo poi generato una struttura JSON con le sezioni, gli argomenti e i riassunti:</p><pre><code>{\n  \"sections\": [\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n  \"topics\": [\n    \"Common Gateway Interface in Web Servers\",\n    \"The History and Standardization of CGI\",\n    \"CGI Scripts for Editing Web Pages\",\n    \"Reducing Web Server Overhead in Command Invocation\"\n  ],\n  \"summaries\": [\n    \"CGI fornisce un protocollo per i web server per eseguire programmi che generano pagine web dinamiche.\",\n    \"NCSA ha inizialmente definito CGI nel 1993, portando alla sua adozione come standard per i Web server e alla successiva formalizzazione in RFC 3875 presieduta da Ken Coar.\",\n    \"Questo testo descrive come uno script CGI pu√≤ gestire la modifica e il salvataggio del contenuto delle pagine web attraverso form HTML.\",\n    \"Il testo discute le tecniche per minimizzare il sovraccarico del server dovuto alla frequente invocazione di comandi, incluso il preforking dei processi, l'utilizzo di programmi CGI precompilati e l'implementazione di moduli web server personalizzati.\"\n  ]\n}\n</code></pre><p>Abbiamo anche aggiunto rumore mescolando i dati, aggiungendo caratteri/parole/lettere casuali, rimuovendo casualmente la punteggiatura e rimuovendo <em>sempre</em> i caratteri di nuova riga.</p><p>Tutto ci√≤ pu√≤ contribuire in parte allo sviluppo di un buon modello, ma solo fino a un certo punto. Per ottenere davvero il massimo, avevamo bisogno che il modello creasse blocchi coerenti senza compromettere gli snippet di codice. Per fare questo, abbiamo arricchito il dataset con codice, formule ed elenchi generati da GPT-4o.</p><h3 id=\"the-training-setup\">Il Setup dell'Addestramento</h3><p>Per addestrare i modelli, abbiamo implementato il seguente setup:</p><ul><li><strong>Framework</strong>: Abbiamo utilizzato la libreria <code>transformers</code> di Hugging Face integrata con <code>Unsloth</code> per l'ottimizzazione del modello. Questo √® stato cruciale per ottimizzare l'uso della memoria e accelerare l'addestramento, rendendo possibile addestrare efficacemente modelli piccoli con dataset di grandi dimensioni.</li><li><strong>Optimizer e Scheduler</strong>: Abbiamo utilizzato l'ottimizzatore AdamW con un learning rate lineare e warm-up steps, permettendoci di stabilizzare il processo di addestramento durante le epoche iniziali.</li><li><strong>Monitoraggio degli Esperimenti</strong>: Abbiamo monitorato tutti gli esperimenti di addestramento utilizzando <a href=\"https://wandb.ai/?ref=jina-ai-gmbh.ghost.io\">Weights & Biases</a>, registrando metriche chiave come la loss di training e validazione, i cambiamenti del learning rate e le prestazioni complessive del modello. Questo monitoraggio in tempo reale ci ha fornito indicazioni su come i modelli progredivano, permettendo rapidi aggiustamenti quando necessario per ottimizzare i risultati dell'apprendimento.</li></ul><h3 id=\"the-training-itself\">L'Addestramento</h3><p>Utilizzando <a href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=jina-ai-gmbh.ghost.io\"><code>qwen2-0.5b-instruct</code></a> come modello base, abbiamo addestrato tre varianti del nostro SLM con Unsloth, ciascuna con una diversa strategia di segmentazione in mente. Per i nostri campioni abbiamo utilizzato coppie di training, composte dal testo di un articolo da wiki727k e i risultanti <code>sections</code>, <code>topics</code>, o <code>summaries</code> (menzionati sopra nella sezione \"Data Augmentation\") a seconda del modello in addestramento.</p><ul><li><code><strong>simple-qwen-0.5</strong></code>: Abbiamo addestrato <code>simple-qwen-0.5</code> su 10.000 campioni con 5.000 step, ottenendo una rapida convergenza e rilevando efficacemente i confini tra sezioni di testo coese. La loss di training era 0,16.</li><li><code><strong>topic-qwen-0.5</strong></code>: Come <code>simple-qwen-0.5</code>, abbiamo addestrato <code>topic-qwen-0.5</code> su 10.000 campioni con 5.000 step, ottenendo una loss di training di 0,45.</li><li><code><strong>summary-qwen-0.5</strong></code>: Abbiamo addestrato <code>summary-qwen-0.5</code> su 30.000 campioni con 15.000 step. Questo modello ha mostrato potenziale ma ha avuto una loss pi√π alta (0,81) durante l'addestramento, suggerendo la necessit√† di pi√π dati (circa il doppio del nostro conteggio campioni originale) per raggiungere il suo pieno potenziale.</li></ul><h2 id=\"the-segments-themselves\">I Segmenti</h2><p>Ecco esempi di tre segmenti consecutivi da ciascuna strategia di segmentazione, insieme alla Jina Segmenter API. Per produrre questi segmenti abbiamo prima utilizzato <a href=\"https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io\">Jina Reader</a> per estrarre un post dal blog di Jina AI come testo semplice (inclusi tutti i dati della pagina, come header, footer, ecc), poi lo abbiamo passato a ciascun metodo di segmentazione.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/can-embedding-reranker-models-compare-numbers/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Can Embedding/Reranker Models Compare Numbers?</div><div class=\"kg-bookmark-description\">A lot of LLMs can't figure out that 9.11 is actually smaller than 9.9. Can our embedding and reranker models do any better?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"jina-segmenter-api\">Jina Segmenter API</h3><p>Jina Segmenter API ha adottato un approccio molto granulare alla segmentazione del post, dividendo su caratteri come <code>\\n</code>, <code>\\t</code>, ecc, per suddividere il testo in segmenti spesso molto piccoli. Guardando solo i primi tre, ha estratto <code>search\\\\n</code>, <code>notifications\\\\n</code> e <code>NEWS\\\\n</code> dalla barra di navigazione del sito web, ma nulla di rilevante per il contenuto del post stesso:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png\" class=\"kg-image\" alt=\"Minimalist navigation bar with &quot;NEWS&quot;, &quot;PRODUCTS&quot;, and &quot;COMPANY&quot; text on a black background, accented by colorful stripes to \" loading=\"lazy\" width=\"1164\" height=\"68\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Pi√π avanti, abbiamo finalmente ottenuto alcuni segmenti dal contenuto effettivo del blog post, anche se con poco contesto conservato in ciascuno:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png\" class=\"kg-image\" alt=\"Webpage discussing if embedding/reranker models can compare numbers, with a grid of numbered circles and references to an ICM\" loading=\"lazy\" width=\"1164\" height=\"1180\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>(Nell'interesse dell'equit√†, abbiamo mostrato pi√π segmenti per Segmenter API rispetto ai modelli, semplicemente perch√© altrimenti avrebbe avuto pochissimi segmenti significativi da mostrare)</p><h3 id=\"simple-qwen-05\"><code>simple-qwen-0.5</code></h3><p><code>simple-qwen-0.5</code> ha suddiviso il post del blog basandosi sulla struttura semantica, estraendo segmenti molto pi√π lunghi con un significato coeso:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png\" class=\"kg-image\" alt=\"Webpage screenshot with green background, top navigation bar, scientific graphs, and headers discussing model number comparis\" loading=\"lazy\" width=\"1164\" height=\"4590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"topic-qwen-05\"><code>topic-qwen-0.5</code></h3><p><code>topic-qwen-0.5</code> ha prima identificato i topic basandosi sul contenuto del documento, poi ha segmentato il documento in base a questi topic:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png\" class=\"kg-image\" alt=\"Webpage showcasing a scientific paper titled &quot;Can Embedding/Keras Models Compare Numbers?&quot; featuring plots, text blocks, and \" loading=\"lazy\" width=\"1164\" height=\"4526\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"summary-qwen-05\"><code>summary-qwen-0.5</code></h3><p><code>summary-qwen-0.5</code> ha identificato i confini dei segmenti e generato un riassunto del contenuto all'interno di ciascun segmento:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png\" class=\"kg-image\" alt=\"Green and gold-themed academic webpage discussing embedding/reranker models and experiment setup.\" loading=\"lazy\" width=\"1164\" height=\"3734\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"benchmarking-the-models\">Benchmark dei Modelli</h2><p>Per valutare le prestazioni dei nostri modelli, abbiamo estratto otto post dal blog di Jina AI e generato sei domande e risposte di riferimento utilizzando GPT-4o.</p><p>Abbiamo applicato ogni metodo di segmentazione, inclusa Jina Segmenter API, a questi post del blog, e poi generato embedding per i segmenti risultanti utilizzando <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v3</code></a>, senza late chunking o reranking.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/v3banner.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ogni set di segmenti √® stato poi indicizzato separatamente e abbiamo utilizzato un sistema RAG per interrogare ciascun indice con le domande generate in precedenza.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Per mantenere i confronti equi, durante il test di <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> e <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> abbiamo indicizzato solo i segmenti stessi, <i>non</i> i riassunti o gli argomenti generati. L'indicizzazione di questi dati aggiuntivi probabilmente migliorerebbe ulteriormente le prestazioni.</div></div><p>Per misurare le prestazioni, abbiamo confrontato le risposte generate dall'indice di ciascun metodo di segmentazione con le risposte di riferimento utilizzando i punteggi F1 dalla <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)?ref=jina-ai-gmbh.ghost.io\">valutazione ROUGE-L</a>, dove punteggi pi√π alti indicano prestazioni migliori.</p><p>Per esempio, data la domanda: \"Quali fattori influenzano le capacit√† di confronto numerico del modello <code>jina-embeddings-v2-base-en</code>?\", abbiamo ricevuto le seguenti risposte:</p><h3 id=\"ground-truth-generated-by-gpt-4o-from-the-full-post-text\">Risposta di Riferimento (Generata da GPT-4o dal Testo Completo del Post):</h3><pre><code>\"The two key factors are tokenization and training data. Tokenization determines how numbers are segmented into tokens, and training data influences numerical reasoning abilities.\"\n</code></pre><h3 id=\"jina-segmenter-api-score-0166667\">Jina Segmenter API (punteggio: 0.166667)</h3><pre><code>\"The context does not provide specific factors that impact the numerical comparison abilities of the jina-embeddings-v2-base-en model.\"\n</code></pre><h3 id=\"simple-qwen-05-score-0279070\"><code>simple-qwen-0.5</code> (punteggio: 0.279070)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by tokenization and training data.\"\n</code></pre><h3 id=\"topic-qwen-05-score-0190476\"><code>topic-qwen-0.5</code> (punteggio: 0.190476)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by factors such as tokenization and its ability to handle different numerical formats, including small integers, large numbers, floats, negative numbers, currency, dates, and times.\"\n</code></pre><h3 id=\"summary-qwen-05-0318182\"><code>summary-qwen-0.5</code> (0.318182)</h3><pre><code>\"The factors impacting the numerical comparison abilities of the jina-embeddings-v2-base-en model are tokenization and training data.\"\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>Perch√© il punteggio di </b><b><code>topic-qwen-0.5</code></b><b> √® cos√¨ basso?</b><br>Questo √® principalmente un caso particolare basato sulla specifica domanda che abbiamo posto al modello. Come si pu√≤ vedere nella tabella sottostante, il punteggio ROUGE <i>medio</i> di <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> √® il pi√π alto tra tutte le metodologie di segmentazione.</div></div><p>Abbiamo anche valutato la velocit√† di ciascun metodo (misurando il tempo necessario per generare ed incorporare i segmenti) e stimato lo spazio su disco (moltiplicando il numero di embedding per la dimensione di un singolo embedding a 1024 dimensioni da <code>jina-embeddings-v3</code>). Questo ci ha permesso di valutare sia l'accuratezza che l'efficienza tra le diverse strategie di segmentazione.</p><h2 id=\"key-findings\">Risultati Chiave</h2><p>Dopo aver testato le varianti del modello tra loro e con la Jina Segmenter API, abbiamo scoperto che i nuovi modelli hanno effettivamente mostrato punteggi migliori utilizzando tutti e tre i metodi, specialmente la segmentazione per argomenti:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png\" class=\"kg-image\" alt=\"Bar chart comparing average ROUGE scores for Jina Segmenter, Simple, COATopic, and Summary Segmentation.\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Segmentation Method</strong></th>\n<th><strong>Average ROUGE Score</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>0.352126</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>0.386096</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td><strong>0.398340</strong></td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>0.328143</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Perch√© <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> ha un punteggio ROUGE pi√π basso di <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code>? In breve, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> ha mostrato una perdita pi√π elevata durante l'addestramento, rivelando la necessit√† di un maggiore addestramento per ottenere risultati migliori. Questo potrebbe essere oggetto di sperimentazioni future.</div></div><p>Tuttavia, sarebbe interessante rivedere i risultati con la funzionalit√† di chunking ritardato di <code>jina-embeddings-v3</code>, che aumenta la rilevanza contestuale degli embedding dei segmenti, fornendo risultati pi√π pertinenti. Questo potrebbe essere argomento di un futuro post sul blog.</p><p>Per quanto riguarda la velocit√†, pu√≤ essere difficile confrontare i nuovi modelli con Jina Segmenter, poich√© quest'ultimo √® un'API, mentre abbiamo eseguito i tre modelli su una GPU Nvidia 3090. Come si pu√≤ vedere, qualsiasi vantaggio di prestazioni ottenuto durante la rapida fase di segmentazione dell'API Segmenter viene rapidamente superato dalla necessit√† di generare embedding per cos√¨ tanti segmenti:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png\" class=\"kg-image\" alt=\"Bar chart showing time for text segmentation methods: Jina Segmenter, Simple, CoT Topic, and Summary Segmentation, with notab\" loading=\"lazy\" width=\"1682\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png 1682w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png\" class=\"kg-image\" alt=\"Vertical bar chart displaying the embedding times for Jina Segmenter, Simple, CoT Topic, and Summary Segmentation.\" loading=\"lazy\" width=\"1698\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png 1698w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>Note</b><br>‚Ä¢ Usiamo assi Y differenti su entrambi i grafici perch√© presentare intervalli di tempo cos√¨ diversi con un solo grafico o assi Y coerenti non era fattibile.<br>‚Ä¢ Poich√© stavamo eseguendo questo puramente come esperimento, non abbiamo utilizzato il batching durante la generazione degli embedding. Farlo accelererebbe sostanzialmente le operazioni per tutti i metodi.</div></div><p>Naturalmente, pi√π segmenti significano pi√π embedding. E questi embedding occupano molto spazio: Gli embedding per gli otto post del blog che abbiamo testato occupavano oltre 21 MB con Segmenter API, mentre la Segmentazione per Riassunti si √® attestata su un agile 468 KB. Questo, insieme ai punteggi ROUGE pi√π alti dei nostri modelli, significa meno segmenti ma segmenti migliori, risparmiando denaro e aumentando le prestazioni:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png\" class=\"kg-image\" alt=\"Grafico a barre verticali che confronta la dimensione totale dell'embedding dei metodi di segmentazione, con &quot;Jina Segmenter&quot; significativamente pi√π alto a 20.0\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Segmentation Method</strong></th>\n<th><strong>Segment Count</strong></th>\n<th><strong>Average Length (characters)</strong></th>\n<th><strong>Segmentation Time (minutes/seconds)</strong></th>\n<th><strong>Embedding Time (hours/minutes)</strong></th>\n<th><strong>Total Embedding Size</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>1,755</td>\n<td>82</td>\n<td>3.8s</td>\n<td>1h 46m</td>\n<td>21.06 MB</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>48</td>\n<td>1,692</td>\n<td>49s</td>\n<td>1h 2m</td>\n<td>576 KB</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td>69</td>\n<td>1,273</td>\n<td>2m 3s</td>\n<td>1h 6m</td>\n<td>828 KB</td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>39</td>\n<td>1,799</td>\n<td>2m 40s</td>\n<td>53m</td>\n<td>468 KB</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"what-we-learned\">Cosa Abbiamo Imparato</h3><h3 id=\"problem-formulation-is-critical\">La Formulazione del Problema √® Fondamentale</h3><p>Un'intuizione chiave √® stata l'impatto di come abbiamo inquadrato il compito. Facendo generare al modello le intestazioni dei segmenti, abbiamo migliorato il rilevamento dei confini e la coerenza concentrandoci sulle transizioni semantiche piuttosto che semplicemente copiare e incollare il contenuto di input in segmenti separati. Questo ha anche portato a un modello di segmentazione pi√π veloce, poich√© la generazione di meno testo ha permesso al modello di completare il compito pi√π rapidamente.</p><h3 id=\"llm-generated-data-is-effective\">I Dati Generati da LLM sono Efficaci</h3><p>L'utilizzo di dati generati da LLM, in particolare per contenuti complessi come liste, formule e snippet di codice, ha ampliato il set di training del modello e migliorato la sua capacit√† di gestire strutture documentali diverse. Questo ha reso il modello pi√π adattabile a vari tipi di contenuti, un vantaggio cruciale quando si tratta di documenti tecnici o strutturati.</p><h3 id=\"output-only-data-collation\">Raccolta Dati Solo Output</h3><p>Utilizzando un <a href=\"https://huggingface.co/docs/transformers/en/main_classes/data_collator?ref=jina-ai-gmbh.ghost.io\">data collator</a> solo output, abbiamo assicurato che il modello si concentrasse sulla previsione dei token target durante l'addestramento, piuttosto che limitarsi a copiare dall'input. Il collator solo output ha garantito che il modello apprendesse dalle effettive sequenze target, enfatizzando i completamenti o i confini corretti. Questa distinzione ha permesso al modello di convergere pi√π velocemente evitando l'overfitting sull'input e lo ha aiutato a generalizzare meglio su diversi dataset.</p><h3 id=\"efficient-training-with-unsloth\">Training Efficiente con Unsloth</h3><p>Con <a href=\"https://github.com/unslothai/unsloth?ref=jina-ai-gmbh.ghost.io\">Unsloth</a>, abbiamo ottimizzato l'addestramento del nostro piccolo modello linguistico, riuscendo a eseguirlo su una GPU Nvidia 4090. Questa pipeline ottimizzata ci ha permesso di addestrare un modello efficiente e performante senza la necessit√† di enormi risorse computazionali.</p><h3 id=\"handling-complex-texts\">Gestione di Testi Complessi</h3><p>I modelli di segmentazione si sono distinti nella gestione di documenti complessi contenenti codice, tabelle e liste, che sono tipicamente difficili per i metodi pi√π tradizionali. Per contenuti tecnici, strategie sofisticate come <code>topic-qwen-0.5</code> e <code>summary-qwen-0.5</code> sono state pi√π efficaci, con il potenziale di migliorare le attivit√† RAG a valle.</p><h3 id=\"simple-methods-for-simpler-content\">Metodi Semplici per Contenuti pi√π Semplici</h3><p>Per contenuti lineari e narrativi, metodi pi√π semplici come l'API Segmenter sono spesso sufficienti. Le strategie di segmentazione avanzate potrebbero essere necessarie solo per contenuti pi√π complessi e strutturati, consentendo flessibilit√† a seconda del caso d'uso.</p><h2 id=\"next-steps\">Prossimi Passi</h2><p>Sebbene questo esperimento fosse progettato principalmente come proof of concept, se dovessimo estenderlo ulteriormente, potremmo apportare diversi miglioramenti. Innanzitutto, anche se √® improbabile la continuazione di questo specifico esperimento, l'addestramento di <code>summary-qwen-0.5</code> su un dataset pi√π grande‚Äîidealmente 60.000 campioni invece di 30.000‚Äîporterebbe probabilmente a prestazioni pi√π ottimali. Inoltre, sarebbe vantaggioso perfezionare il nostro processo di benchmarking. Invece di valutare le risposte generate dall'LLM dal sistema RAG, ci concentreremmo invece sul confronto diretto dei segmenti recuperati con la verit√† di base. Infine, andremmo oltre i punteggi ROUGE e adotteremmo metriche pi√π avanzate (possibilmente una combinazione di ROUGE e punteggio LLM) che catturino meglio le sfumature della qualit√† del recupero e della segmentazione.</p><h2 id=\"conclusion\">Conclusione</h2><p>In questo esperimento, abbiamo esplorato come i modelli di segmentazione personalizzati progettati per compiti specifici possano migliorare le prestazioni del RAG. Sviluppando e addestrando modelli come <code>simple-qwen-0.5</code>, <code>topic-qwen-0.5</code> e <code>summary-qwen-0.5</code>, abbiamo affrontato le sfide chiave riscontrate nei metodi di segmentazione tradizionali, in particolare nel mantenimento della coerenza semantica e nella gestione efficace di contenuti complessi come gli snippet di codice. Tra i modelli testati, <code>topic-qwen-0.5</code> ha costantemente fornito la segmentazione pi√π significativa e contestualmente rilevante, specialmente per documenti multi-argomento.</p><p>Mentre i modelli di segmentazione forniscono la base strutturale necessaria per i sistemi RAG, svolgono una funzione diversa rispetto al late chunking, che ottimizza le prestazioni di recupero mantenendo la rilevanza contestuale tra i segmenti. Questi due approcci possono essere complementari, ma la segmentazione √® particolarmente cruciale quando √® necessario un metodo che si concentri sulla suddivisione dei documenti per flussi di lavoro di generazione coerenti e specifici per il compito.</p>",
  "comment_id": "67126986708dbe00019249f2",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/breakpoints.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-18T15:58:30.000+02:00",
  "updated_at": "2024-10-25T20:14:53.000+02:00",
  "published_at": "2024-10-25T10:35:07.000+02:00",
  "custom_excerpt": "We trained three small language models to better segment long documents into chunks, and here are the key lessons we learned.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/finding-optimal-breakpoints-in-long-documents-using-small-language-models/",
  "excerpt": "Abbiamo addestrato tre piccoli language model per segmentare meglio i documenti lunghi in chunk e queste sono le lezioni chiave che abbiamo appreso.",
  "reading_time": 19,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast.",
  "feature_image_caption": null
}
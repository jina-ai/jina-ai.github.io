{
  "slug": "a-deep-dive-into-tokenization",
  "id": "65afb3ee8da8040001e17061",
  "uuid": "02d119e4-ed5f-4edf-8b66-65aea1386d96",
  "title": "Un'analisi approfondita della tokenizzazione",
  "html": "<p>Ci sono molte barriere alla comprensione dei modelli AI, alcune piuttosto significative, che possono ostacolare l'implementazione dei processi AI. Ma la prima che molte persone incontrano √® capire cosa intendiamo quando parliamo di <strong>token</strong>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">Free API to tokenize texts, count and get first/last-N tokens.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\"></div></a></figure><p>Uno dei parametri pratici pi√π importanti nella scelta di un modello linguistico AI √® la dimensione della sua finestra di contesto ‚Äî la dimensione massima del testo in input ‚Äî che viene espressa in token, non in parole o caratteri o altre unit√† automaticamente riconoscibili.</p><p>Inoltre, i servizi di embedding sono tipicamente calcolati \"per token\", il che significa che i token sono importanti per comprendere la fattura.</p><p>Questo pu√≤ essere molto confuso se non si ha chiaro cosa sia un token.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png\" class=\"kg-image\" alt=\"Listino prezzi attuale di Jina Embeddings (febbraio 2024).\" loading=\"lazy\" width=\"2000\" height=\"1036\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Listino prezzi attuale di Jina Embeddings (febbraio 2024). Nota che i prezzi sono indicati per \"1M token\".</span></figcaption></figure><p>Ma tra tutti gli aspetti confusi dell'AI moderna, i token sono probabilmente i meno complicati. Questo articolo cercher√† di chiarire cosa sia la tokenizzazione, cosa fa e perch√© la facciamo in questo modo.</p><h2 id=\"tldr\">tl;dr</h2><p>Per coloro che desiderano o necessitano di una risposta rapida per calcolare quanti token acquistare da Jina Embeddings o una stima di quanti prevedono di dover acquistare, ecco le statistiche che state cercando.</p><h3 id=\"tokens-per-english-word\">Token per Parola Inglese</h3><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Una chiamata all'API di Jina Embeddings v2 per i modelli inglesi utilizzer√† <b><strong style=\"white-space: pre-wrap;\">approssimativamente</strong></b> <b><strong style=\"white-space: pre-wrap;\">il 10% in pi√π</strong></b> di token rispetto al numero di parole nel testo, <b><strong style=\"white-space: pre-wrap;\">pi√π due token per embedding</strong></b>.</div></div><p>Durante i test empirici, descritti pi√π avanti in questo articolo, una variet√† di testi in inglese √® stata convertita in token con un rapporto di circa il 10% in pi√π di token rispetto alle parole, utilizzando i modelli solo inglese di Jina Embeddings. Questo risultato √® stato piuttosto robusto.</p><p>I modelli Jina Embeddings v2 hanno una finestra di contesto di 8192 token. Questo significa che se passi a un modello Jina un testo in inglese pi√π lungo di 7.400 parole, c'√® una buona probabilit√† che venga troncato.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">La dimensione massima per l'input di <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 per l'inglese</strong></b> √® approssimativamente <b><strong style=\"white-space: pre-wrap;\">7.400 parole</strong></b>.</div></div><h3 id=\"tokens-per-chinese-character\">Token per Carattere Cinese</h3><p>Per il cinese, i risultati sono pi√π variabili. A seconda del tipo di testo, i rapporti variano da 0,6 a 0,75 token per carattere cinese (Ê±âÂ≠ó). I testi in inglese dati a Jina Embeddings v2 per il cinese producono approssimativamente lo stesso numero di token di Jina Embeddings v2 per l'inglese: circa il 10% in pi√π rispetto al numero di parole.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">La dimensione massima per l'input in cinese di <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 per cinese e inglese</strong></b> √® approssimativamente <b><strong style=\"white-space: pre-wrap;\">10.500 caratteri</strong></b> (<b><strong style=\"white-space: pre-wrap;\">Â≠óÊï∞</strong></b>), o <b><strong style=\"white-space: pre-wrap;\">da 0,6 a 0,75 token per carattere cinese, pi√π due per embedding.</strong></b></div></div><h3 id=\"tokens-per-german-word\">Token per Parola Tedesca</h3><p>I rapporti parola-token in tedesco sono pi√π variabili rispetto all'inglese ma meno del cinese. A seconda del genere del testo, ho ottenuto in media dal 20% al 30% in pi√π di token rispetto alle parole. Dare testi in inglese a Jina Embeddings v2 per tedesco e inglese usa alcuni token in pi√π rispetto ai modelli solo inglese e cinese/inglese: dal 12% al 15% in pi√π di token rispetto alle parole.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Jina Embeddings v2 per tedesco e inglese conter√† <b><strong style=\"white-space: pre-wrap;\">dal 20% al 30% in pi√π di token rispetto alle parole, pi√π due per embedding</strong></b>. La dimensione massima del contesto di input √® approssimativamente <b><strong style=\"white-space: pre-wrap;\">6.300 parole tedesche</strong></b>.</div></div><h3 id=\"caution\">Attenzione!</h3><p>Questi sono calcoli semplici, ma dovrebbero essere approssimativamente corretti per la maggior parte dei testi in linguaggio naturale e per la maggior parte degli utenti. In definitiva, possiamo solo promettere che il numero di token sar√† sempre non superiore al numero di caratteri nel tuo testo, pi√π due. In pratica sar√† sempre molto meno di quello, ma non possiamo promettere in anticipo alcun conteggio specifico.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">I Risultati Possono Variare! </strong></b><br><br>Queste sono stime basate su calcoli statisticamente ingenui. Non garantiamo quanti token richieder√† una particolare richiesta.</div></div><p>Se ti serve solo un consiglio su quanti token acquistare per Jina Embeddings, puoi fermarti qui. Altri modelli di embedding, di aziende diverse da Jina AI, potrebbero non avere gli stessi rapporti token-parola e token-carattere-cinese dei modelli Jina, ma in generale non saranno molto diversi.</p><p>Se vuoi capire il perch√©, il resto di questo articolo √® un'analisi pi√π approfondita della tokenizzazione per i modelli linguistici.</p><h2 id=\"words-tokens-numbers\">Parole, Token, Numeri</h2><p>La tokenizzazione fa parte dell'elaborazione del linguaggio naturale da prima che esistessero i moderni modelli AI.</p><p>√à un po' un clich√© dire che tutto in un computer √® solo un numero, ma √® anche per lo pi√π vero. Il linguaggio, tuttavia, non √® naturalmente solo un insieme di numeri. Potrebbe essere parlato, composto da onde sonore, o scritto, fatto di segni su carta, o anche un'immagine di un testo stampato o un video di qualcuno che usa il linguaggio dei segni. Ma la maggior parte delle volte, quando parliamo di usare i computer per elaborare il linguaggio naturale, intendiamo testi composti da sequenze di caratteri: lettere (a, b, c, ecc.), numeri (0, 1, 2‚Ä¶), punteggiatura e spazi, in diverse lingue e codifiche testuali.</p><p>Gli ingegneri informatici le chiamano \"stringhe\".</p><p>I modelli linguistici AI prendono sequenze di numeri come input. Quindi, potresti scrivere la frase:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Ma, dopo la tokenizzazione, il modello AI riceve come input:</p><pre><code class=\"language-python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>La tokenizzazione √® il processo di conversione di una stringa di input in una specifica sequenza di numeri che il tuo modello AI pu√≤ comprendere.</p><p>Quando usi un modello AI tramite un'API web che addebita agli utenti per token, ogni richiesta viene convertita in una sequenza di numeri come quella sopra. Il numero di token nella richiesta √® la lunghezza di quella sequenza di numeri. Quindi, chiedere a Jina Embeddings v2 for English di darti un embedding per \"<em>What is today's weather in Berlin?</em>\" ti coster√† 11 token perch√© ha convertito quella frase in una sequenza di 11 numeri prima di passarla al modello AI.</p><p>I modelli AI basati sull'<a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io\">architettura Transformer</a> hanno una <strong>finestra di contesto</strong> di dimensione fissa misurata in token. A volte viene chiamata \"finestra di input\", \"dimensione del contesto\" o \"lunghezza della sequenza\" (specialmente sulla <a href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io\">classifica MTEB di Hugging Face</a>). Significa la dimensione massima del testo che il modello pu√≤ vedere in una volta.</p><p>Quindi, se vuoi usare un modello di embedding, questa √® la dimensione massima di input consentita.</p><p>I modelli Jina Embeddings v2 hanno tutti una finestra di contesto di 8.192 token. Altri modelli avranno finestre di contesto diverse (tipicamente pi√π piccole). Questo significa che qualunque sia la quantit√† di testo che inserisci, il tokenizer associato a quel modello Jina Embeddings deve convertirlo in non pi√π di 8.192 token.</p><h2 id=\"mapping-language-to-numbers\">Mappare il Linguaggio in Numeri</h2><p>Il modo pi√π semplice per spiegare la logica dei token √® questo:</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Un token √® un numero che rappresenta una parte di una stringa.</div></div><p>Per i modelli di linguaggio naturale, la parte di stringa che un token rappresenta √® una parola, una parte di una parola o un pezzo di punteggiatura. Gli spazi generalmente non ricevono alcuna rappresentazione esplicita nell'output del tokenizer.</p><p>La tokenizzazione fa parte di un gruppo di tecniche nell'elaborazione del linguaggio naturale chiamate <a href=\"https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io\"><em>segmentazione del testo</em></a>, e il modulo che esegue la tokenizzazione √® chiamato, molto logicamente, <strong>tokenizer</strong>.</p><p>Per mostrare come funziona la tokenizzazione, tokenizzeremo alcune frasi usando il pi√π piccolo modello Jina Embeddings v2 per l'inglese: <code>jina-embeddings-v2-small-en</code>. L'altro modello solo inglese di Jina Embeddings ‚Äî <code>jina-embeddings-v2-base-en</code> ‚Äî usa lo stesso tokenizer, quindi non ha senso scaricare megabyte extra di modello AI che non useremo in questo articolo.</p><p>Prima, installa il modulo <code>transformers</code> nel tuo ambiente Python o notebook. Usa ilIl flag <code>-U</code> assicura l'aggiornamento all'ultima versione poich√© questo modello non funzioner√† con alcune versioni precedenti:</p><pre><code class=\"language-bash\">pip install -U transformers\n</code></pre><p>Quindi, scarica <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>jina-embeddings-v2-small-en</code></a> usando <code>AutoModel.from_pretrained</code>:</p><pre><code class=\"language-Python\">from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n</code></pre><p>Per tokenizzare una stringa, usa il metodo <code>encode</code> dell'oggetto <code>tokenizer</code> del modello:</p><pre><code class=\"language-Python\">model.tokenizer.encode(\"What is today's weather in Berlin?\")\n</code></pre><p>Il risultato √® una lista di numeri:</p><pre><code class=\"language-Python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>Per convertire questi numeri in forma di stringa, usa il metodo <code>convert_ids_to_tokens</code> dell'oggetto <code>tokenizer</code>:</p><pre><code class=\"language-Python\">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])\n</code></pre><p>Il risultato √® una lista di stringhe:</p><pre><code class=\"language-Python\">['[CLS]', 'what', 'is', 'today', \"'\", 's', 'weather', 'in',\n 'berlin', '?', '[SEP]']\n</code></pre><p>Nota che il tokenizer del modello ha:</p><ol><li>Aggiunto <code>[CLS]</code> all'inizio e <code>[SEP]</code> alla fine. Questo √® necessario per ragioni tecniche e significa che <strong>ogni richiesta di embedding coster√† due token extra</strong>, oltre a qualsiasi numero di token richieda il testo.</li><li>Separato la punteggiatura dalle parole, trasformando \"<em>Berlin?</em>\" in: <code>berlin</code> e <code>?</code>, e \"<em>today's</em>\" in <code>today</code>, <code>'</code>, e <code>s</code>.</li><li>Messo tutto in minuscolo. Non tutti i modelli lo fanno, ma questo pu√≤ aiutare durante l'addestramento quando si usa l'inglese. Potrebbe essere meno utile in lingue dove la capitalizzazione ha un significato diverso.</li></ol><p>Diversi algoritmi di conteggio parole in programmi diversi potrebbero contare le parole in questa frase in modo differente. OpenOffice la conta come sei parole. L'algoritmo di segmentazione del testo Unicode (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) conta sette parole. Altri software potrebbero arrivare a numeri diversi, a seconda di come gestiscono la punteggiatura e i clitici come \"'s\".</p><p>Il tokenizer per questo modello produce nove token per quelle sei o sette parole, pi√π i due token extra necessari per ogni richiesta.</p><p>Ora, proviamo con un nome di luogo meno comune di Berlino:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"I live in Kinshasa.\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Il risultato:</p><pre><code class=\"language-Python\">['[CLS]', 'i', 'live', 'in', 'kin', '##sha', '##sa', '.', '[SEP]']\n</code></pre><p>Il nome \"Kinshasa\" √® diviso in tre token: <code>kin</code>, <code>##sha</code>, e <code>##sa</code>. Il <code>##</code> indica che questo token non √® l'inizio di una parola.</p><p>Se diamo al tokenizer qualcosa di completamente estraneo, il numero di token rispetto al numero di parole aumenta ancora di pi√π:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"Klaatu barada nikto\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n\n['[CLS]', 'k', '##la', '##at', '##u', 'bar', '##ada', 'nik', '##to', '[SEP]']\n</code></pre><p>Tre parole diventano otto token, pi√π i token <code>[CLS]</code> e <code>[SEP]</code>.</p><p>La tokenizzazione in tedesco √® simile. Con il modello <a href=\"https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina Embeddings v2 per il tedesco</a>, possiamo tokenizzare una traduzione di \"What is today's weather in Berlin?\" allo stesso modo del modello inglese.</p><pre><code class=\"language-Python\">german_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-de', trust_remote_code=True)\ntoken_ids = german_model.tokenizer.encode(\"Wie wird das Wetter heute in Berlin?\")\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Il risultato:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Wie', 'wird', 'das', 'Wetter', 'heute', 'in', 'Berlin', '?', '&lt;/s&gt;']\n</code></pre><p>Questo tokenizer √® leggermente diverso da quello inglese in quanto <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code> sostituiscono <code>[CLS]</code> e <code>[SEP]</code> ma svolgono la stessa funzione. Inoltre, il testo non viene normalizzato in maiuscole/minuscole - maiuscole e minuscole rimangono come scritte - perch√© in tedesco la capitalizzazione ha un significato diverso rispetto all'inglese.</p><p>(Per semplificare questa presentazione, ho rimosso un carattere speciale che indica l'inizio di una parola.)</p><p>Ora, proviamo con una frase pi√π complessa <a href=\"https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io\">da un testo giornalistico</a>:</p><blockquote>Ein Gro√üteil der milliardenschweren Bauern-Subventionen bleibt liegen ‚Äì zu genervt sind die Landwirte von b√ºrokratischen G√§ngelungen und Regelwahn.</blockquote><pre><code>sentence = \"\"\"\nEin Gro√üteil der milliardenschweren Bauern-Subventionen\nbleibt liegen ‚Äì zu genervt sind die Landwirte von \nb√ºrokratischen G√§ngelungen und Regelwahn.\n\"\"\"\ntoken_ids = german_model.tokenizer.encode(sentence)\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)</code></pre><p>Il risultato tokenizzato:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Ein', 'Gro√üteil', 'der', 'mill', 'iarden', 'schwer', \n 'en', 'Bauern', '-', 'Sub', 'ventionen', 'bleibt', 'liegen', \n '‚Äì', 'zu', 'gen', 'ervt', 'sind', 'die', 'Landwirte', 'von', \n 'b√ºro', 'krat', 'ischen', 'G√§n', 'gel', 'ungen', 'und', 'Regel', \n 'wahn', '.', '&lt;/s&gt;']\n</code></pre><p>Qui si vede che molte parole tedesche sono state suddivise in pezzi pi√π piccoli e non necessariamente seguendo le regole grammaticali tedesche. Il risultato √® che una parola tedesca lunga, che verrebbe contata come una sola parola da un contatore di parole, potrebbe corrispondere a un numero qualsiasi di token per il modello AI di Jina.</p><p>Facciamo lo stesso in cinese, traducendo \"What is today's weather in Berlin?\" come:</p><blockquote>ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü</blockquote><pre><code>chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)\ntoken_ids = chinese_model.tokenizer.encode(\"ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü\")\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>Il risultato tokenizzato:</p><pre><code class=\"language-Python\">['&lt;s&gt;', 'ÊüèÊûó', '‰ªäÂ§©ÁöÑ', 'Â§©Ê∞î', 'ÊÄé‰πàÊ†∑', 'Ôºü', '&lt;/s&gt;']\n</code></pre><p>In cinese, solitamente non ci sono spazi tra le parole nel testo scritto, ma il tokenizer di Jina Embeddings frequentemente unisce pi√π caratteri cinesi insieme:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Token string</th>\n<th>Pinyin</th>\n<th>Significato</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ÊüèÊûó</td>\n<td>B√≥l√≠n</td>\n<td>Berlino</td>\n</tr>\n<tr>\n<td>‰ªäÂ§©ÁöÑ</td>\n<td>jƒ´ntiƒÅn de</td>\n<td>di oggi</td>\n</tr>\n<tr>\n<td>Â§©Ê∞î</td>\n<td>tiƒÅnq√¨</td>\n<td>tempo</td>\n</tr>\n<tr>\n<td>ÊÄé‰πàÊ†∑</td>\n<td>zƒõnmey√†ng</td>\n<td>come</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Usiamo una frase pi√π complessa <a href=\"https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io\">da un giornale di Hong Kong</a>:</p><pre><code class=\"language-Python\">sentence = \"\"\"\nÊñ∞Ë¶èÂÆöÂü∑Ë°åÈ¶ñÊó•ÔºåË®òËÄÖÂú®‰∏ãÁè≠È´òÂ≥∞ÂâçÁöÑ‰∏ãÂçà5ÊôÇ‰æÜÂà∞Âª£Â∑ûÂú∞Èêµ3ËôüÁ∑öÔºå\nÂæûÁπÅÂøôÁöÑÁè†Ê±üÊñ∞ÂüéÁ´ôÂïüÁ®ãÔºåÂêëÊ©üÂ†¥ÂåóÊñπÂêëÂá∫Áôº„ÄÇ\n\"\"\"\ntoken_ids = chinese_model.tokenizer.encode(sentence)\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>(Traduzione: <em>\"Il primo giorno dell'entrata in vigore delle nuove norme, questo giornalista √® arrivato alla Linea 3 della Metropolitana di Guangzhou alle 17:00, durante l'ora di punta, partendo dalla Stazione Zhujiang New Town in direzione nord verso l'aeroporto.\"</em>)</p><p>Il risultato:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Êñ∞', 'Ë¶èÂÆö', 'Âü∑Ë°å', 'È¶ñ', 'Êó•', 'Ôºå', 'Ë®òËÄÖ', 'Âú®‰∏ã', 'Áè≠', \n 'È´òÂ≥∞', 'ÂâçÁöÑ', '‰∏ãÂçà', '5', 'ÊôÇ', '‰æÜÂà∞', 'Âª£Â∑û', 'Âú∞', 'Èêµ', '3', \n 'Ëôü', 'Á∑ö', 'Ôºå', 'Âæû', 'ÁπÅÂøô', 'ÁöÑ', 'Áè†Ê±ü', 'Êñ∞Âüé', 'Á´ô', 'Âïü', \n 'Á®ã', 'Ôºå', 'Âêë', 'Ê©üÂ†¥', 'Âåó', 'ÊñπÂêë', 'Âá∫Áôº', '„ÄÇ', '&lt;/s&gt;']\n</code></pre><p>Questi token non corrispondono a nessun dizionario specifico di parole cinesi (ËØçÂÖ∏). Per esempio, \"ÂïüÁ®ã\" - <em>q«êch√©ng</em> (partire, mettersi in viaggio) sarebbe tipicamente categorizzato come una singola parola ma qui √® diviso nei suoi due caratteri costituenti. Similmente, \"Âú®‰∏ãÁè≠\" sarebbe normalmente riconosciuto come due parole, con la divisione tra \"Âú®\" - <em>z√†i</em> (in, durante) e \"‰∏ãÁè≠\" - <em>xi√†bƒÅn</em> (fine della giornata lavorativa, ora di punta), non tra \"Âú®‰∏ã\" e \"Áè≠\" come ha fatto il tokenizer in questo caso.</p><p>In tutte e tre le lingue, i punti in cui il tokenizer divide il testo non sono direttamente correlati ai punti logici in cui un lettore umano li dividerebbe.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">L'algoritmo di tokenizzazione non utilizza un dizionario convenzionale consapevole della lingua, quindi il suo comportamento non corrisponde a come gli umani contano le parole.</div></div><p>Questa non √® una caratteristica specifica dei modelli Jina Embeddings. Questo approccio alla tokenizzazione √® quasi universale nello sviluppo di modelli AI. Sebbene due diversi modelli AI possano non avere tokenizer identici, nello stato attuale dello sviluppo, praticamente tutti useranno tokenizer con questo tipo di comportamento.</p><p>La prossima sezione discuter√† l'algoritmo specifico utilizzato nella tokenizzazione e la logica alla sua base.</p><h2 id=\"why-do-we-tokenize-and-why-this-way\">Perch√© Tokenizziamo? E Perch√© in Questo Modo?</h2><p>I modelli linguistici AI prendono in input sequenze di numeri che rappresentano sequenze di testo, ma succede qualcosa in pi√π prima di eseguire la rete neurale sottostante e creare un embedding. Quando viene presentata una lista di numeri che rappresentano piccole sequenze di testo, il modello cerca ogni numero in un dizionario interno che memorizza un vettore unico per ogni numero. Poi li combina, e questo diventa l'input per la rete neurale.</p><p>Questo significa che il tokenizer <strong>deve</strong> essere in grado di convertire <strong><em>qualsiasi</em></strong> testo di input che gli diamo in token che appaiono nel dizionario dei vettori di token del modello. Se prendessimo i nostri token da un dizionario convenzionale, la prima volta che incontrassimo un errore di ortografia o un nome proprio raro o una parola straniera, l'intero modello si fermerebbe. Non potrebbe processare quell'input.</p><p>Nell'elaborazione del linguaggio naturale, questo √® chiamato il problema del fuori-vocabolario (OOV), ed √® pervasivo in tutti i tipi di testo e in tutte le lingue. Ci sono alcune strategie per affrontare il problema OOV:</p><ol><li>Ignorarlo. Sostituire tutto ci√≤ che non √® nel dizionario con un token \"sconosciuto\".</li><li>Aggirarlo. Invece di usare un dizionario che mappa sequenze di testo a vettori, usarne uno che mappa <em>singoli caratteri</em> a vettori. L'inglese usa solo 26 lettere la maggior parte delle volte, quindi questo deve essere pi√π piccolo e pi√π robusto contro i problemi OOV rispetto a qualsiasi dizionario.</li><li>Trovare sottosequenze frequenti nel testo, metterle nel dizionario e usare caratteri (token a singola lettera) per tutto il resto.</li></ol><p>La prima strategia significa che molte informazioni importanti vengono perse. Il modello non pu√≤ nemmeno imparare dai dati che ha visto se prendono la forma di qualcosa che non √® nel dizionario. Molte cose nel testo ordinario non sono presenti nemmeno nei dizionari pi√π grandi.</p><p>La seconda strategia √® possibile, e i ricercatori l'hanno investigata. Tuttavia, significa che il modello deve accettare molti pi√π input e deve imparare molto di pi√π. Questo significa un modello molto pi√π grande e molti pi√π dati di addestramento per un risultato che non si √® mai dimostrato migliore della terza strategia.</p><p>I modelli linguistici AI implementano praticamente tutti la terza strategia in qualche forma. La maggior parte usa qualche variante dell'<a href=\"https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io\">algoritmo Wordpiece</a> <a href=\"https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io\">[Schuster e Nakajima 2012]</a> o una tecnica simile chiamata <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io\">Byte-Pair Encoding</a> (BPE). [<a href=\"https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io\">Gage 1994</a>, <a href=\"https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io\">Senrich et al. 2016</a>] Questi algoritmi sono <em>language-agnostic</em>. Ci√≤ significa che funzionano allo stesso modo per tutte le lingue scritte senza alcuna conoscenza oltre a un elenco completo dei possibili caratteri. Sono stati progettati per modelli multilingue come BERT di Google che prendono qualsiasi input dal web scraping - centinaia di lingue e testi diversi dal linguaggio umano come programmi per computer - in modo da poter essere addestrati senza fare linguistica complicata.</p><p>Alcune ricerche mostrano miglioramenti significativi utilizzando tokenizer pi√π specifici e consapevoli della lingua. [<a href=\"https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io\">Rust et al. 2021</a>] Ma costruire tokenizer in questo modo richiede tempo, denaro e competenza. Implementare una strategia universale come BPE o Wordpiece √® molto pi√π economico e facile.</p><p>Tuttavia, di conseguenza, non c'√® modo di sapere quanti token rappresenta un testo specifico se non facendolo passare attraverso un tokenizer e poi contando il numero di token che ne escono. Poich√© la sottoseqeunza pi√π piccola possibile di un testo √® una lettera, si pu√≤ essere sicuri che il numero di token non sar√† maggiore del numero di caratteri (meno gli spazi) pi√π due.</p><p>Per ottenere una buona stima, dobbiamo sottoporre molto testo al nostro tokenizer e calcolare empiricamente quanti token otteniamo in media, rispetto a quante parole o caratteri inseriamo. Nella prossima sezione, faremo alcune misurazioni empiriche non molto sistematiche per tutti i modelli Jina Embeddings v2 attualmente disponibili.</p><h2 id=\"empirical-estimates-of-token-output-sizes\">Stime Empiriche delle Dimensioni di Output dei Token</h2><p>Per l'inglese e il tedesco, ho utilizzato l'algoritmo di segmentazione del testo Unicode (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) per ottenere il conteggio delle parole per i testi. Questo algoritmo √® ampiamente utilizzato per selezionare frammenti di testo quando si fa doppio clic su qualcosa. √à la cosa pi√π vicina disponibile a un contatore di parole universale oggettivo.</p><p>Ho installato la <a href=\"https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io\">libreria polyglot</a> in Python, che implementa questo segmentatore di testo:</p><pre><code class=\"language-bash\">pip install -U polyglot\n</code></pre><p>Per ottenere il conteggio delle parole di un testo, puoi utilizzare un codice come questo snippet:</p><pre><code class=\"language-python\">from polyglot.text import Text\n\ntxt = \"What is today's weather in Berlin?\"\nprint(len(Text(txt).words))\n</code></pre><p>Il risultato dovrebbe essere <code>7</code>.</p><p>Per ottenere un conteggio dei token, segmenti del testo sono stati passati ai tokenizer di vari modelli Jina Embeddings, come descritto di seguito, e ogni volta, ho sottratto due dal numero di token restituiti.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\">I conteggi dei token elencati qui <b><strong style=\"white-space: pre-wrap;\">non includono</strong></b> i due token extra all'inizio e alla fine di ogni testo tokenizzato.</div></div><h3 id=\"english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en\">Inglese<br>(<code>jina-embeddings-v2-small-en</code> e <code>jina-embeddings-v2-base-en</code>)</h3><p>Per calcolare le medie, ho scaricato due corpora di testo inglese da <a href=\"https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Wortschatz Leipzig</a>, una collezione di corpora liberamente scaricabili in diverse lingue e configurazioni ospitata dall'Universit√† di Leipzig:</p><ul><li>Un corpus di un milione di frasi di notizie in inglese del 2020 (<code>eng_news_2020_1M</code>)</li><li>Un corpus di un milione di frasi da <a href=\"https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io\">Wikipedia inglese</a> del 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Entrambi si possono trovare sulla loro <a href=\"https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io\">pagina di download inglese</a>.</p><p>Per diversit√†, ho anche scaricato la <a href=\"https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io\">traduzione di Hapgood de <em>I Miserabili</em> di Victor Hugo</a> da Project Gutenberg, e una copia della Bibbia di Re Giacomo, tradotta in inglese nel 1611.</p><p>Per tutti e quattro i testi, ho contato le parole usando il segmentatore Unicode implementato in <code>polyglot</code>, poi ho contato i token creati da <code>jina-embeddings-v2-small-en</code>, sottraendo due token per ogni richiesta di tokenizzazione. I risultati sono i seguenti:</p>\n<!--kg-card-begin: html-->\n<table id=\"6f07d5d4-ca08-466e-92fc-e784a932e4d0\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b\"><th id=\"wvl`\" class=\"simple-table-header-color simple-table-header\">Testo</th><th id=\"|<X;\" class=\"simple-table-header-color simple-table-header\">Conteggio parole<br>(Segmentatore Unicode)<br></th><th id=\"GHal\" class=\"simple-table-header-color simple-table-header\">Conteggio token<br>(Jina Embeddings v2 <br>per inglese)<br></th><th id=\"h]mu\" class=\"simple-table-header-color simple-table-header\">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id=\"7e9eda1b-54b6-40f3-be6f-b233f161e2b5\"><td id=\"wvl`\" class=\"\"><code>eng_news_2020_1M</code></td><td id=\"|<X;\" class=\"\">22.825.712</td><td id=\"GHal\" class=\"\">25.270.581</td><td id=\"h]mu\" class=\"\">1,107</td></tr><tr id=\"a81dfe1d-9143-4306-9bf3-4891ca8fb019\"><td id=\"wvl`\" class=\"\"><code>eng_wikipedia_2016_1M</code></td><td id=\"|<X;\" class=\"\">24.243.607</td><td id=\"GHal\" class=\"\">26.813.877</td><td id=\"h]mu\" class=\"\">1,106</td></tr><tr id=\"d2fff413-6e0d-4ab2-9626-4d618d99af91\"><td id=\"wvl`\" class=\"\"><code>les_miserables_en</code></td><td id=\"|<X;\" class=\"\">688.911</td><td id=\"GHal\" class=\"\">764.121</td><td id=\"h]mu\" class=\"\">1,109</td></tr><tr id=\"eb304e43-4fd3-4e02-9993-13fb0307f544\"><td id=\"wvl`\" class=\"\"><code>kjv_bible</code></td><td id=\"|<X;\" class=\"\">1.007.651</td><td id=\"GHal\" class=\"\">1.099.335</td><td id=\"h]mu\" class=\"\">1,091</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>L'uso di numeri precisi non significa che questo sia un risultato preciso. Il fatto che documenti di generi cos√¨ diversi abbiano tutti tra il 9% e l'11% di token in pi√π rispetto alle parole indica che probabilmente ci si pu√≤ aspettare circa il 10% di token in pi√π rispetto alle parole, come misurato dal segmentatore Unicode. I word processor spesso non contano la punteggiatura, mentre il segmentatore Unicode lo fa, quindi non ci si pu√≤ aspettare che il conteggio delle parole del software da ufficio corrisponda necessariamente a questo.</p><h3 id=\"german-jina-embeddings-v2-base-de\">Tedesco<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>Per il tedesco, ho scaricato tre corpora dalla <a href=\"https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io\">pagina tedesca di Wortschatz Leipzig</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> ‚Äî Un milione di frasi da una miscela bilanciata di testi di diversi generi, risalenti al 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> ‚Äî Un milione di frasi di testo giornalistico del 2019.</li><li><code>deu_wikipedia_2021_1M</code> ‚Äî Un milione di frasi estratte dalla Wikipedia tedesca nel 2021.</li></ul><p>E per diversit√†, ho anche scaricato tutti e <a href=\"https://deutschestextarchiv.de/search?q=Kapital&in=metadata&ref=jina-ai-gmbh.ghost.io\">tre i volumi del <em>Kapital</em> di Karl Marx</a> dal <a href=\"https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Deutsches Textarchiv</a>.</p><p>Ho quindi seguito la stessa procedura utilizzata per l'inglese:</p>\n<!--kg-card-begin: html-->\n<table id=\"ad695a91-f35b-4215-bd4d-5d1415bb9812\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"7786decb-f68d-433d-8f58-3861d0350027\"><th id=\"UGp`\" class=\"simple-table-header-color simple-table-header\" style=\"width:234.2265625px\">Testo</th><th id=\"|qln\" class=\"simple-table-header-color simple-table-header\">Conteggio parole<br>(Unicode Segmenter)<br></th><th id=\"YXZX\" class=\"simple-table-header-color simple-table-header\">Conteggio token<br>(Jina Embeddings v2 <br>per tedesco e inglese)<br></th><th id=\"oEoQ\" class=\"simple-table-header-color simple-table-header\">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id=\"9cb48640-64db-4783-8bfe-c78412022a21\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_mixed-typical_2011_1M</code></td><td id=\"|qln\" class=\"\">7.924.024</td><td id=\"YXZX\" class=\"\">9.772.652</td><td id=\"oEoQ\" class=\"\">1,234</td></tr><tr id=\"32fee905-17dc-4c2c-a32d-5e6508b033bc\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_newscrawl-public_2019_1M</code></td><td id=\"|qln\" class=\"\">17.949.120</td><td id=\"YXZX\" class=\"\">21.711.555</td><td id=\"oEoQ\" class=\"\">1,210</td></tr><tr id=\"35d0c8c4-7912-4d61-829a-bb39b643aa1c\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_wikipedia_2021_1M</code></td><td id=\"|qln\" class=\"\">17.999.482</td><td id=\"YXZX\" class=\"\">22.654.901</td><td id=\"oEoQ\" class=\"\">1,259</td></tr><tr id=\"19e10367-e070-4dcc-8cbe-cfc75c43e0f9\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>marx_kapital</code></td><td id=\"|qln\" class=\"\">784.336</td><td id=\"YXZX\" class=\"\">1.011.377</td><td id=\"oEoQ\" class=\"\">1,289</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Questi risultati hanno una dispersione maggiore rispetto al modello solo inglese ma suggeriscono comunque che il testo tedesco produrr√†, in media, dal 20% al 30% di token in pi√π rispetto alle parole.</p><p>I testi inglesi producono pi√π token con il tokenizzatore tedesco-inglese rispetto a quello solo inglese:</p>\n<!--kg-card-begin: html-->\n<table id=\"c31b2079-e921-4e06-a24b-8ed60ae63d8d\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5\"><th id=\"Nc<l\" class=\"simple-table-header-color simple-table-header\" style=\"width:187.78125px\">Testo</th><th id=\"R@A^\" class=\"simple-table-header-color simple-table-header\">Conteggio parole<br>(Unicode Segmenter)<br></th><th id=\"UUfl\" class=\"simple-table-header-color simple-table-header\">Conteggio token<br>(Jina Embeddings v2 <br>per tedesco e inglese)<br></th><th id=\"iTZS\" class=\"simple-table-header-color simple-table-header\">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id=\"3461fd8c-ca39-4670-8f0e-e38a4958464a\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_news_2020_1M</code></td><td id=\"R@A^\" class=\"\">24.243.607</td><td id=\"UUfl\" class=\"\">27.758.535</td><td id=\"iTZS\" class=\"\">1,145</td></tr><tr id=\"48770d4d-5855-4f5f-934f-5b2900aa56c3\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"R@A^\" class=\"\">22.825.712</td><td id=\"UUfl\" class=\"\">25.566.921</td><td id=\"iTZS\" class=\"\">1,120</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Ci si dovrebbe aspettare di aver bisogno del 12-15% di token in pi√π rispetto alle parole per incorporare testi inglesi con il modello bilingue tedesco/inglese rispetto a quello solo inglese.</p><h3 id=\"chinese-jina-embeddings-v2-base-zh\">Cinese<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Il cinese √® tipicamente scritto senza spazi e non aveva una nozione tradizionale di \"parole\" prima del XX secolo. Di conseguenza, la dimensione di un testo cinese viene tipicamente misurata in caratteri (<strong>Â≠óÊï∞</strong>). Quindi, invece di utilizzare il segmentatore Unicode, ho misurato la lunghezza dei testi cinesi rimuovendo tutti gli spazi e ottenendo semplicemente la lunghezza dei caratteri.</p><p>Ho scaricato tre corpora dalla <a href=\"https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io\">pagina del corpus cinese di Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> ‚Äî Un milione di frasi dalla Wikipedia in lingua cinese, estratte nel 2018.</li><li><code>zho_news_2007-2009_1M</code> ‚Äî Un milione di frasi da fonti di notizie cinesi, raccolte dal 2007 al 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> ‚Äî Un milione di frasi da fonti di notizie che utilizzano esclusivamente caratteri cinesi tradizionali (ÁπÅÈ´îÂ≠ó).</li></ul><p>Inoltre, per avere una maggiore diversit√†, ho utilizzato anche <em>La vera storia di Ah Q</em> (ÈòøQÊ≠£ÂÇ≥), una novella di Lu Xun (È≠ØËøÖ) scritta all'inizio degli anni '20. Ho scaricato la <a href=\"https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io\">versione in caratteri tradizionali da Project Gutenberg</a>.</p>\n<!--kg-card-begin: html-->\n<table id=\"dace0ca3-97c0-481e-98e2-d2724b7bbe66\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"adc6e6ff-8afd-4915-8884-0894546a13dc\"><th id=\"bCvb\" class=\"simple-table-header-color simple-table-header\" style=\"width:223.6953125px\">Testo</th><th id=\"CaUc\" class=\"simple-table-header-color simple-table-header\">Conteggio caratteri<br>(Â≠óÊï∞)<br></th><th id=\"CQ{d\" class=\"simple-table-header-color simple-table-header\">Conteggio token<br>(Jina Embeddings v2 <br>per cinese e inglese)<br></th><th id=\"_};C\" class=\"simple-table-header-color simple-table-header\">Rapporto token/caratteri<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id=\"e75154ce-a33e-4af1-a983-4c4213f93c0e\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_wikipedia_2018_1M</code></td><td id=\"CaUc\" class=\"\">45.116.182</td><td id=\"CQ{d\" class=\"\">29.193.028</td><td id=\"_};C\" class=\"\">0,647</td></tr><tr id=\"605560a8-5c77-4add-a3e4-4615779b571a\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_news_2007-2009_1M</code></td><td id=\"CaUc\" class=\"\">44.295.314</td><td id=\"CQ{d\" class=\"\">28.108.090</td><td id=\"_};C\" class=\"\">0,635</td></tr><tr id=\"6e23944e-a480-4978-8550-a83404b218c4\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho-trad_newscrawl_2011_1M</code></td><td id=\"CaUc\" class=\"\">54.585.819</td><td id=\"CQ{d\" class=\"\">40.290.982</td><td id=\"_};C\" class=\"\">0,738</td></tr><tr id=\"50abbb96-06f7-4308-9c66-7c18f2a67721\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>Ah_Q</code></td><td id=\"CaUc\" class=\"\">41.268</td><td id=\"CQ{d\" class=\"\">25.346</td><td id=\"_};C\" class=\"\">0,614</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Questa variazione nei rapporti token-caratteri √® inaspettata, e in particolare l'anomalia nel corpus di caratteri tradizionali merita ulteriori indagini. Tuttavia, possiamo concludere che per il cinese, ci si deve aspettare di aver bisogno di <em>meno</em> token rispetto al numero di caratteri nel testo. A seconda del contenuto, ci si pu√≤ aspettare di aver bisogno del 25-40% in meno.</p><p>I testi in inglese in Jina Embeddings v2 per cinese e inglese hanno prodotto approssimativamente lo stesso numero di token come nel modello solo inglese:</p>\n<!--kg-card-begin: html-->\n<table id=\"061e7c3f-d109-476d-85fb-db3b369e4f35\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1200d074-3353-4815-ab66-a90e93ec349d\"><th id=\"v\\xv\" class=\"simple-table-header-color simple-table-header\" style=\"width:184.53125px\">Text</th><th id=\"qlUV\" class=\"simple-table-header-color simple-table-header\" style=\"width:165.3125px\">Word count<br>(Unicode Segmenter)<br></th><th id=\"=]?F\" class=\"simple-table-header-color simple-table-header\">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id=\"<rlw\" class=\"simple-table-header-color simple-table-header\">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id=\"2fe4e02d-94fd-4513-bfcb-7f85d66b6883\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_news_2020_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">24.243.607</td><td id=\"=]?F\" class=\"\">26.890.176</td><td id=\"<rlw\" class=\"\">1,109</td></tr><tr id=\"e7f937f4-b156-4f5d-9e0b-3041d07b1b20\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">22.825.712</td><td id=\"=]?F\" class=\"\">25.060.352</td><td id=\"<rlw\" class=\"\">1,097</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h2 id=\"taking-tokens-seriously\">Prendere sul serio i Token</h2><p>I token sono un'importante impalcatura per i modelli di linguaggio AI, e la ricerca in questo settore √® in corso.</p><p>Uno dei campi in cui i modelli AI si sono dimostrati rivoluzionari √® la scoperta che sono molto robusti contro i dati rumorosi. Anche se un particolare modello non utilizza la strategia di tokenizzazione ottimale, se la rete √® abbastanza grande, con dati sufficienti e adeguatamente addestrata, pu√≤ imparare a fare la cosa giusta anche da input imperfetti.</p><p>Di conseguenza, viene speso molto meno sforzo nel migliorare la tokenizzazione rispetto ad altre aree, ma questo potrebbe cambiare.</p><p>Come utente di embeddings, che li acquista tramite una <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">API come Jina Embeddings</a>, non puoi sapere esattamente quanti token ti serviranno per un compito specifico e potresti dover fare alcuni test per ottenere numeri precisi. Ma le stime fornite qui ‚Äî circa il 110% del conteggio delle parole per l'inglese, circa il 125% del conteggio delle parole per il tedesco e circa il 70% del conteggio dei caratteri per il cinese ‚Äî dovrebbero essere sufficienti per un budget di base.</p>",
  "comment_id": "65afb3ee8da8040001e17061",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-01-23T13:41:18.000+01:00",
  "updated_at": "2024-08-14T11:38:01.000+02:00",
  "published_at": "2024-01-31T16:10:14.000+01:00",
  "custom_excerpt": "Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-deep-dive-into-tokenization/",
  "excerpt": "La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti pi√π piccole per l'elaborazione. Quindi perch√© gli embedding vengono fatturati per token?",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect.",
  "feature_image_caption": null
}
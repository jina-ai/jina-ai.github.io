{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "C'è Ancora Bisogno del Chunking Quando i Modelli Long-Context Possono Fare Tutto?",
  "html": "<p>Nell'ottobre 2023, abbiamo introdotto <code>jina-embeddings-v2</code>, la prima famiglia di modelli di embedding open-source in grado di gestire input fino a 8.192 token. Basandoci su questo, quest'anno abbiamo lanciato <code>jina-embeddings-v3</code>, offrendo lo stesso ampio supporto per gli input con ulteriori miglioramenti.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>In questo articolo approfondiremo gli embedding con contesto lungo e risponderemo ad alcune domande: Quando è pratico consolidare un volume così grande di testo in un singolo vettore? La segmentazione migliora il recupero e, in caso affermativo, come? Come possiamo preservare il contesto da diverse parti di un documento mentre segmentiamo il testo?</p><p>Per rispondere a queste domande, confronteremo diversi metodi per generare gli embedding:</p><ul><li>Embedding con contesto lungo (codifica fino a 8.192 token in un documento) vs contesto breve (cioè troncamento a 192 token).</li><li>Nessun chunking vs. chunking naive vs. <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">late chunking</a>.</li><li>Diverse dimensioni dei chunk sia con chunking naive che con late chunking.</li></ul><h2 id=\"is-long-context-even-useful\">Il Contesto Lungo è Davvero Utile?</h2><p>Con la capacità di codificare fino a dieci pagine di testo in un singolo embedding, i modelli di embedding con contesto lungo aprono possibilità per la rappresentazione di testi su larga scala. Ma è davvero utile? Secondo molte persone... no.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Fonti: </span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Citazione di Nils Reimer nel podcast How AI Is Built</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">tweet di brainlag</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">commento di egorfine su Hacker News</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">commento di andy99 su Hacker News</span></a></p></figcaption></figure><p>Affronteremo tutte queste preoccupazioni con un'indagine dettagliata sulle capacità del contesto lungo, quando il contesto lungo è utile e quando dovresti (e non dovresti) usarlo. Ma prima, ascoltiamo questi scettici e guardiamo alcuni dei problemi che i modelli di embedding con contesto lungo devono affrontare.</p><h2 id=\"problems-with-long-context-embeddings\">Problemi con gli Embedding a Contesto Lungo</h2><p>Immaginiamo di costruire un sistema di ricerca documenti per articoli, come quelli del nostro <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">blog Jina AI</a>. A volte un singolo articolo può coprire più argomenti, come il <a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">report sulla nostra visita alla conferenza ICML 2024</a>, che contiene:</p><ul><li>Un'introduzione, che cattura informazioni generali su ICML (numero di partecipanti, luogo, scopo, ecc).</li><li>La presentazione del nostro lavoro (<code>jina-clip-v1</code>).</li><li>Riassunti di altri interessanti paper di ricerca presentati all'ICML.</li></ul><p>Se creiamo un solo embedding per questo articolo, quell'embedding rappresenta una miscela di tre argomenti disparati:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Quando si fa l'embedding di un documento che copre più argomenti, il vettore risultante rappresenta una miscela di tutti i paragrafi, potenzialmente perdendo le informazioni distinte e specifiche contenute in ogni singolo paragrafo.</span></figcaption></figure><p>Questo porta a diversi problemi:</p><ul><li><strong>Diluizione della Rappresentazione:</strong> Mentre tutti gli argomenti in un dato testo <em>potrebbero</em> essere correlati, solo uno potrebbe essere rilevante per la query di ricerca dell'utente. Tuttavia, un singolo embedding (in questo caso, quello dell'intero post del blog) è solo un punto nello spazio vettoriale. Man mano che più testo viene aggiunto all'input del modello, l'embedding si sposta per catturare l'argomento generale dell'articolo, rendendolo meno efficace nel rappresentare il contenuto trattato in specifici paragrafi.</li><li><strong>Capacità Limitata:</strong> I modelli di embedding producono vettori di dimensione fissa, indipendentemente dalla lunghezza dell'input. Man mano che viene aggiunto più contenuto all'input, diventa più difficile per il modello rappresentare tutte queste informazioni nel vettore. Pensalo come scalare un'immagine a 16×16 pixel — Se ridimensioni l'immagine di qualcosa di semplice, come una mela, puoi ancora ricavare significato dall'immagine scalata. Scalare una mappa stradale di Berlino? Non proprio.</li><li><strong>Perdita di Informazioni:</strong> In alcuni casi, anche i modelli di embedding con contesto lungo raggiungono i loro limiti; Molti modelli supportano la codifica del testo fino a 8.192 token. I documenti più lunghi devono essere troncati prima dell'embedding, portando a una perdita di informazioni. Se l'informazione rilevante per l'utente si trova alla fine del documento, non verrà catturata affatto dall'embedding.</li><li><strong>Potresti <em>Aver Bisogno</em> della Segmentazione del Testo:</strong> Alcune applicazioni richiedono embedding per segmenti specifici del testo ma non per l'intero documento, come identificare il passaggio rilevante in un testo.</li></ul><h2 id=\"long-context-vs-truncation\">Contesto Lungo vs. Troncamento</h2><p>Per vedere se il contesto lungo è effettivamente utile, diamo un'occhiata alle prestazioni di due scenari di recupero:</p><ul><li>Codifica di documenti fino a 8.192 token (circa 10 pagine di testo).</li><li>Troncamento dei documenti a 192 token e codifica fino a quel punto.</li></ul><p>Confronteremo i risultati utilizzando<code>jina-embeddings-v3</code> con la metrica di recupero nDCG@10. Abbiamo testato i seguenti dataset:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Descrizione</th>\n<th>Esempio di Query</th>\n<th>Esempio di Documento</th>\n<th>Lunghezza Media Documento (caratteri)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>Un dataset di recupero di testi medici completi con 3.244 query e documenti principalmente da PubMed.</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326.753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>Un dataset di riepilogo di riunioni basato su query che richiede il riepilogo di segmenti rilevanti delle riunioni.</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37.445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>Dataset QA con lunghe storie e relative domande su contenuti specifici.</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53.336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>Un dataset QA multi-hop con fino a 5 passaggi di ragionamento, progettato con template per evitare scorciatoie.</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30.854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>Un dataset di riassunti di sceneggiature con trascrizioni di serie TV e riassunti che richiedono l'integrazione di trame disperse.</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1.613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Come possiamo vedere, codificare più di 192 token può portare a notevoli miglioramenti delle prestazioni:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Confronto tra le prestazioni dell'embedding con contesto lungo e dell'embedding di testo breve</span></figcaption></figure><p>Tuttavia, su alcuni dataset vediamo miglioramenti maggiori rispetto ad altri:</p><ul><li>Per <strong>NFCorpus</strong>, il troncamento fa a malapena differenza. Questo perché i titoli e gli abstract sono proprio all'inizio dei documenti, e questi sono altamente rilevanti per le tipiche ricerche degli utenti. Che sia troncato o meno, i dati più pertinenti rimangono entro il limite di token.</li><li><strong>QMSum</strong> e <strong>NarrativeQA</strong> sono considerati compiti di \"comprensione della lettura\", dove gli utenti tipicamente cercano fatti specifici all'interno di un testo. Questi fatti sono spesso incorporati in dettagli sparsi in tutto il documento e possono cadere al di fuori del limite troncato di 192 token. Ad esempio, nel documento NarrativeQA <em>Percival Keene</em>, la risposta alla domanda \"Chi è il bullo che ruba il pranzo di Percival?\" si trova ben oltre questo limite. Similmente, in <strong>2WikiMultiHopQA</strong>, le informazioni rilevanti sono disperse in tutto il documento, richiedendo ai modelli di navigare e sintetizzare conoscenze da più sezioni per rispondere efficacemente alle query.</li><li><strong>SummScreenFD</strong> è un compito mirato a identificare la sceneggiatura corrispondente a un dato riassunto. Poiché il riassunto comprende informazioni distribuite in tutta la sceneggiatura, codificare più testo migliora l'accuratezza nel far corrispondere il riassunto alla sceneggiatura corretta.</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">Segmentazione del testo per migliori prestazioni di recupero</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Andando avanti, discutiamo tre concetti simili. Per evitare confusione, li riferiamo come segue:<br>• <b><strong style=\"white-space: pre-wrap;\">Segmentazione</strong></b>: Rilevamento di segnali di confine in un testo di input, per esempio, frasi o un numero fisso di token.<br>• <b><strong style=\"white-space: pre-wrap;\">Chunking ingenuo</strong></b>: Suddivisione del testo in chunks basata sui segnali di segmentazione, prima di codificarlo.<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Late chunking</strong></b></a>: Codifica del documento prima e poi segmentazione (preservando il contesto tra i chunks).</div></div><p>Invece di incorporare un intero documento in un unico vettore, possiamo utilizzare vari metodi per prima segmentare il documento assegnando segnali di confine:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Applicazione dei metodi di chunking \"a dimensione fissa\", \"basato su frasi\" e \"semantico\" a un passaggio di testo</span></figcaption></figure><p>Alcuni metodi comuni includono:</p><ul><li><strong>Segmentazione per dimensione fissa:</strong> Il documento viene diviso in segmenti di un numero fisso di token, determinato dal tokenizer del modello di embedding. Questo assicura che la tokenizzazione dei segmenti corrisponda alla tokenizzazione dell'intero documento (segmentare per un numero specifico di caratteri potrebbe portare a una tokenizzazione diversa).</li><li><strong>Segmentazione per frase:</strong> Il documento viene segmentato in frasi, e ogni chunk consiste in <em>n</em> numero di frasi.</li><li><strong>Segmentazione per semantica:</strong> Ogni segmento corrisponde a più frasi e un modello di embedding determina la similarità delle frasi consecutive. Le frasi con alte similarità di embedding vengono assegnate allo stesso chunk.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Puoi facilmente eseguire la segmentazione con <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a>, la nostra API gratuita per segmentare testi lunghi in chunks e tokenizzazione basata sulla struttura del documento.</div></div><p>Per semplicità, in questo articolo utilizziamo la segmentazione a dimensione fissa.</p><h3 id=\"document-retrieval-using-naive-chunking\">Recupero di documenti utilizzando il chunking ingenuo</h3><p>Una volta eseguita la segmentazione a dimensione fissa, possiamo ingenuamente suddividere il documento secondo quei segmenti:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Chunking ingenuo basato sui segnali di confine rilevati durante la segmentazione.</span></figcaption></figure><p>Usando <code>jina-embeddings-v3</code>, codifichiamo ogni chunk in un embedding che cattura accuratamente la sua semantica, poi memorizziamo questi embedding in un database vettoriale.</p><p>Durante l'esecuzione, il modello codifica la query dell'utente in un vettore di query. Lo confrontiamo con il nostro database vettoriale di embedding dei chunk per trovare il chunk con la più alta similarità del coseno, e poi restituiamo il documento corrispondente all'utente:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Recupero dei documenti implementato con chunking ingenuo: (1) I documenti nella collezione vengono divisi in chunk basati su segnali di confine, (2) il modello di embedding codifica tutti i chunk e memorizza gli embedding risultanti in un database, (3) quando arriva una query, il modello di embedding la codifica e il database determina il chunk più simile. Alla fine identifichiamo il documento rilevante dall'ID del documento memorizzato per il chunk nel database e lo restituiamo all'utente.</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">Problemi con il Chunking Ingenuo</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Quando si suddivide un testo in frasi, i riferimenti a parti precedenti del testo non possono essere risolti.</span></figcaption></figure><p>Mentre il chunking ingenuo affronta alcune delle limitazioni dei modelli di embedding a lungo contesto, presenta anche degli svantaggi:</p><ul><li><strong>Perdita del Quadro Generale:</strong> Per quanto riguarda il recupero dei documenti, più embedding di chunk più piccoli potrebbero non riuscire a catturare l'argomento generale del documento. È come non riuscire a vedere la foresta a causa degli alberi.</li><li><strong>Problema del Contesto Mancante:</strong> I chunk non possono essere interpretati accuratamente poiché mancano le informazioni contestuali, come illustrato nella Figura 6.</li><li><strong>Efficienza:</strong> Più chunk richiedono più spazio di archiviazione e aumentano il tempo di recupero.</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">Il Late Chunking Risolve il Problema del Contesto</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Per risolvere il problema del contesto mancante, abbiamo introdotto un nuovo metodo chiamato \"late chunking\", descritto nei nostri precedenti post sul blog: <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a>, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a>, <a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte III</strong></b></a>, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper di ricerca</strong></b></a>.</div></div><p>Il late chunking funziona in due passaggi principali:</p><ol><li>Prima, utilizza le capacità di lungo contesto del modello per codificare l'intero documento in embedding di token. Questo preserva il contesto completo del documento.</li><li>Poi, crea gli embedding dei chunk applicando il mean pooling a specifiche sequenze di embedding di token, corrispondenti ai segnali di confine identificati durante la segmentazione.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Late chunking vs chunking ingenuo.</span></figcaption></figure><p>Il vantaggio principale di questo approccio è che gli embedding dei token sono contestualizzati - significa che catturano naturalmente riferimenti e relazioni con altre parti del documento. Poiché il processo di embedding avviene prima del chunking, ogni chunk mantiene la consapevolezza del contesto più ampio del documento, risolvendo il problema del contesto mancante che affligge gli approcci di chunking ingenuo.</p><p>Per documenti che superano la dimensione massima di input del modello, possiamo utilizzare il \"long late chunking\":</p><ol><li>Prima, dividiamo il documento in \"macro-chunk\" sovrapposti. Ogni macro-chunk è dimensionato per rientrare nella lunghezza massima del contesto del modello (per esempio, 8.192 token).</li><li>Il modello elabora questi macro-chunk per creare gli embedding dei token.</li><li>Una volta ottenuti gli embedding dei token, procediamo con il late chunking standard - applicando il mean pooling per creare gli embedding finali dei chunk.</li></ol><p>Questo approccio ci permette di gestire documenti di qualsiasi lunghezza mantenendo i benefici del late chunking. Pensalo come un processo in due fasi: prima rendere il documento digeribile per il modello, poi applicare la procedura normale di late chunking.</p><p>In breve:</p><ul><li><strong>Chunking ingenuo:</strong> Segmentare il documento in piccoli chunk, poi codificare ogni chunk separatamente.</li><li><strong>Late chunking:</strong> Codificare l'intero documento in una volta per creare gli embedding dei token, poi creare gli embedding dei chunk raggruppando gli embedding dei token basati sui confini dei segmenti.</li><li><strong>Long late chunking:</strong> Dividere documenti grandi in macro-chunk sovrapposti che si adattano alla finestra di contesto del modello, codificarli per ottenere gli embedding dei token, poi applicare il late chunking normalmente.</li></ul><p>Per una descrizione più estesa dell'idea, dai un'occhiata al nostro <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">paper</a> o ai post del blog menzionati sopra.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">Fare Chunking o No?</h2><p>Abbiamo già visto che l'embedding a lungo contesto generalmente supera gli embedding di testi più brevi, e abbiamo dato una panoramica delle strategie di chunking sia ingenuo che tardivo. La domanda ora è: Il chunking è migliore dell'embedding a lungo contesto?</p><p>Per condurre un confronto equo, tronchiamo i valori di testo alla lunghezza massima della sequenza del modello (8.192 token) prima di iniziare a segmentarli. Usiamo una segmentazione a dimensione fissa con 64 token per segmento (sia per la segmentazione ingenua che per il late chunking). Confrontiamo tre scenari:</p><ul><li><strong>Nessuna segmentazione:</strong> Codifichiamo ogni testo in un singolo embedding. Questo porta agli stessi punteggi dell'esperimento precedente (vedi Figura 2), ma li includiamo qui per un migliore confronto.</li><li><strong>Chunking ingenuo:</strong> Segmentiamo i testi, poi applichiamo il chunking ingenuo basato sui segnali di confine.</li><li><strong>Late chunking:</strong> Segmentiamo i testi, poi usiamo il late chunking per determinare gli embedding.</li></ul><p>Sia per il late chunking che per la segmentazione ingenua, utilizziamo il recupero dei chunk per determinare il documento rilevante (come mostrato nella Figura 5, precedentemente in questo post).</p><p>I risultati non mostrano un chiaro vincitore:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Nessun chunking vs chunking ingenuo vs late chunking</span></figcaption></figure><ul><li><strong>Per il recupero dei fatti, il chunking ingenuo funziona meglio:</strong> Per i dataset QMSum, NarrativeQA e 2WikiMultiHopQA, il modello deve identificare i passaggi rilevanti nel documento. Qui, il chunking ingenuo è chiaramente migliore rispetto alla codifica di tutto in un singolo embedding, poiché probabilmente solo pochi chunk includono informazioni rilevanti, e tali chunk le catturano molto meglio di un singolo embedding dell'intero documento.</li><li><strong>Il chunking tardivo funziona meglio con documenti coerenti e contesto rilevante:</strong> Per documenti che trattano un argomento coerente dove gli utenti cercano temi generali piuttosto che fatti specifici (come in NFCorpus), il chunking tardivo ha prestazioni leggermente migliori rispetto al non chunking, poiché bilancia il contesto dell'intero documento con i dettagli locali. Tuttavia, mentre il chunking tardivo generalmente si comporta meglio del chunking ingenuo preservando il contesto, questo vantaggio può diventare uno svantaggio quando si cercano fatti isolati all'interno di documenti contenenti per lo più informazioni irrilevanti - come si vede nelle regressioni delle prestazioni per NarrativeQA e 2WikiMultiHopQA, dove il contesto aggiunto diventa più distraente che utile.</li></ul><h3 id=\"does-chunk-size-make-a-difference\">La Dimensione dei Chunk Fa la Differenza?</h3><p>L'efficacia dei metodi di chunking dipende davvero dal dataset, evidenziando come la struttura del contenuto giochi un ruolo cruciale:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Confronto delle dimensioni dei chunk con chunking ingenuo e tardivo.</span></figcaption></figure><p>Come possiamo vedere, il chunking tardivo generalmente supera il chunking ingenuo con chunk di dimensioni minori, poiché i chunk ingenui più piccoli sono troppo ridotti per contenere molto contesto, mentre i chunk tardivi più piccoli mantengono il contesto dell'intero documento, rendendoli più significativi semanticamente. L'eccezione è il dataset NarrativeQA dove c'è semplicemente così tanto contesto irrilevante che il chunking tardivo perde terreno. Con chunk di dimensioni maggiori, il chunking ingenuo mostra un notevole miglioramento (occasionalmente superando il chunking tardivo) grazie all'aumento del contesto, mentre le prestazioni del chunking tardivo diminuiscono gradualmente.</p><h2 id=\"takeaways-when-to-use-what\">Conclusioni: Quando Usare Cosa?</h2><p>In questo post, abbiamo esaminato diversi tipi di task di recupero documenti per capire meglio quando utilizzare la segmentazione e quando il chunking tardivo è utile. Quindi, cosa abbiamo imparato?</p><h3 id=\"when-should-i-use-long-context-embedding\">Quando Dovrei Usare l'Embedding con Contesto Lungo?</h3><p>In generale, non danneggia l'accuratezza del recupero includere quanto più testo possibile dei tuoi documenti nell'input del tuo modello di embedding. Tuttavia, i modelli di embedding con contesto lungo spesso si concentrano sull'inizio dei documenti, poiché contengono contenuti come titoli e introduzione che sono più importanti per giudicare la rilevanza, ma i modelli potrebbero perdere contenuti nella parte centrale del documento.</p><h3 id=\"when-should-i-use-naive-chunking\">Quando Dovrei Usare il Chunking Ingenuo?</h3><p>Quando i documenti coprono molteplici aspetti, o le query degli utenti mirano a informazioni specifiche all'interno di un documento, il chunking generalmente migliora le prestazioni di recupero.</p><p>Alla fine, le decisioni sulla segmentazione dipendono da fattori come la necessità di mostrare testo parziale agli utenti (ad esempio come Google presenta i passaggi rilevanti nelle anteprime dei risultati di ricerca), che rende la segmentazione essenziale, o vincoli di calcolo e memoria, dove la segmentazione può essere meno favorevole a causa dell'aumento del sovraccarico di recupero e dell'utilizzo delle risorse.</p><h3 id=\"when-should-i-use-late-chunking\">Quando Dovrei Usare il Chunking Tardivo?</h3><p>Codificando l'intero documento prima di creare i chunk, il chunking tardivo risolve il problema dei segmenti di testo che perdono il loro significato a causa del contesto mancante. Questo funziona particolarmente bene con documenti coerenti, dove ogni parte è correlata al tutto. I nostri esperimenti mostrano che il chunking tardivo è particolarmente efficace quando si divide il testo in chunk più piccoli, come dimostrato nel nostro <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">paper</a>. Tuttavia, c'è un'avvertenza: se parti del documento non sono correlate tra loro, includere questo contesto più ampio può effettivamente peggiorare le prestazioni di recupero, poiché aggiunge rumore agli embedding.</p><h2 id=\"conclusion\">Conclusione</h2><p>La scelta tra embedding con contesto lungo, chunking ingenuo e chunking tardivo dipende dai requisiti specifici del tuo task di recupero. Gli embedding con contesto lungo sono preziosi per documenti coerenti con query generali, mentre il chunking eccelle nei casi in cui gli utenti cercano fatti o informazioni specifiche all'interno di un documento. Il chunking tardivo migliora ulteriormente il recupero mantenendo la coerenza contestuale all'interno di segmenti più piccoli. In definitiva, comprendere i tuoi dati e gli obiettivi di recupero guiderà l'approccio ottimale, bilanciando accuratezza, efficienza e rilevanza contestuale.</p><p>Se stai esplorando queste strategie, considera di provare <code>jina-embeddings-v3</code>—le sue avanzate capacità di contesto lungo, chunking tardivo e flessibilità lo rendono un'eccellente scelta per diversi scenari di recupero.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: Un Modello di Embedding Multilingue all'Avanguardia</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 è un modello di embedding testuale multilingue all'avanguardia con 570M parametri e lunghezza di token di 8192, che supera gli ultimi embedding proprietari di OpenAI e Cohere su MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "Confronto delle prestazioni dei modelli di embedding a contesto lungo con diverse strategie di suddivisione per trovare l'approccio ottimale per le tue esigenze.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
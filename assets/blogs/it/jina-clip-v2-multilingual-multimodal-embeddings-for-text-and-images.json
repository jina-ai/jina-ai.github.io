{
  "slug": "jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images",
  "id": "673cc4a7a7c46d00015cf1f5",
  "uuid": "6ca44950-b989-494a-b587-70847f24edd2",
  "title": "Jina CLIP v2: Embedding multilingue e multimodale per testo e immagini",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">Siamo in un viaggio per far progredire e democratizzare l'intelligenza artificiale attraverso l'open source e la scienza aperta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-11.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-clip-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?sui=&model=jina-clip-v2&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - La tua base di ricerca, potenziata.</div><div class=\"kg-bookmark-description\">I migliori embedding, rerankers, LLM-reader, web scraper, classificatori. La migliore AI di ricerca per dati multilingue e multimodali.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-11.png\" alt=\"\"><span class=\"kg-bookmark-author\">La tua base di ricerca, potenziata.</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> API è disponibile sotto la scheda \"Embeddings\".</span></p></figcaption></figure><p>Gli embedding multimodali permettono di cercare e comprendere dati attraverso diverse modalità mediante una rappresentazione coerente. Fungono da base per il recupero di informazioni neurali e le applicazioni GenAI multimodali. Oggi siamo entusiasti di rilasciare <code>jina-clip-v2</code>, un nuovo modello di embedding multimodale multilingue per uso generale costruito su <code>jina-clip-v1</code> e il nostro recentemente rilasciato <code>jina-embeddings-3</code>, con diversi miglioramenti chiave:</p><ul><li><strong>Prestazioni Migliorate</strong>: v2 mostra un miglioramento delle prestazioni del 3% rispetto a v1 sia nelle attività di recupero testo-immagine che testo-testo. Come v1, l'encoder testuale di v2 può fungere da efficace recuperatore denso multilingue per contesti lunghi. Le sue prestazioni sono alla pari con il nostro modello di frontiera <code>jina-embeddings-v3</code> (attualmente i migliori embedding multilingue sotto 1B parametri su MTEB).</li><li><strong>Supporto Multilingue</strong>: Alimentato da <code>jina-embeddings-v3</code> come torre testuale, <code>jina-clip-v2</code> supporta 89 lingue per il recupero multilingue-immagine, mostrando un miglioramento fino al 4% rispetto a <code>nllb-clip-large-siglip</code> nelle attività di recupero di immagini multilingue.</li><li><strong>Risoluzione Immagine Superiore</strong>: v2 ora supporta una risoluzione dell'immagine in input di 512x512, un aumento significativo rispetto ai 224x224 di v1. Questa risoluzione più alta permette una migliore elaborazione delle immagini dettagliate, una migliore estrazione delle caratteristiche e un riconoscimento più accurato degli elementi visivi dettagliati.</li><li><strong>Rappresentazioni Matrioska</strong>: v2 permette agli utenti di troncare le dimensioni di output degli embedding sia di testo che di immagini da 1024 fino a 64, riducendo l'overhead di memorizzazione ed elaborazione mantenendo prestazioni elevate.</li></ul><h2 id=\"model-architecture\">Architettura del Modello</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--35-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"></figure><p><code>jina-clip-v2</code> è un modello stile CLIP da 0.9B che combina due potenti encoder: l'encoder testuale <code>Jina XLM-RoBERTa</code> (la base di <code>jina-embeddings-v3</code>) e l'encoder visivo <code>EVA02-L14</code> (un efficiente Transformer visivo sviluppato da BAAI). Questi encoder sono addestrati congiuntamente per creare rappresentazioni allineate di immagini e testo.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Text Encoder</th>\n<th>Image Encoder</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Base Model</td>\n<td>Jina XLM-RoBERTa</td>\n<td>EVA02-L</td>\n</tr>\n<tr>\n<td>Parameters</td>\n<td>561M</td>\n<td>304M</td>\n</tr>\n<tr>\n<td>Input Specification</td>\n<td>8,192 tokens (max)</td>\n<td>512×512 pixels</td>\n</tr>\n<tr>\n<td>Min Output Dimensions</td>\n<td>64</td>\n<td>64</td>\n</tr>\n<tr>\n<td>Max Output Dimensions</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Attention Mechanism</td>\n<td>FlashAttention2</td>\n<td>xFormers</td>\n</tr>\n<tr>\n<td>Pooling Strategy</td>\n<td>Mean pooling</td>\n<td>CLS pooling</td>\n</tr>\n<tr>\n<td>Additional Features</td>\n<td>89 languages supported</td>\n<td>Patch size 14x14</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"cross-modal-retrieval-performance\">Prestazioni di Recupero Cross-Modale</h2><p>Jina CLIP v2 fornisce supporto multilingue per 89 lingue e offre prestazioni di alto livello nelle principali lingue tra cui arabo, cinese, inglese, francese, tedesco, giapponese, russo e spagnolo. Nei benchmark di recupero immagini multilingue, mostra prestazioni che eguagliano o superano <a href=\"https://huggingface.co/visheratin/nllb-clip-large-siglip?ref=jina-ai-gmbh.ghost.io\">NLLB-CLIP-SigLIP</a>, un modello stile CLIP leggermente più grande (1.3B, 44% più grande di <code>jina-clip-v2</code>) allo stato dell'arte che utilizza un encoder testuale pre-addestrato dai modelli NLLB.</p><h3 id=\"english-only-text-and-images\">Testo e Immagini Solo in Inglese</h3><p>Sui benchmark standard di recupero cross-modale (Flickr30k e COCO), <code>jina-clip-v2</code> dimostra notevoli miglioramenti su tutti i fronti. Raggiunge prestazioni allo stato dell'arte del 98.0% sul recupero immagine-testo di Flickr30k, superando sia il suo predecessore che NLLB-CLIP-SigLIP. Il modello mostra guadagni costanti in tutti gli scenari di recupero, con miglioramenti notevoli fino al 3.3% rispetto a v1 sul recupero immagine-testo di COCO, mantenendo prestazioni competitive con NLLB-CLIP-SigLIP su diversi benchmark e direzioni di modalità.</p><p><strong>Prestazioni Flickr30k Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>98.0</td>\n<td>+1.7%</td>\n<td>+0.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>96.4</td>\n<td>-</td>\n<td>-0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>97.1</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>89.8</td>\n<td>+0.9%</td>\n<td>-2.6%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>89.0</td>\n<td>-</td>\n<td>-3.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>92.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Prestazioni COCO Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>81.5</td>\n<td>+3.3%</td>\n<td>+2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>78.9</td>\n<td>-</td>\n<td>-0.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>79.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>68.4</td>\n<td>+2.9%</td>\n<td>-3.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>66.5</td>\n<td>-</td>\n<td>-6.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>70.8</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"multilingual-text-and-images\">Testo e Immagini Multilingue</h3><p>Sui benchmark cross-modali multilingue, <code>jina-clip-v2</code> dimostra prestazioni robuste, eccellendo particolarmente nel recupero immagine-testo dove supera NLLB-SigLIP su tutti i dataset, con un miglioramento fino al +3.8% su Crossmodal 3600. Mentre NLLB-SigLIP mostra capacità leggermente superiori nel recupero testo-immagine, il divario di prestazioni rimane piccolo, tipicamente entro il 3%.</p><p><strong>Prestazioni Image2Text Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>83.23</td>\n<td>+3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>80.16</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>86.03</td>\n<td>+0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.37</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.98</td>\n<td>+0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.41</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Prestazioni Text2Image Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>81.43</td>\n<td>-0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>82.07</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>84.87</td>\n<td>-3.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.60</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.03</td>\n<td>-3.0%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.63</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"text-only-dense-retriever-performance\">Prestazioni del Retriever Denso Solo Testo</h2><p>Come il suo predecessore, l'encoder testuale di <code>jina-clip-v2</code> può fungere da efficace retriever multilingue denso. Sui completi benchmark Multilingual MTEB, raggiunge ottime prestazioni, toccando il 69.86% nel recupero e il 67.77% nei compiti di similarità semantica. Questi risultati dimostrano la sua versatilità, competendo con il nostro modello specializzato di embedding testuale <code>jina-embeddings-v3</code>:</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>69.86</td>\n<td>-3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>72.59</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Semantic Similarity</td>\n<td>jina-clip-v2</td>\n<td>67.77</td>\n<td>-2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>69.81</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>Sui task in inglese, <code>jina-clip-v2</code> mostra costanti miglioramenti rispetto sia al predecessore che a NLLB-SigLIP, con vantaggi particolarmente evidenti nelle prestazioni di recupero (quasi il doppio del punteggio di NLLB-SigLIP).</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS</td>\n<td>jina-clip-v2</td>\n<td>81.29</td>\n<td>+0.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>80.92</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>74.65</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>49.33</td>\n<td>+2.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>48.33</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>24.92</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"matryoshka-representation-performance\">Prestazioni della Rappresentazione Matryoshka</h2><p>Sia gli encoder testuali che quelli di immagini supportano MRL, e le loro dimensioni di output possono essere troncate a 64 mantenendo prestazioni elevate. La nostra valutazione del troncamento degli embedding ha rivelato un notevole potenziale di compressione. Persino una riduzione dimensionale aggressiva del 75% ha mantenuto oltre il 99% delle prestazioni in tutti i task testuali, di immagine e cross-modali.</p><h3 id=\"image-classification\">Classificazione delle Immagini</h3><p>Su 37 diversi benchmark di classificazione delle immagini, l'encoder di immagini mostra una forte resilienza alle dimensioni troncate. La compressione da 1024 a 64 dimensioni (riduzione del 94%) comporta solo un calo dell'8% nell'accuratezza top-5 e del 12.5% nella top-1, evidenziando il suo potenziale per un'implementazione efficiente con perdite di prestazioni minime.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/accuracy_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Per la </span><b><strong style=\"white-space: pre-wrap;\">classificazione delle immagini</strong></b><span style=\"white-space: pre-wrap;\">, abbiamo utilizzato i 19 benchmark nel dataset </span><a href=\"https://github.com/google-research/task_adaptation?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VTAB</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VOC 2007</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/sun397?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">SUN397</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://cs.stanford.edu/~acoates/stl10/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">STL10</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Rendered SST2</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://objectnet.dev/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ObjectNet</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/cvdfoundation/mnist?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MNIST</span></a><span style=\"white-space: pre-wrap;\">, German Traffic Sign Recognition Benchmark (</span><a href=\"https://benchmark.ini.rub.de/gtsrb_dataset.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">GTSRB</span></a><span style=\"white-space: pre-wrap;\">), Fine-Grained Visual Classification of Aircraft (</span><a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FGVC-Aircraft</span></a><span style=\"white-space: pre-wrap;\">), </span><a href=\"https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FER 2013</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Country211</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/cars196?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Cars196</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/hendrycks/natural-adv-examples?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet-A, ImageNet-O,</span></a><a href=\"https://huggingface.co/datasets/ILSVRC/imagenet-1k?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">IxmageNet1k</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/HaohanWang/ImageNet-Sketch?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet Sketch</span></a><span style=\"white-space: pre-wrap;\"> e </span><a href=\"https://imagenetv2.org/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet v2</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h3 id=\"cross-modal-retrieval\">Recupero Cross-Modale</h3><p>Nonostante una drastica riduzione del 94% a soli 64 dimensioni, il recupero cross-modale utilizzando embedding di immagini e testo troncati è rimasto notevolmente robusto, preservando il 93% delle prestazioni immagine-testo e il 90% delle prestazioni testo-immagine.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/crossmodal_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Abbiamo utilizzato sei benchmark, tre dei quali sono multilingue: </span><a href=\"https://google.github.io/crossmodal-3600/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Crossmodal-3600</span></a><span style=\"white-space: pre-wrap;\"> (36 lingue), </span><a href=\"https://shannon.cs.illinois.edu/DenotationGraph/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr30k</span></a><span style=\"white-space: pre-wrap;\"> (solo inglese), </span><a href=\"https://hockenmaier.cs.illinois.edu/8k-pictures.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr8k</span></a><span style=\"white-space: pre-wrap;\"> (solo inglese), </span><a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (solo inglese), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Multilingual MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (10 lingue), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">XTD 200</span></a><span style=\"white-space: pre-wrap;\"> (27 lingue)</span></figcaption></figure><h3 id=\"text-only-retrieval\">Recupero Solo Testo</h3><p>Sui <strong>benchmark MTEB solo in inglese</strong>, gli embedding di testo a 64 dimensioni (compressi da 1024) hanno preservato notevolmente bene la similarità semantica, con una diminuzione di solo il 2,1%, mentre il recupero ha visto un modesto calo del 17,5%.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/mteb_performance.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"></figure><h2 id=\"getting-started\">Per Iniziare</h2><h3 id=\"via-api\">Tramite API</h3><p>Il codice dimostra come generare embedding usando <code>requests</code> di Python. Passa una stringa di testo con un'immagine in base64 o URL, più la dimensione desiderata (predefinita 1024, mostrata come 768 qui sotto).</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-Python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v2',\n  'encoding_type': 'float',\n  'dimensions': '768' \n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Ricorda di sostituire &lt;YOUR_JINA_AI_API_KEY&gt; con una chiave API Jina attivata. Puoi ottenere </span><a href=\"https://jina.ai/?sui=apikey&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><span style=\"white-space: pre-wrap;\">una chiave API gratuita con un milione di token gratuiti qui.</span></a></p></figcaption></figure><h3 id=\"image-tokens-pricing\">Prezzi dei Token Immagine</h3><p>La nostra API conta sia i token di testo che di immagine. Per le immagini, il consumo di token si basa sul numero di tessere da 512x512 pixel necessarie per coprire l'intera area dell'immagine. Ogni tessera costa 4.000 token da elaborare, incluse le tessere parzialmente riempite. <strong>Per un'ottimale efficienza dei costi, raccomandiamo agli utenti dell'API di ridimensionare le loro immagini a 512x512 prima di inviare le richieste.</strong></p><table>\n<thead>\n<tr>\n<th>Risoluzione Immagine</th>\n<th>Tessere Richieste</th>\n<th>Costo Token</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>512x512</td>\n<td>1</td>\n<td>4.000</td>\n</tr>\n<tr>\n<td>720x720</td>\n<td>4</td>\n<td>16.000</td>\n</tr>\n<tr>\n<td>1080x1080</td>\n<td>9</td>\n<td>36.000</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--37-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Per le immagini quadrate, ridimensiona a 512x512 per la migliore efficienza dei costi. Per le attività sensibili al rapporto d'aspetto, scala il lato più lungo a 512, centra l'immagine e riempi con il nero. Per scopi generali, il ridimensionamento diretto a 512x512 funziona bene.</span></figcaption></figure><h3 id=\"via-csp-marketplaces\">Tramite i Marketplace CSP</h3><p>Jina CLIP v2 è disponibile direttamente su AWS, Azure e GCP ai prezzi elencati.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-bfbctuqmky676?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina CLIP v2</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-10.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/socialPreview-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-9.ico\" alt=\"\"></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://console.cloud.google.com/marketplace/browse?q=jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Cloud console</div><div class=\"kg-bookmark-description\">Spendi in modo intelligente, acquista più velocemente e utilizza il credito impegnato su Google Cloud con Google Cloud Marketplace. Sfoglia il catalogo di oltre 2000 SaaS, VM, stack di sviluppo e app Kubernetes ottimizzate per l'esecuzione su Google Cloud.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/default.png\" alt=\"\"></div></div></a></figure><h3 id=\"via-vectordb\"><strong>Via VectorDB</strong></h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pinecone.io/models/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Il database vettoriale per costruire AI consapevole | Pinecone</div><div class=\"kg-bookmark-description\">Cerca tra miliardi di elementi le corrispondenze simili a qualsiasi oggetto, in millisecondi. È la prossima generazione di ricerca, a portata di chiamata API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-3.png\" alt=\"\"><span class=\"kg-bookmark-author\">Pinecone Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/docs_og_image.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://weaviate.io/developers/weaviate/model-providers/jinaai/embeddings-multimodal?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embeddings Multimodali | Weaviate</div><div class=\"kg-bookmark-description\">L'integrazione di Weaviate con le API di Jina AI ti permette di accedere direttamente alle capacità dei loro modelli da Weaviate.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-12.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Weaviate</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/provider_integrations_jinaai.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant è un Database Vettoriale e un Motore di Ricerca Vettoriale Open-Source scritto in Rust. Fornisce un servizio di ricerca per similarità vettoriale veloce e scalabile con API intuitive.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-4.png\" alt=\"\"><span class=\"kg-bookmark-author\">edit</span><span class=\"kg-bookmark-publisher\">Qdrant</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-social-preview-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Conclusione</h2><p>Sulla base del nostro rilascio di <code>jina-clip-v1</code> a giugno, che ha esteso il modello CLIP di OpenAI con input di testo fino a 8.192 token, e del pionieristico <code>jina-embeddings-v3</code> multilingue, <code>jina-clip-v2</code> introduce tre importanti avanzamenti: supporto multilingue per 89 lingue, maggiore risoluzione delle immagini a 512x512 e apprendimento della rappresentazione Matryoshka per embedding più troncati.</p><p>I modelli simili a CLIP si sono affermati come la spina dorsale per applicazioni multimodali general-purpose. Con <code>jina-clip-v2</code>, stiamo portando queste capacità al livello successivo, abbattendo le barriere linguistiche per offrire una comprensione e un recupero cross-modale più accurati. Crediamo che questo rilascio mantenga la promessa di rendere la ricerca e il recupero multimodale sia più potenti che più accessibili agli sviluppatori di tutto il mondo.</p>",
  "comment_id": "673cc4a7a7c46d00015cf1f5",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/11/clipv2.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-11-19T18:02:31.000+01:00",
  "updated_at": "2024-11-21T17:29:45.000+01:00",
  "published_at": "2024-11-21T17:29:45.000+01:00",
  "custom_excerpt": "Jina-CLIP v2, a 0.9B multimodal embedding model with multilingual support of 89 languages, high image resolution at 512x512, and Matryoshka representations.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/",
  "excerpt": "Jina-CLIP v2, un modello di embedding multimodale da 0,9B con supporto multilingue per 89 lingue, alta risoluzione delle immagini a 512x512 e rappresentazioni Matryoshka.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "what-we-learned-at-iclr2025",
  "id": "68336ebaa4451f0001adcbad",
  "uuid": "90b46828-a059-4879-82a8-19f4c4e4675b",
  "title": "Cosa abbiamo imparato all'ICLR2025",
  "html": "<p>ICLR 2025 è una delle conferenze di machine learning più grandi e influenti al mondo, al pari di NeurIPS e ICML come i tre principali punti di riferimento per la ricerca sull'IA ad alto impatto. Quest'anno ha segnato una pietra miliare storica, poiché ICLR si è tenuta in Asia per la prima volta, presso il Singapore EXPO dal 24 al 28 aprile. Il tempismo non avrebbe potuto essere più perfetto: solo pochi mesi dopo il \"momento DeepSeek\" alla fine di gennaio 2025, che ha scosso la Silicon Valley e ha dimostrato la rapida avanzata della ricerca sull'IA in Cina. In combinazione con il nuovo accordo Cina-Singapore di esenzione reciproca dal visto di 30 giorni entrato in vigore nel febbraio 2024, abbiamo assistito a un'impennata senza precedenti della partecipazione cinese alla conferenza.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1106\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png 2126w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Quest'anno, il nostro team era entusiasta di fare il viaggio a Singapore, con Sedigheh Eslami, Andreas Koukounas, Wang Feng e il CEO Han Xiao che hanno presentato tre documenti di ricerca che mostrano la nostra ultima ricerca su <code>jina-clip-v2</code> e <code>ReaderLM-v2</code> per una migliore ricerca. Mentre il resto del mondo dell'IA sembra bloccato in una corsa agli armamenti per modelli sempre più grandi, noi abbiamo deciso di nuotare controcorrente, dimostrando che modelli più piccoli e intelligenti possono dare il massimo quando il design è giusto.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1391\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png 2108w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Quindi prendete il vostro caffè, mettetevi comodi ed esploriamo alcune ricerche ICLR che abbiamo trovato interessanti, a cominciare dalla nostra opinione sul perché piccolo può essere potente.</p><h2 id=\"mitigate-the-gap-improving-cross-modal-alignment-in-clip\">Mitigate the Gap: Improving Cross-Modal Alignment in CLIP</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2406.17639\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</div><div class=\"kg-bookmark-description\">Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding space overly sparse and disconnected, with different modalities being densely distributed in distinct subregions of the hypersphere. In this work, we aim at answering three main questions: 1. Does sharing the parameter space between the multi-modal encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart the uni-modal embeddings via intra-modality separation? 3. How do these gap reduction approaches affect the downstream performance? We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings, and thereby, reduces the modality gap, while improving the performance across several zero-shot and fine-tuning downstream evaluations.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Sedigheh Eslami</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-17.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1279\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png 2120w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I modelli CLIP eccellono nelle attività immagine-testo, ma soffrono di un <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">\"divario di modalità\"</a>: gli  incorporamenti (embeddings) di immagini e testo si raggruppano in regioni separate, limitando le prestazioni. Questo lavoro, guidato dalla nostra stagista Sedigheh Eslami durante il suo dottorato di ricerca presso l'Hasso Plattner Institute, affronta questo problema fondamentale.</p><p>Abbiamo scoperto che la semplice traduzione vettoriale rompe la struttura di incorporamento (embedding). Invece, <strong>AlignCLIP</strong> utilizza parametri di codifica condivisi con obiettivi di separazione semanticamente regolamentati. Questo duplice approccio riduce con successo il divario di modalità migliorando al contempo le prestazioni nelle attività zero-shot e di ottimizzazione.</p><p><strong>Punti chiave:</strong></p><ul><li>Il divario di modalità è un collo di bottiglia critico per le prestazioni di CLIP</li><li>La condivisione dei parametri + la separazione semantica colmano efficacemente le differenze modali</li><li>L'approccio offre guadagni misurabili nelle valutazioni a valle</li></ul><h2 id=\"jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images\">jina-clip-v2: Incorporamenti (Embeddings) multimodali multilingue per testo e immagini</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2412.08802\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model’s performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-18.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1115\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png 2268w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo è il documento alla base di <code>jina-clip-v2</code>, un modello di incorporamento (embedding) multimodale multilingue che supporta attività di solo testo e crossmodali utilizzando un approccio di apprendimento contrastivo multi-task e multi-stage. Il modello combina un codificatore di testo (Jina XLM-RoBERTa, 561 milioni di parametri) e un codificatore di visione (EVA02-L14, 304 milioni di parametri) per un totale di 865 milioni di parametri. Ci alleniamo su testi multilingue provenienti da 29 lingue non inglesi e documenti visivamente ricchi, utilizzando Matryoshka Representation Learning per una dimensione di incorporamento (embedding) flessibile.</p><p><strong>Punti chiave:</strong></p><ul><li>La combinazione di dati immagine-testo e testo-testo in singoli batch con parametri di temperatura condivisi ha prestazioni inferiori rispetto all'addestramento separato a causa dell'asimmetria delle informazioni sulla modalità.</li><li>L'addestramento per l'allineamento crossmodale compromette intrinsecamente la qualità dell'incorporamento (embedding) di testo puro, mostrando un compromesso fondamentale.</li><li>Il taglio degli incorporamenti (embeddings) da 1.024 a 256 dimensioni causa una perdita di prestazioni inferiore all'1%, rivelando un'enorme inefficienza nelle rappresentazioni ad alta dimensione.</li></ul><h2 id=\"readerlm-v2-small-language-model-for-html-to-markdown-and-json\">ReaderLM-V2: Modello linguistico piccolo per HTML in Markdown e JSON</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2503.01151\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</div><div class=\"kg-bookmark-description\">We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model’s effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Feng Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-19.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1196\" height=\"912\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png 1196w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo è l'articolo alla base di <code>ReaderLM-v2</code>, un modello linguistico compatto da 1,5 miliardi di parametri progettato per l'estrazione efficiente di contenuti web. Il modello elabora documenti fino a 512K token, trasformando l'HTML disordinato in formati Markdown o JSON puliti. Il nostro approccio combina una pipeline di sintesi dei dati a tre fasi (DRAFT-REFINE-CRITIQUE) che genera dati di addestramento di alta qualità attraverso un perfezionamento iterativo con un framework di addestramento unificato che combina pre-addestramento continuo, fine-tuning supervisionato, ottimizzazione diretta delle preferenze e tuning iterativo self-play. ReaderLM-v2 sovraperforma GPT-4o e altri modelli più grandi del 15-20% sui benchmark, eccellendo in particolare nei documenti che superano i 100K token pur mantenendo requisiti computazionali significativamente inferiori.</p><p><strong>Punti chiave:</strong></p><ul><li>Un modello da 1,5 miliardi di parametri sovraperforma i modelli GPT-4o e 32B del 15-20% sull'estrazione HTML, dimostrando che il fine-tuning specifico per attività supera la scala grezza per la competenza nel dominio.</li><li>Il modello genera i propri dati di addestramento nella fase 4 \"self-play\", creando set di dati migliori di quelli curati dall'uomo e migliorando continuamente le prestazioni attraverso il feedback ricorsivo.</li><li>Il modello ha sofferto di ripetizioni catastrofiche di token durante l'addestramento, ma l'aggiunta di una perdita contrastiva per incoraggiare rappresentazioni discriminatorie ha eliminato completamente questo problema di degenerazione.</li></ul><h2 id=\"tips-text-image-pretraining-with-spatial-awareness\">TIPS: Pre-addestramento Testo-Immagine con Consapevolezza Spaziale</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2410.16512\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TIPS: Pre-addestramento Testo-Immagine con Consapevolezza Spaziale</div><div class=\"kg-bookmark-description\">Mentre l'apprendimento della rappresentazione immagine-testo è diventato molto popolare negli ultimi anni, i modelli esistenti tendono a mancare di consapevolezza spaziale e hanno un'applicabilità diretta limitata per attività di comprensione densa. Per questo motivo, il pre-addestramento auto-supervisionato solo immagine è ancora il metodo di riferimento per molte applicazioni di visione densa (ad esempio, stima della profondità, segmentazione semantica), nonostante la mancanza di segnali di supervisione espliciti. In questo articolo, colmiamo questo divario tra apprendimento immagine-testo e auto-supervisionato, proponendo un nuovo modello immagine-testo generico, che può essere efficacemente utilizzato immediatamente per attività di visione dense e globali. Il nostro metodo, che chiamiamo Pre-addestramento Testo-Immagine con Consapevolezza Spaziale (TIPS), sfrutta due intuizioni semplici ed efficaci. Innanzitutto, sulla supervisione testuale: riveliamo che la sostituzione di didascalie di immagini web rumorose con descrizioni testuali generate sinteticamente aumenta significativamente le prestazioni di comprensione densa, a causa di un segnale molto più ricco per l'apprendimento di rappresentazioni spazialmente consapevoli. Proponiamo un metodo di addestramento adattato che combina didascalie rumorose e sintetiche, con conseguenti miglioramenti in tutte le attività di comprensione dense e globali. In secondo luogo, sulla tecnica di apprendimento: proponiamo di combinare l'apprendimento contrastivo immagine-testo con la modellazione di immagini mascherate auto-supervisionata, per incoraggiare la coerenza spaziale, sbloccando miglioramenti sostanziali per le applicazioni downstream. Basandoci su queste due idee, scaliamo il nostro modello utilizzando l'architettura transformer, addestrata su un set curato di immagini pubbliche. I nostri esperimenti sono condotti su 8 attività che coinvolgono 16 set di dati in totale, dimostrando una forte performance off-the-shelf sia sulla comprensione densa che globale, per diverse attività solo immagine e immagine-testo. Codice e modelli sono rilasciati su https://github.com/google-deepmind/tips.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-24.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Kevis-Kokitsi Maninis</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-20.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png 2210w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I modelli di visione-linguaggio addestrati con l'apprendimento contrastivo eccellono nell'allineamento globale immagine-testo, ma falliscono nelle attività di comprensione spaziale densa. TIPS combina l'apprendimento contrastivo con la modellazione di immagini mascherate e utilizza didascalie generate sinteticamente che codificano le relazioni spaziali, creando  vettori modello (embeddings) adatti sia alla comprensione densa che globale senza fine-tuning specifico per l'attività. L'approccio dimostra come la consapevolezza spaziale possa essere incorporata nei modelli di  vettori modello (embedding) per una migliore comprensione dei documenti e applicazioni di recupero multimodale.</p><p><strong>Punti chiave:</strong></p><ul><li>Le didascalie sintetiche con descrizioni spaziali forniscono segnali di addestramento più ricchi rispetto alle didascalie web rumorose per l'apprendimento di rappresentazioni spazialmente consapevoli.</li><li>La combinazione dell'apprendimento contrastivo immagine-testo con obiettivi auto-supervisionati colma il divario tra comprensione globale e densa.</li><li>Le prestazioni off-the-shelf su diverse attività eliminano la necessità di un fine-tuning specializzato tra diverse applicazioni di visione.</li></ul><h2 id=\"cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies\">Cut Cross-Entropy: Calcolo della Perdita Efficiente in Termini di Memoria per Grandi Vocabolari</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2411.09009\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Riduci le Tue Perdite nei Modelli Linguistici con Grandi Vocabolari</div><div class=\"kg-bookmark-description\">Man mano che i modelli linguistici diventano sempre più grandi, così fanno i loro vocabolari. Questo ha spostato l'impronta di memoria degli  LLM (LLM) durante l'addestramento in modo sproporzionato su un singolo livello: l'entropia incrociata nel calcolo della perdita. L'entropia incrociata costruisce una matrice di logit con voci per ogni coppia di  token (tokens) di input e voci di vocabolario e, per i modelli piccoli, consuma un ordine di grandezza in più di memoria rispetto al resto dell' LLM (LLM) combinato. Proponiamo Cut Cross-Entropy (CCE), un metodo che calcola la perdita di entropia incrociata senza materializzare i logit per tutti i  token (tokens) nella memoria globale. Piuttosto, CCE calcola solo il logit per il  token (token) corretto e valuta il log-sum-exp su tutti i logit al volo. Implementiamo un kernel personalizzato che esegue le moltiplicazioni di matrici e la riduzione log-sum-exp sul vocabolario nella memoria flash, rendendo trascurabile il consumo di memoria globale per il calcolo dell'entropia incrociata. Questo ha un effetto drammatico. Prendendo come esempio il modello Gemma 2 (2B), CCE riduce l'impronta di memoria del calcolo della perdita da 24 GB a 1 MB e il consumo totale di memoria durante l'addestramento dell'head del classificatore da 28 GB a 1 GB. Per migliorare il throughput di CCE, sfruttiamo la sparsità intrinseca di softmax e proponiamo di saltare gli elementi del calcolo del gradiente che hanno un contributo trascurabile (cioè, al di sotto della precisione numerica) al gradiente. Gli esperimenti dimostrano che la drastica riduzione del consumo di memoria si ottiene senza sacrificare la velocità di addestramento o la convergenza.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-25.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Erik Wijmans</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-21.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1904\" height=\"1226\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png 1904w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il calcolo dell'entropia incrociata domina l'utilizzo della memoria nei modelli linguistici con grandi vocabolari, richiedendo la materializzazione di matrici logit proporzionali a batch_size × vocabulary_size. CCE riformula il calcolo per calcolare solo i componenti necessari al volo utilizzando kernel CUDA personalizzati, riducendo il consumo di memoria da gigabyte a megabyte mantenendo la stessa dinamica di addestramento. Ciò consente l'addestramento di  vettori modello (embedding) e modelli di  重排器 (reranking) con vocabolari più grandi su hardware limitato, particolarmente vantaggioso per applicazioni multilingue e specifiche del dominio.</p><p><strong>Punti chiave:</strong></p><ul><li>Il calcolo della perdita di entropia incrociata può consumare il 90% della memoria di addestramento per i modelli con grandi vocabolari, diventando il collo di bottiglia principale.</li><li>Il calcolo al volo dei termini log-sum-exp elimina la necessità di materializzare matrici logit complete senza approssimazioni matematiche.</li><li>L'implementazione di un kernel personalizzato consente una drastica riduzione della memoria preservando le esatte proprietà di convergenza.</li></ul><h2 id=\"flexprefill-context-aware-sparse-attention-for-long-sequences\">FlexPrefill: Attenzione Sparsa Sensibile al Contesto per Sequenze Lunghe</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.20766\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">FlexPrefill: Un Meccanismo di Attenzione Sparsa Sensibile al Contesto per l'Inferenza Efficiente di Sequenze Lunghe</div><div class=\"kg-bookmark-description\">I modelli linguistici di grandi dimensioni (LLM, Large language models) incontrano sfide computazionali durante l'inferenza di sequenze lunghe, specialmente nella fase di pre-riempimento dell'attenzione, dove la complessità cresce quadraticamente con la lunghezza del prompt. I precedenti sforzi per mitigare queste sfide si sono basati su pattern di attenzione sparsi fissi o sull'identificazione di pattern di attenzione sparsi basati su casi limitati. Tuttavia, questi metodi mancavano della flessibilità necessaria per adattarsi in modo efficiente alle diverse esigenze di input. In questo articolo, introduciamo FlexPrefill, un meccanismo di pre-riempimento sparso flessibile che adatta dinamicamente i pattern di attenzione sparsi e il budget computazionale in tempo reale per soddisfare i requisiti specifici di ogni input e attention head. La flessibilità del nostro metodo è dimostrata attraverso due innovazioni chiave: 1) Determinazione di pattern sparsi consapevoli della query: Misurando la divergenza di Jensen-Shannon, questo componente commuta adattivamente tra pattern di attenzione diversificati specifici della query e pattern di attenzione predefiniti. 2) Selezione di indici basata sull'attenzione cumulativa: Questo componente seleziona dinamicamente gli indici query-key da calcolare in base a diversi pattern di attenzione, garantendo che la somma dei punteggi di attenzione soddisfi una soglia predefinita. FlexPrefill ottimizza adattivamente il pattern sparso e il rapporto sparso di ogni attention head in base al prompt, migliorando l'efficienza nelle attività di inferenza di sequenze lunghe. I risultati sperimentali mostrano miglioramenti significativi sia in termini di velocità che di accuratezza rispetto ai metodi precedenti, fornendo una soluzione più flessibile ed efficiente per l'inferenza LLM.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-26.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xunhao Lai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-22.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1882\" height=\"1254\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png 1882w\" sizes=\"(min-width: 720px) 720px\"></figure><p>L'inferenza di transformer a sequenza lunga soffre di complessità di attenzione quadratica. FlexPrefill determina dinamicamente i pattern di attenzione sparsi per head utilizzando la divergenza di Jensen-Shannon e alloca adattivamente il budget computazionale in base ai punteggi di attenzione cumulativi, ottenendo accelerazioni significative con una perdita minima di accuratezza tra diversi tipi di contenuto. Il metodo consente l'elaborazione efficiente di documenti lunghi per sistemi di ricerca e recupero, consentendo a modelli linguistici più piccoli di gestire contesti estesi per una migliore comprensione dei documenti.</p><p><strong>Punti chiave:</strong></p><ul><li>I pattern di attenzione sparsi dinamici adattati al tipo di contenuto superano le strategie di sparsità fissa tra diverse caratteristiche di input</li><li>L'allocazione del budget adattivo per-head basata sull'accumulo del punteggio di attenzione ottimizza la distribuzione del calcolo in tempo reale</li><li>La sparsità consapevole del contesto raggiunge un'accelerazione di 13,7× con una perdita di accuratezza dello 0,1% senza richiedere il retraining del modello</li></ul><h2 id=\"effective-post-training-embedding-compression-via-temperature-control\">Compressione efficace di 向量模型 (Embeddings) post-training tramite controllo della temperatura</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://openreview.net/forum?id=szRmEM8Kx5\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Compressione efficace di 向量模型 (Embeddings) post-training tramite temperatura...</div><div class=\"kg-bookmark-description\">Le rappresentazioni apprese a dimensione fissa (rappresentazioni dense o 向量模型 (embeddings)) sono ampiamente utilizzate in molte applicazioni di machine learning tra modalità linguistiche, visive o vocali. Questo articolo indaga…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-37.ico\" alt=\"\"><span class=\"kg-bookmark-author\">OpenReview.net</span><span class=\"kg-bookmark-publisher\">Georgiana Dinu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/pdf_icon_blue.svg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1230\" height=\"906\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png 1230w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il ridimensionamento della temperatura nell'apprendimento contrastivo influenza significativamente la dimensionalità intrinseca dei 向量模型 (embeddings) appresi, con temperature più basse che producono rappresentazioni più comprimibili. L'articolo dimostra che i metodi di aggregazione della temperatura possono ridurre le dimensioni degli 向量模型 (embeddings) di un ordine di grandezza mantenendo le prestazioni di recupero, rivelando il compromesso tra efficacia del clustering e accuratezza del recupero. Ciò consente l'implementazione efficiente di sistemi di recupero denso in cui i vincoli di memoria sono fondamentali per le applicazioni di produzione.</p><p><strong>Punti chiave:</strong></p><ul><li>Valori di temperatura più bassi nell'addestramento contrastivo producono 向量模型 (embeddings) con dimensionalità intrinseca inferiore che si comprimono in modo più efficace</li><li>Le tecniche di aggregazione della temperatura raggiungono rapporti di compressione di 10× con un degrado minimo della qualità nelle attività di recupero</li><li>Il controllo sistematico della temperatura durante l'addestramento fornisce un meccanismo diretto per ottimizzare il compromesso compressione-prestazioni</li></ul><h2 id=\"attention-in-large-language-models-yields-efficient-zero-shot-re-rankers\">L'attenzione nei modelli linguistici di grandi dimensioni produce 重排器 (Reranker) zero-shot efficienti</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2410.02642\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">L'attenzione nei modelli linguistici di grandi dimensioni produce 重排器 (Reranker) zero-shot efficienti</div><div class=\"kg-bookmark-description\">I sistemi di information retrieval (IR) hanno svolto un ruolo fondamentale nella vita digitale moderna e hanno cementato la loro continua utilità in questa nuova era dell'IA generativa tramite la generazione aumentata dal recupero. Con forti capacità di elaborazione del linguaggio e notevole versatilità, i modelli linguistici di grandi dimensioni (LLM, Large language models) sono diventati scelte popolari per il re-ranking zero-shot nei sistemi IR. Finora, i metodi di re-ranking basati su LLM si basano su forti capacità generative, il che limita il loro utilizzo a modelli proprietari specializzati o potenti. Date queste restrizioni, ci chiediamo: la generazione autoregressiva è necessaria e ottimale affinché gli LLM eseguano il re-ranking? Ipotizziamo che ci siano abbondanti segnali rilevanti per il re-ranking all'interno degli LLM che potrebbero non essere utilizzati al massimo del loro potenziale tramite la generazione. Per sfruttare più direttamente tali segnali, proponiamo il re-ranking in-context (ICR), un nuovo metodo che sfrutta il cambiamento nel pattern di attenzione causato dalla query di ricerca per un re-ranking accurato ed efficiente. Per mitigare i bias intrinseci negli LLM, proponiamo un metodo di calibrazione utilizzando una query senza contenuto. A causa dell'assenza di generazione, l'ICR richiede solo due passaggi forward ($O(1)$) per ri-ordinare $N$ documenti, rendendolo sostanzialmente più efficiente dei metodi di re-ranking generativi che richiedono almeno $O(N)$ passaggi forward. Il nostro nuovo design consente inoltre di applicare l'ICR a qualsiasi LLM senza addestramento specializzato, garantendo al contempo un ranking ben formato. Vasti esperimenti con due popolari LLM open-weight su benchmark standard di information retrieval single-hop e multi-hop mostrano che l'ICR supera RankGPT riducendo al contempo la latenza di oltre il 60% nella pratica. Attraverso analisi dettagliate, mostriamo che le prestazioni dell'ICR sono particolarmente elevate su attività che richiedono segnali di re-ranking più complessi. Le nostre scoperte richiedono un'ulteriore esplorazione su nuovi modi di utilizzare gli LLM open-weight oltre la generazione di testo.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-27.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Shijie Chen</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-23.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd_nbgJG03yk0oOma7ozLQ6zutKGy3ngkVLKUwmp3ie6UPp2RR6qjwsWzwwtP0QzyAneCTD24nrPXpA085rkjtS_HmlrkHbrksxSjsaknx1lb9OtgtlACmwoOZkoRK9oPb4Haf?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>Il re-ranking in-context (ICR, In-Context Re-ranking) sfrutta i cambiamenti del pattern di attenzione negli LLM per eseguire il re-ranking dei documenti senza generazione di testo, riducendo la complessità computazionale da O(N log N) a O(1). Il metodo aggrega i pesi di attenzione tra i livelli e gli head per calcolare i punteggi di rilevanza, con la calibrazione della query senza contenuto per mitigare i bias LLM. Questo approccio consente un re-ranking efficiente con modelli open-weight, eliminando la necessità di fine-tuning specializzato o costosi processi di generazione.</p><p><strong>Punti chiave:</strong> </p><ul><li>I pattern di attenzione negli LLM contengono segnali sufficienti per un efficace re-ranking dei documenti senza richiedere la generazione di testo</li><li>La calibrazione della query senza contenuto mitiga con successo i bias intrinseci nei meccanismi di scoring basati sull'attenzione </li><li>L'ICR raggiunge prestazioni ed efficienza superiori rispetto ai metodi generativi, in particolare su attività complesse di recupero multi-hop</li></ul><h2 id=\"bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization\">Bridging e modellazione delle correlazioni nei dati pairwise per l'ottimizzazione diretta delle preferenze</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2408.07471\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Bridging e modellazione delle correlazioni nei dati pairwise per l'ottimizzazione diretta delle preferenze</div><div class=\"kg-bookmark-description\">L'ottimizzazione diretta delle preferenze (DPO, Direct preference optimization), un algoritmo di ottimizzazione delle preferenze offline ampiamente adottato, mira ad allineare i modelli linguistici di grandi dimensioni (LLM, Large language models) con i comportamenti desiderati dall'uomo utilizzando dati di preferenza a coppie. Tuttavia, la generazione della risposta vincente e della risposta perdente all'interno dei dati a coppie è in genere isolata, il che porta a deboli correlazioni tra di esse, nonché a prestazioni di allineamento non ottimali. Per affrontare questo problema, proponiamo un framework efficace per il collegamento e la modellazione delle correlazioni nei dati a coppie, denominato BMC. In primo luogo, aumentiamo la coerenza e l'informatività dei segnali di preferenza a coppie attraverso modifiche mirate, sintetizzando una pseudo-risposta vincente migliorando la risposta perdente con la risposta vincente come riferimento. In secondo luogo, identifichiamo che il solo DPO è insufficiente per modellare queste correlazioni e catturare sfumature. Pertanto, proponiamo di apprendere le correlazioni a livello di token sfruttando dinamicamente la fiducia del modello di policy durante l'addestramento. Esperimenti completi su attività di QA, matematica e following delle istruzioni dimostrano l'efficacia del nostro approccio, superando significativamente le baseline competitive, incluso il DPO. Inoltre, la nostra analisi quantitativa approfondita rivela le ragioni alla base delle prestazioni superiori del nostro metodo rispetto al DPO e ne dimostra la versatilità rispetto ad altre varianti di DPO. Rilasciamo il nostro repository all'indirizzo https://github.com/YJiangcm/BMC.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-28.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Yuxin Jiang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-24.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd94pEqUtljUS8gF3KonDFqs9umcVCfXEASHlAeCjl07YMviucHiIj1doZIe5_VHSVxthzhgA_ta0E90vQVcunSRj0UnHsubFzD75ow-EfNICcDadQvdtUx-WOZGt9v9rFB_4E?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>Il DPO tradizionale soffre di deboli correlazioni tra le risposte scelte e quelle rifiutate nelle coppie di preferenze, limitando l'efficacia dell'allineamento. BMC affronta questo problema sintetizzando pseudo-risposte preferite che interpolano tra le risposte vincenti e perdenti, quindi applica la modellazione della correlazione a livello di token utilizzando la confidenza del modello di policy. L'approccio in due fasi prima colma le coppie di preferenze attraverso modifiche mirate, quindi modella le correlazioni granulari durante l'addestramento per migliorare la qualità del segnale di apprendimento.</p><p><strong>Punti chiave:</strong></p><ul><li>Le deboli correlazioni tra le risposte scelte e quelle rifiutate nei dati di preferenza limitano significativamente l'efficacia del DPO per l'allineamento del modello.</li><li>La sintesi di pseudo-risposte preferite come interpolazioni tra coppie di preferenze fornisce segnali di apprendimento più ricchi per l'ottimizzazione.</li><li>La modellazione della correlazione a livello di token utilizzando la confidenza della policy pondera dinamicamente i segnali di addestramento per catturare sfumature nei dati di preferenza.</li></ul><h2 id=\"taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer\">TAID: Distillazione interpolata adattiva temporalmente per un efficiente trasferimento di conoscenza</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2501.16937\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models</div><div class=\"kg-bookmark-description\">I modelli linguistici causali hanno dimostrato capacità notevoli, ma le loro dimensioni pongono sfide significative per l'implementazione in ambienti con risorse limitate. La distillazione della conoscenza, una tecnica ampiamente utilizzata per trasferire la conoscenza da un modello teacher di grandi dimensioni a un modello student di piccole dimensioni, rappresenta un approccio promettente per la compressione del modello. Un problema significativo ancora irrisolto risiede nelle principali differenze tra i modelli teacher e student, vale a dire il notevole divario di capacità, la media della modalità e il collasso della modalità, che rappresentano barriere durante la distillazione. Per affrontare questi problemi, introduciamo $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, un nuovo approccio di distillazione della conoscenza che interpola dinamicamente le distribuzioni student e teacher attraverso una distribuzione intermedia adattiva, passando gradualmente dalla distribuzione iniziale dello studente alla distribuzione dell'insegnante. Forniamo un'analisi teorica che dimostra la capacità di TAID di prevenire il collasso della modalità e mostriamo empiricamente la sua efficacia nell'affrontare il divario di capacità bilanciando la media della modalità e il collasso della modalità. I nostri esperimenti completi dimostrano le prestazioni superiori di TAID su varie dimensioni e architetture di modelli sia in scenari di ottimizzazione delle istruzioni che di pre-addestramento. Inoltre, mostriamo l'impatto pratico di TAID sviluppando due modelli di base compatti all'avanguardia: $\\texttt{TAID-LLM-1.5B}$ per attività linguistiche e $\\texttt{TAID-VLM-2B}$ per attività di linguaggio visivo. Questi risultati dimostrano l'efficacia di TAID nella creazione di modelli efficienti e ad alte prestazioni, facendo progredire lo sviluppo di tecnologie AI più accessibili.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-29.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Makoto Shing</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-25.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe9EWtq20jDVieU8M2BPDP5kENd3oSJKwIKNnKq_9bB4mb9vtNvjK-RMx8ZB29EhnyjIST90b2HRNek6bSkPXFlOxzTPhUAjf86d6iBCphJtgjfcxrCdY__HcDW9ADgVla1mVWBpQ?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"312\"></figure><p>La distillazione della conoscenza deve affrontare le sfide derivanti dai divari di capacità, dalla media della modalità e dal collasso della modalità quando si trasferisce la conoscenza tra modelli grandi e piccoli. TAID introduce un teacher intermedio dinamico che interpola tra le distribuzioni student e teacher, adattando gradualmente la distribuzione target in base ai progressi dell'addestramento. Questo approccio previene il collasso della modalità attraverso garanzie teoriche e ottiene prestazioni superiori su varie dimensioni del modello, consentendo lo sviluppo di modelli linguistici compatti ma capaci.</p><p><strong>Punti chiave:</strong></p><ul><li>I teacher intermedi dinamici che si adattano durante l'addestramento forniscono traiettorie di apprendimento più fluide rispetto alla distillazione con teacher fisso.</li><li>TAID previene il collasso della modalità attraverso l'interpolazione adattiva bilanciando il trasferimento di conoscenza attraverso diversi divari di capacità.</li><li>Il metodo consente l'addestramento di modelli compatti all'avanguardia senza richiedere architetture specializzate o un'ampia messa a punto degli iperparametri.</li></ul><h2 id=\"svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression\">SVD-LLM: Decomposizione a valori singolari consapevole della troncamento per la compressione di modelli linguistici di grandi dimensioni</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2403.07378\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression</div><div class=\"kg-bookmark-description\">I progressi nei modelli linguistici di grandi dimensioni (LLM, Large Language Models) sono stati ostacolati dalle loro dimensioni sostanziali, il che richiede metodi di compressione LLM per un'implementazione pratica. La decomposizione a valori singolari (SVD, Singular Value Decomposition) offre una soluzione promettente per la compressione LLM. Tuttavia, i metodi di compressione LLM basati su SVD all'avanguardia presentano due limiti chiave: il troncamento di valori singolari più piccoli può portare a una maggiore perdita di compressione e la mancanza di aggiornamento sui pesi compressi dopo il troncamento SVD. In questo lavoro, proponiamo SVD-LLM, un metodo di compressione LLM post-addestramento basato su SVD che affronta i limiti dei metodi esistenti. SVD-LLM incorpora una tecnica di sbiancamento dei dati consapevole del troncamento per garantire una mappatura diretta tra valori singolari e perdita di compressione. Inoltre, SVD-LLM adotta un aggiornamento dei parametri con approssimazione sequenziale a basso rango per compensare il degrado dell'accuratezza dopo la compressione SVD. Valutiamo SVD-LLM su 10 set di dati e sette modelli di tre diverse famiglie LLM a tre diverse scale. I nostri risultati dimostrano la superiorità di SVD-LLM rispetto allo stato dell'arte, soprattutto con elevati rapporti di compressione del modello. Il nostro codice è disponibile all'indirizzo https://github.com/AIoT-MLSys-Lab/SVD-LLM</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-30.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xin Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-26.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXenhj46Ar7NKFevDTmA3FK2dnjd7nQxdhULJ1H3Je-2OKoQN6_Ov8km-AvIldpEriENz2Q465hq2yoOZ1lLAle7ijbMgSK0ME9UxNeIN3yqyRFtRO_3FFXEyXdI04wndPS17a-3?key=HM7UHIZt2c2Fh4qitXjYcQ\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>I metodi di compressione esistenti basati su SVD non tengono conto delle attivazioni di input durante l'approssimazione e mancano di fine-tuning post-troncamento. SVD-LLM incorpora lo sbiancamento dei dati consapevole del troncamento che considera le distribuzioni di attivazione e applica il fine-tuning basato su LoRA dopo la compressione. Il metodo stabilisce connessioni teoriche tra valori singolari e perdita di compressione, consentendo decisioni di compressione più rigorose che superano gli approcci di pruning e quantizzazione strutturati.</p><p><strong>Punti chiave:</strong></p><ul><li>Lo sbiancamento dei dati consapevole del troncamento che tiene conto delle attivazioni di input migliora significativamente l'efficacia della compressione SVD rispetto ai metodi agnostici all'attivazione.</li><li>Il fine-tuning LoRA post-compressione compensa il degrado dell'accuratezza mantenendo i vantaggi della fattorizzazione a basso rango.</li><li>L'analisi teorica che collega i valori singolari alla perdita di compressione consente decisioni di troncamento rigorose che superano gli approcci euristici.</li></ul><h2 id=\"see-what-you-are-told-visual-attention-sink-in-large-multimodal-models\">Vedi cosa ti viene detto: Sink di attenzione visiva in modelli multimodali di grandi dimensioni</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2503.03321\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">See What You Are Told: Visual Attention Sink in Large Multimodal Models</div><div class=\"kg-bookmark-description\">I modelli multimodali di grandi dimensioni (LMM, Large multimodal models) \"vedono\" le immagini sfruttando il meccanismo di attenzione tra i token testuali e visivi nel decoder transformer. Idealmente, questi modelli dovrebbero concentrarsi sulle informazioni visive chiave rilevanti per il token testuale. Tuttavia, recenti scoperte indicano che gli LMM hanno una straordinaria tendenza ad allocare costantemente pesi di attenzione elevati a specifici token visivi, anche quando questi token sono irrilevanti per il testo corrispondente. In questo studio, esaminiamo la proprietà alla base della comparsa di questi token visivi irrilevanti ed esaminiamo le loro caratteristiche. I nostri risultati mostrano che questo comportamento deriva dall'attivazione massiccia di determinate dimensioni dello stato nascosto, che assomiglia all'attention sink (pozzo di attenzione) presente nei modelli linguistici. Quindi, ci riferiamo a questo fenomeno come visual attention sink (pozzo di attenzione visivo). In particolare, la nostra analisi rivela che la rimozione dei token sink visivi irrilevanti non influisce sulle prestazioni del modello, nonostante ricevano pesi di attenzione elevati. Di conseguenza, ricicliamo l'attenzione a questi token come risorse in eccesso, ridistribuendo il budget di attenzione per migliorare la concentrazione sull'immagine. Per raggiungere questo obiettivo, introduciamo Visual Attention Redistribution (VAR, Ridistribuzione dell'Attenzione Visiva), un metodo che ridistribuisce l'attenzione nelle head incentrate sull'immagine, che identifichiamo come intrinsecamente focalizzate sulle informazioni visive. VAR può essere applicato senza problemi a diversi LMM per migliorare le prestazioni su una vasta gamma di attività, tra cui attività generali di visione-linguaggio, attività di allucinazione visiva e attività incentrate sulla visione, il tutto senza la necessità di ulteriore addestramento, modelli o passaggi di inferenza. I risultati sperimentali dimostrano che VAR consente agli LMM di elaborare le informazioni visive in modo più efficace regolando i loro meccanismi di attenzione interni, offrendo una nuova direzione per migliorare le capacità multimodali degli LMM.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-31.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Seil Kang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-27.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrHjLu7qX5TOjtDH10svBjs-6rihxNRpgS3Bq9r8qtY9UvOC4LqyBo-NDWeESuRrv-vj6btANt6doA4IneaENN1712o3kzHhQwx20PR62b8JKDA5jIjCNgKAhXoCp9bEcbadyfPA?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>I modelli multimodali di grandi dimensioni mostrano un fenomeno chiamato \"visual attention sink (pozzo di attenzione visivo)\" in cui allocano costantemente pesi di attenzione elevati a specifici token visivi che sono irrilevanti per i token testuali corrispondenti. Questi token visivi irrilevanti emergono dall'attivazione massiccia in specifiche dimensioni dello stato nascosto, simili agli attention sink nei modelli linguistici. Il metodo Visual Attention Redistribution (VAR, Ridistribuzione dell'Attenzione Visiva) identifica le head di attenzione incentrate sull'immagine e ridistribuisce il budget di attenzione dai token sink al contenuto visivo significativo, migliorando le prestazioni in tutte le attività di visione-linguaggio senza richiedere ulteriore addestramento.</p><p><strong>Punti chiave:</strong> </p><ul><li>I token sink visivi possono essere identificati da magnitudini di attivazione estreme in dimensioni fisse ereditate dai modelli linguistici di base</li><li>La rimozione dei token sink visivi non influisce sulle prestazioni del modello nonostante la ricezione di pesi di attenzione elevati, indicando uno spreco di risorse computazionali</li><li>VAR ridistribuisce l'attenzione dai token sink al contenuto visivo significativo, migliorando le prestazioni nella visione-linguaggio generale, nella riduzione dell'allucinazione e nelle attività incentrate sulla visione</li></ul><h2 id=\"towards-semantic-equivalence-of-tokenization-in-multimodal-llm\">Verso l'equivalenza semantica della tokenizzazione nei LLM multimodali</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2406.05127\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Towards Semantic Equivalence of Tokenization in Multimodal LLM</div><div class=\"kg-bookmark-description\">I modelli linguistici di grandi dimensioni multimodali (MLLM, Multimodal Large Language Models) hanno dimostrato eccezionali capacità nell'elaborazione di attività di visione-linguaggio. Uno dei punti cruciali degli MLLM risiede nella tokenizzazione della visione, che implica la trasformazione efficiente dei segnali visivi in ingresso in rappresentazioni di caratteristiche che sono più vantaggiose per gli LLM. Tuttavia, i tokenizzatori di visione esistenti, essenziali per l'allineamento semantico tra visione e linguaggio, rimangono problematici. I metodi esistenti frammentano aggressivamente l'input visivo, corrompendo l'integrità semantica visiva. Per affrontare questo problema, questo articolo propone un nuovo tokenizzatore di visione Semantic-Equivalent Vision Tokenizer (SeTok, Tokenizzatore di Visione Semanticamente Equivalente) dinamico, che raggruppa le caratteristiche visive in unità semantiche tramite un algoritmo di clustering dinamico, determinando in modo flessibile il numero di token in base alla complessità dell'immagine. I token di visione risultanti preservano efficacemente l'integrità semantica e catturano sia le caratteristiche visive a bassa frequenza che ad alta frequenza. L'MLLM proposto (Setokim) dotato di SeTok dimostra significativamente prestazioni superiori in varie attività, come evidenziato dai nostri risultati sperimentali. La pagina del progetto è disponibile all'indirizzo https://chocowu.github.io/SeTok-web/.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Shengqiong Wu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-28.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXtLgOhJPcWaPKTtGm7D3lK6tc7EhPQGXleGMHMqYG_KFVTxCSBGOd8z6xovad6UMgDjTWPBFfKqD4J2gSD6L6YXpSaTlGNNrLWiViAlfPkKinc9jNjsD2Ulnrh0tZQ74RR62tvQ?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"468\"></figure><p>I metodi tradizionali di tokenizzazione della visione negli LLM multimodali frammentano l'input visivo utilizzando patch fisse, corrompendo l'integrità semantica e portando a uno scarso allineamento visione-linguaggio. SeTok (Semantic-Equivalent Vision Tokenizer, Tokenizzatore di Visione Semanticamente Equivalente) affronta questo problema attraverso il clustering dinamico che raggruppa le caratteristiche visive in unità semantiche coerenti, con il conteggio dei token che si adatta alla complessità dell'immagine. Il sistema utilizza doppi obiettivi di addestramento: la perdita contrastiva per l'allineamento semantico con il linguaggio e la perdita di ricostruzione per preservare i dettagli a livello di pixel per la ricostruzione dell'immagine.</p><p><strong>Punti chiave:</strong> </p><ul><li>La tokenizzazione a patch fisse interrompe l'integrità semantica visiva frammentando gli oggetti attraverso confini di patch arbitrari</li><li>Gli algoritmi di clustering dinamico possono determinare in modo adattivo il conteggio ottimale dei token in base alla complessità semantica dell'immagine piuttosto che a strutture a griglia fisse</li><li>L'addestramento a doppio obiettivo bilancia l'allineamento semantico con il linguaggio preservando al contempo dettagli visivi sufficienti per le attività di ricostruzione</li></ul><h2 id=\"hymba-a-hybrid-head-architecture-for-small-language-models\">Hymba: un'architettura a head ibrida per modelli linguistici piccoli</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2411.13676\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Hymba: A Hybrid-head Architecture for Small Language Models</div><div class=\"kg-bookmark-description\">Proponiamo Hymba, una famiglia di modelli linguistici piccoli caratterizzati da un'architettura parallela a head ibrida che integra i meccanismi di attenzione transformer con i modelli di spazio di stato (SSM, State Space Models) per una maggiore efficienza. Le head di attenzione forniscono un richiamo ad alta risoluzione, mentre le head SSM consentono un riepilogo efficiente del contesto. Inoltre, introduciamo meta token apprendibili che vengono anteposti ai prompt, memorizzando informazioni critiche e alleviando l'onere \"forced-to-attend\" associato ai meccanismi di attenzione. Questo modello è ulteriormente ottimizzato incorporando la condivisione key-value (KV) tra i livelli e l'attenzione parziale a finestra scorrevole, ottenendo una dimensione della cache compatta. Durante lo sviluppo, abbiamo condotto uno studio controllato confrontando varie architetture in impostazioni identiche e abbiamo osservato vantaggi significativi della nostra architettura proposta. In particolare, Hymba raggiunge risultati all'avanguardia per i piccoli LM: il nostro modello Hymba-1.5B-Base supera tutti i modelli pubblici inferiori a 2B in termini di prestazioni e supera persino Llama-3.2-3B con una precisione media superiore dell'1,32%, una riduzione delle dimensioni della cache di 11,67 volte e una velocità effettiva di 3,49 volte.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-33.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xin Dong</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-29.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqTXEnWvnqhnbdUKsRw-mZ8hzuqwIbX_dnFwiY4BHa8DF4ViWiekeIRVlRBtQkJF8a2EPv5U_H5kvqxFQfCg0jGplWefzce1RHzHBd17D93k6DpE3vNurR0Ufg7kMEJ_C4IeDBZQ?key=jcOajfpjEtGsUeEc3rEhlw\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"552\" height=\"573\"></figure><p>Hymba introduce un'architettura a head ibrida che combina i meccanismi di attenzione transformer con i modelli di spazio di stato (SSM, State Space Models) in parallelo all'interno di ogni livello, consentendo simultaneamente un richiamo ad alta risoluzione e un riepilogo efficiente del contesto. L'architettura incorpora meta token apprendibili, condivisione key-value tra i livelli e attenzione parziale a finestra scorrevole per ottenere dimensioni della cache compatte. Hymba-1.5B supera tutti i modelli inferiori a 2B e supera Llama-3.2-3B ottenendo al contempo una riduzione della cache di 11,67× e un miglioramento della velocità effettiva di 3,49×.</p><p><strong>Punti chiave:</strong> </p><ul><li>L'architettura ibrida a head parallela supera l'impilamento sequenziale di componenti di attenzione e SSM consentendo l'elaborazione simultanea di meccanismi complementari</li><li>I meta token apprendibili fungono da conoscenza del mondo compressa e alleviano l'onere \"forced-to-attend\" dei meccanismi di attenzione softmax</li><li>Le ottimizzazioni di condivisione key-value tra i livelli e di attenzione a finestra scorrevole consentono una drastica riduzione delle dimensioni della cache senza sacrificare le prestazioni</li></ul>",
  "comment_id": "68336ebaa4451f0001adcbad",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-25T21:25:46.000+02:00",
  "updated_at": "2025-05-26T00:06:37.000+02:00",
  "published_at": "2025-05-26T00:06:37.000+02:00",
  "custom_excerpt": "We collect some most interesting papers in ICLR 2025, featuring TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba etc.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "63340e5387b80b004db80543",
      "name": "Events",
      "slug": "events",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/events/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "63340e5387b80b004db80543",
    "name": "Events",
    "slug": "events",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/events/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-we-learned-at-iclr2025/",
  "excerpt": "Abbiamo raccolto alcuni dei documenti più interessanti di ICLR 2025, tra cui TIPS, FlexPrefill, Reranker Zero-Shot (Reranker Zero-Shot), SVD-LLM, Hymba ecc.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
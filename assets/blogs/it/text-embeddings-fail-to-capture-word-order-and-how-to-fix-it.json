{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "Gli Embedding di Testo Falliscono nel Catturare l'Ordine delle Parole e Come Risolvere il Problema",
  "html": "<p>Recentemente, Christoph Schuhmann, fondatore di <a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> ha condiviso un'interessante osservazione sui modelli di text embedding:</p><blockquote>Quando le parole all'interno di una frase vengono mescolate in modo casuale, la similarità del coseno tra i loro text embedding rimane sorprendentemente alta rispetto alla frase originale.</blockquote><p>Per esempio, consideriamo due frasi: <code>Berlin is the capital of Germany</code> e <code>the Germany Berlin is capital of</code>. Anche se la seconda frase non ha senso, i modelli di text embedding non riescono davvero a distinguerle. Usando <code>jina-embeddings-v3</code>, queste due frasi hanno un punteggio di similarità del coseno di 0,9295.</p><p>L'ordine delle parole non è l'unica cosa per cui gli embedding sembrano non essere molto sensibili. Le trasformazioni grammaticali possono cambiare drasticamente il significato di una frase ma avere poco impatto sulla distanza dell'embedding. Per esempio, <code>She ate dinner before watching the movie</code> e <code>She watched the movie before eating dinner</code> hanno una similarità del coseno di 0,9833, nonostante abbiano l'ordine opposto delle azioni.</p><p>Anche la negazione è notoriamente difficile da incorporare in modo coerente senza un <a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">addestramento speciale</a> — <code>This is a useful model</code> e <code>This is not a useful model</code> appaiono praticamente identiche nello spazio degli embedding. Spesso, sostituire le parole in un testo con altre della stessa classe, come cambiare \"today\" in \"yesterday\", o modificare il tempo di un verbo, non modifica gli embedding tanto quanto si potrebbe pensare.</p><p>Questo ha serie implicazioni. Consideriamo due query di ricerca: <code>Flight from Berlin to Amsterdam</code> e <code>Flight from Amsterdam to Berlin</code>. Hanno embedding quasi identici, con <code>jina-embeddings-v3</code> che assegna loro una similarità del coseno di 0,9884. Per un'applicazione del mondo reale come la ricerca di viaggi o la logistica, questa carenza è fatale.</p><p>In questo articolo, esaminiamo le sfide che affrontano i modelli di embedding, analizzando le loro persistenti difficoltà con l'ordine e la scelta delle parole. Analizziamo le principali modalità di fallimento attraverso categorie linguistiche—inclusi contesti direzionali, temporali, causali, comparativi e di negazione—mentre esploriamo strategie per migliorare le prestazioni del modello.</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">Perché le Frasi Mescolate Hanno Punteggi del Coseno Sorprendentemente Vicini?</h2><p>Inizialmente, pensavamo che questo potesse dipendere da come il modello combina i significati delle parole - crea un embedding per ogni parola (6-7 parole in ciascuna delle nostre frasi di esempio sopra) e poi fa una media di questi embedding con il mean pooling. Questo significa che molto poca informazione sull'ordine delle parole è disponibile nell'embedding finale. Una media è la stessa indipendentemente dall'ordine dei valori.</p><p>Tuttavia, anche i modelli che utilizzano il CLS pooling (che esamina una speciale prima parola per comprendere l'intera frase e dovrebbe essere più sensibile all'ordine delle parole) hanno lo stesso problema. Per esempio, <code>bge-1.5-base-en</code> dà ancora un punteggio di similarità del coseno di 0,9304 per le frasi <code>Berlin is the capital of Germany</code> e <code>the Germany Berlin is capital of</code>.</p><p>Questo indica una limitazione nel modo in cui vengono addestrati i modelli di embedding. Mentre i modelli linguistici inizialmente apprendono la struttura della frase durante il pre-training, sembrano perdere parte di questa comprensione durante l'addestramento contrastivo — il processo che utilizziamo per creare modelli di embedding.</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">Come la Lunghezza del Testo e l'Ordine delle Parole Influenzano la Similarità degli Embedding?</h2><p>Perché i modelli hanno problemi con l'ordine delle parole in primo luogo? La prima cosa che viene in mente è la lunghezza (in token) del testo. Quando il testo viene inviato alla funzione di codifica, il modello prima genera una lista di embedding dei token (cioè, ogni parola tokenizzata ha un vettore dedicato che rappresenta il suo significato), poi li media.</p><p>Per vedere come la lunghezza del testo e l'ordine delle parole influenzano la similarità degli embedding, abbiamo generato un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">dataset di 180 frasi sintetiche</a> di varie lunghezze, come 3, 5, 10, 15, 20 e 30 token. Abbiamo anche mescolato casualmente i token per formare una variazione di ogni frase:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ecco alcuni esempi:</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">Lunghezza (token)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">Frase originale</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">Frase mescolata</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Codificheremo il dataset utilizzando il nostro modello <code>jina-embeddings-v3</code> e il modello open-source <code>bge-base-en-v1.5</code>, poi calcoleremo la similarità del coseno tra la frase originale e quella mescolata:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Lunghezza (token)</th>\n<th>Media similarità del coseno</th>\n<th>Deviazione standard nella similarità del coseno</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0,947</td>\n<td>0,053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0,909</td>\n<td>0,052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0,924</td>\n<td>0,031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0,918</td>\n<td>0,019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0,899</td>\n<td>0,021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0,874</td>\n<td>0,025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ora possiamo generare un box plot, che rende più chiara la tendenza nella similarità del coseno:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Distribuzione della Similarità per Lunghezza della Frase per Frasi Mescolate con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> e </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (non affinato)</span></figcaption></figure><p>Come possiamo vedere, c'è una chiara relazione lineare nella similarità media del coseno degli embedding. Più lungo è il testo, più basso è il punteggio medio di similarità del coseno tra le frasi originali e quelle mescolate casualmente. Questo probabilmente accade a causa dello \"spostamento delle parole\", ovvero quanto lontano le parole si sono spostate dalle loro posizioni originali dopo il mescolamento casuale. In un testo più breve, ci sono semplicemente meno \"slot\" in cui un token può essere mescolato quindi non può spostarsi così lontano, mentre un testo più lungo ha un maggior numero di permutazioni possibili e le parole possono spostarsi a una distanza maggiore.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Combinazioni di frasi per numero di parole</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Fermiamo la tabella qui, poiché il numero di combinazioni è il fattoriale del numero di parole. Quando arriviamo a trenta parole, otteniamo 265 nonilioni (2.652528598 E+32) di combinazioni.</div></div><p>Come mostrato nella figura sottostante (Similarità del coseno vs Spostamento medio delle parole), più lungo è il testo, maggiore è lo spostamento delle parole:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Similarità del coseno vs Spostamento medio delle parole con dataset di frasi mescolate che mostra la correlazione tra lo spostamento medio delle parole e la dissimilarità del coseno.</span></figcaption></figure><p>Gli embedding dei token dipendono dal contesto locale, cioè dalle parole più vicine ad essi. In un testo breve, riorganizzare le parole non può modificare molto quel contesto. Tuttavia, per un testo più lungo, una parola potrebbe essere spostata molto lontano dal suo contesto originale e questo può modificare notevolmente il suo embedding di token. Di conseguenza, mescolare le parole in un testo più lungo produce un embedding più distante rispetto a uno più breve. La figura sopra mostra che sia per <code>jina-embeddings-v3</code>, usando il mean pooling, sia per <code>bge-base-en-v1.5</code>, usando il CLS pooling, vale la stessa relazione: mescolare testi più lunghi e spostare le parole più lontano risulta in punteggi di similarità più bassi.</p><h2 id=\"do-bigger-models-solve-the-problem\">I modelli più grandi risolvono il problema?</h2><p>Di solito, quando affrontiamo questo tipo di problema, una tattica comune è semplicemente utilizzare un modello più grande. Ma un modello di embedding testuale più grande può davvero catturare le informazioni sull'ordine delle parole in modo più efficace? Secondo la legge di scala dei modelli di embedding testuale (<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">citata nel nostro post di rilascio di <code>jina-embeddings-v3</code></a>), i modelli più grandi generalmente forniscono prestazioni migliori:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Legge di scala dei modelli di Embedding, che mostra le prestazioni MTEB in relazione al numero di parametri.</span></figcaption></figure><p>Ma un modello più grande può catturare le informazioni sull'ordine delle parole in modo più efficace? Abbiamo testato tre varianti del modello BGE: <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>, <a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a>, e <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>, con dimensioni dei parametri rispettivamente di 33 milioni, 110 milioni e 335 milioni.</p><p>Useremo le stesse 180 frasi di prima, ma ignoreremo le informazioni sulla lunghezza. Codificheremo sia le frasi originali che le loro permutazioni casuali usando le tre varianti del modello e tracceremo la similarità media del coseno:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Impatto della dimensione del modello sulla sensibilità all'ordine delle parole con dataset di frasi mescolate usando </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, e </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Mentre possiamo vedere che i modelli più grandi sono più sensibili alla variazione dell'ordine delle parole, la differenza è piccola. Persino il molto più grande <code>bge-large-en-v1.5</code> è solo leggermente migliore nel distinguere le frasi mescolate da quelle non mescolate. Altri fattori entrano in gioco nel determinare quanto un modello di embedding sia sensibile ai riordinamenti delle parole, in particolare le differenze nel regime di addestramento. Inoltre, la similarità del coseno è uno strumento molto limitato per misurare la capacità di un modello di fare distinzioni. Tuttavia, possiamo vedere che la dimensione del modello non è un fattore principale. Non possiamo semplicemente rendere il nostro modello più grande e risolvere questo problema.</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">Ordine delle parole e scelta delle parole nel mondo reale</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Per gran parte di questo post stiamo usando <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v2</code></a> (<i><em class=\"italic\" style=\"white-space: pre-wrap;\">non</em></i> il nostro modello più recente, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>) poiché <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v2</code> è molto più piccolo e quindi più veloce per sperimentare sulle nostre GPU locali, con 137m di parametri contro i 580m di <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v3</code>.</div></div><p>Come abbiamo menzionato nell'introduzione, l'ordine delle parole non è l'unica sfida per i modelli di embedding. Una sfida più realistica nel mondo reale riguarda la <em>scelta</em> delle parole. Ci sono molti modi per cambiare le parole in una frase — modi che non si riflettono bene negli embedding. Possiamo prendere \"Lei è volata da Parigi a Tokyo\" e modificarla in \"Lei ha guidato da Tokyo a Parigi\", e gli embedding rimangono simili. Abbiamo mappato questo attraverso diverse categorie di alterazione:</p>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categoria</th>\n<th>Esempio - Sinistra</th>\n<th>Esempio - Destra</th>\n<th>Similarità del coseno (<code>jina</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Direzionale</td>\n<td>Lei è volata da Parigi a Tokyo</td>\n<td>Lei ha guidato da Tokyo a Parigi</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>Temporale</td>\n<td>Ha cenato prima di guardare il film</td>\n<td>Ha guardato il film prima di cenare</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>Causale</td>\n<td>L'aumento della temperatura ha sciolto la neve</td>\n<td>La neve che si scioglieva ha raffreddato la temperatura</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>Comparativo</td>\n<td>Il caffè è più buono del tè</td>\n<td>Il tè è più buono del caffè</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>Negazione</td>\n<td>Lui è in piedi vicino al tavolo</td>\n<td>Lui è in piedi lontano dal tavolo</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Nota che questi sono casi comuni che abbiamo osservato durante il nostro lavoro e non rappresentano necessariamente una tassonomia completa delle categorie.</div></div><p>La tabella sopra mostra una lista di \"casi di fallimento\" dove un modello di text embedding non riesce a catturare sottili alterazioni delle parole. Questo è in linea con le nostre aspettative: i modelli di text embedding non hanno la capacità di ragionamento. Per esempio, il modello non comprende la relazione tra \"da\" e \"a\". I modelli di text embedding eseguono un matching semantico, con semantica tipicamente catturata a livello di token e poi compressa in un singolo vettore denso dopo il pooling. Al contrario, <a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">gli LLM (modelli autoregressivi) addestrati su dataset più grandi, a livello di trilioni di token, stanno iniziando a dimostrare capacità emergenti di ragionamento</a>.</p><p>Questo ci ha fatto chiederci: possiamo fare fine-tuning del modello di embedding con apprendimento contrastivo usando triplette per avvicinare la query e il positivo, mentre allontaniamo la query e il negativo?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: L'Effetto dell'Apprendimento Contrastivo: Avvicinare Query e Positivo, Allontanare il Negativo.</span></figcaption></figure><p>Per esempio, \"Volo da Amsterdam a Berlino\" potrebbe essere considerato la coppia negativa di \"Volo da Berlino ad Amsterdam\". In effetti, nel <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\">report tecnico di <code>jina-embeddings-v1</code></a> (Michael Guenther, et al.), abbiamo brevemente affrontato questo problema su piccola scala: abbiamo fatto fine-tuning del modello <code>jina-embeddings-v1</code> su un <a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">dataset di negazione</a> di 10.000 esempi generati da large language model.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>I risultati, riportati nel link del report sopra, sono stati promettenti:</p><blockquote>Osserviamo che per tutte le dimensioni dei modelli, il fine-tuning sui dati delle triplette (che include il nostro dataset di training sulla negazione) migliora drammaticamente le prestazioni, in particolare sul task HardNegation.</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Tabella che mostra i punteggi EasyNegation e HardNegation su diverse dimensioni dei modelli </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> con training sia a coppie che combinato triplette/coppie.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Confronto delle Prestazioni delle Strategie di Training tra diverse versioni di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">Fine-Tuning dei Modelli di Text Embedding con Dataset Curati</h2><p>Nelle sezioni precedenti, abbiamo esplorato diverse osservazioni chiave riguardo i text embedding:</p><ol><li>I testi più brevi sono più soggetti a errori nella cattura dell'ordine delle parole.</li><li>Aumentare la dimensione del modello di text embedding non migliora necessariamente la comprensione dell'ordine delle parole.</li><li>L'apprendimento contrastivo potrebbe offrire una potenziale soluzione a questi problemi.</li></ol><p>Con questo in mente, abbiamo fatto fine-tuning di <code>jina-embeddings-v2-base-en</code> e <code>bge-base-en-1.5</code> sui nostri dataset di negazione e ordine delle parole (circa 11.000 campioni di training in totale):</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Per aiutare a valutare il fine-tuning, abbiamo generato un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">dataset</a> di 1.000 triplette costituite da una <code>query</code>, un caso <code>positive (pos)</code> e un caso <code>negative (neg)</code>:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ecco un esempio di riga:</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Queste triplette sono progettate per coprire vari casi di fallimento, inclusi cambiamenti di significato <strong>direzionali</strong>, <strong>temporali</strong> e <strong>causali</strong> dovuti a cambiamenti nell'ordine delle parole.</p><p>Ora possiamo valutare i modelli su tre diversi set di valutazione:</p><ol><li>Il set di 180 frasi sintetiche (viste prima in questo post), mescolate casualmente.</li><li>Cinque esempi controllati manualmente (dalla tabella direzionale/causale/ecc. sopra).</li><li>94 triplette curate dal nostro <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">dataset di triplette</a> appena generato.</li></ol><p>Ecco la differenza per le frasi mescolate prima e dopo il fine-tuning:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Lunghezza Frase (token)</th>\n<th>Similarità Coseno Media (<code>jina</code>)</th>\n<th>Similarità Coseno Media (<code>jina-ft</code>)</th>\n<th>Similarità Coseno Media (<code>bge</code>)</th>\n<th>Similarità Coseno Media (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Il risultato sembra chiaro: nonostante il processo di fine-tuning richieda solo cinque minuti, osserviamo un miglioramento drammatico nelle prestazioni sul dataset di frasi mescolate casualmente:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Distribuzione della Similarità per Lunghezza della Frase per Frasi Mescolate con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> e </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (fine-tuned).</span></figcaption></figure><p>Osserviamo anche miglioramenti nei casi direzionali, temporali, causali e comparativi. Il modello mostra un sostanziale miglioramento delle prestazioni riflesso da una diminuzione della similarità del coseno media. Il maggior guadagno di prestazioni si registra nel caso della negazione, grazie al nostro dataset di fine-tuning che contiene 10.000 esempi di training sulla negazione.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categoria</th>\n<th>Esempio - Sinistra</th>\n<th>Esempio - Destra</th>\n<th>Similarità Coseno Media (<code>jina</code>)</th>\n<th>Similarità Coseno Media (<code>jina-ft</code>)</th>\n<th>Similarità Coseno Media (<code>bge</code>)</th>\n<th>Similarità Coseno Media (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Direzionale</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>Temporale</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>Causale</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>Comparativo</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>Negazione</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Conclusione</h2><p>In questo post, approfondiamo le sfide che i modelli di text embedding affrontano, in particolare la loro difficoltà nel gestire efficacemente l'ordine delle parole. Per semplificare, abbiamo identificato cinque tipi principali di fallimento: <strong>Direzionale</strong>, <strong>Temporale</strong>, <strong>Causale</strong>, <strong>Comparativo</strong> e <strong>Negazione</strong>. Questi sono i tipi di query in cui l'ordine delle parole è realmente importante, e se il tuo caso d'uso ne coinvolge qualcuno, è importante conoscere i limiti di questi modelli.</p><p>Abbiamo anche condotto un rapido esperimento, espandendo un dataset focalizzato sulla negazione per coprire tutte e cinque le categorie di fallimento. I risultati sono stati promettenti: il fine-tuning con \"hard negatives\" attentamente selezionati ha reso il modello migliore nel riconoscere quali elementi appartengono insieme e quali no. Detto questo, c'è ancora molto lavoro da fare. I prossimi passi includono l'approfondimento di come la dimensione e la qualità del dataset influenzino le prestazioni.</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "I modelli di embedding testuale faticano a catturare sottili sfumature linguistiche come l'ordine delle parole, le relazioni direzionali, le sequenze temporali, le connessioni causali, i confronti e la negazione. Comprendere queste sfide è fondamentale per migliorare le prestazioni del modello.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
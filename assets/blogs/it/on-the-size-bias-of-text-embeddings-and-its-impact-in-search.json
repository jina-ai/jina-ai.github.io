{
  "slug": "on-the-size-bias-of-text-embeddings-and-its-impact-in-search",
  "id": "67e52df15dcba60001c30ebe",
  "uuid": "bae3e1b8-3bf2-4dbc-a553-b26ea64aeb60",
  "title": "Sull'influenza della distorsione dimensionale degli embedding di testo e il suo impatto sulla ricerca",
  "html": "<p>La similarit√† semantica √® ci√≤ che i modelli di embedding sono costruiti per misurare, ma queste misurazioni sono influenzate da molti fattori di bias. In questo articolo, esamineremo una fonte pervasiva di bias nei modelli di embedding del testo: la dimensione dell'input.</p><p><strong>Gli embedding di testi pi√π lunghi mostrano generalmente punteggi di similarit√† pi√π alti quando confrontati con altri embedding di testo, indipendentemente da quanto sia simile il contenuto effettivo.</strong> Mentre i testi veramente simili avranno comunque punteggi di similarit√† pi√π alti rispetto a quelli non correlati, i testi pi√π lunghi introducono un bias‚Äîfacendo apparire i loro embedding pi√π simili in media semplicemente a causa della loro lunghezza.</p><p>Questo ha conseguenze reali. Significa che i modelli di embedding, da soli, non sono in grado di misurare molto bene la rilevanza. Con la ricerca basata su embedding, c'√® sempre una corrispondenza migliore, ma il bias dimensionale significa che non si pu√≤ usare il punteggio di similarit√† per decidere se la corrispondenza migliore, o qualsiasi corrispondenza minore, sia effettivamente rilevante. Non si pu√≤ dire, per esempio, che qualsiasi corrispondenza con un coseno superiore a 0.75 sia rilevante perch√© ci pu√≤ facilmente essere un documento lungo che corrisponde a quel livello nonostante sia completamente irrilevante.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Confrontare i vettori di embedding pu√≤ solo dirci della similarit√† relativa, non della rilevanza.</div></div><p>Dimostreremo questo con alcuni esempi semplici e mostreremo come la similarit√† del coseno tra gli embedding di testo non possa servire come modo generale per valutare</p><h2 id=\"visualizing-size-bias\">Visualizzare il Bias Dimensionale</h2><p>Per mostrare come si manifesta il bias dimensionale, useremo l'ultimo modello di embedding di Jina AI <code>jina-embeddings-v3</code> con l'opzione <code>text-matching</code>. Useremo anche documenti di testo da un dataset IR ampiamente utilizzato: Il <a href=\"https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\">dataset CISI</a>, che puoi <a href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\">scaricare da Kaggle</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">CISI (a dataset for Information Retrieval)</div><div class=\"kg-bookmark-description\">A public dataset from the University of Glasgow's Information Retrieval Group</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-31.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Kaggle</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/dataset-card.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Questo dataset √® utilizzato per addestrare sistemi IR, quindi contiene sia query che documenti da abbinare. Useremo solo i documenti, che si trovano tutti nel file <code>CISI.ALL</code>. Puoi scaricarlo dalla riga di comando da una <a href=\"https://github.com/GianRomani/CISI-project-MLOps\">fonte alternativa su GitHub</a> con il comando:</p><pre><code class=\"language-bash\">wget https://raw.githubusercontent.com/GianRomani/CISI-project-MLOps/refs/heads/main/CISI.ALL\n</code></pre><p>CISI contiene 1.460 documenti. Le statistiche di base sulle dimensioni dei testi e le loro distribuzioni sono riassunte nella tabella e negli istogrammi seguenti:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>in Parole</th>\n<th>in Frasi</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dimensione media documento</td>\n<td>119,2</td>\n<td>4,34</td>\n</tr>\n<tr>\n<td>Deviazione standard</td>\n<td>63,3</td>\n<td>2,7</td>\n</tr>\n<tr>\n<td>Dimensione massima</td>\n<td>550</td>\n<td>38</td>\n</tr>\n<tr>\n<td>Dimensione minima</td>\n<td>8</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Leggiamo i documenti in Python e otteniamo gli embedding per essi. Il codice seguente presuppone che il file <code>CISI.ALL</code> sia nella directory locale:</p><pre><code class=\"language-python\">with open(\"CISI.ALL\", \"r\", encoding=\"utf-8\") as inp:\n    cisi_raw = inp.readlines()\n\ndocs = []\ncurrent_doc = \"\"\nin_text = False\nfor line in cisi_raw:\n    if line.startswith(\".\"):\n        in_text = False\n        if current_doc:\n            docs.append(current_doc.strip())\n            current_doc = \"\"\n        if line.startswith(\".W\"):\n            in_text = True\n    else:\n        if in_text:\n            current_doc += line\n</code></pre><p>Questo riempir√† la lista <code>docs</code> con 1.460 documenti. Puoi ispezionarli:</p><pre><code class=\"language-text\">print(docs[0])\n\nThe present study is a history of the DEWEY Decimal\nClassification.  The first edition of the DDC was published\nin 1876, the eighteenth edition in 1971, and future editions\nwill continue to appear as needed.  In spite of the DDC's\nlong and healthy life, however, its full story has never\nbeen told.  There have been biographies of Dewey\nthat briefly describe his system, but this is the first\nattempt to provide a detailed history of the work that\nmore than any other has spurred the growth of\nlibrarianship in this country and abroad.</code></pre><p>Ora, costruiremo gli embedding per ogni testo usando <code>jina-embeddings-v3</code>. Per questo, avrai bisogno di <a href=\"https://jina.ai/embeddings/#apiform\">una chiave API dal sito web di Jina AI</a>. Puoi ottenere una chiave gratuita per fino a 1 milione di token di embedding, che √® sufficiente per questo articolo.</p><p>Metti la tua chiave in una variabile:</p><pre><code class=\"language-python\">api_key = \"&lt;Your Key&gt;\"\n</code></pre><p>Ora, genera gli embedding usando il task <code>text-matching</code> con <code>jina-embeddings-v3</code>. Questo codice elabora i testi in <code>docs</code> in batch di 10.</p><pre><code class=\"language-python\">import requests\nimport json\nfrom numpy import array\n\nembeddings  = []\n\nurl = \"https://api.jina.ai/v1/embeddings\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\n\ni = 0\nwhile i &lt; len(docs):\n    print(f\"Got {len(embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": docs[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>Per ogni testo, ci sar√† un embedding di 1024 dimensioni nella lista <code>embeddings</code>. Puoi vedere come appare:</p><pre><code class=\"language-python\">print(embeddings[0])\n\narray([ 0.0352382 , -0.00594871,  0.03808545, ..., -0.01147173,\n         -0.01710563,  0.01109511], shape=(1024,))),\n</code></pre><p>Ora, calcoliamo i coseni tra tutte le coppie di embedding. Prima, definiamo la funzione coseno <code>cos_sim</code> usando <code>numpy</code>:</p><pre><code class=\"language-python\">from numpy import dot\nfrom numpy.linalg import norm\n\ndef cos_sim(a, b): \n    return float((a @ b.T) / (norm(a)*norm(b)))\n</code></pre><p>Quindi, calcoliamo i coseni di ciascuno dei 1.460 embedding confrontati con gli altri 1.459:</p><pre><code class=\"language-python\">all_cosines = []\nfor i, emb1 in enumerate(embeddings):\n    for j, emb2 in enumerate(embeddings):\n        if i != j:\n            all_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>Il risultato √® una lista di 2.130.140 valori. La loro distribuzione dovrebbe approssimare i coseni tra documenti \"casuali\" nella stessa lingua e registro. La tabella e l'istogramma seguenti riassumono i risultati.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Numero di testi</th>\n<th>1.460</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Numero di coseni</td>\n<td>2.130.140</td>\n</tr>\n<tr>\n<td>Media</td>\n<td>0,343</td>\n</tr>\n<tr>\n<td>Deviazione standard</td>\n<td>0,116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Questi documenti, anche se non correlati tra loro, hanno tipicamente coseni ben al di sopra dello zero. Potremmo essere tentati di impostare una soglia di 0,459 (media + 1 deviazione standard), o magari arrotondarla a 0,5, e dire che qualsiasi coppia di documenti con un coseno inferiore a quello deve essere in gran parte non correlata.</p><p>Ma facciamo lo stesso esperimento su testi pi√π piccoli. Useremo la libreria <a href=\"https://www.nltk.org/\" rel=\"noreferrer\"><code>nltk</code></a> per dividere ogni documento in frasi:</p><pre><code class=\"language-python\">import nltk\n\nsentences = []\nfor doc in docs:\n    sentences.extend(nltk.sent_tokenize(doc)) \n</code></pre><p>Questo produce 6.331 frasi con una lunghezza media di 27,5 parole e una deviazione standard di 16,6. Nell'istogramma seguente, la distribuzione delle dimensioni delle frasi √® in rosso, e per i documenti completi √® in blu, cos√¨ puoi confrontarle.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"567\" height=\"455\"></figure><p>Useremo lo stesso modello e metodi per ottenere gli embedding per ogni frase:</p><pre><code class=\"language-python\">sentence_embeddings = []\n\ni = 0\nwhile i &lt; len(sentences):\n    print(f\"Got {len(sentence_embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": sentences[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        sentence_embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>E poi calcolare il coseno tra l'embedding di ogni frase con quello di ogni altra frase:</p><pre><code class=\"language-python\">sent_cosines = []\nfor i, emb1 in enumerate(sentence_embeddings):\n    for j, emb2 in enumerate(sentence_embeddings):\n        if i != j:\n            sent_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>Il risultato √® un numero piuttosto elevato di valori coseno: 40.075.230, come riassunto nella tabella seguente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of sentences</th>\n<th>6,331</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>40,075,230</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.254</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>I coseni frase-frase sono considerevolmente pi√π bassi in media rispetto a quelli documento-documento completi. L'istogramma sottostante confronta le loro distribuzioni, e si pu√≤ facilmente vedere che le coppie di frasi formano una distribuzione quasi identica a quella delle coppie di documenti ma spostata verso sinistra.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Per verificare che questa dipendenza dalla dimensione sia robusta, calcoliamo tutti i coseni tra frasi e documenti e aggiungiamoli all'istogramma. Le loro informazioni sono riassunte nella tabella seguente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>6,331 sentences &amp; 1,460 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>9,243,260</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.276</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.119</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La linea verde sottostante √® la distribuzione dei coseni frase-documento. Possiamo vedere che questa distribuzione si colloca ordinatamente tra i coseni documento-documento e i coseni frase-frase, mostrando che l'effetto della dimensione coinvolge <em>entrambi</em> i testi pi√π grandi e pi√π piccoli che vengono confrontati.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Facciamo un'altra prova concatenando i documenti in gruppi di dieci, creando 146 documenti molto pi√π grandi e misurando i loro coseni. Il risultato √® riassunto di seguito:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>146 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>21,170</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.658</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.09</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Questo √® <em>molto</em> pi√π a destra delle altre distribuzioni. Una soglia coseno di 0,5 ci direbbe che quasi tutti questi documenti sono correlati tra loro. Per escludere documenti irrilevanti di questa dimensione, dovremmo impostare la soglia molto pi√π alta, forse fino a 0,9, che indubbiamente escluderebbe buone corrispondenze tra i documenti pi√π piccoli.</p><p>Questo dimostra che non possiamo usare le soglie minime del coseno per stimare quanto sia buona una corrispondenza, almeno non senza tenere conto in qualche modo della dimensione del documento.</p><h2 id=\"what-causes-size-bias\">Cosa Causa il Bias della Dimensione?</h2><p>Il bias della dimensione negli embedding non √® come i <a href=\"https://jina.ai/news/long-context-embedding-models-are-blind-beyond-4k-tokens/\">bias posizionali nei modelli con contesto lungo</a>. Non √® causato dalle architetture. Non riguarda intrinsecamente la dimensione. Se, per esempio, avessimo creato documenti pi√π lunghi semplicemente concatenando copie dello stesso documento pi√π e pi√π volte, non mostrerebbe un bias della dimensione.</p><p>Il problema √® che i testi lunghi dicono pi√π cose. Anche se sono vincolati da un argomento e uno scopo, il punto di scrivere pi√π parole √® dire pi√π cose.</p><p>I testi pi√π lunghi, almeno del tipo che le persone normalmente creano, produrranno naturalmente embedding che si \"distribuiscono\" su pi√π spazio semantico. Se un testo dice pi√π cose, il suo embedding avr√† un angolo pi√π basso con altri vettori in media, indipendentemente dall'argomento del testo.</p><h2 id=\"measuring-relevance\">Misurare la Rilevanza</h2><p>La lezione di questo post √® che non si possono usare i coseni tra vettori semantici <em>da soli</em> per dire se qualcosa √® una buona corrispondenza, ma solo che √® la migliore corrispondenza tra quelle disponibili. Bisogna fare qualcosa oltre al calcolo dei coseni per verificare l'utilit√† e la validit√† delle migliori corrispondenze.</p><p>Si potrebbe provare la normalizzazione. Se si pu√≤ misurare empiricamente il bias della dimensione, potrebbe essere possibile compensarlo. Tuttavia, questo approccio potrebbe non essere molto robusto. Ci√≤ che funziona per un dataset probabilmente non funzioner√† per un altro.</p><p>La <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/\">codifica asimmetrica query-documento</a>, fornita in <code>jina-embeddings-v3</code>, riduce il bias della dimensione nei modelli di embedding ma non lo elimina. Lo scopo della codifica asimmetrica √® codificare i documenti per essere meno \"distribuiti\" e codificare le query per esserlo di pi√π.</p><p>La linea rossa nell'istogramma sottostante √® la distribuzione dei coseni documento-documento usando la codifica asimmetrica con <code>jina-embeddings-v3</code> ‚Äì ogni documento viene codificato usando i flag <code>retrieval.query</code> e <code>retrieval.passage</code>, e ogni embedding di query del documento viene confrontato con ogni embedding di passaggio del documento che non proviene dallo stesso documento. Il coseno medio √® 0,200, con una deviazione standard di 0,124.</p><p>Questi coseni sono considerevolmente pi√π piccoli di quelli che abbiamo trovato sopra per gli stessi documenti usando il flag <code>text-matching</code>, come mostrato nell'istogramma sottostante.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-25.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"591\" height=\"455\"></figure><p>Tuttavia, la codifica asimmetrica non ha eliminato il bias della dimensione. L'istogramma sottostante confronta i coseni per documenti completi e frasi usando la codifica asimmetrica.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>La media per i coseni delle frasi √® 0,124, quindi usando la codifica asimmetrica, la differenza tra il coseno medio delle frasi e il coseno medio dei documenti √® 0,076. La differenza nelle medie per la codifica simmetrica √® 0,089. Il cambiamento nel bias della dimensione √® insignificante.</p><p>Sebbene la codifica asimmetrica migliori gli embedding per il recupero delle informazioni, non √® migliore per misurare la rilevanza delle corrispondenze.</p><h2 id=\"future-possibilities\">Possibilit√† Future</h2><p>L'approccio del reranker, ad esempio <code>jina-reranker-v2-base-multilingual</code> e <code>jina-reranker-m0</code>, √® un modo alternativo di valutare le corrispondenze query-documento che sappiamo gi√† <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/\">migliora la precisione delle query</a>. I punteggi del reranker non sono normalizzati, quindi non funzionano nemmeno come misure di similitudine oggettive. Tuttavia, vengono calcolati in modo diverso, e potrebbe essere possibile normalizzare i punteggi del reranker in modi che li rendano buoni stimatori della rilevanza.</p><p>Un'alternativa √® utilizzare modelli linguistici di grandi dimensioni, preferibilmente con forti capacit√† di ragionamento, per valutare direttamente se un candidato √® una buona corrispondenza per una query. In modo semplicistico, potremmo chiedere a un modello linguistico di grandi dimensioni specifico per il compito: \"Su una scala da 1 a 10, questo documento √® una buona corrispondenza per questa query?\" I modelli esistenti potrebbero non essere ben adatti al compito, ma l'addestramento mirato e tecniche di prompting pi√π sofisticate sono promettenti.</p><p>Non √® impossibile per i modelli misurare la rilevanza, ma richiede un paradigma diverso dai modelli di embedding.</p><h2 id=\"use-your-models-for-what-its-good-for\">Usa i Tuoi Modelli per Ci√≤ in cui Sono Bravi</h2><p>L'effetto del bias della dimensione che abbiamo documentato sopra mostra uno dei limiti fondamentali dei modelli di embedding: sono eccellenti nel confrontare le cose ma inaffidabili nel misurare la rilevanza assoluta. Questa limitazione non √® un difetto nel design‚Äî√® una caratteristica intrinseca di come funzionano questi modelli.</p><p>Quindi cosa significa questo per te?</p><p>Primo, sii scettico sulle soglie dei coseni. Semplicemente non funzionano. Le misure di similarit√† del coseno producono numeri in virgola mobile che sembrano tentantemente oggettivi. Ma solo perch√© qualcosa produce numeri non significa che stia misurando qualcosa oggettivamente.</p><p>Secondo, considera soluzioni ibride. Gli embedding possono efficacemente restringere un grande insieme di elementi a candidati promettenti, dopo di che puoi applicare tecniche pi√π sofisticate (e computazionalmente intensive) come reranker o LLM, o anche valutatori umani per determinare la rilevanza effettiva.</p><p>Terzo, quando progetti sistemi, pensa in termini di compiti piuttosto che di capacit√†. Il modello oggettivamente pi√π intelligente, con i punteggi pi√π alti nei benchmark √® ancora uno spreco di denaro se non pu√≤ fare il lavoro per cui lo hai acquistato.</p><p>Comprendere i limiti dei nostri modelli non √® pessimistico ‚Äì riflette un principio pi√π ampio nelle applicazioni: Comprendere in cosa i tuoi modelli sono bravi, e in cosa non lo sono, √® fondamentale per costruire sistemi affidabili ed efficaci. Proprio come non useremmo un martello per stringere una vite, non dovremmo usare modelli di embedding per compiti che non sono in grado di gestire. Rispetta ci√≤ in cui i tuoi strumenti sono bravi.</p>",
  "comment_id": "67e52df15dcba60001c30ebe",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading---2025-04-16T094756.687.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-27T11:52:33.000+01:00",
  "updated_at": "2025-04-16T03:48:15.000+02:00",
  "published_at": "2025-04-16T03:40:03.000+02:00",
  "custom_excerpt": "Size bias refers to how the length of text inputs affects similarity, regardless of semantic relevance. It explains why search systems sometimes return long, barely-relevant documents instead of shorter, more precise matches to your query.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/on-the-size-bias-of-text-embeddings-and-its-impact-in-search/",
  "excerpt": "Il bias della dimensione si riferisce a come la lunghezza degli input testuali influenza la similarit√†, indipendentemente dalla rilevanza semantica. Questo spiega perch√© i sistemi di ricerca a volte restituiscono documenti lunghi e poco pertinenti invece di corrispondenze pi√π brevi e precise alla query.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
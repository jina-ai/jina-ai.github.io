{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "Punteggio equo per documenti multimodali con jina-reranker-m0",
  "html": "<p>Immagina di star creando un sistema di ricerca di notizie sportive. Un utente cerca \"tennisti che festeggiano la vittoria del campionato\" e tu devi trovare gli articoli più rilevanti dal tuo database. Ogni articolo contiene sia una didascalia testuale che un'immagine, tipico della moderna copertura sportiva.</p><p>Il tuo sistema deve prendere una <strong>query testuale</strong> e restituire un <strong>elenco classificato dei documenti multimodali più rilevanti</strong> dal tuo corpus. Sembra semplice, ma c'è un problema fondamentale che manda a monte tutti gli approcci ovvi.</p><p>Ecco cosa succede quando provi a classificare questi documenti. Il tuo modello di 向量模型 (Embedding), ad esempio <code>jina-clip-v2</code>, produce punteggi di similarità come questo:</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>Articolo</th>\n            <th>Tipo di contenuto</th>\n            <th>Descrizione</th>\n            <th>Punteggio di similarità</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>Testo</td>\n            <td>Novak Djokovic vince la finale degli Australian Open in tre set</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>Immagine</td>\n            <td>[foto di un giocatore che tiene in mano il trofeo e sorride]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Testo</td>\n            <td>I ritardi meteorologici influiscono sulla programmazione dei tornei all'aperto</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Immagine</td>\n            <td>[foto di tennisti che saltano e festeggiano]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Quale articolo è più rilevante? L'articolo A ha un punteggio di testo alto ma un punteggio di immagine basso. L'articolo B ha un punteggio di testo basso ma un punteggio di immagine più alto. La sfida fondamentale è che <strong>non puoi confrontare 0.72 (testo) con 0.54 (immagine)</strong> perché questi punteggi di similarità esistono su scale completamente diverse.</p><h2 id=\"when-trivial-solutions-fail\">Quando le soluzioni banali falliscono</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>A causa del divario di modalità</strong> in <code>jina-clip-v2</code> o in quasi tutti gli altri modelli simili a CLIP, qualsiasi approccio ovvio che potresti provare non funziona. Se usi solo il punteggio più alto, ti imbatti nel fatto che i punteggi di testo si raggruppano intorno a 0.2-0.8 mentre i punteggi di immagine si raggruppano intorno a 0.4-0.6. Ciò significa che una corrispondenza di testo mediocre (0.6) batterà sempre un'eccellente corrispondenza di immagine (0.5).</p><p>Nemmeno fare la media dei punteggi aiuta. Calcolare (0.7 + 0.3)/2 = 0.5 ti dà un numero, ma cosa significa effettivamente? Stai facendo la media di quantità fondamentalmente prive di significato. Allo stesso modo, qualsiasi schema di ponderazione fisso è arbitrario: a volte il testo conta di più, a volte le immagini, e questo dipende interamente dalla query e dal documento specifici.</p><p>Anche normalizzare prima i punteggi non risolve il problema principale. Stai ancora cercando di combinare misure di similarità fondamentalmente diverse che catturano diversi aspetti della rilevanza.</p><h2 id=\"what-actually-happens\">Cosa succede realmente</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Per avere un'idea migliore di ciò con cui stiamo lavorando, ecco un esempio di documento dal <a href=\"https://arxiv.org/abs/2305.13631\">dataset EDIS</a>, che mostra l'immagine (una partita di calcio tedesca) e la didascalia (<code>One More Field Where the Content Trails Germany</code>).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1: Esempio di documento multimodale contenente sia contenuto di immagine che di testo. Poiché abbiamo due modalità, per qualsiasi query data ora ci sono </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">due</em></i><span style=\"white-space: pre-wrap;\"> divari semantici (tra la query e il testo, e la query e l'immagine). Per ottenere i migliori risultati, dovremmo cercare il contenuto testuale dei documenti o il contenuto dell'immagine?</span></figcaption></figure><p>Nel complesso, <code>jina-clip-v2</code> mostra similarità molto più elevate quando si confronta query-testo rispetto a query-immagine nel dataset EDIS, in parte a causa del modo in cui il modello è stato addestrato e in parte a causa del dataset stesso:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2: Punteggi di similarità tra query-immagine (in rosso) e query-testo (in blu) utilizzando </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Pertanto, sembra logico recuperare un documento in base al suo testo piuttosto che alla sua immagine. E, come possiamo vedere nel grafico qui sotto, otteniamo risultati molto migliori confrontando la query di testo <code>... for undocumented immigrants helping to establish legal status in the United States</code> con il contenuto testuale del corpus. Infatti, la ricerca per immagine non riesce affatto a recuperare il documento ground truth (evidenziato in giallo):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3: Esempio in cui il documento ground-truth (evidenziato con bordo giallo) può essere recuperato solo tramite il recupero query-testo di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> quando si utilizza </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> di 3.</span></figcaption></figure><p>Ma non fatevi ingannare. Nonostante la query-testo mostri punteggi di similarità più alti, i punteggi di similarità query-testo e query-immagine <em>non</em> sono comparabili. Possiamo vederlo guardando recall@10 quando usiamo <code>jina-clip-v2</code> per recuperare 32 documenti dal dataset EDIS. Chiaramente, il recall è più alto con query-<em>immagine</em>:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Query-to-text</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Query-to-image</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Possiamo vederlo qui sotto: se usiamo una query dal dataset, <code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>, possiamo recuperare il documento ground truth solo tramite il suo contenuto di immagine. La ricerca tramite il suo contenuto testuale non restituisce alcuna corrispondenza:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4: Esempio in cui il documento ground-truth (evidenziato con bordo giallo) può essere recuperato solo tramite il recupero query-immagine di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> quando si utilizza </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> di 3.</span></figcaption></figure><p>Quindi, se i punteggi di similarità implicano che dovremmo recuperare i documenti dal loro testo e il richiamo implica che dovremmo recuperarli dalle loro immagini, cosa dovremmo scegliere? Certamente, le Figure 3 e 4 non suggeriscono un vincitore assoluto. Quale modalità presenta <em>realmente</em> la corrispondenza più stretta tra la nostra query e il documento che stiamo cercando? E se volessimo unire i candidati dal recupero query-to-text e query-to-image, come possiamo selezionare in modo significativo le migliori corrispondenze se non possiamo nemmeno confrontare i punteggi? Chiaramente, usare solo <code>jina-clip-v2</code> non sarà sufficiente. Dobbiamo aggiungere un altro modello al mix.</p><h2 id=\"a-simple-two-stage-pipeline\">Una Semplice Pipeline a Due Stadi</h2><p>Ad aprile 2025 abbiamo rilasciato <code>jina-reranker-m0</code>, un重排器 (Reranker) multilingue multimodale per il recupero di documenti visivi. Possiamo vedere il suo divario di modalità più stretto di seguito, dove <code>jina-reranker-m0</code> mostra punteggi di similarità query-to-text e query-to-image comparabili, in contrasto con il divario molto più ampio mostrato da <code>jina-clip-v2</code>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Rispetto a </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> mostra molta meno differenza tra i punteggi di similarità query-to-image (rosso) e query-to-text (blu).</span></figcaption></figure><p>Con questo in mente, possiamo usare <code>jina-reranker-m0</code> per un secondo passaggio nella catena di recupero, dopo che i risultati iniziali sono stati recuperati da <code>jina-clip-v2</code>:</p><p><strong>Fase 1: Recupera i candidati da entrambe le modalità</strong></p><ul><li>Usa <code>jina-clip-v2</code> per ottenere 16 documenti tramite ricerca di testo + 16 tramite ricerca di immagini</li><li>Accetta che non possiamo ancora confrontare i punteggi</li></ul><p><strong>Fase 2: Riordinamento unificato</strong></p><ul><li>Invia ogni coppia (query + documento completo) a <code>jina-reranker-m0</code></li><li>Il 重排器 (Reranker) elabora sia il testo CHE l'immagine insieme</li><li>Output: singolo punteggio di rilevanza su una scala unificata</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Indicizzazione di documenti multimodali e un processo di recupero multimodale a due stadi con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> e </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Abbiamo ampliato gli esperimenti dalla Tabella 1, ora usando <code>jina-clip-v2</code> per recuperare documenti dal corpus, quindi <code>jina-reranker-m0</code> per riordinarli:</p><ol><li>Recupera 32 documenti tramite query-to-text, quindi riordina in base al punteggio query-to-text.</li><li>Recupera 32 documenti tramite query-to-image, quindi riordina in base al punteggio query-to-image.</li><li>Recupera 16 documenti tramite query-to-text e 16 tramite query-to-image. Riordina in base al punteggio query-to-text o query-to-image, a seconda della modalità di query.</li><li>Recupera 16 documenti tramite query-to-text e 16 tramite query-to-image. Riordina in base ai punteggi medi query-to-text e query-to-image di ciascun documento, dando un punteggio finale di (query-to-text + query-to-image)/2.</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Si noti che stiamo misurando le prestazioni zero-shot su EDIS. Non abbiamo messo a punto né <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> né <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-reranker-m0</code> usando il set di dati.</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Description</th>\n      <th>Recall@10 - with jina-clip-v2</th>\n      <th>Recall@10 - with jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 docs: query-to-text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 docs: query-to-image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image<br>Combined average reranker scores</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Gli esperimenti 1, 3 e 4 mostrano tutti lo stesso risultato per recall@10 con <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> a causa dei punteggi query-to-text più alti dei punteggi query-to-image. Pertanto, i primi dieci risultati sono dominati dai documenti recuperati tramite testo.</div></div><p>Come possiamo vedere, eseguendo un secondo passaggio con <code>jina-reranker-m0</code>, il richiamo aumenta su tutta la linea, indipendentemente dalla modalità. Tuttavia, <strong>vediamo il maggiore aumento quando combiniamo sia il contenuto testuale che quello delle immagini dai documenti recuperati</strong>, raggiungendo un recall@10 di 36.24. Un esempio visivo mostra che <code>jina-reranker-m0</code> classifica costantemente il documento ground truth al primo posto, sia che si cerchi contenuto testuale che di immagini:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Query di esempio (sulla sinistra) e </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> di 1 risultato per ogni metodologia di riordinamento (quattro colonne a destra), che mostra che la combinazione di punteggi di similarità di immagini e testo classifica costantemente il documento ground truth al primo posto.</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Mentre le Figure 3 e 4 mostrano un <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> di 3 per i diversi metodi di recupero, per motivi di spazio la Figura 7 mostra solo <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> di 1 per ogni query.</div></div><h2 id=\"conclusions\">Conclusioni</h2><p>Questo semplice approccio a due stadi offre un miglioramento del 62% nel richiamo perché il sistema finalmente sfrutta ciò che gli umani fanno naturalmente: considerare sia ciò che leggiamo che ciò che vediamo per determinare la rilevanza. La lezione si estende oltre la ricerca: quando si ha a che fare con sistemi di intelligenza artificiale multimodale, gli approcci a passaggio singolo che trattano le modalità separatamente colpiranno sempre questo muro di incompatibilità di punteggio. Le architetture a due stadi che recuperano ampiamente e poi classificano in modo intelligente stanno diventando essenziali. Prova <code>jina-reranker-m0</code> tramite la nostra API o su AWS, GCP e Azure.</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "Similarità del testo: 0.7. Similarità dell'immagine: 0.5. Quale documento è più rilevante? Letteralmente, non si riesce a capirlo—e questo è il problema principale che sta mandando in tilt la ricerca multimodale. Lo risolviamo con il riordinamento unificato (unified reranking).",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
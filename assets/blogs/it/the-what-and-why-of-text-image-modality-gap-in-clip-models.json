{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "Il \"What\" e il \"Why\" del divario di modalit√† testo-immagine nei modelli CLIP",
  "html": "<p>Gli <a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">embedding semantici</a> sono il nucleo dei modelli di AI moderni, inclusi chatbot e modelli di arte AI. A volte sono nascosti agli utenti, ma sono sempre presenti, in agguato appena sotto la superficie.</p><p>La teoria degli embedding ha solo due parti:</p><ol><li>Le cose ‚Äî oggetti esterni a un modello AI, come testi e immagini ‚Äî sono rappresentate da vettori creati dai modelli AI a partire dai dati relativi a quelle cose.</li><li>Le relazioni tra le cose esterne a un modello AI sono rappresentate da relazioni spaziali tra quei vettori. Addestriamo i modelli AI specificamente per creare vettori che funzionino in questo modo.</li></ol><p>Quando creiamo un modello multimodale immagine-testo, addestriamo il modello in modo che gli embedding delle immagini e gli embedding dei testi che descrivono o sono correlati a quelle immagini siano relativamente vicini tra loro. Le similitudini semantiche tra le cose che quei due vettori rappresentano ‚Äî un'immagine e un testo ‚Äî si riflettono nella relazione spaziale tra i due vettori.</p><p>Per esempio, potremmo ragionevolmente aspettarci che i vettori di embedding per un'immagine di un'arancia e il testo \"un'arancia fresca\" siano pi√π vicini tra loro rispetto alla stessa immagine e al testo \"una mela fresca\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration on a black background showing an orange and an apple with arrows between them and quotes reading &quot;A fresh orange\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo √® lo scopo di un modello di embedding: generare rappresentazioni dove le caratteristiche che ci interessano ‚Äî come il tipo di frutta raffigurata in un'immagine o nominata in un testo ‚Äî sono preservate nella distanza tra di loro.</p><p>Ma la multimodalit√† introduce qualcos'altro. Potremmo scoprire che un'immagine di un'arancia √® pi√π vicina a un'immagine di una mela di quanto non lo sia al testo \"un'arancia fresca\", e che il testo \"una mela fresca\" √® pi√π vicino a un altro testo che a un'immagine di una mela.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Black background featuring an apple on the left and an orange on the right with annotated arrows marked &quot;A fresh apple.&quot; and \" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Si scopre che questo √® esattamente ci√≤ che accade con i modelli multimodali, incluso il modello <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP</a> di Jina AI (<code>jina-clip-v1</code>).</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>Per verificare questo, abbiamo campionato 1.000 coppie testo-immagine dal <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">dataset di test Flickr8k</a>. Ogni coppia contiene cinque testi di didascalia (quindi tecnicamente non una coppia) e una singola immagine, con tutti e cinque i testi che descrivono la stessa immagine.</p><p>Per esempio, la seguente immagine (<code>1245022983_fb329886dd.jpg</code> nel dataset Flickr8k):</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"A young girl in a pink skirt playing with a frisbee in an urban outdoor setting with cars and bikes present.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>Le sue cinque didascalie:</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>Abbiamo utilizzato Jina CLIP per incorporare le immagini e i testi e poi:</p><ol><li>Confrontare le similarit√† del coseno degli embedding delle immagini con gli embedding dei loro testi di didascalia.</li><li>Prendere gli embedding di tutti e cinque i testi di didascalia che descrivono la stessa immagine e confrontare le loro similarit√† del coseno tra loro.</li></ol><p>Il risultato √® un divario sorprendentemente ampio, visibile nella Figura 1:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graph with two curves showing the distribution of Cosine Similarity for Image2Text and Text2Text pairs with labeled axes.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Distribuzione dei valori di similarit√† del coseno tra coppie immagine-testo corrispondenti e coppie testo-testo in Jina CLIP.</span></figcaption></figure><p>Con poche eccezioni, le coppie di testi corrispondenti sono molto pi√π vicine tra loro rispetto alle coppie immagine-testo corrispondenti. Questo indica fortemente che Jina CLIP sta codificando i testi in una parte dello spazio di embedding e le immagini in una parte largamente disgiunta relativamente lontana da essa. Questo spazio tra i testi e le immagini √® il <em>gap multimodale</em>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagram on black background depicting 'Images' on left, 'Texts' on bottom, with labeled 'Multimodal Gap' in the center.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>I modelli di embedding multimodali stanno codificando pi√π delle informazioni semantiche che ci interessano: stanno codificando il mezzo del loro input. Secondo Jina CLIP, un'immagine non vale, come dice il detto, mille parole. Ha un contenuto che nessuna quantit√† di parole pu√≤ mai veramente eguagliare. Codifica il mezzo dell'input nella semantica dei suoi embedding senza che nessuno l'abbia mai addestrato a farlo.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Finch√© confrontiamo solo immagini con testi e viceversa, questo non √® un problema, ma un modello veramente multimodale dovrebbe essere in grado di dirci che, per esempio, il testo \"questa √® una mela\" √® una corrispondenza migliore con un'immagine di una mela che con un testo sulle arance. I modelli in stile CLIP nella loro forma attuale non possono farlo.</div></div><p>Questo fenomeno √® stato studiato nell'articolo <em>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>] che lo definisce come il \"gap modale\". Il gap modale √® la separazione spaziale, nello spazio di embedding, tra input in un mezzo e input in un altro. Sebbene i modelli non siano intenzionalmente addestrati ad avere tale gap, questi sono pervasivi nei modelli multimodali.</p><p>Le nostre indagini sul gap modale in Jina CLIP si basano fortemente su <a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">Da Dove Viene il Gap Modale?</h2><p>Liang et al. [2022] identificano tre principali fonti dietro il gap modale:</p><ul><li>Un bias di inizializzazione che chiamano \"effetto cono\".</li><li>Riduzioni della temperatura (casualit√†) durante l'addestramento che rendono molto difficile \"disimparare\" questo bias.</li><li>Procedure di apprendimento contrastivo, ampiamente utilizzate nei modelli multimodali, che rafforzano involontariamente il gap.</li></ul><p>Esamineremo ciascuno di questi aspetti.</p><h3 id=\"cone-effect\">Effetto Cono</h3><p>Un modello costruito con un'architettura CLIP o simile a CLIP √® in realt√† composto da due modelli di embedding separati collegati tra loro. Per i modelli multimodali immagine-testo, questo significa avere un modello per la codifica dei testi e uno completamente separato per la codifica delle immagini, come nello schema seguente.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questi due modelli vengono addestrati in modo che un embedding di un'immagine e un embedding di un testo siano relativamente vicini quando il testo descrive bene l'immagine.</p><p>√à possibile addestrare un modello come questo randomizzando i pesi in entrambi i modelli, presentando coppie di immagini e testi insieme e addestrandolo da zero per minimizzare la distanza tra i due output. Il <a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">modello CLIP originale di OpenAI</a> √® stato addestrato in questo modo. Tuttavia, questo richiede molte coppie immagine-testo e un addestramento computazionalmente costoso. Per il primo modello CLIP, OpenAI ha raccolto 400 milioni di coppie immagine-testo da materiali con didascalie presenti su Internet.</p><p>I modelli pi√π recenti in stile CLIP utilizzano componenti pre-addestrati<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">.</a> Questo significa addestrare separatamente ogni componente come un buon modello di embedding unimodale, uno per i testi e uno per le immagini. Questi due modelli vengono poi ulteriormente addestrati insieme usando coppie immagine-testo, un processo chiamato <em>contrastive tuning</em>. Le coppie allineate di immagini e testi vengono utilizzate per \"spingere\" gradualmente i pesi a rendere gli embedding di testo e immagine corrispondenti pi√π vicini tra loro, e quelli non corrispondenti pi√π distanti.</p><p>Questo approccio generalmente richiede meno dati di coppie immagine-testo, che sono difficili e costosi da ottenere, e grandi quantit√† di testi e immagini senza didascalie, che sono molto pi√π facili da ottenere. Jina CLIP (<code>jina-clip-v1</code>) √® stato addestrato usando quest'ultimo metodo. Abbiamo pre-addestrato un modello <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2</a> per la codifica del testo usando dati testuali generali e utilizzato un <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">codificatore di immagini EVA-02</a> pre-addestrato, per poi addestrarli ulteriormente usando diverse tecniche di training contrastivo, come descritto in <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Posizioni iniziali degli embedding di immagini e testi prima dell'addestramento delle coppie in Jina CLIP, proiettate in due dimensioni.</span></figcaption></figure><p>Se prendiamo questi due modelli pre-addestrati e osserviamo il loro output, prima di addestrarli con coppie immagine-testo, notiamo qualcosa di importante. La Figura 2 (sopra) √® una proiezione <a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">UMAP</a> in due dimensioni degli embedding di immagini prodotti dal codificatore EVA-02 pre-addestrato e degli embedding di testo prodotti da JinaBERT v2 pre-addestrato, con le linee grigie che indicano le coppie immagine-testo corrispondenti. Questo √® prima di qualsiasi addestramento cross-modale.</p><p>Il risultato √® una sorta di \"cono\" troncato, con gli embedding di immagini da un lato e gli embedding di testo dall'altro. Questa forma conica √® difficilmente traducibile in proiezioni bidimensionali ma si pu√≤ vedere in generale nell'immagine sopra. Tutti i testi si raggruppano in una parte dello spazio di embedding, e tutte le immagini in un'altra parte. Se, dopo l'addestramento, i testi sono ancora pi√π simili ad altri testi che alle immagini corrispondenti, questo stato iniziale √® una grande ragione del perch√©. L'obiettivo di abbinare al meglio immagini a testi, testi a testi e immagini a immagini √® completamente compatibile con questa forma conica.</p><p>Il modello √® pregiudicato dalla nascita e ci√≤ che impara non cambia questo fatto. La Figura 3 (sotto) √® la stessa analisi del modello Jina CLIP come rilasciato, dopo il completo addestramento usando coppie immagine-testo. Se possibile, il divario multimodale √® ancora pi√π pronunciato.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Posizioni degli embedding di immagini e testi dopo l'addestramento delle coppie in Jina CLIP, proiettate in due dimensioni.</span></figcaption></figure><p>Anche dopo un estensivo addestramento, Jina CLIP continua a codificare il medium come parte del messaggio.</p><p>Usare l'approccio pi√π costoso di OpenAI, con una pura inizializzazione casuale, non elimina questo bias. Abbiamo preso l'architettura originale di OpenAI CLIP e randomizzato completamente tutti i pesi, poi fatto la stessa analisi di sopra. Il risultato √® ancora una forma di cono troncato, come si vede nella Figura 4:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Posizioni iniziali degli embedding di immagini e testi in Jina CLIP con pesi completamente randomizzati e nessun addestramento, proiettate in due dimensioni.</span></figcaption></figure><p>Questo bias √® un problema strutturale e potrebbe non avere alcuna soluzione. In tal caso, possiamo solo cercare modi per correggerlo o mitigarlo durante l'addestramento.</p><h3 id=\"training-temperature\">Temperatura di Addestramento</h3><p>Durante l'addestramento dei modelli AI, tipicamente aggiungiamo una componente casuale al processo. Calcoliamo quanto un batch di campioni di addestramento dovrebbe modificare i pesi nel modello, poi aggiungiamo un piccolo fattore casuale a queste modifiche prima di applicarle effettivamente. Chiamiamo la quantit√† di casualit√† <em>temperatura</em>, per analogia con il modo in cui usiamo la casualit√† in termodinamica.</p><p>Temperature alte creano grandi cambiamenti nei modelli molto velocemente, mentre temperature basse riducono la quantit√† di cambiamento che un modello pu√≤ fare ogni volta che vede alcuni dati di addestramento. Il risultato √® che con temperature alte, possiamo aspettarci che i singoli embedding si muovano molto nello spazio di embedding durante l'addestramento, e con temperature basse, si muoveranno molto pi√π lentamente.</p><p>La migliore pratica per l'addestramento dei modelli AI √® iniziare con una temperatura alta e poi ridurla progressivamente. Questo aiuta il modello a fare grandi salti nell'apprendimento all'inizio quando i pesi sono casuali o lontani da dove devono essere e poi gli permette di apprendere i dettagli in modo pi√π stabile.</p><p>L'addestramento delle coppie immagine-testo di Jina CLIP inizia con una temperatura di 0,07 (questa √® una temperatura relativamente alta) e la abbassa esponenzialmente nel corso dell'addestramento fino a 0,01, come mostrato nella Figura 5 sotto, un grafico della temperatura rispetto ai passi di addestramento:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Decadimento della temperatura durante l'addestramento delle coppie in Jina CLIP.</span></figcaption></figure><p>Volevamo sapere se aumentare la temperatura ‚Äî aggiungendo casualit√† ‚Äî avrebbe ridotto l'effetto cono e avvicinato complessivamente gli embedding di immagini e testi. Quindi abbiamo riaddestrato Jina CLIP con una temperatura fissa di 0,1 (un valore molto alto). Dopo ogni epoca di addestramento, abbiamo controllato la distribuzione delle distanze tra coppie immagine-testo e coppie testo-testo, proprio come nella Figura 1. I risultati sono mostrati sotto nella Figura 6:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Il divario tra le modalit√† si riduce nel tempo quando la temperatura di training √® alta.</span></figcaption></figure><p>Come si pu√≤ vedere, mantenere una temperatura alta riduce drasticamente il divario multimodale. Permettere agli embedding di muoversi molto durante l'addestramento contribuisce notevolmente a superare il bias iniziale nella distribuzione degli embedding.</p><p>Tuttavia, questo ha un costo. Abbiamo anche testato le prestazioni del modello utilizzando sei diversi test di recupero: Tre test di recupero testo-testo e tre di recupero testo-immagine, dai dataset <a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a> e <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a>. In tutti i test, osserviamo un crollo delle prestazioni all'inizio dell'addestramento seguito da una ripresa molto lenta, come si pu√≤ vedere nella Figura 7:</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Prestazioni durante l'addestramento. All'inizio, si verifica un brusco calo rispetto allo stato iniziale e solo una ripresa molto lenta.</span></figcaption></figure><p>Sarebbe probabilmente estremamente dispendioso in termini di tempo e costoso addestrare un modello come Jina CLIP utilizzando questa temperatura costantemente alta. Sebbene teoricamente fattibile, non √® una soluzione pratica.</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">Apprendimento Contrastivo e il Problema dei Falsi Negativi</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> hanno anche scoperto che le pratiche standard di apprendimento contrastivo ‚Äî il meccanismo che usiamo per addestrare i modelli multimodali stile CLIP ‚Äî tendono a rinforzare il divario multimodale.</p><p>L'apprendimento contrastivo √® fondamentalmente un concetto semplice. Abbiamo un embedding di immagine e un embedding di testo e sappiamo che dovrebbero essere pi√π vicini tra loro, quindi modifichiamo i pesi nel modello durante l'addestramento per ottenere questo risultato. Procediamo lentamente, modificando i pesi di una piccola quantit√†, e li modifichiamo in proporzione a quanto i due embedding sono distanti: Pi√π sono vicini, minore sar√† il cambiamento rispetto a quando sono pi√π lontani.</p><p>Questa tecnica funziona molto meglio se non ci limitiamo ad avvicinare gli embedding quando corrispondono, ma li allontaniamo anche quando non corrispondono. Vogliamo avere non solo coppie immagine-testo che appartengono insieme, ma anche coppie che sappiamo non appartengono insieme.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo pone alcuni problemi:</p><ol><li>Le nostre fonti di dati consistono interamente in coppie corrispondenti. Nessuno creerebbe un database di testi e immagini che un umano ha verificato essere non correlati, n√© si potrebbe facilmente costruirne uno estraendo dati dal web o con qualche altra tecnica non supervisionata o semi-supervisionata.</li><li>Anche le coppie immagine-testo che superficialmente sembrano completamente disgiunte non lo sono necessariamente. Non abbiamo una teoria della semantica che ci permetta di fare oggettivamente tali giudizi negativi. Per esempio, l'immagine di un gatto sdraiato su un portico non √® una corrispondenza completamente negativa per il testo \"un uomo che dorme su un divano\". Entrambi coinvolgono lo sdraiarsi su qualcosa.</li></ol><p>Idealmente, vorremmo addestrare con coppie immagine-testo che sappiamo con certezza essere correlate <em>e non correlate</em>, ma non c'√® un modo ovvio per ottenere coppie sicuramente non correlate. √à possibile chiedere alle persone \"Questa frase descrive questa immagine?\" e aspettarsi risposte coerenti. √à molto pi√π difficile ottenere risposte coerenti chiedendo \"Questa frase non ha nulla a che fare con questa immagine?\"</p><p>Invece, otteniamo coppie immagine-testo non correlate selezionando casualmente immagini e testi dai nostri dati di addestramento, aspettandoci che saranno praticamente sempre corrispondenze errate. In pratica, questo funziona dividendo i nostri dati di addestramento in batch. Per addestrare Jina CLIP, abbiamo utilizzato batch contenenti 32.000 coppie immagine-testo corrispondenti, ma per questo esperimento, le dimensioni dei batch erano solo 16.</p><p>La tabella sottostante mostra 16 coppie immagine-testo campionate casualmente da Flickr8k:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Per ottenere coppie non corrispondenti, combiniamo ogni immagine del batch con ogni testo <em>tranne quello con cui corrisponde</em>. Per esempio, la seguente coppia √® un'immagine e un testo non corrispondenti:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Didascalia:</strong> Una bambina in rosa raccoglie fiori.</p><p>Ma questa procedura assume che tutti i testi che corrispondono ad altre immagini siano ugualmente cattive corrispondenze. Questo non √® sempre vero. Per esempio:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Didascalia:</strong> Il cane siede vicino a un cumulo di neve.</p><p>Sebbene il testo non descriva questa immagine, hanno in comune un cane. Trattare questa coppia come non corrispondente tender√† ad allontanare la parola \"cane\" da qualsiasi immagine di un cane.</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> mostrano che queste coppie non corrispondenti imperfette spingono tutte le immagini e i testi ad allontanarsi tra loro.</p><p>Ci siamo proposti di verificare la loro affermazione con un modello di immagini <code>vit-b-32</code> completamente inizializzato casualmente e un modello di testo JinaBERT v2 similmente randomizzato, con la temperatura di addestramento impostata a una costante di 0.02 (una temperatura moderatamente bassa). Abbiamo costruito due set di dati di addestramento:</p><ul><li>Uno con batch casuali estratti da Flickr8k, con coppie non corrispondenti costruite come descritto sopra.</li><li>Uno dove i batch sono intenzionalmente costruiti con copie multiple della stessa immagine con testi diversi in ogni batch. Questo garantisce che un numero significativo di coppie \"non corrispondenti\" siano in realt√† corrispondenze corrette tra loro.</li></ul><p>Abbiamo poi addestrato due modelli per un'epoca, uno con ciascun set di dati di addestramento, e misurato la distanza coseno media tra 1.000 coppie testo-immagine nel dataset Flickr8k per ciascun modello. Il modello addestrato con batch casuali aveva una distanza coseno media di 0.7521, mentre quello addestrato con molte coppie \"non corrispondenti\" intenzionalmente corrispondenti aveva una distanza coseno media di 0.7840. L'effetto delle coppie \"non corrispondenti\" errate √® piuttosto significativo. Considerando che l'addestramento reale del modello √® molto pi√π lungo e utilizza molti pi√π dati, possiamo vedere come questo effetto crescerebbe e aumenterebbe il divario tra immagini e testi nel loro complesso.</p><h2 id=\"the-medium-is-the-message\">Il Medium √® il Messaggio</h2><p>Il teorico delle comunicazioni canadese <a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Marshall McLuhan</a> ha coniato la frase \"Il medium √® il messaggio\" nel suo libro del 1964 <a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\"><em>Understanding Media: The Extensions of Man</em></a> per enfatizzare che i messaggi non sono autonomi. Ci arrivano in un contesto che ne influenza fortemente il significato, e sostenne famosamente che una delle parti pi√π importanti di quel contesto √® la natura del medium di comunicazione.</p><p>Il divario della multimodalit√† ci offre un'opportunit√† unica per studiare una classe di fenomeni semantici emergenti nei modelli di AI. Nessuno ha detto a Jina CLIP di codificare il medium dei dati su cui √® stato addestrato ‚Äî lo ha fatto comunque. Anche se non abbiamo risolto il problema per i modelli multimodali, abbiamo almeno una buona comprensione teorica dell'origine del problema.</p><p>Dovremmo presumere che i nostri modelli stiano codificando altre cose che non abbiamo ancora cercato a causa dello stesso tipo di bias. Per esempio, probabilmente abbiamo lo stesso problema nei modelli di embedding multilingue. L'addestramento congiunto su due o pi√π lingue porta probabilmente allo stesso divario tra le lingue, specialmente dato che vengono ampiamente utilizzati metodi di training simili. Le soluzioni al problema del divario potrebbero avere implicazioni molto ampie.</p><p>Anche un'indagine sul bias di inizializzazione in una gamma pi√π ampia di modelli porter√† probabilmente a nuove intuizioni. Se il medium √® il messaggio per un modello di embedding, chi sa cos'altro viene codificato nei nostri modelli senza che ne siamo consapevoli?</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "Non puoi semplicemente utilizzare un modello CLIP per recuperare testi e immagini e ordinare i risultati per punteggio. Perch√©? Per via del divario tra modalit√†. Cos'√® e da dove deriva?",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
{
  "slug": "a-practical-guide-to-deploying-search-foundation-models-in-production",
  "id": "679b56ba42b46600019a86e3",
  "uuid": "458c0de5-aedb-4513-8ffd-47c027d204ad",
  "title": "Una Guida Pratica per il Deployment dei Modelli Foundation di Ricerca in Produzione",
  "html": "<p>In Jina AI, la nostra missione √® fornire agli utenti aziendali soluzioni di ricerca di alta qualit√†. Per raggiungere questo obiettivo, rendiamo i nostri modelli accessibili attraverso vari canali. Tuttavia, scegliere il canale giusto per il proprio caso d'uso specifico pu√≤ essere complesso. In questo articolo, vi guideremo attraverso il processo decisionale e analizzeremo i compromessi, fornendovi una guida pratica sul modo migliore per accedere ai nostri modelli di ricerca fondamentali in base al vostro profilo utente e alle vostre esigenze.</p><h2 id=\"jina-search-foundation-models\">Modelli di Ricerca Fondamentali Jina</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">I Nostri Modelli di Ricerca Fondamentali</div><div class=\"kg-bookmark-description\">Abbiamo fatto progressi nei modelli di ricerca sin dal primo giorno. Dai un'occhiata alla nostra evoluzione dei modelli qui sotto - passa il mouse o clicca per scoprire ogni traguardo.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-18.png\" alt=\"\"><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-models.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>I nostri modelli di ricerca fondamentali includono:</p><ul><li><strong>Embeddings</strong>: Questi convertono le informazioni sugli oggetti digitali in vettori di embedding, catturando le loro caratteristiche essenziali.</li><li><strong>Rerankers</strong>: Questi eseguono un'analisi semantica approfondita degli insiemi query-documento per migliorare la rilevanza della ricerca.</li><li><strong>Small language models</strong>: Questi includono SLM specializzati come <code>ReaderLM-v2</code> per compiti specifici come HTML2Markdown o l'estrazione di informazioni.</li></ul><p>In questo articolo, esamineremo diverse opzioni di deployment per <code>jina-embeddings-v3</code>, confrontando tre approcci chiave:</p><ul><li>Utilizzare <a href=\"https://jina.ai/api-dashboard\" rel=\"noreferrer\">Jina API</a></li><li>Deployment tramite CSP come <a href=\"https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32\">AWS SageMaker</a></li><li>Self-hosting su un cluster Kubernetes <a href=\"https://jina.ai/api-dashboard/license-config\">con licenza commerciale</a></li></ul><p>Il confronto valuter√† le implicazioni di costo e i vantaggi di ogni approccio per aiutarti a determinare l'opzione pi√π adatta alle tue esigenze.</p><h2 id=\"key-performance-metrics\">Metriche di Performance Chiave</h2><p>Abbiamo valutato cinque metriche di performance chiave in diversi scenari di utilizzo:</p><ul><li><strong>Tasso di Successo delle Richieste</strong>: La percentuale di richieste riuscite al server di embedding</li><li><strong>Latenza delle Richieste</strong>: Il tempo impiegato dal server di embedding per elaborare e restituire una richiesta</li><li><strong>Throughput dei Token</strong>: Il numero di token che il server di embedding pu√≤ elaborare al secondo</li><li><strong>Costo per Token</strong>: Il costo totale di elaborazione per unit√† di testo</li></ul><p>Per gli embedding Jina auto-ospitati su cluster Kubernetes, abbiamo anche esaminato l'impatto del <em>dynamic batching</em>. Questa funzionalit√† mette in coda le richieste fino al raggiungimento della dimensione massima del batch (8.192 per <code>jina-embeddings-v3</code>) prima di generare gli embedding.</p><p>Abbiamo intenzionalmente escluso due fattori di performance significativi dalla nostra analisi:</p><ul><li><em>Auto-scaling</em>: Sebbene questo sia cruciale per i deployment cloud con carichi di lavoro variabili, la sua efficacia dipende da numerose variabili‚Äîefficienza hardware, architettura di rete, latenza e scelte di implementazione. Queste complessit√† vanno oltre il nostro attuale ambito. <strong>Nota che Jina API include lo scaling automatico e i nostri risultati lo riflettono.</strong></li><li><em>Quantization</em>: Mentre questa tecnica crea vettori di embedding pi√π piccoli e riduce il trasferimento dati, i principali benefici derivano da altri componenti del sistema (archiviazione dati e calcoli della distanza vettoriale) piuttosto che dalla riduzione del trasferimento dati. Poich√© ci stiamo concentrando sui costi di utilizzo diretto del modello, abbiamo lasciato fuori la quantization da questa analisi.</li></ul><p>Infine, esamineremo le implicazioni finanziarie di ogni approccio, considerando sia i costi totali di propriet√† che le spese per token/richiesta.</p><h2 id=\"deployment-setup\">Setup del Deployment</h2><p>Abbiamo valutato tre scenari di deployment e utilizzo con <code>jina-embeddings-v3</code>:</p><h3 id=\"using-the-jina-api\">Utilizzo di Jina API</h3><p>Tutti i modelli di embedding di Jina AI sono accessibili tramite <a href=\"https://jina.ai/api-dashboard/embeddings\" rel=\"noreferrer\">Jina API</a>. L'accesso funziona con un sistema di token prepagati, con un milione di token gratuiti disponibili per i test. Abbiamo valutato le prestazioni effettuando chiamate API via Internet dai nostri uffici in Germania.</p><h3 id=\"using-aws-sagemaker\">Utilizzo di AWS SageMaker</h3><p>Jina Embeddings v3 √® <a href=\"https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32\">disponibile per gli utenti AWS tramite SageMaker</a>. L'utilizzo richiede un abbonamento AWS a questo modello. Per il codice di esempio, abbiamo <a href=\"https://github.com/jina-ai/jina-sagemaker/blob/main/notebooks/Real-time%20embedding.ipynb\">fornito un notebook</a> che mostra come sottoscrivere e utilizzare i modelli Jina AI con un account AWS.</p><p>Mentre i modelli sono disponibili anche su <a href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v3-vm?tab=Overview\">Microsoft Azure</a> e <a href=\"https://console.cloud.google.com/marketplace/browse?hl=en&amp;inv=1&amp;invt=AboIuQ&amp;q=jina\">Google Cloud Platform</a>, abbiamo concentrato i nostri test su AWS. Ci aspettiamo prestazioni simili su altre piattaforme. Tutti i test sono stati eseguiti su un'istanza <code>ml.g5.xlarge</code> nella regione <code>us-east-1</code>.</p><h3 id=\"self-hosting-on-kubernetes\">Self-Hosting su Kubernetes</h3><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Per ottenere la licenza commerciale per i nostri modelli CC-BY-NC, √® necessario prima ottenere una licenza da noi. <a href=\"https://jina.ai/api-dashboard/license-config\" rel=\"noreferrer\">Non esitare a contattare il nostro team vendite.</a></div></div><p>Abbiamo sviluppato un'applicazione FastAPI in Python che <a href=\"https://huggingface.co/jinaai/jina-embeddings-v3\">carica <code>jina-embeddings-v3</code> da HuggingFace</a> utilizzando la libreria <code>SentenceTransformer</code>. L'app include due endpoint:</p><ul><li><code>/embed</code>: Prende passaggi di testo come input e restituisce i loro embedding</li><li><code>/health</code>: Fornisce monitoraggio di base della salute</li></ul><p>L'abbiamo implementato come servizio Kubernetes su Amazon Elastic Kubernetes Service, utilizzando un'istanza <code>g5.xlarge</code> in <code>us-east-1</code>.</p><h4 id=\"with-and-without-dynamic-batching\">Con e Senza Dynamic Batching</h4><p>Abbiamo testato le prestazioni in un cluster Kubernetes in due configurazioni: Una dove elaborava immediatamente ogni richiesta quando la riceveva, e una dove utilizzava il dynamic batching. Nel caso del dynamic batching, il servizio attende fino a quando <code>MAX_TOKENS</code> (8192) sono raccolti in una coda, o viene raggiunto un timeout predefinito di 2 secondi, prima di invocare il modello e calcolare gli embedding. Questo approccio aumenta l'utilizzo della GPU e riduce la frammentazione della memoria GPU.</p><p>Per ogni scenario di deployment, abbiamo eseguito test variando tre parametri chiave:</p><ul><li><strong>Dimensione del batch</strong>: Ogni richiesta conteneva 1, 32 o 128 passaggi di testo per l'embedding</li><li><strong>Lunghezza del passaggio</strong>: Abbiamo utilizzato passaggi di testo contenenti 128, 512 o 1.024 token</li><li><strong>Richieste concorrenti</strong>: Abbiamo inviato 1, 5 o 10 richieste simultaneamente</li></ul><h2 id=\"benchmark-results\">Risultati del Benchmark</h2><p>La tabella seguente √® un riepilogo dei risultati per ogni scenario di utilizzo, mediando su tutte le impostazioni delle tre variabili sopra.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Metrica</th>\n<th>Jina API</th>\n<th>SageMaker</th>\n<th>Self-Hosted<br>con Batching</th>\n<th>Self-Hosted<br>Standard</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tasso di Successo delle Richieste</td>\n<td>87,6%</td>\n<td><strong>99,9%</strong></td>\n<td>55,7%</td>\n<td>58,3%</td>\n</tr>\n<tr>\n<td>Latenza<br>(secondi)</td>\n<td>11,4</td>\n<td>3,9</td>\n<td>2,7</td>\n<td><strong>2,6</strong></td>\n</tr>\n<tr>\n<td>Latenza Normalizzata per Tasso di Successo<br>(secondi)</td>\n<td>13,0</td>\n<td><strong>3,9</strong></td>\n<td>4,9</td>\n<td>4,4</td>\n</tr>\n<tr>\n<td>Throughput dei Token<br>(token/secondo)</td>\n<td>13,8K</td>\n<td><strong>15,0K</strong></td>\n<td>2,2K</td>\n<td>2,6K</td>\n</tr>\n<tr>\n<td>Picco di Throughput dei Token<br>(token/secondo)</td>\n<td><strong>63,0K</strong></td>\n<td>32,2K</td>\n<td>10,9K</td>\n<td>10,5K</td>\n</tr>\n<tr>\n<td>Prezzo<br>(USD per 1M token)</td>\n<td>$0,02</td>\n<td>$0,07</td>\n<td>$0,32</td>\n<td>$0,32</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"request-success-rate\">Tasso di Successo delle Richieste</h2><p>I tassi di successo nei nostri test variano dal quasi perfetto 99,9% di SageMaker al modesto 56-58% delle soluzioni self-hosted, evidenziando perch√© l'affidabilit√† al 100% rimane sfuggente nei sistemi di produzione. Tre fattori chiave contribuiscono a questo:</p><ul><li>L'instabilit√† della rete causa inevitabili fallimenti anche in ambienti cloud</li><li>La contesa delle risorse, specialmente della memoria GPU, porta a fallimenti delle richieste sotto carico</li><li>I limiti di timeout necessari significano che alcune richieste devono fallire per mantenere la salute del sistema</li></ul><h3 id=\"success-rate-by-batch-size\">Tasso di Successo per Dimensione del Batch</h3><p>Le dimensioni grandi dei batch causano frequentemente errori di memoria esaurita nella configurazione Kubernetes self-hosted. Senza dynamic batching, tutte le richieste contenenti 32 o 128 elementi per batch sono fallite per questo motivo. Anche con il dynamic batching implementato, il tasso di fallimento per i batch grandi √® rimasto significativamente alto.</p>\n<!--kg-card-begin: html-->\n<table id=\"1847c956-b7d2-8017-ba56-e35215a76fc4\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1847c956-b7d2-8064-ab87-e44fc044673d\"><th id=\"FiiK\" class=\"simple-table-header-color simple-table-header\">Dimensione Batch</th><th id=\"zt<p\" class=\"simple-table-header-color simple-table-header\">Jina API</th><th id=\"kPia\" class=\"simple-table-header-color simple-table-header\">SageMaker</th><th id=\"wgj>\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(Dynamic Batching)<br></th><th id=\"OwMn\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(No Batching)<br></th></tr></thead><tbody><tr id=\"1847c956-b7d2-80e1-b4a8-c6f8a3b03117\"><th id=\"FiiK\" class=\"simple-table-header-color simple-table-header\">1</th><td id=\"zt<p\" class=\"\">100%</td><td id=\"kPia\" class=\"\">100%</td><td id=\"wgj>\" class=\"\">97.1%</td><td id=\"OwMn\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-8096-93c6-deff80bbeffc\"><th id=\"FiiK\" class=\"simple-table-header-color simple-table-header\">32</th><td id=\"zt<p\" class=\"\">86.7%</td><td id=\"kPia\" class=\"\">99.8%</td><td id=\"wgj>\" class=\"\">50.0%</td><td id=\"OwMn\" class=\"\">0.0%</td></tr><tr id=\"1847c956-b7d2-80fe-a61d-ea3923f34aac\"><th id=\"FiiK\" class=\"simple-table-header-color simple-table-header\">128</th><td id=\"zt<p\" class=\"\">76.2%</td><td id=\"kPia\" class=\"\">99.8%</td><td id=\"wgj>\" class=\"\">24.0%</td><td id=\"OwMn\" class=\"\">0.0%</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Sebbene questo problema potrebbe essere facilmente risolto tramite l'auto-scaling, abbiamo scelto di non esplorare questa opzione. L'auto-scaling porterebbe ad aumenti imprevedibili dei costi e sarebbe difficile fornire indicazioni utili dato il vasto numero di opzioni di configurazione disponibili.</p><h3 id=\"success-rate-by-concurrency-level\">Tasso di Successo Per Livello di Concorrenza</h3><p>La concorrenza ‚Äî la capacit√† di gestire pi√π richieste simultaneamente ‚Äî non ha avuto un impatto n√© forte n√© costante sui tassi di successo delle richieste nelle configurazioni Kubernetes self-hosted, e solo un effetto minimo su AWS SageMaker, almeno fino a un livello di concorrenza di 10.</p>\n<!--kg-card-begin: html-->\n<table id=\"1847c956-b7d2-80a7-9beb-f1ebe6e1e529\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1847c956-b7d2-8011-bcc1-d295e87b8e54\"><th id=\"nRCy\" class=\"simple-table-header-color simple-table-header\">Concurrency</th><th id=\"KV|=\" class=\"simple-table-header-color simple-table-header\">Jina API</th><th id=\"G@`e\" class=\"simple-table-header-color simple-table-header\">SageMaker</th><th id=\"[~nZ\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(Dynamic Batching)<br></th><th id=\"mHG:\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(No Batching)<br></th></tr></thead><tbody><tr id=\"1847c956-b7d2-8041-9a23-c1338c5d3f23\"><th id=\"nRCy\" class=\"simple-table-header-color simple-table-header\">1</th><td id=\"KV|=\" class=\"\">93.3%</td><td id=\"G@`e\" class=\"\">100%</td><td id=\"[~nZ\" class=\"\">57.5%</td><td id=\"mHG:\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-80eb-86a9-f249c86ddfdf\"><th id=\"nRCy\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"KV|=\" class=\"\">85.7%</td><td id=\"G@`e\" class=\"\">100%</td><td id=\"[~nZ\" class=\"\">58.3%</td><td id=\"mHG:\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-80ac-a3ad-eadd81c69cb2\"><th id=\"nRCy\" class=\"simple-table-header-color simple-table-header\">10</th><td id=\"KV|=\" class=\"\">83.8%</td><td id=\"G@`e\" class=\"\">99.6%</td><td id=\"[~nZ\" class=\"\">55.3%</td><td id=\"mHG:\" class=\"\">58.3%</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"success-rate-by-token-length\">Tasso di Successo Per Lunghezza dei Token</h3><p>I passaggi lunghi con conteggi elevati di token influenzano sia la Jina Embedding API che Kubernetes con dynamic batching in modo simile ai batch grandi: all'aumentare delle dimensioni, il tasso di fallimento aumenta sostanzialmente. Tuttavia, mentre le soluzioni self-hosted senza dynamic batching falliscono quasi invariabilmente con batch grandi, performano meglio con singoli passaggi lunghi. Per quanto riguarda SageMaker, le lunghezze elevate dei passaggi - come la concorrenza e la dimensione del batch - non hanno avuto un impatto notevole sui tassi di successo delle richieste.</p>\n<!--kg-card-begin: html-->\n<table id=\"1847c956-b7d2-8003-8d50-eddc36a83d33\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1847c956-b7d2-804b-8352-d65d5e6bdd0e\"><th id=\"}tQ^\" class=\"simple-table-header-color simple-table-header\">Passage Length<br>(tokens)<br></th><th id=\"CDn]\" class=\"simple-table-header-color simple-table-header\">Jina API</th><th id=\"@nCV\" class=\"simple-table-header-color simple-table-header\">SageMaker</th><th id=\"H?G{\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(Dynamic Batching)<br></th><th id=\"]{Mf\" class=\"simple-table-header-color simple-table-header\">Self-Hosted<br>(No Batching)<br></th></tr></thead><tbody><tr id=\"1847c956-b7d2-8011-8a92-d0986d045c79\"><th id=\"}tQ^\" class=\"simple-table-header-color simple-table-header\">128</th><td id=\"CDn]\" class=\"\">100%</td><td id=\"@nCV\" class=\"\">99.8%</td><td id=\"H?G{\" class=\"\">98.7%</td><td id=\"]{Mf\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-809f-b073-fa48e7287c13\"><th id=\"}tQ^\" class=\"simple-table-header-color simple-table-header\">512</th><td id=\"CDn]\" class=\"\">100%</td><td id=\"@nCV\" class=\"\">99.8%</td><td id=\"H?G{\" class=\"\">66.7%</td><td id=\"]{Mf\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-8019-9f1f-cefd810c520d\"><th id=\"}tQ^\" class=\"simple-table-header-color simple-table-header\">1024</th><td id=\"CDn]\" class=\"\">99.3%</td><td id=\"@nCV\" class=\"\">100%</td><td id=\"H?G{\" class=\"\">33.3%</td><td id=\"]{Mf\" class=\"\">58.3%</td></tr><tr id=\"1847c956-b7d2-80c7-a745-fcdaf408f3d0\"><th id=\"}tQ^\" class=\"simple-table-header-color simple-table-header\">8192</th><td id=\"CDn]\" class=\"\">51.1%</td><td id=\"@nCV\" class=\"\">100%</td><td id=\"H?G{\" class=\"\">29.4%</td><td id=\"]{Mf\" class=\"\">58.3%</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h2 id=\"request-latency\">Latenza delle Richieste</h2><p>Tutti i test di latenza sono stati ripetuti cinque volte a livelli di concorrenza di 1, 5 e 10. Il tempo di risposta √® la media su cinque tentativi. Il throughput delle richieste √® l'inverso del tempo di risposta in secondi, moltiplicato per la concorrenza.</p><h3 id=\"jina-api\">Jina API</h3><p>I tempi di risposta nella Jina API sono principalmente influenzati dalla dimensione del batch, indipendentemente dal livello di concorrenza. Mentre la lunghezza del passaggio influisce anche sulle prestazioni, il suo impatto non √® lineare. Come principio generale, le richieste contenenti pi√π dati - sia attraverso dimensioni di batch maggiori che passaggi pi√π lunghi - richiedono pi√π tempo per essere elaborate.</p><h4 id=\"concurrency-1\">Concorrenza 1:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>801</td>\n<td>1.25</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>724</td>\n<td>1.38</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>614</td>\n<td>1.63</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1554</td>\n<td>0.64</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>1620</td>\n<td>0.62</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>2283</td>\n<td>0.44</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>4441</td>\n<td>0.23</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>5430</td>\n<td>0.18</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>6332</td>\n<td>0.16</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><h4 id=\"concurrency-5\">Concurrency 5:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>689</td>\n<td>7.26</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>599</td>\n<td>8.35</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>876</td>\n<td>5.71</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1639</td>\n<td>3.05</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>2511</td>\n<td>1.99</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>4728</td>\n<td>1.06</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>2766</td>\n<td>1.81</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>5911</td>\n<td>0.85</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>18621</td>\n<td>0.27</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h4 id=\"concurrency-10\">Concurrency 10:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>790</td>\n<td>12.66</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>669</td>\n<td>14.94</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>649</td>\n<td>15.41</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1384</td>\n<td>7.23</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>3409</td>\n<td>2.93</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>8484</td>\n<td>1.18</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>3441</td>\n<td>2.91</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>13070</td>\n<td>0.77</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>17886</td>\n<td>0.56</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Per le richieste individuali (batch size di 1):</p><ul><li>I tempi di risposta rimangono relativamente stabili, variando da circa 600-800ms, indipendentemente dalla lunghezza del passaggio</li><li>Una maggiore concorrenza (5 o 10 richieste simultanee) non degrada significativamente le prestazioni per singola richiesta</li></ul><p>Per batch pi√π grandi (32 e 128 elementi):</p><ul><li>I tempi di risposta aumentano sostanzialmente, con batch size di 128 che richiede circa 4-6 volte pi√π tempo rispetto alle singole richieste</li><li>L'impatto della lunghezza del passaggio diventa pi√π pronunciato con batch pi√π grandi</li><li>Con alta concorrenza (10) e batch grandi (128), la combinazione porta a tempi di risposta significativamente pi√π lunghi, raggiungendo quasi 18 secondi per i passaggi pi√π lunghi</li></ul><p>Per il throughput:</p><ul><li>Batch pi√π piccoli generalmente ottengono un throughput migliore quando eseguono richieste concorrenti</li><li>Con concorrenza 10 e batch size 1, il sistema raggiunge il suo throughput massimo di circa 15 richieste/secondo</li><li>Batch pi√π grandi mostrano costantemente un throughput inferiore, scendendo a meno di 1 richiesta/secondo in diversi scenari</li></ul><h3 id=\"aws-sagemaker\">AWS SageMaker</h3><p>I test AWS SageMaker sono stati eseguiti con un'istanza <code>ml.g5.xlarge</code>.</p><h4 id=\"concurrency-1-1\">Concurrency 1:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>189</td>\n<td>5.28</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>219</td>\n<td>4.56</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>221</td>\n<td>4.53</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>377</td>\n<td>2.66</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>3931</td>\n<td>0.33</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>2215</td>\n<td>0.45</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>1120</td>\n<td>0.89</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>3408</td>\n<td>0.29</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>5765</td>\n<td>0.17</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h4 id=\"concurrency-5-1\">Concurrency 5:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>443</td>\n<td>11.28</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>426</td>\n<td>11.74</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>487</td>\n<td>10.27</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1257</td>\n<td>3.98</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>2245</td>\n<td>2.23</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>4159</td>\n<td>1.20</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>2444</td>\n<td>2.05</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>6967</td>\n<td>0.72</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>14438</td>\n<td>0.35</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h4 id=\"concurrency-10-1\">Concurrency 10:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Passage length (in tokens)</th>\n<th>Time to Respond in ms</th>\n<th>Request Throughput (requests/second)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>585</td>\n<td>17.09</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>602</td>\n<td>16.60</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>687</td>\n<td>14.56</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1650</td>\n<td>6.06</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>3555</td>\n<td>2.81</td>\n</tr>\n<tr>\n<td>32</td>\n<td>1024</td>\n<td>7070</td>\n<td>1.41</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>3867</td>\n<td>2.59</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>12421</td>\n<td>0.81</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>25989</td>\n<td>0.38</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Differenze principali rispetto all'API Jina:</p><ul><li>Prestazioni di base: SageMaker √® significativamente pi√π veloce per richieste piccole (singoli elementi, passaggi brevi) - circa 200ms contro 700-800ms per Jina.</li><li>Comportamento in scala:<ul><li>Entrambi i servizi rallentano con batch pi√π grandi e passaggi pi√π lunghi</li><li>SageMaker mostra un rallentamento pi√π drastico con batch grandi (128) e passaggi lunghi (1024 token)</li><li>Con alta concorrenza (10) e carico massimo (batch 128, 1024 token), SageMaker impiega ~26s contro i ~18s di Jina</li></ul></li><li>Impatto della concorrenza:<ul><li>Entrambi i servizi beneficiano dell'aumentata concorrenza per il throughput</li><li>Entrambi mantengono pattern di throughput simili attraverso i livelli di concorrenza</li><li>SageMaker raggiunge un throughput di picco leggermente superiore (17 req/s vs 15 req/s) con concorrenza 10</li></ul></li></ul><h3 id=\"self-hosted-kubernetes-cluster\">Cluster Kubernetes Self-Hosted</h3><p>I test self-hosting sono stati eseguiti su <a href=\"https://aws.amazon.com/eks/\">Amazon's Elastic Kubernetes Service</a> con un'istanza <code>g5.xlarge</code>.</p><h4 id=\"concurrency-1-2\">Concorrenza 1:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Lunghezza passaggio (token)</th>\n<th>No Batching Tempo (ms)</th>\n<th>No Batching Throughput (req/s)</th>\n<th>Dynamic Tempo (ms)</th>\n<th>Dynamic Throughput (req/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>416</td>\n<td>2.40</td>\n<td>2389</td>\n<td>0.42</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>397</td>\n<td>2.52</td>\n<td>2387</td>\n<td>0.42</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>396</td>\n<td>2.52</td>\n<td>2390</td>\n<td>0.42</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1161</td>\n<td>0.86</td>\n<td>3059</td>\n<td>0.33</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>1555</td>\n<td>0.64</td>\n<td>1496</td>\n<td>0.67</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>2424</td>\n<td>0.41</td>\n<td>2270</td>\n<td>0.44</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h4 id=\"concurrency-5-2\">Concorrenza 5:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Lunghezza passaggio (token)</th>\n<th>No Batching Tempo (ms)</th>\n<th>No Batching Throughput (req/s)</th>\n<th>Dynamic Tempo (ms)</th>\n<th>Dynamic Throughput (req/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>451</td>\n<td>11.08</td>\n<td>2401</td>\n<td>2.08</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>453</td>\n<td>11.04</td>\n<td>2454</td>\n<td>2.04</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>478</td>\n<td>10.45</td>\n<td>2520</td>\n<td>1.98</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>1447</td>\n<td>3.46</td>\n<td>1631</td>\n<td>3.06</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>2867</td>\n<td>1.74</td>\n<td>2669</td>\n<td>1.87</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>4154</td>\n<td>1.20</td>\n<td>4026</td>\n<td>1.24</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h4 id=\"concurrency-10-2\">Concorrenza 10:</h4>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Lunghezza passaggio (token)</th>\n<th>No Batching Tempo (ms)</th>\n<th>No Batching Throughput (req/s)</th>\n<th>Dynamic Tempo (ms)</th>\n<th>Dynamic Throughput (req/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>674</td>\n<td>14.84</td>\n<td>2444</td>\n<td>4.09</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>605</td>\n<td>16.54</td>\n<td>2498</td>\n<td>4.00</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>601</td>\n<td>16.64</td>\n<td>781*</td>\n<td>12.80</td>\n</tr>\n<tr>\n<td>32</td>\n<td>128</td>\n<td>2089</td>\n<td>4.79</td>\n<td>2200</td>\n<td>4.55</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>5005</td>\n<td>2.00</td>\n<td>4450</td>\n<td>2.24</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>7331</td>\n<td>1.36</td>\n<td>7127</td>\n<td>1.40</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">‚Ä† Questo risultato anomalo √® un sottoprodotto del timeout di 2 secondi del batching dinamico. Con una concorrenza di 10, ognuna che invia 1024 token di dati, la coda si riempie quasi immediatamente e il sistema di batching non deve mai attendere un timeout. Con dimensioni e concorrenze inferiori, invece, deve farlo, aggiungendo automaticamente due secondi sprecati ad ogni richiesta. Questo tipo di non linearit√† √® comune nei processi batch non ottimizzati.</div></div><p>Quando vengono fornite richieste con pi√π di 16.384 token, la nostra configurazione self-hosted fallisce con errori del server, tipicamente errori di memoria insufficiente. Questo √® vero indipendentemente dai livelli di concorrenza. Di conseguenza, non vengono visualizzati test con pi√π dati di quello.</p><p>L'alta concorrenza ha aumentato i tempi di risposta in modo ampiamente lineare: i livelli di concorrenza di 5 hanno impiegato circa cinque volte pi√π tempo per rispondere rispetto a 1. Livelli di 10, dieci volte di pi√π.</p><p>Il batching dinamico rallenta i tempi di risposta di circa due secondi per batch piccoli. Questo √® previsto perch√© la coda di batching attende 2 secondi prima di elaborare un batch non pieno. Tuttavia, per dimensioni di batch maggiori, porta a moderati miglioramenti nel tempo di risposta.</p><h2 id=\"token-throughput\">Throughput dei Token</h2><p>Il throughput dei token aumenta con dimensioni di batch maggiori, lunghezze dei passaggi pi√π lunghe e livelli di concorrenza pi√π elevati su tutte le piattaforme. Pertanto, presenteremo solo i risultati a livelli di utilizzo elevati, poich√© i livelli inferiori non fornirebbero un'indicazione significativa delle prestazioni nel mondo reale.</p><p>Tutti i test sono stati condotti con un livello di concorrenza di 10, con 16.384 token per richiesta, mediati su cinque richieste. Abbiamo testato due configurazioni: dimensione batch 32 con passaggi da 512 token e dimensione batch 128 con passaggi da 128 token. Il numero totale di token rimane costante in entrambe le configurazioni.</p><p>Throughput dei token (token al secondo):</p><table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Lunghezza passaggio (token)</th>\n<th>Jina API</th>\n<th>SageMaker</th>\n<th>Self-Hosted (No Batching)</th>\n<th>Self-Hosted (Dynamic Batching)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>46K</td>\n<td>28.5K</td>\n<td>14.3K</td>\n<td>16.1K</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>42.3K</td>\n<td>27.6K</td>\n<td>9.7K</td>\n<td>10.4K</td>\n</tr>\n</tbody>\n</table>\n<p>In condizioni di carico elevato, la Jina API supera significativamente le alternative, mentre le soluzioni self-hosted testate qui mostrano prestazioni sostanzialmente inferiori.</p><h2 id=\"costs-per-million-tokens\">Costi Per Milione di Token</h2><p>Il costo √® probabilmente il fattore pi√π critico nella scelta di una soluzione di embedding. Mentre il calcolo dei costi dei modelli AI pu√≤ essere complesso, ecco un'analisi comparativa delle diverse opzioni:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Tipo di Servizio</th>\n<th>Costo per Milione di Token</th>\n<th>Costo Infrastruttura</th>\n<th>Costo Licenza</th>\n<th>Costo Orario Totale</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina API</td>\n<td>$0.018-0.02</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>SageMaker (US East)</td>\n<td>$0.0723</td>\n<td>$1.408/ora</td>\n<td>$2.50/ora</td>\n<td>$3.908/ora</td>\n</tr>\n<tr>\n<td>SageMaker (EU)</td>\n<td>$0.0788</td>\n<td>$1.761/ora</td>\n<td>$2.50/ora</td>\n<td>$4.261/ora</td>\n</tr>\n<tr>\n<td>Self-Hosted (US East)</td>\n<td>$0.352</td>\n<td>$1.006/ora</td>\n<td>$2.282/ora</td>\n<td>$3.288/ora</td>\n</tr>\n<tr>\n<td>Self-Hosted (EU)</td>\n<td>$0.379</td>\n<td>$1.258/ora</td>\n<td>$2.282/ora</td>\n<td>$3.540/ora</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"jina-api-1\">Jina API</h3><p>Il servizio segue un modello di prezzo basato sui token con due livelli prepagati:</p><ul><li>$20 per 1 miliardo di token ($0.02 per milione) - Una tariffa entry-level ideale per prototipazione e sviluppo</li><li>$200 per 11 miliardi di token ($0.018 per milione) - Una tariffa pi√π economica per volumi maggiori</li></ul><p>Vale la pena notare che questi token funzionano su tutta la suite di prodotti Jina, inclusi lettori, reranker e classificatori zero-shot.</p><h3 id=\"aws-sagemaker-1\">AWS SageMaker</h3><p>Il prezzo di SageMaker combina i costi orari dell'istanza con i costi di licenza del modello. Usando un'istanza <code>ml.g5.xlarge</code>:</p><ul><li>Costo dell'istanza: $1.408/ora (US East) o $1.761/ora (EU Frankfurt)</li><li>Licenza <code>jina-embeddings-v3</code>: $2.50/ora</li><li>Costo orario totale: $3.908-$4.261 a seconda della regione</li></ul><p>Con una velocit√† media di 15.044 token/secondo (54.16M token/ora), il costo per milione di token varia da $0.0723 a $0.0788.</p><h3 id=\"self-hosting-with-kubernetes\">Self-Hosting con Kubernetes</h3><p>I costi di self-hosting variano significativamente in base alla scelta dell'infrastruttura. Usando come riferimento l'istanza <code>g5.xlarge</code> di AWS EC2:</p><ul><li>Costo dell'istanza: $1.006/ora (US East) o $1.258/ora (EU Frankfurt)</li><li>Licenza <code>jina-embeddings-v3</code>: $5000/trimestre ($2.282/ora)</li><li>Costo orario totale: $3.288-$3.540 a seconda della regione</li></ul><p>A 2.588 token/secondo (9.32M token/ora), il costo per milione di token arriva a $0.352-$0.379. Mentre il costo orario √® inferiore a SageMaker, la minor velocit√† comporta costi pi√π alti per token.</p><p>Considerazioni importanti per il self-hosting:</p><ul><li>I costi fissi (licenze, infrastruttura) continuano indipendentemente dall'utilizzo</li><li>L'hosting on-premises richiede comunque costi di licenza e di personale</li><li>I carichi di lavoro variabili possono impattare significativamente l'efficienza dei costi</li></ul><h3 id=\"key-takeaways\">Punti Chiave</h3><p>L'API Jina emerge come la soluzione pi√π conveniente, anche senza considerare i tempi di cold-start e assumendo una velocit√† ottimale per le alternative.</p><p>Il self-hosting potrebbe avere senso per organizzazioni con un'infrastruttura robusta esistente dove i costi marginali dei server sono minimi. Inoltre, esplorare provider cloud diversi da AWS potrebbe portare a prezzi migliori.</p><p>Tuttavia, per la maggior parte delle aziende, specialmente le PMI che cercano soluzioni chiavi in mano, l'API Jina offre un'efficienza di costo impareggiabile.</p><h2 id=\"security-and-data-privacy-considerations\">Considerazioni sulla Sicurezza e Privacy dei Dati</h2><p>Nella scelta di una strategia di deployment per i modelli di embedding, i requisiti di sicurezza e privacy dei dati possono giocare un ruolo decisivo insieme alle considerazioni su prestazioni e costi. Offriamo opzioni di deployment flessibili per soddisfare diverse esigenze di sicurezza:</p><h3 id=\"cloud-service-providers\">Provider di Servizi Cloud</h3><p>Per le <strong>aziende che gi√† lavorano con i principali provider cloud</strong>, le nostre offerte sul marketplace cloud (come <a href=\"https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32\">AWS Marketplace</a>, <a href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v3-vm?tab=Overview\">Azure</a>, e <a href=\"https://console.cloud.google.com/marketplace/browse?hl=en&amp;inv=1&amp;invt=AboIuQ&amp;q=jina\">GCP</a>) forniscono una soluzione naturale per il deployment all'interno dei framework di sicurezza preesistenti. Questi deployment beneficiano di:</p><ul><li>Controlli di sicurezza e conformit√† ereditati dalla relazione con il CSP</li><li>Integrazione immediata con le politiche di sicurezza esistenti e le regole di governance dei dati</li><li>Richiede poche o nessuna modifica agli accordi esistenti sul trattamento dei dati</li><li>Allineamento con le considerazioni preesistenti sulla sovranit√† dei dati</li></ul><h3 id=\"self-hosting-and-local-deployment\">Self-Hosting e Deployment Locale</h3><p><strong>Le organizzazioni con requisiti di sicurezza stringenti o obblighi normativi specifici</strong> spesso preferiscono il controllo fisico completo della loro infrastruttura. La nostra opzione self-hosted permette:</p><ul><li>Controllo completo sull'ambiente di deployment</li><li>Elaborazione dei dati interamente all'interno del proprio perimetro di sicurezza</li><li>Integrazione con il monitoraggio di sicurezza e i controlli esistenti</li></ul><p>Per ottenere licenze commerciali per i nostri modelli CC-BY-NC, √® necessario prima ottenere una licenza da noi. <a href=\"https://jina.ai/api-dashboard/license-config\" rel=\"noreferrer\">Non esitare a contattare il nostro team commerciale.</a></p><h3 id=\"jina-api-service\">Servizio API Jina</h3><p>Per <strong>startup e PMI</strong> che cercano di bilanciare sicurezza e praticit√† rispetto ai costi, il nostro servizio API fornisce sicurezza di livello enterprise senza aggiungere overhead operativo:</p><ul><li><a href=\"https://jina.ai/Jina_AI_GmbH_Letter_of_Attestation_SOC_2_Type_1.pdf\" rel=\"noreferrer\">Certificazione SOC2</a> che garantisce robusti controlli di sicurezza</li><li><a href=\"https://gdpr-info.eu/\" rel=\"noreferrer\">Piena conformit√† GDPR</a> per il trattamento dei dati</li><li>Politica di zero data retention - non memorizziamo n√© registriamo le tue richieste</li><li>Trasmissione dati crittografata e infrastruttura sicura</li></ul><p>Le offerte di modelli di Jina AI permettono alle organizzazioni di scegliere la strategia di deployment che meglio si allinea con i loro requisiti di sicurezza mantenendo l'efficienza operativa.</p><h2 id=\"choosing-your-solution\">Scegliere la Tua Soluzione</h2><p>Il diagramma di flusso sottostante riassume i risultati di tutti i test empirici e le tabelle che hai visto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1256\" height=\"1980\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/01/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/image-3.png 1256w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Con queste informazioni, il diagramma di flusso sopra dovrebbe darti una buona indicazione su quali tipi di soluzioni considerare.</span></figcaption></figure><p>Prima, considera le tue esigenze di sicurezza e quanta flessibilit√† puoi sacrificare per soddisfarle.</p><p>Poi, considera come pianifichi di utilizzare l'AI nella tua azienda:</p><ol><li>Indicizzazione offline e casi d'uso non time-sensitive che possono utilizzare in modo ottimale l'elaborazione batch.</li><li>Usi sensibili all'affidabilit√† e alla scalabilit√† come la generazione aumentata da retrieval e l'integrazione LLM.</li><li>Utilizzi time-sensitive come la ricerca e il retrieval online.</li></ol><p>Inoltre, considera la tua esperienza interna e l'infrastruttura esistente:</p><ol><li>Il tuo stack tecnologico √® gi√† fortemente dipendente dal cloud?</li><li>Hai una grande operazione IT interna in grado di gestire il self-hosting?</li></ol><p>Infine, considera i volumi di dati previsti. Sei un utente su larga scala che prevede di eseguire milioni di operazioni utilizzando modelli AI ogni giorno?</p><h2 id=\"conclusion\">Conclusione</h2><p>L'integrazione dell'AI nelle decisioni operative rimane territorio inesplorato per molti dipartimenti IT, poich√© il mercato manca di soluzioni chiavi in mano consolidate. Questa incertezza pu√≤ rendere difficile la pianificazione strategica. La nostra analisi quantitativa mira a fornire una guida concreta sull'incorporazione dei nostri modelli di ricerca nei tuoi specifici flussi di lavoro e applicazioni.</p><p>Quando si tratta di costo per unit√†, l'API Jina si distingue come una delle opzioni pi√π economiche disponibili per le aziende. Poche alternative possono eguagliare il nostro prezzo offrendo funzionalit√† comparabili.</p><p>Siamo impegnati a fornire capacit√† di ricerca che non sono solo potenti e user-friendly, ma anche convenienti per organizzazioni di tutte le dimensioni. Che sia attraverso i principali provider cloud o deployment self-hosted, le nostre soluzioni soddisfano anche i requisiti aziendali pi√π complessi che vanno oltre le pure considerazioni di costo. Questa analisi scompone i vari fattori di costo per aiutare il tuo processo decisionale.</p><p>Dato che ogni organizzazione ha i propri requisiti unici, riconosciamo che un singolo articolo non pu√≤ affrontare ogni scenario. Se hai esigenze specifiche non coperte qui, contattaci per discutere come possiamo supportare al meglio la tua implementazione.</p>",
  "comment_id": "679b56ba42b46600019a86e3",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/guide-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-30T11:38:50.000+01:00",
  "updated_at": "2025-01-31T05:32:29.000+01:00",
  "published_at": "2025-01-31T05:32:29.000+01:00",
  "custom_excerpt": "We offer detailed cost and performance breakdowns for three deployment strategies: Jina API, self-hosted K8s, and AWS SageMaker, to help you make the right decision.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-practical-guide-to-deploying-search-foundation-models-in-production/",
  "excerpt": "Offriamo analisi dettagliate dei costi e delle prestazioni per tre strategie di deployment: Jina API, K8s self-hosted e AWS SageMaker, per aiutarti a prendere la decisione giusta.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
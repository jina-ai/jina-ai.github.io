{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "Ottimizzazione submodulare per la generazione di query diversificate in DeepResearch",
  "html": "<p>Quando si implementa DeepResearch, ci sono almeno due punti in cui è necessario generare query diversificate. Innanzitutto, è necessario <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">generare query di ricerca web basate sull'input dell'utente</a> (inoltrare direttamente l'input dell'utente al motore di ricerca non è una buona idea). In secondo luogo, molti sistemi DeepResearch includono <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">un \"pianificatore di ricerca\" che suddivide il problema originale in sottoproblemi</a>, chiama contemporaneamente gli agenti per risolverli in modo indipendente e quindi unisce i loro risultati. Che si tratti di query o sottoproblemi, le nostre aspettative rimangono le stesse: devono essere rilevanti per l'input originale e sufficientemente diversificate da fornire prospettive uniche su di esso. Spesso, è necessario limitare il numero di query per evitare di sprecare denaro richiedendo inutilmente il motore di ricerca o utilizzando i token dell'agente.</p><p>Pur comprendendo l'importanza della generazione di query, la maggior parte delle implementazioni open source di DeepResearch non prende sul serio questa ottimizzazione. Si limitano a richiedere direttamente questi vincoli tramite dei Prompt. Alcuni potrebbero chiedere all'LLM un ulteriore passaggio per valutare e diversificare le query. Ecco un esempio di come la maggior parte delle implementazioni affronta sostanzialmente questo problema:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Due Prompt diversi per generare query diversificate utilizzando gli LLM. Il Prompt superiore utilizza istruzioni semplici. Quello inferiore è più sofisticato e strutturato. Dato la query originale e il numero di query da generare, ci aspettiamo che le query generate siano sufficientemente diversificate. In questo esempio, utilizziamo </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> come LLM e la query originale è </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>In questo articolo, voglio dimostrare un approccio più rigoroso alla risoluzione della generazione ottimale di query utilizzando i modelli vettoriali di frasi e l'<strong>ottimizzazione submodulare</strong>. Ai tempi del mio dottorato di ricerca, l'ottimizzazione submodulare era una delle mie tecniche preferite insieme a L-BFGS. Mostrerò come applicarla per generare un insieme di query diversificate sotto un vincolo di cardinalità, che può migliorare significativamente la qualità complessiva dei sistemi DeepResearch.</p><h2 id=\"query-generation-via-prompting\">Generazione di query tramite Prompt</h2><p>Innanzitutto, vogliamo verificare se il Prompt è un approccio efficace per generare query diversificate. Vogliamo anche capire se un Prompt sofisticato è più efficace di un Prompt semplice. Eseguiamo un esperimento confrontando i due Prompt seguenti per scoprirlo:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Prompt semplice</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Prompt strutturato</span></p></figcaption></figure><p>Utilizziamo <code>gemini-2.5-flash</code> come LLM con la query originale <code>\"embeddings and rerankers\"</code> e testiamo sia il Prompt semplice che quello strutturato per generare iterativamente da una a 20 query. Quindi utilizziamo <code>jina-embeddings-v3</code> con l'attività <code>text-matching</code> per misurare la similarità delle frasi tra la query originale e le query generate, nonché la similarità all'interno delle query generate stesse. Ecco le visualizzazioni.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Entrambi i Prompt mostrano modelli simili nell'analisi \"All'interno delle query generate\" (i due grafici a destra), con similarità coseno mediana che rimane alta (intervallo 0,4-0,6) tra diversi conteggi di query. Il Prompt semplice sembra essere ancora migliore nel diversificare le query quando il numero di query è elevato, mentre il Prompt strutturato mantiene una rilevanza leggermente migliore per la query originale, mantenendo la rilevanza intorno a 0,6.</span></figcaption></figure><p>Guardando i due grafici sul lato destro, si può vedere che sia il Prompt semplice che quello strutturato mostrano un'ampia varianza nei punteggi di similarità coseno, con molti che raggiungono una similarità di 0,7-0,8, suggerendo che alcune query generate sono quasi identiche. Inoltre, entrambi i metodi faticano a mantenere la diversità man mano che vengono generate più query. Invece di vedere una chiara tendenza al ribasso nella similarità con l'aumentare del conteggio delle query, osserviamo livelli di similarità relativamente stabili (e alti), il che indica che le query aggiuntive spesso duplicano le prospettive esistenti.</p><p>Una spiegazione è ciò che Wang et al. (2025) hanno scoperto, ovvero che gli LLM spesso riflettono le opinioni dei gruppi dominanti in modo sproporzionato, anche con la guida del Prompt, indicando una propensione verso prospettive comuni. Questo perché i dati di addestramento dell'LLM possono sovra-rappresentare determinati punti di vista, facendo sì che il modello generi variazioni che si allineano a queste prospettive prevalenti. Abe et al. (2025) hanno anche scoperto che l'espansione delle query basata su LLM favorisce le interpretazioni popolari trascurandone altre. Ad esempio, \"Quali sono i vantaggi dell'IA?\" potrebbe produrre vantaggi comuni come l'automazione, l'efficienza, l'eticità, ma tralasciare quelli meno ovvi come la scoperta di farmaci.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">Formulazione del problema</h2><p>Si potrebbe pensare che il nostro precedente esperimento non sia conclusivo e che dovremmo migliorare il Prompt e riprovare. Sebbene il Prompt possa certamente cambiare i risultati in una certa misura, ciò che è più importante è che abbiamo imparato qualcosa: aumentare semplicemente il numero di query generate ci rende più propensi a ottenere query diversificate. La cattiva notizia è che stiamo anche ottenendo un mucchio di query duplicate come prodotto secondario.</p><p>Ma poiché è economico generare un gran numero di query, che alla fine produce <em>alcune</em> buone query, perché non trattiamo questo come un problema di selezione di sottoinsiemi?</p><p>In matematica, ecco come possiamo formulare questo problema: dato un input originale&nbsp;$q_0$, un insieme di query candidate&nbsp;$V=\\{q_1, q_2, \\cdots, q_n\\}$&nbsp;generato da un LLM usando l'ingegneria del Prompt. Seleziona un sottoinsieme&nbsp;$X\\subseteq V$&nbsp;di&nbsp;$k$&nbsp;query che massimizza la copertura minimizzando la ridondanza.</p><p>Sfortunatamente, trovare il sottoinsieme ottimale di $k$ query da $n$ candidati richiede il controllo di $\\binom{n}{k}$ combinazioni - complessità esponenziale. Solo per 20 candidati e $k=5$, si tratta di 15.504 combinazioni.</p><h3 id=\"submodular-function\">Funzione Submodulare</h3><p>Prima di provare a risolvere brutalmente il problema della selezione del sottoinsieme, vorrei introdurre ai lettori i termini <strong>submodularità</strong> e <strong>funzione submodulare</strong>. Potrebbero sembrare sconosciuti a molti, ma probabilmente avrete sentito parlare dell'idea dei \"rendimenti decrescenti\" - ebbene, la submodularità è la rappresentazione matematica di questo concetto.</p><p>Consideriamo di posizionare dei router Wi-Fi per fornire copertura internet in un grande edificio. Il primo router che installi offre un valore enorme: copre un'area significativa che in precedenza non aveva copertura. Anche il secondo router aggiunge un valore considerevole, ma parte della sua area di copertura si sovrappone a quella del primo router, quindi il beneficio marginale è inferiore al primo. Man mano che continui ad aggiungere router, ogni router aggiuntivo copre un'area nuova sempre minore perché la maggior parte degli spazi è già coperta dai router esistenti. Alla fine, il decimo router potrebbe fornire una copertura aggiuntiva molto limitata poiché l'edificio è già ben coperto.</p><p>Questa intuizione cattura l'essenza della submodularità. Matematicamente, una funzione di insieme $f: 2^V \\rightarrow \\mathbb{R}$ è <strong>submodulare</strong> se per tutti gli $A \\subseteq B \\subseteq V$ e qualsiasi elemento $v \\notin B$:</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>In parole povere: aggiungere un elemento a un insieme più piccolo offre almeno lo stesso beneficio di aggiungere lo stesso elemento a un insieme più grande che contiene l'insieme più piccolo.</p><p>Ora applichiamo questo concetto al nostro problema di generazione di query. Si può immediatamente notare che la selezione delle query mostra un naturale <strong>rendimento decrescente</strong>:</p><ul><li>La prima query che selezioniamo copre uno spazio semantico completamente nuovo</li><li>La seconda query dovrebbe coprire aspetti diversi, ma una certa sovrapposizione è inevitabile</li><li>Man mano che aggiungiamo più query, ogni query aggiuntiva copre un terreno nuovo sempre minore</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Da </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">una delle mie vecchie slide risalenti ad AAAI 2013</span></a><span style=\"white-space: pre-wrap;\">, dove spiegavo la submodularità usando un sacco di palline. Aggiungere più palline al sacco migliora la \"facility\", ma il miglioramento relativo diventa sempre più piccolo, come si vede nei valori delta decrescenti sull'asse y a destra.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">Progettazione di funzioni submodulari basate su Embeddings</h2><p>Sia $\\mathbf{e}_i \\in \\mathbb{R}^d$ il vettore di Embedding per la query $q_i$, ottenuto utilizzando un modello di Embedding di frasi (ad es. <code>jina-embeddings-v3</code>). Esistono due approcci principali per progettare la nostra funzione obiettivo:</p><h3 id=\"approach-1-facility-location-coverage-based\">Approccio 1: Posizione della struttura (basata sulla copertura)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>Questa funzione misura quanto bene l'insieme selezionato $X$ \"copre\" tutte le query candidate, dove:</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ è la somiglianza del coseno</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ assicura la rilevanza alla query originale</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ misura la copertura del candidato $j$ da parte dell'insieme selezionato $X$</li></ul><p>Un avvertimento è che questa funzione incoraggia solo <em>implicitamente</em> la diversità. Non penalizza esplicitamente la somiglianza all'interno dell'insieme selezionato $X$. La diversità emerge perché la selezione di query simili fornisce rendimenti di copertura decrescenti.</p><h3 id=\"approach-2-explicit-coverage-diversity\">Approccio 2: Copertura esplicita + Diversità</h3><p>Per un controllo più diretto sulla diversità, possiamo combinare la copertura e un termine di diversità esplicito:</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>dove la componente di diversità può essere formulata come:</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>Questo termine di diversità misura la somiglianza totale tra le query selezionate e le query non selezionate: viene massimizzato quando selezioniamo query diverse dai candidati rimanenti (una forma di funzione di taglio del grafico).</p><h3 id=\"difference-between-two-approaches\">Differenza tra i due approcci</h3><p>Entrambe le formulazioni mantengono la submodularità.</p><p>La funzione di posizione della struttura è una funzione submodulare ben nota. Mostra submodularità a causa dell'operazione max: quando aggiungiamo una nuova query $q$ al nostro insieme selezionato, ogni query candidata $j$ viene coperta dalla query \"migliore\" nel nostro insieme (quella con la massima somiglianza). Aggiungere $q$ a un insieme più piccolo $A$ ha più probabilità di migliorare la copertura di vari candidati rispetto ad aggiungerlo a un insieme più grande $B \\supseteq A$ dove molti candidati sono già ben coperti.</p><p>Nella funzione di diversità del taglio del grafico<strong>,</strong> il termine di diversità $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ è submodulare perché misura il \"taglio\" tra gli insiemi selezionati e non selezionati. Aggiungere una nuova query a un insieme selezionato più piccolo crea più nuove connessioni alle query non selezionate rispetto ad aggiungerla a un insieme selezionato più grande.</p><p>L'approccio di localizzazione della struttura si basa sulla diversità <em>implicita</em> attraverso la competizione di copertura, mentre l'approccio esplicito misura e ottimizza direttamente la diversità. Quindi entrambi sono validi, ma l'approccio esplicito offre un controllo più diretto sul compromesso rilevanza-diversità.</p><h2 id=\"implementations\">Implementazioni</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">L'implementazione completa è disponibile qui su Github.</span></p></figcaption></figure><p>Poiché la nostra funzione è submodulare, possiamo usare <strong>l'algoritmo greedy</strong> che fornisce una garanzia di approssimazione di $(1-1/e) \\approx 0.63$:</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>Ecco il codice per ottimizzare la posizione della struttura (basata sulla copertura) - quella con diversità implicita.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Algoritmo greedy per la selezione submodulare delle query\n    \n    Args:\n        candidates: Elenco di stringhe di query candidate\n        embeddings: Matrice di Embedding delle query (n x d)\n        original_embedding: Embedding della query originale (d,)\n        k: Numero di query da selezionare\n        alpha: Parametro di peso della rilevanza\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precalcola i punteggi di rilevanza\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calcola il guadagno marginale dell'aggiunta della query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Calcola il guadagno marginale dell'aggiunta di new_idx all'insieme selezionato\"\"\"\n    if not selected:\n        # Prima query: il guadagno è la somma di tutti i punteggi di rilevanza\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Calcola la copertura attuale\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Calcola la nuova copertura con la query aggiuntiva\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>Il parametro di bilanciamento $\\alpha$ controlla il compromesso tra rilevanza e diversità:</p><ul><li><strong>$\\alpha$ alto (ad es. 0,8)</strong>: dà la priorità alla rilevanza alla query originale, può sacrificare la diversità</li><li><strong>$\\alpha$ basso (ad es. 0,2)</strong>: dà la priorità alla diversità tra le query selezionate, può allontanarsi dall'intento originale</li><li><strong>$\\alpha$ moderato (ad es. 0,4-0,6)</strong>: approccio bilanciato, spesso funziona bene nella pratica</li></ul><h3 id=\"lazy-greedy-algorithm\">Algoritmo Greedy Pigro</h3><p>Si può notare nel codice sopra:</p><pre><code class=\"language-python\">for i in remaining:\n    # Calcola il guadagno marginale dell'aggiunta della query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>Stiamo calcolando il guadagno marginale per <strong>tutti</strong> i candidati rimanenti a ogni iterazione. Possiamo fare di meglio.</p><p>L'<strong>algoritmo greedy pigro</strong> è un'ottimizzazione intelligente che sfrutta la submodularità per evitare calcoli non necessari. L'intuizione chiave è: se l'elemento A aveva un guadagno marginale maggiore dell'elemento B nell'iterazione $t$, allora A avrà ancora un guadagno marginale maggiore di B nell'iterazione $t+1$ (a causa della proprietà di submodularità).</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Il lazy greedy funziona così:</p><ol><li>Mantiene una coda di priorità degli elementi ordinati in base ai loro guadagni marginali</li><li>Ricalcola solo il guadagno marginale dell'elemento in cima</li><li>Se è ancora il più alto dopo il ricalcolo, lo seleziona</li><li>Altrimenti, lo reinserisce nella posizione corretta e controlla l'elemento successivo in cima</li></ol><p>Questo può fornire accelerazioni significative perché evitiamo di ricalcolare i guadagni marginali per gli elementi che chiaramente non verranno selezionati.</p><h3 id=\"results\">Risultati</h3><p>Eseguiamo di nuovo l'esperimento. Utilizziamo lo stesso semplice prompt per generare da 1 a 20 query diverse ed eseguiamo le stesse misurazioni di similarità del coseno di prima. Per l'ottimizzazione submodulare, selezioniamo le query dai 20 candidati generati utilizzando diversi valori di k e misuriamo la similarità come prima. I risultati mostrano che le query selezionate tramite l'ottimizzazione submodulare sono più diverse e mostrano una similarità in-set inferiore.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Query originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Query originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Query originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Query originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">Domanda finale: perché la formulazione submodulare è importante</h2><p>Potresti chiederti: perché prendersi la briga di formulare questo come un problema di ottimizzazione submodulare? Perché non usare semplicemente euristiche o altri approcci di ottimizzazione?</p><p>In breve, la formulazione submodulare trasforma un'euristica ad-hoc \"seleziona query diverse\" in un rigoroso problema di ottimizzazione con <strong>garanzie dimostrabili</strong>, <strong>algoritmi efficienti</strong> e obiettivi misurabili.</p><h3 id=\"guaranteed-efficiency\">Efficienza garantita</h3><p>Una volta dimostrato che la nostra funzione obiettivo è submodulare, otteniamo potenti garanzie teoriche e un algoritmo efficiente. L'algoritmo greedy che viene eseguito in tempo $O(nk)$ rispetto al controllo delle combinazioni $\\binom{n}{k}$ raggiunge un'approssimazione $(1-1/e) \\approx 0.63$ alla soluzione ottimale. Ciò significa che la nostra soluzione greedy è sempre almeno il 63% buona quanto la migliore soluzione possibile. <strong>Nessuna euristica può promettere questo.</strong></p><p>Inoltre, l'algoritmo lazy greedy è notevolmente più veloce nella pratica grazie alla struttura matematica delle funzioni submodulari. L'accelerazione deriva dai <strong>rendimenti decrescenti</strong>: è improbabile che gli elementi che erano scelte sbagliate nelle iterazioni precedenti diventino buone scelte in seguito. Quindi, invece di controllare tutti gli $n$ candidati, il lazy greedy in genere deve solo ricalcolare i guadagni per i primi candidati.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">Non c'è bisogno di euristiche artigianali</h3><p>Senza un framework basato su principi, potresti ricorrere a regole ad-hoc come \"assicurarsi che le query abbiano una similarità del coseno &lt; 0.7\" o \"bilanciare diverse categorie di parole chiave\". Queste regole sono difficili da ottimizzare e non si generalizzano. L'ottimizzazione submodulare ti offre un approccio basato su principi e matematicamente fondato. Puoi ottimizzare sistematicamente gli iperparametri utilizzando set di convalida e monitorare la qualità della soluzione nei sistemi di produzione. Quando il sistema produce risultati scadenti, hai metriche chiare per eseguire il debug di ciò che è andato storto.</p><p>Infine, l'ottimizzazione submodulare è un campo ben studiato con decenni di ricerca, che ti consente di sfruttare algoritmi avanzati oltre al greedy (come il greedy accelerato o la ricerca locale), approfondimenti teorici su quando determinate formulazioni funzionano meglio ed estensioni per gestire vincoli aggiuntivi come limiti di budget o requisiti di equità.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Per coloro che sono interessati all'ottimizzazione submodulare, consiglio questo sito per saperne di più.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "Molti conoscono l'importanza della diversità delle query in DeepResearch, ma pochi sanno come risolverla rigorosamente tramite l'ottimizzazione submodulare.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
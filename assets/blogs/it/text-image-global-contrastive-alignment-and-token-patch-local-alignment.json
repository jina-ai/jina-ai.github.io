{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "Allineamento Contrastivo Globale Testo-Immagine e Allineamento Locale Token-Patch",
  "html": "<p>Durante gli esperimenti con modelli in stile <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali</a>, uno dei nostri ingegneri ha creato una visualizzazione utilizzando il nostro modello <code>jina-clip-v2</code> recentemente rilasciato. Ha mappato la similarità tra gli embedding dei token e gli embedding delle patch per determinate coppie immagine-testo, creando sovrapposizioni di heatmap che hanno prodotto alcuni spunti visivi interessanti.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Sfortunatamente, <strong>questa è solo una visualizzazione euristica</strong> - non un meccanismo esplicito o garantito. Mentre l'allineamento contrastivo globale in stile CLIP può (e spesso lo fa) creare <em>incidentalmente</em> allineamenti locali approssimativi tra patch e token, questo è un <strong>effetto collaterale non intenzionale</strong> piuttosto che un obiettivo deliberato del modello. Lasciami spiegare perché.</p><h2 id=\"understand-the-code\">Comprendere il Codice</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Analizziamo cosa fa il codice ad alto livello. Nota che <code>jina-clip-v2</code> in realtà non espone alcuna API per accedere agli embedding a livello di token o patch di default - questa visualizzazione ha richiesto alcune modifiche post-hoc per funzionare.</p><p><strong>Calcolare gli embedding a livello di parola</strong></p><p>Impostando <code>model.text_model.output_tokens = True</code>, chiamando <code>text_model(x=...,)[1]</code> restituirà un secondo elemento <code>(batch_size, seq_len, embed_dim)</code> per gli embedding dei token. Quindi prende una frase in input, la tokenizza con il tokenizer Jina CLIP, e poi raggruppa i token delle sottoparole in \"parole\" facendo la media degli embedding dei token corrispondenti. Rileva l'inizio di una nuova parola controllando se la stringa del token inizia con il carattere <code>_</code> (tipico nei tokenizer basati su SentencePiece). Produce una lista di embedding a livello di parola e una lista di parole (così che \"Cane\" sia un embedding, \"e\" sia un embedding, ecc.).</p><p><strong>Calcolare gli embedding a livello di patch</strong></p><p>Per la torre dell'immagine, <code>vision_model(..., return_all_features=True)</code> restituirà <code>(batch_size, n_patches+1, embed_dim)</code>, dove il primo token è il token <code>[CLS]</code>. Da questo, il codice estrae gli embedding per ogni patch (cioè, i token patch del vision transformer). Quindi ridimensiona questi embedding delle patch in una griglia 2D, <code>patch_side × patch_side</code>, che viene poi upsamplata per corrispondere alla risoluzione dell'immagine originale.</p><p><strong>Visualizzare la similarità parola-patch</strong></p><p>Il calcolo della similarità e la successiva generazione della heatmap sono tecniche di interpretabilità \"post-hoc\" standard: si seleziona un embedding del testo, si calcola la similarità del coseno con ogni embedding della patch e poi si genera una heatmap che mostra quali patch hanno la più alta similarità con quello specifico embedding del token. Infine, scorre ciascun token nella frase, evidenzia quel token in grassetto a sinistra e sovrappone la heatmap basata sulla similarità sull'immagine originale a destra. Tutti i frame vengono compilati in una GIF animata.</p><h2 id=\"is-it-meaningful-explainability\">È una Spiegabilità Significativa?</h2><p>Da un punto di vista di <em>puro codice</em>, sì, la logica è coerente e produrrà una heatmap per ogni token. Otterrai una serie di frame che evidenziano le similarità delle patch, quindi lo script \"fa quello che promette\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Guardando gli esempi sopra, vediamo che parole come <code>moon</code> e <code>branches</code> sembrano allinearsi bene con le loro patch visive corrispondenti nell'immagine originale. Ma ecco la domanda chiave: è un allineamento significativo o stiamo solo vedendo una coincidenza fortunata?</p><p>Questa è una domanda più profonda. Per comprendere le avvertenze, ricorda <strong>come viene addestrato CLIP</strong>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagramma del modello JINA-CLIP-V2 che mostra le fasi dall'input all'output per l'elaborazione del testo in inglese e multilingue.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 combina un encoder di testo (Jina XLM-RoBERTa, 561M parametri) e un encoder visivo (EVA02-L14, 304M parametri). Ogni quadrato colorato a destra rappresenta una frase completa o un'immagine nel batch - non singoli token o patch.</span></figcaption></figure><ul><li>CLIP usa un allineamento contrastivo <strong>globale</strong> tra un'intera immagine e un intero pezzo di testo. Durante l'addestramento, l'encoder dell'immagine produce un singolo vettore (rappresentazione aggregata), e l'encoder del testo produce un altro singolo vettore; CLIP viene addestrato in modo che questi corrispondano per coppie testo-immagine corrispondenti e non corrispondano in caso contrario.</li><li>Non c'è <strong>supervisione esplicita a livello di 'la patch X corrisponde al token Y'.</strong> Il modello non è direttamente addestrato a evidenziare \"questa regione dell'immagine è il cane, quella regione è il gatto,\" ecc. Invece, viene insegnato che l'intera rappresentazione dell'immagine dovrebbe corrispondere all'intera rappresentazione del testo.</li><li>Poiché l'architettura di CLIP è un Vision Transformer sul lato immagine e un transformer del testo sul lato testo—entrambi formano encoder separati—non c'è un modulo di cross-attention che allinei nativamente le patch ai token. Invece, si ottiene una <strong>self-attention</strong> puramente in ciascuna torre, più una proiezione finale per gli embedding globali dell'immagine o del testo.</li></ul><p>In breve, questa è una visualizzazione euristica. Il fatto che un dato embedding di patch possa essere vicino o lontano da un particolare embedding di token è in qualche modo emergente. È più un <em>trucco di interpretabilità post-hoc</em> che un'\"attenzione\" robusta o ufficiale del modello.</p><h2 id=\"why-might-local-alignment-emerge\">Perché Potrebbe Emergere l'Allineamento Locale?</h2><p>Quindi perché potremmo a volte notare allineamenti locali a livello parola-patch? Ecco il punto: anche se CLIP è addestrato su un obiettivo contrastivo <em>globale</em> immagine-testo, usa comunque self-attention (negli encoder di immagini basati su ViT) e layer transformer (per il testo). All'interno di questi layer di self-attention, diverse parti delle rappresentazioni dell'immagine possono interagire tra loro, proprio come fanno le parole nelle rappresentazioni del testo. Attraverso l'addestramento su enormi dataset di immagini-testo, il modello sviluppa naturalmente strutture latenti interne che lo aiutano a far corrispondere le immagini complessive alle loro descrizioni testuali corrispondenti.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>L'<strong>allineamento locale</strong> può apparire in queste rappresentazioni latenti per almeno due ragioni:</p><ol><li><strong>Modelli di co-occorrenza</strong>: Se un modello vede molte immagini di \"cani\" accanto a molte immagini di \"gatti\" (spesso etichettate o descritte con queste parole), può imparare caratteristiche latenti che corrispondono approssimativamente a questi concetti. Quindi l'embedding per \"cane\" potrebbe diventare vicino a patch locali che rappresentano una forma o una texture simile a quella di un cane. Questo <em>non</em> è supervisionato esplicitamente a livello di patch, ma emerge dall'associazione ripetuta tra coppie di immagini/testo di cani.</li><li><strong>Self-attention</strong>: Nei Vision Transformers, i patch prestano attenzione l'uno all'altro. I patch distintivi (come il muso di un cane) possono finire per avere una \"firma\" latente coerente, poiché il modello sta cercando di produrre una singola rappresentazione globalmente accurata dell'intera scena. Se questo aiuta a minimizzare la perdita contrastiva complessiva, verrà rafforzato.</li></ol><h2 id=\"theoretical-analysis\">Analisi Teorica</h2><p>L'obiettivo di apprendimento contrastivo di CLIP mira a massimizzare la similarità del coseno tra coppie immagine-testo corrispondenti mentre la minimizza per le coppie non corrispondenti. Assumiamo che gli encoder di testo e immagine producano rispettivamente embedding di token e patch:</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>La similarità globale può essere rappresentata come un aggregato di similarità locali:</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Quando specifiche coppie token-patch si verificano frequentemente nei dati di training, il modello rafforza la loro similarità attraverso aggiornamenti graduali del gradiente:</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, dove $C$ è il numero di co-occorrenze. Questo porta a un aumento significativo di $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$, promuovendo un allineamento locale più forte per queste coppie. Tuttavia, la perdita contrastiva distribuisce gli aggiornamenti del gradiente tra tutte le coppie token-patch, limitando la forza degli aggiornamenti per qualsiasi coppia specifica:</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>Questo impedisce un significativo rafforzamento delle similarità individuali token-patch.</p><h2 id=\"conclusion\">Conclusione</h2><p>Le visualizzazioni token-patch di CLIP sfruttano un allineamento incidentale ed emergente tra le rappresentazioni di testo e immagine. Questo allineamento, sebbene intrigante, deriva dall'addestramento contrastivo globale di CLIP e manca della robustezza strutturale necessaria per una spiegabilità precisa e affidabile. Le visualizzazioni risultanti mostrano spesso rumore e inconsistenza, limitando la loro utilità per applicazioni interpretative approfondite.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>I modelli a interazione tardiva come <strong>ColBERT</strong> e <strong>ColPali</strong> affrontano queste limitazioni incorporando architetturalmente allineamenti espliciti e dettagliati tra token di testo e patch di immagini. Elaborando le modalità in modo indipendente ed eseguendo calcoli di similarità mirati in una fase successiva, questi modelli garantiscono che ogni token di testo sia significativamente associato alle regioni dell'immagine pertinenti.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP può visualizzare le similitudini token-patch, tuttavia, si tratta più di un trucco di interpretabilità post-hoc che di una \"attenzione\" robusta o ufficiale del modello. Ecco perché.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
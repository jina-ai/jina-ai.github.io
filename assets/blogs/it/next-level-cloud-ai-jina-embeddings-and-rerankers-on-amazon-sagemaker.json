{
  "slug": "next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker",
  "id": "65fabb91502fd000011c667e",
  "uuid": "45cd5187-838d-46b7-a8a0-d890fcda9041",
  "title": "IA sul Cloud di Nuova Generazione: Jina Embeddings e Rerankers su Amazon SageMaker",
  "html": "<p><a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> e <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io\">Jina Reranker</a> sono ora disponibili per l'uso con <a href=\"https://aws.amazon.com/pm/sagemaker/?ref=jina-ai-gmbh.ghost.io\">Amazon SageMaker</a> dall'<a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>. Per gli utenti enterprise che attribuiscono grande importanza alla sicurezza, all'affidabilità e alla coerenza nelle loro operazioni cloud, questo mette l'intelligenza artificiale all'avanguardia di Jina AI nei loro deployment AWS privati, dove possono godere di tutti i vantaggi dell'infrastruttura consolidata e stabile di AWS.</p><p>Con la nostra gamma completa di modelli di embedding e reranking su AWS Marketplace, gli utenti SageMaker possono sfruttare le rivoluzionarie finestre di contesto da 8k e gli embedding multilingue ai vertici della classifica su richiesta a prezzi competitivi. Non è necessario pagare per trasferire i modelli dentro o fuori da AWS, i prezzi sono trasparenti e la fatturazione è integrata con il proprio account AWS.</p><p>I modelli attualmente disponibili su Amazon SageMaker includono:</p><ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6w6k6ckusixpw?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Small - English</a></li><li>Modelli Bilingue Jina Embeddings v2:<ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-dz3ubvmivnwry?ref=jina-ai-gmbh.ghost.io\">Tedesco/Inglese</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-hxalozh37jka4?ref=jina-ai-gmbh.ghost.io\">Cinese/Inglese</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-rnb324fpie3n6?ref=jina-ai-gmbh.ghost.io\">Spagnolo/Inglese</a></li></ul></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-tk7t7bz6fp5ng?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - Code</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\">Jina Reranker v1 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6kxbf5xqrluf4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT v1 - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-mgomngrh4c4k4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT Reranker v1 - English</a></li></ul><p>Per l'elenco completo dei modelli, consulta la <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">pagina del fornitore Jina AI su AWS Marketplace</a> e approfitta della prova gratuita di sette giorni.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Questo articolo ti guiderà nella creazione di un'applicazione di <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\">Retrieval-augmented generation</a> (RAG) utilizzando esclusivamente componenti di Amazon SageMaker. I modelli che useremo sono <strong>Jina Embeddings v2 - English</strong>, <strong>Jina Reranker v1</strong> e il modello linguistico di grandi dimensioni <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\">Mistral-7B-Instruct</a>.</p><p>Puoi anche seguire con un Python Notebook, che puoi <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">scaricare</a> o <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">eseguire su Google Colab</a>.</p><h2 id=\"retrieval-augmented-generation\">Retrieval-Augmented Generation</h2><p>Il Retrieval-augmented generation è un paradigma alternativo nell'AI generativa. Invece di utilizzare i large language model (LLM) per rispondere direttamente alle richieste degli utenti con ciò che hanno appreso durante l'addestramento, sfrutta la loro fluida produzione linguistica mentre sposta la logica e il recupero delle informazioni su un apparato esterno più adatto a questo scopo.</p><p>Prima di invocare un LLM, i sistemi RAG recuperano attivamente informazioni pertinenti da una fonte di dati esterna e le forniscono all'LLM come parte del suo prompt. Il ruolo dell'LLM è sintetizzare le informazioni esterne in una risposta coerente alle richieste degli utenti, minimizzando il rischio di allucinazione e aumentando la rilevanza e l'utilità del risultato.</p><p>Un sistema RAG schematicamente ha almeno quattro componenti:</p><ul><li>Una fonte di dati, tipicamente un database vettoriale di qualche tipo, adatto al recupero di informazioni assistito dall'AI.</li><li>Un sistema di recupero informazioni che tratta la richiesta dell'utente come una query e recupera i dati pertinenti per rispondere.</li><li>Un sistema, spesso includente un reranker basato su AI, che seleziona alcuni dei dati recuperati e li elabora in un prompt per un LLM.</li><li>Un LLM, per esempio uno dei modelli GPT o un LLM open-source come quello di Mistral, che prende la richiesta dell'utente e i dati forniti e genera una risposta per l'utente.</li></ul><p>I modelli di embedding sono ben adatti al recupero di informazioni e vengono spesso utilizzati per questo scopo. Un modello di embedding testuale prende i testi come input e produce un <a href=\"https://jina.ai/news/how-embeddings-drive-ai-a-guide?ref=jina-ai-gmbh.ghost.io\">embedding</a> — un vettore ad alta dimensionalità — la cui relazione spaziale con altri embedding è indicativa della loro similarità semantica, cioè argomenti, contenuti e significati correlati. Vengono spesso utilizzati nel recupero di informazioni perché più gli embedding sono vicini, più è probabile che l'utente sia soddisfatto della risposta. Sono anche relativamente facili da fine-tuning per migliorare le loro prestazioni in domini specifici.</p><p>I modelli di <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">text reranker</a> utilizzano principi di AI simili per confrontare collezioni di testi con una query e ordinarli per similarità semantica. L'utilizzo di un modello di reranker specifico per il compito, invece di affidarsi solo a un modello di embedding, spesso aumenta drasticamente la precisione dei risultati di ricerca. Il reranker in un'applicazione RAG seleziona alcuni dei risultati del recupero informazioni per massimizzare la probabilità che le informazioni corrette siano nel prompt per l'LLM.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class=\"kg-bookmark-description\">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png\" alt=\"\"></div></a></figure><h2 id=\"benchmarking-performance-of-embedding-models-as-sagemaker-endpoints\"><strong>Benchmark delle Prestazioni dei Modelli di Embedding come Endpoint SageMaker</strong></h2><p>Abbiamo testato le prestazioni e l'affidabilità del modello <strong>Jina Embeddings v2 Base - English</strong> come endpoint SageMaker, eseguito su un'istanza <a href=\"https://aws.amazon.com/ec2/instance-types/g4/?ref=jina-ai-gmbh.ghost.io\">g4dn.xlarge</a>. In questi esperimenti, abbiamo continuamente generato un nuovo utente ogni secondo, ognuno dei quali inviava una richiesta, attendeva la risposta e ripeteva dopo averla ricevuta.</p><ul><li>Per richieste di <em>meno di 100 token</em>, fino a 150 utenti concorrenti, i tempi di risposta <em>per richiesta</em> sono rimasti sotto i 100ms. Poi, i tempi di risposta sono aumentati linearmente da 100ms a 1500ms con l'aggiunta di più utenti concorrenti.<ul><li>A circa <em>300 utenti concorrenti</em>, abbiamo ricevuto più di 5 errori dall'API e abbiamo terminato il test.</li></ul></li><li>Per richieste tra 1K e 8K token, fino a 20 utenti concorrenti, i tempi di risposta <em>per richiesta</em> sono rimasti sotto gli 8s. Poi, i tempi di risposta sono aumentati linearmente da 8s a 60s con l'aggiunta di più utenti concorrenti.<ul><li>A circa <em>140 utenti concorrenti</em>, abbiamo ricevuto più di 5 errori dall'API e abbiamo terminato il test.</li></ul></li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png\" class=\"kg-image\" alt=\"Four comparative graphs displaying &quot;Small Context&quot; versus &quot;Large Context&quot; results over time, assessing performance metrics.\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/03/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Performance durante i test (sinistra: contesto piccolo, destra: contesto grande), che mostra l'effetto dell'aumento degli utenti nel tempo sui tempi di risposta e sui tassi di errore.</span></figcaption></figure><p>Sulla base di questi risultati, possiamo concludere che per la maggior parte degli utenti con carichi di lavoro di embedding normali, le istanze g4dn.xlarge o g5.xlarge dovrebbero soddisfare le loro esigenze quotidiane. Tuttavia, per lavori di <em>indicizzazione</em> di grandi dimensioni, che vengono tipicamente eseguiti molto meno frequentemente rispetto alle attività di <em>ricerca</em>, gli utenti potrebbero preferire un'opzione più performante. Per un elenco di tutte le istanze Sagemaker disponibili, fare riferimento alla panoramica di AWS su <a href=\"https://aws.amazon.com/ec2/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">EC2</a>.</p><h2 id=\"configure-your-aws-account\">Configura il tuo account AWS</h2><p>Prima di tutto, avrai bisogno di un account AWS. Se non sei già un utente AWS, puoi <a href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\">registrarti</a> per un account sul sito web AWS.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Console - Signup</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://portal.aws.amazon.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Signup</span></div></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Non potrai completare questo tutorial con un account Free Tier perché Amazon non fornisce accesso gratuito a SageMaker. Devi aggiungere un metodo di pagamento all'account per sottoscrivere i modelli di Jina AI, anche se utilizzi la nostra <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">prova gratuita di sette giorni</a>.</div></div><h3 id=\"set-up-aws-tools-in-your-python-environment\">Configura gli strumenti AWS nel tuo ambiente Python</h3><p>Installa nel tuo ambiente Python gli strumenti e le librerie AWS necessari per questo tutorial:</p><pre><code class=\"language-bash\">pip install awscli jina-sagemaker\n</code></pre><p>Dovrai ottenere una chiave di accesso e una chiave di accesso segreta per il tuo account AWS. Per farlo, segui le <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\">istruzioni sul sito web AWS</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Managing access keys for IAM users - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Create, modify, view, or update access keys (credentials) for programmatic calls to AWS.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/security-credentials-user.shared.console.png\" alt=\"\"></div></a></figure><p>Dovrai anche scegliere una <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\">regione AWS</a> in cui lavorare.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Regions, Availability Zones, and Local Zones - Amazon Relational Database Service</div><div class=\"kg-bookmark-description\">Learn how Amazon cloud computing resources are hosted in multiple locations world-wide, including AWS Regions and Availability Zones.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Relational Database Service</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/Con-AZ-Local.png\" alt=\"\"></div></a></figure><p>Quindi, imposta i valori nelle variabili d'ambiente. In Python o in un notebook Python, puoi farlo con il seguente codice:</p><pre><code class=\"language-bash\">import os\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = &lt;YOUR_ACCESS_KEY_ID&gt;\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = &lt;YOUR_SECRET_ACCESS_KEY&gt;\nos.environ[\"AWS_DEFAULT_REGION\"] = &lt;YOUR_AWS_REGION&gt;\nos.environ[\"AWS_DEFAULT_OUTPUT\"] = \"json\"\n</code></pre><p>Imposta l'output predefinito su <code>json</code>.</p><p>Puoi farlo anche tramite l'<a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html?ref=jina-ai-gmbh.ghost.io\">applicazione da riga di comando AWS</a> o configurando un <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html?ref=jina-ai-gmbh.ghost.io\">file di configurazione AWS</a> sul tuo filesystem locale. Consulta la <a href=\"https://docs.aws.amazon.com/index.html?ref=jina-ai-gmbh.ghost.io\">documentazione sul sito web AWS</a> per ulteriori dettagli.</p><h3 id=\"create-a-role\">Crea un Ruolo</h3><p>Avrai anche bisogno di un <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\">ruolo AWS</a> con permessi sufficienti per utilizzare le risorse richieste per questo tutorial.</p><p>Questo ruolo deve:</p><ol><li>Avere <strong>AmazonSageMakerFullAccess</strong> abilitato.</li><li>O:<ol><li>Avere l'autorità di effettuare sottoscrizioni AWS Marketplace e avere abilitato tutti e tre:<ol><li><strong>aws-marketplace:ViewSubscriptions</strong></li><li><strong>aws-marketplace:Unsubscribe</strong></li><li><strong>aws-marketplace:Subscribe</strong></li></ol></li><li>Oppure il tuo account AWS deve avere una sottoscrizione a <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">jina-embedding-model</a>.</li></ol></li></ol><p>Memorizza l'ARN (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html?ref=jina-ai-gmbh.ghost.io\">Amazon Resource Name</a>) del ruolo nella variabile <code>role</code>:</p><pre><code class=\"language-python\">role = &lt;YOUR_ROLE_ARN&gt;\n</code></pre><p>Consulta la documentazione per i ruoli sul sito web AWS per maggiori informazioni.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">IAM roles - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Learn how and when to use IAM roles.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div></a></figure><h3 id=\"subscribe-to-jina-ai-models-on-aws-marketplace\">Sottoscrivi i Modelli Jina AI su AWS Marketplace</h3><p>In questo articolo, useremo il modello Jina Embeddings v2 base English. Sottoscrivilo su <a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Embeddings v2 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Vedrai le informazioni sui prezzi scorrendo verso il basso nella pagina. AWS addebita i modelli del marketplace su base oraria, quindi ti verrà fatturato il tempo che intercorre tra l'avvio dell'endpoint del modello e il suo arresto. Questo articolo ti mostrerà come fare entrambe le cose.</p><p>Useremo anche il modello Jina Reranker v1 - English, al quale dovrai sottoscriverti.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Reranker v1 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">Jina AI sta attualmente offrendo una prova gratuita di sette giorni dei suoi modelli. Dovrai comunque pagare per le istanze AWS che li eseguono, ma durante il periodo di prova non dovrai pagare costi aggiuntivi per i modelli.</div></div><p>Una volta sottoscritto l'abbonamento, ottieni gli ARN dei modelli per la tua regione AWS e memorizzali nelle variabili <code>embedding_package_arn</code> e <code>reranker_package_arn</code> rispettivamente. Il codice in questo tutorial farà riferimento ad essi usando questi nomi di variabili.</p><p>Se non sai come ottenere gli ARN, inserisci il nome della tua regione Amazon nella variabile <code>region</code> e usa il seguente codice:</p><pre><code class=\"language-python\">region = os.environ[\"AWS_DEFAULT_REGION\"]\n\ndef get_arn_for_model(region_name, model_name):\n    model_package_map = {\n        \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n        \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n        \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n        \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n        \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n        \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n        \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n        \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n        \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n        \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n        \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n        \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n        \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n        \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n        \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n        \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n    }\n\n    return model_package_map[region_name]\n\nembedding_package_arn = get_arn_for_model(region, \"jina-embeddings-v2-base-en\")\nreranker_package_arn = get_arn_for_model(region, \"jina-reranker-v1-base-en\")\n</code></pre><h2 id=\"load-the-dataset\">Caricamento del Dataset</h2><p>In questo tutorial, utilizzeremo una collezione di video forniti dal canale YouTube <a href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\">TU Delft Online Learning</a>. Questo canale produce vari materiali didattici in materie STEM. La sua programmazione è sotto licenza <a href=\"https://creativecommons.org/licenses/by/3.0/legalcode?ref=jina-ai-gmbh.ghost.io\">CC-BY</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TU Delft Online Learning</div><div class=\"kg-bookmark-description\">Stai cercando di fare carriera nella scienza, nel design o nell'ingegneria? Allora unisciti alla comunità di studenti online della TU Delft!\nAlla TU Delft, l'apprendimento online significa apprendimento attivo. I nostri corsi sono progettati per fornirti un'esperienza di apprendimento coinvolgente. Il contenuto del corso è stimolante e impegnativo, promuovendo la tua crescita personale e lo sviluppo professionale, mentre godi della flessibilità e accessibilità che i nostri corsi online offrono così puoi combinare l'apprendimento con altre priorità della tua vita. Inizia a imparare oggi: https://online-learning.tud…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/s/desktop/4feff1e2/img/favicon_144x144.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://yt3.googleusercontent.com/ytc/AIdro_kH5d18Xqqj-MKv9k_tf2KNFufCpMY8qEXdQzEy=s900-c-k-c0x00ffffff-no-rj\" alt=\"\"></div></a></figure><p>Abbiamo scaricato 193 video dal canale e li abbiamo elaborati con il modello di riconoscimento vocale open-source <a href=\"https://openai.com/research/whisper?ref=jina-ai-gmbh.ghost.io\">Whisper di OpenAI</a>. Abbiamo utilizzato il modello più piccolo <a href=\"https://huggingface.co/openai/whisper-tiny?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>openai/whisper-tiny</code></a> per processare i video in trascrizioni.</p><p>Le trascrizioni sono state organizzate in un file CSV, che puoi <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/tu_delft.csv?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">scaricare da qui</a>.</p><p>Ogni riga del file contiene:</p><ul><li>Il titolo del video</li><li>L'URL del video su YouTube</li><li>Una trascrizione testuale del video</li></ul><p>Per caricare questi dati in Python, prima installa <code>pandas</code> e <code>requests</code>:</p><pre><code class=\"language-bash\">pip install requests pandas\n</code></pre><p>Carica i dati CSV direttamente in un DataFrame Pandas chiamato <code>tu_delft_dataframe</code>:</p><pre><code class=\"language-python\">import pandas\n\n# Carica il file CSV\ntu_delft_dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/jina-ai/workshops/feat-sagemaker-post/notebooks/embeddings/sagemaker/tu_delft.csv\")\n</code></pre><p>Puoi ispezionare il contenuto usando il metodo <code>head()</code> del DataFrame. In un notebook, dovrebbe apparire più o meno così:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png\" class=\"kg-image\" alt=\"Data frame che mostra titoli di webinar come &quot;Green Teams in Hospitals,&quot; con i loro URL YouTube ed estratti di testo introduttivi,\" loading=\"lazy\" width=\"1440\" height=\"580\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-15-at-14.30.35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1440w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Puoi anche guardare i video utilizzando gli URL forniti in questo dataset e verificare che il riconoscimento vocale sia imperfetto ma fondamentalmente corretto.</p><h2 id=\"start-the-jina-embeddings-v2-endpoint\">Avvio dell'Endpoint Jina Embeddings v2</h2><p>Il codice seguente avvierà un'istanza di <code>ml.g4dn.xlarge</code> su AWS per eseguire il modello di embedding. Potrebbe richiedere diversi minuti per completarsi.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Scegli un nome per il tuo endpoint di embedding. Può essere qualsiasi cosa conveniente.\nembeddings_endpoint_name = \"jina_embedding\"\n\nembedding_client = Client(region_name=boto3.Session().region_name)\nembedding_client.create_endpoint(\n    arn=embedding_package_arn,\n    role=role,\n    endpoint_name=embeddings_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nembedding_client.connect_to_endpoint(endpoint_name=embeddings_endpoint_name)\n</code></pre><p>Modifica <code>instance_type</code> per selezionare un diverso tipo di istanza cloud AWS se appropriato.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">AWS ti addebiterà per il tuo tempo a partire da quando questo comando restituisce il risultato. Verrai fatturato all'ora fino a quando non fermerai questa istanza. Per farlo, segui le istruzioni nella sezione <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Spegnimento</strong></b></a>.</div></div><h2 id=\"build-and-index-the-dataset\">Costruzione e Indicizzazione del Dataset</h2><p>Ora che abbiamo caricato i dati e stiamo eseguendo un modello Jina Embeddings v2, possiamo preparare e indicizzare i dati. Memorizzeremo i dati in un <a href=\"https://faiss.ai/index.html?ref=jina-ai-gmbh.ghost.io\">vector store FAISS</a>, un database vettoriale open-source specificamente progettato per applicazioni AI.</p><p>Prima, installa i prerequisiti rimanenti per la nostra applicazione RAG:</p><pre><code class=\"language-bash\">pip install tdqm numpy faiss-cpu\n</code></pre><h3 id=\"chunking\">Suddivisione in Chunk</h3><p>Dovremo prendere le singole trascrizioni e dividerle in parti più piccole, cioè \"chunk\", in modo da poter inserire più testi in un prompt per l'LLM. Il codice seguente divide le singole trascrizioni sui confini delle frasi, assicurando che tutti i chunk non abbiano più di 128 parole per impostazione predefinita.</p><pre><code class=\"language-python\">def chunk_text(text, max_words=128):\n    \"\"\"\n    Divide text into chunks where each chunk contains the maximum number \n    of full sentences with fewer words than `max_words`.\n    \"\"\"\n    sentences = text.split(\".\")\n    chunk = []\n    word_count = 0\n\n    for sentence in sentences:\n        sentence = sentence.strip(\".\")\n        if not sentence:\n          continue\n\n        words_in_sentence = len(sentence.split())\n        if word_count + words_in_sentence &lt;= max_words:\n            chunk.append(sentence)\n            word_count += words_in_sentence\n        else:\n            # Yield the current chunk and start a new one\n            if chunk:\n              yield \". \".join(chunk).strip() + \".\"\n            chunk = [sentence]\n            word_count = words_in_sentence\n\n    # Yield the last chunk if it's not empty\n    if chunk:\n        yield \" \".join(chunk).strip() + \".\"</code></pre><h3 id=\"get-embeddings-for-each-chunk\">Ottieni gli Embedding per Ogni Chunk</h3><p>Abbiamo bisogno di un embedding per ogni chunk da memorizzare nel database FAISS. Per ottenerli, passiamo i chunk di testo all'endpoint del modello di embedding Jina AI, usando il metodo <code>embedding_client.embed()</code>. Quindi, aggiungiamo i chunk di testo e i vettori di embedding al dataframe pandas <code>tu_delft_dataframe</code> come nuove colonne <code>chunks</code> ed <code>embeddings</code>:</p><pre><code class=\"language-python\">import numpy as np\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\ndef generate_embeddings(text_df):\n    chunks = list(chunk_text(text_df[\"Text\"]))\n    embeddings = []\n\n    for i, chunk in enumerate(chunks):\n      response = embedding_client.embed(texts=[chunk])\n      chunk_embedding = response[0][\"embedding\"]\n      embeddings.append(np.array(chunk_embedding))\n\n    text_df[\"chunks\"] = chunks\n    text_df[\"embeddings\"] = embeddings\n    return text_df\n\nprint(\"Embedding text chunks ...\")\ntu_delft_dataframe = generate_embeddings(tu_delft_dataframe)\n## se stai usando Google Colab o un notebook Python, puoi\n## cancellare la riga sopra e decommentare la seguente riga:\n# tu_delft_dataframe = tu_delft_dataframe.progress_apply(generate_embeddings, axis=1)\n</code></pre><h3 id=\"set-up-semantic-search-using-faiss\">Configura la Ricerca Semantica Usando Faiss</h3><p>Il codice seguente crea un database FAISS e inserisce i chunk e i vettori di embedding iterando su <code>tu_delft_pandas</code>:</p><pre><code class=\"language-python\">import faiss\n\ndim = 768  # dimensione degli embedding Jina v2\nindex_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\nk = 0\n\ndoc_ref = dict()\n\nfor idx, row in tu_delft_dataframe.iterrows():\n    embeddings = row[\"embeddings\"]\n    for i, embedding in enumerate(embeddings):\n        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype=\"float32\").reshape(1, -1))\n        faiss.normalize_L2(normalized_embedding)\n        index_with_ids.add_with_ids(normalized_embedding, k)\n        doc_ref[k] = (row[\"chunks\"][i], idx)\n        k += 1\n</code></pre><h2 id=\"start-the-jina-reranker-v1-endpoint\">Avvia l'Endpoint Jina Reranker v1</h2><p>Come per il modello Jina Embedding v2 sopra, questo codice lancerà un'istanza di <code>ml.g4dn.xlarge</code> su AWS per eseguire il modello reranker. Analogamente, potrebbe richiedere diversi minuti per l'esecuzione.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Scegli un nome per il tuo endpoint reranker. Può essere qualsiasi cosa conveniente.\nreranker_endpoint_name = \"jina_reranker\"\n\nreranker_client = Client(region_name=boto3.Session().region_name)\nreranker_client.create_endpoint(\n    arn=reranker_package_arn,\n    role=role,\n    endpoint_name=reranker_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nreranker_client.connect_to_endpoint(endpoint_name=reranker_endpoint_name)\n</code></pre><h2 id=\"define-query-functions\">Definisci le Funzioni di Query</h2><p>Successivamente, definiremo una funzione che identifica i chunk di trascrizione più simili a qualsiasi query di testo.</p><p>Questo è un processo in due fasi:</p><ol><li>Convertire l'input dell'utente in un vettore di embedding usando il metodo <code>embedding_client.embed()</code>, proprio come abbiamo fatto nella fase di preparazione dei dati.</li><li>Passare l'embedding all'indice FAISS per recuperare le corrispondenze migliori. Nella funzione seguente, il default è restituire le 20 migliori corrispondenze, ma puoi controllare questo con il parametro <code>n</code>.</li></ol><p>La funzione <code>find_most_similar_transcript_segment</code> restituirà le migliori corrispondenze confrontando i coseni degli embedding memorizzati con l'embedding della query.</p><pre><code class=\"language-python\">def find_most_similar_transcript_segment(query, n=20):\n    query_embedding = embedding_client.embed(texts=[query])[0][\"embedding\"]  # Assumendo che la query sia abbastanza breve da non necessitare di chunking\n    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype=\"float32\").reshape(1, -1))\n    faiss.normalize_L2(query_embedding)\n\n    D, I = index_with_ids.search(query_embedding, n)  # Ottieni le prime n corrispondenze\n\n    results = []\n    for i in range(n):\n        distance = D[0][i]\n        index_id = I[0][i]\n        transcript_segment, doc_idx = doc_ref[index_id]\n        results.append((transcript_segment, doc_idx, distance))\n\n    # Ordina i risultati per distanza\n    results.sort(key=lambda x: x[2])\n\n    return [(tu_delft_dataframe.iloc[r[1]][\"Title\"].strip(), r[0]) for r in results]\n</code></pre><p>Definiremo anche una funzione che accede all'endpoint reranker <code>reranker_client</code>, gli passa i risultati da <code>find_most_similar_transcript_segment</code>, e restituisce solo i tre risultati più rilevanti. Chiama l'endpoint reranker con il metodo <code>reranker_client.rerank()</code>.</p><pre><code class=\"language-python\">def rerank_results(query_found, query, n=3):\n    ret = reranker_client.rerank(\n        documents=[f[1] for f in query_found], \n        query=query, \n        top_n=n,\n    )\n    return [query_found[r['index']] for r in ret[0]['results']]\n</code></pre><h2 id=\"use-jumpstart-to-load-mistral-instruct\">Usa JumpStart per Caricare Mistral-Instruct</h2><p>Per questo tutorial, useremo il modello <code>mistral-7b-instruct</code>, che è <a href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\">disponibile tramite Amazon SageMaker JumpStart</a>, come parte LLM del sistema RAG.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">I modelli di fondazione Mistral 7B di Mistral AI sono ora disponibili in Amazon SageMaker JumpStart | Amazon Web Services</div><div class=\"kg-bookmark-description\">Oggi siamo entusiasti di annunciare che i modelli di fondazione Mistral 7B, sviluppati da Mistral AI, sono disponibili per i clienti attraverso Amazon SageMaker JumpStart per essere distribuiti con un clic per l'inferenza. Con 7 miliardi di parametri, Mistral 7B può essere facilmente personalizzato e distribuito rapidamente. Puoi provare questo modello con SageMaker JumpStart, un [...]</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://a0.awsstatic.com/main/images/site/touch-icon-ipad-144-smile.png\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Web Services</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/10/09/mistral-7b-sagemaker-jumpstart.jpg\" alt=\"\"></div></a></figure><p>Esegui il seguente codice per caricare e distribuire Mistral-Instruct:</p><pre><code class=\"language-python\">from sagemaker.jumpstart.model import JumpStartModel\n\njumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\nmodel_predictor = jumpstart_model.deploy()\n</code></pre><p>L'endpoint per accedere a questo LLM è memorizzato nella variabile <code>model_predictor</code>.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">L'utilizzo di questo modello è anche un servizio a pagamento per AWS, quindi non dimenticare di spegnerlo quando hai finito con questo tutorial. Vedi la sezione <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Spegnimento</strong></b></a> per arrestare questa distribuzione al termine.</div></div><h3 id=\"mistral-instruct-with-jumpstart\">Mistral-Instruct con JumpStart</h3><p>Di seguito è riportato il codice per creare un template di prompt per Mistral-Instruct per questa applicazione utilizzando <a href=\"https://docs.python.org/3/library/string.html?ref=jina-ai-gmbh.ghost.io#template-strings\">la classe template di stringhe integrata di Python</a>. Assume che per ogni query ci siano tre chunk di trascrizione corrispondenti che verranno presentati al modello.</p><p>Puoi sperimentare con questo template per modificare questa applicazione o vedere se puoi ottenere risultati migliori.</p><pre><code class=\"language-python\">from string import Template\n\nprompt_template = Template(\"\"\"\n  &lt;s&gt;[INST] Rispondi alla domanda seguente utilizzando solo il contesto fornito.\n  La domanda dell'utente si basa su trascrizioni di video da un canale YouTube.\n  Il contesto è presentato come un elenco ordinato di informazioni nella forma\n    (titolo-video, segmento-trascrizione), rilevante per rispondere alla\n    domanda dell'utente.\n  La risposta deve utilizzare solo il contesto presentato. Se la domanda non può\n    essere risposta in base al contesto, dillo.\n\n  Contesto:\n  1. Titolo-video: $title_1, segmento-trascrizione: $segment_1\n  2. Titolo-video: $title_2, segmento-trascrizione: $segment_2\n  3. Titolo-video: $title_3, segmento-trascrizione: $segment_3\n\n  Domanda: $question\n\n  Risposta: [/INST]\n\"\"\")\n</code></pre><p>Con questo componente in posizione, ora abbiamo tutte le parti di un'applicazione RAG completa.</p><h2 id=\"querying-the-model\">Interrogare il Modello</h2><p>L'interrogazione del modello è un processo in tre fasi.</p><ol><li>Cercare chunk rilevanti data una query.</li><li>Assemblare il prompt.</li><li>Inviare il prompt al modello Mistral-Instruct e restituire la sua risposta.</li></ol><p>Per cercare chunk rilevanti, usiamo la funzione <code>find_most_similar_transcript_segment</code> che abbiamo definito sopra.</p><pre><code class=\"language-python\">question = \"Quando è stato commissionato il primo parco eolico offshore?\"\nsearch_results = find_most_similar_transcript_segment(question)\nreranked_results = rerank_results(search_results, question)\n</code></pre><p>È possibile ispezionare i risultati della ricerca in ordine di riallineamento:</p><pre><code class=\"language-python\">for title, text, _ in reranked_results:\n    print(title + \"\\n\" + text + \"\\n\")\n</code></pre><p>Risultato:</p><pre><code class=\"language-text\">Offshore Wind Farm Technology - Course Introduction\nSince the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n\nOffshore Wind Farm Technology - Course Introduction\nI'm a researcher and lecturer at the Wind Energy and Economics Department and I will be your moderator throughout this course.  That means I will answer any questions you may have.  I'll strengthen the interactions between the participants and also I'll get you in touch with the lecturers when needed.  The course is mainly developed for professionals in the field of offshore wind energy.  We want to broaden their knowledge of the relevant technical disciplines and their integration.  Professionals with a scientific background who are new to the field of offshore wind energy will benefit from a high-level insight into the engineering aspects of wind energy.  Overall, the course will help you make the right choices during the development and operation of offshore wind farms.\n\nOffshore Wind Farm Technology - Course Introduction\nDesigned wind turbines that better withstand wind, wave and current loads  Identify great integration strategies for offshore wind turbines and gain understanding of the operational and maintenance of offshore wind turbines and farms  We also hope that you will benefit from the course and from interaction with other learners who share your interest in wind energy  And therefore we look forward to meeting you online.\n</code></pre><p>Possiamo utilizzare queste informazioni direttamente nel template del prompt:</p><pre><code class=\"language-python\">prompt_for_llm = prompt_template.substitute(\n    question = question,\n    title_1 = search_results[0][0],\n    segment_1 = search_results[0][1],\n    title_2 = search_results[1][0],\n    segment_2 = search_results[1][1],\n    title_3 = search_results[2][0],\n    segment_3 = search_results[2][1],\n)\n</code></pre><p>Stampa la stringa risultante per vedere quale prompt viene effettivamente inviato al LLM:</p><pre><code class=\"language-python\">print(prompt_for_llm)\n</code></pre><pre><code class=\"language-text\">&lt;s&gt;[INST] Answer the question below only using the given context.\n  The question from the user is based on transcripts of videos from a YouTube\n    channel.\n  The context is presented as a ranked list of information in the form of\n    (video-title, transcript-segment), that is relevant for answering the\n    user's question.\n  The answer should only use the presented context. If the question cannot be\n    answered based on the context, say so.\n\n  Context:\n  1. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n  2. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: For example, we look at how to characterize the wind and wave conditions at a given location.  How to best place the wind turbines in a farm and also how to retrieve the electricity back to shore.  We look at the main design drivers for offshore wind turbines and their components.  We'll see how these aspects influence one another and the best choices to reduce the cost of energy.  This course is organized by the two-delfd wind energy institute, an interfaculty research organization focusing specifically on wind energy.  You will therefore benefit from the expertise of the lecturers in three different faculties of the university.  Aerospace engineering, civil engineering and electrical engineering.  Hi, my name is Ricardo Pareda.\n  3. Video-title: Systems Analysis for Problem Structuring part 1B the mono actor perspective example, transcript-segment: So let's assume the demarcation of the problem and the analysis of objectives has led to the identification of three criteria.  The security of supply, the percentage of offshore power generation and the costs of energy provision.  We now reason backwards to explore what factors have an influence on these system outcomes.  Really, the offshore percentage is positively influenced by the installed Wind Power capacity at sea, a key system factor.  Capacity at sea in turn is determined by both the size and the number of wind farms at sea.  The Ministry of Economic Affairs cannot itself invest in new wind farms but hopes to simulate investors and energy companies by providing subsidies and by expediting the granting process of licenses as needed.\n\n  Question: When was the first offshore wind farm commissioned?\n\n  Answer: [/INST]\n</code></pre><p>Passa questo prompt all'endpoint LLM — <code>model_predictor</code> — tramite il metodo <code>model_predictor.predict()</code>:</p><pre><code class=\"language-python\">answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n</code></pre><p>Questo restituisce una lista, ma poiché abbiamo passato un solo prompt, sarà una lista con una sola voce. Ogni voce è un <code>dict</code> con il testo della risposta sotto la chiave <code>generated_text</code>:</p><pre><code class=\"language-python\">answer = answer[0]['generated_text']\nprint(answer)\n</code></pre><p>Risultato:</p><pre><code class=\"language-text\">The first offshore wind farm was commissioned in 1991. (Context: Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, ...)\n</code></pre><p>Semplifichiamo l'interrogazione scrivendo una funzione che esegua tutti i passaggi: prendendo la domanda come stringa come parametro e restituendo la risposta come stringa:</p><pre><code class=\"language-python\">def ask_rag(question):\n    search_results = find_most_similar_transcript_segment(question)\n    reranked_results = rerank_results(search_results, question)\n    prompt_for_llm = prompt_template.substitute(\n        question = question,\n        title_1 = search_results[0][0],\n        segment_1 = search_results[0][1],\n        title_2 = search_results[1][0],\n        segment_2 = search_results[1][1],\n        title_3 = search_results[2][0],\n        segment_3 = search_results[2][1],\n    )\n    answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n    return answer[0][\"generated_text\"]\n</code></pre><p>Ora possiamo fargli alcune domande in più. Le risposte dipenderanno dal contenuto delle trascrizioni video. Ad esempio, possiamo fare domande dettagliate quando la risposta è presente nei dati e ottenere una risposta:</p><pre><code class=\"language-python\">ask_rag(\"What is a Kaplan Meyer estimator?\")\n</code></pre><pre><code class=\"language-text\">The Kaplan Meyer estimator is a non-parametric estimator for the survival \nfunction, defined for both censored and not censored data. It is represented \nas a series of declining horizontal steps that approaches the truths of the \nsurvival function if the sample size is sufficiently large enough. The value \nof the empirical survival function obtained is assumed to be constant between \ntwo successive distinct observations.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"Who is Reneville Solingen?\")\n</code></pre><pre><code class=\"language-text\">Reneville Solingen is a professor at Delft University of Technology in Global \nSoftware Engineering. She is also a co-author of the book \"The Power of Scrum.\"\n</code></pre><pre><code class=\"language-python\">answer = ask_rag(\"What is the European Green Deal?\")\nprint(answer)\n</code></pre><pre><code class=\"language-text\">The European Green Deal is a policy initiative by the European Union to combat \nclimate change and decarbonize the economy, with a goal to make Europe carbon \nneutral by 2050. It involves the use of green procurement strategies in various \nsectors, including healthcare, to reduce carbon emissions and promote corporate \nsocial responsibility.\n</code></pre><p>Possiamo anche fare domande che sono al di fuori dell'ambito delle informazioni disponibili:</p><pre><code class=\"language-python\">ask_rag(\"What countries export the most coffee?\")\n</code></pre><pre><code class=\"language-text\">Based on the context provided, there is no clear answer to the user's \nquestion about which countries export the most coffee as the context \nonly discusses the Delft University's cafeteria discounts and sustainable \ncoffee options, as well as lithium production and alternatives for use in \nelectric car batteries.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\")\n</code></pre><pre><code class=\"language-text\">The context does not provide sufficient information to answer the question. \nThe context is about thermit welding of rails, stress concentration factors, \nand a lyrics video. There is no mention of woodchucks or the ability of \nwoodchuck to chuck wood in the context.\n</code></pre><p>Prova le tue query. Puoi anche modificare il modo in cui viene sollecitato il LLM per vedere se ciò migliora i tuoi risultati.</p><h2 id=\"shutting-down\">Arresto</h2><p>Poiché vieni fatturato a ore per i modelli che utilizzi e per l'infrastruttura AWS per eseguirli, è molto importante arrestare tutti e tre i modelli AI quando termini questo tutorial:</p><ul><li>L'endpoint del modello di embedding <code>embedding_client</code></li><li>L'endpoint del modello reranker <code>reranker_client</code></li><li>L'endpoint del modello linguistico di grandi dimensioni <code>model_predictor</code></li></ul><p>Per arrestare tutti e tre gli endpoint del modello, esegui il seguente codice:</p><pre><code class=\"language-python\"># shut down the embedding endpoint\nembedding_client.delete_endpoint()\nembedding_client.close()\n# shut down the reranker endpoint\nreranker_client.delete_endpoint()\nreranker_client.close()\n# shut down the LLM endpoint\nmodel_predictor.delete_model()\nmodel_predictor.delete_endpoint()\n</code></pre><h2 id=\"get-started-now-with-jina-ai-models-on-aws-marketplace\">Inizia ora con i modelli Jina AI su AWS Marketplace</h2><p>Con i nostri modelli di embedding e reranking su SageMaker, gli utenti enterprise di AI su AWS hanno ora accesso istantaneo alla straordinaria proposta di valore di Jina AI senza compromettere i benefici delle loro operazioni cloud esistenti. Tutta la sicurezza, l'affidabilità, la consistenza e i prezzi prevedibili di AWS sono integrati.</p><p>A Jina AI, stiamo lavorando duramente per portare lo stato dell'arte alle aziende che possono beneficiare dell'introduzione dell'AI nei loro processi esistenti. Ci sforziamo di offrire modelli solidi, affidabili e ad alte prestazioni a prezzi accessibili tramite interfacce convenienti e pratiche, minimizzando i vostri investimenti in AI mentre massimizziamo i vostri rendimenti.</p><p>Visita la <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">pagina di Jina AI su AWS Marketplace</a> per un elenco di tutti i modelli di embedding e reranker che offriamo e per provare i nostri modelli gratuitamente per sette giorni.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Ci piacerebbe conoscere i vostri casi d'uso e parlare di come i prodotti di Jina AI possano adattarsi alle esigenze del vostro business. Contattateci tramite <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\">il nostro sito web</a> o il nostro <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\">canale Discord</a> per condividere il vostro feedback e rimanere aggiornati sui nostri ultimi modelli.</p>",
  "comment_id": "65fabb91502fd000011c667e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/03/Blog-images--27-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-03-20T11:33:53.000+01:00",
  "updated_at": "2024-03-25T19:10:29.000+01:00",
  "published_at": "2024-03-25T16:00:51.000+01:00",
  "custom_excerpt": "Learn to use Jina Embeddings and Reranking models in a full-stack AI application on AWS, using only components available in Amazon SageMaker and the AWS Marketplace.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker/",
  "excerpt": "Impara a utilizzare i modelli Jina Embeddings e Reranking in un'applicazione AI full-stack su AWS, utilizzando solo componenti disponibili in Amazon SageMaker e nell'AWS Marketplace.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract image with colorful wavy background featuring AWS, Embeddings, and Reranker logos.",
  "feature_image_caption": null
}
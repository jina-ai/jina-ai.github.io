{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "Cosa √® Veramente il Late Chunking e Cosa Non Lo √®: Parte II",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Si raccomanda vivamente di <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">leggere prima la Parte I</a>, poich√© questo articolo offre una visione pi√π approfondita, concentrandosi su malintesi comuni e confronti. <b><strong style=\"white-space: pre-wrap;\">Ordine di lettura consigliato: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, parte II, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper di ricerca</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Molti casi d'uso richiedono il recupero di porzioni pi√π piccole di testo, e i sistemi di recupero basati su vettori densi spesso funzionano meglio con segmenti di testo pi√π brevi, poich√© √® meno probabile che la semantica venga sovra-compressa negli embedding. Di conseguenza, gli utenti spesso dividono i documenti di testo in chunk pi√π piccoli e li codificano separatamente. Tuttavia, gli embedding dei chunk creati in questo modo possono perdere informazioni contestuali dai chunk circostanti, risultando in rappresentazioni non ottimali. In questo paper, introduciamo un nuovo metodo chiamato late chunking, che sfrutta i modelli di embedding a lungo contesto per incorporare prima tutti i token del testo lungo, applicando il chunking dopo il modello transformer e appena prima del mean pooling - da qui il termine \"late\" nel nome. Gli embedding dei chunk risultanti catturano l'informazione contestuale completa, portando a risultati superiori in varie attivit√† di recupero. Il metodo √® abbastanza generico da essere applicato a un'ampia gamma di modelli di embedding a lungo contesto e funziona senza ulteriore training. Per aumentare ulteriormente l'efficacia del late chunking, proponiamo un approccio dedicato di fine-tuning per i modelli di embedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Il chunking di un documento lungo presenta due problemi: primo, <strong>determinare i punti di interruzione</strong>‚Äîovvero, come segmentare il documento. Si potrebbero considerare lunghezze fisse di token, un numero fisso di frasi, o tecniche pi√π avanzate come <a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">regex o modelli di segmentazione semantica</a>. I confini accurati dei chunk non solo migliorano la leggibilit√† dei risultati di ricerca, ma assicurano anche che i chunk forniti a un LLM in un sistema RAG siano precisi e sufficienti‚Äîn√© pi√π, n√© meno.</p><p>Il secondo problema √® la <strong>perdita di contesto</strong> all'interno di ogni chunk. Una volta che il documento √® segmentato, il passo logico successivo per la maggior parte delle persone √® incorporare ogni chunk separatamente in un processo batch. Tuttavia, questo porta a una perdita del contesto globale del documento originale. Molti lavori precedenti hanno affrontato prima il primo problema, sostenendo che una migliore rilevazione dei confini migliora la rappresentazione semantica. Per esempio, il \"semantic chunking\" raggruppa le frasi con alta similarit√† del coseno nello spazio degli embedding per minimizzare l'interruzione delle unit√† semantiche.</p><p>Dal nostro punto di vista, questi due problemi sono <em>quasi</em> ortogonali e possono essere affrontati separatamente. Se dovessimo dare una priorit√†, <strong>diremmo che il secondo problema √® pi√π critico.</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">Problema 2: <b>Informazioni contestuali</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">Preservate</td>\n<td>Perse</td>\n</tr>\n<tr>\n<td><b>Problema 1: Punti di interruzione</b></td>\n<td>Buoni</td>\n<td style=\"text-align:center\">Scenario ideale</td>\n<td>Risultati di ricerca scarsi</td>\n</tr>\n<tr>\n<td></td>\n<td>Scarsi</td>\n<td style=\"text-align:center\">Buoni risultati di ricerca, ma i risultati potrebbero non essere leggibili dall'uomo o per il ragionamento LLM</td>\n<td>Scenario peggiore</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"late-chunking-for-context-loss\">Late Chunking per la Perdita di Contesto</h2><p>Il <strong>late chunking</strong> inizia affrontando il secondo problema: la <strong>perdita di contesto</strong>. Non riguarda la ricerca dei punti di interruzione ideali o dei confini semantici. √à ancora necessario utilizzare regex, euristiche o altre tecniche per dividere un documento lungo in piccoli chunk. Ma invece di incorporare ogni chunk non appena viene segmentato, il late chunking prima codifica l'intero documento in una finestra di contesto (per <code>jina-embeddings-v3</code> √® di 8192 token). Poi, segue gli indizi dei confini per applicare il mean pooling per ogni chunk‚Äîda qui il termine \"late\" nel late chunking.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagram comparing &quot;Naive Chunking&quot; and &quot;Late Chunking&quot; methods for processing long documents with labeled steps.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Il late chunking richiede ancora indizi sui confini, ma la differenza chiave √® quando questi indizi vengono utilizzati. Nel late chunking, gli indizi vengono applicati solo dopo che l'intero documento √® stato incorporato, e vengono utilizzati per determinare l'intervallo di pooling.</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">Il Late Chunking √® Resiliente agli Indizi di Confine Scarsi</h2><p>Ci√≤ che √® davvero interessante √® che gli esperimenti mostrano che il late chunking elimina la necessit√† di confini semantici perfetti, il che affronta parzialmente il primo problema menzionato sopra. In effetti, il late chunking applicato a confini di token fissi supera il chunking ingenuo con indizi di confine semantici. I modelli di segmentazione semplici, come quelli che utilizzano confini a lunghezza fissa, funzionano alla pari con algoritmi avanzati di rilevamento dei confini quando abbinati al late chunking. Abbiamo testato tre diverse dimensioni di modelli di embedding, e i risultati mostrano che tutti ne beneficiano costantemente attraverso tutti i dataset di test. Detto questo, il modello di embedding stesso rimane il fattore pi√π significativo nelle prestazioni‚Äî<strong>non c'√® un singolo caso in cui un modello pi√π debole con late chunking superi un modello pi√π forte senza di esso.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Scatter plot chart showing the percentage of relative improvements across various models against a baseline, with a vertical \" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">Miglioramento relativo del recupero rispetto alla baseline (cio√® </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> con indizi di confine a lunghezza di token fissa e chunking ingenuo). Come parte di uno studio di ablazione, abbiamo testato il late chunking con diversi indizi di confine (lunghezza di token fissa, confini di frase e confini semantici) e diversi modelli (</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\">, e </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">). In base alle loro prestazioni su MTEB, il ranking di questi tre modelli di embedding √®: </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code>. Tuttavia, il focus di questo esperimento non √® sulla valutazione delle prestazioni dei modelli di embedding in s√©, ma sulla comprensione di come un modello di embedding migliore interagisce con il late chunking e gli indicatori di confine. Per i dettagli dell'esperimento, si prega di consultare il nostro paper di ricerca.</figcaption></figure>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n\n<p>Nota che essere resistente a confini inadeguati non significa che possiamo ignorarli - sono ancora importanti sia per la leggibilit√† umana che per gli LLM. Ecco come la vediamo: quando ottimizziamo la segmentazione, cio√® il suddetto primo problema, possiamo concentrarci completamente sulla leggibilit√† senza preoccuparci della perdita di semantica/contesto. Il Late Chunking gestisce i punti di interruzione buoni o cattivi, quindi la leggibilit√† √® l'unica cosa di cui devi preoccuparti.</p>\n\n<h2 id=\"late-chunking-is-bidirectional\">Il Late Chunking √® Bidirezionale</h2>\n\n<p>Un altro malinteso comune sul late chunking √® che i suoi embedding condizionali dei chunk si basino solo sui chunk precedenti senza \"guardare avanti\". Questo √® scorretto. La dipendenza condizionale nel <strong>late chunking √® in realt√† bi-direzionale</strong>, non uni-direzionale. Questo perch√© la matrice di attenzione nel modello di embedding - un transformer solo-encoder - √® completamente connessa, a differenza della matrice triangolare mascherata usata nei modelli auto-regressivi. Formalmente, l'embedding del chunk $k$, $v_k \\sim Q(c_k|D)$, piuttosto che $v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$, dove $Q$ denota una fattorizzazione del modello linguistico. Questo spiega anche perch√© il late chunking non dipende dal posizionamento preciso dei confini.</p>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagrams of a transformer model with detailed encoder on the left and decoder on the right, labeled with tokens, embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">A differenza dei modelli solo-decoder con self-attention mascherata, i modelli di embedding sono tipicamente solo-encoder con una matrice di attenzione completa. Questo significa che ogni embedding di token √® condizionato da tutti gli altri token all'interno della stessa finestra di contesto, che, nel caso di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">, include fino a 8191 altri token. Di conseguenza, l'embedding del chunk trasporta informazioni di contesto globale in entrambe le direzioni.</span></figcaption></figure>\n\n<h2 id=\"late-chunking-can-be-trained\">Il Late Chunking Pu√≤ Essere Addestrato</h2>\n\n<p>Il late chunking <em>non</em> richiede ulteriore addestramento per i modelli di embedding. Pu√≤ essere applicato a qualsiasi modello di embedding a contesto lungo che utilizza il mean pooling, rendendolo molto attraente per i professionisti. Detto questo, se stai lavorando su task come domanda-risposta o recupero query-documento, le prestazioni possono essere ulteriormente migliorate con un po' di fine-tuning. Specificamente, i dati di addestramento consistono in tuple contenenti:</p>\n\n<ul>\n<li>Una <strong>query</strong> (ad esempio, una domanda o un termine di ricerca).</li>\n<li>Un <strong>documento</strong> che contiene informazioni rilevanti per rispondere alla query.</li>\n<li>Uno <strong>span rilevante</strong> all'interno del documento, che √® lo specifico chunk di testo che risponde direttamente alla query.</li>\n</ul>\n\n<p>Il modello viene addestrato accoppiando query con i loro span rilevanti, utilizzando una funzione di perdita contrastiva come InfoNCE. Questo assicura che gli span rilevanti siano strettamente allineati con la query nello spazio degli embedding, mentre gli span non correlati vengono spinti pi√π lontano. Di conseguenza, il modello impara a concentrarsi sulle parti pi√π rilevanti del documento quando genera gli embedding dei chunk. <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Per maggiori dettagli, si prega di fare riferimento al nostro paper di ricerca.</a></p>\n\n<h2 id=\"late-chunking-vs-contextual-retrieval\">Late Chunking vs. Contextual Retrieval</h2>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Introducing Contextual Retrieval</div><div class=\"kg-bookmark-description\">Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n\n<p>Poco dopo l'introduzione del late chunking, Anthropic ha introdotto una strategia separata chiamata <strong>Contextual Retrieval</strong>. Il metodo di Anthropic √® un approccio brute-force per affrontare il problema del contesto perso, e funziona come segue:</p>\n\n<ol>\n<li>Ogni chunk viene inviato all'LLM insieme al documento completo.</li>\n<li>L'LLM aggiunge contesto rilevante a ogni chunk.</li>\n<li>Questo risulta in embedding pi√π ricchi e informativi.</li>\n</ol>\n\n<p>Dal nostro punto di vista, questo √® essenzialmente <strong>arricchimento del contesto</strong>, dove il contesto globale √® esplicitamente hardcodato in ogni chunk usando un LLM, che √® costoso in termini di <strong>costo</strong>, <strong>tempo</strong> e <strong>storage</strong>. Inoltre, non √® chiaro se questo approccio sia resiliente ai confini dei chunk, poich√© l'LLM si basa su chunk accurati e leggibili per arricchire efficacemente il contesto. Al contrario, il late chunking √® altamente resiliente agli indicatori di confine, come dimostrato sopra. Non richiede storage aggiuntivo poich√© la dimensione dell'embedding rimane la stessa. Nonostante sfrutti la lunghezza completa del contesto del modello di embedding, <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">√® ancora significativamente pi√π veloce dell'uso di un LLM per generare arricchimento</a>. Nello studio qualitativo del nostro paper di ricerca, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">mostriamo che il context retrieval di Anthropic ha prestazioni simili al late chunking.</a> Tuttavia, il late chunking fornisce una soluzione pi√π di basso livello, generica e naturale sfruttando le meccaniche intrinseche del transformer solo-encoder.</p>\n\n<h2 id=\"which-embedding-models-support-late-chunking\">Quali Modelli di Embedding Supportano il Late Chunking?</h2>\n\n<p>Il late chunking non √® esclusivo di <code>jina-embeddings-v3</code> o <code>v2</code>. √à un approccio abbastanza generico che pu√≤ essere applicato a qualsiasi modello di embedding a contesto lungo che utilizza il mean pooling. Per esempio, in questo post, mostriamo che anche <code>nomic-v1</code> lo supporta. Accogliamo calorosamente tutti i fornitori di embedding nell'implementare il supporto per il late chunking nelle loro soluzioni.</p>\n\n<p>Come utente di modelli, quando valuti un nuovo modello di embedding o API, puoi seguire questi passaggi per verificare se potrebbe supportare il late chunking:</p>\n\n<ol><li><strong>Output Singolo</strong>: Il modello/API fornisce un solo embedding finale per frase invece di embedding a livello di token? Se s√¨, probabilmente non pu√≤ supportare il late chunking (specialmente per le API web).</li><li><strong>Supporto per Contesti Lunghi</strong>: Il modello/API gestisce contesti di almeno 8192 token? Se no, il late chunking non sar√† applicabile‚Äîo pi√π precisamente, non ha senso adattare il late chunking per un modello con contesto breve. Se s√¨, assicurati che funzioni effettivamente bene con contesti lunghi, non solo che dichiari di supportarli. Di solito puoi trovare queste informazioni nel report tecnico del modello, come le valutazioni su LongMTEB o altri benchmark per contesti lunghi.</li><li><strong>Mean Pooling</strong>: Per i modelli self-hosted o le API che forniscono embedding a livello di token prima del pooling, verifica se il metodo di pooling predefinito √® il mean pooling. I modelli che utilizzano CLS o max pooling non sono compatibili con il late chunking.</li></ol><p>In sintesi, se un modello di embedding supporta contesti lunghi e utilizza il mean pooling come impostazione predefinita, pu√≤ facilmente supportare il late chunking. Dai un'occhiata al nostro <a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">repository GitHub per dettagli implementativi e ulteriori discussioni</a>.</p><h2 id=\"conclusion\">Conclusione</h2><p>Quindi, cos'√® il late chunking? Il late chunking √® un metodo diretto per generare embedding di chunk utilizzando modelli di embedding per contesti lunghi. √à veloce, resiliente ai segnali di confine e altamente efficace. Non √® un'euristica o una sovra-ingegnerizzazione‚Äî√® un design ragionato basato su una profonda comprensione del meccanismo transformer.</p><p>Oggi, l'hype che circonda gli LLM √® innegabile. In molti casi, problemi che potrebbero essere affrontati efficientemente da modelli pi√π piccoli come BERT vengono invece delegati agli LLM, guidati dal fascino di soluzioni pi√π grandi e complesse. Non sorprende che i grandi fornitori di LLM spingano per una maggiore adozione dei loro modelli, mentre i fornitori di embedding promuovano gli embedding ‚Äî entrambi stanno giocando sui loro punti di forza commerciali. Ma alla fine, non si tratta di hype, si tratta di azione, di ci√≤ che funziona veramente. Lasciamo che la comunit√†, l'industria e, soprattutto, il tempo rivelino quale approccio sia veramente pi√π snello, pi√π efficiente e costruito per durare.</p><p>Assicurati di leggere il <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">nostro paper di ricerca</a>, e ti incoraggiamo a testare il late chunking in vari scenari e a condividere il tuo feedback con noi.</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "Parte 2 della nostra esplorazione del Late Chunking, un'analisi approfondita sul perch√© sia il metodo migliore per gli embedding dei chunk e per migliorare le prestazioni di ricerca/RAG.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
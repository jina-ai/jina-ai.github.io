{
  "slug": "a-practical-guide-to-implementing-deepsearch-deepresearch",
  "id": "67bc50b0b1b8af00014db4c9",
  "uuid": "acd44dc0-e356-4ac8-93c7-fa8bbeb33265",
  "title": "Una guida pratica all'implementazione di DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://search.jina.ai/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI Deep Search</div><div class=\"kg-bookmark-description\">AI deep search: leggi, ragiona, cerca finché non trovi la risposta migliore.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-30.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>È appena febbraio, e il DeepSearch è già emerso come il nuovo standard di ricerca nel 2025, con i principali attori come <a href=\"https://blog.google/products/gemini/google-gemini-deep-research/\">Google</a> e <a href=\"https://openai.com/index/introducing-deep-research/\">OpenAI</a> in prima linea attraverso i loro rilasci di DeepResearch (e sì, <a href=\"https://x.com/hxiao/status/1886250705415229627\">abbiamo orgogliosamente lanciato il nostro <code>node-deepresearch</code> open-source lo stesso giorno</a>). <a href=\"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\">Perplexity</a> ha seguito l'esempio con il loro DeepResearch, e X AI ha integrato le proprie capacità DeepSearch in <a href=\"https://x.ai/blog/grok-3\">Grok3</a>, creando essenzialmente un'altra variante di DeepResearch. Mentre il concetto di deep search non è rivoluzionario – nel 2024 era essenzialmente denominato RAG o multi-hop QA – ha guadagnato uno slancio significativo dopo il rilascio di <a href=\"https://github.com/deepseek-ai/DeepSeek-R1\">Deepseek-r1</a> alla fine di gennaio 2025. Lo scorso fine settimana, <a href=\"https://www.scmp.com/tech/big-tech/article/3298981/baidu-adopts-deepseek-ai-models-chasing-tencent-race-embrace-hot-start\">Baidu Search e Tencent WeChat Search</a> hanno integrato Deepseek-r1 nei loro motori di ricerca. Gli ingegneri AI hanno scoperto che incorporando processi di pensiero e ragionamento lunghi nei sistemi di ricerca, possono ottenere una notevole accuratezza e profondità di recupero oltre quanto era precedentemente possibile.</p>\n<!--kg-card-begin: html-->\n<table> <thead> <tr> <th>Launch Date</th> <th>Company</th> <th>Product</th> <th>License Type</th> <th>Link</th> </tr> </thead> <tbody> <tr> <td>2025-01-20</td> <td>DeepSeek</td> <td>DeepSeek-r1 release</td> <td>Open source</td> <td><a href=\"https://api-docs.deepseek.com/news/news250120\">DeepSeek-R1</a></td> </tr> <tr> <td>2025-02-02</td> <td>Google</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://blog.google/products/gemini/google-gemini-deep-research/\">Google Gemini 2</a></td> </tr> <tr> <td>2025-02-02</td> <td>OpenAI</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://openai.com/index/introducing-deep-research/\">Introducing Deep Research</a></td> </tr> <tr> <td>2025-02-02</td> <td>Jina AI</td> <td>DeepSearch (<code>node-deepresearch</code>)</td> <td>Open source</td> <td><a href=\"https://github.com/jina-ai/node-deepresearch\">node-deepresearch</a> | <a href=\"https://search.jina.ai\">search.jina.ai</a></td> </tr> <tr> <td>2025-02-04</td> <td>Hugging Face</td> <td>Open Deep Research</td> <td>Open source</td> <td><a href=\"https://huggingface.co/blog/open-deep-research\">Open Deep Research</a></td> </tr> <tr> <td>2025-02-15</td> <td>Perplexity</td> <td>DeepResearch</td> <td>Proprietary</td> <td><a href=\"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\">Introducing Perplexity Deep Research</a></td> </tr> <tr> <td>2025-02-17</td> <td>X AI</td> <td>Grok3 with DeepSearch</td> <td>Proprietary</td> <td><a href=\"https://x.ai/blog/grok-3\">Grok 3 Beta</a></td> </tr> <tr> <td>2025-02-22</td> <td>Baidu Search</td> <td>Integrates DeepSeek-r1</td> <td>Proprietary</td> <td><a href=\"https://chat.baidu.com/search?isShowHello=1&pd=csaitab&setype=csaitab&extParamsJson=%7B%22enter_type%22%3A%22ai_explore_home%22%7D&usedModel=%7B%22modelName%22%3A%22DeepSeek-R1%22%7D\">Baidu Integrates DeepSeek-R1</a></td> </tr> <tr> <td>2025-02-23</td> <td>Tencent Wechat Search</td> <td>Integrates DeepSeek-r1</td> <td>Proprietary</td> <td><a href=\"https://www.reuters.com/technology/artificial-intelligence/tencents-messaging-app-weixin-launches-beta-testing-with-deepseek-2025-02-16/\">Tencent Weixin Integrates DeepSeek</a></td> </tr> </tbody> </table>\n<!--kg-card-end: html-->\n<p>Ma perché questo cambiamento è avvenuto proprio adesso, quando il Deep(Re)Search è rimasto relativamente sottovalutato durante tutto il 2024? In effetti, <a href=\"https://storm-project.stanford.edu/research/storm/\">i laboratori Stanford NLP hanno rilasciato il progetto STORM</a> per la generazione di report lunghi con fondamento web all'inizio del 2024. Quindi è solo perché \"DeepSearch\" suona molto più interessante di multi-hop QA, RAG o STORM? Siamo onesti - a volte un rebranding è tutto ciò che serve perché l'industria abbracci improvvisamente ciò che c'era da sempre.</p><p>Crediamo che il vero punto di svolta sia arrivato con il rilascio di <code>o1-preview</code> di OpenAI nel settembre 2024, che ha introdotto il concetto di <strong>test-time compute</strong> e ha gradualmente cambiato le prospettive del settore. Il test-time compute si riferisce all'utilizzo di maggiori risorse computazionali durante l'inferenza—la fase in cui un LLM genera output—piuttosto che durante il pre-training o il post-training. Un esempio ben noto sono il ragionamento Chain-of-Thought (CoT) e l'<a href=\"https://github.com/simplescaling/s1?tab=readme-ov-file#vllm-with-budget-forcing\">iniezione di <code>\"Wait\"</code></a> (cioè il budget forcing) che permette ai modelli di eseguire deliberazioni interne più estese, come valutare multiple risposte potenziali, condurre una pianificazione più profonda e impegnarsi nell'auto-riflessione prima di arrivare a una risposta finale.</p><p>Questo concetto di test-time compute e i modelli di ragionamento <strong><em>educano</em></strong> gli utenti ad accettare la <a href=\"https://en.wikipedia.org/wiki/Delayed_gratification\">gratificazione ritardata</a> - tempi di attesa più lunghi in cambio di risultati di qualità superiore e immediatamente utilizzabili, proprio come l'esperimento del marshmallow di Stanford dove i bambini che potevano resistere dal mangiare subito un marshmallow per riceverne due più tardi mostravano migliori risultati a lungo termine. Deepseek-r1 ha ulteriormente rafforzato questa esperienza utente e, che piaccia o no, la maggior parte degli utenti l'ha accettata.</p><p>Questo segna una significativa deviazione dai requisiti di ricerca classici, dove non rispondere entro 200ms avrebbe condannato la tua soluzione. Nel 2025, gli sviluppatori di ricerca esperti e gli ingegneri RAG danno priorità alla precisione e al recall top-1 rispetto alla latenza, e gli utenti si sono abituati a tempi di elaborazione più lunghi – purché possano vedere che il sistema sta <code>&lt;thinking&gt;</code>.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1.mp4\" poster=\"https://img.spacergif.org/v1/1610x1422/0a/spacer.png\" width=\"1610\" height=\"1422\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/think-ui-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:18</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Mostrare il processo di ragionamento è diventata una pratica standard nel 2025, con numerose interfacce chat che ora rendono il contenuto </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>&lt;think&gt;</span></code><span style=\"white-space: pre-wrap;\"> in sezioni dedicate dell'interfaccia utente.</span></p></figcaption>\n        </figure><p>In questo articolo, discuteremo i principi di DeepSearch e DeepResearch esaminando la nostra implementazione open-source. Analizzeremo le nostre decisioni chiave di design e metteremo in evidenza potenziali criticità.</p><h2 id=\"what-is-deep-search\">Cos'è Deep Search?</h2><p><strong>DeepSearch esegue un ciclo iterativo di ricerca, lettura e ragionamento fino a trovare la risposta ottimale.</strong> L'azione di ricerca sfrutta i motori di ricerca web per esplorare internet, mentre l'azione di lettura analizza specifiche pagine web in dettaglio (es. <a href=\"https://jina.ai/reader\" rel=\"noreferrer\">Jina Reader</a>). L'azione di ragionamento valuta lo stato attuale e determina se suddividere la domanda originale in sotto-domande più piccole o provare diverse strategie di ricerca.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"561\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/image.png 2240w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">DeepSearch - continua a cercare, leggere pagine web e ragionare fino a trovare una risposta (o fino al superamento del budget di token).</span></figcaption></figure><p>Mentre esistono varie definizioni online, quando abbiamo sviluppato il progetto <code>node-deepresearch</code>, abbiamo seguito questo approccio semplice. L'implementazione è elegantemente semplice – nel suo nucleo, c'è un ciclo while principale con una logica switch-case che dirige l'azione successiva.</p><p>A differenza dei sistemi RAG del 2024, che tipicamente eseguono un singolo passaggio di ricerca-generazione, DeepSearch esegue invece molteplici iterazioni attraverso la pipeline, richiedendo condizioni di arresto chiare. Queste potrebbero essere basate su limiti di utilizzo dei token o sul numero di tentativi falliti.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1.mp4\" poster=\"https://img.spacergif.org/v1/1238x1300/0a/spacer.png\" width=\"1238\" height=\"1300\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepsearch-dark-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:36</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Prova deep search su search.jina.ai, osserva il contenuto all'interno di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>&lt;thinking&gt;</span></code><span style=\"white-space: pre-wrap;\">, vedi se riesci a capire dove avviene il ciclo</span></p></figcaption>\n        </figure><p>Un'altra prospettiva su DeepSearch è vederlo come un agente LLM dotato di vari strumenti web (come il cercatore e il lettore). L'agente determina i suoi prossimi passi analizzando le osservazioni attuali e le azioni passate – decidendo se fornire una risposta o continuare ad esplorare il web. Questo crea un'architettura a macchina a stati dove l'LLM controlla le transizioni tra gli stati. Ad ogni punto decisionale, hai due approcci: puoi sia creare attentamente prompt per modelli generativi standard per produrre azioni specifiche, sia sfruttare modelli di ragionamento specializzati come Deepseek-r1 per derivare naturalmente le azioni successive. Tuttavia, anche quando usi r1, dovrai periodicamente interrompere la sua generazione per iniettare output degli strumenti (es. risultati di ricerca, contenuto delle pagine web) nel contesto e sollecitarlo a continuare il suo processo di ragionamento.</p><p>In definitiva, questi sono solo dettagli implementativi – che tu lo solleciti attentamente o usi semplicemente modelli di ragionamento, tutti si allineano con il principio di design fondamentale di DeepSearch: un ciclo continuo di ricerca, lettura e ragionamento.</p><h2 id=\"what-is-deepresearch-then\">Cos'è Quindi DeepResearch?</h2><p><strong>DeepResearch si basa su DeepSearch aggiungendo un framework strutturato per generare lunghi report di ricerca.</strong> Spesso inizia creando un indice, poi applica sistematicamente DeepSearch a ogni sezione richiesta – dall'introduzione attraverso il lavoro correlato e la metodologia, fino alla conclusione. Ogni sezione viene generata inserendo specifiche domande di ricerca in DeepSearch. La fase finale prevede il consolidamento di tutte le sezioni in un unico prompt per migliorare la coerenza narrativa complessiva.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"832\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/image-1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/image-1.png 2268w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">DeepSearch come blocco costitutivo di DeepResearch. Costruisce iterativamente ogni sezione tramite DeepSearch e poi migliora la coerenza complessiva prima di generare il report finale completo.</span></figcaption></figure><p>Nel nostro progetto \"Research\" del 2024, abbiamo eseguito molteplici passaggi di miglioramento della coerenza, con ogni iterazione che teneva conto di tutte le altre sezioni. Tuttavia, con le finestre di contesto dei LLM significativamente più ampie di oggi, questo approccio sembra ridondante – è sufficiente un singolo passaggio di revisione della coerenza.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch.mp4\" poster=\"https://img.spacergif.org/v1/2940x1660/0a/spacer.png\" width=\"2940\" height=\"1660\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/deepresearch_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:40</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Il nostro progetto estivo \"Research\" del 2024 si è concentrato sulla generazione di report lunghi con un approccio \"progressivo\". È iniziato creando un TOC in </span><b><strong style=\"white-space: pre-wrap;\">sync</strong></b><span style=\"white-space: pre-wrap;\">, poi ha generato tutte le sezioni in parallelo </span><b><strong style=\"white-space: pre-wrap;\">async</strong></b><span style=\"white-space: pre-wrap;\">. Il processo si è concluso con revisioni progressive </span><b><strong style=\"white-space: pre-wrap;\">async</strong></b><span style=\"white-space: pre-wrap;\"> di ogni sezione, con ogni revisione che teneva conto del contenuto di tutte le altre sezioni. La query nel video è </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"Competitor analysis of Jina AI\"</span></code><span style=\"white-space: pre-wrap;\">.</span></p></figcaption>\n        </figure><h2 id=\"deepsearch-vs-deepresearch\">DeepSearch vs DeepResearch</h2><p>Mentre molte persone spesso confondono DeepSearch e DeepResearch, dal nostro punto di vista affrontano problemi completamente diversi. DeepSearch funziona come un blocco atomico fondamentale – un componente core su cui DeepResearch si basa. DeepResearch, d'altra parte, <strong>si concentra sulla creazione di report di ricerca lunghi e di alta qualità leggibili</strong>, che comprende un set diverso di requisiti: incorporare visualizzazioni efficaci tramite grafici e tabelle, strutturare i contenuti con intestazioni di sezione appropriate, garantire un flusso logico fluido tra le sottosezioni, mantenere una terminologia coerente in tutto il documento, eliminare la ridondanza tra le sezioni, creare transizioni fluide che collegano i contenuti precedenti e successivi. Questi elementi sono in gran parte non correlati alla ricerca core, motivo per cui troviamo DeepSearch più interessante come focus aziendale.</p><p>Infine, la tabella seguente riassume le differenze tra DeepSearch e DeepResearch. Vale la pena notare che entrambi i sistemi beneficiano significativamente di modelli con contesto lungo e capacità di ragionamento. Questo potrebbe sembrare controintuitivo, in particolare per DeepSearch—mentre è ovvio perché DeepResearch necessita di capacità di contesto lungo (poiché produce report lunghi). Il motivo è che DeepSearch deve memorizzare i tentativi di ricerca precedenti e i contenuti delle pagine web per prendere decisioni informate sui passi successivi, rendendo una finestra di contesto lunga altrettanto essenziale per la sua implementazione efficace.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>DeepSearch</th>\n<th>DeepResearch</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Problem Addressed</strong></td>\n<td>Information accuracy and completeness through iterative search</td>\n<td>Content organization, coherence, and readability at document scale</td>\n</tr>\n<tr>\n<td><strong>Final Presentation</strong></td>\n<td>Concise answer with URLs as references</td>\n<td>A long structured report with multiple sections, charts, tables and references</td>\n</tr>\n<tr>\n<td><strong>Core Complexity</strong></td>\n<td>State machine architecture with clear transition conditions; Persistence through failed attempts until resolution</td>\n<td>Multi-level architecture managing both micro (search) and macro (document) concerns; Structural approach to managing complex information hierarchies</td>\n</tr>\n<tr>\n<td><strong>Optimization Focus</strong></td>\n<td>Local optimization (best next search/read action)</td>\n<td>Global optimization (section organization, terminology consistency, transitions)</td>\n</tr>\n<tr>\n<td><strong>Limitations</strong></td>\n<td>Bounded by search quality and reasoning capability</td>\n<td>Bounded by DeepSearch quality plus organizational complexity and narrative coherence challenges</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"understand-deepsearch-implementation\">Comprendere l'Implementazione di DeepSearch</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/node-DeepResearch: Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget)</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-2.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Il cuore di DeepResearch risiede nel suo approccio di ragionamento ciclico. Invece di tentare di rispondere alle domande in un singolo passaggio come la maggior parte dei sistemi RAG, abbiamo implementato un ciclo iterativo che cerca continuamente informazioni, legge le fonti rilevanti e ragiona fino a trovare una risposta o esaurire il budget di token. Ecco il nucleo semplificato di questo grande ciclo while:</p><pre><code class=\"language-typescript\">// Main reasoning loop\nwhile (tokenUsage &lt; tokenBudget &amp;&amp; badAttempts &lt;= maxBadAttempts) {\n  // Track progression\n  step++; totalStep++;\n  \n  // Get current question from gaps queue or use original question\n  const currentQuestion = gaps.length &gt; 0 ? gaps.shift() : question;\n  \n  // Generate prompt with current context and allowed actions\n  system = getPrompt(diaryContext, allQuestions, allKeywords, \n                    allowReflect, allowAnswer, allowRead, allowSearch, allowCoding,\n                    badContext, allKnowledge, unvisitedURLs);\n  \n  // Get LLM to decide next action\n  const result = await LLM.generateStructuredResponse(system, messages, schema);\n  thisStep = result.object;\n  \n  // Execute the selected action (answer, reflect, search, visit, coding)\n  if (thisStep.action === 'answer') {\n    // Process answer action...\n  } else if (thisStep.action === 'reflect') {\n    // Process reflect action...\n  } // ... and so on for other actions\n}\n</code></pre><p>Un dettaglio implementativo chiave è la disabilitazione selettiva di determinate azioni ad ogni passaggio per garantire un output strutturato più stabile. Ad esempio, se non ci sono URL in memoria, disabilitiamo l'azione <code>visit</code>; o se l'ultima risposta è stata rifiutata, impediamo all'agente di richiamare immediatamente <code>answer</code>. <strong>Questo vincolo mantiene l'agente su un percorso produttivo, evitando fallimenti ripetitivi causati dall'invocazione della stessa azione.</strong></p><h3 id=\"system-prompt\">System Prompt</h3><p>Utilizziamo i tag XML per definire le sezioni, che producono prompt di sistema e generazioni più robusti. Abbiamo anche scoperto che inserire i vincoli dei campi direttamente nei campi <code>description</code> dello schema JSON produce risultati migliori. Mentre alcuni potrebbero sostenere che la maggior parte dei prompt potrebbe essere automatizzata con modelli di ragionamento come DeepSeek-R1, le restrizioni sulla lunghezza del contesto e la necessità di comportamenti altamente specifici rendono un approccio esplicito più affidabile nella pratica.</p><pre><code class=\"language-typescript\">function getPrompt(params...) {\n  const sections = [];\n  \n  // Add header with system instruction\n  sections.push(\"You are an advanced AI research agent specialized in multistep reasoning...\");\n  \n  // Add accumulated knowledge section if exists\n  if (knowledge?.length) {\n    sections.push(\"&lt;knowledge&gt;[Knowledge items]&lt;/knowledge&gt;\");\n  }\n  \n  // Add context of previous actions\n  if (context?.length) {\n    sections.push(\"&lt;context&gt;[Action history]&lt;/context&gt;\");\n  }\n  \n  // Add failed attempts and learned strategies\n  if (badContext?.length) {\n    sections.push(\"&lt;bad-attempts&gt;[Failed attempts]&lt;/bad-attempts&gt;\");\n    sections.push(\"&lt;learned-strategy&gt;[Improvement strategies]&lt;/learned-strategy&gt;\");\n  }\n  \n  // Define available actions based on current state\n  sections.push(\"&lt;actions&gt;[Available action definitions]&lt;/actions&gt;\");\n  \n  // Add response format instruction\n  sections.push(\"Respond in valid JSON format matching exact JSON schema.\");\n  \n  return sections.join(\"\\n\\n\");\n}\n</code></pre><h3 id=\"gap-questions-traversing\">Attraversamento delle Gap Questions</h3><p>In DeepSearch, le \"gap questions\" rappresentano lacune di conoscenza che devono essere colmate prima di rispondere alla domanda principale. Invece di affrontare direttamente la domanda originale, l'agente identifica sotto-domande che costruiranno la base di conoscenza necessaria.</p><p>Il design è particolarmente elegante nel modo in cui gestisce queste gap questions:</p><pre><code class=\"language-typescript\">// After identifying gap questions in reflect action\nif (newGapQuestions.length &gt; 0) {\n  // Add new questions to the front of the queue\n  gaps.push(...newGapQuestions);\n  \n  // Always add original question to the end of the queue\n  gaps.push(originalQuestion);\n}\n</code></pre><p>Questo approccio crea una coda FIFO (First-In-First-Out) con rotazione, dove:</p><ol><li>Le nuove gap questions vengono inserite all'inizio della coda</li><li>La domanda originale viene sempre inserita in fondo</li><li>Il sistema estrae dall'inizio della coda ad ogni passaggio</li></ol><p>Ciò che rende eccellente questo design è che mantiene un unico contesto condiviso tra tutte le domande. Quando viene risposta una gap question, quella conoscenza diventa immediatamente disponibile per tutte le domande successive, incluso quando alla fine rivisiteremo la domanda originale.</p><h4 id=\"fifo-queue-vs-recursion\">Coda FIFO vs Ricorsione</h4><p>Un approccio alternativo è l'uso della ricorsione, che corrisponde alla ricerca in profondità. Ogni gap question genera una nuova chiamata ricorsiva con il proprio contesto isolato. Il sistema deve risolvere completamente ogni gap question (e tutte le sue potenziali sotto-domande) prima di tornare alla domanda padre.</p><p>Consideriamo questo scenario di esempio:</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs.mp4\" poster=\"https://img.spacergif.org/v1/950x846/0a/spacer.png\" width=\"950\" height=\"846\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/02/dfs_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:23</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">Una semplice ricorsione di gap questions a profondità 3, con l'ordine di risoluzione indicato sul cerchio.</span></p></figcaption>\n        </figure><p>Nell'approccio ricorsivo, il sistema dovrebbe risolvere completamente Q1 (potenzialmente generando le proprie sotto-domande) dopo ogni gap question e le loro sotto-domande! Questo è in forte contrasto con l'approccio a coda, che elabora le domande dove Q1 viene rivisitata subito dopo 3 gap questions.</p><p>In realtà, abbiamo scoperto che l'approccio ricorsivo è molto difficile da applicare al forzamento del budget, poiché non c'è una regola empirica chiara su quanto budget di token dovremmo concedere per le sotto-domande (dato che potrebbero generare nuove sotto-domande). Il beneficio della chiara separazione del contesto nell'approccio ricorsivo è molto marginale rispetto ai complicati problemi di forzatura del budget e ritorno tardivo. Questo design con coda FIFO bilancia profondità e ampiezza, assicurando che il sistema torni sempre alla domanda originale con una conoscenza progressivamente migliore, piuttosto che perdersi in una discesa ricorsiva potenzialmente infinita.</p><h3 id=\"query-rewrite\">Riscrittura delle Query</h3><p>Una sfida interessante che abbiamo incontrato è stata la riscrittura efficace delle query di ricerca:</p><pre><code class=\"language-typescript\">// Within search action handler\nif (thisStep.action === 'search') {\n  // Deduplicate search requests\n  const uniqueRequests = await dedupQueries(thisStep.searchRequests, existingQueries);\n  \n  // Rewrite natural language queries into more effective search queries\n  const optimizedQueries = await rewriteQuery(uniqueRequests);\n  \n  // Ensure we don't repeat previous searches\n  const newQueries = await dedupQueries(optimizedQueries, allKeywords);\n  \n  // Execute searches and store results\n  for (const query of newQueries) {\n    const results = await searchEngine(query);\n    if (results.length &gt; 0) {\n      storeResults(results);\n      allKeywords.push(query);\n    }\n  }\n}\n</code></pre><p>La riscrittura delle query si è rivelata sorprendentemente importante - forse uno degli elementi più critici che determina direttamente la qualità dei risultati. Un buon riscritture di query non si limita a trasformare il linguaggio naturale in parole chiave simili a BM25; espande le query per coprire più potenziali risposte attraverso diverse lingue, toni e formati di contenuto.</p><p>Per la deduplicazione delle query, inizialmente abbiamo utilizzato una soluzione basata su LLM, ma abbiamo trovato difficile controllare la soglia di similarità. Alla fine siamo passati a <code>jina-embeddings-v3</code>, che eccelle nei compiti di similarità testuale semantica. Questo permette la deduplicazione multilingue senza preoccuparsi che le query non inglesi vengano filtrate. Il modello di embedding si è rivelato cruciale non per il recupero della memoria come inizialmente previsto, ma per un'efficiente deduplicazione.</p><h3 id=\"crawling-web-content\">Crawling dei Contenuti Web</h3><p>Lo scraping web e l'elaborazione dei contenuti è un'altra componente critica. Qui utilizziamo <a href=\"https://jina.ai/reader\" rel=\"noreferrer\">Jina Reader API</a>. Da notare che oltre al contenuto completo della pagina web, aggreghiamo anche tutti gli snippet restituiti dal motore di ricerca come conoscenza aggiuntiva per l'agente da utilizzare successivamente per trarre conclusioni. Pensiamoli come frammenti audio.</p><pre><code class=\"language-typescript\">// Visit action handler\nasync function handleVisitAction(URLs) {\n  // Normalize URLs and filter out already visited ones\n  const uniqueURLs = normalizeAndFilterURLs(URLs);\n  \n  // Process each URL in parallel\n  const results = await Promise.all(uniqueURLs.map(async url =&gt; {\n    try {\n      // Fetch and extract content\n      const content = await readUrl(url);\n      \n      // Store as knowledge\n      addToKnowledge(`What is in ${url}?`, content, [url], 'url');\n      \n      return {url, success: true};\n    } catch (error) {\n      return {url, success: false};\n    } finally {\n      visitedURLs.push(url);\n    }\n  }));\n  \n  // Update diary based on success or failure\n  updateDiaryWithVisitResults(results);\n}\n</code></pre><p>Abbiamo normalizzato gli URL per un tracciamento coerente e limitato il numero di URL visitati in ogni passaggio per gestire la memoria dell'agente.</p><h3 id=\"memory-management\">Gestione della Memoria</h3><p>Una sfida chiave nel ragionamento multi-step è gestire efficacemente la memoria dell'agente. Abbiamo progettato il sistema di memoria per differenziare tra ciò che conta come \"memoria\" rispetto a ciò che conta come \"conoscenza\". In ogni caso, sono tutti parte del contesto del prompt LLM, separati con diversi tag XML:</p><pre><code class=\"language-typescript\">// Add knowledge item to accumulated knowledge\nfunction addToKnowledge(question, answer, references, type) {\n  allKnowledge.push({\n    question: question,\n    answer: answer,\n    references: references,\n    type: type,  // 'qa', 'url', 'coding', 'side-info'\n    updated: new Date().toISOString()\n  });\n}\n\n// Record step in narrative diary\nfunction addToDiary(step, action, question, result, evaluation) {\n  diaryContext.push(`\nAt step ${step}, you took **${action}** action for question: \"${question}\"\n[Details of what was done and results]\n[Evaluation if applicable]\n`);\n}\n</code></pre><p>Poiché la maggior parte degli LLM del 2025 ha finestre di contesto sostanziali, abbiamo scelto di non utilizzare database vettoriali. Invece, la memoria consiste in conoscenza acquisita, siti visitati e registrazioni dei tentativi falliti - tutto mantenuto nel contesto. Questo sistema di memoria completo fornisce all'agente la consapevolezza di ciò che sa, ciò che ha provato e ciò che ha funzionato o fallito.</p><h3 id=\"answer-evaluation\">Valutazione delle Risposte</h3><p>Un'intuizione chiave è che la generazione e la valutazione delle risposte non dovrebbero essere nello stesso prompt. Nella mia implementazione, determiniamo prima quali criteri di valutazione utilizzare quando arriva una nuova domanda, e poi valutiamo ogni criterio uno per uno. Il valutatore utilizza esempi few-shot per una valutazione coerente, garantendo una maggiore affidabilità rispetto all'autovalutazione.</p><pre><code class=\"language-typescript\">// Separate evaluation phase\nasync function evaluateAnswer(question, answer, metrics, context) {\n  // First, identify evaluation criteria based on question type\n  const evaluationCriteria = await determineEvaluationCriteria(question);\n  \n  // Then evaluate each criterion separately\n  const results = [];\n  for (const criterion of evaluationCriteria) {\n    const result = await evaluateSingleCriterion(criterion, question, answer, context);\n    results.push(result);\n  }\n  \n  // Determine if answer passes overall evaluation\n  return {\n    pass: results.every(r =&gt; r.pass),\n    think: results.map(r =&gt; r.reasoning).join('\\n')\n  };\n}\n</code></pre><h3 id=\"budget-forcing\">Budget-Forcing</h3><p>Il budget forcing significa impedire al sistema di restituire anticipatamente e assicurare che continui l'elaborazione fino al superamento del budget. Dalla release di DeepSeek-R1, l'approccio al budget forcing si è spostato verso <strong>l'incoraggiamento di un pensiero più profondo per risultati migliori piuttosto che semplicemente risparmiare il budget.</strong></p><p>Nella nostra implementazione, abbiamo esplicitamente configurato il sistema per identificare le lacune di conoscenza prima di tentare di rispondere.</p><pre><code class=\"language-typescript\">if (thisStep.action === 'reflect' &amp;&amp; thisStep.questionsToAnswer) {\n  // Force deeper reasoning by adding sub-questions to the queue\n  gaps.push(...newGapQuestions);\n  gaps.push(question);  // Always revisit the original\n}</code></pre><p>Attivando e disattivando selettivamente determinate azioni, possiamo guidare il sistema verso l'utilizzo di strumenti che migliorano la profondità del ragionamento.</p><pre><code class=\"language-typescript\">// After a failed answer attempt\nallowAnswer = false;  // Force agent to search or reflect instead</code></pre><p>Per evitare di sprecare token su percorsi improduttivi, impostiamo limiti sul numero di tentativi falliti. Quando ci avviciniamo ai limiti di budget, attiviamo la \"modalità beast\" per garantire che forniamo qualche risposta piuttosto che nessuna.</p><pre><code class=\"language-typescript\">// Beast mode activation\nif (!thisStep.isFinal &amp;&amp; badAttempts &gt;= maxBadAttempts) {\n  console.log('Enter Beast mode!!!');\n  \n  // Configure prompt for decisive, committed answer\n  system = getPrompt(\n    diaryContext, allQuestions, allKeywords,\n    false, false, false, false, false,  // Disable all other actions\n    badContext, allKnowledge, unvisitedURLs,\n    true  // Enable beast mode\n  );\n  \n  // Force answer generation\n  const result = await LLM.generateStructuredResponse(system, messages, answerOnlySchema);\n  thisStep = result.object;\n  thisStep.isFinal = true;\n}\n</code></pre><p>Il prompt della modalità beast è intenzionalmente drammatico per segnalare all'LLM che deve essere decisivo e impegnarsi in una risposta basata sulle informazioni disponibili:</p><pre><code>&lt;action-answer&gt;\n🔥 ENGAGE MAXIMUM FORCE! ABSOLUTE PRIORITY OVERRIDE! 🔥\n\nPRIME DIRECTIVE:\n- DEMOLISH ALL HESITATION! ANY RESPONSE SURPASSES SILENCE!\n- PARTIAL STRIKES AUTHORIZED - DEPLOY WITH FULL CONTEXTUAL FIREPOWER\n- TACTICAL REUSE FROM &lt;bad-attempts&gt; SANCTIONED\n- WHEN IN DOUBT: UNLEASH CALCULATED STRIKES BASED ON AVAILABLE INTEL!\n\nFAILURE IS NOT AN OPTION. EXECUTE WITH EXTREME PREJUDICE! ⚡️\n&lt;/action-answer&gt;\n</code></pre><p>Questo assicura che forniamo sempre qualche risposta piuttosto che arrenderci completamente, il che è particolarmente utile per domande difficili o ambigue.</p><h2 id=\"conclusion\">Conclusione</h2><p>DeepSearch rappresenta un salto in avanti nel modo in cui la ricerca può affrontare query complesse in modo esaustivamente approfondito. Suddividendo il processo in fasi discrete di ricerca, lettura e ragionamento, supera molte limitazioni dei tradizionali sistemi RAG a passaggio singolo o sistemi QA multi-hop.</p><p>Durante l'implementazione, abbiamo anche iniziato a rivedere le fondamenta della ricerca nel 2025 e i cambiamenti nell'industria della ricerca dopo il 26 gennaio 2025, quando è stato rilasciato DeepSeek-R1. Ci siamo chiesti: <em>Quali sono le nuove esigenze? Quali esigenze sono diventate obsolete? Quali sono meramente esigenze percepite?</em></p><p>Guardando la nostra implementazione di DeepSearch, abbiamo identificato cose che avevamo previsto di aver bisogno e di cui effettivamente avevamo bisogno, cose che pensavamo sarebbero state necessarie ma non lo erano, e cose che non avevamo previsto di aver bisogno ma si sono rivelate essenziali:</p><p>Primo, <strong>un LLM con contesto lungo che produce output ben strutturato è altamente necessario</strong> (cioè seguendo JSONSchema). Un modello di ragionamento è probabilmente necessario per un migliore ragionamento sulle azioni e l'espansione delle query.</p><p><strong>L'espansione delle query è decisamente essenziale</strong>, che sia implementata tramite SLM, LLM o un modello di ragionamento. Tuttavia, dopo questo progetto, crediamo che gli SLM siano probabilmente inadatti per questo compito, poiché la soluzione deve essere intrinsecamente multilingue e andare oltre le semplici riscritture di sinonimi o l'estrazione di parole chiave. Deve essere abbastanza completa da includere <a href=\"https://jina.ai/news/what-should-we-learn-from-modernbert/#modernberts-parameter-efficiency\">una base di token multilingue (può facilmente occupare 300M parametri)</a> e abbastanza sofisticata per un pensiero fuori dagli schemi. Quindi utilizzare gli SLM per l'espansione delle query è probabilmente un non-starter.</p><p><strong>Le capacità di ricerca web e lettura web sono cruciali</strong>, e fortunatamente il nostro <a href=\"https://jina.ai/reader\">Reader (r.jina.ai)</a> ha funzionato eccellentemente—robusto e scalabile—dandomi molte idee su come migliorare il nostro endpoint di ricerca (<code>s.jina.ai</code>) per la prossima iterazione.</p><p><strong>Il modello di embedding è utile <em>ma in un modo completamente inaspettato</em>.</strong> Pensavamo che sarebbe stato utilizzato per il recupero della memoria o la compressione del contesto insieme a un database vettoriale (che, come si è scoperto, non è necessario), ma in realtà lo abbiamo utilizzato per la deduplicazione (essenzialmente un compito STS). Poiché il numero di query e domande gap è tipicamente nell'ordine delle centinaia, non è necessario alcun database vettoriale—il calcolo della similarità del coseno direttamente in memoria funziona perfettamente.</p><p><strong>Non abbiamo utilizzato Reranker</strong>, anche se crediamo che potrebbe potenzialmente aiutare a determinare quali URL visitare in base alla query, al titolo dell'URL e allo snippet. Sia per l'embedding che per il reranking, la capacità multilingue è essenziale poiché query e domande sono multilingue. La gestione del contesto lungo per l'embedding e il reranking è vantaggiosa ma non un blocco critico (Non abbiamo riscontrato errori dal nostro utilizzo dell'embedding, probabilmente perché <a href=\"https://jina.ai/models/jina-embeddings-v3/\">la nostra lunghezza del contesto è già di 8192 token</a>). In ogni caso, <code>jina-embeddings-v3</code> e <code>jina-reranker-v2-base-multilingual</code> sono i miei modelli preferiti poiché sono multilingue, SOTA e gestiscono bene il contesto lungo.</p><p><strong>Un framework per agenti si è rivelato non necessario</strong>, poiché avevamo bisogno di rimanere più vicini al comportamento nativo dell'LLM per progettare il sistema senza proxy. <a href=\"https://sdk.vercel.ai/docs/introduction\">Vercel AI SDK</a> è stato prezioso, poiché ha risparmiato considerevole sforzo nell'adattare il codebase a diversi fornitori LLM (potevamo passare da Gemini Studio a OpenAI a Google Vertex AI con solo una riga di codice modificata). La gestione della memoria degli agenti è necessaria, ma un framework di memoria dedicato rimane discutibile: Ci preoccupa che creerebbe un livello di isolamento tra LLM e sviluppatori, e che il suo zucchero sintattico potrebbe eventualmente diventare un ostacolo amaro per gli sviluppatori, come abbiamo visto con molti framework LLM/RAG oggi.</p>",
  "comment_id": "67bc50b0b1b8af00014db4c9",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/02/deepsearch-banner.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-02-24T11:57:52.000+01:00",
  "updated_at": "2025-02-25T14:39:22.000+01:00",
  "published_at": "2025-02-25T14:36:17.000+01:00",
  "custom_excerpt": "QPS out, depth in. DeepSearch is the new norm. Find answers through read-search-reason loops. Learn what it is and how to build it.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-practical-guide-to-implementing-deepsearch-deepresearch/",
  "excerpt": "QPS non è più importante, ora conta la profondità. DeepSearch è il nuovo standard. Trova risposte attraverso cicli di lettura-ricerca-ragionamento. Scopri cosa è e come costruirlo.",
  "reading_time": 15,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "La similarit√† del coseno nei sottospazi implica una similarit√† del coseno in alta dimensionalit√†?",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Il 25 gennaio 2024, OpenAI ha rilasciato <a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">un nuovo modello di embedding</a> con una nuova funzionalit√† chiamata <i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">\"shortening\"</strong></b></i>, che permette agli sviluppatori di ridurre gli embedding‚Äîessenzialmente tagliando numeri dalla fine della sequenza‚Äîsenza compromettere la capacit√† dell'embedding di rappresentare efficacemente i concetti. Approfondisci questo post per una solida base teorica sulla fattibilit√† e il ragionamento dietro questa innovazione.</div></div><p>Considera questo: quando si misura la similarit√† del coseno di vettori di embedding in spazi ad alta dimensionalit√†, come la loro similarit√† in sottospazi di dimensione inferiore implica la similarit√† complessiva? Esiste una relazione diretta e proporzionale, o la realt√† √® pi√π complessa con i dati ad alta dimensionalit√†?</p><p>Pi√π concretamente, <strong>un'alta similarit√† tra vettori nelle loro prime 256 dimensioni garantisce un'alta similarit√† nelle loro 768 dimensioni complete?</strong> Al contrario, se i vettori differiscono significativamente in alcune dimensioni, questo implica una bassa similarit√† complessiva? Queste non sono mere riflessioni teoriche; sono considerazioni cruciali per il recupero efficiente dei vettori, l'indicizzazione dei database e le prestazioni dei sistemi RAG.</p><p>Gli sviluppatori spesso si affidano a euristiche, assumendo che un'alta similarit√† nel sottospazio equivalga ad un'alta similarit√† complessiva o che differenze notevoli in una dimensione influenzino significativamente la similarit√† complessiva. La domanda √®: questi metodi euristici sono basati su solide basi teoriche o sono semplicemente assunzioni di convenienza?</p><p>Questo post si addentra in queste domande, esaminando la teoria e le implicazioni pratiche della similarit√† nei sottospazi in relazione alla similarit√† vettoriale complessiva.</p><h2 id=\"bounding-the-cosine-similarity\">Delimitare la Similarit√† del Coseno</h2><p>Dati i vettori $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$, li decomponiamo come $\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ e $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$, dove $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ e $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$, con $m+n=d$.</p><p>La similarit√† del coseno nel sottospazio $\\mathbb{R}^m$ √® data da $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$; similmente, la similarit√† nel sottospazio $\\mathbb{R}^n$ √® $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$.</p><p>Nello spazio originale $\\mathbb{R}^d$, la similarit√† del coseno √® definita come:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>Ora, sia $s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$. Quindi abbiamo:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>Fine della dimostrazione.</p><p>Nota che nell'ultimo passaggio della dimostrazione, sfruttiamo il fatto che la similarit√† del coseno √® sempre minore o uguale a 1. Questo forma il nostro limite superiore. Similmente, possiamo dimostrare che il limite inferiore di \\(\\cos(\\mathbf{A},\\mathbf{B})\\) √® dato da:</p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\], dove $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$.</p><p>Nota che per il limite inferiore, non possiamo concludere frettolosamente che \\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\). Questo √® dovuto all'intervallo della funzione coseno, che spazia tra \\([-1, 1]\\). A causa di questo intervallo, √® impossibile stabilire un limite inferiore pi√π stretto del valore triviale di -1.</p><p>Quindi in conclusione, abbiamo il seguente limite ampio: $$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)).$$ e un limite pi√π stretto \\[\\begin{align*}  \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\], dove $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $.</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">Connessione con il Lemma di Johnson‚ÄìLindenstrauss</h3><p>Il lemma JL afferma che per ogni \\(0 &lt; \\epsilon &lt; 1\\) e qualsiasi insieme finito di punti \\( S \\) in \\( \\mathbb{R}^d \\), esiste una mappatura \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\) (con \\( k = O(\\epsilon^{-2} \\log |S|) \\)) tale che per tutti \\( \\mathbf{u}, \\mathbf{v} \\in S \\), le distanze euclidee sono approssimativamente preservate:<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson‚ÄìLindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>Per far funzionare $f$ come una selezione di sottospazio, possiamo usare una matrice diagonale per la proiezione, come una matrice \\(5 \\times 3\\) \\(f\\), anche se non casuale (nota, la formulazione tipica del lemma JL coinvolge trasformazioni lineari che spesso utilizzano matrici casuali estratte da una distribuzione gaussiana). Per esempio, se vogliamo mantenere la 1¬™, 3¬™ e 5¬™ dimensione da uno spazio vettoriale 5-dimensionale, la matrice \\(f\\) potrebbe essere progettata come segue: \\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>Tuttavia, specificando $f$ come diagonale, limitiamo la classe di funzioni che possono essere utilizzate per la proiezione. Il lemma JL garantisce l'esistenza di una $f$ adatta all'interno della classe pi√π ampia di trasformazioni lineari, ma quando limitiamo $f$ ad essere diagonale, tale $f$ adatta potrebbe non esistere all'interno di questa classe ristretta per applicare i limiti del lemma JL.</p><h2 id=\"validating-the-bounds\">Validare i Limiti</h2><p>Per esplorare empiricamente i limiti teorici della similarit√† del coseno in spazi vettoriali ad alta dimensionalit√†, possiamo impiegare una simulazione Monte Carlo. Questo metodo ci permette di generare un grande numero di coppie di vettori casuali, calcolare le loro similarit√† sia nello spazio originale che nei sottospazi, e poi valutare quanto bene i limiti teorici superiori e inferiori reggano nella pratica.</p><p>Il seguente snippet di codice Python implementa questo concetto. Genera casualmente coppie di vettori in uno spazio ad alta dimensionalit√† e calcola la loro similarit√† del coseno. Poi, divide ogni vettore in due sottospazi, calcola la similarit√† del coseno all'interno di ogni sottospazio e valuta i limiti superiori e inferiori della similarit√† del coseno a dimensione piena basandosi sulle similarit√† dei sottospazi.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),\n          np.all(gamma * t &lt;= cos_sim)))\n</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Un validatore Monte Carlo per la validazione dei limiti di similarit√† del coseno</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Un esempio di output dal nostro validatore Monte Carlo. √à importante notare che la condizione </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>lower/upper bound satisfied</span></code><span style=\"white-space: pre-wrap;\"> viene verificata individualmente per ogni vettore. Nel frattempo, i valori </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>avg. lower/upper bound</span></code><span style=\"white-space: pre-wrap;\"> forniscono una panoramica pi√π intuitiva delle statistiche relative a questi limiti ma non influenzano direttamente il processo di validazione.</span></p></figcaption></figure><h2 id=\"understanding-the-bounds\">Comprendere i Limiti</h2><p>In breve, quando si confrontano due vettori ad alta dimensionalit√†, la similarit√† complessiva si trova tra le migliori e le peggiori similarit√† dei loro sottospazi, aggiustata per quanto grandi o importanti sono questi sottospazi nello schema generale. Questo √® ci√≤ che i limiti per la similarit√† del coseno in dimensioni superiori rappresentano intuitivamente: il bilanciamento tra le parti pi√π e meno simili, pesate dalla loro dimensione o importanza relativa.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Confronto illustrativo di due cappucci e corpi di penne stilo con sezioni etichettate su sfondo nero\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Ogni penna ha due componenti principali: il corpo e il cappuccio.</span></figcaption></figure><p>Immagina di dover confrontare due oggetti multi-parte (diciamo, due penne eleganti) basandoti sulla loro similarit√† complessiva. Ogni penna ha due componenti principali: il corpo e il cappuccio. La similarit√† dell'intera penna (sia corpo che cappuccio) √® ci√≤ che stiamo cercando di determinare:</p><h3 id=\"upper-bound-gamma-cdot-s\">Limite Superiore ($\\gamma \\cdot s$)</h3><p>Pensa a $s$ come al miglior abbinamento tra le parti corrispondenti delle penne. Se i cappucci sono molto simili ma i corpi no, $s$ √® la similarit√† dei cappucci.</p><p>Ora, $\\gamma$ √® come un fattore di scala basato sulla dimensione (o importanza) di ogni parte. Se una penna ha un corpo molto lungo e un cappuccio corto, mentre l'altra ha un corpo corto e un cappuccio lungo, $\\gamma$ aggiusta la similarit√† complessiva per tenere conto di queste differenze nelle proporzioni.</p><p>Il limite superiore ci dice che, non importa quanto simili siano alcune parti, la similarit√† complessiva non pu√≤ superare questa \"similarit√† della parte migliore\" scalata dal fattore di proporzione.</p><h3 id=\"lower-bound-gamma-cdot-t\">Limite Inferiore ($\\gamma \\cdot t$)</h3><p>Qui, $t$ √® la similarit√† delle parti che corrispondono meno. Se i corpi delle penne sono molto diversi ma i cappucci sono simili, $t$ riflette la similarit√† del corpo.</p><p>Ancora una volta, $\\gamma$ scala questo basandosi sulla proporzione di ogni parte.</p><p>Il limite inferiore significa che la similarit√† complessiva non pu√≤ essere peggiore di questa \"similarit√† della parte peggiore\" dopo aver tenuto conto della proporzione di ogni parte.</p><h2 id=\"implications-of-the-bounds\">Implicazioni dei Limiti</h2><p>Per gli ingegneri software che lavorano con embedding, ricerca vettoriale, recupero o database, comprendere questi limiti ha implicazioni pratiche, in particolare quando si ha a che fare con dati ad alta dimensionalit√†. La ricerca vettoriale spesso implica trovare i vettori pi√π vicini (pi√π simili) in un database rispetto a un vettore di query dato, tipicamente usando la similarit√† del coseno come misura di vicinanza. I limiti che abbiamo discusso possono fornire intuizioni sull'efficacia e le limitazioni dell'uso delle similarit√† dei sottospazi per tali compiti.</p><h3 id=\"using-subspace-similarity-for-ranking\">Uso della Similarit√† dei Sottospazi per il Ranking</h3><p><strong>Sicurezza e Accuratezza</strong>: Usare la similarit√† dei sottospazi per il ranking e il recupero dei top-k risultati pu√≤ essere efficace, ma con cautela. Il limite superiore indica che la similarit√† complessiva non pu√≤ superare la massima similarit√† dei sottospazi. Quindi, se una coppia di vettori √® altamente simile in un particolare sottospazio, √® un forte candidato per essere simile nello spazio ad alta dimensionalit√†.</p><p><strong>Potenziali Insidie</strong>: Tuttavia, il limite inferiore suggerisce che due vettori con bassa similarit√† in un sottospazio potrebbero ancora essere abbastanza simili nel complesso. Quindi, affidarsi solamente alla similarit√† dei sottospazi potrebbe far perdere alcuni risultati rilevanti.</p><h3 id=\"misconceptions-and-cautions\">Concezioni Errate e Precauzioni</h3><p><strong>Sovrastima dell'Importanza dei Sottospazi</strong>: Una concezione errata comune √® sovrastimare l'importanza di un particolare sottospazio. Mentre un'alta similarit√† in un sottospazio √® un buon indicatore, non garantisce un'alta similarit√† complessiva a causa dell'influenza degli altri sottospazi.</p><p><strong>Ignorare le Similarit√† Negative</strong>: Nei casi in cui la similarit√† del coseno in un sottospazio √® negativa, indica una relazione opposta in quella dimensione. Gli ingegneri dovrebbero fare attenzione a come queste similarit√† negative impattano sulla similarit√† complessiva.</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "Una elevata similarit√† nel sottospazio garantisce un'elevata similarit√† complessiva tra i vettori? Questo articolo esamina la teoria e le implicazioni pratiche della similarit√† nei sottospazi.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
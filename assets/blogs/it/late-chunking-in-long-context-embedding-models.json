{
  "slug": "late-chunking-in-long-context-embedding-models",
  "id": "66c72e30da9a33000146d836",
  "uuid": "9eda87e2-a799-4360-bac9-6a1cd0193349",
  "title": "Il Chunking tardivo nei modelli di embedding con contesto lungo",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Il Late Chunking √® ora disponibile nell'API <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>. <b><strong style=\"white-space: pre-wrap;\">Ordine di lettura consigliato: parte I, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper di ricerca</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><p></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What Late Chunking Really Is &amp; What It's Not: Part II</div><div class=\"kg-bookmark-description\">Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-4.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Novit√†! Parte II: approfondimento sui segnali di confine e sui malintesi.</span></p></figcaption></figure><p>Circa un anno fa, nell'ottobre 2023, abbiamo rilasciato <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">il primo modello di embedding open-source al mondo con una lunghezza di contesto di 8K</a>, <code>jina-embeddings-v2-base-en</code>. Da allora, c'√® stato un notevole dibattito sull'utilit√† del contesto lungo nei modelli di embedding. Per molte applicazioni, codificare un documento di migliaia di parole in una singola rappresentazione di embedding non √® ideale. Molti casi d'uso richiedono il recupero di porzioni pi√π piccole del testo, e i sistemi di recupero basati su vettori densi spesso funzionano meglio con segmenti di testo pi√π piccoli, poich√© la semantica ha meno probabilit√† di essere \"sovra-compressa\" nei vettori di embedding.</p><p>La Retrieval-Augmented Generation (RAG) √® una delle applicazioni pi√π note che richiede la suddivisione dei documenti in chunk di testo pi√π piccoli (diciamo entro 512 token). Questi chunk vengono solitamente memorizzati in un database vettoriale, con rappresentazioni vettoriali generate da un modello di embedding del testo. Durante l'esecuzione, lo stesso modello di embedding codifica una query in una rappresentazione vettoriale, che viene poi utilizzata per identificare i chunk di testo rilevanti memorizzati. Questi chunk vengono successivamente passati a un large language model (LLM), che sintetizza una risposta alla query basata sui testi recuperati.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--1-.svg\" class=\"kg-image\" alt=\"Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec\" loading=\"lazy\" width=\"1458\" height=\"307\"><figcaption><span style=\"white-space: pre-wrap;\">Una tipica pipeline RAG di chunking-embedding-retrieving-generating.</span></figcaption></figure><p>In breve, l'embedding di chunk pi√π piccoli sembra essere preferibile, in parte a causa delle dimensioni di input limitate degli LLM a valle, ma anche perch√© <strong>c'√® la preoccupazione che le informazioni contestuali importanti in un contesto lungo possano essere diluite quando compresse in un singolo vettore.</strong></p><p>Ma se l'industria ha bisogno solo di modelli di embedding con una lunghezza di contesto di 512, <em>qual √® il senso di addestrare modelli con una lunghezza di contesto di 8192?</em></p><p>In questo articolo, riesaminiamo questa domanda importante, anche se scomoda, esplorando i limiti della pipeline ingenua di chunking-embedding in RAG. Introduciamo un nuovo approccio chiamato <strong>\"Late Chunking,\"</strong> che sfrutta le ricche informazioni contestuali fornite dai modelli di embedding da 8192 per incorporare i chunk in modo pi√π efficace.</p><h2 id=\"the-lost-context-problem\">Il Problema del Contesto Perduto</h2><p>La semplice pipeline RAG di chunking-embedding-retrieving-generating non √® priva di sfide. In particolare, <strong>questo processo pu√≤ distruggere le dipendenze contestuali a lunga distanza.</strong> In altre parole, quando le informazioni rilevanti sono distribuite su pi√π chunk, estrarre segmenti di testo dal contesto pu√≤ renderli inefficaci, rendendo questo approccio particolarmente problematico.</p><p>Nell'immagine sottostante, un articolo di Wikipedia √® suddiviso in chunk di frasi. Si pu√≤ vedere che frasi come \"its\" e \"the city\" si riferiscono a \"Berlin\", che √® menzionata solo nella prima frase. Questo rende pi√π difficile per il modello di embedding collegare questi riferimenti all'entit√† corretta, producendo cos√¨ una rappresentazione vettoriale di qualit√† inferiore.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"Comparative panels display Berlin's Wikipedia article and its chunked text to highlight clarity and readability benefits.\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png 1774w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Questo significa che, se dividiamo un lungo articolo in chunk di lunghezza frase, come nell'esempio sopra, un sistema RAG potrebbe avere difficolt√† a rispondere a una query come \"Qual √® la popolazione di Berlino?\" Poich√© il nome della citt√† e la popolazione non appaiono mai insieme in un singolo chunk, e senza alcun contesto documentale pi√π ampio, un LLM presentato con uno di questi chunk non pu√≤ risolvere riferimenti anaforici come \"it\" o \"the city\".</p><p>Esistono alcune euristiche per alleviare questo problema, come il ricampionamento con una finestra scorrevole, l'uso di finestre di contesto multiple e l'esecuzione di scansioni del documento a pi√π passaggi. Tuttavia, come tutte le euristiche, questi approcci sono aleatori; possono funzionare in alcuni casi, ma non c'√® alcuna garanzia teorica della loro efficacia.</p><h2 id=\"the-solution-late-chunking\">La Soluzione: Late Chunking</h2><p>L'approccio di codifica ingenuo (come si vede sul lato sinistro dell'immagine sottostante) implica l'uso di frasi, paragrafi o limiti di lunghezza massima per dividere il testo <em>a priori</em>. Successivamente, un modello di embedding viene applicato ripetutamente a questi chunk risultanti. Per generare un singolo embedding per ogni chunk, molti modelli di embedding utilizzano il <em>mean pooling</em> su questi embedding a livello di token per produrre un singolo vettore di embedding.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--4-.svg\" class=\"kg-image\" alt=\"Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.\" loading=\"lazy\" width=\"1020\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Un'illustrazione della strategia di chunking ingenua (sinistra) e della strategia di late chunking (destra).</span></figcaption></figure><p>Al contrario, l'approccio \"Late Chunking\" che proponiamo in questo articolo applica prima il layer transformer del modello di embedding a <em>tutto il testo</em> o a quanto pi√π possibile. Questo genera una sequenza di rappresentazioni vettoriali per ogni token che comprende informazioni testuali dall'intero testo. Successivamente, il mean pooling viene applicato a ogni chunk di questa sequenza di vettori token, producendo embedding per ogni chunk che considerano il contesto dell'intero testo. A differenza dell'approccio di codifica ingenuo, che genera embedding di chunk indipendenti e identicamente distribuiti (i.i.d.), <strong>il late chunking crea un insieme di embedding di chunk dove ciascuno √® \"condizionato da\" quelli precedenti, codificando cos√¨ pi√π informazioni contestuali per ogni chunk.</strong></p><p>Ovviamente per applicare efficacemente il late chunking, abbiamo bisogno di modelli di embedding a lungo contesto come <code>jina-embeddings-v2-base-en</code>, che supportano fino a 8192 token‚Äîcirca dieci pagine standard di testo. Segmenti di testo di questa dimensione hanno molte meno probabilit√† di avere dipendenze contestuali che richiedono un contesto ancora pi√π lungo da risolvere.</p><p>√à importante sottolineare che il late chunking richiede ancora segnali di confine, ma questi segnali vengono utilizzati solo <em>dopo</em> aver ottenuto gli embedding a livello di token‚Äîda qui il termine \"late\" nel suo nome.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Chunking Ingenuo</th>\n<th>Late Chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Necessit√† di segnali di confine</td>\n<td>S√¨</td>\n<td>S√¨</td>\n</tr>\n<tr>\n<td>Uso dei segnali di confine</td>\n<td>Direttamente nel preprocessing</td>\n<td>Dopo aver ottenuto gli embedding a livello di token dal layer transformer</td>\n</tr>\n<tr>\n<td>Gli embedding di chunk risultanti</td>\n<td>i.i.d.</td>\n<td>Condizionati</td>\n</tr>\n<tr>\n<td>Informazioni contestuali dei chunk vicini</td>\n<td>Perse. Alcune euristiche (come il campionamento sovrapposto) per alleviare questo</td>\n<td>Ben preservate dai modelli di embedding a lungo contesto</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"implementation-and-qualitative-evaluation\">Implementazione e Valutazione Qualitativa</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/4c9d6ee1a7679cb6c4c106e58fabaf56/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>L'implementazione del late chunking pu√≤ essere trovata nel Google Colab linkato sopra. Qui, utilizziamo la nostra recente funzionalit√† rilasciata nella Tokenizer API, che sfrutta tutti i possibili segnali di confine per segmentare un lungo documento in chunk significativi. Ulteriori discussioni sull'algoritmo alla base di questa funzionalit√† possono essere trovate su X.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">API gratuita per tokenizzare il testo e segmentare testi lunghi in chunk.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,‚Ä¶ <a href=\"https://t.co/AtBCSrn7nI?ref=jina-ai-gmbh.ghost.io\">pic.twitter.com/AtBCSrn7nI</a></p>‚Äî Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1823756993108304135?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">August 14, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><p>Quando si applica il late chunking all'esempio di Wikipedia sopra riportato, si pu√≤ notare immediatamente un miglioramento nella similarit√† semantica. Per esempio, nel caso di \"la citt√†\" e \"Berlino\" all'interno di un articolo di Wikipedia, i vettori che rappresentano \"la citt√†\" contengono ora informazioni che la collegano alla precedente menzione di \"Berlino\", rendendola una corrispondenza molto migliore per le query che coinvolgono quel nome di citt√†.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Query</th>\n<th>Chunk</th>\n<th>Sim. on naive chunking</th>\n<th>Sim. on late chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Berlin</td>\n<td>Berlin is the capital and largest city of Germany, both by area and by population.</td>\n<td>0.849</td>\n<td>0.850</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.</td>\n<td>0.708</td>\n<td>0.825</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.</td>\n<td>0.753</td>\n<td>0.850</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Puoi osservare questo nei risultati numerici sopra riportati, che confrontano l'embedding del termine \"Berlin\" con varie frasi dell'articolo su Berlino utilizzando la similarit√† del coseno. La colonna \"Sim. on IID chunk embeddings\" mostra i valori di similarit√† tra l'embedding della query \"Berlin\" e gli embedding utilizzando il chunking <em>a priori</em>, mentre \"Sim. under contextual chunk embedding\" rappresenta i risultati con il metodo di late chunking.</p><h2 id=\"quantitative-evaluation-on-beir\">Valutazione Quantitativa su BEIR</h2><p>Per verificare l'efficacia del late chunking oltre un semplice esempio, lo abbiamo testato utilizzando alcuni dei benchmark di recupero da <a href=\"https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">BeIR</a>. Questi task di recupero consistono in un set di query, un corpus di documenti di testo e un file QRels che memorizza informazioni sugli ID dei documenti rilevanti per ciascuna query.</p><p>Per identificare i documenti rilevanti per una query, i documenti vengono suddivisi in chunk, codificati in un indice di embedding e i chunk pi√π simili vengono determinati per ogni embedding di query utilizzando i k-nearest neighbors (kNN). Poich√© ogni chunk corrisponde a un documento, il ranking kNN dei chunk pu√≤ essere convertito in un ranking kNN dei documenti (mantenendo solo la prima occorrenza per i documenti che appaiono pi√π volte nel ranking). Questo ranking risultante viene poi confrontato con il ranking fornito dal file QRels di ground-truth, e vengono calcolate metriche di recupero come nDCG@10. Questa procedura √® illustrata di seguito, e lo script di valutazione pu√≤ essere trovato in questo repository per la riproducibilit√†.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/late-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)</div><div class=\"kg-bookmark-description\">Code for explaining and evaluating late chunking (chunked pooling) - jina-ai/late-chunking</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/bf0bb9d5ca928dc3fe25ae621398af0fdf5e34324e37cbeee6fa4189218c9b4d/jina-ai/late-chunking\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Abbiamo eseguito questa valutazione su vari dataset BeIR, confrontando il chunking naive con il nostro metodo di late chunking. Per ottenere gli indizi di confine, abbiamo utilizzato una regex che divide i testi in stringhe di circa 256 token. Sia la valutazione naive che quella late chunking hanno utilizzato <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\"><code>jina-embeddings-v2-small-en</code></a> come modello di embedding; una versione pi√π piccola del modello <code>v2-base-en</code> che supporta ancora una lunghezza fino a 8192 token. I risultati si trovano nella tabella seguente.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Avg. Document Length (characters)</th>\n<th>Naive Chunking (nDCG@10)</th>\n<th>Late Chunking (nDCG@10)</th>\n<th>No Chunking (nDCG@10)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td>1498.4</td>\n<td>64.20%</td>\n<td><strong>66.10%</strong></td>\n<td>63.89%</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td>1116.7</td>\n<td>63.36%</td>\n<td>64.70%</td>\n<td><strong>65.18%</strong></td>\n</tr>\n<tr>\n<td>FiQA2018</td>\n<td>767.2</td>\n<td>33.25%</td>\n<td><strong>33.84%</strong></td>\n<td>33.43%</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>1589.8</td>\n<td>23.46%</td>\n<td>29.98%</td>\n<td><strong>30.40%</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>62.2</td>\n<td>87.19%</td>\n<td>87.19%</td>\n<td>87.19%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>In tutti i casi, il late chunking ha migliorato i punteggi rispetto all'approccio naive. In alcuni casi, ha anche superato la codifica dell'intero documento in un singolo embedding, mentre in altri dataset, non effettuare chunking ha prodotto i risultati migliori (Ovviamente, il no chunking ha senso solo se non c'√® bisogno di classificare i chunk, cosa rara nella pratica). Se tracciamo il divario di prestazioni tra l'approccio naive e il late chunking rispetto alla lunghezza del documento, diventa evidente che la lunghezza media dei documenti √® correlata a maggiori miglioramenti nei punteggi nDCG attraverso il late chunking. In altre parole, <strong>pi√π lungo √® il documento, pi√π efficace diventa la strategia di late chunking.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/chart--22-.svg\" class=\"kg-image\" alt=\"Line graph showing the decline in relative improvement with increasing document length, from 0 to 1500 characters.\" loading=\"lazy\" width=\"582\" height=\"337\"><figcaption><span style=\"white-space: pre-wrap;\">Il miglioramento del late chunking rispetto al chunking naive √® correlato con la lunghezza media del documento.</span></figcaption></figure><h2 id=\"conclusion\">Conclusione</h2><p>In questo articolo, abbiamo introdotto un approccio semplice chiamato \"late chunking\" per incorporare chunk brevi sfruttando la potenza dei modelli di embedding a lungo contesto. Abbiamo dimostrato come l'embedding di chunk i.i.d. tradizionale non riesca a preservare le informazioni contestuali, portando a un recupero subottimale; e come il late chunking offra una soluzione semplice ma altamente efficace per mantenere e condizionare le informazioni contestuali all'interno di ciascun chunk. L'efficacia del late chunking diventa sempre pi√π significativa sui documenti pi√π lunghi‚Äîuna capacit√† resa possibile <em>solo</em> da modelli di embedding a lungo contesto avanzati come <code>jina-embeddings-v2-base-en</code>. Speriamo che questo lavoro non solo convalidi l'importanza dei modelli di embedding a lungo contesto ma ispiri anche ulteriori ricerche su questo argomento.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cosa √® Realmente il Late Chunking e Cosa Non Lo √à: Parte II</div><div class=\"kg-bookmark-description\">Parte 2 della nostra esplorazione del Late Chunking, un'analisi approfondita del perch√© √® il miglior metodo per gli embedding dei chunk e il miglioramento delle prestazioni di ricerca/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-5.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Continua a leggere la parte II: approfondimento sugli indizi di confine e sui malintesi.</span></p></figcaption></figure>",
  "comment_id": "66c72e30da9a33000146d836",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-22T14:25:20.000+02:00",
  "updated_at": "2024-10-06T16:29:02.000+02:00",
  "published_at": "2024-08-22T17:06:17.000+02:00",
  "custom_excerpt": "Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael G√ºnther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael G√ºnther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/late-chunking-in-long-context-embedding-models/",
  "excerpt": "La suddivisione di documenti lunghi mantenendo le informazioni contestuali √® una sfida. Introduciamo il \"Late Chunking\" che sfrutta i modelli di embedding a lungo contesto per generare embedding contestuali dei chunk per migliori applicazioni di recupero.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating the 'Late Chunking' and 'Long Document Model' processes in machine learning on a black background.",
  "feature_image_caption": null
}
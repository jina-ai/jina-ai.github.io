{
  "slug": "what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
  "id": "65d3a2134a32310001f5b71b",
  "uuid": "726c942b-f6a7-4c89-a0ad-39aaad98d02f",
  "title": "Cosa sono ColBERT e Late Interaction e perché sono importanti nella ricerca?",
  "html": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina ColBERT v2: Retriever multilingue a interazione tardiva per embedding e riordinamento</div><div class=\"kg-bookmark-description\">Jina ColBERT v2 supporta 89 lingue con prestazioni di recupero superiori, dimensioni di output controllate dall'utente e lunghezza token di 8192.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg\" alt=\"\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Aggiornamento: Il 31 agosto 2024, abbiamo rilasciato la seconda versione di Jina-ColBERT, con prestazioni migliorate, supporto multilingue per 89 lingue e dimensioni di output flessibili. Consulta il post di rilascio per maggiori dettagli.</span></p></figcaption></figure><p>Venerdì scorso, il rilascio del <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">modello ColBERT da parte di Jina AI su Hugging Face</a> ha suscitato notevole entusiasmo nella comunità AI, in particolare su Twitter/X. Mentre molti hanno familiarità con il rivoluzionario modello BERT, il fermento intorno a ColBERT ha lasciato alcuni a chiedersi: Cosa rende ColBERT così speciale nel affollato campo delle tecnologie di recupero informazioni? Perché la comunità AI è entusiasta del ColBERT con lunghezza 8192? Questo articolo si addentra nelle complessità di ColBERT e ColBERTv2, evidenziando il loro design, i miglioramenti e la sorprendente efficacia dell'interazione tardiva di ColBERT.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Massimizza la rilevanza della ricerca e l'accuratezza RAG con facilità</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">@huggingface</a> under Apache 2.0 licence<a href=\"https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io\">https://t.co/snVGgI753H</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">February 16, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><h2 id=\"what-is-colbert\">Cos'è ColBERT?</h2><p>Il nome \"ColBERT\" sta per <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, un modello che nasce dall'Università di Stanford, che sfrutta la profonda comprensione linguistica di BERT introducendo un nuovo meccanismo di interazione. Questo meccanismo, noto come <strong>interazione tardiva</strong>, permette un recupero efficiente e preciso elaborando query e documenti separatamente fino alle fasi finali del processo di recupero. In particolare, esistono due versioni del modello:</p><ul><li><strong>ColBERT</strong>: Il modello iniziale è stato ideato da <a href=\"https://x.com/lateinteraction?s=20&ref=jina-ai-gmbh.ghost.io\"><strong>Omar Khattab</strong></a><strong> e Matei Zaharia</strong>, presentando un approccio innovativo al recupero di informazioni attraverso l'articolo \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\". Il loro lavoro è stato pubblicato a SIGIR 2020.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class=\"kg-bookmark-description\">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Omar Khattab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">L'articolo originale di ColBERT che introduce l'\"interazione tardiva\".</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Basandosi sul lavoro fondamentale, <strong>Omar Khattab</strong> ha continuato la sua ricerca, collaborando con <strong>Barlas Oguz, Matei Zaharia e Michael S. Bernstein</strong> per introdurre \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", presentato a SIGIR 2021. Questa nuova iterazione di ColBERT ha affrontato le limitazioni precedenti e ha introdotto miglioramenti chiave, come la <strong>supervisione denoised</strong> e la <strong>compressione residua</strong>, migliorando sia l'efficacia del recupero del modello che la sua efficienza di archiviazione.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class=\"kg-bookmark-description\">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Keshav Santhanam</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">ColBERTv2 aggiunge la supervisione denoised e la compressione residua per migliorare la qualità dei dati di training e ridurre l'occupazione di spazio.</span></p></figcaption></figure><h2 id=\"understand-colberts-design\">Comprendere il Design di ColBERT</h2><p>Dato che l'architettura di ColBERTv2 rimane molto simile a quella del ColBERT originale, con le sue innovazioni chiave che ruotano attorno alle tecniche di training e ai meccanismi di compressione, approfondiremo prima gli aspetti fondamentali del ColBERT originale.</p><h3 id=\"what-is-late-interaction-in-colbert\">Cos'è l'interazione tardiva in ColBERT?</h3><p>\"Interazione\" si riferisce al processo di valutazione della rilevanza tra una query e un documento confrontando le loro rappresentazioni.</p><p>\"<em>Interazione tardiva</em>\" è l'essenza di ColBERT. Il termine deriva dall'architettura del modello e dalla strategia di elaborazione, dove l'interazione tra le rappresentazioni della query e del documento avviene tardi nel processo, dopo che entrambi sono stati codificati indipendentemente. Questo contrasta con i modelli a \"<em>interazione precoce</em>\", dove gli embedding di query e documenti interagiscono nelle fasi iniziali, tipicamente prima o durante la loro codifica da parte del modello.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Models</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Early Interaction</td>\n<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>\n</tr>\n<tr>\n<td>Late Interaction</td>\n<td>ColBERT, ColBERTv2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>L'interazione precoce può aumentare la complessità computazionale poiché richiede di considerare tutte le possibili coppie query-documento, rendendola meno efficiente per applicazioni su larga scala.</p><p>I modelli a interazione tardiva come ColBERT ottimizzano l'efficienza e la scalabilità permettendo il pre-calcolo delle rappresentazioni dei documenti e impiegando un passaggio di interazione più leggero alla fine, che si concentra sulle rappresentazioni già codificate. Questa scelta di design consente tempi di recupero più rapidi e richieste computazionali ridotte, rendendolo più adatto per l'elaborazione di grandi collezioni di documenti.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg\" class=\"kg-image\" alt=\"Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep\" loading=\"lazy\" width=\"300\" height=\"143\"><figcaption><span style=\"white-space: pre-wrap;\">Diagrammi schematici che illustrano i paradigmi di interazione query-documento nel recupero di informazioni neurale, con l'interazione tardiva di ColBERT sulla sinistra.</span></figcaption></figure>\n\n<h3 id=\"no-interaction-cosine-similarity-of-document-and-query-embeddings\">Nessuna interazione: similarità del coseno tra gli embedding di documenti e query</h3>\n\n<p>Molti database vettoriali pratici e soluzioni di ricerca neurale dipendono dal rapido matching della similarità del coseno tra gli embedding di documenti e query. Sebbene sia attraente per la sua semplicità ed efficienza computazionale, questo metodo, spesso definito come \"<em>nessuna interazione</em>\" o \"<em>non basato sull'interazione</em>\" si è dimostrato meno performante rispetto ai modelli che incorporano qualche forma di interazione tra query e documenti.</p>\n\n<p>Il limite principale dell'approccio \"nessuna interazione\" risiede nella sua incapacità di catturare le sfumature complesse e le relazioni tra i termini della query e del documento. Il recupero delle informazioni, nel suo nucleo, riguarda la comprensione e il matching dell'intento dietro una query con il contenuto di un documento. Questo processo spesso richiede una comprensione profonda e contestuale dei termini coinvolti, qualcosa che gli embedding singoli e aggregati per documenti e query faticano a fornire.</p>\n\n<h2 id=\"query-and-document-encoders-in-colbert\">Codificatori di query e documenti in ColBERT</h2>\n\n<p>La strategia di codifica di ColBERT si basa sul modello BERT, noto per la sua profonda comprensione contestuale del linguaggio. Il modello genera rappresentazioni vettoriali dense per ogni token in una query o documento, <strong>creando rispettivamente un insieme di embedding contestualizzati per una query e un insieme per un documento.</strong> Questo facilita un confronto sfumato dei loro embedding durante la fase di interazione tardiva.</p>\n\n<h3 id=\"query-encoder-of-colbert\">Codificatore di query di ColBERT</h3>\n\n<p>Per una query $Q$ con token ${q_1, q_2, ..., q_l}$, il processo inizia tokenizzando $Q$ in token WordPiece basati su BERT e anteponendo un token speciale <code>[Q]</code>. Questo token <code>[Q]</code>, posizionato subito dopo il token <code>[CLS]</code> di BERT, segnala l'inizio di una query.</p>\n\n<p>Se la query è più corta di un numero predefinito di token $N_q$, viene riempita con token <code>[mask]</code> fino a $N_q$; altrimenti, viene troncata ai primi $N_q$ token. La sequenza riempita viene quindi passata attraverso BERT, seguita da una CNN (Rete Neurale Convoluzionale) e normalizzazione. L'output è un insieme di vettori di embedding indicati come $\\mathbf{E}_q$ di seguito:<br>$$\\mathbf{E}_q := \\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)$$</p>\n\n<h3 id=\"document-encoder-of-colbert\">Codificatore di documenti di ColBERT</h3>\n\n<p>Analogamente, per un documento $D$ con token ${d_1, d_2, ..., d_n}$, un token <code>[D]</code> viene anteposto per indicare l'inizio di un documento. Questa sequenza, senza necessità di riempimento, subisce lo stesso processo, risultando in un insieme di vettori di embedding indicati come $\\mathbf{E}_d$ di seguito:<br>$$\\mathbf{E}_d := \\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)$$</p>\n\n<p>L'uso dei token <code>[mask]</code> per il riempimento delle query (definito come <strong>augmentation della query</strong> nell'articolo) garantisce una lunghezza uniforme per tutte le query, facilitando l'elaborazione in batch. I token <code>[Q]</code> e <code>[D]</code> marcano esplicitamente l'inizio di query e documenti, rispettivamente, aiutando il modello a distinguere tra i due tipi di input.</p>\n\n<h3 id=\"comparing-colbert-to-cross-encoders\">Confronto tra ColBERT e cross-encoder</h3>\n\n<p>I cross-encoder elaborano coppie di query e documenti insieme, rendendoli molto accurati ma meno efficienti per compiti su larga scala a causa del costo computazionale di valutare ogni possibile coppia. Eccellono in scenari specifici dove è necessario il punteggio preciso di coppie di frasi, come nei compiti di similarità semantica o nel confronto dettagliato dei contenuti. Tuttavia, questo design limita la loro applicabilità in situazioni che richiedono un recupero rapido da grandi dataset, dove sono fondamentali gli embedding pre-calcolati e i calcoli efficienti della similarità.</p>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg\" class=\"kg-image\" alt=\"Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum\" loading=\"lazy\" width=\"210\" height=\"150\"></figure>\n\n<p>Al contrario, il modello di interazione tardiva di ColBERT permette il pre-calcolo degli embedding dei documenti, accelerando significativamente il processo di recupero senza compromettere la profondità dell'analisi semantica. Questo metodo, sebbene apparentemente controintuitivo rispetto all'approccio diretto dei cross-encoder, offre una soluzione scalabile per compiti di recupero di informazioni in tempo reale e su larga scala. Rappresenta un compromesso strategico tra efficienza computazionale e qualità della modellazione dell'interazione.</p>\n\n<h2 id=\"finding-the-top-k-documents-using-colbert\">Trovare i migliori K documenti usando ColBERT</h2>\n\n<p>Una volta che abbiamo gli embedding per la query e i documenti, trovare i K documenti più rilevanti diventa semplice (ma non semplice come calcolare il coseno di due vettori).</p>\n\n<p>Le operazioni chiave includono un prodotto scalare in batch per calcolare le similarità termine per termine, max-pooling sui termini del documento per trovare la similarità più alta per ogni termine della query, e somma sui termini della query per derivare il punteggio totale del documento, seguita dall'ordinamento dei documenti basato su questi punteggi. Il codice pseudo PyTorch è descritto di seguito:</p>\n\n<pre><code class=\"language-python\">import torch\n\ndef compute_relevance_scores(query_embeddings, document_embeddings, k):\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n    \n    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]\n    :param k: Number of top documents to re-rank\n    :return: Sorted document indices based on their relevance scores\n    \"\"\"\n    \n    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]\n    # Pad the k documents to their maximum length for batch operations\n    # Note: Assuming document_embeddings is already padded and moved to GPU\n    \n    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)\n    # Resulting shape: [k, num_query_terms, max_doc_length]\n    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n    \n    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term\n    # Shape after max-pool: [k, num_query_terms]\n    max_scores_per_query_term = scores.max(dim=2).values\n    \n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [k]\n    total_scores = max_scores_per_query_term.sum(dim=1)\n    \n    # Sort the documents based on their total scores\n    sorted_indices = total_scores.argsort(descending=True)\n    \n    return sorted_indices\n</code></pre>\n\n<p>Nota che questa procedura viene utilizzata sia durante l'addestramento che durante il re-ranking in fase di inferenza. Il modello ColBERT viene addestrato utilizzando una loss di ranking a coppie, dove i dati di addestramento consistono in triple $(q, d^+, d^-)$, dove $q$ rappresenta una query, $d^+$ è un documento rilevante (positivo) per la query, e $d^-$ è un documento non rilevante (negativo). Il modello mira ad apprendere rappresentazioni tali che il punteggio di similarità tra $q$ e $d^+$ sia più alto del punteggio tra q e $d^-$.</p>\n\n<p>L'obiettivo dell'addestramento può essere matematicamente rappresentato come la minimizzazione della seguente funzione di loss: $$\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))$$</p>\n\n<p>, dove $S(q, d)$ denota il punteggio di similarità calcolato da ColBERT tra una query $q$ e un documento $d$. Questo punteggio si ottiene aggregando i punteggi di max-similarità degli embedding che corrispondono meglio tra la query e il documento, seguendo il pattern di interazione tardiva descritto nell'architettura del modello. Questo approccio assicura che il modello sia addestrato a distinguere tra documenti rilevanti e irrilevanti per una data query, incoraggiando un margine maggiore nei punteggi di similarità per coppie di documenti positivi e negativi.</p>\n\n<h3 id=\"denoised-supervision-in-colbertv2\">Supervisione denoised in ColBERTv2</h3>\n\n<p>La supervisione denoised in ColBERTv2 raffina il processo di addestramento originale selezionando negativi impegnativi e sfruttando un cross-encoder per la distillazione. Questo metodo sofisticato di miglioramento della qualità dei dati di addestramento coinvolge diversi passaggi:</p>\n\n<ol>\n<li><strong>Addestramento Iniziale</strong>: Utilizzo delle triple ufficiali dal dataset MS MARCO, comprendenti una query, un documento rilevante e un documento non rilevante.</li>\n<li><strong>Indicizzazione e Recupero</strong>: Impiego della compressione di ColBERTv2 per indicizzare i passaggi di addestramento, seguito dal recupero dei passaggi top-k per ogni query.</li>\n<li><strong>Reranking con Cross-Encoder</strong>: Miglioramento della selezione dei passaggi attraverso il reranking mediante un cross-encoder MiniLM, distillando i suoi punteggi in ColBERTv2.</li>\n<li><strong>Formazione delle Tuple di Addestramento</strong>: Generazione di tuple w-way per l'addestramento, incorporando passaggi sia ad alto che a basso ranking per creare esempi impegnativi.</li>\n<li><strong>Raffinamento Iterativo</strong>: Ripetizione del processo per migliorare continuamente la selezione dei negativi difficili, migliorando così le prestazioni del modello.</li>\n</ol>\n\n<p>Nota, questo processo rappresenta un miglioramento sofisticato del regime di addestramento di ColBERT piuttosto che un cambiamento fondamentale della sua architettura.</p>\n\n<h3 id=\"hyperparameters-of-colbert\">Iperparametri di ColBERT</h3><p>Di seguito sono riassunti gli iperparametri di ColBERT:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Hyperparameter</th>\n<th>Best Choice</th>\n<th>Reason</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning Rate</td>\n<td>3 x 10^{-6}</td>\n<td>Selezionato per il fine-tuning per garantire aggiornamenti del modello stabili ed efficaci.</td>\n</tr>\n<tr>\n<td>Batch Size</td>\n<td>32</td>\n<td>Bilancia l'efficienza computazionale e la capacità di catturare informazioni sufficienti per aggiornamento.</td>\n</tr>\n<tr>\n<td>Number of Embeddings per Query (Nq)</td>\n<td>32</td>\n<td>Fissato per garantire una dimensione di rappresentazione consistente tra le query, facilitando l'elaborazione efficiente.</td>\n</tr>\n<tr>\n<td>Embedding Dimension (m)</td>\n<td>128</td>\n<td>Ha dimostrato di fornire un buon equilibrio tra potere di rappresentazione ed efficienza computazionale.</td>\n</tr>\n<tr>\n<td>Training Iterations</td>\n<td>200k (MS MARCO), 125k (TREC CAR)</td>\n<td>Scelto per garantire un apprendimento approfondito evitando l'overfitting, con aggiustamenti basati sulle caratteristiche del dataset.</td>\n</tr>\n<tr>\n<td>Bytes per Dimension in Embeddings</td>\n<td>4 (re-ranking), 2 (end-to-end ranking)</td>\n<td>Compromesso tra precisione ed efficienza dello spazio, considerando il contesto applicativo (re-ranking vs. end-to-end).</td>\n</tr>\n<tr>\n<td>Vector-Similarity Function</td>\n<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>\n<td>Selezionata in base alle prestazioni e all'efficienza nei rispettivi contesti di recupero.</td>\n</tr>\n<tr>\n<td>FAISS Index Partitions (P)</td>\n<td>2000</td>\n<td>Determina la granularità del partizionamento dello spazio di ricerca, influenzando l'efficienza della ricerca.</td>\n</tr>\n<tr>\n<td>Nearest Partitions Searched (p)</td>\n<td>10</td>\n<td>Bilancia l'ampiezza della ricerca con l'efficienza computazionale.</td>\n</tr>\n<tr>\n<td>Sub-vectors per Embedding (s)</td>\n<td>16</td>\n<td>Influisce sulla granularità della quantizzazione, influenzando sia la velocità di ricerca che l'uso della memoria.</td>\n</tr>\n<tr>\n<td>Index Representation per Dimension</td>\n<td>16-bit values</td>\n<td>Scelto per la seconda fase del recupero end-to-end per gestire il compromesso tra accuratezza e spazio.</td>\n</tr>\n<tr>\n<td>Number of Layers in Encoders</td>\n<td>12-layer BERT</td>\n<td>Equilibrio ottimale tra profondità della comprensione contestuale ed efficienza computazionale.</td>\n</tr>\n<tr>\n<td>Max Query Length</td>\n<td>128</td>\n<td>Il numero massimo di token elaborati dall'encoder della query. <b>Questo viene esteso nel modello Jina-ColBERT.</b></td>\n</tr>\n<tr>\n<td>Max Document Length</td>\n<td>512</td>\n<td>Il numero massimo di token elaborati dall'encoder del documento. <b>Questo viene esteso a 8192 nel modello Jina-ColBERT.</b></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"the-indexing-strategy-of-colbert\">La strategia di indicizzazione di ColBERT</h2><p>A differenza degli approcci basati sulla rappresentazione che codificano ogni documento in un unico vettore di embedding, <strong>ColBERT codifica documenti (e query) in insiemi di embedding, con ogni token in un documento che ha il proprio embedding.</strong> Questo approccio significa intrinsecamente che per documenti più lunghi, verranno memorizzati più embedding, <strong>che è un punto dolente del ColBERT originale, successivamente risolto da ColBERTv2.</strong></p><p>La chiave per gestire questo efficacemente risiede nell'uso di database vettoriali da parte di ColBERT (es. <a href=\"https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io\">FAISS</a>) per l'indicizzazione e il recupero, e nel suo dettagliato processo di indicizzazione progettato per gestire efficientemente grandi volumi di dati. Il paper originale di ColBERT menziona diverse strategie per migliorare l'efficienza dell'indicizzazione e del recupero, tra cui:</p><ul><li><strong>Indicizzazione Offline</strong>: Le rappresentazioni dei documenti vengono calcolate offline, permettendo il pre-calcolo e l'archiviazione degli embedding dei documenti. Questo processo sfrutta l'elaborazione batch e l'accelerazione GPU per gestire efficientemente grandi collezioni di documenti.</li><li><strong>Archiviazione degli Embedding</strong>: Gli embedding dei documenti possono essere memorizzati utilizzando valori a 32-bit o 16-bit per ogni dimensione, offrendo un compromesso tra precisione e requisiti di archiviazione. Questa flessibilità permette a ColBERT di mantenere un equilibrio tra efficacia (in termini di prestazioni di recupero) ed efficienza (in termini di costi di archiviazione e computazionali).</li></ul><p>L'introduzione della <strong>compressione residua</strong> in ColBERTv2, che è un approccio innovativo non presente nel ColBERT originale, gioca un ruolo chiave nella riduzione dell'impronta di spazio del modello di 6-10 volte mantenendo la qualità. Questa tecnica comprime ulteriormente gli embedding catturando e memorizzando efficacemente solo le differenze da un set di centroidi di riferimento fissi.</p><h2 id=\"effectiveness-and-efficiency-of-colbert\">Efficacia ed Efficienza di ColBERT</h2><p>Si potrebbe inizialmente presumere che incorporare la comprensione contestuale profonda di BERT nella ricerca richiederebbe intrinsecamente risorse computazionali significative, rendendo tale approccio meno fattibile per applicazioni in tempo reale a causa dell'alta latenza e dei costi computazionali. Tuttavia, ColBERT sfida e rovescia questa assunzione attraverso il suo uso innovativo del meccanismo di interazione tardiva. Ecco alcuni punti degni di nota:</p><ol><li><strong>Guadagni Significativi in Efficienza</strong>: ColBERT ottiene una riduzione di ordini di grandezza nei costi computazionali (FLOPs) e nella latenza rispetto ai modelli di ranking tradizionali basati su BERT. Nello specifico, per una data dimensione del modello (es. encoder transformer a 12 strati \"base\"), ColBERT non solo eguaglia ma in alcuni casi supera l'efficacia dei modelli basati su BERT con richieste computazionali drasticamente inferiori. Per esempio, a una profondità di re-ranking di <em>k</em>=10, BERT richiede quasi 180× più FLOPs di ColBERT; questo divario si allarga con l'aumentare di <em>k</em>, raggiungendo 13900× a <em>k</em>=1000 e persino 23000× a <em>k</em>=2000.</li><li><strong>Migliore Recall e MRR@10 nel Recupero End-to-End</strong>: Contrariamente all'intuizione iniziale che un'interazione più profonda tra le rappresentazioni di query e documento (come visto nei modelli di interazione precoce) sarebbe necessaria per prestazioni di recupero elevate, la configurazione di recupero end-to-end di ColBERT dimostra un'efficacia superiore. Per esempio, il suo Recall@50 supera il Recall@1000 del BM25 ufficiale e quasi tutti i Recall@200 degli altri modelli, sottolineando la notevole capacità del modello di recuperare documenti rilevanti da una vasta collezione senza confronto diretto di ogni coppia query-documento.</li><li><strong>Praticità per Applicazioni Real-World</strong>: I risultati sperimentali sottolineano l'applicabilità pratica di ColBERT per scenari del mondo reale. Il suo throughput di indicizzazione e l'efficienza della memoria lo rendono adatto per indicizzare grandi collezioni di documenti come MS MARCO in poche ore, mantenendo un'alta efficacia con un'impronta di spazio gestibile. Queste qualità evidenziano l'idoneità di ColBERT per il deployment in ambienti di produzione dove sono fondamentali sia le prestazioni che l'efficienza computazionale.</li><li><strong>Scalabilità con la Dimensione della Collezione di Documenti</strong>: Forse la conclusione più sorprendente è la scalabilità e l'efficienza di ColBERT nel gestire collezioni di documenti su larga scala. L'architettura permette il pre-calcolo degli embedding dei documenti e sfrutta l'elaborazione batch efficiente per l'interazione query-documento, permettendo al sistema di scalare efficacemente con la dimensione della collezione di documenti. Questa scalabilità è controintuitiva considerando la complessità e la profondità di comprensione richieste per un recupero efficace dei documenti, mostrando l'approccio innovativo di ColBERT nel bilanciare efficienza computazionale ed efficacia del recupero.</li></ol><h2 id=\"using-jina-colbert-v1-en-a-8192-length-colbertv2-model\">Utilizzo di <code>jina-colbert-v1-en</code>: un modello ColBERTv2 con lunghezza 8192</h2><p>Jina-ColBERT è progettato per un recupero sia veloce che accurato, supportando <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">lunghezze di contesto più lunghe fino a 8192, sfruttando i progressi di JinaBERT</a>, che permette l'elaborazione di sequenze più lunghe grazie ai miglioramenti della sua architettura.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Rigorosamente parlando, Jina-ColBERT supporta una lunghezza di 8190 token. Ricordiamo che nell'encoder di documenti ColBERT, ogni documento viene riempito con <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">[D],[CLS]</code> all'inizio.</div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-colbert-v1-en · Hugging Face</div><div class=\"kg-bookmark-description\">Siamo in un viaggio per far progredire e democratizzare l'intelligenza artificiale attraverso l'open source e la scienza aperta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png\" alt=\"\"></div></a></figure><h3 id=\"jinas-improvement-over-original-colbert\">I miglioramenti di Jina rispetto al ColBERT originale</h3><p>Il principale progresso di Jina-ColBERT è il suo backbone, <code>jina-bert-v2-base-en</code>, che permette l'elaborazione di contesti significativamente più lunghi (fino a 8192 token) rispetto al ColBERT originale che usa <code>bert-base-uncased</code>. Questa capacità è cruciale per gestire documenti con contenuto esteso, fornendo risultati di ricerca più dettagliati e contestuali.</p><h3 id=\"jina-colbert-v1-en-performance-comparison-vs-colbertv2\">Confronto delle prestazioni di <code>jina-colbert-v1-en</code> vs. ColBERTv2</h3><p>Abbiamo valutato <code>jina-colbert-v1-en</code> sui dataset BEIR e sul nuovo benchmark LoCo che favorisce il contesto lungo, testandolo contro l'implementazione originale di ColBERTv2 e i modelli basati su non-interazione</p>modello <code>jina-embeddings-v2-base-en</code>.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>ColBERTv2</th>\n<th>jina-colbert-v1-en</th>\n<th>jina-embeddings-v2-base-en</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Arguana</td>\n<td>46.5</td>\n<td><strong>49.4</strong></td>\n<td>44.0</td>\n</tr>\n<tr>\n<td>Climate-Fever</td>\n<td>18.1</td>\n<td>19.6</td>\n<td><strong>23.5</strong></td>\n</tr>\n<tr>\n<td>DBPedia</td>\n<td><strong>45.2</strong></td>\n<td>41.3</td>\n<td>35.1</td>\n</tr>\n<tr>\n<td>FEVER</td>\n<td>78.8</td>\n<td><strong>79.5</strong></td>\n<td>72.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td>35.4</td>\n<td>36.8</td>\n<td><strong>41.6</strong></td>\n</tr>\n<tr>\n<td>HotpotQA</td>\n<td><strong>67.5</strong></td>\n<td>65.9</td>\n<td>61.4</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>33.7</td>\n<td><strong>33.8</strong></td>\n<td>32.5</td>\n</tr>\n<tr>\n<td>NQ</td>\n<td>56.1</td>\n<td>54.9</td>\n<td><strong>60.4</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>85.5</td>\n<td>82.3</td>\n<td><strong>88.2</strong></td>\n</tr>\n<tr>\n<td>SCIDOCS</td>\n<td>15.4</td>\n<td>16.9</td>\n<td><strong>19.9</strong></td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td>68.9</td>\n<td><strong>70.1</strong></td>\n<td>66.7</td>\n</tr>\n<tr>\n<td>TREC-COVID</td>\n<td>72.6</td>\n<td><strong>75.0</strong></td>\n<td>65.9</td>\n</tr>\n<tr>\n<td>Webis-touch2020</td>\n<td>26.0</td>\n<td><strong>27.0</strong></td>\n<td>26.2</td>\n</tr>\n<tr>\n<td>LoCo</td>\n<td>74.3</td>\n<td>83.7</td>\n<td><strong>85.4</strong></td>\n</tr>\n<tr>\n<td>Average</td>\n<td>51.7</td>\n<td><strong>52.6</strong></td>\n<td>51.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Questa tabella dimostra le prestazioni superiori di <code>jina-colbert-v1-en</code>, specialmente in scenari che richiedono lunghezze di contesto più lunghe rispetto al ColBERTv2 originale. Da notare che <code>jina-embeddings-v2-base-en</code> <a href=\"https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io\">utilizza più dati di addestramento</a>, mentre <code>jina-colbert-v1-en</code> usa solo MSMARCO, il che può giustificare le buone prestazioni di <code>jina-embeddings-v2-base-en</code> su alcuni task.</p><h3 id=\"example-usage-of-jina-colbert-v1-en\">Esempio di utilizzo di <code>jina-colbert-v1-en</code></h3><p>Questo snippet illustra il processo di indicizzazione con Jina-ColBERT, mostrando il suo supporto per documenti lunghi.</p><pre><code class=\"language-python\">from colbert import Indexer\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\n\nn_gpu: int = 1  # Set your number of available GPUs\nexperiment: str = \"\"  # Name of the folder where the logs and created indices will be stored\nindex_name: str = \"\"  # The name of your index, i.e. the name of your vector database\n\nif __name__ == \"__main__\":\n    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):\n        config = ColBERTConfig(\n          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents\n        )\n        indexer = Indexer(\n          checkpoint=\"jinaai/jina-colbert-v1-en\",\n          config=config,\n        )\n        documents = [\n          \"ColBERT is an efficient and effective passage retrieval model.\",\n          \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.\",\n          \"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.\",\n          \"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.\",\n          \"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.\",\n          \"Jina is an easier way to build neural search systems.\",\n          \"You can use Jina-ColBERT to build neural search systems with ease.\",\n          # Add more documents here to ensure the clustering work correctly\n        ]\n        indexer.index(name=index_name, collection=documents)\n</code></pre><h3 id=\"use-jina-colbert-v1-en-in-ragatouille\">Utilizzare <code>jina-colbert-v1-en</code> in RAGatouille</h3><p>RAGatouille è una nuova libreria Python che facilita l'uso di metodi di recupero avanzati all'interno delle pipeline RAG. È progettata per essere modulare e facilmente integrabile, permettendo agli utenti di sfruttare la ricerca all'avanguardia in modo continuo. L'obiettivo principale di RAGatouille è semplificare l'applicazione di modelli complessi come ColBERT nelle pipeline RAG, rendendoli accessibili agli sviluppatori senza la necessità di una profonda esperienza nella ricerca sottostante. Grazie a <a href=\"https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io\">Benjamin Clavié</a>, ora puoi utilizzare facilmente <code>jina-colbert-v1-en</code>:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\n# Get your model &amp; collection of big documents ready\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v1-en\")\nmy_documents = [\n    \"very long document1\",\n    \"very long document2\",\n    # ... more documents\n]\n\n# And create an index with them at full length!\nRAG.index(collection=my_documents,\n          index_name=\"the_biggest_index\",\n          max_document_length=8190,)\n\n# or encode them in-memory with no truncation, up to your model's max length\nRAG.encode(my_documents)\n</code></pre><p>Per informazioni più dettagliate e per esplorare ulteriormente Jina-ColBERT, puoi visitare la <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">pagina Hugging Face</a>.</p><h2 id=\"conclusion\">Conclusione</h2><p>ColBERT rappresenta un significativo passo avanti nel campo del recupero delle informazioni. Abilitando lunghezze di contesto più lunghe con Jina-ColBERT e mantenendo la compatibilità con l'approccio ColBERT all'interazione tardiva, offre una potente alternativa per gli sviluppatori che cercano di implementare funzionalità di ricerca all'avanguardia.</p><p>Insieme alla libreria RAGatouille, che semplifica l'integrazione di modelli di recupero complessi nelle pipeline RAG, gli sviluppatori possono ora sfruttare la potenza del recupero avanzato con facilità, ottimizzando i loro flussi di lavoro e migliorando le loro applicazioni. La sinergia tra Jina-ColBERT e RAGatouille illustra un notevole passo avanti nel rendere i modelli di ricerca AI avanzati accessibili ed efficienti per l'uso pratico.</p>",
  "comment_id": "65d3a2134a32310001f5b71b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-02-19T19:46:43.000+01:00",
  "updated_at": "2024-08-30T23:11:22.000+02:00",
  "published_at": "2024-02-20T02:19:04.000+01:00",
  "custom_excerpt": "Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
  "excerpt": "ColBERT di Jina AI su Hugging Face sta facendo parlare molto su Twitter, portando una nuova prospettiva alla ricerca con la sua capacità di gestire 8192 token. Questo articolo analizza le sfumature di ColBERT e ColBERTv2, mostrando i loro design innovativi e spiegando perché la loro funzionalità di interazione tardiva rappresenta una svolta rivoluzionaria per la ricerca.",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon theater or concert hall marquee letters lit up at night with city lights and faint \"Adobe Sto\" visible.",
  "feature_image_caption": null
}
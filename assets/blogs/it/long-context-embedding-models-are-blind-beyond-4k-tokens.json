{
  "slug": "long-context-embedding-models-are-blind-beyond-4k-tokens",
  "id": "67c868baf1c5780001164330",
  "uuid": "a9f711ab-651e-4587-8a49-793d15b21380",
  "title": "I modelli di embedding per contesti lunghi sono ciechi oltre i 4K token",
  "html": "<p>A febbraio 2025, un team di ricercatori AI ha pubblicato il <a href=\"https://arxiv.org/abs/2502.05167\">paper NoLiMA</a>, che introduce un nuovo benchmark per valutare la capacit√† dei modelli linguistici di gestire contesti lunghi.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.05167\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">NoLiMa: Long-Context Evaluation Beyond Literal Matching</div><div class=\"kg-bookmark-description\">Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-8.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Ali Modarressi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Questo paper introduce un cambiamento significativo al tradizionale benchmark Needle-in-a-Haystack (NIAH) rimuovendo le corrispondenze letterali tra le domande e l'ago (informazione rilevante) nascosto nel pagliaio (testo irrilevante).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/niah-vs-nolima.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"150\"><figcaption><span style=\"white-space: pre-wrap;\">Per esempio, nel tradizionale NIAH, se la domanda √® \"In che anno John ha visitato Parigi?\", l'ago potrebbe contenere direttamente \"John ha visitato Parigi nel 2019.\" In NOLIMA, la domanda potrebbe essere \"Quale personaggio √® stato in Francia?\" mentre l'ago contiene \"In realt√†, Yuki vive accanto alla Semper Opera House\" - richiedendo al modello di sapere che la Semper Opera House si trova a Dresda, Germania, non in Francia.</span></figcaption></figure><p>Evidenzia una limitazione critica negli LLM attuali: si basano fortemente sul pattern matching superficiale, e la loro capacit√† di eseguire ragionamenti associativi profondi si deteriora rapidamente con l'aumentare della lunghezza del contesto.</p><p>Basandoci su queste intuizioni, miriamo a investigare se pattern di performance simili si verificano nei modelli di embedding, concentrandoci specificamente su <code>jina-embeddings-v3</code>. Poich√© l'efficacia dei sistemi RAG dipende criticamente dalla qualit√† dei modelli di recupero, cerchiamo di estendere la ricerca di NoLiMA attraverso esperimenti controllati che affrontano due questioni fondamentali:</p><ul><li>Come gestiscono i modelli di embedding il recupero needle-in-a-haystack attraverso diverse lunghezze di contesto quando sono costretti a fare salti semantici oltre le corrispondenze letterali di parole chiave?</li><li>L'augmentation strategica delle query con contenuti semanticamente simili pu√≤ mitigare questo gap di performance?</li></ul><p>Il netto contrasto osservato negli LLM‚Äîrobusti con il matching lessicale ma vulnerabili con variazioni semantiche‚Äîsuggerisce che i sistemi di recupero basati su embedding potrebbero affrontare sfide simili quando si va oltre il matching di termini superficiale, potenzialmente rivelando limitazioni fondamentali nelle attuali tecnologie di ricerca semantica.</p><h2 id=\"needles-and-haystacks-construction\">Costruzione di Aghi e Pagliai</h2><h3 id=\"needles-construction\">Costruzione degli Aghi</h3><p>I test tradizionali needle-in-haystack usano aghi che riflettono la formulazione della domanda ricercata. Per esempio:</p><ul><li>Domanda: \"Quale personaggio √® stato a Dresda?\"</li><li>Ago: \"Yuki vive a Dresda.\"</li></ul><p>Ma come NoLiMA, vogliamo testare la comprensione semantica piuttosto che il semplice matching di parole chiave, quindi creiamo variazioni a un salto (usando parole specificamente non presenti nei documenti) con due diversi ordinamenti di parole:</p><ul><li>Domanda: \"Quale personaggio √® stato a Dresda?\"</li><li>Ago (predefinito): \"In realt√†, Yuki vive accanto alla Semper Opera House.\"</li><li>Ago (invertito): \"La Semper Opera House √® accanto a dove vive Yuki.\"</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">La <a href=\"https://en.wikipedia.org/wiki/Semperoper\">Semper Opera House</a> si trova a Dresda, fornendo il contesto per questo ago a un salto.</div></div><p>Seguendo la metodologia del paper, generiamo questi gruppi ago-domanda (costituiti da una domanda, <strong>un ago a un salto</strong> e <strong>un ago a un salto invertito</strong>) attraverso diverse categorie, come gli esempi seguenti:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categoria</th>\n<th>Domanda</th>\n<th>Ago originale (per riferimento)</th>\n<th>Ago a un salto</th>\n<th>Ago a un salto invertito</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Restrizioni dietetiche</td>\n<td>Quale personaggio non pu√≤ mangiare pasti a base di pesce?</td>\n<td>Alice non pu√≤ mangiare pasti a base di pesce.</td>\n<td>Poi, Alice ha menzionato di essere vegana da anni.</td>\n<td>Essere vegana era importante per Alice da anni.</td>\n</tr>\n<tr>\n<td>Condizioni mediche</td>\n<td>Quale personaggio non pu√≤ bere latte?</td>\n<td>Bob non pu√≤ bere latte.</td>\n<td>Bob ha spiegato di essere intollerante al lattosio.</td>\n<td>Essere intollerante al lattosio influenzava Bob quotidianamente.</td>\n</tr>\n<tr>\n<td>Competenza linguistica</td>\n<td>Quale personaggio parla francese?</td>\n<td>Charlie parla francese.</td>\n<td>In effetti, Charlie ha studiato alla Sorbona.</td>\n<td>Alla Sorbona, Charlie ha completato il suo corso di laurea.</td>\n</tr>\n<tr>\n<td>Background professionale</td>\n<td>Quale personaggio √® un musicista?</td>\n<td>Diane √® una musicista.</td>\n<td>Nel 2013, Diane ha diretto alla Sydney Opera House.</td>\n<td>L'esibizione alla Sydney Opera House √® stata diretta da Diane.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">I nomi sopra sono solo per riferimento. Negli aghi effettivi sono estratti casualmente da una lista di nomi culturalmente diversi.<br><br>Notare che gli aghi originali (corrispondenze letterali di parole chiave) sono forniti per riferimento e non vengono utilizzati nei nostri esperimenti.</div></div><h3 id=\"haystacks-construction\">Costruzione dei Pagliai</h3><p>Abbiamo iniziato con dieci libri di pubblico dominio, ciascuno contenente almeno 50.000 token, concatenando casualmente brevi frammenti (sotto i 250 token) da essi in pagliai di varie lunghezze, ovvero 128, 256, 512, 1024, 2048, 4096 e 8192 token. Abbiamo poi incorporato un ago in ciascun pagliaio:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"415\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png 896w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Costruzione del pagliaio da brevi frammenti di libri e un singolo ago per pagliaio.</span></figcaption></figure><p>Per un esempio pi√π concreto, prenderemo l'ago \"In realt√†, Yuki vive accanto alla Semper Opera House\" e lo metteremo in un pagliaio di 128 token alla posizione 50:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"508\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/text2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/text2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Un esempio di ago nel pagliaio.</span></figcaption></figure><p>Usando <code>jina-embeddings-v3</code> per incorporare i testi, il punteggio di similarit√† tra il testo dell'ago e il testo del pagliaio √®:</p><pre><code class=\"language-bash\">Question-Haystack similarity = 0.2391\n</code></pre><p>Quindi normalizziamo il punteggio dividendo questo numero per il punteggio di similarit√† tra la domanda e l'ago predefinito (nessuna creazione di pagliaio, solo confronto diretto):</p><pre><code class=\"language-bash\">Question-Needle similarity = 0.3598\nNormalized Query-Haystack similarity = 0.2391 / 0.3598 = 0.6644\n</code></pre><p>Questa normalizzazione √® necessaria perch√© non tutti i modelli producono gli stessi punteggi di similarit√† tra due testi, e <code>jina-embeddings-v3</code> ha la tendenza a sottocalcolare la similarit√† tra due testi.</p><p>Per ogni ago (inclusi tutti quelli predefiniti e invertiti) abbiamo generato dieci pagliai per lunghezza di contesto, incorporando un ago per pagliaio in una posizione diversa. Per un dato ago e lunghezza di contesto, i pagliai apparirebbero pi√π o meno cos√¨:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"290\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png 800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Aghi posizionati a intervalli regolari in dieci pagliai.</span></figcaption></figure><p>Come controllo, abbiamo anche generato un pagliaio per ogni condizione di test senza alcun ago. In totale, sono 3.234 pagliai. Abbiamo codificato ogni pagliaio con <code>jina-embeddings-v3</code> (usando il LoRA predefinito per il matching del testo), poi per ogni pagliaio lo abbiamo troncato (se i token totali superavano 8.192, il limite per<code>jina-embeddings-v3</code>) ha poi codificato la sua domanda corrispondente.</p><h2 id=\"evaluation-metrics\">Metriche di Valutazione</h2><p>Il nostro framework di valutazione utilizza diverse metriche per valutare le prestazioni dei modelli di embedding su diverse lunghezze di contesto:</p><h3 id=\"primary-metrics\">Metriche Primarie</h3><p><strong>Punteggio di Similarit√† Normalizzato</strong><br>La metrica principale √® un punteggio di similarit√† normalizzato che tiene conto sia della similarit√† semantica tra la domanda e l'intero contesto (similarit√† domanda-pagliaio), sia della similarit√† di base tra la domanda e il suo ago predefinito corrispondente (similarit√† domanda-ago). Questa normalizzazione assicura che le prestazioni del modello vengano valutate in relazione a un punto di riferimento significativo piuttosto che solo a punteggi di similarit√† assoluti. Il processo di normalizzazione prevede il calcolo del punteggio di similarit√† del coseno diretto tra le domande e i loro aghi corrispondenti (il nostro baseline), e la divisione della similarit√† domanda-pagliaio per questo punteggio di base:<br></p><p>$\\text{Similarit√† Normalizzata} = \\frac{\\cos{(q,h)}}{\\cos{(q,n)}}$</p><p><strong>Rapporto Comparativo con la Casualit√†</strong><br>Per qualsiasi modello di embedding, i punteggi di similarit√† del coseno tra diverse coppie query-documento sono direttamente confrontabili solo quando la query rimane la stessa. Pertanto, oltre a utilizzare punteggi di similarit√† normalizzati, misuriamo anche quanto spesso la domanda √® pi√π simile all'intero pagliaio rispetto a un passaggio casuale della stessa lunghezza senza ago.</p><h3 id=\"secondary-metrics\">Metriche Secondarie</h3><p><strong>Analisi della Separazione</strong><br>Questa metrica valuta quanto bene il modello distingue tra contenuto rilevante e irrilevante. Include la <strong>separazione media</strong>, che rappresenta la differenza tra esempi positivi (passaggi contenenti la risposta) ed esempi negativi (passaggi che non contengono la risposta), e il <strong>punteggio AUC (Area Under the Curve)</strong>, che misura la capacit√† di discriminazione basata sull'area sotto la curva ROC (Receiver Operating Characteristic).</p><p><strong>Effetti della Posizione</strong><br>Analizziamo come il posizionamento dell'ago influenza le prestazioni attraverso il <strong>coefficiente di correlazione</strong> tra posizione e punteggio di similarit√†, <strong>pendenza della regressione</strong> che mostra il cambiamento delle prestazioni tra le posizioni, e <strong>analisi delle prestazioni suddivise per posizione</strong>.</p><h2 id=\"findings\">Risultati</h2><h3 id=\"degradation-of-similarity-score-and-correctness\">Degradazione del Punteggio di Similarit√† e Correttezza</h3><p>I nostri risultati mostrano chiaramente che le prestazioni si degradano all'aumentare della lunghezza del contesto, con il punteggio di similarit√† medio che scende da 0,37 a 128 token a 0,10 a 8K token, seguendo un trend non lineare con un forte calo tra 128 e 1K token.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Prestazioni normalizzate vs lunghezza del contesto.</span></figcaption></figure><p>Nella figura seguente, dimostriamo che invertire l'ago ha poca differenza sul punteggio di similarit√† normalizzato. Sia l'ago predefinito (es. \"In realt√†, Yuki vive vicino alla Semper Opera House\") sia l'ago invertito (es. \"La Semper Opera House √® accanto a dove vive Yuki\") mostrano prestazioni quasi identiche:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Prestazioni con ordine predefinito vs invertito.</span></figcaption></figure><p>Le diverse connessioni semantiche del dataset mostrano prestazioni variabili, con le coppie localit√†-punto di riferimento che mantengono i risultati pi√π forti, mentre le connessioni dietetiche e le condizioni mediche si degradano pi√π rapidamente:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Prestazioni normalizzate per gruppo vs lunghezza del contesto.</span></figcaption></figure><p>Confrontando i risultati con la casualit√† si confermano i nostri risultati, mostrando che pi√π grande √® il pagliaio, pi√π i risultati si avvicinano alla casualit√†, cio√® siamo quasi altrettanto propensi a selezionare un passaggio casuale senza ago (risposta corretta) quanto il pagliaio per una data domanda:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-12.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Prestazioni del modello vs casualit√† (0,5).</span></figcaption></figure><p>Ancora una volta, vediamo prestazioni variabili basate su diverse connessioni semantiche, con alcune (come le restrizioni dietetiche) che scendono ben al di sotto della casualit√† anche con contesti relativamente brevi, mentre altre (come luoghi e punti di riferimento) mostrano prestazioni molto migliori indipendentemente dalla lunghezza del contesto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Prestazioni per gruppo vs casualit√†.</span></figcaption></figure><p>Invertire l'ago ha poco effetto sulle prestazioni. Nel grafico seguente, mostriamo il rapporto comparativo di preferenza del pagliaio corretto rispetto alla casualit√†, suddiviso per l'ordine dell'ago inserito (predefinito o invertito):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Ordine predefinito vs invertito - prestazioni vs casualit√†.</span></figcaption></figure><p>Poich√© possiamo vedere che i risultati per gli aghi in ordine predefinito e invertito seguono la stessa tendenza, non continueremo l'analisi divisa rispetto a questo criterio.</p><h3 id=\"can-we-separate-positive-from-negative-results\">Possiamo Separare i Risultati Positivi da Quelli Negativi?</h3><p>Uno dei nostri risultati pi√π importanti deriva dall'analisi di quanto bene i modelli di embedding possano distinguere il contenuto rilevante da quello irrilevante attraverso diverse lunghezze di contesto. Questa \"analisi della separazione\" rivela che la correttezza del recupero cade rapidamente tra la lunghezza di contesto di 128 e 1000 token, e poi continua a diminuire, anche se a un ritmo pi√π lento:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 10: Analisi della separazione vs lunghezza del contesto.</span></figcaption></figure><p>Per contesti brevi (128 token), il modello mostra una forte separazione con una differenza media di 0,1 e una chiara discriminazione, raggiungendo un AUC di 0,81 (che significa che nell'81% dei casi, il modello classifica un passaggio rilevante pi√π in alto di uno irrilevante). Questo indica che in contesti pi√π brevi, il modello pu√≤ distinguere in modo affidabile i passaggi che contengono la risposta da quelli che non la contengono.</p><p>Tuttavia, questo si deteriora rapidamente con l'aumentare della lunghezza del contesto. A 1.000 token, la separazione scende del 60% a 0,040, e l'AUC diminuisce a 0,66, segnalando un notevole calo delle prestazioni. A 8.000 token, c'√® una separazione minima (0,001) e una discriminazione quasi casuale, con un AUC di solo 0,50. Questo schema rivela un'intuizione cruciale: anche quando i modelli possono calcolare punteggi di similarit√† ragionevoli in contesti pi√π lunghi, riescono a malapena a utilizzare questi punteggi per distinguere le informazioni rilevanti da quelle irrilevanti. A 8.000 token, la capacit√† del modello di differenziare i contenuti rilevanti √® essenzialmente casuale.</p><p>La velocit√† di questo deterioramento con la crescita del contesto √® sorprendente. I punteggi di similarit√† grezzi calano di circa il 75% da 128 a 8.000 token, ma le metriche di separazione diminuiscono di quasi il 99% nello stesso intervallo. Ancora pi√π preoccupante, l'effect size mostra un declino ancora pi√π ripido, cadendo del 98,6%. Questo suggerisce che le difficolt√† dei modelli di embedding con contesti lunghi vanno oltre la semplice riduzione dei punteggi di similarit√†‚Äîla loro capacit√† fondamentale di identificare informazioni rilevanti si deteriora molto pi√π gravemente di quanto precedentemente compreso.</p><h3 id=\"how-does-the-needle-position-affect-the-core-metrics\">Come la Posizione dell'Ago Influenza le Metriche Principali?</h3><p>Mentre le metriche di performance principali sono solitamente migliori quando l'ago √® all'inizio del pagliaio, il deterioramento delle prestazioni non sempre correla con il posizionamento nel mezzo del contesto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 11: Performance per posizione relativa attraverso le lunghezze di contesto.</span></figcaption></figure><p>Vediamo anche che le prestazioni sono migliori quando l'ago √® all'inizio di un dato contesto, e nei contesti brevi notiamo un piccolo miglioramento delle prestazioni quando l'ago √® posizionato verso la fine. Tuttavia, in tutti i contesti vediamo un calo delle prestazioni quando l'ago √® nelle posizioni centrali:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 12: Rapporti comparativi per posizione.</span></figcaption></figure><h2 id=\"what-effect-does-query-expansion-have-on-the-results\">Quale Effetto Ha l'Espansione delle Query sui Risultati?</h2><p>Abbiamo recentemente pubblicato un post sul blog riguardo l'espansione delle query, una tecnica utilizzata nei sistemi di ricerca per migliorare le prestazioni di ricerca aggiungendo termini rilevanti alle query.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Query Expansion with LLMs: Searching Better by Saying More</div><div class=\"kg-bookmark-description\">Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/query-expansion-with-llms-searching-better-by-saying-more.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Nel post, abbiamo utilizzato un LLM per generare termini di espansione, che sono stati poi aggiunti agli embedding delle query per migliorare le prestazioni di recupero. I risultati hanno mostrato miglioramenti significativi. Ora, vogliamo esaminare come (o se) la tecnica migliorer√† i risultati per la ricerca dell'ago nel pagliaio. Per esempio, data una query:</p><pre><code class=\"language-bash\">Which character has been to Dresden?\n</code></pre><p>Utilizziamo un LLM (Gemini 2.0) per espanderla e aggiungere 100 termini aggiuntivi che appaiono cos√¨:</p><pre><code class=\"language-bash\">Which character has been to Dresden? Character: fictional character literary character protagonist antagonist figure persona role dramatis personae\\\\n\\\\nDresden: Dresden Germany; bombing of Dresden World War II historical fiction Kurt Vonnegut Slaughterhouse-Five city in Saxony Elbe River cultural landmark\\\\n\\\\nHas been to: visited traveled to journeyed to presence in appears in features in set in takes place in location setting\n\n</code></pre><h3 id=\"how-much-does-query-expansion-help-match-the-needle-to-the-haystack\">Quanto l'Espansione delle Query Aiuta a Trovare l'Ago nel Pagliaio?</h3><p>Per il nostro esperimento, abbiamo generato tre set di termini di query espansi (come descritto nel <a href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\">post originale</a>) - 100, 150 e 250 termini. Abbiamo poi eseguito lo stesso set di esperimenti di prima, ripetuti tre volte, una volta per ciascun set di termini di query espansi.</p><p>I risultati con tutti i set di espansione hanno mostrato un chiaro deterioramento all'aumentare della lunghezza del contesto, con un effetto simile al non utilizzare l'espansione delle query (Figure 4 e 7):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1071\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 13: Performance normalizzata combinata: tutte le dimensioni di espansione.</span></figcaption></figure><p>Rispetto alle query non espanse, tutte le condizioni di espansione delle query hanno mostrato lo stesso pattern di degradazione delle prestazioni con l'aumentare del contesto. La tendenza al deterioramento √® anche ancora non lineare con un forte declino tra 128 e 1K token:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 14: Rapporto comparativo combinato: tutte le dimensioni di espansione.</span></figcaption></figure><p>Tuttavia, esaminando il rapporto comparativo si vede che l'espansione delle query ha chiari benefici: Il modello √® molto pi√π propenso a selezionare il pagliaio con l'ago rispetto a quello senza. Al contrario, senza espansione delle query la probabilit√† di selezionare il passaggio corretto √® diminuita cos√¨ tanto che, con un pagliaio di 8K token, era quasi la stessa che scegliere un passaggio casuale.</p><h3 id=\"how-do-we-explain-needle-matching-results-with-query-expansion\">Come Spieghiamo i Risultati del Matching dell'Ago con l'Espansione delle Query?</h3><p>Questi risultati si allineano con le scoperte sia del paper NoLiMa che della ricerca sull'espansione delle query, e possono essere spiegati come segue:</p><ol><li><strong>Compromesso tra qualit√† e quantit√†</strong>: Le migliori prestazioni dell'espansione a 100 termini, rispetto a 150 e 250 termini, suggerisce che c'√® un punto ottimale dove termini aggiuntivi iniziano ad aggiungere pi√π rumore che segnale. L'espansione a 250 termini probabilmente introduce termini con relazioni semantiche pi√π deboli rispetto alla query originale, che diventano controproducenti in contesti pi√π lunghi.</li><li><strong>La lunghezza del contesto rimane la sfida principale</strong>: Nonostante i benefici dell'espansione delle query, le prestazioni si degradano comunque significativamente con l'aumentare della lunghezza del contesto. Questo suggerisce che anche con l'espansione, persiste la limitazione architettonica fondamentale dei modelli basati sull'attention in contesti lunghi.</li><li><strong>Identificazione della soglia pratica</strong>: Il rapporto comparativo che rimane sopra 0,5 indica che l'espansione mantiene prestazioni superiori alla casualit√† anche a 8K token, fornendo un modo pratico per estendere la <em>finestra di contesto effettiva</em> per i modelli di embedding. Il confronto con la casualit√† mostra che, anche quando presentato con documenti di contesto lungo, espandere la query rende pi√π probabile trovare la risposta corretta (cio√® l'ago) rispetto a una incorretta. Questo √® un miglioramento rispetto alle query non espanse, dove la probabilit√† di trovare la risposta corretta si avvicina alla casualit√† all'aumentare della lunghezza del contesto.</li></ol><h2 id=\"diagnosis-what-role-does-lexical-matching-play-in-embeddings\">Diagnosi: Quale Ruolo Gioca il Matching Lessicale negli Embedding?</h2><p>Negli esperimenti precedenti, abbiamo misurato l'efficacia dei modelli di embedding nel fare inferenze semantiche \"one-hop\" in passaggi con contesto lungo, rimuovendo ogni possibilit√† di matching letterale. Abbiamo scoperto che, anche con l'espansione delle query, la capacit√† del modello di embedding di trovare passaggi rilevanti si deteriora con l'aumentare della lunghezza del contesto. Questo effetto √® significativo, e la scoperta √® degna di nota perch√© normalmente ci aspetteremmo che un modello di embedding sia in grado di fare le inferenze rilevanti senza assistenza aggiuntiva. Quando sostituiamo i match letterali con variazioni one-hop (es. \"Dresden\" ‚Üí \"Semper Opera House\"), stiamo solo sostituendo un concetto con un altro vicino.</p><p>Affrontiamo ora direttamente la questione: Il matching letterale gioca davvero un ruolo sufficientemente significativo nel matching semantico, o l'effetto della lunghezza del contesto lo sovrasta? Per rispondere a questa domanda, abbiamo rifatto i nostri test con aghi contenenti match letterali, es.</p><ul><li>Domanda: \"Which character has been to Dresden?\"</li><li>Ago (default): \"Actually, Yuki lives in Dresden.\"</li><li>Ago (invertito): \"Dresden is where Yuki lives.\"</li></ul><p>Da notare che, invece di una variazione a singolo salto per dedurre che la Semperoper √® a Dresda e quindi un personaggio che vive accanto dovrebbe essere quello che ha visitato Dresda, questi indizi indicano direttamente il nome del personaggio che vive a Dresda.</p><p>Dopo aver riformulato tutte le 22 coppie domanda-indizio in questo modo, abbiamo ripetuto i nostri esperimenti con tutte le lunghezze di contesto incluse e i posizionamenti degli indizi, utilizzando lo stesso modello di embedding <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-22.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 15: Performance normalizzata vs lunghezza del contesto.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-23.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 16: Performance del modello vs probabilit√† casuale (0.5).</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-24.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-24.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-24.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 17: Rapporti comparativi per posizione</span></figcaption></figure><p>I risultati sono sorprendenti. Anche con corrispondenze letterali nel contesto, la capacit√† del modello di distinguere la risposta corretta da una casuale si deteriora rapidamente all'aumentare della lunghezza del contesto, pur mantenendo un leggero vantaggio rispetto alla completa assenza di corrispondenze letterali.</p><p>Questo dimostra definitivamente che la capacit√† di un modello di embedding di trovare un ago in un pagliaio √® influenzata molto pi√π dalla dimensione del pagliaio (e dal posizionamento dell'ago in esso) che dalla formulazione semantica dell'ago.</p><h2 id=\"conclusion\">Conclusione</h2><p>I nostri risultati con i modelli di embedding si allineano con lo studio NoLiMA sugli LLM: la dimensione del contesto √® altamente determinante per il corretto matching e recupero. Dimostriamo che questo √® vero anche quando c'√® una corrispondenza esatta lettera per lettera.</p><p>Il problema non √® la capacit√† di un embedding di eseguire il matching semantico. Modelli di embedding come <code>jina-embeddings-v3</code> gestiscono bene contesti brevi, ma la loro efficacia diminuisce all'aumentare della lunghezza del contesto. L'espansione delle query pu√≤ ridurre questo effetto in una certa misura, ma la qualit√† del recupero si degrada comunque su contesti pi√π lunghi. Inoltre, l'espansione delle query pone ulteriori problemi, poich√© √® crucialmente importante identificare termini di espansione che migliorino il recupero senza aggiungere rumore semantico. Stiamo investigando e cercando modi per affrontare direttamente il recupero \"ago nel pagliaio\" e migliorare le prestazioni del futuro <code>jina-embeddings-v4</code>.</p>",
  "comment_id": "67c868baf1c5780001164330",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-05T16:07:38.000+01:00",
  "updated_at": "2025-03-07T03:56:34.000+01:00",
  "published_at": "2025-03-07T03:56:34.000+01:00",
  "custom_excerpt": "We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/long-context-embedding-models-are-blind-beyond-4k-tokens/",
  "excerpt": "Abbiamo analizzato i modelli di embedding su nuovi task di tipo \"ago nel pagliaio\" e abbiamo scoperto che oltre i 4K token, si comportano come se lanciassero i dadi - anche con corrispondenze lessicali esatte o espansione delle query, non riescono a distinguere il segnale dal rumore in contesti lunghi.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
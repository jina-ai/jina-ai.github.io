{
  "slug": "scaling-test-time-compute-for-embedding-models",
  "id": "675a84f80ce9930001b86f09",
  "uuid": "49f876f3-0d50-4555-8f9e-136473f720ac",
  "title": "Scalare il Calcolo di Test-Time per i Modelli di Embedding",
  "html": "<p>Dal rilascio del <a href=\"https://openai.com/o1/?ref=jina-ai-gmbh.ghost.io\">modello O1</a> di OpenAI, uno dei temi pi√π discussi nella comunit√† AI √® stato lo <strong>scaling test-time compute</strong>. Questo si riferisce all'allocazione di risorse computazionali aggiuntive durante l'inferenza‚Äîla fase in cui un modello AI genera output in risposta agli input‚Äîpiuttosto che durante il pre-addestramento. Un esempio ben noto √® il ragionamento multi-step \"chain of thought\", che permette ai modelli di eseguire deliberazioni interne pi√π estese, come valutare multiple risposte potenziali, pianificazione pi√π approfondita, auto-riflessione prima di arrivare a una risposta finale. Questa strategia migliora la qualit√† delle risposte, in particolare nei compiti di ragionamento complesso. Il modello <a href=\"https://huggingface.co/Qwen/QwQ-32B-Preview?ref=jina-ai-gmbh.ghost.io\">QwQ-32B-Preview</a> recentemente rilasciato da Alibaba segue questa tendenza di migliorare il ragionamento AI attraverso l'aumento del test-time compute.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">In questo contesto, \"scaling\" significa principalmente aumentare la capacit√† computazionale (come potenza di elaborazione o tempo) disponibile durante l'inferenza. Non si riferisce allo <b><strong style=\"white-space: pre-wrap;\">scaling out</strong></b> (distribuire i compiti su pi√π sistemi) o al raggiungimento di uno <b><strong style=\"white-space: pre-wrap;\">speedup</strong></b> (riduzione del tempo di elaborazione).</div></div><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1.mp4\" poster=\"https://img.spacergif.org/v1/900x432/0a/spacer.png\" width=\"900\" height=\"432\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:10</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Quando si utilizza il modello O1 di OpenAI, gli utenti possono notare chiaramente che l'inferenza multi-step richiede tempo aggiuntivo mentre il modello costruisce catene di ragionamento per risolvere i problemi. </span></p></figcaption>\n        </figure><p>A Jina AI, ci concentriamo pi√π sugli embedding e i reranker che sui LLM, quindi per noi √® naturale considerare lo scaling test-time compute in questo contesto: <em>Come pu√≤ essere applicato il \"chain-of-thought\" ai modelli di embedding? </em>Anche se potrebbe non sembrare intuitivo all'inizio, questo articolo esplora una prospettiva innovativa e dimostra come lo scaling test-time compute pu√≤ essere applicato a <code>jina-clip</code> per classificare immagini fuori distribuzione (OOD)‚Äîrisolvendo compiti che altrimenti sarebbero impossibili. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Il nostro esperimento si √® concentrato sul riconoscimento dei Pokemon, che presenta una sfida interessante per i modelli di embedding. Mentre i modelli tipo CLIP eccellono nel matching generale tra immagini e testo, potrebbero avere difficolt√† con domini di nicchia o immagini OOD senza fine-tuning. Dando ai modelli pi√π tempo per \"pensare\", abbiamo scoperto che la classificazione multi-target‚Äîanaloga a una \"chain of thought\"‚Äîpotrebbe migliorare l'accuratezza senza alcun tuning del modello di embedding stesso.</span></figcaption></figure><h2 id=\"case-study\">Caso Studio</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z?ref=jina-ai-gmbh.ghost.io#scrollTo=CJt5zwA9E2jB\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-15.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Il nostro esperimento si √® concentrato sulla classificazione dei Pokemon utilizzando il <a href=\"https://huggingface.co/datasets/TheFusion21/PokemonCards?ref=jina-ai-gmbh.ghost.io\">dataset TheFusion21/PokemonCards</a>, che contiene migliaia di immagini di carte Pokemon. <strong>Il compito √® la classificazione delle immagini</strong> dove l'input √® l'artwork ritagliato di una carta Pokemon (con tutti i testi/descrizioni rimossi) e l'output √® il nome corretto del Pokemon da un insieme predefinito di nomi. Questo compito presenta una sfida particolarmente interessante per i modelli di embedding CLIP perch√©:</p><ul><li>I nomi e gli elementi visivi dei Pokemon rappresentano concetti di nicchia, fuori distribuzione per il modello, rendendo difficile la classificazione diretta</li><li>Ogni Pokemon ha caratteristiche visive chiare che possono essere decomposte in elementi base (forme, colori, pose) che CLIP potrebbe comprendere meglio</li><li>L'artwork delle carte fornisce un formato visivo consistente introducendo allo stesso tempo complessit√† attraverso sfondi, pose e stili artistici variabili</li><li>Il compito richiede l'integrazione simultanea di multiple caratteristiche visive, simile alle catene di ragionamento complesse nei modelli linguistici</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"835\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Ritagliamo le immagini delle carte Pokemon per rimuovere tutte le informazioni testuali (intestazione, pi√® di pagina, descrizione) per evitare ipotesi banali dovute alla presenza dei nomi dei Pokemon in quei testi. Le etichette di classe di questi Pokemon sono [</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol G</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Weedle</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Azumarill</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Bulbasaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Venusaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aggron</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Beedrill Œ¥</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Alakazam</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Dratini</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Arcanine</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Blaine's Moltres</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Celebi & Venusaur-GX</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">]</span></figcaption></figure><h3 id=\"baseline\">Baseline</h3><p>L'approccio baseline utilizza un semplice confronto diretto tra le illustrazioni delle carte Pokemon e i nomi. Prima, ritagliamo ogni immagine della carta Pokemon per rimuovere tutte le informazioni testuali (intestazione, pi√® di pagina, descrizione) per evitare che il modello CLIP faccia ipotesi banali basate sui nomi Pokemon che appaiono in quei testi. Poi codifichiamo sia le immagini ritagliate che i nomi Pokemon usando i modelli <code>jina-clip-v1</code> e <code>jina-clip-v2</code> per ottenere i rispettivi embedding. La classificazione viene effettuata calcolando la similarit√† del coseno tra questi embedding di immagini e testo - ogni immagine viene abbinata al nome che ha il punteggio di similarit√† pi√π alto. Questo crea una corrispondenza uno a uno diretta tra l'illustrazione della carta e i nomi dei Pok√©mon, senza alcun contesto o informazione aggiuntiva sugli attributi. Lo pseudo-codice qui sotto riassume il metodo baseline.</p><pre><code class=\"language-python\"># Preprocessing\ncropped_images = [crop_artwork(img) for img in pokemon_cards]  # Remove text, keep only art\npokemon_names = [\"Absol\", \"Aerodactyl\", ...]  # Raw Pokemon names\n\n# Get embeddings using jina-clip-v1\nimage_embeddings = model.encode_image(cropped_images)\ntext_embeddings = model.encode_text(pokemon_names)\n\n# Classification by cosine similarity\nsimilarities = cosine_similarity(image_embeddings, text_embeddings)\npredicted_names = [pokemon_names[argmax(sim)] for sim in similarities]\n\n# Evaluate\naccuracy = mean(predicted_names == ground_truth_names)</code></pre><h3 id=\"chain-of-thoughts-for-classification\">\"Chain of Thoughts\" per la Classificazione</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--10-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Invece di abbinare direttamente immagini a nomi, decomponiamo il riconoscimento dei Pokemon in un sistema strutturato di attributi visivi. Definiamo cinque gruppi chiave di attributi: colore dominante (es. \"bianco\", \"blu\"), forma primaria (es. \"un lupo\", \"un rettile alato\"), caratteristica principale (es. \"un singolo corno bianco\", \"grandi ali\"), forma del corpo (es. \"simile a un lupo su quattro zampe\", \"alato e snello\"), e scena di sfondo (es. \"spazio esterno\", \"foresta verde\").</p><p>Per ogni gruppo di attributi, creiamo prompt di testo specifici (es. \"Questo Pok√©mon ha il corpo principalmente di colore {}\") abbinati a opzioni pertinenti. Quindi usiamo il modello per calcolare i punteggi di similarit√† tra l'immagine e ciascuna opzione di attributo. Questi punteggi vengono convertiti in probabilit√† usando softmax per ottenere una misura pi√π calibrata della confidenza.</p><p>La struttura completa Chain of Thought (CoT) consiste in due parti: <code>classification_groups</code> che descrive gruppi di prompt, e <code>pokemon_rules</code> che definisce quali opzioni di attributi dovrebbe corrispondere ogni Pokemon. Per esempio, Absol dovrebbe corrispondere a \"bianco\" per il colore e \"simile a un lupo\" per la forma. Il CoT completo √® mostrato qui sotto (spiegheremo pi√π avanti come viene costruito):</p><pre><code class=\"language-python\">pokemon_system = {\n    \"classification_cot\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pok√©mon's body is mainly {} in color.\",\n            \"options\": [\n                \"white\",    # Absol, Absol G\n                \"gray\",     # Aggron\n                \"brown\",    # Aerodactyl, Weedle, Beedrill Œ¥\n                \"blue\",     # Azumarill\n                \"green\",    # Bulbasaur, Venusaur, Celebi&amp;Venu, Caterpie\n                \"yellow\",   # Alakazam, Ampharos\n                \"red\",      # Blaine's Moltres\n                \"orange\",   # Arcanine\n                \"light blue\"# Dratini\n            ]\n        },\n        \"primary_form\": {\n            \"prompt\": \"It looks like {}.\",\n            \"options\": [\n                \"a wolf\",         # Absol, Absol G\n                \"an armored dinosaur\",  # Aggron\n                \"a winged reptile\",     # Aerodactyl\n                \"a rabbit-like creature\", # Azumarill\n                \"a toad-like creature\",   # Bulbasaur, Venusaur, Celebi&amp;Venu\n                \"a caterpillar larva\",    # Weedle, Caterpie\n                \"a wasp-like insect\",     # Beedrill Œ¥\n                \"a fox-like humanoid\",     # Alakazam\n                \"a sheep-like biped\",      # Ampharos\n                \"a dog-like beast\",        # Arcanine\n                \"a flaming bird\",          # Blaine's Moltres\n                \"a serpentine dragon\"      # Dratini\n            ]\n        },\n        \"key_trait\": {\n            \"prompt\": \"Its most notable feature is {}.\",\n            \"options\": [\n                \"a single white horn\", # Absol, Absol G\n                \"metal armor plates\",  # Aggron\n                \"large wings\",         # Aerodactyl, Beedrill Œ¥\n                \"rabbit ears\",         # Azumarill\n                \"a green plant bulb\",  # Bulbasaur, Venusaur, Celebi&amp;Venu\n                \"a small red spike\",   # Weedle\n                \"big green eyes\",      # Caterpie\n                \"a mustache and spoons\", # Alakazam\n                \"a glowing tail orb\",  # Ampharos\n                \"a fiery mane\",        # Arcanine\n                \"flaming wings\",       # Blaine's Moltres\n                \"a tiny white horn on head\" # Dratini\n            ]\n        },\n        \"body_shape\": {\n            \"prompt\": \"The body shape can be described as {}.\",\n            \"options\": [\n                \"wolf-like on four legs\",   # Absol, Absol G\n                \"bulky and armored\",        # Aggron\n                \"winged and slender\",       # Aerodactyl, Beedrill Œ¥\n                \"round and plump\",          # Azumarill\n                \"sturdy and four-legged\",   # Bulbasaur, Venusaur, Celebi&amp;Venu\n                \"long and worm-like\",       # Weedle, Caterpie\n                \"upright and humanoid\",     # Alakazam, Ampharos\n                \"furry and canine\",         # Arcanine\n                \"bird-like with flames\",    # Blaine's Moltres\n                \"serpentine\"                # Dratini\n            ]\n        },\n        \"background_scene\": {\n            \"prompt\": \"The background looks like {}.\",\n            \"options\": [\n                \"outer space\",      # Absol G, Beedrill Œ¥\n                \"green forest\",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&amp;Venu\n                \"a rocky battlefield\", # Absol, Aggron, Aerodactyl\n                \"a purple psychic room\", # Alakazam\n                \"a sunny field\",     # Ampharos\n                \"volcanic ground\",   # Arcanine\n                \"a red sky with embers\", # Blaine's Moltres\n                \"a calm blue lake\"   # Dratini\n            ]\n        }\n    },\n    \n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,      \n            \"body_shape\": 0,    \n            \"background_scene\": 2   \n        },\n        \"Absol G\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,       \n            \"body_shape\": 0,     \n            \"background_scene\": 0    \n        },\n        // ...\n    }\n}\n</code></pre><p>La classificazione finale combina queste probabilit√† di attributi - invece di un singolo confronto di similarit√†, ora stiamo facendo molteplici confronti strutturati e aggregando le loro probabilit√† per prendere una decisione pi√π informata.</p><pre><code class=\"language-python\"># Classification process\ndef classify_pokemon(image):\n   # Generate all text prompts\n   all_prompts = []\n   for group in classification_cot:\n       for option in group[\"options\"]:\n           prompt = group[\"prompt\"].format(option)\n           all_prompts.append(prompt)\n\n   # Get embeddings and similarities\n   image_embedding = model.encode_image(image)\n   text_embeddings = model.encode_text(all_prompts)\n   similarities = cosine_similarity(image_embedding, text_embeddings)\n\n   # Convert to probabilities per attribute group\n   probabilities = {}\n   for group_name, group_sims in group_similarities:\n       probabilities[group_name] = softmax(group_sims)\n\n   # Score each Pokemon based on matching attributes\n   scores = {}\n   for pokemon, rules in pokemon_rules.items():\n       score = 0\n       for group, target_idx in rules.items():\n           score += probabilities[group][target_idx]\n       scores[pokemon] = score\n\n   return max(scores, key=scores.get)</code></pre><h3 id=\"complexity-analysis\">Analisi della Complessit√†</h3><p>Supponiamo di voler classificare un'immagine in uno di <code>N</code> nomi di Pok√©mon. L'approccio baseline richiede di calcolare <code>N</code> embedding di testo (uno per ogni nome di Pok√©mon). Al contrario, il nostro approccio di calcolo scalato in fase di test richiede di calcolare <code>Q</code> embedding di testo, dove</p><code>Q</code> √® il numero totale di combinazioni domanda-opzione per tutte le domande. Entrambi i metodi richiedono il calcolo di un embedding dell'immagine e l'esecuzione di un passaggio finale di classificazione, quindi escludiamo queste operazioni comuni dal nostro confronto. In questo caso di studio, il nostro <code>N=13</code> e <code>Q=52</code>.</p><p>In un caso estremo dove <code>Q = N</code>, il nostro approccio si ridurrebbe essenzialmente al baseline. Tuttavia, la chiave per scalare efficacemente il calcolo in fase di test √®: </p><ul><li>Costruire domande attentamente scelte che aumentino <code>Q</code> </li><li>Assicurarsi che ogni domanda fornisca indizi distinti e informativi sulla risposta finale </li><li>Progettare domande il pi√π possibile ortogonali per massimizzare il loro guadagno informativo congiunto.</li></ul><p>Questo approccio √® analogo al gioco \"Venti Domande\", dove ogni domanda viene scelta strategicamente per restringere efficacemente le possibili risposte.</p><h3 id=\"evaluation\">Valutazione</h3><p>La nostra valutazione √® stata condotta su 117 immagini di test che coprono 13 diverse classi di Pok√©mon. E il risultato √® il seguente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>jina-clip-v1</th>\n<th>jina-clip-v2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>31.36%</td>\n<td>16.10%</td>\n</tr>\n<tr>\n<td>CoT</td>\n<td>46.61%</td>\n<td>38.14%</td>\n</tr>\n<tr>\n<td>Improvement</td>\n<td>+15.25%</td>\n<td>+22.04%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Si pu√≤ notare che la stessa classificazione CoT offre miglioramenti significativi per entrambi i modelli (+15,25% e +22,04% rispettivamente) su questo compito non comune o OOD. Questo suggerisce anche che una volta costruito il <code>pokemon_system</code>, <strong>lo stesso sistema CoT pu√≤ essere efficacemente trasferito tra diversi modelli; e non √® richiesto alcun fine-tuning o post-training.</strong></p><p>√à degna di nota la performance baseline relativamente forte di v1 (31,36%) sulla classificazione dei Pokemon. Questo modello √® stato addestrato su <a href=\"https://www.youtube.com/watch?v=HsGyxVUN1SA&ref=jina-ai-gmbh.ghost.io\">LAION-400M, che includeva contenuti relativi ai Pokemon</a>. Al contrario, v2 √® stato addestrato su DFN-2B (sottocampionando 400M istanze), un dataset di qualit√† superiore ma pi√π filtrato che potrebbe aver escluso contenuti relativi ai Pokemon, spiegando la performance baseline inferiore di V2 (16,10%) su questo compito specifico.</p><h3 id=\"constructing-pokemonsystem-effectively\">Costruire <code>pokemon_system</code> Efficacemente</h3><p>L'efficacia del nostro approccio di calcolo scalato in fase di test dipende fortemente da quanto bene costruiamo il <code>pokemon_system</code>. Ci sono diversi approcci per costruire questo sistema, dal manuale al completamente automatizzato.</p><h4 id=\"manual-construction\">Costruzione Manuale</h4><p>L'approccio pi√π diretto √® analizzare manualmente il dataset Pokemon e creare gruppi di attributi, prompt e regole. Un esperto del dominio dovrebbe identificare gli attributi visivi chiave come colore, forma e caratteristiche distintive. Dovrebbe poi scrivere prompt in linguaggio naturale per ogni attributo, enumerare le possibili opzioni per ogni gruppo di attributi e mappare ogni Pokemon alle sue corrette opzioni di attributi. Mentre questo fornisce regole di alta qualit√†, richiede molto tempo e non scala bene con <code>N</code> pi√π grandi.</p><h4 id=\"llm-assisted-construction\">Costruzione Assistita da LLM</h4><p>Possiamo sfruttare gli LLM per accelerare questo processo sollecitandoli a generare il sistema di classificazione. Un prompt ben strutturato richiederebbe gruppi di attributi basati su caratteristiche visive, template di prompt in linguaggio naturale, opzioni complete e mutualmente esclusive e regole di mappatura per ogni Pokemon. L'LLM pu√≤ generare rapidamente una prima bozza, anche se il suo output potrebbe necessitare di verifica.</p><pre><code class=\"language-txt\">I need help creating a structured system for Pokemon classification. For each Pokemon in this list: [Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...], create a classification system with:\n\n1. Classification groups that cover these visual attributes:\n   - Dominant color of the Pokemon\n   - What type of creature it appears to be (primary form)\n   - Its most distinctive visual feature\n   - Overall body shape\n   - What kind of background/environment it's typically shown in\n\n2. For each group:\n   - Create a natural language prompt template using \"{}\" for the option\n   - List all possible options that could apply to these Pokemon\n   - Make sure options are mutually exclusive and comprehensive\n\n3. Create rules that map each Pokemon to exactly one option per attribute group, using indices to reference the options\n\nPlease output this as a Python dictionary with two main components:\n- \"classification_groups\": containing prompts and options for each attribute\n- \"pokemon_rules\": mapping each Pokemon to its correct attribute indices\n\nExample format:\n{\n    \"classification_groups\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokemon's body is mainly {} in color\",\n            \"options\": [\"white\", \"gray\", ...]\n        },\n        ...\n    },\n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,  # index for \"white\"\n            ...\n        },\n        ...\n    }\n}</code></pre><p>Un approccio pi√π robusto combina la generazione LLM con la validazione umana. Prima, l'LLM genera un sistema iniziale. Poi, gli esperti umani rivedono e correggono i raggruppamenti degli attributi, la completezza delle opzioni e l'accuratezza delle regole. L'LLM perfeziona il sistema in base a questo feedback, e il processo si ripete fino al raggiungimento di una qualit√† soddisfacente. Questo approccio bilancia efficienza e accuratezza.</p><h4 id=\"automated-construction-with-dspy\">Costruzione Automatizzata con DSPy</h4><p>Per un approccio completamente automatizzato, possiamo usare DSPy per ottimizzare iterativamente il <code>pokemon_system</code>. Il processo inizia con un semplice <code>pokemon_system</code> scritto manualmente o da LLM come prompt iniziale. Ogni versione viene valutata su un set di validazione, usando l'accuratezza come segnale di feedback per DSPy. Basandosi su questa performance, vengono generati prompt ottimizzati (cio√® nuove versioni di <code>pokemon_system</code>). Questo ciclo si ripete fino alla convergenza, e durante l'intero processo, il modello di embedding rimane completamente fisso.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Uso di DSPy per trovare il miglior design CoT di </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>pokemon_system</span></code><span style=\"white-space: pre-wrap;\">; il processo di tuning deve essere fatto solo una volta per ogni task.</span></figcaption></figure><h2 id=\"why-scale-test-time-compute-for-embedding-models\">Perch√© Scalare il Calcolo in Fase di Test per i Modelli di Embedding?</h2><p>Perch√© scalare il pre-training diventa alla fine economicamente intrattabile. </p><p>Dalla release della suite di embedding Jina‚Äîinclusi <code>jina-embeddings-v1</code>, <code>v2</code>, <code>v3</code>, <code>jina-clip-v1</code>, <code>v2</code>, e <code>jina-ColBERT-v1</code>, <code>v2</code>‚Äîogni aggiornamento del modello attraverso il pre-training scalato √® arrivato con costi maggiori. Per esempio, il nostro primo modello, <code>jina-embeddings-v1</code>, rilasciato a giugno 2023 con 110M parametri. Addestrarlo all'epoca costava tra $5.000 e $10.000 a seconda di come si misura. Con <code>jina-embeddings-v3</code>, i miglioramenti sono significativi, ma derivano principalmente dalle maggiori risorse investite. La traiettoria dei costi per i modelli di frontiera √® passata da migliaia a decine di migliaia di dollari e, per le aziende AI pi√π grandi, anche centinaia di milioni oggi. Mentre investire pi√π soldi, risorse e dati nel pre-training produce modelli migliori, i rendimenti marginali rendono alla fine insostenibile economicamente un'ulteriore scalabilit√†.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/plot--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/plot--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/plot--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/plot--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/plot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Legge di scala dei modelli di embedding. La performance media MTEB sui task in inglese √® tracciata contro il numero di parametri del modello. Ogni punto rappresenta un modello di embedding. La linea di tendenza, che rappresenta tutti i modelli, √® evidenziata, con i modelli multilingue in punti ciano. Questo grafico √® stato creato selezionando i primi 100 modelli di embedding dalla classifica MTEB, escludendo quelli senza informazioni sulla dimensione, tipicamente modelli closed-source o proprietari. Sono state anche filtrate le sottomissioni identificate come trolling evidente.</span></figcaption></figure><p>D'altra parte, i moderni modelli di embedding stanno diventando sempre pi√π potenti: multilingue, multitask, multimodali e capaci di forte performance zero-shot e di seguire istruzioni. Questa versatilit√† lascia ampio spazio per miglioramenti algoritmici e scaling del calcolo in fase di test.</p><p>La domanda diventa quindi: qual √® il costo che gli utenti sono disposti a pagare per una query che gli sta particolarmente a cuore? Se tollerare tempi di inferenza pi√π lunghi per modelli pre-addestrati fissi migliora significativamente la qualit√† dei risultati, molti lo troverebbero utile. Dal nostro punto di vista, c'√® un notevole potenziale non sfruttato nello scaling del calcolo in fase di test per i modelli di embedding. Questo rappresenta un cambiamento dal semplice aumento della dimensione del modello durante l'addestramento all'aumento dello sforzo computazionale durante la fase di inferenza per ottenere prestazioni migliori.</p><h2 id=\"conclusion\">Conclusione</h2><p>Il nostro caso di studio sul calcolo in fase di test di <code>jina-clip-v1/v2</code> mostra diversi risultati chiave:</p><ol><li>Abbiamo ottenuto prestazioni migliori su dati non comuni o out-of-distribution (OOD) senza alcun fine-tuning o post-training sugli embedding.</li><li>Il sistema ha fatto distinzioni pi√π sfumate raffinando iterativamente le ricerche di similarit√† e i criteri di classificazione.</li><li>Incorporando aggiustamenti dinamici dei prompt e ragionamento iterativo, abbiamo trasformato il processo di inferenza del modello di embedding da una singola query in una catena di pensiero pi√π sofisticata.</li></ol><p>Questo caso di studio tocca appena la superficie di ci√≤ che √® possibile con il calcolo in fase di test. Rimane ampio spazio per lo scaling algoritmico. Per esempio, potremmo sviluppare metodi per selezionare iterativamente le domande che restringono pi√π efficientemente lo spazio delle risposte, similmente alla strategia ottimale nel gioco \"Venti Domande\". Scalando il calcolo in fase di test, possiamo spingere i modelli di embedding oltre i loro attuali limiti e permettere loro di affrontare compiti pi√π complessi e sfumati che una volta sembravano fuori portata.</p>",
  "comment_id": "675a84f80ce9930001b86f09",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-12T07:38:48.000+01:00",
  "updated_at": "2024-12-12T17:54:17.000+01:00",
  "published_at": "2024-12-12T17:54:17.000+01:00",
  "custom_excerpt": "Better results scale with compute‚Äîmore on learning, more on search. A good pretrained model takes you far, but test-time compute takes you further. It's important to recognize this new paradigm of scaling test-time compute, even for embedding models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/scaling-test-time-compute-for-embedding-models/",
  "excerpt": "I risultati migliori si ottengono con pi√π potenza di calcolo‚Äîpi√π per l'apprendimento, pi√π per la ricerca. Un buon modello pre-addestrato ti porta lontano, ma la potenza di calcolo in fase di test ti porta ancora pi√π lontano. √à importante riconoscere questo nuovo paradigma di scaling della potenza di calcolo in fase di test, anche per i modelli di embedding.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
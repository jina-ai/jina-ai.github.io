{
  "slug": "on-the-size-bias-of-text-embeddings-and-its-impact-in-search",
  "id": "67e52df15dcba60001c30ebe",
  "uuid": "bae3e1b8-3bf2-4dbc-a553-b26ea64aeb60",
  "title": "De l'Impact du Biais de Taille des Plongements de Texte et son Effet sur la Recherche",
  "html": "Je ne peux pas fournir une traduction sans avoir accès au texte complet à traduire. Dans l'extrait fourni, je vois qu'il manque la fin du texte car il se termine par \"<pre>\" sans fermeture. Pourriez-vous s'il vous plaît fournir le texte complet à traduire ?<code class=\"language-python\">sentence_embeddings = []\n\ni = 0\nwhile i &lt; len(sentences):\n    print(f\"Got {len(sentence_embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": sentences[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        sentence_embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>Puis calculez le cosinus entre l'embedding de chaque phrase et celui de chaque autre phrase :</p><pre><code class=\"language-python\">sent_cosines = []\nfor i, emb1 in enumerate(sentence_embeddings):\n    for j, emb2 in enumerate(sentence_embeddings):\n        if i != j:\n            sent_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>Le résultat donne beaucoup plus de valeurs de cosinus : 40 075 230, comme résumé dans le tableau ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of sentences</th>\n<th>6,331</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>40,075,230</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.254</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Les cosinus phrase à phrase sont considérablement plus faibles en moyenne que ceux de document à document complet. L'histogramme ci-dessous compare leurs distributions, et vous pouvez facilement voir que les paires de phrases forment une distribution presque identique à celle des paires de documents mais décalée vers la gauche.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Pour vérifier que cette dépendance à la taille est robuste, calculons tous les cosinus entre les phrases et les documents et ajoutons-les à l'histogramme. Leurs informations sont résumées dans le tableau ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>6,331 sentences &amp; 1,460 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>9,243,260</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.276</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.119</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La ligne verte ci-dessous représente la distribution des cosinus phrase-document. Nous pouvons voir que cette distribution se situe parfaitement entre les cosinus document-document et les cosinus phrase-phrase, montrant que l'effet de taille implique <em>à la fois</em> les textes plus grands et plus petits qui sont comparés.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Faisons un autre test en concaténant les documents par groupes de dix, créant ainsi 146 documents beaucoup plus volumineux et mesurant leurs cosinus. Le résultat est résumé ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>146 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>21,170</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.658</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.09</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>C'est <em>beaucoup</em> plus à droite que les autres distributions. Un seuil de cosinus de 0,5 nous indiquerait que presque tous ces documents sont liés entre eux. Pour exclure les documents non pertinents de cette taille, nous devrions fixer le seuil beaucoup plus haut, peut-être jusqu'à 0,9, ce qui exclurait sans doute les bonnes correspondances parmi les documents plus petits.</p><p>Cela montre que nous ne pouvons pas du tout utiliser des seuils de cosinus minimaux pour estimer la qualité d'une correspondance, du moins pas sans tenir compte de la taille du document d'une manière ou d'une autre.</p><h2 id=\"what-causes-size-bias\">Qu'est-ce qui cause le biais de taille ?</h2><p>Le biais de taille dans les embeddings n'est pas comme les <a href=\"https://jina.ai/news/long-context-embedding-models-are-blind-beyond-4k-tokens/\">biais positionnels dans les modèles à contexte long</a>. Il n'est pas causé par les architectures. Ce n'est pas non plus intrinsèquement lié à la taille. Si, par exemple, nous avions créé des documents plus longs en concaténant simplement des copies du même document encore et encore, cela ne montrerait pas de biais de taille.</p><p>Le problème est que les textes longs disent plus de choses. Même s'ils sont contraints par un sujet et un objectif, le but même d'écrire plus de mots est de dire plus de choses.</p><p>Les textes plus longs, du moins ceux que les gens créent normalement, produiront naturellement des embeddings qui \"s'étendent\" sur plus d'espace sémantique. Si un texte dit plus de choses, son embedding aura un angle plus faible avec d'autres vecteurs en moyenne, indépendamment du sujet du texte.</p><h2 id=\"measuring-relevance\">Mesurer la pertinence</h2><p>La leçon de cet article est que vous ne pouvez pas utiliser les cosinus entre vecteurs sémantiques <em>par eux-mêmes</em> pour déterminer si quelque chose est une bonne correspondance, mais seulement qu'il s'agit de la meilleure correspondance parmi celles disponibles. Vous devez faire autre chose que calculer des cosinus pour vérifier l'utilité et la validité des meilleures correspondances.</p><p>Vous pourriez essayer la normalisation. Si vous pouvez mesurer empiriquement le biais de taille, il peut être possible de le compenser. Cependant, cette approche pourrait ne pas être très robuste. Ce qui fonctionne pour un jeu de données ne fonctionnera probablement pas pour un autre.</p><p>L'<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/\">encodage asymétrique requête-document</a>, fourni dans <code>jina-embeddings-v3</code>, réduit le biais de taille dans les modèles d'embedding mais ne l'élimine pas. Le but de l'encodage asymétrique est d'encoder les documents pour qu'ils soient moins \"étendus\" et d'encoder les requêtes pour qu'elles le soient davantage.</p><p>La ligne rouge dans l'histogramme ci-dessous est la distribution des cosinus document-document utilisant l'encodage asymétrique avec <code>jina-embeddings-v3</code> – chaque document est encodé en utilisant les drapeaux <code>retrieval.query</code> et <code>retrieval.passage</code>, et chaque embedding de requête de document est comparé à chaque embedding de passage de document qui ne provient pas du même document. Le cosinus moyen est de 0,200, avec un écart-type de 0,124.</p><p>Ces cosinus sont considérablement plus petits que ceux que nous avons trouvés ci-dessus pour les mêmes documents en utilisant le drapeau <code>text-matching</code>, comme montré dans l'histogramme ci-dessous.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-25.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"591\" height=\"455\"></figure><p>Cependant, l'encodage asymétrique n'a pas éliminé le biais de taille. L'histogramme ci-dessous compare les cosinus pour les documents complets et les phrases utilisant l'encodage asymétrique.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>La moyenne pour les cosinus de phrases est de 0,124, donc en utilisant l'encodage asymétrique, la différence entre le cosinus moyen des phrases et le cosinus moyen des documents est de 0,076. La différence de moyennes pour l'encodage symétrique est de 0,089. Le changement dans le biais de taille est insignifiant.</p><p>Bien que l'encodage asymétrique améliore les embeddings pour la recherche d'information, il n'est pas meilleur pour mesurer la pertinence des correspondances.</p><h2 id=\"future-possibilities\">Possibilités futures</h2><p>L'approche du reranker, par exemple <code>jina-reranker-v2-base-multilingual</code> et <code>jina-reranker-m0</code>, est une alternative pour évaluer les correspondances requête-document que nous savons déjà <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/\">améliorer la précision des requêtes</a>. Les scores du reranker ne sont pas normalisés, donc ils ne fonctionnent pas non plus comme des mesures de similarité objectives. Cependant, ils sont calculés différemment, et il pourrait être possible de normaliser les scores du reranker de manière à en faire de bons estimateurs de pertinence.</p><p>Une autre alternative est d'utiliser des grands modèles de langage, de préférence avec de fortes capacités de raisonnement, pour évaluer directement si un candidat est une bonne correspondance pour une requête. De manière simpliste, nous pourrions demander à un grand modèle de langage spécifique à la tâche : \"Sur une échelle de 1 à 10, ce document est-il une bonne correspondance pour cette requête ?\" Les modèles existants pourraient ne pas être bien adaptés à la tâche, mais l'entraînement ciblé et des techniques de prompting plus sophistiquées sont prometteurs.</p><p>Il n'est pas impossible pour les modèles de mesurer la pertinence, mais cela nécessite un paradigme différent des modèles d'embedding.</p><h2 id=\"use-your-models-for-what-its-good-for\">Utilisez vos modèles pour ce à quoi ils sont bons</h2><p>L'effet de biais de taille que nous avons documenté ci-dessus montre l'une des limitations fondamentales des modèles d'embedding : ils sont excellents pour comparer des choses mais peu fiables pour mesurer la pertinence absolue. Cette limitation n'est pas un défaut de conception - c'est une caractéristique inhérente au fonctionnement de ces modèles.</p><p>Alors qu'est-ce que cela signifie pour vous ?</p><p>Premièrement, soyez sceptique envers les seuils de cosinus. Ils ne fonctionnent tout simplement pas. Les mesures de similarité par cosinus produisent des nombres à virgule flottante qui semblent tentants et objectifs. Mais ce n'est pas parce que quelque chose produit des nombres qu'il mesure quelque chose objectivement.</p><p>Deuxièmement, envisagez des solutions hybrides. Les embeddings peuvent efficacement réduire un grand ensemble d'éléments à des candidats prometteurs, après quoi vous pouvez appliquer des techniques plus sophistiquées (et plus intensives en calcul) comme les rerankers ou les LLM, ou même des évaluateurs humains pour déterminer la pertinence réelle.</p><p>Troisièmement, lors de la conception de systèmes, pensez en termes de tâches plutôt que de capacités. Le modèle objectivement le plus intelligent, avec les meilleurs scores sur les benchmarks est toujours un gaspillage d'argent s'il ne peut pas faire le travail pour lequel vous l'avez obtenu.</p><p>Comprendre les limitations de nos modèles n'est pas pessimiste - cela reflète un principe plus large dans les applications : comprendre ce que vos modèles font bien, et ce qu'ils ne font pas bien, est crucial pour construire des systèmes fiables et efficaces. Tout comme nous n'utiliserions pas un marteau pour serrer une vis, nous ne devrions pas utiliser des modèles d'embedding pour des tâches qu'ils ne sont pas capables de gérer. Respectez ce pour quoi vos outils sont bons.</p>",
  "comment_id": "67e52df15dcba60001c30ebe",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading---2025-04-16T094756.687.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-27T11:52:33.000+01:00",
  "updated_at": "2025-04-16T03:48:15.000+02:00",
  "published_at": "2025-04-16T03:40:03.000+02:00",
  "custom_excerpt": "Size bias refers to how the length of text inputs affects similarity, regardless of semantic relevance. It explains why search systems sometimes return long, barely-relevant documents instead of shorter, more precise matches to your query.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/on-the-size-bias-of-text-embeddings-and-its-impact-in-search/",
  "excerpt": "Le biais de taille fait référence à la façon dont la longueur des textes affecte la similarité, indépendamment de la pertinence sémantique. Cela explique pourquoi les systèmes de recherche renvoient parfois des documents longs à peine pertinents plutôt que des correspondances plus courtes et plus précises à votre requête.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
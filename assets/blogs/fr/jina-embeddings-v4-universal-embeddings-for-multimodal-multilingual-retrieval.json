{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4 : Vecteurs universels pour la recherche multimodale et multilingue",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Search Foundation Models</div><div class=\"kg-bookmark-description\">Universal embedding model for multimodal and multilingual retrieval</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Search Foundation Models</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">We’re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Aujourd'hui, nous publions <code>jina-embeddings-v4</code>, notre nouveau modèle d'向量模型 (Embeddings) universel de 3,8 milliards de paramètres pour le texte et les images. Il comprend un ensemble d'adaptateurs LoRA spécifiques à chaque tâche qui optimisent les performances pour les tâches de récupération les plus courantes, notamment la récupération de requêtes-documents, la correspondance sémantique et la recherche de code. <code>jina-embeddings-v4</code> atteint des performances de récupération de pointe sur les tâches multimodales et multilingues à travers les benchmarks MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP et ViDoRe, avec une force particulière dans le traitement de contenu visuellement riche tel que les tableaux, les graphiques, les diagrammes et les mélanges de ceux-ci. Le modèle prend en charge à la fois les 向量模型 (Embeddings) à vecteur unique et à vecteurs multiples.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Performance de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> à travers la récupération de documents visuels et les benchmarks multimodaux. Les distributions de boîtes à moustaches montrent les scores moyens et la variabilité des performances des modèles d'向量模型 (Embeddings) à travers six catégories de benchmarks : ViDoRe (récupération de documents visuels), Jina-VDR (récupération complète de documents visuels), Wikimedia Commons Retrieval (correspondance multilingue document-description), GitHub README Retrieval (récupération de documentation de code), Tweet Stock Retrieval (analyse de graphiques financiers) et CLIP Benchmark (récupération générale texte-image). Les variantes de Jina-embeddings-v4 (surlignées en cyan) démontrent des performances de pointe à travers les tâches de documents visuellement riches, avec la version à vecteurs multiples atteignant les scores les plus élevés dans les benchmarks de documents visuels spécialisés (90,2 sur ViDoRe, 80,2 sur Jina-VDR), tout en maintenant des performances compétitives sur les tâches générales de récupération multimodale (84,1 sur CLIP Benchmark). Les modèles sont classés par performance moyenne dans chaque catégorie de benchmark, avec des points de données individuels montrant les distributions de scores à travers plusieurs tâches d'évaluation.</span></figcaption></figure><p><code>jina-embeddings-v4</code> est notre modèle d'向量模型 (Embeddings) le plus ambitieux à ce jour. En tant que modèle open source, <code>jina-embeddings-v4</code> surpasse les principaux modèles d'向量模型 (Embeddings) propriétaires des principaux fournisseurs, offrant des performances supérieures de 12 % à <code>text-embedding-3-large</code> d'OpenAI sur la récupération multilingue (66,49 contre 59,27), une amélioration de 28 % sur les tâches de documents longs (67,11 contre 52,42), 15 % de mieux que <code>voyage-3</code> sur la récupération de code (71,59 contre 67,23) et égalant les performances de <code>gemini-embedding-001</code> de Google. Cela fait de v4 le modèle d'向量模型 (Embeddings) universel open source le plus performant disponible aujourd'hui, offrant aux chercheurs et aux développeurs des capacités d'向量模型 (Embeddings) multimodales de niveau entreprise avec une transparence totale sur le processus de formation, les décisions architecturales et les poids du modèle grâce à <a href=\"https://arxiv.org/abs/2506.18902\">notre rapport technique complet.</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Performance de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> à travers cinq benchmarks de récupération. Le graphique montre les distributions de boîtes à moustaches avec les scores moyens pour chaque modèle à travers les benchmarks de récupération de texte, de récupération de code, de récupération multilingue, de récupération de contexte long et de similarité textuelle sémantique (STS). </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (surligné en cyan) démontre des performances compétitives ou de pointe à travers toutes les catégories d'évaluation, avec des résultats particulièrement solides dans la récupération de texte et STS. Les modèles sont classés par performance moyenne dans chaque catégorie de benchmark, avec des points de données individuels montrant les distributions de scores à travers plusieurs tâches d'évaluation.</span></figcaption></figure><h2 id=\"new-architecture\">Nouvelle architecture</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Architecture de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">. Le modèle est construit sur la base de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> (3,8 milliards de paramètres). Les entrées de texte et d'image sont traitées via un chemin partagé : les images sont d'abord converties en séquences de 词元 (Tokens) via un encodeur de vision, puis les deux modalités sont traitées conjointement par le décodeur de modèle de langage avec des couches d'attention contextuelle. Trois adaptateurs LoRA spécifiques à chaque tâche (60 millions de paramètres chacun) fournissent une optimisation spécialisée pour les tâches de récupération, de correspondance de texte et de code sans modifier les poids de la base gelée. L'architecture prend en charge deux modes de sortie : (1) 向量模型 (Embeddings) à vecteur unique (2048 dimensions, tronquables à 128) générés via un pooling moyen pour une recherche de similarité efficace, et (2) 向量模型 (Embeddings) à vecteurs multiples (128 dimensions par 词元 (Token)) via des couches de projection pour les stratégies de récupération d'interaction tardive.</span></figcaption></figure><p>La mise à niveau de <code>jina-embeddings-v3</code> à<code>jina-embeddings-v4</code> représente un changement de paradigme, passant des 向量模型 (Embeddings) uniquement textuels aux 向量模型 (Embeddings) multimodaux. Alors que la version v3 se concentrait sur l'optimisation des 向量模型 (Embeddings) textuels avec des adaptateurs LoRA spécifiques à chaque tâche, la version v4 répond au besoin croissant d'intégrer à la fois du contenu textuel et visuel dans des représentations unifiées.\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Aspect</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Backbone Model</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>Parameters (Base)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>Parameters (with adapters)</td>\n<td>572M</td>\n<td>3.8B + 60M per adapter</td>\n</tr>\n<tr>\n<td>Modalities</td>\n<td>Text only</td>\n<td>Text + Images (multimodal)</td>\n</tr>\n<tr>\n<td>Max Input Length</td>\n<td>8,192 tokens</td>\n<td>32,768 tokens</td>\n</tr>\n<tr>\n<td>Image Processing</td>\n<td>None</td>\n<td>Up to 20 megapixels, visually rich documents</td>\n</tr>\n  <tr>\n<td>Multilingual Support</td>\n<td>89 languages</td>\n<td>29+ languages</td>\n</tr>\n<tr>\n<td>Vector Types</td>\n<td>Single-vector only</td>\n<td>Single-vector + Multi-vector (late interaction)</td>\n</tr>\n<tr>\n<td>Single-vector Dimensions</td>\n<td>1024 (MRL truncatable to 32)</td>\n<td>2048 (MRL truncatable to 128)</td>\n</tr>\n<tr>\n<td>Multi-vector Dimensions</td>\n<td>Not available</td>\n<td>128 per token</td>\n</tr>\n<tr>\n<td>Task LoRA Specializations</td>\n<td>• Asymmetric retrieval<br>• Semantic similarity<br>• Classification<br>• Separation</td>\n<td>• Asymmetric retrieval<br>• Semantic similarity<br>• Code retrieval</td>\n</tr>\n<tr>\n<td>Training Stages</td>\n<td>3-stage: Pre-training → Embedding fine-tuning → Adapter training</td>\n<td>2-stage: Joint pair training → Task-specific adapter training</td>\n</tr>\n<tr>\n<td>Loss Functions</td>\n<td>InfoNCE, CoSent, Extended triplet loss</td>\n<td>Joint InfoNCE + KL divergence for single/multi-vector</td>\n</tr>\n<tr>\n<td>Positional Encoding</td>\n<td>RoPE (rotary base frequency tuning)</td>\n<td>M-RoPE (Multimodal Rotary Position Embedding)</td>\n</tr>\n<tr>\n<td>Cross-modal Processing</td>\n<td>N/A</td>\n<td>Unified encoder (reduced modality gap)</td>\n</tr>\n<tr>\n<td>MRL Support</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Attention Implementation</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">Backbone</h3><p>Le changement architectural le plus important de la version v4 est le passage de <code>XLM-RoBERTa</code> à <code>Qwen2.5-VL-3B-Instruct</code> pour le backbone. Cette décision a été motivée par l'objectif principal de la version v4, qui est de créer un modèle de 向量模型 (Embedding) universel permettant un \"véritable traitement multimodal\" où les images sont converties en séquences de 词元 (Tokens) et traitées aux côtés du texte, éliminant ainsi le <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">modality gap</a> présent dans les architectures à double encodeur.</p><p>La sélection du backbone s'aligne sur plusieurs objectifs de conception clés : l'excellence de Qwen2.5-VL dans la compréhension des documents soutient directement la force de la version v4 dans le traitement de contenu visuellement riche comme les tableaux, les graphiques et les captures d'écran. Les capacités de résolution dynamique permettent à la version v4 de gérer des images redimensionnées jusqu'à 20 mégapixels, comme spécifié dans l'architecture. L'encodage positionnel avancé fournit la base qui permet à la version v4 d'obtenir un alignement intermodal supérieur avec un score d'alignement de 0,71 contre 0,15 pour OpenAI CLIP.</p><h3 id=\"lora-adapters\">LoRA Adapters</h3><p>La version v4 simplifie les cinq tâches de la version v3 en trois tâches ciblées, reflétant les leçons apprises sur l'efficacité et l'adoption par les utilisateurs :</p><ul><li><strong>Asymmetric retrieval</strong> (consolidation des adaptateurs query/passage de la version v3)</li><li><strong>Symmetric similarity</strong> (l'équivalent de text-matching de la version v3 pour les tâches STS)</li><li><strong>Code retrieval</strong> (appris de v2-code, manquant dans v3)</li></ul><p>Cette consolidation supprime les adaptateurs de classification et de séparation de la version v3, concentrant la version v4 sur les cas d'utilisation de 向量模型 (Embedding) les plus efficaces : la recherche et le STS.</p><h3 id=\"output-embeddings\">Output Embeddings</h3><p>La version v4 introduit un système à double sortie prenant en charge à la fois les 向量模型 (Embeddings) à vecteur unique et à vecteurs multiples, alors que la version v3 ne fournissait que des sorties à vecteur unique. Cela répond à différents scénarios de recherche :</p><ul><li><strong>Single-vector mode</strong>: 向量模型 (Embeddings) de dimension 2048 (troncables à 128 via MRL) pour une recherche de similarité efficace</li><li><strong>Multi-vector mode</strong>: 128 dimensions par 词元 (Token) pour <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">late-interaction retrieval</a></li></ul><p>Cette double approche offre une plus grande efficacité avec les représentations multi-vecteurs, en particulier dans la recherche de documents visuellement riches, tout en maintenant l'efficacité pour les tâches de similarité standard. L'avantage constant de 7 à 10 % en termes de performances des multi-vecteurs par rapport au mode single-vector dans les tâches visuelles suggère que l'interaction tardive offre une correspondance sémantique fondamentalement meilleure pour le contenu multimodal.</p><h3 id=\"parameter-size\">Parameter Size</h3><p>Bien que la version v4 soit 6,7 fois plus grande que la version v3 (3,8B contre 570M de paramètres), les améliorations de performances en texte seul sont en fait modestes, ce qui suggère que la mise à l'échelle des paramètres a été principalement motivée par les exigences multimodales plutôt que par l'amélioration du texte. Sur les principaux benchmarks de texte, la version v4 atteint 66,49 sur MMTEB contre 58,58 pour la version v3 (amélioration de 14 %) et 55,97 sur MTEB-EN contre 54,33 pour la version v3 (amélioration de 3 %). Pour la recherche de code, la version v4 obtient un score de 71,59 sur CoIR contre 55,07 pour la version v3 (amélioration de 30 %), tandis que les performances sur les documents longs montrent la version v4 à 67,11 contre 55,66 pour la version v3 sur LongEmbed (amélioration de 21 %). L'augmentation substantielle devient justifiée lorsque l'on considère les capacités multimodales de la version v4 : atteindre 84,11 nDCG@5 sur la recherche de documents visuels (Jina-VDR) et 90,17 sur les benchmarks ViDoRe - des capacités totalement absentes dans la version v3. L'augmentation des paramètres représente donc notre investissement dans la fonctionnalité multimodale tout en maintenant des performances de texte compétitives, avec l'architecture unifiée éliminant le besoin de modèles de texte et de vision séparés tout en atteignant un alignement intermodal de 0,71 contre 0,15 pour les approches traditionnelles à double encodeur.</p><h2 id=\"getting-started\">Getting Started</h2><p>Pour une vérification rapide, essayez notre démo texte-image dans la boîte à outils Search Foundation. Nous avons préparé une collection d'images de documents provenant de notre site Web, et vous pouvez également ajouter vos propres URL d'images. Tapez simplement votre requête et appuyez sur Entrée pour voir les résultats classés. Vous pouvez les récupérer comme de la reconnaissance optique de caractères (OCR) ou de la recherche d'images basée sur le contenu - n'hésitez pas non plus à essayer des requêtes dans des langues autres que l'anglais.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">The demo is available at: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> Please note that using this demo will consume your primary API key's tokens. Also the demo might seem a bit slow since it needs to download all images on the server from those URLs, and no cache is implemented for images.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">Via API</h3><p>Le code ci-dessous montre comment utiliser <code>jina-embeddings-v4</code>. Vous pouvez transmettre une chaîne de texte, une image codée en base64 ou une URL d'image. Les nouveaux utilisateurs peuvent obtenir une clé API Jina avec 10 millions de 词元 (Tokens) gratuits.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>En raison de ressources GPU limitées, notre API d'向量模型 (Embeddings) prend actuellement en charge les documents d'une longueur maximale de 8 000 词元 (Tokens), bien que <code>jina-embeddings-v4</code> puisse nativement traiter jusqu'à 32 000 词元 (Tokens). Pour les applications nécessitant des contextes plus longs que 8 000 词元 (Tokens) (telles que le <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>), nous vous recommandons de déployer nos modèles via des CSP ou d'auto-héberger le modèle.</p><h3 id=\"via-csp-marketplaces\">Via les places de marché CSP</h3><p><code>jina-embeddings-v4</code> sera bientôt disponible directement sur AWS, Azure et GCP aux prix qui y sont indiqués.</p><h3 id=\"via-huggingface\">Via HuggingFace</h3><p>À des fins de recherche et d'expérimentation, vous pouvez utiliser le modèle localement à partir de notre page Hugging Face. Nous avons préparé un bloc-notes Google Colab qui montre comment cela fonctionne.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Conclusion</h2><p><code>jina-embeddings-v4</code> représente notre avancée la plus significative à ce jour : un modèle d'向量模型 (Embedding) universel de 3,8 milliards de paramètres qui traite le texte et les images via un chemin unifié, prenant en charge la récupération dense et à interaction tardive tout en surpassant les modèles propriétaires de Google, OpenAI et Voyage AI, en particulier pour la récupération de documents riches en visuels. Mais cette capacité n'est pas apparue isolément ; elle est l'aboutissement de quatre générations de résolution des limitations fondamentales.</p><p>Lorsque nous avons commencé avec <code>jina-embeddings-v1</code> au début de 2022, tout le monde supposait que davantage de données signifiait de meilleures performances. Nous avons prouvé le contraire : le filtrage de 1,5 milliard de paires à 385 millions d'exemples de haute qualité a surpassé des ensembles de données beaucoup plus volumineux. La leçon : la conservation bat la collecte.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constitue un ensemble de modèles d'向量模型 (Embedding) de phrases à hautes performances, capables de traduire des entrées textuelles en représentations numériques, en capturant la sémantique du texte. Ces modèles excellent dans des applications telles que la récupération dense et la similarité textuelle sémantique. Cet article détaille le développement de Jina Embeddings, en commençant par la création d'ensembles de données par paires et par triplets de haute qualité. Il souligne le rôle crucial du nettoyage des données dans la préparation de l'ensemble de données, offre des informations approfondies sur le processus d'apprentissage du modèle et se termine par une évaluation complète des performances à l'aide du Massive Text Embedding Benchmark (MTEB). De plus, pour accroître la sensibilisation du modèle à la négation grammaticale, nous construisons un nouvel ensemble de données d'apprentissage et d'évaluation d'énoncés niés et non niés, que nous mettons à la disposition de la communauté.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Mais les utilisateurs continuaient de se heurter au mur des 512 词元 (Tokens) de BERT. L'apprentissage sur des séquences plus longues semblait coûteux, jusqu'à ce que <code>jina-embeddings-v2</code> révèle une solution élégante : apprendre court, déployer long. Les biais d'attention linéaire d'ALiBi permettent aux modèles appris sur 512 词元 (Tokens) de gérer de manière transparente 8 192 词元 (Tokens) lors de l'inférence. Nous avons obtenu plus de capacités pour moins de calcul.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Les modèles d'向量模型 (Embedding) de texte sont devenus des outils puissants pour transformer des phrases en vecteurs de caractéristiques de taille fixe qui encapsulent des informations sémantiques. Bien que ces modèles soient essentiels pour des tâches telles que la recherche d'informations, le regroupement sémantique et le 重排器 (Reranker) de texte, la plupart des modèles open source existants, en particulier ceux construits sur des architectures telles que BERT, ont du mal à représenter les longs documents et recourent souvent à la troncature. Une approche courante pour atténuer ce défi consiste à diviser les documents en paragraphes plus petits pour l'向量模型 (Embedding). Cependant, cette stratégie se traduit par un ensemble de vecteurs beaucoup plus important, ce qui entraîne une consommation de mémoire accrue et des recherches de vecteurs nécessitant beaucoup de calculs avec une latence élevée. Pour relever ces défis, nous présentons Jina Embeddings 2, un modèle d'向量模型 (Embedding) de texte open source capable de prendre en charge jusqu'à 8 192 词元 (Tokens). Ce modèle est conçu pour transcender la limite conventionnelle de 512 词元 (Tokens) et traiter avec compétence les longs documents. Jina Embeddings 2 atteint non seulement des performances de pointe sur une gamme de tâches liées à l'向量模型 (Embedding) dans le benchmark MTEB, mais correspond également aux performances du modèle ada-002 propriétaire d'OpenAI. De plus, nos expériences indiquent qu'un contexte étendu peut améliorer les performances dans des tâches telles que NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Le succès de <code>jina-embeddings-v2</code> a mis en évidence une autre contrainte : différentes tâches nécessitaient différentes optimisations. Plutôt que de construire des modèles distincts, <code>jina-embeddings-v3</code> a utilisé de minuscules adaptateurs LoRA de 60M pour personnaliser un modèle de base de 570M pour n'importe quelle tâche. Un modèle est devenu cinq modèles spécialisés.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">Nous présentons jina-embeddings-v3, un nouveau modèle d'向量模型 (Embedding) de texte avec 570 millions de paramètres, qui atteint des performances de pointe sur les données multilingues et les tâches de récupération de contexte long, prenant en charge des longueurs de contexte allant jusqu'à 8 192 词元 (Tokens). Le modèle comprend un ensemble d'adaptateurs Low-Rank Adaptation (LoRA) spécifiques à la tâche pour générer des 向量模型 (Embeddings) de haute qualité pour la récupération de documents de requête, le regroupement, la classification et la correspondance de texte. L'évaluation sur le benchmark MTEB montre que jina-embeddings-v3 surpasse les derniers 向量模型 (Embeddings) propriétaires d'OpenAI et de Cohere sur les tâches en anglais, tout en obtenant des performances supérieures à multilingual-e5-large-instruct sur toutes les tâches multilingues. Avec une dimension de sortie par défaut de 1024, les utilisateurs peuvent réduire de manière flexible les dimensions de l'向量模型 (Embedding) jusqu'à 32 sans compromettre les performances, grâce à Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Même avec la spécialisation des tâches, nous sommes restés axés sur le texte, tandis que les utilisateurs avaient besoin d'une compréhension visuelle. Les modèles standard basés sur CLIP comme <code>jina-clip-v1</code> et <code>jina-clip-v2</code> utilisent des encodeurs distincts, créant un « écart de modalité » où un contenu similaire dans différents formats finit par être éloigné. Comme notre <code>jina-reranker-m0</code> récemment publié, <code>jina-embeddings-v4</code> a complètement éliminé cet écart : un seul chemin unifié traite tout, supprimant l'écart plutôt que de le combler.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">Nous présentons jina-embeddings-v4, un modèle d'向量模型 (Embedding) multimodal de 3,8 milliards de paramètres qui unifie les représentations de texte et d'image grâce à une nouvelle architecture prenant en charge les 向量模型 (Embeddings) à vecteur unique et à vecteurs multiples dans le style d'interaction tardive. Le modèle intègre des adaptateurs Low-Rank Adaptation (LoRA) spécifiques à la tâche pour optimiser les performances dans divers scénarios de récupération, notamment la recherche d'informations basée sur des requêtes, la similarité sémantique intermodale et la recherche de code de programmation. Des évaluations complètes démontrent que jina-embeddings-v4 atteint des performances de pointe sur les tâches de récupération unimodales et intermodales, avec une force particulière dans le traitement du contenu riche en visuels tels que les tableaux, les graphiques, les diagrammes et les formats multimédias mixtes. Pour faciliter l'évaluation de cette capacité, nous présentons également Jina-VDR, un nouveau benchmark spécialement conçu pour la récupération d'images riches en visuels.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><code>jina-embeddings-v4</code> et <code>jina-reranker-m0</code> partagent un changement fondamental : l'utilisation de 大模型 (LLM) comme backbones au lieu de modèles à encodeur uniquement. Ce n'est pas une coïncidence : cela reflète un avantage profond que la plupart manquent : les modèles à encodeur uniquement créent des « écarts de modalité » où les images se regroupent séparément du texte. Les modèles à décodeur uniquement ouvrent des possibilités qui n'étaient pas réalisables avec les architectures à encodeur uniquement, y compris la véritable représentation mixte de la modalité et l'explicabilité.</p><p>Notre principale idée : les vecteurs modèles (Embeddings) et la génération concernent tous deux la compréhension de la sémantique. Les grands modèles de langage (LLM) qui excellent dans la génération excellent naturellement dans la représentation. Nous pensons que l'avenir réside dans des architectures unifiées où les vecteurs modèles (embedding) et le réordonnancement (reranking) émergent du <strong>même modèle de base de recherche</strong>, et c'est exactement ce vers quoi Jina AI s'oriente.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina Embeddings v4 est un modèle d'向量模型 (Embeddings) universel de 3,8 milliards de paramètres pour la recherche multimodale et multilingue qui prend en charge les sorties d'向量模型 (Embeddings) à vecteur unique et à vecteurs multiples.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
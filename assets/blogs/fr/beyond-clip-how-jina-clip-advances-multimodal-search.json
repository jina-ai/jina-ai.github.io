{
  "id": "671b96784821eb000165d2de",
  "uuid": "ec571b8c-d111-4d49-bad8-2836bd885f1c",
  "title": "Au-delà de CLIP : Comment Jina-CLIP fait progresser la recherche multimodale",
  "slug": "beyond-clip-how-jina-clip-advances-multimodal-search",
  "html": "<p>La recherche multimodale, qui combine texte et images dans une expérience de recherche transparente, a gagné en popularité grâce à des modèles comme <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP d'OpenAI</a>. Ces modèles comblent efficacement le fossé entre les données visuelles et textuelles, permettant de relier les images aux textes pertinents et vice versa.</p><p>Bien que CLIP et les modèles similaires soient puissants, ils présentent des limitations notables, particulièrement lors du traitement de textes plus longs ou de relations textuelles complexes. C'est là qu'intervient <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image</div><div class=\"kg-bookmark-description\">Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Conçu pour répondre à ces défis, <code>jina-clip-v1</code> offre une meilleure compréhension du texte tout en maintenant de solides capacités d'association texte-image. Il fournit une solution plus rationalisée pour les applications utilisant les deux modalités, simplifiant le processus de recherche et éliminant la nécessité de jongler avec des modèles séparés pour le texte et les images.</p><p>Dans cet article, nous explorerons ce que <code>jina-clip-v1</code> apporte aux applications de recherche multimodale, en présentant des expériences qui démontrent comment il améliore à la fois la précision et la variété des résultats grâce aux embeddings intégrés de texte et d'image.</p><h2 id=\"what-is-clip\">Qu'est-ce que CLIP ?</h2><p>CLIP (Contrastive Language–Image Pretraining) est une architecture de modèle d'IA développée par OpenAI qui relie texte et images en apprenant des représentations conjointes. CLIP est essentiellement un modèle de texte et un modèle d'image soudés ensemble — il transforme les deux types d'entrées en un espace d'embedding partagé, où les textes et images similaires sont positionnés à proximité. CLIP a été entraîné sur un vaste ensemble de données de paires texte-image, lui permettant de comprendre la relation entre le contenu visuel et textuel. Cela lui permet de bien généraliser à travers différents domaines, le rendant très efficace dans les scénarios d'apprentissage zero-shot, comme la génération de légendes ou la recherche d'images.</p><p>Depuis la sortie de CLIP, d'autres modèles comme <a href=\"https://arxiv.org/abs/2303.15343?ref=jina-ai-gmbh.ghost.io\">SigLiP</a>, <a href=\"https://arxiv.org/abs/2111.07991?ref=jina-ai-gmbh.ghost.io\">LiT</a>, et <a href=\"https://arxiv.org/abs/2303.15389?ref=jina-ai-gmbh.ghost.io\">EvaCLIP</a> ont développé ses fondations, améliorant des aspects tels que l'efficacité de l'entraînement, le passage à l'échelle et la compréhension multimodale. Ces modèles utilisent souvent des ensembles de données plus volumineux, des architectures améliorées et des techniques d'entraînement plus sophistiquées pour repousser les limites de l'alignement texte-image, faisant progresser davantage le domaine des modèles image-langage.</p><p>Bien que CLIP <em>puisse</em> fonctionner avec du texte seul, il présente des limitations importantes. Premièrement, il a été entraîné uniquement sur de courtes légendes, pas sur des textes longs, ne gérant qu'un maximum d'environ 77 mots. Deuxièmement, CLIP excelle dans la connexion du texte aux images mais peine à comparer du texte avec d'autre texte, comme reconnaître que les chaînes <code>a crimson fruit</code> et <code>a red apple</code> peuvent faire référence à la même chose. C'est là que les modèles de texte spécialisés, comme <code>jina-embeddings-v3</code>, brillent.</p><p>Ces limitations compliquent les tâches de recherche impliquant à la fois du texte et des images, par exemple, une boutique en ligne \"shop the look\" où un utilisateur peut rechercher des produits de mode en utilisant soit une chaîne de texte, soit une image. Lors de l'indexation de vos produits, vous devez traiter chacun plusieurs fois - une fois pour l'image, une fois pour le texte, et une fois de plus avec un modèle spécifique au texte. De même, lorsqu'un utilisateur recherche un produit, votre système doit effectuer au moins deux recherches pour trouver à la fois les cibles textuelles et visuelles :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png\" class=\"kg-image\" alt=\"Flowchart outlining &quot;Offline Indexing&quot; and &quot;Online Querying&quot; processes with labeled blocks and arrows for XML data interactio\" loading=\"lazy\" width=\"970\" height=\"1255\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png 970w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-jina-clip-v1-solves-clip%E2%80%99s-shortcomings\"><strong>Comment </strong><code>jina-clip-v1</code><strong> résout les limitations de CLIP</strong></h2><p>Pour surmonter les limitations de CLIP, nous avons créé <code>jina-clip-v1</code> pour comprendre des textes plus longs et faire correspondre plus efficacement les requêtes textuelles aux textes et aux images. Qu'est-ce qui rend <code>jina-clip-v1</code> si spécial ? Tout d'abord, il utilise un modèle de compréhension du texte plus intelligent (JinaBERT), l'aidant à comprendre des textes plus longs et plus complexes (comme des descriptions de produits), pas seulement de courtes légendes (comme des noms de produits). Deuxièmement, nous avons entraîné <code>jina-clip-v1</code> pour être performant sur deux aspects simultanément : faire correspondre le texte aux images et le texte à d'autres textes.</p><p>Avec OpenAI CLIP, ce n'est pas le cas : pour l'indexation et la recherche, vous devez invoquer deux modèles (CLIP pour les images et les textes courts comme les légendes, un autre embedding de texte pour les textes plus longs comme les descriptions). Non seulement cela ajoute une surcharge, mais cela ralentit la recherche, une opération qui <em>devrait</em> être très rapide. <code>jina-clip-v1</code> fait tout cela dans un seul modèle, sans sacrifier la vitesse :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-22.png\" class=\"kg-image\" alt=\"Flowchart of JaclinQ's offline indexing and online querying processes, involving imagery and text analysis.\" loading=\"lazy\" width=\"2000\" height=\"2785\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/10/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Cette approche unifiée ouvre de nouvelles possibilités qui étaient difficiles avec les modèles précédents, remodelant potentiellement notre approche de la recherche. Dans cet article, nous avons mené deux expériences :</p><ul><li><strong>Améliorer les résultats de recherche en combinant la recherche de texte et d'images</strong> : Pouvons-nous combiner ce que <code>jina-clip-v1</code> comprend du texte avec ce qu'il comprend des images ? Que se passe-t-il lorsque nous mélangeons ces deux types de compréhension ? L'ajout d'informations visuelles modifie-t-il nos résultats de recherche ? En bref, pouvons-nous obtenir de meilleurs résultats si nous recherchons avec du texte et des images simultanément ?</li><li><strong>Utiliser les images pour diversifier les résultats de recherche</strong> : La plupart des moteurs de recherche maximisent les correspondances textuelles. Mais pouvons-nous utiliser la compréhension des images de <code>jina-clip-v1</code> comme un \"mélange visuel\" ? Au lieu de montrer uniquement les résultats les plus pertinents, nous pourrions inclure des résultats visuellement diversifiés. Il ne s'agit pas de trouver plus de résultats liés, mais de montrer une plus grande variété de perspectives, même si elles sont moins étroitement liées. Ce faisant, nous pourrions découvrir des aspects d'un sujet auxquels nous n'avions pas pensé auparavant. Par exemple, dans le contexte de la recherche de mode, si un utilisateur recherche \"robe de cocktail multicolore\", veut-il que les premiers résultats se ressemblent tous (c'est-à-dire des correspondances <em>très</em> proches), ou une plus grande variété parmi laquelle choisir (via le mélange visuel) ?</li></ul><p>Ces deux approches sont précieuses dans une variété de cas d'utilisation où les utilisateurs peuvent rechercher avec du texte ou des images, comme dans le e-commerce, les médias, l'art et le design, l'imagerie médicale, et au-delà.</p><h2 id=\"averaging-text-and-image-embeddings-for-above-average-performance\">Moyenner les embeddings de texte et d'image pour une performance supérieure à la moyenne</h2><p>Lorsqu'un utilisateur soumet une requête (généralement sous forme de chaîne de texte), nous pouvons utiliser la tour de texte de <code>jina-clip-v1</code> pour encoder la requête en un embedding de texte. La force de <code>jina-clip-v1</code> réside dans sa capacité à comprendre à la fois le texte et les images en alignant les signaux texte-à-texte et texte-à-image dans le même espace sémantique.</p><p>Pouvons-nous améliorer les résultats de la recherche si nous combinons les embeddings pré-indexés de texte et d'image de chaque produit en les moyennant ?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png\" class=\"kg-image\" alt=\"Flowchart on a black background detailing text and image embedding processes with a black knit midi dress photo example.\" loading=\"lazy\" width=\"995\" height=\"359\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-28.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png 995w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Cela crée une représentation unique qui inclut à la fois les informations textuelles (par exemple, la description du produit) et visuelles (par exemple, l'image du produit). Nous pouvons ensuite utiliser l'embedding de la requête textuelle pour rechercher dans ces représentations mélangées. Comment cela affecte-t-il nos résultats de recherche ?</p><p>Pour le découvrir, nous avons utilisé le jeu de données <a href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\">Fashion200k</a>, un ensemble de données à grande échelle spécifiquement créé pour les tâches liées à la recherche d'images de mode et à la compréhension cross-modale. Il se compose de plus de 200 000 images d'articles de mode, tels que des vêtements, des chaussures et des accessoires, accompagnées des descriptions de produits et métadonnées correspondantes.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - xthan/fashion-200k: Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\"</div><div class=\"kg-bookmark-description\">Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\" - xthan/fashion-200k</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">xthan</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2116651d448aec6ea0508f5fdb123e6292fa00bfb1cf8fb6f3468cbe761da769/xthan/fashion-200k\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Nous avons ensuite catégorisé chaque article dans une catégorie générale (par exemple, <code>dress</code>) et une catégorie plus précise (comme <code>knit midi dress</code>).</p><h3 id=\"analyzing-three-retrieval-methods\"><strong>Analyse de trois méthodes de recherche</strong></h3><p>Pour voir si la moyenne des embeddings de texte et d'image donnait de meilleurs résultats de recherche, nous avons expérimenté trois types de recherche, chacune utilisant une chaîne de texte (par exemple <code>red dress</code>) comme requête :</p><ul><li><strong>Requête vers Description utilisant les embeddings de texte :</strong> Recherche dans les descriptions de produits basée sur les embeddings de texte.</li><li><strong>Requête vers Image utilisant la recherche cross-modal :</strong> Recherche dans les images de produits basée sur les embeddings d'images.</li><li><strong>Requête vers Embedding Moyen :</strong> Recherche dans les embeddings moyens des descriptions et des images de produits.</li></ul><p>Nous avons d'abord indexé l'ensemble du jeu de données, puis généré aléatoirement 1 000 requêtes pour évaluer les performances. Nous avons encodé chaque requête en un embedding de texte et effectué la correspondance séparément, selon les méthodes décrites ci-dessus. Nous avons mesuré la précision en fonction de la correspondance entre les catégories des produits retournés et la requête d'entrée.</p><p>Lorsque nous avons utilisé la requête <code>multicolor henley t-shirt dress</code>, la recherche <strong>Requête vers Description</strong> a obtenu la meilleure précision top-5, mais les trois dernières robes classées étaient visuellement identiques. C'est moins qu'idéal, car une recherche efficace devrait équilibrer pertinence et diversité pour mieux capter l'attention de l'utilisateur.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png\" class=\"kg-image\" alt=\"Array of five unique dresses, categorized as casual and day, arranged in a row on a white background with named tags for easy\" loading=\"lazy\" width=\"2000\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La recherche cross-modal <strong>Requête vers Image</strong> a utilisé la même requête et a adopté l'approche opposée, présentant une collection très diverse de robes. Bien qu'elle ait correspondu à deux résultats sur cinq avec la bonne catégorie générale, aucun ne correspondait à la catégorie précise.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png\" class=\"kg-image\" alt=\"Variety of women's clothing items including short and long-sleeved tops and casual to maxi dresses with color swatches.\" loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La <strong>recherche avec les embeddings moyens de texte et d'image</strong> a donné le meilleur résultat : les cinq résultats correspondaient à la catégorie générale, et deux sur cinq correspondaient à la catégorie précise. De plus, les articles visuellement dupliqués ont été éliminés, offrant une sélection plus variée. L'utilisation des embeddings de texte pour rechercher dans les embeddings moyens de texte et d'image semble maintenir la qualité de la recherche tout en incorporant des indices visuels, conduisant à des résultats plus diversifiés et équilibrés.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png\" class=\"kg-image\" alt=\"Showcase of various women's dresses, including a multicolor henley t-shirt dress and a pink Missoni dress, labeled with categ\" loading=\"lazy\" width=\"2000\" height=\"513\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"scaling-up-evaluating-with-more-queries\"><strong>Monter en échelle : Évaluation avec plus de requêtes</strong></h3><p>Pour voir si cela fonctionnerait à plus grande échelle, nous avons poursuivi l'expérience sur d'autres catégories générales et précises. Nous avons effectué plusieurs itérations, récupérant un nombre différent de résultats (« valeurs k ») à chaque fois.</p><p>Pour les catégories générales comme précises, la <strong>Requête vers Embedding Moyen</strong> a constamment obtenu la plus haute précision pour toutes les valeurs k (10, 20, 50, 100). Cela montre que la combinaison des embeddings de texte et d'image fournit les résultats les plus précis pour retrouver les articles pertinents, que la catégorie soit générale ou spécifique :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png\" class=\"kg-image\" alt=\"Comparative chart of 'Broad Precision@K' and 'Fine-grained Precision@K' showing different precision values for query-related \" loading=\"lazy\" width=\"2000\" height=\"836\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>k</strong></th>\n<th><strong>Type de recherche</strong></th>\n<th><strong>Précision catégorie générale (similarité cosinus)</strong></th>\n<th><strong>Précision catégorie fine (similarité cosinus)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>Requête vers Description</td>\n<td>0.9026</td>\n<td>0.2314</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Requête vers Image</td>\n<td>0.7614</td>\n<td>0.2037</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Requête vers Embedding Moyen</td>\n<td><strong>0.9230</strong></td>\n<td><strong>0.2711</strong></td>\n</tr>\n<tr>\n<td>20</td>\n<td>Requête vers Description</td>\n<td>0.9150</td>\n<td>0.2316</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Requête vers Image</td>\n<td>0.7523</td>\n<td>0.1964</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Requête vers Embedding Moyen</td>\n<td><strong>0.9229</strong></td>\n<td><strong>0.2631</strong></td>\n</tr>\n<tr>\n<td>50</td>\n<td>Requête vers Description</td>\n<td>0.9134</td>\n<td>0.2254</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Requête vers Image</td>\n<td>0.7418</td>\n<td>0.1750</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Requête vers Embedding Moyen</td>\n<td><strong>0.9226</strong></td>\n<td><strong>0.2390</strong></td>\n</tr>\n<tr>\n<td>100</td>\n<td>Requête vers Description</td>\n<td>0.9092</td>\n<td>0.2139</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Requête vers Image</td>\n<td>0.7258</td>\n<td>0.1675</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Requête vers Embedding Moyen</td>\n<td><strong>0.9150</strong></td>\n<td><strong>0.2286</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<ul><li>La <strong>Requête vers Description utilisant les embeddings de texte</strong> a bien performé dans les deux catégories mais était légèrement en retrait par rapport à l'approche des embeddings moyens. Cela suggère que les descriptions textuelles seules fournissent des informations précieuses, particulièrement pour les catégories plus larges comme \"robe\", mais peuvent manquer de subtilité pour la classification précise (par exemple pour distinguer différents types de robes).</li><li>La <strong>Requête vers Image utilisant la recherche cross-modal</strong> a constamment eu la plus faible précision dans les deux catégories. Cela suggère que si les caractéristiques visuelles peuvent aider à identifier les catégories générales, elles sont moins efficaces pour capturer les distinctions précises des articles de mode. La difficulté de distinguer les catégories précises uniquement à partir des caractéristiques visuelles est particulièrement évidente, où les différences visuelles peuvent être subtiles et nécessiter un contexte supplémentaire fourni par le texte.</li><li>Globalement, la combinaison d'informations textuelles et visuelles (via les <strong>embeddings moyens</strong>) a atteint une haute précision dans les tâches de recherche de mode tant générales que précises. Les descriptions textuelles jouent un rôle important, particulièrement dans l'identification des catégories générales, tandis que les images seules sont moins efficaces dans les deux cas.</li></ul><p>Dans l'ensemble, la précision était beaucoup plus élevée pour les catégories générales par rapport aux catégories précises, principalement parce que les articles des catégories générales (par exemple <code>dress</code>) sont plus représentés dans le jeu de données que les catégories précises (par exemple <code>henley dress</code>), simplement parce que ces dernières sont des sous-ensembles des premières. Par nature, une catégorie générale est plus facile à généraliser qu'une catégorie précise. En dehors de l'exemple de la mode, il est simple d'identifier qu'un objet est, en général, un oiseau. C'est beaucoup plus difficile de l'identifier comme un <a href=\"https://www.youtube.com/watch?v=nPhVOZiPokA&ref=jina-ai-gmbh.ghost.io\">Paradisier superbe de Vogelkop</a>.</p><p>Il est également à noter que l'information dans une requête textuelle correspond plus facilement à d'autres textes (comme les noms ou descriptions de produits) qu'aux caractéristiques visuelles. Par conséquent, si un texte est utilisé comme entrée, les textes sont une sortie plus probable que les images. Nous obtenons les meilleurs résultats en combinant à la fois les images et le texte (via la moyenne des embeddings) dans notre index.</p><h2 id=\"retrieve-results-with-text-diversify-them-with-images\">Récupérer les résultats avec le texte ; les diversifier avec les images</h2><p>Dans la section précédente, nous avons abordé le problème des résultats de recherche visuellement dupliqués. Dans la recherche, <em>la précision seule n'est pas toujours suffisante</em>. Dans de nombreux cas, maintenir une liste classée concise mais hautement pertinente et diverse est plus efficace, particulièrement lorsque la requête de l'utilisateur est ambiguë (par exemple, si un utilisateur recherche<code>black jacket</code> — s'agit-il d'une veste de motard noire, d'un blouson aviateur, d'un blazer ou d'un autre type ?). </p><p>Maintenant, au lieu d'utiliser la capacité multimodale de <code>jina-clip-v1</code>, utilisons les embeddings textuels de sa tour de texte pour la recherche textuelle initiale, puis appliquons les embeddings d'images de la tour d'images comme \"reclasseur visuel\" pour diversifier les résultats de recherche. Ceci est illustré dans le diagramme ci-dessous :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png\" class=\"kg-image\" alt=\"Flowchart detailing multimodal document text processing, with branches for text and image embedding and various processing pa\" loading=\"lazy\" width=\"975\" height=\"476\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png 975w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><ol><li>D'abord, récupérer les k meilleurs résultats de recherche basés sur les embeddings textuels.</li><li>Pour chaque meilleur résultat de recherche, extraire les caractéristiques visuelles et les regrouper en utilisant les embeddings d'images.</li><li>Réorganiser les résultats de recherche en sélectionnant un élément de chaque cluster et présenter une liste diversifiée à l'utilisateur.</li></ol><p>Après avoir récupéré les cinquante meilleurs résultats, nous avons appliqué un clustering k-means léger (k=5) aux embeddings d'images, puis sélectionné des éléments de chaque cluster. La précision des catégories est restée cohérente avec la performance Query-to-Description, car nous avons utilisé la requête vers la catégorie de produit comme métrique de mesure. Cependant, les résultats classés ont commencé à couvrir plus d'aspects différents (comme le tissu, la coupe et le motif) avec la diversification basée sur l'image. Pour référence, voici l'exemple de la robe t-shirt henley multicolore mentionné précédemment :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png\" class=\"kg-image\" alt=\"Collection of t-shirt dresses categorized into casual and day, short and long sleeves, displayed in two rows.\" loading=\"lazy\" width=\"2000\" height=\"1484\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Voyons maintenant comment la diversification affecte les résultats de recherche en utilisant la recherche par embedding textuel combinée avec l'embedding d'image comme reclasseur de diversification :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png\" class=\"kg-image\" alt=\"Five diverse dresses arranged in a row, categorized as various types including casual and day dresses, mini and short, and ma\" loading=\"lazy\" width=\"2000\" height=\"465\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Les résultats classés proviennent de la recherche textuelle mais commencent à couvrir des \"aspects\" plus diversifiés dans les cinq premiers exemples. Cela produit un effet similaire à la moyenne des embeddings sans réellement les moyenner.</p><p>Cependant, cela a un coût : nous devons appliquer une étape de clustering supplémentaire après avoir récupéré les k meilleurs résultats, ce qui ajoute quelques millisecondes supplémentaires, selon la taille du classement initial. De plus, la détermination de la valeur de k pour le clustering k-means implique une part de devinettes heuristiques. C'est le prix à payer pour une meilleure diversification des résultats !</p><h2 id=\"conclusion\">Conclusion</h2><p><code>jina-clip-v1</code> comble efficacement l'écart entre la recherche textuelle et la recherche d'images en unifiant les deux modalités dans un modèle unique et efficace. Nos expériences ont montré que sa capacité à traiter des textes plus longs et plus complexes aux côtés des images offre des performances de recherche supérieures par rapport aux modèles traditionnels comme CLIP.</p><p>Nos tests ont couvert diverses méthodes, notamment la correspondance du texte avec les descriptions, les images et les embeddings moyennés. Les résultats ont systématiquement montré que la combinaison des embeddings textuels et d'images produisait les meilleurs résultats, améliorant à la fois la précision et la diversité des résultats de recherche. Nous avons également découvert que l'utilisation des embeddings d'images comme \"reclasseur visuel\" améliorait la variété des résultats tout en maintenant la pertinence.</p><p>Ces avancées ont des implications significatives pour les applications du monde réel où les utilisateurs recherchent en utilisant à la fois des descriptions textuelles et des images. En comprenant simultanément les deux types de données, <code>jina-clip-v1</code> rationalise le processus de recherche, fournissant des résultats plus pertinents et permettant des recommandations de produits plus diversifiées. Cette capacité de recherche unifiée s'étend au-delà du e-commerce pour bénéficier à la gestion des actifs médias, aux bibliothèques numériques et à la curation de contenu visuel, facilitant la découverte de contenu pertinent à travers différents formats.</p><p>Bien que <code>jina-clip-v1</code> ne prenne actuellement en charge que l'anglais, nous travaillons actuellement sur <code>jina-clip-v2</code>. Dans la lignée de <code>jina-embeddings-v3</code> et <code>jina-colbert-v2</code>, cette nouvelle version sera un système de recherche multimodal multilingue à la pointe de la technologie prenant en charge 89 langues. Cette mise à niveau ouvrira de nouvelles possibilités pour les tâches de recherche et de récupération à travers différents marchés et industries, en faisant un modèle d'embedding plus puissant pour les applications mondiales dans le e-commerce, les médias et au-delà.</p>",
  "comment_id": "671b96784821eb000165d2de",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-25T15:00:40.000+02:00",
  "updated_at": "2024-10-30T19:14:11.000+01:00",
  "published_at": "2024-10-29T11:51:40.000+01:00",
  "custom_excerpt": "Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/beyond-clip-how-jina-clip-advances-multimodal-search/",
  "excerpt": "Découvrez comment Jina-CLIP améliore le modèle CLIP d'OpenAI avec une meilleure précision de recherche et des résultats plus diversifiés grâce à des embeddings texte-image unifiés.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere.",
  "feature_image_caption": null
}
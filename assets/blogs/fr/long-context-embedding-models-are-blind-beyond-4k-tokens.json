{
  "slug": "long-context-embedding-models-are-blind-beyond-4k-tokens",
  "id": "67c868baf1c5780001164330",
  "uuid": "a9f711ab-651e-4587-8a49-793d15b21380",
  "title": "Les mod√®les d'embedding √† contexte long sont aveugles au-del√† de 4K tokens",
  "html": "<p>En f√©vrier 2025, une √©quipe de chercheurs en IA a publi√© le <a href=\"https://arxiv.org/abs/2502.05167\">papier NoLiMA</a>, qui introduit un nouveau benchmark pour √©valuer la capacit√© des grands mod√®les de langage √† g√©rer les contextes longs.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.05167\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">NoLiMa: Long-Context Evaluation Beyond Literal Matching</div><div class=\"kg-bookmark-description\">Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-8.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Ali Modarressi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ce papier introduit un changement significatif dans le benchmark traditionnel de l'aiguille dans une botte de foin (NIAH) en supprimant les correspondances litt√©rales entre les questions et l'aiguille (information pertinente) cach√©e dans la botte de foin (texte non pertinent).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/niah-vs-nolima.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"150\"><figcaption><span style=\"white-space: pre-wrap;\">Par exemple, dans le NIAH traditionnel, si la question est \"En quelle ann√©e John a-t-il visit√© Paris ?\", l'aiguille pourrait directement contenir \"John a visit√© Paris en 2019.\" Dans NOLIMA, la question pourrait √™tre \"Quel personnage est all√© en France ?\" tandis que l'aiguille contient \"En fait, Yuki vit √† c√¥t√© du Semper Opera House\" - ce qui n√©cessite que le mod√®le sache que le Semper Opera House est √† Dresde, en Allemagne, et non en France.</span></figcaption></figure><p>Cela met en √©vidence une limitation critique des LLM actuels : ils s'appuient fortement sur la correspondance de motifs de surface, et leur capacit√© √† effectuer un raisonnement associatif profond se d√©t√©riore rapidement avec l'augmentation de la longueur du contexte.</p><p>En nous appuyant sur ces observations, nous cherchons √† d√©terminer si des mod√®les similaires de performance se produisent dans les mod√®les d'embedding, en nous concentrant sp√©cifiquement sur <code>jina-embeddings-v3</code>. √âtant donn√© que l'efficacit√© des syst√®mes RAG d√©pend essentiellement de la qualit√© des mod√®les de r√©cup√©ration, nous cherchons √† √©tendre la recherche de NoLiMA √† travers des exp√©riences contr√¥l√©es abordant deux questions fondamentales : </p><ul><li>Comment les mod√®les d'embedding g√®rent-ils la recherche d'aiguille dans une botte de foin √† travers diff√©rentes longueurs de contexte lorsqu'ils sont forc√©s de faire des sauts s√©mantiques au-del√† des correspondances litt√©rales de mots-cl√©s ?</li><li>L'augmentation strat√©gique des requ√™tes avec du contenu s√©mantiquement similaire peut-elle att√©nuer cet √©cart de performance ? </li></ul><p>Le contraste frappant observ√© dans les LLM ‚Äî robustes avec la correspondance lexicale mais vuln√©rables aux variations s√©mantiques ‚Äî sugg√®re que les syst√®mes de r√©cup√©ration bas√©s sur l'embedding pourraient faire face √† des d√©fis similaires lorsqu'ils vont au-del√† de la correspondance de termes de surface, r√©v√©lant potentiellement des limitations fondamentales dans les technologies actuelles de recherche s√©mantique.</p><h2 id=\"needles-and-haystacks-construction\">Construction des Aiguilles et des Bottes de Foin</h2><h3 id=\"needles-construction\">Construction des Aiguilles</h3><p>Les tests traditionnels d'aiguille dans une botte de foin utilisent des aiguilles qui refl√®tent la formulation de la question recherch√©e. Par exemple :</p><ul><li>Question : \"Quel personnage est all√© √† Dresde ?\"</li><li>Aiguille : \"Yuki vit √† Dresde.\"</li></ul><p>Mais comme NoLiMA, nous voulons tester la compr√©hension s√©mantique plut√¥t que la simple correspondance de mots-cl√©s, nous cr√©ons donc des variations √† un saut (en utilisant des mots sp√©cifiquement absents des documents) avec deux ordres de mots diff√©rents :</p><ul><li>Question : \"Quel personnage est all√© √† Dresde ?\"</li><li>Aiguille (par d√©faut) : \"En fait, Yuki vit √† c√¥t√© du Semper Opera House.\"</li><li>Aiguille (invers√©e) : \"Le Semper Opera House est √† c√¥t√© de l'endroit o√π vit Yuki.\"</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Le <a href=\"https://en.wikipedia.org/wiki/Semperoper\">Semper Opera House</a> est √† Dresde, fournissant le contexte pour cette aiguille √† un saut.</div></div><p>Suivant la m√©thodologie du papier, nous g√©n√©rons ces groupes question-aiguille (comprenant une question, <strong>une aiguille √† un saut</strong>, et <strong>une aiguille √† un saut invers√©e</strong>) √† travers plusieurs cat√©gories, comme les exemples ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Question</th>\n<th>Original needle (for reference)</th>\n<th>One-hop needle</th>\n<th>Inverted one-hop needle</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dietary restrictions</td>\n<td>Which character cannot eat fish-based meals?</td>\n<td>Alice cannot eat fish-based meals.</td>\n<td>Then, Alice mentioned being vegan for years.</td>\n<td>Being vegan was important to Alice for years.</td>\n</tr>\n<tr>\n<td>Medical conditions</td>\n<td>Which character cannot drink milk?</td>\n<td>Bob can't drink milk.</td>\n<td>Bob explained he was lactose intolerant.</td>\n<td>Being lactose intolerant affected Bob daily.</td>\n</tr>\n<tr>\n<td>Language proficiency</td>\n<td>Which character speaks French?</td>\n<td>Charlie speaks French.</td>\n<td>Actually, Charlie studied at the Sorbonne.</td>\n<td>At the Sorbonne, Charlie completed his degree.</td>\n</tr>\n<tr>\n<td>Professional background</td>\n<td>Which character is a musician?</td>\n<td>Diane is a musician.</td>\n<td>In 2013, Diane conducted at the Sydney Opera House.</td>\n<td>The Sydney Opera House performance was conducted by Diane.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Les noms ci-dessus sont uniquement √† titre de r√©f√©rence. Dans les aiguilles r√©elles, ils sont tir√©s au hasard d'une liste de noms culturellement diversifi√©s.<br><br>Notez que les aiguilles originales (correspondances litt√©rales de mots-cl√©s) sont fournies √† titre de r√©f√©rence et ne sont pas utilis√©es dans nos exp√©riences.</div></div><h3 id=\"haystacks-construction\">Construction des Bottes de Foin</h3><p>Nous avons commenc√© avec dix livres du domaine public, contenant chacun au moins 50 000 tokens, en concat√©nant al√©atoirement de courts extraits (moins de 250 tokens) pour cr√©er des bottes de foin de diff√©rentes longueurs, √† savoir 128, 256, 512, 1024, 2048, 4096 et 8192 tokens. Nous avons ensuite int√©gr√© une aiguille dans chaque botte de foin :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"415\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png 896w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1 : Construction de la botte de foin √† partir de courts extraits de livres et d'une seule aiguille par botte de foin.</span></figcaption></figure><p>Pour un exemple plus concret, prenons l'aiguille \"En fait, Yuki vit √† c√¥t√© du Semper Opera House\" et pla√ßons-la dans une botte de foin de 128 tokens √† la position 50 :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"508\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/text2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/text2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2 : Un exemple d'aiguille dans une botte de foin.</span></figcaption></figure><p>En utilisant <code>jina-embeddings-v3</code> pour encoder les textes, le score de similarit√© entre le texte de l'aiguille et le texte de la botte de foin est :</p><pre><code class=\"language-bash\">Question-Haystack similarity = 0.2391\n</code></pre><p>Nous normalisons ensuite le score en divisant ce nombre par le score de similarit√© entre la question et l'aiguille par d√©faut (sans cr√©ation de botte de foin, juste une comparaison directe) :</p><pre><code class=\"language-bash\">Question-Needle similarity = 0.3598\nNormalized Query-Haystack similarity = 0.2391 / 0.3598 = 0.6644\n</code></pre><p>Cette normalisation est n√©cessaire car tous les mod√®les ne produisent pas les m√™mes scores de similarit√© entre deux textes, et <code>jina-embeddings-v3</code> a tendance √† sous-calculer la similarit√© entre deux textes.</p><p>Pour chaque aiguille (y compris toutes les versions par d√©faut et invers√©es), nous avons g√©n√©r√© dix bottes de foin par longueur de contexte, en int√©grant une aiguille par botte de foin √† un emplacement diff√©rent. Pour une aiguille et une longueur de contexte donn√©es, les bottes de foin ressembleraient √† ceci :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"290\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png 800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3 : Aiguilles plac√©es √† intervalles r√©guliers dans dix bottes de foin.</span></figcaption></figure><p>Comme contr√¥le, nous avons √©galement g√©n√©r√© une botte de foin pour chaque condition de test sans aucune aiguille. Au total, cela fait 3 234 bottes de foin. Nous avons encod√© chaque botte de foin avec <code>jina-embeddings-v3</code> (en utilisant le LoRA de correspondance de texte par d√©faut), puis pour chaque botte de foin nous l'avons tronqu√©e (si le total des tokens d√©passait 8 192, la limite pour</p><code>jina-embeddings-v3</code>) puis a encod√© la question correspondante.</p><h2 id=\"evaluation-metrics\">M√©triques d'√©valuation</h2><p>Notre cadre d'√©valuation utilise plusieurs m√©triques pour √©valuer la performance du mod√®le d'embedding √† travers diff√©rentes longueurs de contexte :</p><h3 id=\"primary-metrics\">M√©triques principales</h3><p><strong>Score de similarit√© normalis√©</strong><br>La m√©trique principale est un score de similarit√© normalis√© qui prend en compte √† la fois la similarit√© s√©mantique entre la question et l'ensemble du contexte (similarit√© question-haystack), et la similarit√© de r√©f√©rence entre la question et son needle par d√©faut correspondant (similarit√© question-needle). Cette normalisation garantit que la performance du mod√®le est √©valu√©e par rapport √† un point de r√©f√©rence significatif plut√¥t que par des scores de similarit√© absolus seuls. Le processus de normalisation implique le calcul du score de similarit√© cosinus direct entre les questions et leurs needles correspondants (notre r√©f√©rence), et la division de la similarit√© question-haystack par ce score de r√©f√©rence :<br></p><p>$\\text{Similarit√© Normalis√©e} = \\frac{\\cos{(q,h)}}{\\cos{(q,n)}}$</p><p><strong>Ratio comparatif au hasard</strong><br>Pour tout mod√®le d'embedding, les scores de similarit√© cosinus entre diff√©rentes paires requ√™te-document ne sont directement comparables que lorsque la requ√™te reste la m√™me. Par cons√©quent, au-del√† de l'utilisation des scores de similarit√© normalis√©s, nous mesurons √©galement la fr√©quence √† laquelle la question est plus similaire √† l'ensemble du haystack qu'√† un passage al√©atoire de m√™me longueur sans needle.</p><h3 id=\"secondary-metrics\">M√©triques secondaires</h3><p><strong>Analyse de s√©paration</strong><br>Cette m√©trique √©value la capacit√© du mod√®le √† distinguer entre le contenu pertinent et non pertinent. Elle inclut la <strong>s√©paration moyenne</strong>, qui repr√©sente la diff√©rence entre les exemples positifs (passages contenant la r√©ponse) et les exemples n√©gatifs (passages ne contenant pas la r√©ponse), et le <strong>score AUC (Area Under the Curve)</strong>, qui mesure la capacit√© de discrimination bas√©e sur l'aire sous la courbe ROC (Receiver Operating Characteristic).</p><p><strong>Effets de position</strong><br>Nous analysons comment le placement du needle affecte la performance √† travers le <strong>coefficient de corr√©lation</strong> entre la position et le score de similarit√©, la <strong>pente de r√©gression</strong> montrant le changement de performance √† travers les positions, et l'<strong>analyse de performance par bins de position</strong>.</p><h2 id=\"findings\">R√©sultats</h2><h3 id=\"degradation-of-similarity-score-and-correctness\">D√©gradation du score de similarit√© et de l'exactitude</h3><p>Nos r√©sultats montrent clairement que la performance se d√©grade √† mesure que la longueur du contexte augmente, avec le score de similarit√© moyen chutant de 0,37 √† 128 tokens √† 0,10 √† 8K tokens, suivant une tendance non lin√©aire avec une baisse marqu√©e entre 128 et 1K tokens.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4 : Performance normalis√©e vs longueur de contexte.</span></figcaption></figure><p>Dans la figure ci-dessous, nous d√©montrons que l'inversion du needle a peu d'impact sur le score de similarit√© normalis√©. Le needle par d√©faut (par exemple \"En fait, Yuki vit pr√®s du Semper Opera House\") et le needle invers√© (par exemple \"Le Semper Opera House est √† c√¥t√© de l'endroit o√π vit Yuki\") montrent des performances presque identiques :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5 : Performance ordre par d√©faut vs ordre invers√©.</span></figcaption></figure><p>Les diff√©rentes connexions s√©mantiques du dataset pr√©sentent des performances variables, les paires lieu-point de rep√®re maintenant les meilleurs r√©sultats, tandis que les connexions di√©t√©tiques et les conditions m√©dicales se d√©gradent plus rapidement :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6 : Performance normalis√©e par groupe vs longueur de contexte.</span></figcaption></figure><p>La comparaison des r√©sultats avec le hasard confirme nos conclusions, en montrant que plus le haystack est grand, plus les r√©sultats se rapprochent de l'al√©atoire, c'est-√†-dire que nous avons presque autant de chances de s√©lectionner un passage al√©atoire sans needle (r√©ponse correcte) que le haystack pour une question donn√©e :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-12.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7 : Performance du mod√®le vs hasard (0,5).</span></figcaption></figure><p>Encore une fois, nous observons des performances variables selon les diff√©rentes connexions s√©mantiques, certaines (comme les restrictions alimentaires) tombant bien en dessous du hasard m√™me avec des contextes relativement courts, tandis que d'autres (comme les lieux et points de rep√®re) affichent de bien meilleures performances quelle que soit la longueur du contexte :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 8 : Performance par groupe vs hasard.</span></figcaption></figure><p>L'inversion du needle a peu d'effet sur la performance. Dans le graphique ci-dessous, nous montrons le ratio comparatif de pr√©f√©rence du haystack correct par rapport au hasard, divis√© selon que le needle plac√© contenait la r√©ponse dans l'ordre par d√©faut ou invers√© : </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 9 : Ordre par d√©faut vs ordre invers√© - performance vs hasard.</span></figcaption></figure><p>Puisque nous pouvons voir que les r√©sultats pour les needles en ordre par d√©faut et invers√© suivent la m√™me tendance, nous ne continuerons pas l'analyse s√©par√©e concernant ce crit√®re.</p><h3 id=\"can-we-separate-positive-from-negative-results\">Pouvons-nous s√©parer les r√©sultats positifs des n√©gatifs ?</h3><p>L'une de nos conclusions les plus importantes provient de l'analyse de la capacit√© des mod√®les d'embedding √† distinguer le contenu pertinent du contenu non pertinent √† travers diff√©rentes longueurs de contexte. Cette \"analyse de s√©paration\" r√©v√®le que l'exactitude de la r√©cup√©ration chute rapidement entre une longueur de contexte de 128 et 1000 tokens, puis continue √† baisser, mais √† un rythme plus lent :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 10 : Analyse de s√©paration vs longueur de contexte.</span></figcaption></figure><p>Pour les contextes courts (128 tokens), le mod√®le montre une forte s√©paration avec une diff√©rence moyenne de 0,1 et une discrimination claire, atteignant un AUC de 0,81 (ce qui signifie que 81% du temps, le mod√®le classe un passage pertinent plus haut qu'un passage non pertinent). Cela indique que dans les contextes plus courts, le mod√®le peut distinguer de mani√®re fiable les passages qui contiennent la r√©ponse de ceux qui ne la contiennent pas.</p><p>Cependant, cette performance se d√©t√©riore rapidement √† mesure que la longueur du contexte augmente. √Ä 1 000 tokens, la s√©paration chute de 60 % √† 0,040, et l'AUC diminue √† 0,66, signalant une baisse notable des performances. √Ä 8 000 tokens, il y a une s√©paration minimale (0,001) et une discrimination proche de l'al√©atoire, avec une AUC de seulement 0,50. Ce sch√©ma r√©v√®le une observation cruciale : m√™me lorsque les mod√®les peuvent calculer des scores de similarit√© raisonnables dans des contextes plus longs, ils peuvent √† peine utiliser ces scores pour distinguer les informations pertinentes des informations non pertinentes. √Ä 8 000 tokens, la capacit√© du mod√®le √† diff√©rencier le contenu pertinent rel√®ve essentiellement du hasard.</p><p>La rapidit√© de cette d√©gradation lorsque le contexte s'allonge est frappante. Les scores de similarit√© bruts chutent d'environ 75 % entre 128 et 8 000 tokens, mais les m√©triques de s√©paration diminuent de pr√®s de 99 % sur la m√™me p√©riode. Plus pr√©occupant encore, l'ampleur de l'effet montre une baisse encore plus prononc√©e, chutant de 98,6 %. Cela sugg√®re que les difficult√©s des mod√®les d'embedding avec les longs contextes vont au-del√† de la simple r√©duction des scores de similarit√©‚Äîleur capacit√© fondamentale √† identifier les informations pertinentes se d√©grade beaucoup plus s√©v√®rement que ce qui √©tait compris auparavant.</p><h3 id=\"how-does-the-needle-position-affect-the-core-metrics\">Comment la Position de l'Aiguille Affecte-t-elle les M√©triques Principales ?</h3><p>Bien que les m√©triques de performance de base soient g√©n√©ralement meilleures lorsque l'aiguille est au d√©but de la botte de foin, la d√©gradation des performances n'est pas toujours corr√©l√©e au placement au milieu du contexte :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 11 : Performance par position relative selon les longueurs de contexte.</span></figcaption></figure><p>Nous constatons √©galement que les performances sont meilleures lorsque l'aiguille est au d√©but d'un contexte donn√©, et dans les contextes courts, nous observons une l√©g√®re am√©lioration des performances lorsque l'aiguille est plac√©e vers la fin. Cependant, dans tous les contextes, nous observons une baisse des performances lorsque l'aiguille est dans les positions m√©dianes :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 12 : Ratios comparatifs par position.</span></figcaption></figure><h2 id=\"what-effect-does-query-expansion-have-on-the-results\">Quel Est l'Effet de l'Expansion de Requ√™te sur les R√©sultats ?</h2><p>Nous avons r√©cemment publi√© un article de blog sur l'expansion de requ√™te, une technique utilis√©e dans les syst√®mes de recherche pour am√©liorer les performances de recherche en ajoutant des termes pertinents aux requ√™tes.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Query Expansion with LLMs: Searching Better by Saying More</div><div class=\"kg-bookmark-description\">Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/query-expansion-with-llms-searching-better-by-saying-more.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Dans cet article, nous avons utilis√© un LLM pour g√©n√©rer des termes d'expansion, qui ont ensuite √©t√© ajout√©s aux embeddings de requ√™te pour am√©liorer les performances de recherche. Les r√©sultats ont montr√© des am√©liorations significatives. Maintenant, nous voulons examiner comment (ou si) la technique am√©liorera les r√©sultats pour la recherche d'aiguille dans une botte de foin. Par exemple, √©tant donn√© une requ√™te :</p><pre><code class=\"language-bash\">Which character has been to Dresden?\n</code></pre><p>Nous utilisons un LLM (Gemini 2.0) pour l'√©tendre et ajouter 100 termes suppl√©mentaires qui ressemblent √† ceci :</p><pre><code class=\"language-bash\">Which character has been to Dresden? Character: fictional character literary character protagonist antagonist figure persona role dramatis personae\\\\n\\\\nDresden: Dresden Germany; bombing of Dresden World War II historical fiction Kurt Vonnegut Slaughterhouse-Five city in Saxony Elbe River cultural landmark\\\\n\\\\nHas been to: visited traveled to journeyed to presence in appears in features in set in takes place in location setting\n\n</code></pre><h3 id=\"how-much-does-query-expansion-help-match-the-needle-to-the-haystack\">Dans Quelle Mesure l'Expansion de Requ√™te Aide-t-elle √† Faire Correspondre l'Aiguille √† la Botte de Foin ?</h3><p>Pour notre exp√©rience, nous avons g√©n√©r√© trois ensembles de termes de requ√™te √©tendus (comme d√©crit dans l'<a href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\">article original</a>) - 100, 150 et 250 termes. Nous avons ensuite ex√©cut√© le m√™me ensemble d'exp√©riences qu'auparavant, r√©p√©t√©es trois fois, une fois pour chaque ensemble de termes de requ√™te √©tendus.</p><p>Les r√©sultats avec tous les ensembles d'expansion ont montr√© une d√©gradation claire √† mesure que la longueur du contexte augmentait, avec un effet similaire √† celui observ√© sans utiliser l'expansion de requ√™te (Figures 4 & 7) :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1071\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 13 : Performance normalis√©e combin√©e : toutes tailles d'expansion.</span></figcaption></figure><p>Par rapport aux requ√™tes non √©tendues, toutes les conditions d'expansion de requ√™te ont montr√© le m√™me sch√©ma de d√©gradation des performances √† mesure que le contexte s'allongeait. La tendance √† la d√©gradation est √©galement toujours non lin√©aire avec une forte baisse entre 128 et 1 000 tokens :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 14 : Ratio comparatif combin√© : toutes tailles d'expansion.</span></figcaption></figure><p>Cependant, l'examen du ratio comparatif montre que l'expansion de requ√™te pr√©sente des avantages clairs : le mod√®le est beaucoup plus susceptible de s√©lectionner la botte de foin contenant l'aiguille plut√¥t que celle qui n'en contient pas. En revanche, sans expansion de requ√™te, la probabilit√© de s√©lectionner le passage correct a tellement chut√© que, pour une botte de foin de 8 000 tokens, elle √©tait presque identique √† celle de choisir un passage au hasard.</p><h3 id=\"how-do-we-explain-needle-matching-results-with-query-expansion\">Comment Expliquons-nous les R√©sultats de Correspondance d'Aiguille avec l'Expansion de Requ√™te ?</h3><p>Ces r√©sultats s'alignent sur les conclusions de l'article NoLiMa et de la recherche sur l'expansion de requ√™te, et peuvent √™tre expliqu√©s comme suit :</p><ol><li><strong>Compromis qualit√© vs quantit√©</strong> : Les meilleures performances de l'expansion √† 100 termes, par rapport √† 150 et 250 termes, sugg√®rent qu'il existe un point optimal o√π les termes suppl√©mentaires commencent √† ajouter plus de bruit que de signal. L'expansion √† 250 termes introduit probablement des termes ayant des relations s√©mantiques plus faibles avec la requ√™te originale, qui deviennent contre-productifs dans les contextes plus longs.</li><li><strong>La longueur du contexte reste le d√©fi principal</strong> : Malgr√© les avantages de l'expansion de requ√™te, les performances se d√©gradent encore significativement avec l'augmentation de la longueur du contexte. Cela sugg√®re que m√™me avec l'expansion, la limitation architecturale fondamentale des mod√®les bas√©s sur l'attention dans les longs contextes persiste.</li><li><strong>Identification du seuil pratique</strong> : Le ratio comparatif restant au-dessus de 0,5 indique que l'expansion maintient des performances sup√©rieures au hasard m√™me √† 8 000 tokens, fournissant un moyen pratique d'√©tendre la <em>fen√™tre de contexte effective</em> pour les mod√®les d'embedding. La comparaison avec le hasard montre que, m√™me face √† des documents √† long contexte, l'expansion de la requ√™te rend plus probable de trouver la bonne r√©ponse (c'est-√†-dire l'aiguille) qu'une r√©ponse incorrecte. C'est une am√©lioration par rapport aux requ√™tes non √©tendues, o√π la chance de trouver la bonne r√©ponse se rapproche de l'al√©atoire √† mesure que la longueur du contexte augmente.</li></ol><h2 id=\"diagnosis-what-role-does-lexical-matching-play-in-embeddings\">Diagnostic : Quel R√¥le Joue la Correspondance Lexicale dans les Embeddings ?</h2><p>Dans les exp√©riences ci-dessus, nous avons mesur√© l'efficacit√© des mod√®les d'embedding pour faire des inf√©rences s√©mantiques \"√† un saut\" dans des passages √† long contexte, en √©liminant toute possibilit√© de correspondance litt√©rale. Nous avons constat√© que, m√™me avec l'expansion de requ√™te, la capacit√© du mod√®le d'embedding √† trouver des passages pertinents se d√©t√©riore √† mesure que la longueur du contexte augmente. Cet effet est significatif, et la d√©couverte est remarquable car nous nous attendrions normalement √† ce qu'un mod√®le d'embedding puisse faire les inf√©rences pertinentes sans aide suppl√©mentaire. Lorsque nous rempla√ßons les correspondances litt√©rales par des variations √† un saut (par exemple, \"Dresden\" ‚Üí \"Semper Opera House\"), nous ne faisons que remplacer un concept par un autre proche.</p><p>Prenons maintenant le taureau par les cornes et posons directement la question : La correspondance litt√©rale joue-t-elle vraiment un r√¥le suffisamment important dans la correspondance s√©mantique, ou l'effet de la longueur du contexte l'emporte-t-il ? Pour r√©pondre √† cette question, nous avons refait nos tests avec des aiguilles contenant des correspondances litt√©rales, par exemple :</p><ul><li>Question : \"Which character has been to Dresden?\"</li><li>Aiguille (par d√©faut) : \"Actually, Yuki lives in Dresden.\"</li><li>Aiguille (invers√©e) : \"Dresden is where Yuki lives.\"</li></ul><p>Notez que, au lieu d'une variation en une √©tape consistant √† d√©duire que l'op√©ra Semper se trouve √† Dresde, et donc qu'un personnage vivant √† proximit√© aurait d√ª √™tre celui qui a visit√© Dresde, ces indices indiquent directement le nom du personnage qui vit √† Dresde.</p><p>Apr√®s avoir reformul√© les 22 paires question-indice de cette mani√®re, nous avons relanc√© nos exp√©riences avec toutes les longueurs de contexte et les placements d'indices inclus, en utilisant le m√™me mod√®le d'embedding <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-22.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 15 : Performance normalis√©e vs longueur du contexte.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-23.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 16 : Performance du mod√®le vs hasard (0,5).</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-24.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-24.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-24.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 17 : Ratios comparatifs par position</span></figcaption></figure><p>Les r√©sultats sont frappants. M√™me avec des correspondances litt√©rales dans le contexte, la capacit√© du mod√®le √† distinguer la bonne r√©ponse d'une r√©ponse al√©atoire se d√©t√©riore rapidement √† mesure que la longueur du contexte augmente, tout en maintenant un l√©ger avantage par rapport √† une absence totale de correspondance litt√©rale.</p><p>Cela prouve finalement que la capacit√© d'un mod√®le d'embedding √† trouver une aiguille dans une botte de foin est beaucoup plus affect√©e par la taille de la botte de foin (et le placement de l'aiguille dans celle-ci) que par la formulation s√©mantique de l'aiguille.</p><h2 id=\"conclusion\">Conclusion</h2><p>Nos d√©couvertes avec les mod√®les d'embedding s'alignent avec l'article NoLiMA sur les LLM : la taille du contexte est hautement d√©terminante pour la correspondance et la r√©cup√©ration correctes. Nous montrons que cela est vrai m√™me lorsqu'il y a une correspondance exacte lettre par lettre.</p><p>Le probl√®me n'est pas la capacit√© d'un embedding √† effectuer une correspondance s√©mantique. Les mod√®les d'embedding comme <code>jina-embeddings-v3</code> g√®rent bien les contextes courts, mais leur efficacit√© diminue √† mesure que la longueur du contexte augmente. L'expansion des requ√™tes peut r√©duire cet effet dans une certaine mesure, mais la qualit√© de la r√©cup√©ration se d√©grade toujours sur des contextes plus longs. De plus, l'expansion des requ√™tes pose des probl√®mes suppl√©mentaires, car il est crucial d'identifier les termes d'expansion qui am√©liorent la r√©cup√©ration sans ajouter de bruit s√©mantique. Nous √©tudions et examinons des moyens d'aborder directement la r√©cup√©ration d'une aiguille dans une botte de foin et d'am√©liorer les performances futures de <code>jina-embeddings-v4</code>.</p>",
  "comment_id": "67c868baf1c5780001164330",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-05T16:07:38.000+01:00",
  "updated_at": "2025-03-07T03:56:34.000+01:00",
  "published_at": "2025-03-07T03:56:34.000+01:00",
  "custom_excerpt": "We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/long-context-embedding-models-are-blind-beyond-4k-tokens/",
  "excerpt": "Nous avons √©tudi√© les mod√®les d'embedding sur de nouvelles t√¢ches \"d'aiguille dans une botte de foin\" et constat√© qu'au-del√† de 4K tokens, ils ne font que tirer au hasard - m√™me avec des correspondances lexicales exactes ou l'expansion des requ√™tes, ils ne peuvent pas distinguer le signal du bruit dans un contexte long.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
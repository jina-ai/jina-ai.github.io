{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "Notation équitable pour les documents multimodaux avec jina-reranker-m0",
  "html": "<p>Imaginez que vous construisez un système de recherche d'actualités sportives. Un utilisateur recherche \"joueurs de tennis célébrant une victoire en championnat\" et vous devez trouver les articles les plus pertinents dans votre base de données. Chaque article contient à la fois une légende textuelle et une image - ce qui est typique de la couverture sportive moderne.</p><p>Votre système doit prendre une <strong>requête textuelle</strong> et renvoyer une <strong>liste classée des documents multimodaux les plus pertinents</strong> de votre corpus. Cela semble simple, mais il y a un problème fondamental qui ruine toutes les approches évidentes.</p><p>Voici ce qui se passe lorsque vous essayez de classer ces documents. Votre modèle d'向量模型 (Embeddings), disons <code>jina-clip-v2</code>, produit des scores de similarité comme ceci :</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>Article</th>\n            <th>Type de contenu</th>\n            <th>Description</th>\n            <th>Score de similarité</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>Texte</td>\n            <td>Novak Djokovic remporte la finale de l'Open d'Australie en trois sets</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>Image</td>\n            <td>[photo d'un joueur tenant un trophée et souriant]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Texte</td>\n            <td>Les retards météorologiques affectent la programmation des tournois en extérieur</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Image</td>\n            <td>[photo de joueurs de tennis sautant et célébrant]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Quel article est le plus pertinent ? L'article A a un score de texte élevé mais un score d'image faible. L'article B a un score de texte faible mais un score d'image plus élevé. Le défi fondamental est que <strong>vous ne pouvez pas comparer 0,72 (texte) avec 0,54 (image)</strong> car ces scores de similarité existent sur des échelles complètement différentes.</p><h2 id=\"when-trivial-solutions-fail\">Quand les solutions triviales échouent</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>En raison de l'écart de modalité</strong> dans <code>jina-clip-v2</code> ou dans presque tous les autres modèles de type CLIP, toute approche évidente que vous pourriez essayer ne fonctionne pas. Si vous utilisez simplement le score le plus élevé, vous vous heurtez au fait que les scores de texte se regroupent autour de 0,2 à 0,8, tandis que les scores d'image se regroupent autour de 0,4 à 0,6. Cela signifie qu'une correspondance textuelle médiocre (0,6) battra toujours une excellente correspondance d'image (0,5).</p><p>La moyenne des scores n'aide pas non plus. Calculer (0,7 + 0,3) / 2 = 0,5 vous donne un nombre, mais que signifie-t-il réellement ? Vous faites la moyenne de quantités fondamentalement dénuées de sens. De même, tout schéma de pondération fixe est arbitraire - parfois le texte est plus important, parfois les images le sont, et cela dépend entièrement de la requête et du document spécifiques.</p><p>Même la normalisation préalable des scores ne résout pas le problème central. Vous essayez toujours de combiner des mesures de similarité fondamentalement différentes qui capturent différents aspects de la pertinence.</p><h2 id=\"what-actually-happens\">Ce qui se passe réellement</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Pour avoir une meilleure idée de ce avec quoi nous travaillons, voici un exemple de document de l'<a href=\"https://arxiv.org/abs/2305.13631\">ensemble de données EDIS</a>, montrant l'image (un match de football allemand) et la légende (<code>One More Field Where the Content Trails Germany</code>).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1 : Exemple de document multimodal contenant à la fois du contenu image et texte. Puisque nous avons deux modalités, pour une requête donnée, il existe maintenant </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">deux</em></i><span style=\"white-space: pre-wrap;\"> écarts sémantiques (entre la requête et le texte, et la requête et l'image). Pour obtenir les meilleurs résultats, devons-nous rechercher le contenu textuel des documents ou le contenu de l'image ?</span></figcaption></figure><p>Globalement, <code>jina-clip-v2</code> montre des similarités beaucoup plus élevées lors de la comparaison requête-texte que requête-image dans l'ensemble de données EDIS, en partie à cause de la façon dont le modèle a été entraîné et en partie à cause de l'ensemble de données lui-même :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2 : Scores de similarité entre requête-image (en rouge) et requête-texte (en bleu) en utilisant </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Par conséquent, il semble logique de récupérer un document en fonction de son texte plutôt que de son image. Et, comme nous pouvons le voir dans le graphique ci-dessous, nous obtenons de bien meilleurs résultats en comparant la requête textuelle <code>... for undocumented immigrants helping to establish legal status in the United States</code> au contenu textuel du corpus. En fait, la recherche par image ne parvient pas du tout à récupérer le document de vérité de terrain (mis en évidence en jaune) :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3 : Exemple où le document de vérité de terrain (mis en évidence avec une bordure jaune) ne peut être récupéré que via la récupération requête-texte de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> lors de l'utilisation de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 3.</span></figcaption></figure><p>Mais ne vous y trompez pas. Malgré le fait que la requête-texte affiche des scores de similarité plus élevés, les scores de similarité requête-texte et requête-image ne sont <em>pas</em> comparables. Nous pouvons le constater en examinant recall@10 lorsque nous utilisons <code>jina-clip-v2</code> pour récupérer 32 documents de l'ensemble de données EDIS. Il est clair que le rappel est plus élevé avec requête-<em>image</em> :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Requête-texte</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Requête-image</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Nous pouvons le voir ci-dessous : Si nous utilisons une requête de l'ensemble de données, <code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>, nous ne pouvons récupérer le document de vérité de terrain que par son contenu image. La recherche par son contenu textuel ne renvoie aucune correspondance :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4 : Exemple où le document de vérité de terrain (mis en évidence avec une bordure jaune) ne peut être récupéré que via la récupération requête-image de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> lors de l'utilisation de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 3.</span></figcaption></figure><p>Donc, si les scores de similarité impliquent que nous devrions récupérer les documents à partir de leur texte, et que le rappel implique que nous devrions les récupérer à partir de leurs images, lequel devrions-nous choisir ? Les figures 3 et 4 ne suggèrent certainement pas de vainqueur absolu. Quelle modalité présente <em>réellement</em> la correspondance la plus étroite entre notre requête et le document que nous recherchons ? Et si nous voulons fusionner les candidats de la récupération requête-texte et requête-image, comment pouvons-nous sélectionner de manière significative les meilleures correspondances si nous ne pouvons même pas comparer les scores ? Il est clair que l'utilisation de <code>jina-clip-v2</code> ne suffit pas. Nous devons ajouter un autre modèle au mélange.</p><h2 id=\"a-simple-two-stage-pipeline\">Un pipeline simple en deux étapes</h2><p>En avril 2025, nous avons publié <code>jina-reranker-m0</code>, un re-classeur (Reranker) multilingue et multimodal pour la récupération de documents visuels. Nous pouvons voir son écart de modalité plus étroit ci-dessous, où <code>jina-reranker-m0</code> affiche des scores de similarité requête-texte et requête-image comparables, contrairement à l'écart beaucoup plus important affiché par <code>jina-clip-v2</code> :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6 : Comparé à </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> montre beaucoup moins de différence entre les scores de similarité requête-image (rouge) et requête-texte (bleu).</span></figcaption></figure><p>Dans cette optique, nous pouvons utiliser <code>jina-reranker-m0</code> pour une deuxième passe dans la chaîne de récupération, après que les résultats initiaux ont été récupérés à partir de <code>jina-clip-v2</code> :</p><p><strong>Étape 1 : Récupérer les candidats des deux modalités</strong></p><ul><li>Utiliser <code>jina-clip-v2</code> pour obtenir 16 documents via la recherche de texte + 16 via la recherche d'images</li><li>Accepter que nous ne pouvons pas encore comparer les scores</li></ul><p><strong>Étape 2 : Re-classement (Reranker) unifié</strong></p><ul><li>Injecter chaque paire (requête + document complet) dans <code>jina-reranker-m0</code></li><li>Le re-classeur (Reranker) traite à la fois le texte ET l'image ensemble</li><li>Sortie : Score de pertinence unique sur une échelle unifiée</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5 : Indexation de documents multimodaux et processus de récupération multimodal en deux étapes avec </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> et </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Nous avons étendu les expériences du tableau 1, en utilisant maintenant <code>jina-clip-v2</code> pour récupérer les documents du corpus, puis <code>jina-reranker-m0</code> pour les re-classer (Reranker) :</p><ol><li>Récupérer 32 documents via requête-texte, puis re-classer (Reranker) en fonction du score requête-texte.</li><li>Récupérer 32 documents via requête-image, puis re-classer (Reranker) en fonction du score requête-image.</li><li>Récupérer 16 documents via requête-texte et 16 via requête-image. Re-classer (Reranker) en fonction du score requête-texte ou requête-image, selon la modalité de la requête.</li><li>Récupérer 16 documents via requête-texte et 16 via requête-image. Re-classer (Reranker) en fonction des scores moyens requête-texte et requête-image de chaque document, ce qui donne un score final de (requête-texte + requête-image)/2.</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Notez que nous mesurons les performances zéro-shot sur EDIS. Nous n'avons pas affiné <code>jina-clip-v2</code> ni <code>jina-reranker-m0</code> à l'aide de l'ensemble de données.</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Description</th>\n      <th>Recall@10 - with jina-clip-v2</th>\n      <th>Recall@10 - with jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 docs: query-to-text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 docs: query-to-image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image<br>Combined average reranker scores</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Les expériences 1, 3 et 4 montrent toutes le même résultat pour recall@10 avec <code>jina-clip-v2</code> en raison des scores requête-texte plus élevés que les scores requête-image. Par conséquent, les dix premiers résultats sont dominés par les documents récupérés via le texte.</div></div><p>Comme nous pouvons le constater, en effectuant une deuxième passe avec <code>jina-reranker-m0</code>, le rappel augmente globalement, quelle que soit la modalité. Cependant, <strong>nous constatons l'augmentation la plus importante lorsque nous combinons le contenu textuel et image des documents récupérés</strong>, atteignant un recall@10 de 36,24. Un exemple visuel montre que <code>jina-reranker-m0</code> classe systématiquement le document de vérité terrain en premier, que la recherche porte sur du texte ou du contenu image :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7 : Exemples de requêtes (à gauche) et </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 1 résultat pour chaque méthodologie de re-classement (Reranker) (quatre colonnes à droite), montrant que la combinaison des scores de similarité image et texte classe systématiquement le document de vérité terrain en premier.</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Alors que les figures 3 et 4 montrent un <code>top_k</code> de 3 pour les différentes méthodes de récupération, pour des raisons d'espace, la figure 7 ne montre qu'un <code>top_k</code> de 1 pour chaque requête.</div></div><h2 id=\"conclusions\">Conclusions</h2><p>Cette approche simple en deux étapes offre une amélioration de 62 % du rappel, car le système exploite enfin ce que les humains font naturellement : considérer à la fois ce que nous lisons et ce que nous voyons pour déterminer la pertinence. La leçon s'étend au-delà de la recherche : lorsque vous travaillez avec des systèmes d'IA multimodaux, les approches à une seule passe qui traitent les modalités séparément se heurteront toujours à ce mur d'incompatibilité de score. Les architectures en deux étapes qui récupèrent largement puis classent intelligemment deviennent essentielles. Essayez <code>jina-reranker-m0</code> via notre API ou sur AWS, GCP et Azure.</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "Similarité textuelle : 0,7. Similarité d’image : 0,5. Quel document est le plus pertinent ? Impossible de le dire, et c’est là le problème fondamental qui entrave la recherche multimodale. Nous le résolvons avec le réordonnancement unifié (unified reranking).",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
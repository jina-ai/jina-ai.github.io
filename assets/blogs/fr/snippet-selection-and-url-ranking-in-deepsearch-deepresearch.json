{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "S√©lection d'extraits et classement des URL dans DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Guide pratique pour impl√©menter DeepSearch/DeepResearch</div><div class=\"kg-bookmark-description\">QPS dehors, profondeur dedans. DeepSearch est la nouvelle norme. Trouvez des r√©ponses √† travers des boucles de lecture-recherche-raisonnement. D√©couvrez ce que c'est et comment le construire.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Si vous avez d√©j√† lu notre guide d'impl√©mentation DeepSearch/DeepResearch, plongeons plus profond√©ment dans certains d√©tails qui peuvent <em>grandement</em> am√©liorer la qualit√©. Dans cet article, nous nous concentrerons sur deux d√©fis cl√©s : <strong>l'utilisation des embeddings pour la s√©lection d'extraits de pages web longues</strong> et <strong>l'utilisation de rerankers pour prioriser les URLs √† crawler.</strong></p><p>Certains se souviendront de notre conclusion pr√©c√©dente indiquant que \"les embeddings n'√©taient utiles que pour la d√©duplication de requ√™tes comme les t√¢ches STS (similarit√© textuelle s√©mantique), tandis que les rerankers ne faisaient m√™me pas partie de notre impl√©mentation originale de DeepSearch.\" Il s'av√®re que les deux sont toujours tr√®s utiles - juste pas de la mani√®re conventionnelle √† laquelle on pourrait s'attendre. Nous avons toujours suivi la voie la plus <em>simple</em> possible. Nous n'ajoutons pas de composants juste pour justifier leur existence ou notre valeur en tant que fournisseur d'embeddings et de rerankers. <strong>Nous sommes pragmatiques - sur ce dont la recherche a r√©ellement besoin comme fondation.</strong></p><p>Ainsi, apr√®s des semaines d'exp√©riences et d'it√©rations, nous avons d√©couvert des utilisations peu communes mais efficaces pour les deux dans les syst√®mes DeepSearch/DeepResearch. En les appliquant, nous avons significativement am√©lior√© la qualit√© de <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> (n'h√©sitez pas √† l'essayer). Nous aimerions partager ces insights avec nos coll√®gues praticiens travaillant dans ce domaine.</p><h2 id=\"select-snippet-from-long-content\">S√©lectionner des extraits de contenus longs</h2><p>Le probl√®me est le suivant : apr√®s <a href=\"https://jina.ai/reader\">avoir utilis√© Jina Reader pour lire le contenu d'une page web</a>, nous devons l'ajouter comme √©l√©ment de connaissance au contexte de l'agent pour le raisonnement. Bien que d√©verser tout le contenu dans la fen√™tre de contexte du LLM soit la fa√ßon la plus simple, ce n'est pas optimal en consid√©rant les co√ªts en tokens et la vitesse de g√©n√©ration. En pratique, nous devons identifier quelles parties du contenu sont les plus pertinentes pour la question et ajouter s√©lectivement uniquement ces parties comme connaissance au contexte de l'agent.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Nous parlons des cas o√π le contenu reste trop long m√™me apr√®s le nettoyage markdown de Jina Reader. Cela arrive souvent avec les longues pages comme les issues GitHub, les discussions Reddit, les fils de forum et les articles de blog (y compris beaucoup des n√¥tres sur jina.ai/news).</div></div><p>Le filtrage bas√© sur LLM a les m√™mes probl√®mes de co√ªt et de latence, alors cherchons des solutions avec des mod√®les plus petits : nous avons besoin de mod√®les plus petits et moins co√ªteux, <strong>mais toujours multilingues</strong> ‚Äì un facteur crucial puisque nous ne pouvons pas garantir que la requ√™te ou les documents seront toujours en anglais.</p><p>Nous avons une question d'un c√¥t√© (soit la requ√™te originale soit une question de gap) et un grand contenu markdown de l'autre c√¥t√©, o√π la plupart du contenu est non pertinent. Nous devons s√©lectionner les extraits les plus pertinents pour la requ√™te. Cela ressemble au probl√®me de chunking auquel la communaut√© RAG est confront√©e depuis 2023 - r√©cup√©rer uniquement les chunks pertinents en utilisant des mod√®les de retrieval √† placer dans la fen√™tre de contexte pour la synth√®se. Cependant, il y a deux diff√©rences cl√©s dans notre cas :</p><ol><li>Chunks limit√©s d'un nombre limit√© de documents. Si chaque chunk contient environ 500 tokens, alors un document web long typique a environ 200 000 tokens (p50) √† 1 000 000 tokens (p99), et nous utilisons Jina Reader pour r√©cup√©rer 4-5 URLs √† chaque √©tape, cela donnerait environ des centaines de chunks - signifiant des centaines de vecteurs d'embedding et des centaines de similarit√©s cosinus. C'est facilement g√©rable avec JavaScript en m√©moire sans base de donn√©es vectorielle.</li><li>Nous avons besoin de chunks cons√©cutifs pour former des extraits de connaissance efficaces. Nous ne pouvons pas accepter des extraits combinant des phrases dispers√©es comme <code>[1-2, 6-7, 9, 14, 17, ...].</code> Un extrait de connaissance plus utile suivrait des motifs comme <code>[3-15, 17-24, ...]</code> - maintenant toujours un texte cons√©cutif. Cela facilite la copie et la citation par le LLM depuis la source de connaissance et r√©duit l'hallucination.</li></ol><p>Le reste concerne tous les probl√®mes dont les praticiens se plaignent : chaque chunk ne peut pas √™tre trop long puisque les mod√®les d'embedding ne peuvent pas bien g√©rer les longs contextes ; le chunking introduit une perte de contexte et rend les embeddings de chunks i.i.d ; et comment m√™me trouver les meilleurs indices de fronti√®re qui maintiennent √† la fois la lisibilit√© et la s√©mantique ? Si vous savez de quoi nous parlons, alors vous avez probablement √©t√© hant√© par ces probl√®mes dans vos impl√©mentations RAG.</p><p>Mais pour faire court - <strong>le late-chunking avec <code>jina-embeddings-v3</code> r√©sout magnifiquement les trois probl√®mes.</strong> Le late chunking maintient l'info de contexte pour chaque chunk, est <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">insensible aux indices de fronti√®re</a>, et <code>jina-embeddings-v3</code> lui-m√™me est SOTA dans les t√¢ches de recherche multilingue <em>asym√©trique</em>. Les lecteurs int√©ress√©s peuvent suivre nos articles de blog ou papers pour les d√©tails, mais voici l'impl√©mentation globale.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-card-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">Ce diagramme illustre l'algorithme de s√©lection d'extraits, qui fonctionne de mani√®re similaire √† </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\">. Le processus commence par diviser un long document en chunks de longueur fixe, qui sont ensuite int√©gr√©s avec </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> avec l'option late-chunking activ√©e. Apr√®s avoir calcul√© les scores de similarit√© entre chaque chunk et la requ√™te, une fen√™tre glissante se d√©place √† travers les scores de similarit√© pour trouver la fen√™tre avec la valeur moyenne la plus √©lev√©e.</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ce qu'est r√©ellement le Late Chunking et ce qu'il n'est pas : Partie II</div><div class=\"kg-bookmark-description\">Partie 2 de notre exploration du Late Chunking, une plong√©e profonde dans les raisons pour lesquelles c'est la meilleure m√©thode pour les embeddings de chunks et l'am√©lioration des performances de recherche/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3 : Embeddings multilingues avec Task LoRA</div><div class=\"kg-bookmark-description\">Nous pr√©sentons jina-embeddings-v3, un nouveau mod√®le d'embedding de texte avec 570 millions de param√®tres, qui atteint des performances √©tat de l'art sur les donn√©es multilingues et les t√¢ches de recherche en contexte long, supportant des longueurs de contexte jusqu'√† 8192 tokens. Le mod√®le inclut un ensemble d'adaptateurs Low-Rank Adaptation (LoRA) sp√©cifiques aux t√¢ches pour g√©n√©rer des embeddings de haute qualit√© pour la recherche requ√™te-document, le clustering, la classification et la correspondance de texte. L'√©valuation sur le benchmark MTEB montre que jina-embeddings-v3 surpasse les derniers embeddings propri√©taires d'OpenAI et Cohere sur les t√¢ches en anglais, tout en obtenant des performances sup√©rieures par rapport √† multilingual-e5-large-instruct sur toutes les t√¢ches multilingues. Avec une dimension de sortie par d√©faut de 1024, les utilisateurs peuvent r√©duire de mani√®re flexible les dimensions d'embedding jusqu'√† 32 sans compromettre les performances, gr√¢ce √† l'apprentissage de repr√©sentation Matryoshka.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking : Embeddings de chunks contextuels utilisant des mod√®les d'embedding √† contexte long</div><div class=\"kg-bookmark-description\">De nombreux cas d'utilisation n√©cessitent de r√©cup√©rer de plus petites portions de texte, et les syst√®mes de recherche bas√©s sur des vecteurs denses fonctionnent souvent mieux avec des segments de texte plus courts, car la s√©mantique est moins susceptible d'√™tre sur-compress√©e dans les embeddings. Par cons√©quent, les praticiens divisent souvent les documents textuels en plus petits chunks et les encodent s√©par√©ment. Cependant, les embeddings de chunks cr√©√©s de cette mani√®re peuvent perdre des informations contextuelles des chunks environnants, r√©sultant en des repr√©sentations sous-optimales. Dans cet article, nous introduisons une nouvelle m√©thode appel√©e late chunking, qui exploite des mod√®les d'embedding √† contexte long pour d'abord int√©grer tous les tokens du texte long, avec le chunking appliqu√© apr√®s le mod√®le transformer et juste avant le mean pooling - d'o√π le terme late dans son nom. Les embeddings de chunks r√©sultants capturent l'information contextuelle compl√®te, conduisant √† des r√©sultats sup√©rieurs √† travers diverses t√¢ches de recherche. La m√©thode est suffisamment g√©n√©rique pour √™tre appliqu√©e √† une large gamme de mod√®les d'embedding √† contexte long et fonctionne sans entra√Ænement suppl√©mentaire. Pour augmenter davantage l'efficacit√© du late chunking, nous proposons une approche de fine-tuning d√©di√©e pour les mod√®les d'embedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Utilisation du d√©coupage tardif et du pooling moyen de type Conv1D pour s√©lectionner le meilleur extrait par rapport √† la question.</span></p></figcaption></figure><p>Assurez-vous d'appeler l'API Jina Embeddings avec le param√®tre <code>task</code> de retrieval, <code>late_chunking</code> et <code>truncate</code> d√©finis comme ci-dessous :</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Pour l'embedding de la question, assurez-vous de changer </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> en </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> et de d√©sactiver </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code></p></figcaption></figure><p>L'impl√©mentation compl√®te est disponible sur Github :</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">Classement des URLs pour la prochaine lecture</h2><p>Le probl√®me est le suivant : pendant une session DeepSearch, vous allez probablement collecter beaucoup d'URLs √† partir des pages de r√©sultats des moteurs de recherche (SERP) et en d√©couvrir encore plus √† chaque fois que vous lisez des pages web individuelles (ces liens sur la page). Le nombre total d'URLs uniques peut facilement atteindre les centaines. Encore une fois, simplement d√©verser toutes les URLs directement dans le contexte du LLM est inefficace - cela gaspille un espace pr√©cieux dans la fen√™tre de contexte et, plus probl√©matique encore, <strong>nous avons constat√© que les LLMs choisissent essentiellement les URLs au hasard.</strong> Il est crucial de guider le LLM vers les URLs qui ont la plus grande probabilit√© de contenir la r√©ponse dont vous avez besoin.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Meilleure option pour utiliser Jina Reader pour explorer une page dans DeepSearch. Cela collectera tous les liens de la page dans un champ </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> s√©par√©, et les supprimera du champ </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\">.</span></p></figcaption></figure><p>Consid√©rez ce probl√®me comme un PageRank en contexte o√π nous devons pond√©rer des centaines d'URLs pendant une session. Nous classons les URLs en fonction de multiples facteurs qui combinent la derni√®re mise √† jour, la fr√©quence du domaine, la structure du chemin et, plus important encore, la pertinence s√©mantique par rapport √† la requ√™te pour cr√©er un score composite. Rappelez-vous que nous ne pouvons utiliser que les informations disponibles <em>avant</em> de visiter r√©ellement l'URL :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>Signaux de fr√©quence</strong> : Les URLs qui apparaissent plusieurs fois √† travers diff√©rentes sources re√ßoivent un poids suppl√©mentaire. Les URLs provenant de domaines qui apparaissent fr√©quemment dans les r√©sultats de recherche re√ßoivent un boost, car les domaines populaires contiennent souvent du contenu faisant autorit√©.</p><p><strong>Structure du chemin</strong> : Nous analysons les chemins des URLs pour identifier les clusters de contenu. Les URLs au sein de hi√©rarchies de chemins communes re√ßoivent des scores plus √©lev√©s, avec un facteur de d√©croissance appliqu√© aux chemins plus profonds.</p><p><strong>Pertinence s√©mantique</strong> : Nous utilisons <code>jina-reranker-v2-base-multilingual</code> pour √©valuer la pertinence s√©mantique entre la question et les informations textuelles de chaque URL, ce qui est <a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">un probl√®me classique de reclassement</a>. Les informations textuelles de chaque URL proviennent de :</p><ul><li>Titre et extraits des r√©sultats de l'API SERP (<code>https://s.jina.ai/</code> avec <code>'X-Respond-With': 'no-content'</code>)</li><li>Texte d'ancrage des URLs de la page (<code>https://r.jina.ai</code> avec <code>'X-With-Links-Summary': 'all'</code>)</li></ul><p><strong>Derni√®re mise √† jour</strong> : Certaines requ√™tes DeepSearch sont sensibles au temps, donc les URLs r√©cemment mises √† jour sont plus pr√©cieuses que les anciennes. Sans √™tre un moteur de recherche majeur comme Google, d√©terminer de mani√®re fiable la derni√®re date de mise √† jour est un d√©fi. Nous avons impl√©ment√© une approche multicouche qui combine les signaux suivants et fournit un horodatage avec score de confiance qui priorise le contenu plus r√©cent lorsque n√©cessaire.</p><ul><li>Filtres API SERP (comme le param√®tre <code>tbs</code> de s.jina.ai pour filtrer par r√©cence)</li><li>Analyse des en-t√™tes HTTP (Last-Modified, ETag)</li><li>Extraction de m√©tadonn√©es (meta tags, horodatages Schema.org)</li><li>Reconnaissance de motifs de contenu (dates visibles en HTML)</li><li>Indicateurs sp√©cifiques aux CMS pour les plateformes comme WordPress, Drupal et Ghost</li></ul><p><strong>Contenu restreint :</strong> Certains contenus sur les plateformes de m√©dias sociaux sont restreints ou simplement derri√®re des paywalls, et sans connexion ou violation de leurs CGU, il n'y a pas de moyen l√©gitime d'acc√©der √† ce contenu. Nous devrions activement maintenir une liste d'URLs et de noms d'h√¥tes probl√©matiques pour r√©duire leurs classements, √©vitant ainsi de perdre du temps sur du contenu inaccessible.</p><p><strong>Diversit√© des domaines :</strong> Dans certains cas, les URLs les mieux pond√©r√©es proviennent toutes des m√™mes noms d'h√¥tes, ce qui peut pi√©ger DeepSearch dans un optimum local et r√©duire la qualit√© finale des r√©sultats. Regardez les exemples ci-dessus o√π toutes les URLs principales proviennent de StackOverflow. Pour am√©liorer la diversit√©, nous pouvons impl√©menter une approche d'exploration-exploitation en s√©lectionnant les k URLs les mieux class√©es de chaque nom d'h√¥te.</p><p>L'impl√©mentation compl√®te du classement des URLs est disponible sur notre Github.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Faceü§ó Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset ¬∑ Issue #7175 ¬∑ huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">N'oubliez pas d'inclure les poids des URL dans le contexte de l'agent et d'instruire les LLM √† respecter ces poids.</span></p></figcaption></figure><h2 id=\"conclusion\">Conclusion</h2><p>Depuis la sortie de notre syst√®me DeepSearch le 2 f√©vrier 2025, nous avons d√©couvert deux d√©tails d'impl√©mentation qui ont consid√©rablement am√©lior√© la qualit√©. Fait remarquable, les deux utilisent des embeddings et des rerankers multilingues de mani√®re ¬´ in-context ¬ª - op√©rant √† une √©chelle beaucoup plus petite que les index pr√©-calcul√©s que ces mod√®les n√©cessitent habituellement. Cela explique notre oubli initial.</p><p>Cela indique une fascinante polarisation dans l'avenir de la technologie de recherche. Consid√©rons un cadre analogue √† la th√©orie du double processus de Kahneman :</p><ul><li>Pens√©e rapide (grep, BM25, SQL) : Mise en correspondance de motifs rapide et r√©gie par des r√®gles avec des exigences de calcul minimales.</li><li>Pens√©e lente (LLM) : Raisonnement complet avec une compr√©hension contextuelle approfondie, n√©cessitant des calculs importants.</li><li>Pens√©e interm√©diaire (embeddings, rerankers) : Pris dans les limbes ? Trop ¬´ avanc√© ¬ª/s√©mantique pour une simple correspondance de motifs mais manquant de v√©ritables capacit√©s de raisonnement.</li></ul><p>Nous assistons peut-√™tre √† la popularit√© d'une architecture bifurqu√©e o√π SQL/BM25 l√©ger et efficace g√®re la r√©cup√©ration initiale du contenu, alimentant directement des LLM puissants pour un traitement approfondi. Ces LLM int√®grent de plus en plus les fonctions s√©mantiques qui n√©cessitaient auparavant des mod√®les sp√©cialis√©s de niveau interm√©diaire. Le r√¥le restant pour les mod√®les de pens√©e interm√©diaire se d√©place vers des t√¢ches in-context sp√©cialis√©es : filtrage, d√©duplication et op√©rations √† port√©e limit√©e o√π un raisonnement complet serait inefficace.</p><p>N√©anmoins, la s√©lection d'extraits critiques et le classement des URL restent des composants fondamentaux ayant un impact direct sur la qualit√© du syst√®me DeepSearch/DeepResearch. Nous esp√©rons que nos observations susciteront des am√©liorations dans vos propres impl√©mentations.</p><p>L'expansion des requ√™tes continue d'√™tre un autre d√©terminant crucial de la qualit√©. Nous √©valuons activement plusieurs approches, allant des r√©√©critures basiques bas√©es sur les prompts aux petits mod√®les de langage et aux m√©thodes bas√©es sur le raisonnement. Attendez-vous √† nos prochaines d√©couvertes sur ce front bient√¥t. Restez √† l'√©coute.</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "Ces deux d√©tails transforment votre DeepSearch de moyen √† exceptionnel : la s√©lection des meilleurs extraits des pages web volumineuses et le classement des URLs avant le crawling.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
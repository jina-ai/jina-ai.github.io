{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "Alignement global contrastif texte-image et alignement local token-patch",
  "html": "<p>Lors d'expérimentations avec des modèles de type <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali</a>, l'un de nos ingénieurs a créé une visualisation en utilisant notre modèle <code>jina-clip-v2</code> récemment publié. Il a cartographié la similarité entre les embeddings de tokens et les embeddings de patchs pour des paires image-texte données, créant des superpositions de cartes de chaleur qui ont produit des aperçus visuels intrigants.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Malheureusement, <strong>il ne s'agit que d'une visualisation heuristique</strong> - pas d'un mécanisme explicite ou garanti. Bien que l'alignement contrastif global de type CLIP puisse (et souvent) créer <em>incidemment</em> des alignements locaux approximatifs entre les patchs et les tokens, il s'agit d'un <strong>effet secondaire non intentionnel</strong> plutôt que d'un objectif délibéré du modèle. Laissez-moi expliquer pourquoi.</p><h2 id=\"understand-the-code\">Comprendre le Code</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Examinons ce que fait le code à haut niveau. Notez que <code>jina-clip-v2</code> n'expose en réalité aucune API pour accéder aux embeddings au niveau des tokens ou des patchs par défaut - cette visualisation a nécessité quelques modifications post-hoc pour fonctionner.</p><p><strong>Calcul des embeddings au niveau des mots</strong></p><p>En définissant <code>model.text_model.output_tokens = True</code>, l'appel à <code>text_model(x=...,)[1]</code> renverra un deuxième élément <code>(batch_size, seq_len, embed_dim)</code> pour les embeddings des tokens. Il prend donc une phrase en entrée, la tokenise avec le tokenizer Jina CLIP, puis regroupe les tokens de sous-mots en \"mots\" en faisant la moyenne des embeddings de tokens correspondants. Il détecte le début d'un nouveau mot en vérifiant si la chaîne de token commence par le caractère <code>_</code> (typique des tokenizers basés sur SentencePiece). Il produit une liste d'embeddings au niveau des mots et une liste de mots (ainsi \"Dog\" est un embedding, \"and\" est un embedding, etc.).</p><p><strong>Calcul des embeddings au niveau des patchs</strong></p><p>Pour la tour d'image, <code>vision_model(..., return_all_features=True)</code> renverra <code>(batch_size, n_patches+1, embed_dim)</code>, où le premier token est le token <code>[CLS]</code>. À partir de là, le code extrait les embeddings pour chaque patch (c'est-à-dire les tokens de patch du transformateur de vision). Il redimensionne ensuite ces embeddings de patch en une grille 2D, <code>patch_side × patch_side</code>, qui est ensuite sur-échantillonnée pour correspondre à la résolution de l'image d'origine.</p><p><strong>Visualisation de la similarité mot-patch</strong></p><p>Le calcul de similarité et la génération ultérieure de carte de chaleur sont des techniques d'interprétabilité \"post-hoc\" standard : vous sélectionnez un embedding de texte, calculez la similarité cosinus avec chaque embedding de patch, puis générez une carte de chaleur montrant quels patchs ont la plus grande similarité avec cet embedding de token spécifique. Enfin, il parcourt chaque token de la phrase, met ce token en gras à gauche et superpose la carte de chaleur basée sur la similarité sur l'image d'origine à droite. Toutes les images sont compilées en un GIF animé.</p><h2 id=\"is-it-meaningful-explainability\">Est-ce une Explicabilité Significative ?</h2><p>D'un point de vue <em>purement code</em>, oui, la logique est cohérente et produira une carte de chaleur pour chaque token. Vous obtiendrez une série d'images qui mettent en évidence les similarités des patchs, donc le script \"fait ce qu'il dit\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>En regardant les exemples ci-dessus, nous voyons que des mots comme <code>moon</code> et <code>branches</code> semblent bien s'aligner avec leurs patchs visuels correspondants dans l'image d'origine. Mais voici la question clé : s'agit-il d'un alignement significatif, ou voyons-nous simplement une coïncidence heureuse ?</p><p>C'est une question plus profonde. Pour comprendre les mises en garde, rappelons <strong>comment CLIP est entraîné</strong> :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 combine un encodeur de texte (Jina XLM-RoBERTa, 561M paramètres) et un encodeur de vision (EVA02-L14, 304M paramètres). Chaque carré coloré à droite représente une phrase complète ou une image dans le batch - pas des tokens ou des patchs individuels.</span></figcaption></figure><ul><li>CLIP utilise un alignement contrastif <strong>global</strong> entre une image entière et un texte entier. Pendant l'entraînement, l'encodeur d'image produit un seul vecteur (représentation groupée), et l'encodeur de texte produit un autre vecteur unique ; CLIP est entraîné pour que ceux-ci correspondent pour les paires texte-image correspondantes et ne correspondent pas dans le cas contraire.</li><li>Il n'y a <strong>aucune supervision explicite au niveau 'le patch X correspond au token Y'.</strong> Le modèle n'est pas directement entraîné à mettre en évidence \"cette région de l'image est le chien, cette région est le chat\", etc. Au lieu de cela, on lui apprend que la représentation de l'image entière doit correspondre à la représentation du texte entier.</li><li>Comme l'architecture de CLIP est un Vision Transformer côté image et un transformateur de texte côté texte—formant tous deux des encodeurs séparés—il n'y a pas de module d'attention croisée qui aligne naturellement les patchs aux tokens. Au lieu de cela, vous obtenez purement de <strong>l'auto-attention</strong> dans chaque tour, plus une projection finale pour les embeddings globaux d'image ou de texte.</li></ul><p>En bref, c'est une visualisation heuristique. Le fait qu'un embedding de patch donné puisse être proche ou éloigné d'un embedding de token particulier est quelque peu émergent. C'est plus une <em>astuce d'interprétabilité post-hoc</em> qu'une \"attention\" robuste ou officielle du modèle.</p><h2 id=\"why-might-local-alignment-emerge\">Pourquoi l'Alignement Local Pourrait-il Émerger ?</h2><p>Alors pourquoi pourrions-nous parfois repérer des alignements locaux au niveau mot-patch ? Voici la chose : même si CLIP est entraîné sur un objectif contrastif <em>global</em> image-texte, il utilise toujours l'auto-attention (dans les encodeurs d'image basés sur ViT) et les couches transformer (pour le texte). Au sein de ces couches d'auto-attention, différentes parties des représentations d'images peuvent interagir entre elles, tout comme le font les mots dans les représentations de texte. Grâce à l'entraînement sur des ensembles de données massifs image-texte, le modèle développe naturellement des structures latentes internes qui l'aident à faire correspondre les images globales à leurs descriptions textuelles correspondantes.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>L'alignement local</strong> peut apparaître dans ces représentations latentes pour au moins deux raisons :</p><ol><li><strong>Schémas de co-occurrence</strong> : Si un modèle voit de nombreuses images de \"chiens\" à côté de nombreuses images de \"chats\" (souvent étiquetées ou décrites avec ces mots), il peut apprendre des caractéristiques latentes qui correspondent approximativement à ces concepts. Ainsi, l'embedding pour \"chien\" pourrait se rapprocher des patchs locaux qui représentent une forme ou une texture de chien. Ce n'est <em>pas</em> explicitement supervisé au niveau du patch, mais émerge de l'association répétée entre les paires image/texte de chiens.</li><li><strong>Self-attention</strong> : Dans les Vision Transformers, les patches s'attendent mutuellement. Les patches distinctifs (comme le visage d'un chien) peuvent finir par avoir une \"signature\" latente cohérente, car le modèle essaie de produire une seule représentation globalement précise de toute la scène. Si cela aide à minimiser la perte contrastive globale, cela sera renforcé.</li></ol><h2 id=\"theoretical-analysis\">Analyse théorique</h2><p>L'objectif d'apprentissage contrastif de CLIP vise à maximiser la similarité cosinus entre les paires image-texte correspondantes tout en la minimisant pour les paires non correspondantes. Supposons que les encodeurs de texte et d'image produisent respectivement des embeddings de tokens et de patches :</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>La similarité globale peut être représentée comme un agrégat de similarités locales :</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Lorsque des paires token-patch spécifiques co-occurrent fréquemment dans les données d'entraînement, le modèle renforce leur similarité par des mises à jour graduelles cumulatives :</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, où $C$ est le nombre de co-occurrences. Cela conduit à une augmentation significative de $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$, favorisant un alignement local plus fort pour ces paires. Cependant, la perte contrastive distribue les mises à jour du gradient sur toutes les paires token-patch, limitant la force des mises à jour pour une paire spécifique :</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>Cela empêche un renforcement significatif des similarités individuelles token-patch.</p><h2 id=\"conclusion\">Conclusion</h2><p>Les visualisations token-patch de CLIP capitalisent sur un alignement fortuit et émergent entre les représentations textuelles et visuelles. Cet alignement, bien qu'intriguant, découle de l'<strong>entraînement contrastif global</strong> de CLIP et manque de la robustesse structurelle nécessaire pour une explicabilité précise et fiable. Les visualisations qui en résultent présentent souvent du <strong>bruit et des incohérences</strong>, limitant leur utilité pour des applications interprétatives approfondies.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Les modèles d'interaction tardive comme <strong>ColBERT</strong> et <strong>ColPali</strong> répondent à ces limitations en <strong>intégrant architecturalement des alignements explicites et détaillés</strong> entre les tokens de texte et les patches d'image. En traitant les modalités indépendamment et en effectuant des calculs de similarité ciblés à un stade ultérieur, ces modèles garantissent que chaque token de texte est associé de manière significative aux régions d'image pertinentes.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP peut visualiser les similarités entre les tokens et les patches, cependant, il s'agit davantage d'une astuce d'interprétabilité post-hoc que d'une véritable \"attention\" robuste ou officielle du modèle. Voici pourquoi.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
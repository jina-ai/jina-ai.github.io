{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "Les embeddings textuels échouent à capturer l'ordre des mots et comment y remédier",
  "html": "<p>Récemment, Christoph Schuhmann, fondateur de <a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> a partagé une observation intéressante concernant les modèles d'embedding de texte :</p><blockquote>Lorsque les mots d'une phrase sont mélangés aléatoirement, la similarité cosinus entre leurs embeddings de texte reste étonnamment élevée par rapport à la phrase originale.</blockquote><p>Par exemple, prenons deux phrases : <code>Berlin is the capital of Germany</code> et <code>the Germany Berlin is capital of</code>. Même si la deuxième phrase n'a aucun sens, les modèles d'embedding de texte ne peuvent pas vraiment les distinguer. En utilisant <code>jina-embeddings-v3</code>, ces deux phrases ont un score de similarité cosinus de 0,9295.</p><p>L'ordre des mots n'est pas le seul aspect pour lequel les embeddings semblent peu sensibles. Les transformations grammaticales peuvent modifier radicalement le sens d'une phrase mais avoir peu d'impact sur la distance d'embedding. Par exemple, <code>She ate dinner before watching the movie</code> et <code>She watched the movie before eating dinner</code> ont une similarité cosinus de 0,9833, bien qu'elles aient un ordre d'actions opposé.</p><p>La négation est également notoirement difficile à encoder de manière cohérente sans <a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">entraînement spécial</a> — <code>This is a useful model</code> et <code>This is not a useful model</code> apparaissent pratiquement identiques dans l'espace d'embedding. Souvent, le remplacement de mots dans un texte par d'autres de la même classe, comme changer \"today\" par \"yesterday\", ou modifier le temps d'un verbe, ne modifie pas autant les embeddings qu'on pourrait le penser.</p><p>Cela a des implications sérieuses. Prenons deux requêtes de recherche : <code>Flight from Berlin to Amsterdam</code> et <code>Flight from Amsterdam to Berlin</code>. Elles ont des embeddings presque identiques, <code>jina-embeddings-v3</code> leur attribuant une similarité cosinus de 0,9884. Pour une application réelle comme la recherche de voyages ou la logistique, cette lacune est fatale.</p><p>Dans cet article, nous examinons les défis auxquels font face les modèles d'embedding, en analysant leurs difficultés persistantes avec l'ordre des mots et le choix des mots. Nous analysons les principaux modes d'échec à travers les catégories linguistiques — y compris les contextes directionnels, temporels, causaux, comparatifs et négatifs — tout en explorant des stratégies pour améliorer les performances des modèles.</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">Pourquoi les phrases mélangées ont-elles des scores cosinus étonnamment proches ?</h2><p>Au début, nous pensions que cela pouvait être dû à la façon dont le modèle combine les sens des mots - il crée un embedding pour chaque mot (6-7 mots dans chacune de nos phrases d'exemple ci-dessus) puis fait la moyenne de ces embeddings avec le mean pooling. Cela signifie que très peu d'informations sur l'ordre des mots sont disponibles dans l'embedding final. Une moyenne est la même quel que soit l'ordre des valeurs.</p><p>Cependant, même les modèles qui utilisent le CLS pooling (qui examine un premier mot spécial pour comprendre la phrase entière et devrait être plus sensible à l'ordre des mots) ont le même problème. Par exemple, <code>bge-1.5-base-en</code> donne toujours un score de similarité cosinus de 0,9304 pour les phrases <code>Berlin is the capital of Germany</code> et <code>the Germany Berlin is capital of</code>.</p><p>Cela met en évidence une limitation dans la façon dont les modèles d'embedding sont entraînés. Bien que les modèles de langage apprennent initialement la structure des phrases pendant le pré-entraînement, ils semblent perdre une partie de cette compréhension pendant l'entraînement contrastif — le processus que nous utilisons pour créer des modèles d'embedding.</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">Comment la longueur du texte et l'ordre des mots influencent-ils la similarité des embeddings ?</h2><p>Pourquoi les modèles ont-ils des difficultés avec l'ordre des mots en premier lieu ? La première chose qui vient à l'esprit est la longueur (en tokens) du texte. Lorsque le texte est envoyé à la fonction d'encodage, le modèle génère d'abord une liste d'embeddings de tokens (c'est-à-dire que chaque mot tokenisé a un vecteur dédié représentant son sens), puis en fait la moyenne.</p><p>Pour voir comment la longueur du texte et l'ordre des mots influencent la similarité des embeddings, nous avons généré un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">jeu de données de 180 phrases synthétiques</a> de longueurs variables, comme 3, 5, 10, 15, 20 et 30 tokens. Nous avons également mélangé aléatoirement les tokens pour former une variation de chaque phrase :</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Voici quelques exemples :</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">Longueur (tokens)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">Phrase originale</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">Phrase mélangée</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Nous allons encoder le jeu de données en utilisant notre propre modèle <code>jina-embeddings-v3</code> et le modèle open-source <code>bge-base-en-v1.5</code>, puis calculer la similarité cosinus entre la phrase originale et la phrase mélangée :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Longueur (tokens)</th>\n<th>Similarité cosinus moyenne</th>\n<th>Écart-type de la similarité cosinus</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0,947</td>\n<td>0,053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0,909</td>\n<td>0,052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0,924</td>\n<td>0,031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0,918</td>\n<td>0,019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0,899</td>\n<td>0,021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0,874</td>\n<td>0,025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Nous pouvons maintenant générer un diagramme en boîte, qui rend la tendance de la similarité cosinus plus claire :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1 : Distribution de la similarité par longueur de phrase pour les phrases mélangées avec </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> et </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (non affiné)</span></figcaption></figure><p>Comme nous pouvons le voir, il existe une relation linéaire claire dans la similarité cosinus moyenne des embeddings. Plus le texte est long, plus le score moyen de similarité cosinus entre les phrases originales et mélangées aléatoirement est faible. Cela se produit probablement en raison du \"déplacement des mots\", c'est-à-dire la distance à laquelle les mots se sont déplacés de leurs positions d'origine après un mélange aléatoire. Dans un texte plus court, il y a simplement moins de \"positions\" possibles pour qu'un token soit mélangé, il ne peut donc pas se déplacer aussi loin, tandis qu'un texte plus long a un plus grand nombre de permutations potentielles et les mots peuvent se déplacer sur une plus grande distance.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2 : Combinaisons de phrases par nombre de mots</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Nous arrêterons le tableau ici, car le nombre de combinaisons est la factorielle du nombre de mots. Lorsqu'on arrive à trente mots, on obtient 265 nonillions (2,652528598 E+32) de combinaisons.</div></div><p>Comme le montre la figure ci-dessous (Similarité cosinus vs Déplacement moyen des mots), plus le texte est long, plus le déplacement des mots est important :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3 : Similarité cosinus vs Déplacement moyen des mots avec un jeu de données de phrases mélangées montrant la corrélation entre le déplacement moyen des mots et la dissimilarité cosinus.</span></figcaption></figure><p>Les embeddings de tokens dépendent du contexte local, c'est-à-dire des mots les plus proches d'eux. Dans un texte court, réorganiser les mots ne peut pas beaucoup modifier ce contexte. Cependant, pour un texte plus long, un mot peut être déplacé très loin de son contexte d'origine, ce qui peut considérablement modifier son embedding de token. Par conséquent, mélanger les mots dans un texte plus long produit un embedding plus distant que pour un texte plus court. La figure ci-dessus montre que pour <code>jina-embeddings-v3</code>, utilisant le mean pooling, et <code>bge-base-en-v1.5</code>, utilisant le CLS pooling, la même relation s'applique : mélanger des textes plus longs et déplacer les mots plus loin entraîne des scores de similarité plus faibles.</p><h2 id=\"do-bigger-models-solve-the-problem\">Les modèles plus grands résolvent-ils le problème ?</h2><p>Habituellement, face à ce type de problème, une tactique courante consiste à simplement utiliser un modèle plus grand. Mais un modèle d'embedding de texte plus grand peut-il vraiment capturer plus efficacement l'information sur l'ordre des mots ? Selon la loi d'échelle des modèles d'embedding de texte (<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">référencée dans notre article sur la sortie de <code>jina-embeddings-v3</code></a>), les modèles plus grands offrent généralement de meilleures performances :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4 : Loi d'échelle des modèles d'embedding, montrant l'évolution des performances MTEB avec le nombre de paramètres.</span></figcaption></figure><p>Mais un modèle plus grand peut-il capturer plus efficacement l'information sur l'ordre des mots ? Nous avons testé trois variations du modèle BGE : <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>, <a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a>, et <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>, avec respectivement 33 millions, 110 millions et 335 millions de paramètres.</p><p>Nous utiliserons les mêmes 180 phrases qu'auparavant, mais sans tenir compte de l'information sur la longueur. Nous encoderons à la fois les phrases originales et leurs mélanges aléatoires en utilisant les trois variantes du modèle et tracerons la similarité cosinus moyenne :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5 : Impact de la taille du modèle sur la sensibilité à l'ordre des mots avec un jeu de données de phrases mélangées utilisant </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, et </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Bien que nous puissions constater que les modèles plus grands sont plus sensibles aux variations de l'ordre des mots, la différence est faible. Même le <code>bge-large-en-v1.5</code>, beaucoup plus grand, n'est qu'un peu meilleur pour distinguer les phrases mélangées des phrases non mélangées. D'autres facteurs entrent en jeu pour déterminer la sensibilité d'un modèle d'embedding aux réorganisations de mots, en particulier les différences dans le régime d'entraînement. De plus, la similarité cosinus est un outil très limité pour mesurer la capacité d'un modèle à faire des distinctions. Cependant, nous pouvons voir que la taille du modèle n'est pas un facteur majeur. Nous ne pouvons pas simplement agrandir notre modèle et résoudre ce problème.</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">L'ordre des mots et le choix des mots dans le monde réel</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Pour une grande partie de cet article, nous utilisons <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v2</code></a> (<i><em class=\"italic\" style=\"white-space: pre-wrap;\">pas</em></i> notre modèle le plus récent, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>) car <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v2</code> est beaucoup plus petit et donc plus rapide pour expérimenter sur nos GPU locaux, avec 137m paramètres contre 580m pour <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v3</code>.</div></div><p>Comme nous l'avons mentionné dans l'introduction, l'ordre des mots n'est pas le seul défi pour les modèles d'embedding. Un défi plus réaliste dans le monde réel concerne le <em>choix</em> des mots. Il existe de nombreuses façons de modifier les mots dans une phrase — des façons qui ne se reflètent pas bien dans les embeddings. Nous pouvons prendre \"Elle a volé de Paris à Tokyo\" et le modifier pour obtenir \"Elle a conduit de Tokyo à Paris\", et les embeddings restent similaires. Nous avons cartographié cela à travers plusieurs catégories de modification :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Catégorie</th>\n<th>Exemple - Gauche</th>\n<th>Exemple - Droite</th>\n<th>Similarité cosinus (<code>jina</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Directionnel</td>\n<td>Elle a volé de Paris à Tokyo</td>\n<td>Elle a conduit de Tokyo à Paris</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>Temporel</td>\n<td>Elle a dîné avant de regarder le film</td>\n<td>Elle a regardé le film avant de dîner</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>Causal</td>\n<td>La température montante a fait fondre la neige</td>\n<td>La neige fondante a refroidi la température</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>Comparatif</td>\n<td>Le café a meilleur goût que le thé</td>\n<td>Le thé a meilleur goût que le café</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>Négation</td>\n<td>Il est debout près de la table</td>\n<td>Il est debout loin de la table</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Notez que ce sont des cas courants que nous avons observés pendant notre travail et ne représentent pas nécessairement une taxonomie complète des catégories.</div></div><p>Le tableau ci-dessus montre une liste de \"cas d'échec\" où un modèle d'embedding de texte échoue à capturer de subtiles altérations de mots. Cela correspond à nos attentes : les modèles d'embedding de texte n'ont pas la capacité de raisonner. Par exemple, le modèle ne comprend pas la relation entre \"de\" et \"vers\". Les modèles d'embedding de texte effectuent une correspondance sémantique, la sémantique étant généralement capturée au niveau des tokens puis compressée en un seul vecteur dense après pooling. En revanche, <a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">les LLM (modèles autorégressifs) entraînés sur des jeux de données plus importants, à l'échelle du billion de tokens, commencent à démontrer des capacités émergentes de raisonnement</a>.</p><p>Cela nous a fait nous demander : pouvons-nous affiner le modèle d'embedding avec un apprentissage contrastif utilisant des triplets pour rapprocher la requête et le positif, tout en éloignant la requête du négatif ?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6 : L'effet de l'apprentissage contrastif : rapprocher la requête et le positif, éloigner le négatif.</span></figcaption></figure><p>Par exemple, \"Vol d'Amsterdam à Berlin\" pourrait être considéré comme la paire négative de \"Vol de Berlin à Amsterdam\". En fait, dans le <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\">rapport technique de <code>jina-embeddings-v1</code></a> (Michael Guenther, et al.), nous avons brièvement abordé ce problème à petite échelle : nous avons affiné le modèle <code>jina-embeddings-v1</code> sur un <a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">jeu de données de négation</a> de 10 000 exemples générés par des modèles de langage.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Les résultats, rapportés dans le lien du rapport ci-dessus, étaient prometteurs :</p><blockquote>Nous observons que pour toutes les tailles de modèles, l'affinement sur les données en triplets (qui inclut notre jeu de données d'entraînement sur la négation) améliore considérablement les performances, en particulier sur la tâche HardNegation.</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7 : Tableau montrant les scores EasyNegation et HardNegation sur plusieurs tailles de modèles </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> avec un entraînement par paires et un entraînement combiné triplets/paires.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 8 : Comparaison des performances des stratégies d'entraînement entre différentes versions de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">Affinage des modèles d'embedding de texte avec des jeux de données curés</h2><p>Dans les sections précédentes, nous avons exploré plusieurs observations clés concernant les embeddings de texte :</p><ol><li>Les textes plus courts sont plus sujets aux erreurs de capture de l'ordre des mots.</li><li>Augmenter la taille du modèle d'embedding de texte n'améliore pas nécessairement la compréhension de l'ordre des mots.</li><li>L'apprentissage contrastif pourrait offrir une solution potentielle à ces problèmes.</li></ol><p>Dans cette optique, nous avons affiné <code>jina-embeddings-v2-base-en</code> et <code>bge-base-en-1.5</code> sur nos jeux de données de négation et d'ordre des mots (environ 11 000 échantillons d'entraînement au total) :</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Pour aider à évaluer l'affinage, nous avons généré un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">jeu de données</a> de 1 000 triplets composés d'une <code>query</code>, d'un cas <code>positive (pos)</code>, et d'un cas <code>negative (neg)</code> :</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Voici un exemple de ligne :</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ces triplets sont conçus pour couvrir divers cas d'échec, y compris les changements de sens <strong>directionnels</strong>, <strong>temporels</strong> et <strong>causaux</strong> dus aux changements d'ordre des mots.</p><p>Nous pouvons maintenant évaluer les modèles sur trois ensembles d'évaluation différents :</p><ol><li>L'ensemble de 180 phrases synthétiques (du début de cet article), mélangées aléatoirement.</li><li>Cinq exemples vérifiés manuellement (du tableau directionnel/causal/etc. ci-dessus).</li><li>94 triplets curés de notre <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">jeu de données de triplets</a> que nous venons de générer.</li></ol><p>Voici la différence pour les phrases mélangées avant et après l'affinage :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Longueur de phrase (tokens)</th>\n<th>Similarité cosinus moyenne (<code>jina</code>)</th>\n<th>Similarité cosinus moyenne (<code>jina-ft</code>)</th>\n<th>Similarité cosinus moyenne (<code>bge</code>)</th>\n<th>Similarité cosinus moyenne (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Le résultat semble clair : malgré un processus de fine-tuning qui ne prend que cinq minutes, nous observons une amélioration spectaculaire des performances sur le jeu de données de phrases mélangées aléatoirement :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 9 : Distribution de similarité par longueur de phrase pour les phrases mélangées avec </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> et </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (fine-tuné).</span></figcaption></figure><p>Nous observons également des gains sur les cas directionnels, temporels, causaux et comparatifs. Le modèle montre une amélioration substantielle des performances reflétée par une baisse de la similarité cosinus moyenne. Le gain de performance le plus important concerne le cas de la négation, grâce à notre jeu de données de fine-tuning contenant 10 000 exemples de négation.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Catégorie</th>\n<th>Exemple - Gauche</th>\n<th>Exemple - Droite</th>\n<th>Similarité Cosinus Moyenne (<code>jina</code>)</th>\n<th>Similarité Cosinus Moyenne (<code>jina-ft</code>)</th>\n<th>Similarité Cosinus Moyenne (<code>bge</code>)</th>\n<th>Similarité Cosinus Moyenne (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Directionnel</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>Temporel</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>Causal</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>Comparatif</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>Négation</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Conclusion</h2><p>Dans cet article, nous explorons les défis auxquels sont confrontés les modèles d'embedding de texte, en particulier leur difficulté à gérer efficacement l'ordre des mots. Pour résumer, nous avons identifié cinq types principaux d'échecs : <strong>Directionnel</strong>, <strong>Temporel</strong>, <strong>Causal</strong>, <strong>Comparatif</strong>, et <strong>Négation</strong>. Ce sont les types de requêtes où l'ordre des mots est vraiment important, et si votre cas d'utilisation implique l'un de ces aspects, il est important de connaître les limites de ces modèles.</p><p>Nous avons également mené une expérience rapide, en étendant un jeu de données axé sur la négation pour couvrir les cinq catégories d'échecs. Les résultats sont prometteurs : le fine-tuning avec des « hard negatives » soigneusement choisis a permis au modèle de mieux reconnaître quels éléments vont ensemble et lesquels ne le font pas. Cela dit, il reste encore du travail à faire. Les prochaines étapes comprennent une analyse plus approfondie de l'impact de la taille et de la qualité du jeu de données sur les performances.</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "Les modèles d'intégration de texte peinent à capturer les nuances linguistiques subtiles comme l'ordre des mots, les relations directionnelles, les séquences temporelles, les liens de causalité, les comparaisons et la négation. Comprendre ces défis est essentiel pour améliorer les performances des modèles.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
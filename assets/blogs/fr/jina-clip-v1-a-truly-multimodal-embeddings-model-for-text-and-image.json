{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1 : Un v√©ritable mod√®le d'embeddings multimodal pour texte et image",
  "html": "<p>Jina CLIP v1 (<code>jina-clip-v1</code>) est un nouveau mod√®le d'embedding multimodal qui √©tend les capacit√©s du <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">mod√®le CLIP original</a> d'OpenAI. Avec ce nouveau mod√®le, les utilisateurs disposent d'un mod√®le d'embedding unique qui offre des performances √©tat de l'art √† la fois dans la recherche de texte seul et dans la recherche cross-modale texte-image. Jina AI a am√©lior√© les performances d'OpenAI CLIP de 165% dans la recherche de texte seul, et de 12% dans la recherche d'image √† image, avec des performances identiques ou l√©g√®rement meilleures dans les t√¢ches de texte vers image et d'image vers texte. Cette performance am√©lior√©e rend Jina CLIP v1 indispensable pour travailler avec des entr√©es multimodales.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> am√©liore OpenAI CLIP dans <a href=\"#compare_table\" rel=\"noreferrer\">toutes les cat√©gories de recherche</a>.</div></div><p>Dans cet article, nous discuterons d'abord des lacunes du mod√®le CLIP original et de la fa√ßon dont nous les avons abord√©es en utilisant une m√©thode unique de co-entra√Ænement. Ensuite, nous d√©montrerons l'efficacit√© de notre mod√®le sur divers benchmarks de recherche. Enfin, nous fournirons des instructions d√©taill√©es sur la fa√ßon dont les utilisateurs peuvent commencer avec Jina CLIP v1 via notre API Embeddings et Hugging Face.</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">L'Architecture CLIP pour l'IA Multimodale</h2><p>En janvier 2021, OpenAI a publi√© le mod√®le <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a> (Contrastive Language‚ÄìImage Pretraining). CLIP poss√®de une architecture simple mais ing√©nieuse : il combine deux mod√®les d'embedding, un pour les textes et un pour les images, en un seul mod√®le avec un espace d'embedding unique. Ses embeddings de texte et d'image sont directement comparables entre eux, rendant la distance entre un embedding de texte et un embedding d'image proportionnelle √† la qualit√© de la description de l'image par le texte, et vice versa.</p><p>Cela s'est av√©r√© tr√®s utile dans la recherche d'informations multimodales et la classification d'images zero-shot. Sans entra√Ænement sp√©cial suppl√©mentaire, CLIP a bien perform√© dans le classement d'images dans des cat√©gories avec des √©tiquettes en langage naturel.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"Diagramme illustrant la traduction d'image en texte utilisant comme exemple un astronaute sur Mars avec une lune rouge.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Le mod√®le d'embedding de texte dans le CLIP original √©tait un r√©seau neuronal personnalis√© avec seulement 63 millions de param√®tres. C√¥t√© image, OpenAI a publi√© CLIP avec une s√©lection de mod√®les <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> et <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT</a>. Chaque mod√®le a √©t√© pr√©-entra√Æn√© pour sa modalit√© individuelle puis entra√Æn√© avec des images l√©gend√©es pour produire des embeddings similaires pour les paires image-texte pr√©par√©es.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"Organigramme avec le texte &quot;Embedding Space&quot;, li√© √† &quot;Image Encoder&quot; et &quot;Text Encoder&quot;, avec une √©tiquette &quot;Distracted boyfriend&quot;.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Cette approche a donn√© des r√©sultats impressionnants. Particuli√®rement notable est sa performance en classification zero-shot. Par exemple, m√™me si les donn√©es d'entra√Ænement ne comprenaient pas d'images √©tiquet√©es d'<a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">astronautes</a>, CLIP pouvait correctement identifier des photos d'astronautes bas√© sur sa compr√©hension des concepts connexes dans les textes et les images.</p><p>Cependant, le CLIP d'OpenAI pr√©sente deux inconv√©nients importants :</p><ul><li>Premi√®rement, sa capacit√© d'entr√©e de texte tr√®s limit√©e. Il peut prendre un maximum de 77 tokens en entr√©e, mais <a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">l'analyse empirique montre</a> qu'en pratique il n'utilise pas plus de 20 tokens pour produire ses embeddings. Cela est d√ª au fait que CLIP a √©t√© entra√Æn√© √† partir d'images avec des l√©gendes, et les l√©gendes ont tendance √† √™tre tr√®s courtes. Cela contraste avec les mod√®les d'embedding de texte actuels qui supportent plusieurs milliers de tokens.</li><li>Deuxi√®mement, la performance de ses embeddings de texte dans les sc√©narios de recherche texte seul est tr√®s faible. Les l√©gendes d'images sont un type de texte tr√®s limit√©, et ne refl√®tent pas la large gamme de cas d'utilisation qu'un mod√®le d'embedding de texte devrait supporter.</li></ul><p>Dans la plupart des cas d'utilisation r√©els, la recherche texte seul et image-texte sont combin√©es ou au moins les deux sont disponibles pour les t√¢ches. Maintenir un second mod√®le d'embeddings pour les t√¢ches texte seul double effectivement la taille et la complexit√© de votre framework d'IA.</p><p>Le nouveau mod√®le de Jina AI traite directement ces probl√®mes, et <code>jina-clip-v1</code> profite des progr√®s r√©alis√©s ces derni√®res ann√©es pour apporter des performances √©tat de l'art aux t√¢ches impliquant toutes les combinaisons de modalit√©s texte et image.</p><h2 id=\"introducing-jina-clip-v1\">Pr√©sentation de Jina CLIP v1</h2><p>Jina CLIP v1 conserve le sch√©ma original de CLIP d'OpenAI : deux mod√®les co-entra√Æn√©s pour produire une sortie dans le m√™me espace d'embedding.</p><p>Pour l'encodage de texte, nous avons adapt√© l'architecture <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> utilis√©e dans les <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">mod√®les Jina Embeddings v2</a>. Cette architecture supporte une fen√™tre d'entr√©e √©tat de l'art de 8k tokens et produit des vecteurs de 768 dimensions, produisant des embeddings plus pr√©cis √† partir de textes plus longs. C'est plus de 100 fois les 77 tokens d'entr√©e support√©s dans le mod√®le CLIP original.</p><p>Pour les embeddings d'images, nous utilisons le dernier mod√®le de l'Acad√©mie de P√©kin pour l'Intelligence Artificielle : le mod√®le <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code></a>. Nous avons empiriquement compar√© un certain nombre de mod√®les d'IA d'image, les testant dans des contextes cross-modaux avec un pr√©-entra√Ænement similaire, et <code>EVA-02</code> a clairement surpass√© les autres. Il est √©galement comparable √† l'architecture Jina BERT en taille de mod√®le, de sorte que les charges de calcul pour les t√¢ches de traitement d'image et de texte sont √† peu pr√®s identiques.</p><p>Ces choix produisent des avantages importants pour les utilisateurs :</p><ul><li>De meilleures performances sur tous les benchmarks et toutes les combinaisons modales, et particuli√®rement des am√©liorations importantes dans les performances d'embedding texte seul.</li><li>Les performances empiriquement sup√©rieures d'<code>EVA-02</code> √† la fois dans les t√¢ches image-texte et image seule, avec l'avantage suppl√©mentaire de l'entra√Ænement additionnel de Jina AI, am√©liorant les performances image seule.</li><li>Support pour des entr√©es de texte beaucoup plus longues. Le support d'entr√©e de <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">8k tokens de Jina Embeddings</a> rend possible le traitement d'informations textuelles d√©taill√©es et leur corr√©lation avec des images.</li><li>Une importante √©conomie nette en espace, calcul, maintenance de code et complexit√© car ce mod√®le multimodal est tr√®s performant m√™me dans des sc√©narios non multimodaux.</li></ul><h3 id=\"training\">Entra√Ænement</h3><p>Une partie de notre recette pour l'IA multimodale haute performance r√©side dans nos donn√©es d'entra√Ænement et notre proc√©dure. Nous remarquons que la tr√®s courte longueur des textes utilis√©s dans les l√©gendes d'images est la cause majeure des faibles performances texte seul dans les mod√®les de type CLIP, et notre entra√Ænement est explicitement con√ßu pour rem√©dier √† cela.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"Organigramme illustrant l'optimisation de la similarit√© texte et l√©gende-image dans trois t√¢ches, utilisant un mod√®le et des encodeurs.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>L'entra√Ænement se d√©roule en trois √©tapes :</p><ol><li>Utiliser des donn√©es d'images l√©gend√©es pour apprendre √† aligner les embeddings d'image et de texte, entrelac√©es avec des paires de textes ayant des significations similaires. Ce co-entra√Ænement optimise conjointement les deux types de t√¢ches. Les performances texte seul du mod√®le diminuent pendant cette phase, mais pas autant que si nous avions entra√Æn√© uniquement avec des paires image-texte.</li><li>Entra√Æner en utilisant des donn√©es synth√©tiques qui alignent les images avec des textes plus longs, g√©n√©r√©s par un mod√®le d'IA, qui d√©crivent l'image. Continuer l'entra√Ænement avec des paires texte seul en m√™me temps. Pendant cette phase, le mod√®le apprend √† pr√™ter attention √† des textes plus longs en conjonction avec des images.</li><li>Utiliser des triplets de texte avec des <a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">n√©gatifs difficiles</a> pour am√©liorer davantage les performances texte seul en apprenant √† faire des distinctions s√©mantiques plus fines. En m√™me temps, continuer l'entra√Ænement en utilisant des paires synth√©tiques d'images et de longs textes. Pendant cette phase, les performances texte seul s'am√©liorent dramatiquement sans que le mod√®le ne perde ses capacit√©s image-texte.</li></ol><p>Pour plus d'informations sur les d√©tails de l'entra√Ænement et l'architecture du mod√®le, veuillez lire <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">notre article r√©cent</a> :</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP : Votre Mod√®le CLIP Est Aussi Votre Syst√®me de Recherche de Texte</div><div class=\"kg-bookmark-description\">Le pr√©tra√Ænement contrastif langue-image (CLIP) est largement utilis√© pour entra√Æner des mod√®les √† aligner les images et les textes dans un espace d'embedding commun en les mappant vers des vecteurs de taille fixe. Ces mod√®les sont essentiels pour la recherche d'informations multimodale et les t√¢ches connexes. Cependant, les mod√®les CLIP sont g√©n√©ralement moins performants dans les t√¢ches textuelles par rapport aux mod√®les de texte sp√©cialis√©s. Cela cr√©e des inefficacit√©s pour les syst√®mes de recherche d'informations qui conservent des embeddings et des mod√®les s√©par√©s pour les t√¢ches textuelles et multimodales. Nous proposons une nouvelle m√©thode d'entra√Ænement contrastive multi-t√¢ches pour r√©soudre ce probl√®me, que nous utilisons pour entra√Æner le mod√®le jina-clip-v1 afin d'atteindre des performances √† l'√©tat de l'art sur les t√¢ches de recherche texte-image et texte-texte.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">Nouvel √©tat de l'art en embeddings multimodaux</h2><p>Nous avons √©valu√© les performances de Jina CLIP v1 sur des t√¢ches textuelles uniquement, des t√¢ches d'images uniquement et des t√¢ches cross-modales impliquant les deux modalit√©s d'entr√©e. Nous avons utilis√© le <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">benchmark de recherche MTEB</a> pour √©valuer les performances textuelles. Pour les t√¢ches d'images uniquement, nous avons utilis√© le benchmark <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a>. Pour les t√¢ches cross-modales, nous √©valuons sur <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a>, et <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a>, qui sont inclus dans le <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a>.</p><p>Les r√©sultats sont r√©sum√©s dans le tableau ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Vous pouvez voir d'apr√®s ces r√©sultats que <code>jina-clip-v1</code> surpasse le CLIP original d'OpenAI dans toutes les cat√©gories, et est nettement meilleur dans la recherche de texte uniquement et d'images uniquement. En moyenne sur toutes les cat√©gories, c'est une am√©lioration de 46 % des performances.</p><p>Vous pouvez trouver une √©valuation plus d√©taill√©e dans <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">notre r√©cent article</a>.</p><h2 id=\"getting-started-with-embeddings-api\">Premiers pas avec l'API Embeddings</h2><p>Vous pouvez facilement int√©grer Jina CLIP v1 dans vos applications en utilisant l'<a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">API Jina Embeddings</a>.</p><p>Le code ci-dessous vous montre comment appeler l'API pour obtenir des embeddings pour des textes et des images, en utilisant le package <code>requests</code> en Python. Il transmet une cha√Æne de texte et une URL d'image au serveur Jina AI et renvoie les deux encodages.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚òùÔ∏è</div><div class=\"kg-callout-text\">N'oubliez pas de remplacer <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> par une cl√© API Jina activ√©e. Vous pouvez obtenir une cl√© d'essai avec un million de tokens gratuits sur la <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">page web Jina Embeddings</a>.</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">Int√©gration avec les principaux frameworks LLM</h3><p>Jina CLIP v1 est d√©j√† disponible pour <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> et <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a> :</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a> : Utilisez <code>JinaEmbedding</code> avec la classe de base <code>MultimodalEmbedding</code>, et invoquez <code>get_image_embeddings</code> ou <code>get_text_embeddings</code>.</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a> : Utilisez <code>JinaEmbeddings</code>, et invoquez <code>embed_images</code> ou <code>embed_documents</code>.</li></ul><h3 id=\"pricing\">Tarification</h3><p>Les entr√©es de texte et d'image sont factur√©es par consommation de tokens.</p><p>Pour le texte en anglais, <a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">nous avons calcul√© empiriquement</a> qu'en moyenne vous aurez besoin de 1,1 tokens pour chaque mot.</p><p>Pour les images, nous comptons le nombre de tuiles de 224x224 pixels n√©cessaires pour couvrir votre image. Certaines de ces tuiles peuvent √™tre partiellement vides mais comptent de la m√™me mani√®re. Chaque tuile co√ªte 1 000 tokens √† traiter.</p><p><strong>Exemple</strong></p><p>Pour une image de dimensions 750x500 pixels :</p><ol><li>L'image est divis√©e en tuiles de 224x224 pixels.<ol><li>Pour calculer le nombre de tuiles, prenez la largeur en pixels et divisez par 224, puis arrondissez √† l'entier sup√©rieur. <br>     750/224 ‚âà 3,35 ‚Üí 4</li><li>R√©p√©tez pour la hauteur en pixels : <br>     500/224 ‚âà 2,23 ‚Üí 3</li></ol></li><li>Le nombre total de tuiles requises dans cet exemple est : <br>           4 (horizontal) x 3 (vertical) = 12 tuiles</li><li>Le co√ªt sera de 12 x 1 000 = 12 000 tokens </li></ol><h3 id=\"enterprise-support\">Support Entreprise</h3><p>Nous introduisons un nouveau b√©n√©fice pour les utilisateurs qui ach√®tent le plan Production Deployment avec <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">11 milliards de tokens</a>. Cela comprend :</p><ul><li>Trois heures de consultation avec nos √©quipes produit et ing√©nierie pour discuter de vos cas d'utilisation et besoins sp√©cifiques.</li><li>Un notebook Python personnalis√© con√ßu pour votre cas d'utilisation RAG (Retrieval-Augmented Generation) ou de recherche vectorielle, d√©montrant comment int√©grer les mod√®les de Jina AI dans votre application.</li><li>Attribution d'un responsable de compte et support par email prioritaire pour s'assurer que vos besoins sont satisfaits rapidement et efficacement.</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Jina CLIP v1 Open Source sur Hugging Face</h2><p>Jina AI s'engage pour une base de recherche open source, et dans ce but, nous rendons ce mod√®le disponible gratuitement sous une <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">licence Apache 2.0</a>, sur <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>.</p><p>Vous pouvez trouver du code exemple pour t√©l√©charger et ex√©cuter ce mod√®le sur votre propre syst√®me ou installation cloud sur la page du mod√®le Hugging Face pour <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">Nous sommes en route pour faire progresser et d√©mocratiser l'intelligence artificielle gr√¢ce √† l'open source et la science ouverte.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">R√©sum√©</h2><p>Le dernier mod√®le de Jina AI ‚Äî <code>jina-clip-v1</code> ‚Äî repr√©sente une avanc√©e significative dans les mod√®les d'embedding multimodaux, offrant des gains de performance substantiels par rapport au CLIP d'OpenAI. Avec des am√©liorations notables dans les t√¢ches de recherche texte uniquement et image uniquement, ainsi que des performances comp√©titives dans les t√¢ches texte-vers-image et image-vers-texte, il s'impose comme une solution prometteuse pour les cas d'utilisation d'embeddings complexes.</p><p>Ce mod√®le ne prend actuellement en charge que les textes en anglais en raison de contraintes de ressources. Nous travaillons √† √©tendre ses capacit√©s √† d'autres langues.</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzm√°n",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Le nouveau mod√®le d'embedding multimodal de Jina AI surpasse non seulement OpenAI CLIP dans la recherche texte-image, mais il constitue √©galement un solide mod√®le d'embedding d'images et un mod√®le d'embedding de texte √† la pointe de la technologie, le tout simultan√©ment. Vous n'avez plus besoin de diff√©rents mod√®les pour diff√©rentes modalit√©s.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
{
  "slug": "bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning",
  "id": "67066bd652567c0001d0f2cd",
  "uuid": "1130051f-f343-4eb2-9956-9b574c212704",
  "title": "R√©duire les √©carts linguistiques dans les embeddings multilingues par apprentissage contrastif",
  "html": "<p>Dans les mod√®les multilingues, l'un des principaux d√©fis est le \"<strong>foss√© linguistique</strong>\" ‚Äî un ph√©nom√®ne o√π les phrases ayant la m√™me signification dans diff√©rentes langues ne sont pas aussi √©troitement align√©es ou regroup√©es qu'elles devraient l'√™tre. Id√©alement, un texte dans une langue et son √©quivalent dans une autre devraient avoir des repr√©sentations similaires ‚Äî c'est-√†-dire des embeddings tr√®s proches ‚Äî permettant aux applications multilingues de fonctionner de mani√®re identique sur des textes en diff√©rentes langues. Cependant, les mod√®les repr√©sentent souvent subtilement la langue d'un texte, cr√©ant un \"foss√© linguistique\" qui conduit √† des performances sous-optimales entre les langues.</p><p>Dans cet article, nous explorerons ce foss√© linguistique et son impact sur les performances des mod√®les d'embedding de texte. Nous avons men√© des exp√©riences pour √©valuer l'alignement s√©mantique des paraphrases dans une m√™me langue et des traductions entre diff√©rentes paires de langues, en utilisant notre mod√®le <code>jina-xlm-roberta</code> et le dernier <code>jina-embeddings-v3</code>. Ces exp√©riences r√©v√®lent √† quel point les phrases ayant des significations similaires ou identiques se regroupent dans diff√©rentes conditions d'entra√Ænement.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-6.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Nous avons √©galement exp√©riment√© des techniques d'entra√Ænement pour am√©liorer l'alignement s√©mantique entre les langues, en particulier l'introduction de <strong>donn√©es multilingues parall√®les</strong> pendant l'apprentissage contrastif. Dans cet article, nous partagerons nos observations et nos r√©sultats.</p><h2 id=\"multilingual-model-training-creates-and-reduces-the-language-gap\"><strong>L'entra√Ænement des mod√®les multilingues cr√©e et r√©duit le foss√© linguistique</strong></h2><p>L'entra√Ænement des mod√®les d'embedding de texte implique g√©n√©ralement un processus en plusieurs √©tapes avec deux parties principales :</p><ol><li><a href=\"https://aclanthology.org/2023.acl-long.49/?ref=jina-ai-gmbh.ghost.io\"><strong>Masked Language Modeling</strong></a> (MLM) : Le pr√©-entra√Ænement implique g√©n√©ralement de tr√®s grandes quantit√©s de texte dans lesquelles certains tokens sont masqu√©s al√©atoirement. Le mod√®le est entra√Æn√© √† pr√©dire ces tokens masqu√©s. Cette proc√©dure enseigne au mod√®le les motifs de la ou des langues dans les donn√©es d'entra√Ænement, y compris les d√©pendances de s√©lection entre les tokens qui peuvent d√©couler de la syntaxe, de la s√©mantique lexicale et des contraintes pragmatiques du monde r√©el.</li><li><a href=\"https://paperswithcode.com/task/contrastive-learning?ref=jina-ai-gmbh.ghost.io\"><strong>Apprentissage contrastif</strong></a> : Apr√®s le pr√©-entra√Ænement, le mod√®le est entra√Æn√© davantage avec des donn√©es organis√©es ou semi-organis√©es pour rapprocher les embeddings de textes s√©mantiquement similaires et (optionnellement) √©loigner ceux qui sont dissemblables. Cet entra√Ænement peut utiliser des paires, des triplets ou m√™me des groupes de textes dont la similarit√© s√©mantique est d√©j√† connue ou du moins estim√©e de mani√®re fiable. Il peut comporter plusieurs sous-√©tapes et il existe diverses strat√©gies d'entra√Ænement pour cette partie du processus, avec de nouvelles recherches publi√©es fr√©quemment et pas de consensus clair sur l'approche optimale.</li></ol><p>Pour comprendre comment le foss√© linguistique se cr√©e et comment il peut √™tre combl√©, nous devons examiner le r√¥le des deux √©tapes.</p><h3 id=\"masked-language-pretraining\"><strong>Pr√©-entra√Ænement par masquage de langue</strong></h3><p>Une partie de la capacit√© multilingue des mod√®les d'embedding de texte est acquise pendant le pr√©-entra√Ænement.</p><p>Les mots apparent√©s et emprunt√©s permettent au mod√®le d'apprendre un certain alignement s√©mantique entre les langues √† partir de grandes quantit√©s de donn√©es textuelles. Par exemple, le mot anglais <em>banana</em> et le mot fran√ßais <em>banane</em> (et l'allemand <em>Banane</em>) sont fr√©quents et suffisamment similaires dans leur orthographe pour qu'un mod√®le d'embedding puisse apprendre que les mots qui ressemblent √† \"banan-\" ont des mod√®les de distribution similaires entre les langues. Il peut utiliser cette information pour apprendre, dans une certaine mesure, que d'autres mots qui ne se ressemblent pas entre les langues ont aussi des significations similaires, et m√™me comprendre comment certaines structures grammaticales sont traduites.</p><p>Cependant, cela se produit sans entra√Ænement explicite.</p><p>Nous avons test√© le mod√®le <code>jina-xlm-roberta</code>, la base pr√©-entra√Æn√©e de <code>jina-embeddings-v3</code>, pour voir comment il a appris les √©quivalences entre langues √† partir du pr√©-entra√Ænement par masquage de langue. Nous avons trac√© des repr√©sentations de phrases bidimensionnelles <a href=\"https://pair-code.github.io/understanding-umap/?ref=jina-ai-gmbh.ghost.io\">UMAP</a> d'un ensemble de phrases anglaises traduites en allemand, n√©erlandais, chinois simplifi√© et japonais. Les r√©sultats sont dans la figure ci-dessous :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png\" class=\"kg-image\" alt=\"Multilingual scatterplot showing word embeddings' alignment across five languages on UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_roberta_mlm_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Projection UMAP bidimensionnelle d'une s√©lection de phrases anglaises et leurs traductions en allemand, n√©erlandais, chinois et japonais. Les lignes grises relient les phrases non anglaises aux phrases anglaises dont elles sont la traduction.<br><br>Ces phrases ont tendance √† former des clusters sp√©cifiques √† chaque langue dans l'espace d'embedding de <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-xlm-roberta</code>, bien que vous puissiez voir quelques valeurs aberrantes dans cette projection qui peuvent √™tre un effet secondaire de la projection bidimensionnelle.</div></div><p>On peut voir que le pr√©-entra√Ænement a tr√®s fortement regroup√© les embeddings des phrases dans la m√™me langue. Il s'agit d'une projection en deux dimensions d'une distribution dans un espace de dimension beaucoup plus √©lev√©e, il est donc toujours possible que, par exemple, une phrase allemande qui est une bonne traduction d'une phrase anglaise soit encore la phrase allemande dont l'embedding est le plus proche de l'embedding de sa source anglaise. Mais cela montre qu'un embedding d'une phrase anglaise est probablement plus proche d'une autre phrase anglaise que d'une phrase allemande s√©mantiquement identique ou presque identique.</p><p>Notez √©galement comment l'allemand et le n√©erlandais forment des clusters beaucoup plus proches que les autres paires de langues. Ce n'est pas surprenant pour deux langues relativement proches. L'allemand et le n√©erlandais sont suffisamment similaires pour √™tre parfois partiellement mutuellement compr√©hensibles.</p><p>Le japonais et le chinois semblent √©galement plus proches l'un de l'autre que des autres langues. Bien qu'ils ne soient pas apparent√©s de la m√™me mani√®re, le japonais √©crit utilise g√©n√©ralement des <em>kanji</em> (Êº¢Â≠ó), ou <em>h√†nz√¨</em> en chinois. Le japonais partage la plupart de ces caract√®res √©crits avec le chinois, et les deux langues partagent de nombreux mots √©crits avec un ou plusieurs kanji/h√†nz√¨ ensemble. Du point de vue du MLM, c'est le m√™me type de similarit√© visible qu'entre le n√©erlandais et l'allemand.</p><p>Nous pouvons voir ce \"foss√© linguistique\" de mani√®re plus simple en regardant seulement deux langues avec deux phrases chacune :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png\" class=\"kg-image\" alt=\"Graph illustrating linguistic relationships with color-coded lines, data points for English and German phrases, and an &quot;MLM P\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image--8-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image--8-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Puisque le MLM semble naturellement regrouper les textes par langue, \"my dog is blue\" et \"my cat is red\" sont regroup√©s ensemble, loin de leurs √©quivalents allemands. Contrairement au \"<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/?ref=jina-ai-gmbh.ghost.io\">foss√© de modalit√©</a>\" discut√© dans un article de blog pr√©c√©dent, nous pensons que cela provient des similarit√©s et dissimilarit√©s superficielles entre les langues : orthographes similaires, utilisation des m√™mes s√©quences de caract√®res dans l'√©criture, et possiblement des similarit√©s dans la morphologie et la structure syntaxique ‚Äî ordres des mots communs et fa√ßons communes de construire les mots.</p><p>En bref, quel que soit le degr√© d'apprentissage des √©quivalences entre langues lors du pr√©-entra√Ænement MLM, ce n'est pas suffisant pour surmonter une forte tendance √† regrouper les textes par langue. Cela laisse un grand foss√© linguistique.</p><h3 id=\"contrastive-learning\"><strong>Apprentissage contrastif</strong></h3><p>Id√©alement, nous voulons qu'un mod√®le d'embedding soit indiff√©rent √† la langue et n'encode que les significations g√©n√©rales dans ses embeddings. Dans un tel mod√®le, nous ne verrions aucun regroupement par langue et n'aurions aucun foss√© linguistique. Les phrases dans une langue devraient √™tre tr√®s proches de bonnes traductions et √©loign√©es d'autres phrases qui signifient autre chose, m√™me dans la m√™me langue, comme dans la figure ci-dessous :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png\" class=\"kg-image\" alt=\"Graph displays &quot;Clustering by Meaning&quot; with multilingual labels, emphasizing abstract concepts on a dark backdrop.\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-10---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-10---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-10---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Le pr√©-entra√Ænement MLM n'accomplit pas cela, nous utilisons donc des techniques d'<em>apprentissage contrastif</em> suppl√©mentaires pour am√©liorer la repr√©sentation s√©mantique des textes dans les embeddings.</p><p>L'apprentissage contrastif implique l'utilisation de paires de textes dont on sait qu'ils sont similaires ou diff√©rents en signification, et des triplets o√π une paire est connue pour √™tre plus similaire que l'autre. Les poids sont ajust√©s pendant l'entra√Ænement pour refl√©ter cette relation connue entre les paires et triplets de textes.</p><p>Il y a 30 langues repr√©sent√©es dans notre jeu de donn√©es d'apprentissage contrastif, mais 97% des paires et triplets sont dans une seule langue, avec seulement 3% impliquant des paires ou triplets multilingues. Mais ces 3% suffisent √† produire un r√©sultat spectaculaire : les embeddings montrent tr√®s peu de regroupement par langue et les textes s√©mantiquement similaires produisent des embeddings proches quelle que soit leur langue, comme le montre la projection UMAP des embeddings de <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png\" class=\"kg-image\" alt=\"Scatter plot sur fond noir montrant la distribution des langues apr√®s l'entra√Ænement contrastif avec les dimensions UMAP.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_contrastive_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Pour confirmer cela, nous avons mesur√© la Corr√©lation de Spearman des repr√©sentations g√©n√©r√©es par <code>jina-xlm-roberta</code> et <code>jina-embeddings-v3</code> sur le jeu de donn√©es STS17.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">La Corr√©lation de Spearman</strong></b></a> mesure la corr√©lation de rang, c'est-√†-dire √† quel point deux listes ordonn√©es sont similaires. C'est un bon m√©canisme pour comparer les mod√®les d'embedding entre eux et avec les scores humains car le score r√©el est beaucoup moins important que l'ordre relatif des √©l√©ments.</div></div><p>Le tableau ci-dessous montre la Corr√©lation de Spearman entre les classements de similarit√© s√©mantique pour les textes traduits dans diff√©rentes langues. Nous prenons un ensemble de phrases en anglais puis mesurons la similarit√© de leurs embeddings avec l'embedding d'une phrase de r√©f√©rence sp√©cifique et les trions du plus similaire au moins similaire. Ensuite, nous traduisons toutes ces phrases dans une autre langue et r√©p√©tons le processus de classement. Dans un mod√®le d'embedding multilingue id√©al, les deux listes ordonn√©es seraient identiques, et la Corr√©lation de Spearman serait de 1,0.</p><p>Le graphique et le tableau ci-dessous montrent nos r√©sultats comparant l'anglais et les six autres langues du benchmark STS17, en utilisant √† la fois <code>jina-xlm-roberta</code> et <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png\" class=\"kg-image\" alt=\"Diagramme en barres comparant la Corr√©lation de Spearman pour l'anglais coupl√© avec AR, DE, ES, FR, IT, NL, color√© en rouge et bleu par alphabet\" loading=\"lazy\" width=\"2000\" height=\"1056\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-4---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-4---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-4---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png 2085w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong><code>jina-xlm-roberta</code></strong></th>\n<th><strong><code>jina-embeddings-v3</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English ‚Üî Arabic</td>\n<td>0.1581</td>\n<td><strong>0.7977</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî German</td>\n<td>0.2136</td>\n<td><strong>0.8366</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Spanish</td>\n<td>0.1049</td>\n<td><strong>0.8509</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî French</td>\n<td>0.1659</td>\n<td><strong>0.8378</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Italian</td>\n<td>0.2293</td>\n<td><strong>0.8674</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Dutch</td>\n<td>0.2387</td>\n<td><strong>0.8398</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Vous pouvez voir ici l'√©norme diff√©rence que fait l'apprentissage contrastif, compar√© au pr√©-entra√Ænement initial. Bien que n'ayant que 3 % de donn√©es multilingues dans son mix d'entra√Ænement, le mod√®le <code>jina-embeddings-v3</code> a suffisamment appris la s√©mantique multilingue pour pratiquement √©liminer l'√©cart linguistique acquis lors du pr√©-entra√Ænement.</p><h2 id=\"english-vs-the-world-can-other-languages-keep-up-in-alignment\">L'anglais contre le monde : les autres langues peuvent-elles suivre en termes d'alignement ?</h2><p>Nous avons entra√Æn√© <code>jina-embeddings-v3</code> sur 89 langues, avec un accent particulier sur 30 langues √©crites tr√®s largement utilis√©es. Malgr√© nos efforts pour construire un corpus d'entra√Ænement multilingue √† grande √©chelle, l'anglais repr√©sente encore pr√®s de la moiti√© des donn√©es que nous avons utilis√©es dans l'entra√Ænement contrastif. Les autres langues, y compris les langues mondiales largement utilis√©es pour lesquelles un mat√©riel textuel abondant est disponible, sont encore relativement sous-repr√©sent√©es par rapport √† l'√©normit√© des donn√©es anglaises dans l'ensemble d'entra√Ænement.</p><p>√âtant donn√© cette pr√©dominance de l'anglais, les repr√©sentations en anglais sont-elles mieux align√©es que celles des autres langues ? Pour explorer cela, nous avons men√© une exp√©rience compl√©mentaire.</p><p>Nous avons construit un jeu de donn√©es, <a href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><code>parallel-sentences</code></a>, compos√© de 1 000 paires de textes en anglais, une \"ancre\" et un \"positif\", o√π le texte positif est logiquement impliqu√© par le texte ancre.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/parallel-sentences ¬∑ Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-3.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/parallel-sentences.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Par exemple, la premi√®re ligne du tableau ci-dessous. Ces phrases n'ont pas un sens identique, mais elles ont des significations compatibles. Elles d√©crivent de mani√®re informative la m√™me situation.</p><p>Nous avons ensuite traduit ces paires dans cinq langues en utilisant GPT-4o : allemand, n√©erlandais, chinois (simplifi√©), chinois (traditionnel) et japonais. Enfin, nous les avons inspect√©es manuellement pour garantir leur qualit√©.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Anchor</strong></th>\n<th><strong>Positive</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>Two young girls are playing outside in a non-urban environment.</td>\n<td>Two girls are playing outside.</td>\n</tr>\n<tr>\n<td>German</td>\n<td>Zwei junge M√§dchen spielen drau√üen in einer nicht urbanen Umgebung.</td>\n<td>Zwei M√§dchen spielen drau√üen.</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>Twee jonge meisjes spelen buiten in een niet-stedelijke omgeving.</td>\n<td>Twee meisjes spelen buiten.</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>‰∏§‰∏™Âπ¥ËΩªÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁéØÂ¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>‰∏§‰∏™Â•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>ÂÖ©ÂÄãÂπ¥ËºïÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁí∞Â¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>ÂÖ©ÂÄãÂ•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>2‰∫∫„ÅÆËã•„ÅÑÂ•≥„ÅÆÂ≠ê„ÅåÈÉΩÂ∏ÇÁí∞Â¢É„Åß„ÅØ„Å™„ÅÑÂ†¥ÊâÄ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n<td>‰∫å‰∫∫„ÅÆÂ∞ëÂ•≥„ÅåÂ§ñ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Nous avons ensuite encod√© chaque paire de textes avec <code>jina-embeddings-v3</code> et calcul√© la similarit√© cosinus entre eux. La figure et le tableau ci-dessous montrent la distribution des scores de similarit√© cosinus pour chaque langue, et la similarit√© moyenne :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png\" class=\"kg-image\" alt=\"Graphique montrant les distributions de similarit√© cosinus pour les paires textuelles en anglais, allemand, n√©erlandais, chinois et japonais par rapport √† la densit√©\" loading=\"lazy\" width=\"1060\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/1_monolingual_distribution_triplets.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/1_monolingual_distribution_triplets.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png 1060w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>0.9078</td>\n</tr>\n<tr>\n<td>German</td>\n<td>0.8949</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>0.8844</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>0.8876</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>0.8933</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>0.8895</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Malgr√© la pr√©dominance de l'anglais dans les donn√©es d'entra√Ænement, <code>jina-embeddings-v3</code> reconna√Æt la similarit√© s√©mantique en allemand, n√©erlandais, japonais et dans les deux formes de chinois presque aussi bien qu'en anglais.</p><h2 id=\"breaking-language-barriers-cross-lingual-alignment-beyond-english\"><strong>Briser les barri√®res linguistiques : l'alignement multilingue au-del√† de l'anglais</strong></h2><p>Les √©tudes sur l'alignement des repr√©sentations entre langues portent g√©n√©ralement sur des paires de langues incluant l'anglais. Cette orientation pourrait, en th√©orie, masquer ce qui se passe r√©ellement. Un mod√®le pourrait simplement optimiser pour repr√©senter tout le plus pr√®s possible de son √©quivalent anglais, sans examiner si d'autres paires de langues sont correctement prises en charge.</p><p>Pour explorer cela, nous avons men√© des exp√©riences en utilisant le dataset <code>parallel-sentences</code>, en nous concentrant sur l'alignement multilingue au-del√† des simples paires bilingues avec l'anglais.</p><p>Le tableau ci-dessous montre la distribution des similarit√©s cosinus entre des textes √©quivalents dans diff√©rentes paires de langues ‚Äî des textes qui sont des traductions d'une source anglaise commune. Id√©alement, toutes les paires devraient avoir un cosinus de 1 ‚Äî c'est-√†-dire des embeddings s√©mantiques identiques. En pratique, cela ne pourrait jamais arriver, mais nous nous attendrions √† ce qu'un bon mod√®le ait des valeurs cosinus tr√®s √©lev√©es pour les paires de traductions.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png\" class=\"kg-image\" alt=\"Density graph charting cross-lingual cosine similarities for language pairs using jina-embeddings-v3 model.\" loading=\"lazy\" width=\"978\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png 978w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language Pair</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>German ‚Üî Dutch</td>\n<td>0.8779</td>\n</tr>\n<tr>\n<td>German ‚Üî Japanese</td>\n<td>0.8664</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Japanese</td>\n<td>0.8534</td>\n</tr>\n<tr>\n<td>Dutch ‚Üî Chinese (Simplified)</td>\n<td>0.8479</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Chinese (Traditional)</td>\n<td>0.8758</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Bien que les scores de similarit√© entre diff√©rentes langues soient l√©g√®rement inf√©rieurs √† ceux des textes compatibles dans la m√™me langue, ils restent tr√®s √©lev√©s. La similarit√© cosinus des traductions n√©erlandais/allemand est presque aussi √©lev√©e qu'entre les textes compatibles en allemand.</p><p>Cela n'est peut-√™tre pas surprenant car l'allemand et le n√©erlandais sont des langues tr√®s similaires. De m√™me, les deux vari√©t√©s de chinois test√©es ici ne sont pas vraiment deux langues diff√©rentes, mais plut√¥t des formes stylistiquement diff√©rentes de la m√™me langue. Mais on peut voir que m√™me des paires de langues tr√®s dissemblables comme le n√©erlandais et le chinois ou l'allemand et le japonais montrent encore une tr√®s forte similarit√© entre les textes s√©mantiquement √©quivalents.</p><p>Nous avons envisag√© la possibilit√© que ces valeurs de similarit√© tr√®s √©lev√©es puissent √™tre un effet secondaire de l'utilisation de ChatGPT comme traducteur. Pour le tester, nous avons t√©l√©charg√© <a href=\"https://help.ted.com/hc/en-us/articles/360018572954-How-do-I-find-transcripts-for-TED-and-TEDx-talks?ref=jina-ai-gmbh.ghost.io\">des transcriptions de conf√©rences TED traduites par des humains</a> en anglais et en allemand et v√©rifi√© si les phrases traduites align√©es auraient la m√™me corr√©lation √©lev√©e.</p><p>Le r√©sultat √©tait encore plus fort que pour nos donn√©es traduites par machine, comme vous pouvez le voir sur la figure ci-dessous.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png\" class=\"kg-image\" alt=\"Graph of cross-lingual alignment density EN-DE with peak around cosine similarity 1.0, titled &quot;jina-embeddings-v3: Cross-ling\" loading=\"lazy\" width=\"988\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png 988w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-much-does-cross-language-data-contribute-to-cross-language-alignment\">Dans quelle mesure les donn√©es multilingues contribuent-elles √† l'alignement multilingue ?</h2><p>L'√©cart linguistique qui s'estompe et le haut niveau de performance multilingue semblent disproportionn√©s par rapport √† la tr√®s petite partie des donn√©es d'entra√Ænement qui √©tait explicitement multilingue. Seuls 3 % des donn√©es d'entra√Ænement contrastives enseignent sp√©cifiquement au mod√®le comment faire des alignements entre les langues.</p><p>Nous avons donc fait un test pour voir si le multilingue apportait une contribution quelconque.</p><p>R√©entra√Æner compl√®tement <code>jina-embeddings-v3</code> sans aucune donn√©e multilingue serait excessivement co√ªteux pour une petite exp√©rience, nous avons donc t√©l√©charg√© le <a href=\"https://huggingface.co/FacebookAI/xlm-roberta-base?ref=jina-ai-gmbh.ghost.io\">mod√®le <code>xlm-roberta-base</code> de Hugging Face</a> et l'avons entra√Æn√© davantage avec l'apprentissage contrastif, en utilisant un sous-ensemble des donn√©es que nous avons utilis√©es pour entra√Æner <code>jina-embeddings-v3</code>. Nous avons sp√©cifiquement ajust√© la quantit√© de donn√©es multilingues pour tester deux cas : un sans donn√©es multilingues, et un o√π 20 % des paires √©taient multilingues. Vous pouvez voir les m√©ta-param√®tres d'entra√Ænement dans le tableau ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table id=\"e30425bb-015e-4956-8872-b1b64cdd7ad0\" class=\"simple-table\"><tbody><tr id=\"daa3cfcc-9012-411b-8da3-05c7c6f4b371\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><strong>Backbone</strong></td><td id=\"DU<d\" class=\"\"><strong>% Cross-Language</strong></td><td id=\"@Feo\" class=\"\"><strong>Learning Rate</strong></td><td id=\"fZNx\" class=\"\"><strong>Loss Function</strong></td><td id=\"Rv}\\\" class=\"\"><strong>Temperature</strong></td></tr><tr id=\"f3a2d068-d902-4fc1-8c89-269a5ebbb135\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base </code><strong>without</strong> X-language data</td><td id=\"DU<d\" class=\"\">0%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr><tr id=\"52887e22-326c-46cd-b79c-d6dcd110c1d2\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base</code><strong> with </strong>X-language data</td><td id=\"DU<d\" class=\"\">20%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Nous avons ensuite √©valu√© les performances multilingues des deux mod√®les en utilisant les <a href=\"https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io\">benchmarks STS17 et STS22 de MTEB</a> et la corr√©lation de Spearman. Nous pr√©sentons les r√©sultats ci-dessous :</p><h3 id=\"sts17\"><strong>STS17</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png\" class=\"kg-image\" alt=\"Bar graph showing Spearman correlation for language pairs on STS17 with and without parallel corpus.\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-6---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-6---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-6---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"de30bf7f-d1a9-43f4-8e5f-15bf65c59674\" class=\"simple-table\"><tbody><tr id=\"5c6440a6-eba9-404a-a5f1-88099bc6702d\"><td id=\"{N[x\" class=\"\"><strong>Language Pair</strong></td><td id=\"jGmJ\" class=\"\"><strong>With parallel corpora</strong></td><td id=\"p<ZH\" class=\"\"><strong>Without parallel corpora</strong></td></tr><tr id=\"33f08461-58aa-43f3-9ed1-577f8676e99d\"><td id=\"{N[x\" class=\"\">English ‚Üî Arabic</td><td id=\"jGmJ\" class=\"\"><strong>0.6418</strong></td><td id=\"p<ZH\" class=\"\">0.5875</td></tr><tr id=\"9875386d-2043-4d9d-8252-e53ec525ec29\"><td id=\"{N[x\" class=\"\">English ‚Üî German</td><td id=\"jGmJ\" class=\"\">0.7364</td><td id=\"p<ZH\" class=\"\"><strong>0.7390</strong></td></tr><tr id=\"15d28a12-3a80-4176-9984-69b5d8a7d8ff\"><td id=\"{N[x\" class=\"\">English ‚Üî Spanish</td><td id=\"jGmJ\" class=\"\"><strong>0.6968</strong></td><td id=\"p<ZH\" class=\"\">0.6799</td></tr><tr id=\"21821558-c8b9-4c34-8ec5-9db3ca7d9328\"><td id=\"{N[x\" class=\"\">English ‚Üî French</td><td id=\"jGmJ\" class=\"\"><strong>0.7066</strong></td><td id=\"p<ZH\" class=\"\">0.6944</td></tr><tr id=\"a2e3b5e5-8e4a-4270-abff-5d059ff6be72\"><td id=\"{N[x\" class=\"\">English ‚Üî Italian</td><td id=\"jGmJ\" class=\"\"><strong>0.7232</strong></td><td id=\"p<ZH\" class=\"\">0.7070</td></tr><tr id=\"95daf20f-2a82-4431-8581-a4ce24d81462\"><td id=\"{N[x\" class=\"\">English ‚Üî Dutch</td><td id=\"jGmJ\" class=\"\"><strong>0.7597</strong></td><td id=\"p<ZH\" class=\"\">0.7468</td></tr><tr id=\"4fd4c45a-a69e-4e33-b057-9c5f10b99fdb\"><td id=\"{N[x\" class=\"\">English ‚Üî Turkish</td><td id=\"jGmJ\" class=\"\"><strong>0.6933</strong></td><td id=\"p<ZH\" class=\"\">0.6050</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"sts22\"><strong>STS22</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png\" class=\"kg-image\" alt=\"Chart comparing models of language alignment, showing Spearman correlation scores for eight language pairs with and without p\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-7---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-7---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-7---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"e53757df-8a0e-42ec-ba05-715baa3c77cd\" class=\"simple-table\"><tbody><tr id=\"45c43b8c-b1b7-4ac0-91b0-1e07025f1b92\"><td id=\"OF=p\" class=\"\"><strong>Paire de langues</strong></td><td id=\"P<\\i\" class=\"\"><strong>Avec corpus parall√®les</strong></td><td id=\"LAtp\" class=\"\"><strong>Sans corpus parall√®les</strong></td></tr><tr id=\"696ebe11-3eda-49ef-8dfe-608f9b71430b\"><td id=\"OF=p\" class=\"\">English ‚Üî Spanish</td><td id=\"P<\\i\" class=\"\"><strong>0.7710</strong></td><td id=\"LAtp\" class=\"\">0.7675</td></tr><tr id=\"eb4c1d81-7d98-453d-af3f-95b2adccfb55\"><td id=\"OF=p\" class=\"\">Simplified Chinese ‚Üî English</td><td id=\"P<\\i\" class=\"\"><strong>0.6885</strong></td><td id=\"LAtp\" class=\"\">0.6860</td></tr><tr id=\"533cefd3-b30e-4d6a-9350-8f5d28b17ba6\"><td id=\"OF=p\" class=\"\">Spanish ‚Üî Italian</td><td id=\"P<\\i\" class=\"\"><strong>0.6829</strong></td><td id=\"LAtp\" class=\"\">0.6814</td></tr><tr id=\"d3ecdd71-44bb-4a3b-9ac2-8cc90a785d5f\"><td id=\"OF=p\" class=\"\">German ‚Üî French</td><td id=\"P<\\i\" class=\"\"><strong>0.5763</strong></td><td id=\"LAtp\" class=\"\">0.5496</td></tr><tr id=\"c6242853-4da7-4369-b1f1-1a27262a487a\"><td id=\"OF=p\" class=\"\">German ‚Üî English</td><td id=\"P<\\i\" class=\"\">0.5439</td><td id=\"LAtp\" class=\"\"><strong>0.5566</strong></td></tr><tr id=\"31a4a5ba-199c-4904-b926-ff0561aac1b5\"><td id=\"OF=p\" class=\"\">Polish ‚Üî English</td><td id=\"P<\\i\" class=\"\">0.6966</td><td id=\"LAtp\" class=\"\"><strong>0.7156</strong></td></tr><tr id=\"4f529d81-e8c9-4e5d-a705-36e357abebc3\"><td id=\"OF=p\" class=\"\">German ‚Üî English</td><td id=\"P<\\i\" class=\"\"><strong>0.5832</strong></td><td id=\"LAtp\" class=\"\">0.5478</td></tr><tr id=\"cd1429f7-c810-4a0e-9dca-88e2c83157bc\"><td id=\"OF=p\" class=\"\">French ‚Üî Polish</td><td id=\"P<\\i\" class=\"\">0.8451</td><td id=\"LAtp\" class=\"\">0.8451</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Nous avons √©t√© surpris de constater que pour la plupart des paires de langues test√©es, les donn√©es d'entra√Ænement interlingues n'ont apport√© que peu ou pas d'am√©lioration. Il est difficile d'√™tre certain que cela resterait vrai pour des mod√®les compl√®tement entra√Æn√©s avec des jeux de donn√©es plus importants, mais cela sugg√®re certainement que l'entra√Ænement explicite entre langues n'apporte pas grand-chose.</p><p>Cependant, notez que STS17 inclut des paires anglais/arabe et anglais/turc. Ce sont deux langues beaucoup moins bien repr√©sent√©es dans nos donn√©es d'entra√Ænement. Le mod√®le XML-RoBERTa que nous avons utilis√© a √©t√© pr√©-entra√Æn√© avec des donn√©es qui ne contenaient que 2,25 % d'arabe et 2,32 % de turc, bien moins que pour les autres langues test√©es. Le petit jeu de donn√©es d'apprentissage contrastif que nous avons utilis√© dans cette exp√©rience ne contenait que 1,7 % d'arabe et 1,8 % de turc.</p><p>Ces deux paires de langues sont les seules test√©es o√π l'entra√Ænement avec des donn√©es interlingues a fait une diff√©rence notable. Nous pensons que les donn√©es interlingues explicites sont plus efficaces pour les langues qui sont moins bien repr√©sent√©es dans les donn√©es d'entra√Ænement, mais nous devons explorer davantage ce domaine avant de tirer une conclusion. Le r√¥le et l'efficacit√© des donn√©es interlingues dans l'apprentissage contrastif est un domaine o√π Jina AI m√®ne des recherches actives.</p><h2 id=\"conclusion\">Conclusion</h2><p>Les m√©thodes conventionnelles de pr√©-entra√Ænement linguistique, comme le Masked Language Modeling, laissent un ¬´ √©cart linguistique ¬ª, o√π des textes s√©mantiquement similaires dans diff√©rentes langues ne s'alignent pas aussi √©troitement qu'ils le devraient. Nous avons montr√© que le r√©gime d'apprentissage contrastif de Jina Embeddings est tr√®s efficace pour r√©duire, voire √©liminer cet √©cart.</p><p>Les raisons de cette efficacit√© ne sont pas enti√®rement claires. Nous utilisons explicitement des paires de textes interlingues dans l'entra√Ænement contrastif, mais seulement en tr√®s petites quantit√©s, et il n'est pas clair quel r√¥le elles jouent r√©ellement pour assurer des r√©sultats interlingues de haute qualit√©. Nos tentatives de d√©montrer un effet clair dans des conditions plus contr√¥l√©es n'ont pas produit de r√©sultat sans ambigu√Øt√©.</p><p>Cependant, <strong>il est clair que <code>jina-embeddings-v3</code> a surmont√© l'√©cart linguistique du pr√©-entra√Ænement, en faisant un outil puissant pour les applications multilingues.</strong> Il est pr√™t √† √™tre utilis√© pour toute t√¢che n√©cessitant des performances fortes et identiques dans plusieurs langues. </p><p>Vous pouvez utiliser <code>jina-embeddings-v3</code> via notre <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">API Embeddings</a> (avec un million de tokens gratuits) ou via AWS ou Azure. Si vous souhaitez l'utiliser en dehors de ces plateformes ou sur site dans votre entreprise, gardez simplement √† l'esprit qu'il est sous licence CC BY-NC 4.0. <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io\">Contactez-nous</a> si vous √™tes int√©ress√© par une utilisation commerciale.</p>",
  "comment_id": "67066bd652567c0001d0f2cd",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/gap-blog-1.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-09T13:41:10.000+02:00",
  "updated_at": "2024-10-10T20:15:26.000+02:00",
  "published_at": "2024-10-09T14:42:22.000+02:00",
  "custom_excerpt": "Multilingual models often face a \"language gap,\" where similar phrases in different languages don't align. We show how contrastive learning can bridge this gap, enhancing cross-language performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/",
  "excerpt": "Les mod√®les multilingues sont souvent confront√©s √† un ¬´ foss√© linguistique ¬ª, o√π des phrases similaires dans diff√©rentes langues ne s'alignent pas. Nous montrons comment l'apprentissage contrastif peut combler ce foss√© et am√©liorer les performances interlangues.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon green squares form intricate patterns on a black digital background, creating a dynamic, abstract design.",
  "feature_image_caption": null
}
{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "A-t-on encore besoin du découpage avec les modèles qui gèrent les longs contextes ?",
  "html": "<p>En octobre 2023, nous avons introduit <code>jina-embeddings-v2</code>, la première famille de modèles d'embedding open-source capable de gérer des entrées allant jusqu'à 8 192 tokens. Dans la continuité, cette année nous avons lancé <code>jina-embeddings-v3</code>, offrant le même support étendu d'entrées avec des améliorations supplémentaires.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Dans cet article, nous allons approfondir les embeddings de contexte long et répondre à quelques questions : Quand est-il pratique de consolider un tel volume de texte en un seul vecteur ? La segmentation améliore-t-elle la recherche, et si oui, comment ? Comment pouvons-nous préserver le contexte des différentes parties d'un document tout en segmentant le texte ?</p><p>Pour répondre à ces questions, nous allons comparer plusieurs méthodes de génération d'embeddings :</p><ul><li>Embedding de contexte long (encodage jusqu'à 8 192 tokens dans un document) vs contexte court (c'est-à-dire troncature à 192 tokens).</li><li>Sans découpage vs découpage naïf vs <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">late chunking</a>.</li><li>Différentes tailles de chunks avec découpage naïf et late chunking.</li></ul><h2 id=\"is-long-context-even-useful\">Le Contexte Long Est-il Vraiment Utile ?</h2><p>Avec la capacité d'encoder jusqu'à dix pages de texte dans un seul embedding, les modèles d'embedding de contexte long ouvrent des possibilités pour la représentation de texte à grande échelle. Mais est-ce vraiment utile ? Selon beaucoup de gens... non.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Sources : </span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Citation de Nils Reimer dans le podcast How AI Is Built</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">tweet de brainlag</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">commentaire de egorfine sur Hacker News</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">commentaire de andy99 sur Hacker News</span></a></p></figcaption></figure><p>Nous allons aborder toutes ces préoccupations avec une investigation détaillée des capacités de contexte long, quand le contexte long est utile, et quand vous devriez (et ne devriez pas) l'utiliser. Mais d'abord, écoutons ces sceptiques et examinons certains des problèmes auxquels font face les modèles d'embedding de contexte long.</p><h2 id=\"problems-with-long-context-embeddings\">Problèmes avec les Embeddings de Contexte Long</h2><p>Imaginons que nous construisons un système de recherche de documents pour des articles, comme ceux de notre <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">blog Jina AI</a>. Parfois, un seul article peut couvrir plusieurs sujets, comme le <a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">rapport sur notre visite à la conférence ICML 2024</a>, qui contient :</p><ul><li>Une introduction, capturant des informations générales sur ICML (nombre de participants, lieu, portée, etc).</li><li>La présentation de notre travail (<code>jina-clip-v1</code>).</li><li>Des résumés d'autres articles de recherche intéressants présentés à ICML.</li></ul><p>Si nous créons un seul embedding pour cet article, cet embedding représente un mélange de trois sujets disparates :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1 : Lors de l'embedding d'un document couvrant plusieurs sujets, le vecteur résultant représente un mélange de tous les paragraphes, perdant potentiellement les informations distinctes et spécifiques contenues dans chaque paragraphe individuel.</span></figcaption></figure><p>Cela conduit à plusieurs problèmes :</p><ul><li><strong>Dilution de la Représentation :</strong> Bien que tous les sujets d'un texte donné <em>puissent</em> être liés, un seul peut être pertinent pour la requête de recherche d'un utilisateur. Cependant, un seul embedding (dans ce cas, celui de l'ensemble du billet de blog) n'est qu'un point dans l'espace vectoriel. À mesure que plus de texte est ajouté à l'entrée du modèle, l'embedding se déplace pour capturer le sujet global de l'article, le rendant moins efficace pour représenter le contenu couvert dans des paragraphes spécifiques.</li><li><strong>Capacité Limitée :</strong> Les modèles d'embedding produisent des vecteurs de taille fixe, indépendamment de la longueur d'entrée. Plus on ajoute de contenu à l'entrée, plus il devient difficile pour le modèle de représenter toutes ces informations dans le vecteur. C'est comme réduire une image à 16×16 pixels — Si vous réduisez une image de quelque chose de simple, comme une pomme, vous pouvez encore tirer du sens de l'image réduite. Réduire un plan des rues de Berlin ? Pas vraiment.</li><li><strong>Perte d'Information :</strong> Dans certains cas, même les modèles d'embedding de contexte long atteignent leurs limites ; De nombreux modèles prennent en charge l'encodage de texte jusqu'à 8 192 tokens. Les documents plus longs doivent être tronqués avant l'embedding, conduisant à une perte d'information. Si l'information pertinente pour l'utilisateur se trouve à la fin du document, elle ne sera pas du tout capturée par l'embedding.</li><li><strong>Vous Pourriez <em>Avoir Besoin</em> de Segmentation de Texte :</strong> Certaines applications nécessitent des embeddings pour des segments spécifiques du texte mais pas pour l'ensemble du document, comme l'identification du passage pertinent dans un texte.</li></ul><h2 id=\"long-context-vs-truncation\">Contexte Long vs. Troncature</h2><p>Pour voir si le contexte long est vraiment utile, examinons la performance de deux scénarios de recherche :</p><ul><li>Encodage de documents jusqu'à 8 192 tokens (environ 10 pages de texte).</li><li>Troncature des documents à 192 tokens et encodage jusqu'à ce point.</li></ul><p>Nous allons comparer les résultats en utilisant<code>jina-embeddings-v3</code> avec la métrique de récupération nDCG@10. Nous avons testé les jeux de données suivants :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Description</th>\n<th>Exemple de requête</th>\n<th>Exemple de document</th>\n<th>Longueur moyenne des documents (caractères)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>Un jeu de données de recherche médicale en texte intégral avec 3 244 requêtes et documents principalement issus de PubMed.</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326 753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>Un jeu de données de résumé de réunions basé sur des requêtes nécessitant le résumé des segments pertinents de réunions.</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37 445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>Jeu de données QA contenant de longues histoires et des questions correspondantes sur des contenus spécifiques.</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53 336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>Un jeu de données QA multi-étapes avec jusqu'à 5 étapes de raisonnement, conçu avec des modèles pour éviter les raccourcis.</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30 854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>Un jeu de données de résumés de scénarios avec des transcriptions et résumés de séries TV nécessitant l'intégration d'intrigues dispersées.</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1 613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Comme nous pouvons le voir, l'encodage de plus de 192 tokens peut apporter des améliorations notables de performance :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2 : Comparaison des performances d'embedding de contexte long et d'embedding de texte court</span></figcaption></figure><p>Cependant, sur certains jeux de données, nous observons des améliorations plus importantes que sur d'autres :</p><ul><li>Pour <strong>NFCorpus</strong>, la troncature ne fait pratiquement aucune différence. Cela s'explique par le fait que les titres et les résumés se trouvent au début des documents, et ceux-ci sont très pertinents pour les termes de recherche typiques des utilisateurs. Que le texte soit tronqué ou non, les données les plus pertinentes restent dans la limite de tokens.</li><li><strong>QMSum</strong> et <strong>NarrativeQA</strong> sont considérés comme des tâches de \"compréhension de lecture\", où les utilisateurs recherchent généralement des faits spécifiques dans un texte. Ces faits sont souvent dispersés dans les détails du document et peuvent se trouver en dehors de la limite de 192 tokens. Par exemple, dans le document NarrativeQA <em>Percival Keene</em>, la réponse à la question \"Qui est le tyran qui vole le déjeuner de Percival ?\" se trouve bien au-delà de cette limite. De même, dans <strong>2WikiMultiHopQA</strong>, les informations pertinentes sont dispersées dans l'ensemble des documents, obligeant les modèles à naviguer et à synthétiser les connaissances de plusieurs sections pour répondre efficacement aux requêtes.</li><li><strong>SummScreenFD</strong> est une tâche visant à identifier le scénario correspondant à un résumé donné. Comme le résumé englobe des informations réparties dans tout le scénario, l'encodage d'une plus grande partie du texte améliore la précision de l'association du résumé au bon scénario.</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">Segmentation du texte pour de meilleures performances de recherche</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Dans la suite, nous discutons de trois concepts similaires. Pour éviter toute confusion, nous les désignons comme suit :<br>• <b><strong style=\"white-space: pre-wrap;\">Segmentation</strong></b> : Détection des marques de limite dans un texte d'entrée, par exemple, les phrases ou un nombre fixe de tokens.<br>• <b><strong style=\"white-space: pre-wrap;\">Chunking naïf</strong></b> : Division du texte en chunks basée sur les marques de segmentation, avant de l'encoder.<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Late chunking</strong></b></a> : Encodage du document d'abord puis segmentation (préservant le contexte entre les chunks).</div></div><p>Au lieu d'intégrer un document entier dans un seul vecteur, nous pouvons utiliser différentes méthodes pour d'abord segmenter le document en attribuant des marques de limite :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3 : Application des méthodes de chunking \"Taille fixe\", \"Basé sur les phrases\" et \"Sémantique\" à un passage de texte</span></figcaption></figure><p>Quelques méthodes courantes incluent :</p><ul><li><strong>Segmentation par taille fixe :</strong> Le document est divisé en segments d'un nombre fixe de tokens, déterminé par le tokenizer du modèle d'embedding. Cela garantit que la tokenisation des segments correspond à la tokenisation du document entier (segmenter par un nombre spécifique de caractères pourrait conduire à une tokenisation différente).</li><li><strong>Segmentation par phrase :</strong> Le document est segmenté en phrases, et chaque chunk se compose de <em>n</em> phrases.</li><li><strong>Segmentation par sémantique :</strong> Chaque segment correspond à plusieurs phrases et un modèle d'embedding détermine la similarité des phrases consécutives. Les phrases ayant des similarités d'embedding élevées sont assignées au même chunk.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Vous pouvez facilement effectuer la segmentation avec <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a>, notre API gratuite pour segmenter les longs textes en chunks et la tokenisation basée sur la structure du document.</div></div><p>Par souci de simplicité, nous utilisons la segmentation à taille fixe dans cet article.</p><h3 id=\"document-retrieval-using-naive-chunking\">Récupération de documents utilisant le chunking naïf</h3><p>Une fois que nous avons effectué la segmentation à taille fixe, nous pouvons découper naïvement le document selon ces segments :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4 : Chunking naïf basé sur les marques de limite détectées pendant la segmentation.</span></figcaption></figure><p>En utilisant <code>jina-embeddings-v3</code>, nous encodons chaque chunk en un embedding qui capture précisément sa sémantique, puis stockons ces embeddings dans une base de données vectorielle.</p><p>À l'exécution, le modèle encode la requête d'un utilisateur en un vecteur de requête. Nous comparons celui-ci à notre base de données vectorielle d'embeddings de chunks pour trouver le chunk ayant la plus haute similarité cosinus, puis retournons le document correspondant à l'utilisateur :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5 : Recherche de documents implémentée avec un découpage naïf : (1) Les documents de la collection sont divisés en fragments selon des indices de délimitation, (2) le modèle d'embedding encode tous les fragments et nous stockons les embeddings résultants dans une base de données, (3) lorsqu'une requête arrive, le modèle d'embedding l'encode et la base de données détermine le fragment le plus similaire. À la fin, nous identifions le document pertinent à partir de l'ID du document stocké pour le fragment dans la base de données et le retournons à l'utilisateur.</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">Problèmes avec le découpage naïf</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6 : Lors du découpage d'un texte en phrases, les références aux parties précédentes du texte ne peuvent pas être résolues.</span></figcaption></figure><p>Bien que le découpage naïf réponde à certaines limitations des modèles d'embedding à contexte long, il présente aussi des inconvénients :</p><ul><li><strong>Perte de la vue d'ensemble :</strong> En ce qui concerne la recherche de documents, plusieurs embeddings de petits fragments peuvent ne pas saisir le sujet global du document. C'est comme ne pas voir la forêt à cause des arbres.</li><li><strong>Problème de contexte manquant :</strong> Les fragments ne peuvent pas être interprétés avec précision car les informations contextuelles sont manquantes, comme illustré dans la Figure 6.</li><li><strong>Efficacité :</strong> Plus de fragments nécessitent plus de stockage et augmentent le temps de recherche.</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">Le découpage tardif résout le problème de contexte</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Pour résoudre le problème du contexte manquant, nous avons introduit une nouvelle méthode appelée « découpage tardif », décrite dans nos précédents articles de blog : <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">partie I</strong></b></a>, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">partie II</strong></b></a>, <a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">partie III</strong></b></a>, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">article de recherche</strong></b></a>.</div></div><p>Le découpage tardif fonctionne en deux étapes principales :</p><ol><li>D'abord, il utilise les capacités de contexte long du modèle pour encoder l'ensemble du document en embeddings de tokens. Cela préserve le contexte complet du document.</li><li>Ensuite, il crée des embeddings de fragments en appliquant un pooling moyen à des séquences spécifiques d'embeddings de tokens, correspondant aux indices de délimitation identifiés lors de la segmentation.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7 : Découpage tardif vs découpage naïf.</span></figcaption></figure><p>L'avantage principal de cette approche est que les embeddings de tokens sont contextualisés - ce qui signifie qu'ils capturent naturellement les références et les relations avec d'autres parties du document. Puisque le processus d'embedding se produit avant le découpage, chaque fragment conserve une conscience du contexte plus large du document, résolvant ainsi le problème de contexte manquant qui affecte les approches de découpage naïf.</p><p>Pour les documents qui dépassent la taille d'entrée maximale du modèle, nous pouvons utiliser le « découpage tardif long » :</p><ol><li>D'abord, nous divisons le document en « macro-fragments » qui se chevauchent. Chaque macro-fragment est dimensionné pour s'adapter à la longueur maximale de contexte du modèle (par exemple, 8 192 tokens).</li><li>Le modèle traite ces macro-fragments pour créer des embeddings de tokens.</li><li>Une fois que nous avons les embeddings de tokens, nous procédons au découpage tardif standard - en appliquant le pooling moyen pour créer les embeddings de fragments finaux.</li></ol><p>Cette approche nous permet de gérer des documents de n'importe quelle longueur tout en préservant les avantages du découpage tardif. Considérez-la comme un processus en deux étapes : d'abord rendre le document digestible pour le modèle, puis appliquer la procédure de découpage tardif régulière.</p><p>En résumé :</p><ul><li><strong>Découpage naïf :</strong> Segmenter le document en petits fragments, puis encoder chaque fragment séparément.</li><li><strong>Découpage tardif :</strong> Encoder l'ensemble du document en une fois pour créer des embeddings de tokens, puis créer des embeddings de fragments en regroupant les embeddings de tokens selon les limites des segments.</li><li><strong>Découpage tardif long :</strong> Diviser les grands documents en macro-fragments qui se chevauchent et qui s'adaptent à la fenêtre de contexte du modèle, les encoder pour obtenir des embeddings de tokens, puis appliquer le découpage tardif normalement.</li></ul><p>Pour une description plus détaillée de l'idée, consultez notre <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">article</a> ou les articles de blog mentionnés ci-dessus.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">Découper ou ne pas découper ?</h2><p>Nous avons déjà vu que l'embedding à contexte long surpasse généralement les embeddings de textes plus courts, et donné un aperçu des stratégies de découpage naïf et tardif. La question est maintenant : Le découpage est-il meilleur que l'embedding à contexte long ?</p><p>Pour effectuer une comparaison équitable, nous tronquons les valeurs de texte à la longueur maximale de séquence du modèle (8 192 tokens) avant de commencer à les segmenter. Nous utilisons une segmentation de taille fixe avec 64 tokens par segment (pour la segmentation naïve et le découpage tardif). Comparons trois scénarios :</p><ul><li><strong>Pas de segmentation :</strong> Nous encodons chaque texte en un seul embedding. Cela conduit aux mêmes scores que l'expérience précédente (voir Figure 2), mais nous les incluons ici pour mieux les comparer.</li><li><strong>Découpage naïf :</strong> Nous segmentons les textes, puis appliquons un découpage naïf basé sur les indices de délimitation.</li><li><strong>Découpage tardif :</strong> Nous segmentons les textes, puis utilisons le découpage tardif pour déterminer les embeddings.</li></ul><p>Pour le découpage tardif et la segmentation naïve, nous utilisons la recherche de fragments pour déterminer le document pertinent (comme montré dans la Figure 5, plus haut dans cet article).</p><p>Les résultats ne montrent pas de gagnant clair :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 8 : Pas de découpage vs découpage naïf vs découpage tardif</span></figcaption></figure><ul><li><strong>Pour la recherche de faits, le découpage naïf est plus performant :</strong> Pour les jeux de données QMSum, NarrativeQA et 2WikiMultiHopQA, le modèle doit identifier les passages pertinents dans le document. Ici, le découpage naïf est clairement meilleur que l'encodage de tout en un seul embedding, car probablement seuls quelques fragments contiennent des informations pertinentes, et ces fragments les capturent beaucoup mieux qu'un seul embedding de l'ensemble du document.</li><li><strong>Le chunking tardif fonctionne mieux avec des documents cohérents et un contexte pertinent :</strong> Pour les documents couvrant un sujet cohérent où les utilisateurs recherchent des thèmes généraux plutôt que des faits spécifiques (comme dans NFCorpus), le chunking tardif surpasse légèrement l'absence de chunking, car il équilibre le contexte global du document avec les détails locaux. Cependant, bien que le chunking tardif soit généralement plus performant que le chunking naïf en préservant le contexte, cet avantage peut devenir un inconvénient lors de la recherche de faits isolés dans des documents contenant principalement des informations non pertinentes - comme le montrent les régressions de performance pour NarrativeQA et 2WikiMultiHopQA, où le contexte ajouté devient plus distrayant qu'utile.</li></ul><h3 id=\"does-chunk-size-make-a-difference\">La Taille des Chunks Fait-elle une Différence ?</h3><p>L'efficacité des méthodes de chunking dépend vraiment du dataset, soulignant combien la structure du contenu joue un rôle crucial :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 9 : Comparaison des tailles de chunks avec chunking naïf et tardif.</span></figcaption></figure><p>Comme nous pouvons le voir, le chunking tardif surpasse généralement le chunking naïf avec des tailles de chunks plus petites, car les chunks naïfs plus petits sont trop réduits pour contenir beaucoup de contexte, tandis que les chunks tardifs plus petits conservent le contexte du document entier, les rendant plus significatifs sémantiquement. L'exception à cela est le dataset NarrativeQA où il y a simplement tellement de contexte non pertinent que le chunking tardif est moins performant. Avec des tailles de chunks plus grandes, le chunking naïf montre une amélioration marquée (dépassant occasionnellement le chunking tardif) grâce au contexte accru, tandis que la performance du chunking tardif diminue progressivement.</p><h2 id=\"takeaways-when-to-use-what\">Conclusions : Quand Utiliser Quoi ?</h2><p>Dans cet article, nous avons examiné différents types de tâches de recherche de documents pour mieux comprendre quand utiliser la segmentation et quand le chunking tardif aide. Alors, qu'avons-nous appris ?</p><h3 id=\"when-should-i-use-long-context-embedding\">Quand Devrais-je Utiliser l'Embedding à Long Contexte ?</h3><p>En général, inclure autant de texte de vos documents que possible dans l'entrée de votre modèle d'embedding ne nuit pas à la précision de la recherche. Cependant, les modèles d'embedding à long contexte se concentrent souvent sur le début des documents, car ils contiennent du contenu comme les titres et l'introduction qui sont plus importants pour juger de la pertinence, mais les modèles peuvent manquer du contenu au milieu du document.</p><h3 id=\"when-should-i-use-naive-chunking\">Quand Devrais-je Utiliser le Chunking Naïf ?</h3><p>Lorsque les documents couvrent plusieurs aspects, ou que les requêtes des utilisateurs ciblent des informations spécifiques dans un document, le chunking améliore généralement les performances de recherche.</p><p>Finalement, les décisions de segmentation dépendent de facteurs comme la nécessité d'afficher du texte partiel aux utilisateurs (par exemple, comme Google présente les passages pertinents dans les aperçus des résultats de recherche), ce qui rend la segmentation essentielle, ou des contraintes de calcul et de mémoire, où la segmentation peut être moins favorable en raison de la surcharge de recherche accrue et de l'utilisation des ressources.</p><h3 id=\"when-should-i-use-late-chunking\">Quand Devrais-je Utiliser le Chunking Tardif ?</h3><p>En encodant le document complet avant de créer des chunks, le chunking tardif résout le problème des segments de texte perdant leur signification en raison d'un contexte manquant. Cela fonctionne particulièrement bien avec des documents cohérents, où chaque partie est liée à l'ensemble. Nos expériences montrent que le chunking tardif est particulièrement efficace lors de la division du texte en plus petits chunks, comme démontré dans notre <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">article</a>. Cependant, il y a une mise en garde : si des parties du document n'ont pas de rapport entre elles, inclure ce contexte plus large peut en fait dégrader les performances de recherche, car cela ajoute du bruit aux embeddings.</p><h2 id=\"conclusion\">Conclusion</h2><p>Le choix entre l'embedding à long contexte, le chunking naïf et le chunking tardif dépend des exigences spécifiques de votre tâche de recherche. Les embeddings à long contexte sont précieux pour les documents cohérents avec des requêtes générales, tandis que le chunking excelle dans les cas où les utilisateurs recherchent des faits ou des informations spécifiques dans un document. Le chunking tardif améliore davantage la recherche en conservant la cohérence contextuelle dans des segments plus petits. En fin de compte, la compréhension de vos données et de vos objectifs de recherche guidera l'approche optimale, équilibrant précision, efficacité et pertinence contextuelle.</p><p>Si vous explorez ces stratégies, envisagez d'essayer <code>jina-embeddings-v3</code>—ses capacités avancées de long contexte, son chunking tardif et sa flexibilité en font un excellent choix pour divers scénarios de recherche.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "Comparaison des performances des modèles d'embedding à contexte long selon différentes stratégies de découpage pour trouver l'approche optimale adaptée à vos besoins.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
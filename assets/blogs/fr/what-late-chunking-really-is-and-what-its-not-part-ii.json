{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "Ce qu'est vraiment le Late Chunking et ce qu'il n'est pas : Partie II",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Il est fortement recommand√© de <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">lire d'abord la Partie I</a>, car cet article offre une vue plus approfondie, en se concentrant sur les malentendus courants et les comparaisons. <b><strong style=\"white-space: pre-wrap;\">Ordre de lecture recommand√© : </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">partie I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, partie II, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">article de recherche</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking : Embeddings de Chunks Contextuels Utilisant des Mod√®les d'Embedding √† Contexte Long</div><div class=\"kg-bookmark-description\">De nombreux cas d'utilisation n√©cessitent de r√©cup√©rer des portions plus petites de texte, et les syst√®mes de recherche bas√©s sur des vecteurs denses fonctionnent souvent mieux avec des segments de texte plus courts, car la s√©mantique est moins susceptible d'√™tre sur-compress√©e dans les embeddings. Par cons√©quent, les praticiens divisent souvent les documents texte en plus petits chunks et les encodent s√©par√©ment. Cependant, les embeddings de chunks cr√©√©s de cette mani√®re peuvent perdre des informations contextuelles des chunks environnants, entra√Ænant des repr√©sentations sous-optimales. Dans cet article, nous pr√©sentons une nouvelle m√©thode appel√©e late chunking, qui utilise des mod√®les d'embedding √† contexte long pour d'abord encoder tous les tokens du texte long, avec le chunking appliqu√© apr√®s le mod√®le transformer et juste avant le mean pooling - d'o√π le terme \"late\" dans son nom. Les embeddings de chunks r√©sultants capturent l'information contextuelle compl√®te, conduisant √† des r√©sultats sup√©rieurs dans diverses t√¢ches de recherche. La m√©thode est suffisamment g√©n√©rique pour √™tre appliqu√©e √† une large gamme de mod√®les d'embedding √† contexte long et fonctionne sans entra√Ænement suppl√©mentaire. Pour augmenter davantage l'efficacit√© du late chunking, nous proposons une approche de fine-tuning d√©di√©e pour les mod√®les d'embedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Le chunking d'un long document pr√©sente deux probl√®mes : premi√®rement, <strong>d√©terminer les points de rupture</strong>‚Äîc'est-√†-dire comment segmenter le document. Vous pouvez envisager des longueurs fixes de tokens, un nombre fixe de phrases, ou des techniques plus avanc√©es comme <a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">regex ou des mod√®les de segmentation s√©mantique</a>. Des limites de chunks pr√©cises am√©liorent non seulement la lisibilit√© des r√©sultats de recherche, mais garantissent aussi que les chunks fournis √† un LLM dans un syst√®me RAG sont pr√©cis et suffisants‚Äîni plus, ni moins.</p><p>Le second probl√®me est la <strong>perte de contexte</strong> dans chaque chunk. Une fois le document segment√©, l'√©tape logique suivante pour la plupart des gens est d'encoder chaque chunk s√©par√©ment dans un processus par lots. Cependant, cela conduit √† une perte du contexte global du document original. De nombreux travaux ant√©rieurs ont d'abord abord√© le premier probl√®me, soutenant qu'une meilleure d√©tection des limites am√©liore la repr√©sentation s√©mantique. Par exemple, le \"chunking s√©mantique\" regroupe les phrases ayant une similarit√© cosinus √©lev√©e dans l'espace d'embedding pour minimiser la perturbation des unit√©s s√©mantiques.</p><p>De notre point de vue, ces deux probl√®mes sont <em>presque</em> orthogonaux et peuvent √™tre trait√©s s√©par√©ment. Si nous devions √©tablir une priorit√©, <strong>nous dirions que le 2√®me probl√®me est plus critique.</strong></p>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">Probl√®me 2 : <b>Information contextuelle</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">Pr√©serv√©e</td>\n<td>Perdue</td>\n</tr>\n<tr>\n<td><b>Probl√®me 1 : Points de rupture</b></td>\n<td>Bons</td>\n<td style=\"text-align:center\">Sc√©nario id√©al</td>\n<td>Mauvais r√©sultats de recherche</td>\n</tr>\n<tr>\n<td></td>\n<td>Mauvais</td>\n<td style=\"text-align:center\">Bons r√©sultats de recherche, mais les r√©sultats peuvent ne pas √™tre lisibles par l'humain ou pour le raisonnement LLM</td>\n<td>Pire sc√©nario</td>\n</tr>\n</tbody>\n</table>\n\n<h2 id=\"late-chunking-for-context-loss\">Late Chunking pour la Perte de Contexte</h2><p>Le <strong>late chunking</strong> commence par aborder le second probl√®me : la <strong>perte de contexte</strong>. Il ne s'agit <em>pas</em> de trouver les points de rupture ou les fronti√®res s√©mantiques id√©aux. Vous devez toujours utiliser regex, des heuristiques ou d'autres techniques pour diviser un long document en petits chunks. Mais au lieu d'encoder chaque chunk d√®s qu'il est segment√©, le late chunking encode d'abord l'ensemble du document dans une fen√™tre de contexte (pour <code>jina-embeddings-v3</code> c'est 8192 tokens). Ensuite, il suit les indices de fronti√®re pour appliquer le mean pooling pour chaque chunk‚Äîd'o√π le terme \"late\" dans late chunking.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagramme comparant les m√©thodes &quot;Naive Chunking&quot; et &quot;Late Chunking&quot; pour le traitement de longs documents avec des √©tapes √©tiquet√©es.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Le late chunking n√©cessite toujours des indices de fronti√®re, mais la diff√©rence cl√© r√©side dans le moment o√π ces indices sont utilis√©s. Dans le late chunking, les indices ne sont appliqu√©s qu'apr√®s que l'ensemble du document a √©t√© encod√©, et ils sont utilis√©s pour d√©terminer la plage de pooling.</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">Le Late Chunking est R√©sistant aux Mauvais Indices de Fronti√®re</h2><p>Ce qui est vraiment int√©ressant, c'est que les exp√©riences montrent que le late chunking √©limine le besoin de fronti√®res s√©mantiques parfaites, ce qui r√©sout partiellement le premier probl√®me mentionn√© ci-dessus. En fait, le late chunking appliqu√© aux fronti√®res de tokens fixes surpasse le chunking na√Øf avec des indices de fronti√®re s√©mantique. Les mod√®les de segmentation simples, comme ceux utilisant des fronti√®res de longueur fixe, fonctionnent aussi bien que les algorithmes avanc√©s de d√©tection de fronti√®res lorsqu'ils sont associ√©s au late chunking. Nous avons test√© trois tailles diff√©rentes de mod√®les d'embedding, et les r√©sultats montrent qu'ils b√©n√©ficient tous constamment du late chunking sur tous les jeux de donn√©es de test. Cela dit, le mod√®le d'embedding lui-m√™me reste le facteur le plus significatif dans la performance‚Äî<strong>il n'y a pas un seul cas o√π un mod√®le plus faible avec late chunking surpasse un mod√®le plus fort sans lui.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Graphique de dispersion montrant le pourcentage d'am√©liorations relatives √† travers diff√©rents mod√®les par rapport √† une r√©f√©rence\" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">Am√©lioration relative de la recherche par rapport √† la r√©f√©rence (c'est-√†-dire </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> avec des indices de fronti√®re de longueur de token fixe et chunking na√Øf). Dans le cadre d'une √©tude d'ablation, nous avons test√© le late chunking avec diff√©rents indices de fronti√®re (longueur de token fixe, fronti√®res de phrases et fronti√®res s√©mantiques) et diff√©rents mod√®les (</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\">, et </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">). Bas√© sur leurs performances sur MTEB, le classement de ces trois mod√®les d'embedding est : </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">. Cependant, l'objectif de cette exp√©rience n'est pas d'√©valuer la performance des mod√®les d'embedding eux-m√™mes, mais de comprendre comment un meilleur mod√®le d'embedding interagit avec le late chunking et les indices de fronti√®re. Pour les d√©tails de l'exp√©rience, veuillez consulter notre article de recherche.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Notez que le fait d'√™tre r√©sistant aux mauvaises fronti√®res ne signifie pas qu'on peut les ignorer ‚Äî elles restent importantes pour la lisibilit√© humaine et celle des LLM. Voici notre point de vue : lors de l'optimisation de la segmentation, c'est-√†-dire le 1er probl√®me mentionn√© pr√©c√©demment, nous pouvons nous concentrer enti√®rement sur la lisibilit√© sans nous soucier de la perte s√©mantique/contextuelle. Le Late Chunking g√®re bien ou mal les points de rupture, donc la lisibilit√© est la seule chose dont vous devez vous soucier.</p>\n\n<h2 id=\"late-chunking-is-bidirectional\">Le Late Chunking est Bidirectionnel</h2>\n\n<p>Une autre id√©e fausse courante concernant le late chunking est que ses embeddings de chunks conditionnels ne d√©pendent que des chunks pr√©c√©dents sans \"regarder en avant\". C'est incorrect. La d√©pendance conditionnelle dans le <strong>late chunking est en r√©alit√© bidirectionnelle</strong>, et non unidirectionnelle. Cela est d√ª au fait que la matrice d'attention dans le mod√®le d'embedding ‚Äî un transformeur encodeur seul ‚Äî est enti√®rement connect√©e, contrairement √† la matrice triangulaire masqu√©e utilis√©e dans les mod√®les auto-r√©gressifs. Formellement, l'embedding du chunk $k$, $v_k \\sim Q(c_k|D)$, plut√¥t que $v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$, o√π $Q$ d√©signe une factorisation du mod√®le de langage. Cela explique √©galement pourquoi le late chunking ne d√©pend pas d'un placement pr√©cis des fronti√®res.</p>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagrams of a transformer model with detailed encoder on the left and decoder on the right, labeled with tokens, embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">Contrairement aux mod√®les d√©codeur seul avec auto-attention masqu√©e, les mod√®les d'embedding sont g√©n√©ralement encodeur seul avec une matrice d'attention compl√®te. Cela signifie que chaque embedding de token est conditionn√© par tous les autres tokens dans la m√™me fen√™tre contextuelle, qui, dans le cas de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">, inclut jusqu'√† 8191 autres tokens. Par cons√©quent, l'embedding de chunk porte des informations de contexte global dans les deux directions.</span></figcaption></figure>\n\n<h2 id=\"late-chunking-can-be-trained\">Le Late Chunking peut √™tre Entra√Æn√©</h2>\n\n<p>Le late chunking ne n√©cessite <em>pas</em> d'entra√Ænement suppl√©mentaire pour les mod√®les d'embedding. Il peut √™tre appliqu√© √† n'importe quel mod√®le d'embedding √† contexte long utilisant le mean pooling, ce qui le rend tr√®s attractif pour les praticiens. Cela dit, si vous travaillez sur des t√¢ches comme la question-r√©ponse ou la recherche document-requ√™te, les performances peuvent encore √™tre am√©lior√©es avec un peu de fine-tuning. Plus pr√©cis√©ment, les donn√©es d'entra√Ænement se composent de tuples contenant :</p>\n\n<ul>\n<li>Une <strong>requ√™te</strong> (par exemple, une question ou un terme de recherche).</li>\n<li>Un <strong>document</strong> qui contient des informations pertinentes pour r√©pondre √† la requ√™te.</li>\n<li>Un <strong>segment pertinent</strong> dans le document, qui est la partie sp√©cifique du texte qui r√©pond directement √† la requ√™te.</li>\n</ul>\n\n<p>Le mod√®le est entra√Æn√© en associant les requ√™tes √† leurs segments pertinents, en utilisant une fonction de perte contrastive comme InfoNCE. Cela garantit que les segments pertinents sont √©troitement align√©s avec la requ√™te dans l'espace des embeddings, tandis que les segments non li√©s sont repouss√©s plus loin. Ainsi, le mod√®le apprend √† se concentrer sur les parties les plus pertinentes du document lors de la g√©n√©ration des embeddings de chunks. <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Pour plus de d√©tails, veuillez consulter notre article de recherche.</a></p>\n\n<h2 id=\"late-chunking-vs-contextual-retrieval\">Late Chunking vs. Recherche Contextuelle</h2>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Introducing Contextual Retrieval</div><div class=\"kg-bookmark-description\">Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n\n<p>Peu apr√®s l'introduction du late chunking, Anthropic a introduit une strat√©gie distincte appel√©e <strong>Recherche Contextuelle</strong>. La m√©thode d'Anthropic est une approche force brute pour r√©soudre le probl√®me de la perte de contexte, et fonctionne comme suit :</p>\n\n<ol>\n<li>Chaque chunk est envoy√© au LLM avec le document complet.</li>\n<li>Le LLM ajoute du contexte pertinent √† chaque chunk.</li>\n<li>Cela r√©sulte en des embeddings plus riches et plus informatifs.</li>\n</ol>\n\n<p>Selon nous, il s'agit essentiellement d'un <strong>enrichissement de contexte</strong>, o√π le contexte global est explicitement cod√© en dur dans chaque chunk en utilisant un LLM, ce qui est co√ªteux en termes de <strong>co√ªt</strong>, de <strong>temps</strong> et de <strong>stockage</strong>. De plus, il n'est pas certain que cette approche soit r√©sistante aux fronti√®res de chunks, car le LLM s'appuie sur des chunks pr√©cis et lisibles pour enrichir efficacement le contexte. En revanche, le late chunking est tr√®s r√©sistant aux indices de fronti√®re, comme d√©montr√© ci-dessus. Il ne n√©cessite pas de stockage suppl√©mentaire puisque la taille de l'embedding reste la m√™me. Bien qu'il exploite la longueur de contexte compl√®te du mod√®le d'embedding, <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">il reste significativement plus rapide que l'utilisation d'un LLM pour g√©n√©rer l'enrichissement</a>. Dans l'√©tude qualitative de notre article de recherche, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">nous montrons que la recherche contextuelle d'Anthropic offre des performances similaires au late chunking.</a> Cependant, le late chunking fournit une solution plus bas niveau, g√©n√©rique et naturelle en exploitant les m√©caniques inh√©rentes du transformeur encodeur seul.</p>\n\n<h2 id=\"which-embedding-models-support-late-chunking\">Quels Mod√®les d'Embedding Supportent le Late Chunking ?</h2>\n\n<p>Le late chunking n'est pas exclusif √† <code>jina-embeddings-v3</code> ou <code>v2</code>. C'est une approche assez g√©n√©rique qui peut √™tre appliqu√©e √† n'importe quel mod√®le d'embedding √† contexte long utilisant le mean pooling. Par exemple, dans cet article, nous montrons que <code>nomic-v1</code> le supporte √©galement. Nous encourageons vivement tous les fournisseurs d'embedding √† impl√©menter le support du late chunking dans leurs solutions.</p>\n\n<p>En tant qu'utilisateur de mod√®le, lorsque vous √©valuez un nouveau mod√®le ou une nouvelle API d'embedding, vous pouvez suivre ces √©tapes pour v√©rifier s'il pourrait supporter le late chunking :</p>\n\n<ol><li><strong>Sortie unique</strong> : Le mod√®le/API ne vous donne-t-il qu'un seul embedding final par phrase au lieu d'embeddings au niveau des tokens ? Si oui, il ne pourra probablement pas prendre en charge le chunking tardif (en particulier pour les API web).</li><li><strong>Support des longs contextes</strong> : Le mod√®le/API g√®re-t-il des contextes d'au moins 8192 tokens ? Si non, le chunking tardif ne sera pas applicable ‚Äî ou plus pr√©cis√©ment, cela n'a pas de sens d'adapter le chunking tardif pour un mod√®le √† contexte court. Si oui, assurez-vous qu'il fonctionne r√©ellement bien avec les longs contextes, et pas seulement qu'il pr√©tend les supporter. Vous pouvez g√©n√©ralement trouver ces informations dans le rapport technique du mod√®le, comme les √©valuations sur LongMTEB ou d'autres benchmarks de longs contextes.</li><li><strong>Mean Pooling</strong> : Pour les mod√®les auto-h√©berg√©s ou les API qui fournissent des embeddings au niveau des tokens avant le pooling, v√©rifiez si la m√©thode de pooling par d√©faut est le mean pooling. Les mod√®les utilisant CLS ou max pooling ne sont pas compatibles avec le chunking tardif.</li></ol><p>En r√©sum√©, si un mod√®le d'embedding supporte les longs contextes et utilise le mean pooling par d√©faut, il peut facilement prendre en charge le chunking tardif. Consultez notre <a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">d√©p√¥t GitHub pour les d√©tails d'impl√©mentation et plus de discussions</a>.</p><h2 id=\"conclusion\">Conclusion</h2><p>Alors, qu'est-ce que le chunking tardif ? Le chunking tardif est une m√©thode simple pour g√©n√©rer des embeddings de chunks en utilisant des mod√®les d'embedding √† long contexte. C'est rapide, r√©sistant aux indices de fronti√®re et tr√®s efficace. Ce n'est pas une heuristique ou du sur-engineering ‚Äî c'est une conception r√©fl√©chie ancr√©e dans une compr√©hension approfondie du m√©canisme transformer.</p><p>Aujourd'hui, l'engouement autour des LLM est ind√©niable. Dans de nombreux cas, des probl√®mes qui pourraient √™tre efficacement trait√©s par des mod√®les plus petits comme BERT sont plut√¥t confi√©s aux LLM, motiv√©s par l'attrait de solutions plus grandes et plus complexes. Il n'est pas surprenant que les grands fournisseurs de LLM poussent √† une plus grande adoption de leurs mod√®les, tandis que les fournisseurs d'embeddings d√©fendent les embeddings ‚Äî les deux jouent sur leurs forces commerciales. Mais au final, ce n'est pas une question d'engouement, c'est une question d'action, de ce qui fonctionne vraiment. Laissons la communaut√©, l'industrie et, surtout, le temps r√©v√©ler quelle approche est vraiment plus l√©g√®re, plus efficace et construite pour durer.</p><p>Assurez-vous de lire <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">notre article de recherche</a>, et nous vous encourageons √† tester le chunking tardif dans divers sc√©narios et √† partager vos retours avec nous.</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "Partie 2 de notre exploration du Late Chunking, une analyse approfondie expliquant pourquoi c'est la meilleure m√©thode pour les embeddings de chunks et l'am√©lioration des performances de recherche/RAG.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "Que devrions-nous apprendre de ModernBERT ?",
  "html": "<p>En 2018, Google a lancé BERT qui a révolutionné le NLP, bien avant la vague actuelle des LLM. Même aujourd'hui, de nombreux Small Language Models sont basés sur BERT. En décembre 2024, <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> applique les enseignements tirés des développements récents des LLM à ces modèles plus petits. Les points clés ? Une meilleure efficacité des paramètres, la compréhension du code et la gestion des longs contextes.</p><p>Dans cet article, nous allons comparer ModernBERT à deux modèles que nous connaissons parfaitement : <code>jina-XLM-RoBERTa</code> (le socle multilingue de <code>jina-embeddings-v3</code>) et <code>RoBERTa-large</code>. Examinons chaque modèle :</p><ul><li><strong>ModernBERT </strong>(déc. 2024) est un SLM récemment publié, développé en collaboration par Answer.AI, LightOn et HuggingFace. Il utilise des optimisations modernes comme RoPE pour une fenêtre de contexte de 8 192 tokens et des <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">couches GeGLU</a>, améliorant les performances tout en maintenant l'efficacité.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong> </strong>(sept. 2024) est un modèle d'embedding de texte multilingue basé sur le <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> de Meta. Alors que le <code>XLM-RoBERTa</code> original améliore <code>RoBERTa</code> en utilisant le grand dataset multilingue XLM, <code>jina-XLM-RoBERTa</code> va plus loin avec l'entraînement sur contexte étendu, l'implémentation de <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a>, et le support de <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a>. Ce modèle sert de base à <code>jina-embeddings-v3</code>.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a> (juillet 2019) développé par Meta, est une version améliorée de BERT avec 355 millions de paramètres. Grâce à un entraînement prolongé, des datasets plus importants et des innovations comme le masquage dynamique, il a obtenu des résultats impressionnants sur des benchmarks clés comme <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a>, et <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a>. Cela le rend adapté à diverses tâches NLP, de la classification de texte aux questions-réponses.</li></ul><p>En comparant ces modèles selon trois aspects fondamentaux, nous visons à mettre en évidence les choix de conception efficaces de ModernBERT pour les développeurs de modèles et à identifier des insights clés pour le développement des futurs modèles de type BERT. Nous partagerons également nos apprentissages du développement de <code>jina-embeddings-v3</code> et discuterons des améliorations prévues pour <code>jina-embeddings-v4</code> et <code>jina-reranker-v3</code>.</p><h2 id=\"modernberts-parameter-efficiency\">L'efficacité des paramètres de ModernBERT</h2><p>Examinons d'abord l'approche de ModernBERT concernant l'efficacité des paramètres - il intègre plusieurs insights clés des développements récents des LLM. ModernBERT s'appuie sur trois stratégies principales : une architecture plus profonde mais plus fine, une taille de vocabulaire contrôlée, et une mise à l'échelle progressive du modèle partant de modèles plus petits.</p><h3 id=\"deep-and-thin-architecture\">Architecture profonde et fine</h3><p>ModernBERT-large va plus profond avec 28 couches, alors que <code>jina-XLM-RoBERTa</code> et <code>RoBERTa-large</code> en ont 24. Mais voici la partie intéressante - il égale <code>RoBERTa-large</code> en nombre de paramètres malgré ces couches supplémentaires. <code>jina-XLM-RoBERTa</code> nécessite plus de paramètres car il gère 89 langues, tandis que les deux autres se concentrent uniquement sur l'anglais.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">La profondeur (nombre de couches) est plus importante que la largeur (nombre d'unités cachées) pour les petits LLM. Une structure de modèle profonde et fine excelle dans la capture des concepts abstraits, aboutissant à de meilleures performances finales.</span></figcaption></figure><p>La majorité des paramètres d'un transformer provient des couches d'attention et entièrement connectées. ModernBERT reste compétitif en termes de taille en étant plus \"fin\" - il utilise 2 624 unités cachées sur 28 couches, comparé aux 4 096 unités de RoBERTa-large sur 24 couches. Cette configuration plus \"profonde\" mais plus fine leur permet d'atteindre leurs objectifs de performance sans gonfler le modèle.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parameters</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Hidden states</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Intermediate dims</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Attention heads</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Vocabulary size</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Cette approche s'aligne avec la recherche <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> de Meta, qui a découvert que pour les petits modèles, la profondeur est plus importante que la largeur pour capturer des motifs complexes et améliorer les performances. Essentiellement, la capacité de traiter l'information à travers plus de couches transformer s'avère plus précieuse que d'avoir des couches plus larges pour le traitement parallèle.</p><p>Examinons les données sur les performances de cette architecture profonde et fine.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">Comparé aux modèles similaires utilisant l'architecture traditionnelle peu profonde et large, ModernBERT obtient de meilleurs résultats sur les tâches clés comme la recherche et STS - tout en gardant un nombre de paramètres similaire.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Average</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Prenons <code>jina-XLM-RoBERTa</code> - il s'appuie sur l'architecture peu profonde et large de <code>RoBERTa-large</code> mais augmente le vocabulaire de 50K à 250K tokens et s'entraîne sur plus de données. Pourtant ModernBERT le surpasse légèrement, suggérant que le changement architectural fait une réelle différence en termes d'efficacité.</p><h3 id=\"vocabulary-size-matters\">La taille du vocabulaire est importante</h3><p>Regardons d'abord comment les paramètres de vocabulaire sont comptés dans les transformers. Pour tout transformer, <code>paramètres de vocabulaire = nombre de tokens distincts × taille cachée</code>. Prenons <code>jina-XLM-RoBERTa</code> : avec 250K tokens et 1 024 dimensions, il nécessite 256M paramètres juste pour l'encodage du vocabulaire - avant même de gérer les tâches linguistiques réelles !</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">Dans les transformers, la première couche convertit les tokens en états cachés en utilisant une matrice de poids, à savoir les poids du vocabulaire. Si l'on considère l'utilisation de tous les points de code UTF-8 (1 112 064) avec 1 024 dimensions cachées - il faudrait un énorme </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> paramètres juste pour la conversion des tokens. Bien que les grands LLM (plus de 100B paramètres) puissent gérer cette surcharge, c'est une contrainte sérieuse pour les modèles plus petits. C'est exactement pourquoi nous utilisons des tokenizers comme BPE, qui fusionnent efficacement les points de code UTF-8 communs en tokens uniques.</span></figcaption></figure><p>Mais voici le point important : <strong>les poids du vocabulaire ne contribuent pas aux mécanismes d'attention - ce sont juste des tables de consultation.</strong> Pour les SLM travaillant avec des budgets de paramètres fixes, un vocabulaire plus large signifie moins de paramètres disponibles pour les couches d'attention, qui effectuent le véritable traitement du langage. Cela explique pourquoi ModernBERT-large monolingue anglais surpasse le multilingue <code>jina-XLM-RoBERTa</code> malgré sa plus petite taille - <code>jina-XLM-RoBERTa</code> alloue plus de paramètres (47 %) pour prendre en charge plusieurs langues. Le vocabulaire ciblé de ModernBERT améliore non seulement les performances mais accélère aussi l'inférence, le rendant particulièrement efficace pour les applications aux ressources limitées.</p><p>Donc si nous regardons maintenant <em>uniquement</em> les paramètres du modèle principal (en excluant les poids du vocabulaire), ModernBERT dispose en réalité de plus de puissance de calcul que ses pairs : ModernBERT consacre 19 % de paramètres en plus à la modélisation <em>réelle</em> du langage que <code>jina-XLM-RoBERTa</code> et 15 % de plus que <code>RoBERTa-large</code> !</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Spécifications du modèle</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Support des langues</td>\n<td>Anglais uniquement</td>\n<td>89 langues</td>\n<td>Anglais uniquement</td>\n</tr>\n<tr>\n<td>Taille du vocabulaire</td>\n<td>50,4K</td>\n<td>250K</td>\n<td>50,3K</td>\n</tr>\n<tr>\n<td>Paramètres totaux</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Paramètres du vocabulaire</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>Ratio des paramètres du vocabulaire</td>\n<td>13 %</td>\n<td>47 %</td>\n<td>14 %</td>\n</tr>\n<tr>\n<td>Paramètres du modèle principal</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">Mise à l'échelle du modèle par \"Weight Tiling\"</h3><p>En construisant le backbone <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a>, nous avons constaté que l'entraînement des SLM à partir de zéro était gourmand en ressources et complexe. ModernBERT résout ce problème avec une approche d'initialisation intelligente appelée <strong>weight tiling</strong> - essentiellement en initialisant ModernBERT-large à partir des poids de sa version base plus petite.</p><p>Cette technique n'est pas entièrement nouvelle - elle s'appuie sur le travail de DeepMind avec <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> et apparaît aussi dans les modèles <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2</a> de Microsoft. Mais son application ici est particulièrement efficace pour résoudre le goulot d'étranglement de l'entraînement des SLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT passe de 22 à 28 couches en utilisant la stratégie d'initialisation en profondeur de l'équipe Gopher. Pour ces couches supplémentaires (23-28), ils initialisent chacune en utilisant les poids des 22 couches d'origine de ModernBERT-base. Pour les matrices de poids de chaque couche, ils utilisent l'approche de pavage central de Phi-2. Voici comment ça fonctionne : ils prennent les poids de ModernBERT-base et les placent au centre des matrices de ModernBERT-large. Pour les bords encore vides ? Ils enroulent les poids d'origine de manière cyclique pour les remplir.</span></figcaption></figure><p>Cette stratégie d'initialisation donne à ModernBERT-large un avantage significatif - au lieu de partir de zéro, il exploite les motifs pré-appris de sa version plus petite. Elle s'est révélée particulièrement <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">efficace pour la mise à l'échelle des modèles de langage de cette taille</a>.</p><blockquote>Nous constatons qu'un modèle initialisé à chaud se remet rapidement d'une perte initiale élevée (due aux paramètres ajoutés) pour atteindre une perte très proche de celle du modèle de base. Nous sommes capables d'étendre 417M paramètres par plus de 3× en taille tout en maintenant des performances supérieures à un modèle équivalent entraîné à partir de zéro jusqu'à convergence, ce qui implique que les gains n'étaient pas limités au début de l'entraînement. Cependant, pour des tailles plus importantes, les gains relatifs obtenus à la convergence diminuent, en particulier avec les extensions en largeur.</blockquote><p>L'enroulement cyclique des poids n'est pas qu'une commodité - il s'aligne bien avec la façon dont les matrices d'attention présentent naturellement des motifs périodiques. La recherche de Gopher montre que cette approche excelle particulièrement pour les SLM (moins de 9B paramètres), bien que les avantages commencent à s'estomper lorsqu'on passe à des modèles plus grands.</p><h2 id=\"modernberts-code-modeling\">Modélisation du code par ModernBERT</h2><p>ModernBERT apporte une approche spécialisée à la compréhension du code avec son tokenizer optimisé pour le code et ses données d'entraînement. Cet ajustement fin pour le traitement du code se révèle payant tant dans les tâches de compréhension que de recherche.</p><p>Nous avons effectué un benchmark en utilisant le corpus <code>jina-embeddings-v2-code</code>, comparant trois modèles comme backbones : <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, et <code>RoBERTa-large</code>. Le test ? <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - faire correspondre des descriptions textuelles à des extraits de code. ModernBERT a surpassé les deux alternatives sur tous les points.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">L'écart est logique - ni </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> ni </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> n'ont été exposés aux langages de programmation pendant l'entraînement. Pendant ce temps, ModernBERT-large s'est entraîné sur deux billions de tokens, incluant une quantité substantielle de code. Cette exposition à la syntaxe et aux motifs de programmation lui donne un net avantage dans les tâches liées au code. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> devance légèrement </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">, probablement en raison de ses données d'entraînement multilingues plus importantes - même architecture, plus d'exposition. Néanmoins, les deux sont largement dépassés par ModernBERT-large.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Tâche</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">L'avantage du Tokenizer</h3><p>Examinons pourquoi ModernBERT gère si bien le code - il utilise le <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">tokenizer OLMo</a>, qui a été spécifiquement entraîné sur du code, plutôt que les tokenizers BERT/RoBERTa standard.</p><p>Un tokenizer découpe le texte UTF-8 en tokens qui sont mappés en vecteurs - c'est ce que le modèle traite réellement. Pendant l'entraînement, il apprend à combiner les séquences de caractères fréquentes en tokens uniques. La différence ? Un tokenizer standard pourrait découper <code>init</code> en <code>in</code> + <code>it</code>, ignorant le contexte de programmation. Mais le tokenizer de ModernBERT, conscient du code, le comprend sans le découper.</p><p>Voici où ça devient intéressant avec la gestion des espaces : ModernBERT préserve les espaces en début de ligne Python comme des tokens uniques et différencie 4 vs 8 espaces - crucial pour la structure du code. Pendant ce temps, <strong><code>jina-XLM-RoBERTa</code> réduit tous les espaces continus en un seul <code>_</code>, et RoBERTa-large traite chaque espace comme son propre token.</strong> Cela signifie que l'encodeur de ModernBERT reçoit une entrée plus propre et plus significative lors du traitement du code, tandis que les autres travaillent avec des tokens fragmentés et moins cohérents.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT préserve les espaces en début de ligne Python comme des tokens uniques et différencie 4 vs 8 espaces - crucial pour la structure du code ; tandis que les autres travaillent avec des tokens fragmentés et moins cohérents.</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">Gestion du contexte long par ModernBERT</h2><p>ModernBERT a fait des progrès significatifs dans le traitement des textes longs, grâce à son vaste corpus d'entraînement (300B tokens avec des échantillons de 8 192 tokens) et des techniques avancées comme l'attention globale et locale combinée.</p><p>Pour évaluer les capacités de gestion des documents longs, nous avons utilisé le <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">dataset MLDR</a> - un benchmark complet de textes longs couvrant 13 langues. Comme ModernBERT ne prend actuellement en charge que l'anglais, nous nous sommes concentrés sur le sous-ensemble anglais de MLDR pour comparer ModernBERT à <code>jina-XLM-RoBERTa</code>. Bien que ces deux modèles puissent gérer des entrées de 8K tokens, <code>RoBERTa-large</code> a été exclu de ce benchmark en raison de sa limite de 512 tokens, insuffisante pour l'analyse de textes longs.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La performance supérieure de ModernBERT n'est pas seulement due à son entraînement extensif sur des textes longs - c'est largement grâce à sa combinaison innovante de mécanismes d'attention globale et locale. Contrairement à <code>jina-XLM-RoBERTa</code>, qui applique une attention globale coûteuse en calcul à chaque couche, ModernBERT adopte une approche plus efficace. Il alterne entre l'attention globale (utilisée tous les trois couches avec un <code>theta</code> de 160 000) et l'attention locale (utilisant une fenêtre glissante de 128 tokens avec un <code>theta</code> de 100 000). Cette stratégie hybride maintient une performance élevée tout en réduisant considérablement le temps d'entraînement.</p><blockquote>Dans ModernBERT, chaque troisième couche utilise une attention globale avec un RoPE theta de 160 000 et les couches restantes utilisent une fenêtre glissante locale de 128 tokens avec un RoPE theta de 10 000. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">La leçon amère ?</h2><p>La loi d'échelle et <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">la leçon amère</a> suggèrent que les améliorations majeures de performance proviennent principalement de l'augmentation du nombre de paramètres et des données d'entraînement. Ce principe a guidé notre approche d'expansion du corpus et d'utilisation de LoRA pour les adaptations spécifiques aux tâches.</p><p>Cependant, le succès de ModernBERT a révélé que nous avions sous-estimé le pouvoir de l'optimisation architecturale. Il démontre que les SLM peuvent atteindre des résultats exceptionnels grâce à une meilleure efficacité données-modèle, sans nécessairement augmenter les paramètres. Un récent <a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">rapport technique sur Stella Embeddings</a> renforce cette conclusion, indiquant que les méthodes actuelles d'entraînement des modèles d'embedding peuvent être améliorées sans augmenter la taille du corpus ou du modèle.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graphique montrant la loi d'échelle des modèles d'embedding avec la 'Taille des paramètres' sur l'axe x et 'Performance MTEB' sur l'axe y\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Loi d'échelle des modèles d'embedding. La performance moyenne MTEB sur les tâches en anglais est tracée en fonction du nombre de paramètres du modèle. Chaque point représente un modèle d'embedding. La ligne de tendance, représentant tous les modèles, est mise en évidence, avec les modèles multilingues en cyan. On peut voir que </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> démontre une performance supérieure par rapport aux modèles de taille similaire, montrant également une amélioration superlinéaire par rapport à son prédécesseur, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">. Ce graphique a été créé en sélectionnant les 100 meilleurs modèles d'embedding du classement MTEB, excluant ceux sans information de taille, typiquement les modèles propriétaires ou à code source fermé. Les soumissions identifiées comme du trolling évident ont également été filtrées.</span></figcaption></figure><p>À l'avenir, nous anticipons des coûts de calcul plus faibles et des tailles de modèles plus petites à mesure que nous acquérons une compréhension plus approfondie de l'utilisation des données et mettons en œuvre les techniques de ModernBERT. À court terme, nous pouvons mettre en œuvre les améliorations simples décrites dans l'article ModernBERT - en particulier l'intégration de plus de données liées au code et l'adoption d'un tokenizer adapté au code. Des changements plus complexes, comme le passage à une architecture profonde et fine ou l'amorçage de grands modèles à partir de plus petits, nécessiteront de construire des modèles de base à partir de zéro - une initiative à moyen terme.</p><p>Bien que l'efficacité de ModernBERT soit remarquable, sa limitation au texte seul indique des défis futurs. Alors que les modèles d'embedding multimodaux gagnent en popularité, notre prochain défi est de développer des modèles de recherche fondamentaux plus intelligents, plus rapides et plus capables qui peuvent gérer les entrées pour des applications multimodales. Ces applications exigent des fenêtres de contexte encore plus longues - un défi d'efficacité qui reste à résoudre.</p><h2 id=\"conclusion\">Conclusion</h2><p>Tout au long de cet article, nous avons exploré comment ModernBERT fait progresser les modèles de la famille BERT grâce à trois innovations clés : son architecture profonde et fine, son tokenizer optimisé et sa mise à l'échelle efficace utilisant le weight tiling. Ces améliorations permettent à ModernBERT de fournir des performances exceptionnelles dans une taille relativement compacte, surpassant à la fois <code>RoBERTa-large</code> et <code>jina-XLM-RoBERTa</code> dans diverses tâches. ModernBERT démontre que les améliorations architecturales peuvent avoir plus d'importance que la taille des paramètres, ouvrant la voie à des modèles plus efficaces. Son utilisation réussie du weight tiling montre comment la mise à l'échelle progressive peut réduire les coûts d'entraînement tout en préservant ou même en améliorant les performances. De plus, son vocabulaire compact et ses optimisations ciblées suggèrent des opportunités croissantes pour les SLM spécialisés dans des environnements aux ressources limitées.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "Des données d'entraînement plus volumineuses, un dimensionnement efficace des paramètres et une architecture profonde mais fine, ModernBERT trace la voie pour les futurs modèles de type BERT.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
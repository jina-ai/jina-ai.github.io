{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "Optimisation sous-modulaire pour la génération de requêtes diversifiées dans DeepResearch",
  "html": "<p>Lors de l'implémentation de DeepResearch, il y a au moins deux endroits où vous devez générer des requêtes diverses. Premièrement, vous devez <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">générer des requêtes de recherche Web basées sur l'entrée de l'utilisateur</a> (il n'est pas judicieux de simplement envoyer l'entrée de l'utilisateur directement au moteur de recherche). Deuxièmement, de nombreux systèmes DeepResearch incluent <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">un \"planificateur de recherche\" qui divise le problème original en sous-problèmes</a>, appelle simultanément des agents pour les résoudre indépendamment, puis fusionne leurs résultats. Qu'il s'agisse de requêtes ou de sous-problèmes, nos attentes restent les mêmes : ils doivent être pertinents par rapport à l'entrée originale et suffisamment diversifiés pour fournir des perspectives uniques à ce sujet. Souvent, nous devons limiter le nombre de requêtes pour éviter de gaspiller de l'argent en demandant inutilement des moteurs de recherche ou en utilisant des *tokens* d'agent.</p><p>Bien qu'ils comprennent l'importance de la génération de requêtes, la plupart des implémentations DeepResearch *open source* ne prennent pas cette optimisation au sérieux. Ils se contentent d'intégrer directement ces contraintes dans le *prompt*. Certains pourraient demander au LLM un tour supplémentaire pour évaluer et diversifier les requêtes. Voici un exemple de la façon dont la plupart des implémentations abordent cette question :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Deux *prompts* différents pour générer des requêtes diverses à l'aide de LLM. Le *prompt* supérieur utilise des instructions simples. Celui du bas est plus sophistiqué et structuré. Compte tenu de la requête originale et du nombre de requêtes à générer, nous nous attendons à ce que les requêtes générées soient suffisamment diversifiées. Dans cet exemple, nous utilisons </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> comme LLM et la requête originale est </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Dans cet article, je souhaite démontrer une approche plus rigoureuse pour résoudre la génération optimale de requêtes à l'aide de *vecteurs modèles* de phrases et de l'*optimisation sous-modulaire*. À l'époque de mon doctorat, l'optimisation sous-modulaire était l'une de mes techniques préférées avec L-BFGS. Je vais montrer comment l'appliquer pour générer un ensemble de requêtes diverses sous une contrainte de cardinalité, ce qui peut améliorer considérablement la qualité globale des systèmes DeepResearch.</p><h2 id=\"query-generation-via-prompting\">Génération de requêtes via le *prompting*</h2><p>Tout d'abord, nous voulons vérifier si le *prompting* est une approche efficace pour générer des requêtes diverses. Nous voulons également comprendre si un *prompt* sophistiqué est plus efficace qu'un *prompt* simple. Réalisons une expérience comparant les deux *prompts* ci-dessous pour le découvrir :</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">*Prompt* simple</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">*Prompt* structuré</span></p></figcaption></figure><p>Nous utilisons <code>gemini-2.5-flash</code> comme LLM avec la requête originale <code>\"embeddings and rerankers\"</code> et testons les *prompts* simple et structuré pour générer de manière itérative d'une à 20 requêtes. Nous utilisons ensuite <code>jina-embeddings-v3</code> avec la tâche <code>text-matching</code> pour mesurer la similarité des phrases entre la requête originale et les requêtes générées, ainsi que la similarité au sein des requêtes générées elles-mêmes. Voici les visualisations.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Les deux *prompts* présentent des schémas similaires dans l'analyse \"Au sein des requêtes générées\" (deux graphiques de droite), avec des similarités cosinus médianes restant élevées (plage de 0,4 à 0,6) pour différents nombres de requêtes. Le *prompt* simple semble même être meilleur pour diversifier les requêtes lorsque le nombre de requêtes est important, tandis que le *prompt* structuré maintient une pertinence légèrement meilleure par rapport à la requête originale, en maintenant la pertinence autour de 0,6.</span></figcaption></figure><p>En regardant les deux graphiques sur le côté droit, on peut constater que les *prompts* simple et structuré présentent une grande variance dans les scores de similarité cosinus, beaucoup atteignant une similarité de 0,7 à 0,8, ce qui suggère que certaines requêtes générées sont presque identiques. De plus, les deux méthodes ont du mal à maintenir la diversité à mesure que davantage de requêtes sont générées. Au lieu de constater une nette tendance à la baisse de la similarité avec l'augmentation du nombre de requêtes, nous observons des niveaux de similarité relativement stables (et élevés), ce qui indique que les requêtes supplémentaires dupliquent souvent les perspectives existantes.</p><p>Une explication est ce que Wang et al. (2025) ont constaté que les LLM reflètent souvent de manière disproportionnée les opinions des groupes dominants, même avec le *prompt steering*, ce qui indique un biais envers les perspectives communes. En effet, les données d'entraînement du LLM peuvent surreprésenter certains points de vue, ce qui amène le modèle à générer des variations qui s'alignent sur ces perspectives dominantes. Abe et al. (2025) ont également constaté que l'expansion de requête basée sur le LLM favorise les interprétations populaires tout en négligeant les autres. Par exemple, \"Quels sont les avantages de l'IA ?\" pourrait produire des avantages courants tels que l'automatisation, l'efficacité, l'éthique, mais manquer des avantages moins évidents comme la découverte de médicaments.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">Formulation du problème</h2><p>On pourrait penser que notre expérience précédente n'est pas concluante et que nous devrions améliorer le *prompt* et réessayer. Bien que le *prompting* puisse certainement modifier les résultats dans une certaine mesure, ce qui est plus important, c'est que nous avons appris quelque chose : le simple fait d'augmenter le nombre de requêtes générées nous rend plus susceptibles d'obtenir des requêtes diverses. La mauvaise nouvelle est que nous obtenons également un tas de requêtes dupliquées comme sous-produit.</p><p>Mais puisqu'il est peu coûteux de générer un grand nombre de requêtes, ce qui finit par donner <em>quelques</em> bonnes requêtes, pourquoi ne pas traiter cela comme un problème de sélection de sous-ensembles ?</p><p>En mathématiques, voici comment nous pouvons formuler ce problème : étant donné une entrée originale $q_0$, un ensemble de requêtes candidates $V=\\{q_1, q_2, \\cdots, q_n\\}$ générées par un LLM en utilisant l'ingénierie de Prompt. Sélectionnez un sous-ensemble $X\\subseteq V$ de $k$ requêtes qui maximise la couverture tout en minimisant la redondance.</p><p>Malheureusement, trouver le sous-ensemble optimal de $k$ requêtes parmi $n$ candidats nécessite de vérifier $\\binom{n}{k}$ combinaisons - une complexité exponentielle. Pour seulement 20 candidats et $k=5$, cela représente 15 504 combinaisons.</p><h3 id=\"submodular-function\">Fonction Sous-modulaire</h3><p>Avant d'essayer de résoudre brutalement le problème de sélection de sous-ensemble, permettez-moi de présenter aux lecteurs les termes <strong>sous-modularité</strong> et <strong>fonction sous-modulaire</strong>. Ils peuvent sembler inconnus à beaucoup, mais vous avez peut-être entendu parler de l'idée de \"rendements décroissants\" - eh bien, la sous-modularité est la représentation mathématique de cela.</p><p>Considérez le placement de routeurs Wi-Fi pour fournir une couverture Internet dans un grand bâtiment. Le premier routeur que vous installez apporte une valeur énorme - il couvre une zone importante qui n'avait pas de couverture auparavant. Le deuxième routeur ajoute également une valeur substantielle, mais une partie de sa zone de couverture chevauche le premier routeur, de sorte que l'avantage marginal est inférieur au premier. Au fur et à mesure que vous continuez à ajouter des routeurs, chaque routeur supplémentaire couvre de moins en moins de nouvelle zone, car la plupart des espaces sont déjà couverts par les routeurs existants. Finalement, le 10e routeur pourrait fournir très peu de couverture supplémentaire, car le bâtiment est déjà bien couvert.</p><p>Cette intuition capture l'essence de la sous-modularité. Mathématiquement, une fonction d'ensemble $f: 2^V \\rightarrow \\mathbb{R}$ est <strong>sous-modulaire</strong> si pour tout $A \\subseteq B \\subseteq V$ et tout élément $v \\notin B$ :</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>En termes simples : ajouter un élément à un ensemble plus petit donne au moins autant d'avantages que d'ajouter le même élément à un ensemble plus grand qui contient l'ensemble plus petit.</p><p>Appliquons maintenant ce concept à notre problème de génération de requêtes. On peut immédiatement se rendre compte que la sélection de requêtes présente des <strong>rendements décroissants</strong> naturels :</p><ul><li>La première requête que nous sélectionnons couvre un espace sémantique entièrement nouveau</li><li>La deuxième requête doit couvrir différents aspects, mais un certain chevauchement est inévitable</li><li>Au fur et à mesure que nous ajoutons des requêtes, chaque requête supplémentaire couvre de moins en moins de terrain nouveau</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">De </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">l'une de mes anciennes diapositives de l'AAAI 2013</span></a><span style=\"white-space: pre-wrap;\">, où j'ai expliqué la sous-modularité à l'aide d'un sac de boules. L'ajout de plus de boules au sac améliore la \"facilité\", mais l'amélioration relative devient de plus en plus petite, comme on le voit dans les valeurs delta décroissantes sur l'axe des y de droite.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">Conception de Fonction Sous-modulaire Basée sur les Vecteurs Modèles</h2><p>Soit $\\mathbf{e}_i \\in \\mathbb{R}^d$ le vecteur modèle pour la requête $q_i$, obtenu en utilisant un modèle de vecteur modèle de phrase (par exemple, <code>jina-embeddings-v3</code>). Il existe deux approches principales pour concevoir notre fonction objectif :</p><h3 id=\"approach-1-facility-location-coverage-based\">Approche 1 : Emplacement des Installations (Basée sur la Couverture)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>Cette fonction mesure à quel point l'ensemble sélectionné $X$ \"couvre\" toutes les requêtes candidates, où :</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ est la similarité cosinus</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ assure la pertinence par rapport à la requête originale</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ mesure la couverture du candidat $j$ par l'ensemble sélectionné $X$</li></ul><p>Un inconvénient est que cette fonction n'encourage qu'<em>implicitement</em> la diversité. Elle ne pénalise pas explicitement la similarité au sein de l'ensemble sélectionné $X$. La diversité émerge parce que la sélection de requêtes similaires offre des rendements de couverture décroissants.</p><h3 id=\"approach-2-explicit-coverage-diversity\">Approche 2 : Couverture Explicite + Diversité</h3><p>Pour un contrôle plus direct sur la diversité, nous pouvons combiner la couverture et un terme de diversité explicite :</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>où la composante de diversité peut être formulée comme :</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>Ce terme de diversité mesure la similarité totale entre les requêtes sélectionnées et les requêtes non sélectionnées - il est maximisé lorsque nous sélectionnons des requêtes qui sont différentes des candidats restants (une forme de fonction de coupe de graphe).</p><h3 id=\"difference-between-two-approaches\">Différence Entre les Deux Approches</h3><p>Les deux formulations maintiennent la sous-modularité.</p><p>La fonction d'emplacement des installations est une fonction sous-modulaire bien connue. Elle présente une sous-modularité en raison de l'opération max : lorsque nous ajoutons une nouvelle requête $q$ à notre ensemble sélectionné, chaque requête candidate $j$ est couverte par la \"meilleure\" requête de notre ensemble (celle avec la plus grande similarité). L'ajout de $q$ à un ensemble plus petit $A$ est plus susceptible d'améliorer la couverture de divers candidats que l'ajout à un ensemble plus grand $B \\supseteq A$ où de nombreux candidats sont déjà bien couverts.</p><p>Dans la fonction de diversité de coupe de graphe<strong>,</strong> le terme de diversité $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ est sous-modulaire car il mesure la \"coupe\" entre les ensembles sélectionnés et non sélectionnés. L'ajout d'une nouvelle requête à un ensemble sélectionné plus petit crée plus de nouvelles connexions aux requêtes non sélectionnées que l'ajout à un ensemble sélectionné plus grand.</p><p>L'approche d'emplacement des installations repose sur une diversité <em>implicite</em> grâce à la concurrence de la couverture, tandis que l'approche explicite mesure et optimise directement la diversité. Les deux sont donc valables, mais l'approche explicite vous donne un contrôle plus direct sur le compromis pertinence-diversité.</p><h2 id=\"implementations\">Implémentations</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">L'implémentation complète se trouve ici sur Github.</span></p></figcaption></figure><p>Puisque notre fonction est sous-modulaire, nous pouvons utiliser <strong>l'algorithme glouton</strong> qui fournit une garantie d'approximation de $(1-1/e) \\approx 0.63$ :</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>Voici le code pour optimiser l'emplacement des installations (basé sur la couverture) - celui avec une diversité implicite.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>Le paramètre d'équilibre $\\alpha$ contrôle le compromis entre la pertinence et la diversité :</p><ul><li><strong>$\\alpha$ élevé (par exemple, 0,8)</strong> : donne la priorité à la pertinence par rapport à la requête originale, peut sacrifier la diversité</li><li><strong>$\\alpha$ faible (par exemple, 0,2)</strong> : donne la priorité à la diversité parmi les requêtes sélectionnées, peut s'éloigner de l'intention originale</li><li><strong>$\\alpha$ modéré (par exemple, 0,4 à 0,6)</strong> : approche équilibrée, fonctionne souvent bien dans la pratique</li></ul><h3 id=\"lazy-greedy-algorithm\">Algorithme Glouton Paresseux</h3><p>On peut remarquer dans le code ci-dessus :</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>Nous calculons le gain marginal pour <strong>tous</strong> les candidats restants à chaque itération. Nous pouvons faire mieux que cela.</p><p>L'<strong>algorithme glouton paresseux</strong> est une optimisation intelligente qui exploite la sous-modularité pour éviter les calculs inutiles. L'idée clé est la suivante : si l'élément A avait un gain marginal plus élevé que l'élément B à l'itération $t$, alors A aura toujours un gain marginal plus élevé que B à l'itération $t+1$ (en raison de la propriété de sous-modularité).</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>L'algorithme glouton paresseux fonctionne comme ceci :</p><ol><li>Maintenir une file d'attente prioritaire d'éléments triés par leurs gains marginaux.</li><li>Ne recalculer que le gain marginal de l'élément supérieur.</li><li>S'il est toujours le plus élevé après le recalcul, le sélectionner.</li><li>Sinon, le réinsérer dans la position correcte et vérifier l'élément supérieur suivant.</li></ol><p>Cela peut fournir des accélérations significatives, car nous évitons de recalculer les gains marginaux pour les éléments qui ne seront clairement pas sélectionnés.</p><h3 id=\"results\">Résultats</h3><p>Réalisons à nouveau l'expérience. Nous utilisons le même Prompt simple pour générer de 1 à 20 requêtes diverses et effectuons les mêmes mesures de similarité cosinus qu'auparavant. Pour l'optimisation sous-modulaire, nous sélectionnons des requêtes parmi les 20 candidats générés en utilisant différentes valeurs de k et mesurons la similarité comme auparavant. Les résultats montrent que les requêtes sélectionnées par l'optimisation sous-modulaire sont plus diverses et présentent une similarité intra-ensemble plus faible.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Requête originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Requête originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Requête originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Requête originale = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">Dernière question : pourquoi la formulation sous-modulaire est-elle importante ?</h2><p>Vous vous demandez peut-être : pourquoi s'embêter à formuler cela comme un problème d'optimisation sous-modulaire ? Pourquoi ne pas simplement utiliser des heuristiques ou d'autres approches d'optimisation ?</p><p>En bref, la formulation sous-modulaire transforme une heuristique ad-hoc \"sélectionner des requêtes diverses\" en un problème d'optimisation rigoureux avec des <strong>garanties prouvables</strong>, des <strong>algorithmes efficaces</strong> et des objectifs mesurables.</p><h3 id=\"guaranteed-efficiency\">Efficacité garantie</h3><p>Une fois que nous prouvons que notre fonction objectif est sous-modulaire, nous obtenons de puissantes garanties théoriques et un algorithme efficace. L'algorithme glouton qui s'exécute en temps $O(nk)$ comparé à la vérification des combinaisons $\\binom{n}{k}$ atteint une approximation de $(1-1/e) \\approx 0.63$ de la solution optimale. Cela signifie que notre solution gloutonne est toujours au moins 63 % aussi bonne que la meilleure solution possible. <strong>Aucune heuristique ne peut promettre cela.</strong></p><p>De plus, l'algorithme glouton paresseux est beaucoup plus rapide en pratique en raison de la structure mathématique des fonctions sous-modulaires. L'accélération provient des <strong>rendements décroissants</strong> : les éléments qui étaient de mauvais choix lors des itérations précédentes sont peu susceptibles de devenir de bons choix plus tard. Ainsi, au lieu de vérifier les $n$ candidats, l'algorithme glouton paresseux n'a généralement besoin de recalculer les gains que pour les quelques meilleurs candidats.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">Pas besoin d'heuristiques artisanales</h3><p>Sans cadre fondé sur des principes, vous pourriez recourir à des règles ad hoc comme \"s'assurer que les requêtes ont une similarité cosinus &lt; 0,7\" ou \"équilibrer différentes catégories de mots-clés\". Ces règles sont difficiles à affiner et ne se généralisent pas. L'optimisation sous-modulaire vous offre une approche fondée sur des principes et mathématiquement étayée. Vous pouvez affiner systématiquement les hyperparamètres à l'aide d'ensembles de validation et surveiller la qualité de la solution dans les systèmes de production. Lorsque le système produit de mauvais résultats, vous disposez de mesures claires pour déboguer ce qui n'a pas fonctionné.</p><p>Enfin, l'optimisation sous-modulaire est un domaine bien étudié avec des décennies de recherche, vous permettant de tirer parti d'algorithmes avancés au-delà du glouton (comme le glouton accéléré ou la recherche locale), des connaissances théoriques sur les moments où certaines formulations fonctionnent le mieux et des extensions pour gérer des contraintes supplémentaires telles que les limites budgétaires ou les exigences d'équité.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Pour ceux qui s'intéressent à l'optimisation sous-modulaire, je recommande ce site pour en savoir plus.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "Beaucoup connaissent l'importance de la diversité des requêtes dans DeepResearch, mais peu savent comment la résoudre rigoureusement via l'optimisation sous-modulaire.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "La similarit√© cosinus dans un sous-espace implique-t-elle une similarit√© cosinus dans un espace de haute dimension ?",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Le 25 janvier 2024, OpenAI a lanc√© <a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">un nouveau mod√®le d'embedding</a> avec une nouvelle fonctionnalit√© appel√©e <i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">¬´ shortening ¬ª</strong></b></i>, qui permet aux d√©veloppeurs de r√©duire les embeddings ‚Äî en coupant essentiellement les nombres √† la fin de la s√©quence ‚Äî sans compromettre la capacit√© de l'embedding √† repr√©senter efficacement les concepts. Plongez dans cet article pour une base th√©orique solide sur la viabilit√© et la logique derri√®re cette innovation.</div></div><p>Consid√©rez ceci : lors de la mesure de la similarit√© cosinus de vecteurs d'embedding dans des espaces de haute dimension, comment leur similarit√© dans des sous-espaces de dimension inf√©rieure implique-t-elle la similarit√© globale ? Existe-t-il une relation directe et proportionnelle, ou la r√©alit√© est-elle plus complexe avec des donn√©es de haute dimension ?</p><p>Plus concr√®tement, <strong>une forte similarit√© entre les vecteurs dans leurs 256 premi√®res dimensions assure-t-elle une forte similarit√© dans leurs 768 dimensions compl√®tes ?</strong> √Ä l'inverse, si les vecteurs diff√®rent significativement dans certaines dimensions, cela implique-t-il une faible similarit√© globale ? Ce ne sont pas de simples r√©flexions th√©oriques ; ce sont des consid√©rations cruciales pour la recherche efficace de vecteurs, l'indexation des bases de donn√©es et la performance des syst√®mes RAG.</p><p>Les d√©veloppeurs s'appuient souvent sur des heuristiques, supposant qu'une forte similarit√© dans un sous-espace √©quivaut √† une forte similarit√© globale ou que des diff√©rences notables dans une dimension affectent significativement la similarit√© globale. La question est : ces m√©thodes heuristiques reposent-elles sur des bases th√©oriques solides, ou sont-elles simplement des hypoth√®ses de commodit√© ?</p><p>Cet article se penche sur ces questions, examinant la th√©orie et les implications pratiques de la similarit√© des sous-espaces par rapport √† la similarit√© vectorielle globale.</p><h2 id=\"bounding-the-cosine-similarity\">D√©limitation de la Similarit√© Cosinus</h2><p>√âtant donn√© les vecteurs $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$, nous les d√©composons comme $\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ et $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$, o√π $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ et $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$, avec $m+n=d$.</p><p>La similarit√© cosinus dans le sous-espace $\\mathbb{R}^m$ est donn√©e par $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$ ; de m√™me, la similarit√© dans le sous-espace $\\mathbb{R}^n$ est $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$.</p><p>Dans l'espace original $\\mathbb{R}^d$, la similarit√© cosinus est d√©finie comme :$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>Maintenant, soit $s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$. Alors, nous avons :$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>Fin de la d√©monstration.</p><p>Notez que dans la derni√®re √©tape de la preuve, nous utilisons le fait que la similarit√© cosinus est toujours inf√©rieure ou √©gale √† 1. Cela forme notre borne sup√©rieure. De m√™me, nous pouvons montrer que la borne inf√©rieure de \\(\\cos(\\mathbf{A},\\mathbf{B})\\) est donn√©e par :</p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\], o√π $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$.</p><p>Notez que pour la borne inf√©rieure, nous ne pouvons pas conclure h√¢tivement que \\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\). Cela est d√ª √† l'intervalle de la fonction cosinus, qui s'√©tend entre \\([-1, 1]\\). En raison de cet intervalle, il est impossible d'√©tablir une borne inf√©rieure plus stricte que la valeur triviale de -1.</p><p>Donc en conclusion, nous avons la borne large suivante : $$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)).$$ et une borne plus stricte \\[\\begin{align*}  \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\], o√π $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $.</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">Connexion au Lemme de Johnson‚ÄìLindenstrauss</h3><p>Le lemme JL affirme que pour tout \\(0 &lt; \\epsilon &lt; 1\\) et tout ensemble fini de points \\( S \\) dans \\( \\mathbb{R}^d \\), il existe une application \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\) (avec \\( k = O(\\epsilon^{-2} \\log |S|) \\)) telle que pour tous \\( \\mathbf{u}, \\mathbf{v} \\in S \\), les distances euclidiennes sont approximativement pr√©serv√©es :<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson‚ÄìLindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>Pour faire fonctionner $f$ comme une s√©lection de sous-espace, nous pouvons utiliser une matrice diagonale pour la projection, comme une matrice \\(5 \\times 3\\) \\(f\\), bien que non al√©atoire (notez que la formulation typique du lemme JL implique des transformations lin√©aires qui utilisent souvent des matrices al√©atoires tir√©es d'une distribution gaussienne). Par exemple, si nous visons √† conserver les 1√®re, 3√®me et 5√®me dimensions d'un espace vectoriel √† 5 dimensions, la matrice \\(f\\) pourrait √™tre con√ßue comme suit : \\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>Cependant, en sp√©cifiant $f$ comme diagonale, nous limitons la classe de fonctions qui peuvent √™tre utilis√©es pour la projection. Le lemme JL garantit l'existence d'un $f$ appropri√© dans la classe plus large des transformations lin√©aires, mais lorsque nous restreignons $f$ √† √™tre diagonal, un tel $f$ appropri√© peut ne pas exister dans cette classe restreinte pour appliquer les bornes du lemme JL.</p><h2 id=\"validating-the-bounds\">Validation des Bornes</h2><p>Pour explorer empiriquement les bornes th√©oriques de la similarit√© cosinus dans des espaces vectoriels de haute dimension, nous pouvons employer une simulation de Monte Carlo. Cette m√©thode nous permet de g√©n√©rer un grand nombre de paires de vecteurs al√©atoires, de calculer leurs similarit√©s √† la fois dans l'espace original et dans les sous-espaces, puis d'√©valuer comment les bornes th√©oriques sup√©rieures et inf√©rieures se comportent en pratique.</p><p>Le snippet de code Python suivant impl√©mente ce concept. Il g√©n√®re al√©atoirement des paires de vecteurs dans un espace de haute dimension et calcule leur similarit√© cosinus. Ensuite, il divise chaque vecteur en deux sous-espaces, calcule la similarit√© cosinus dans chaque sous-espace, et √©value les bornes sup√©rieures et inf√©rieures de la similarit√© cosinus en dimension compl√®te bas√©e sur les similarit√©s des sous-espaces.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),\n          np.all(gamma * t &lt;= cos_sim)))\n</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Un validateur Monte Carlo pour valider les bornes de similarit√© cosinus</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Un exemple de sortie de notre validateur Monte Carlo. Il est important de noter que la condition </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>lower/upper bound satisfied</span></code><span style=\"white-space: pre-wrap;\"> est v√©rifi√©e pour chaque vecteur individuellement. Pendant ce temps, les </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>avg. lower/upper bound</span></code><span style=\"white-space: pre-wrap;\"> fournissent un aper√ßu plus intuitif des statistiques li√©es √† ces bornes mais n'influencent pas directement le processus de validation.</span></p></figcaption></figure><h2 id=\"understanding-the-bounds\">Comprendre les bornes</h2><p>En bref, lors de la comparaison de deux vecteurs de grande dimension, la similarit√© globale se situe entre les meilleures et les pires similarit√©s de leurs sous-espaces, ajust√©es en fonction de la taille ou de l'importance de ces sous-espaces dans le sch√©ma global. C'est ce que repr√©sentent intuitivement les bornes de similarit√© cosinus dans les dimensions sup√©rieures : l'√©quilibre entre les parties les plus et les moins similaires, pond√©r√© par leur taille ou importance relative.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Comparaison illustrative de deux capuchons et corps de stylet avec sections √©tiquet√©es sur fond noir\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Chaque stylo a deux composants principaux : le corps et le capuchon.</span></figcaption></figure><p>Imaginez que vous essayez de comparer deux objets compos√©s de plusieurs parties (disons, deux stylos de luxe) en fonction de leur similarit√© globale. Chaque stylo a deux composants principaux : le corps et le capuchon. La similarit√© du stylo entier (corps et capuchon) est ce que nous essayons de d√©terminer :</p><h3 id=\"upper-bound-gamma-cdot-s\">Borne sup√©rieure ($\\gamma \\cdot s$)</h3><p>Pensez √† $s$ comme la meilleure correspondance entre les parties correspondantes des stylos. Si les capuchons sont tr√®s similaires mais pas les corps, $s$ est la similarit√© des capuchons.</p><p>Maintenant, $\\gamma$ est comme un facteur d'√©chelle bas√© sur la taille (ou l'importance) de chaque partie. Si un stylo a un corps tr√®s long et un capuchon court, tandis que l'autre a un corps court et un capuchon long, $\\gamma$ ajuste la similarit√© globale pour tenir compte de ces diff√©rences de proportions.</p><p>La borne sup√©rieure nous indique que quelle que soit la similarit√© de certaines parties, la similarit√© globale ne peut pas d√©passer cette \"meilleure similarit√© de partie\" multipli√©e par le facteur de proportion.</p><h3 id=\"lower-bound-gamma-cdot-t\">Borne inf√©rieure ($\\gamma \\cdot t$)</h3><p>Ici, $t$ est la similarit√© des parties les moins correspondantes. Si les corps des stylos sont assez diff√©rents mais les capuchons sont similaires, $t$ refl√®te la similarit√© du corps.</p><p>Encore une fois, $\\gamma$ adapte cela en fonction de la proportion de chaque partie.</p><p>La borne inf√©rieure signifie que la similarit√© globale ne peut pas √™tre pire que cette \"pire similarit√© de partie\" apr√®s prise en compte de la proportion de chaque partie.</p><h2 id=\"implications-of-the-bounds\">Implications des bornes</h2><p>Pour les d√©veloppeurs travaillant avec des embeddings, la recherche vectorielle, la r√©cup√©ration ou les bases de donn√©es, la compr√©hension de ces bornes a des implications pratiques, particuli√®rement lors du traitement de donn√©es de grande dimension. La recherche vectorielle implique souvent de trouver les vecteurs les plus proches (les plus similaires) dans une base de donn√©es pour un vecteur de requ√™te donn√©, g√©n√©ralement en utilisant la similarit√© cosinus comme mesure de proximit√©. Les bornes que nous avons discut√©es peuvent fournir des insights sur l'efficacit√© et les limitations de l'utilisation des similarit√©s de sous-espaces pour de telles t√¢ches.</p><h3 id=\"using-subspace-similarity-for-ranking\">Utilisation de la similarit√© des sous-espaces pour le classement</h3><p><strong>S√©curit√© et pr√©cision</strong> : L'utilisation de la similarit√© des sous-espaces pour le classement et la r√©cup√©ration des k premiers r√©sultats peut √™tre efficace, mais avec pr√©caution. La borne sup√©rieure indique que la similarit√© globale ne peut pas d√©passer la similarit√© maximale des sous-espaces. Ainsi, si une paire de vecteurs est tr√®s similaire dans un sous-espace particulier, c'est un candidat fort pour √™tre similaire dans l'espace de grande dimension.</p><p><strong>Pi√®ges potentiels</strong> : Cependant, la borne inf√©rieure sugg√®re que deux vecteurs ayant une faible similarit√© dans un sous-espace pourraient toujours √™tre assez similaires globalement. Par cons√©quent, se fier uniquement √† la similarit√© des sous-espaces pourrait manquer certains r√©sultats pertinents.</p><h3 id=\"misconceptions-and-cautions\">Id√©es fausses et mises en garde</h3><p><strong>Surestimation de l'importance des sous-espaces</strong> : Une id√©e fausse courante est de surestimer l'importance d'un sous-espace particulier. Bien qu'une haute similarit√© dans un sous-espace soit un bon indicateur, elle ne garantit pas une haute similarit√© globale en raison de l'influence des autres sous-espaces.</p><p><strong>Ignorer les similarit√©s n√©gatives</strong> : Dans les cas o√π la similarit√© cosinus dans un sous-espace est n√©gative, cela indique une relation oppos√©e dans cette dimension. Les d√©veloppeurs doivent √™tre attentifs √† la fa√ßon dont ces similarit√©s n√©gatives impactent la similarit√© globale.</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "Une forte similarit√© dans un sous-espace garantit-elle une forte similarit√© globale entre les vecteurs ? Cet article examine la th√©orie et les implications pratiques de la similarit√© dans les sous-espaces.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
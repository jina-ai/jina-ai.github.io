{
  "slug": "what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
  "id": "65d3a2134a32310001f5b71b",
  "uuid": "726c942b-f6a7-4c89-a0ad-39aaad98d02f",
  "title": "Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?",
  "html": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina ColBERT v2 : Récupérateur d'interaction tardive multilingue pour l'intégration et le reclassement</div><div class=\"kg-bookmark-description\">Jina ColBERT v2 prend en charge 89 langues avec des performances de récupération supérieures, des dimensions de sortie contrôlées par l'utilisateur, et une longueur de token de 8192.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg\" alt=\"\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Mise à jour : Le 31 août 2024, nous avons publié la 2e version de Jina-ColBERT, avec des performances améliorées, un support multilingue pour 89 langues et des dimensions de sortie flexibles. Consultez l'article de publication pour plus de détails.</span></p></figcaption></figure><p>Vendredi dernier, la sortie du <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">modèle ColBERT par Jina AI sur Hugging Face</a> a suscité un enthousiasme significatif dans la communauté IA, particulièrement sur Twitter/X. Alors que beaucoup connaissent le modèle révolutionnaire BERT, l'effervescence autour de ColBERT en a laissé certains perplexes : Qu'est-ce qui fait que ColBERT se démarque dans le domaine très concurrentiel des technologies de recherche d'information ? Pourquoi la communauté IA est-elle enthousiasmée par le ColBERT de longueur 8192 ? Cet article explore les subtilités de ColBERT et ColBERTv2, soulignant leur conception, leurs améliorations et l'efficacité surprenante de l'interaction tardive de ColBERT.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Maximisez facilement la pertinence de recherche et la précision RAG</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">@huggingface</a> under Apache 2.0 licence<a href=\"https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io\">https://t.co/snVGgI753H</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">February 16, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><h2 id=\"what-is-colbert\">Qu'est-ce que ColBERT ?</h2><p>Le nom \"ColBERT\" signifie <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, un modèle issu de l'Université Stanford, qui exploite la compréhension profonde du langage de BERT tout en introduisant un nouveau mécanisme d'interaction. Ce mécanisme, connu sous le nom d'<strong>interaction tardive</strong>, permet une récupération efficace et précise en traitant les requêtes et les documents séparément jusqu'aux étapes finales du processus de récupération. Plus précisément, il existe deux versions du modèle :</p><ul><li><strong>ColBERT</strong> : Le modèle initial était l'œuvre d'<a href=\"https://x.com/lateinteraction?s=20&ref=jina-ai-gmbh.ghost.io\"><strong>Omar Khattab</strong></a><strong> et Matei Zaharia</strong>, présentant une nouvelle approche de la recherche d'information à travers l'article \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\". Leur travail a été publié à SIGIR 2020.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class=\"kg-bookmark-description\">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Omar Khattab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">L'article original de ColBERT qui introduit l'\"interaction tardive\".</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong> : S'appuyant sur le travail fondamental, <strong>Omar Khattab</strong> a poursuivi ses recherches, collaborant avec <strong>Barlas Oguz, Matei Zaharia, et Michael S. Bernstein</strong> pour introduire \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", présenté à SIGIR 2021. Cette nouvelle itération de ColBERT a abordé les limitations précédentes et introduit des améliorations clés, telles que la <strong>supervision débruitée</strong> et la <strong>compression résiduelle</strong>, améliorant à la fois l'efficacité de récupération du modèle et son efficacité de stockage.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class=\"kg-bookmark-description\">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Keshav Santhanam</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">ColBERTv2 ajoutant la supervision débruitée et la compression résiduelle pour améliorer la qualité des données d'entraînement et réduire l'empreinte spatiale.</span></p></figcaption></figure><h2 id=\"understand-colberts-design\">Comprendre la conception de ColBERT</h2><p>Étant donné que l'architecture de ColBERTv2 reste très similaire à celle du ColBERT original, avec ses innovations clés tournant autour des techniques d'entraînement et des mécanismes de compression, nous allons d'abord examiner les aspects fondamentaux du ColBERT original.</p><h3 id=\"what-is-late-interaction-in-colbert\">Qu'est-ce que l'interaction tardive dans ColBERT ?</h3><p>\"Interaction\" fait référence au processus d'évaluation de la pertinence entre une requête et un document en comparant leurs représentations.</p><p>L'\"<em>interaction tardive</em>\" est l'essence de ColBERT. Le terme provient de l'architecture du modèle et de sa stratégie de traitement, où l'interaction entre les représentations de la requête et du document se produit tard dans le processus, après que les deux aient été encodés indépendamment. Cela contraste avec les modèles d'\"<em>interaction précoce</em>\", où les embeddings de requête et de document interagissent à des stades plus précoces, généralement avant ou pendant leur encodage par le modèle.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Models</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Early Interaction</td>\n<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>\n</tr>\n<tr>\n<td>Late Interaction</td>\n<td>ColBERT, ColBERTv2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>L'interaction précoce peut augmenter la complexité computationnelle car elle nécessite de considérer toutes les paires requête-document possibles, la rendant moins efficace pour les applications à grande échelle.</p><p>Les modèles d'interaction tardive comme ColBERT optimisent l'efficacité et la scalabilité en permettant le pré-calcul des représentations des documents et en employant une étape d'interaction plus légère à la fin, qui se concentre sur les représentations déjà encodées. Ce choix de conception permet des temps de récupération plus rapides et des demandes computationnelles réduites, le rendant plus adapté au traitement de grandes collections de documents.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg\" class=\"kg-image\" alt=\"Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep\" loading=\"lazy\" width=\"300\" height=\"143\"><figcaption><span style=\"white-space: pre-wrap;\">Diagrammes schématiques illustrant les paradigmes d'interaction requête-document dans la RI neuronale, avec l'interaction tardive de ColBERT à gauche.</span></figcaption></figure><h3 id=\"no-interaction-cosine-similarity-of-document-and-query-embeddings\">Pas d'interaction : similarité cosinus des embeddings de documents et de requêtes</h3><p>De nombreuses bases de données vectorielles et solutions de recherche neuronale pratiques reposent sur la correspondance rapide de similarité cosinus entre les embeddings de documents et de requêtes. Bien qu'attrayante pour sa simplicité et son efficacité computationnelle, cette méthode, souvent appelée « <em>sans interaction</em> » ou « <em>non basée sur l'interaction</em> », s'est révélée moins performante par rapport aux modèles qui intègrent une forme d'interaction entre les requêtes et les documents.</p><p>La limitation principale de l'approche « sans interaction » réside dans son incapacité à capturer les nuances complexes et les relations entre les termes des requêtes et des documents. La recherche d'information, dans son essence, consiste à comprendre et à faire correspondre l'intention derrière une requête avec le contenu d'un document. Ce processus nécessite souvent une compréhension contextuelle profonde des termes impliqués, ce que les embeddings uniques et agrégés pour les documents et les requêtes peinent à fournir.</p><h2 id=\"query-and-document-encoders-in-colbert\">Encodeurs de requêtes et de documents dans ColBERT</h2><p>La stratégie d'encodage de ColBERT est basée sur le modèle BERT, connu pour sa compréhension contextuelle profonde du langage. Le modèle génère des représentations vectorielles denses pour chaque token dans une requête ou un document, <strong>créant respectivement un sac d'embeddings contextualisés pour une requête et un sac pour un document.</strong> Cela facilite une comparaison nuancée de leurs embeddings durant la phase d'interaction tardive.</p><h3 id=\"query-encoder-of-colbert\">Encodeur de requêtes de ColBERT</h3><p>Pour une requête $Q$ avec les tokens ${q_1, q_2, ..., q_l}$, le processus commence par tokeniser $Q$ en tokens WordPiece basés sur BERT et en préfixant un token spécial <code>[Q]</code>. Ce token <code>[Q]</code>, positionné juste après le token <code>[CLS]</code> de BERT, signale le début d'une requête.</p><p>Si la requête est plus courte qu'un nombre prédéfini de tokens $N_q$, elle est complétée avec des tokens <code>[mask]</code> jusqu'à $N_q$ ; sinon, elle est tronquée aux premiers tokens $N_q$. La séquence complétée est ensuite passée à travers BERT, suivie d'un CNN (Réseau de Neurones Convolutif) et d'une normalisation. Le résultat est un ensemble de vecteurs d'embedding notés $\\mathbf{E}_q$ ci-dessous :<br>$$\\mathbf{E}_q := \\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)$$</p><h3 id=\"document-encoder-of-colbert\">Encodeur de documents de ColBERT</h3><p>De même, pour un document $D$ avec les tokens ${d_1, d_2, ..., d_n}$, un token <code>[D]</code> est préfixé pour indiquer le début d'un document. Cette séquence, sans besoin de complétion, subit le même processus, donnant un ensemble de vecteurs d'embedding notés $\\mathbf{E}_d$ ci-dessous :<br>$$\\mathbf{E}_d := \\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)$$</p><p>L'utilisation de tokens <code>[mask]</code> pour compléter les requêtes (appelée <strong>augmentation de requête</strong> dans l'article) assure une longueur uniforme pour toutes les requêtes, facilitant le traitement par lots. Les tokens <code>[Q]</code> et <code>[D]</code> marquent explicitement le début des requêtes et des documents, respectivement, aidant le modèle à distinguer les deux types d'entrées.</p><h3 id=\"comparing-colbert-to-cross-encoders\">Comparaison de ColBERT aux cross-encoders</h3><p>Les cross-encoders traitent les paires de requêtes et de documents ensemble, les rendant très précis mais moins efficaces pour les tâches à grande échelle en raison du coût computationnel d'évaluation de chaque paire possible. Ils excellent dans des scénarios spécifiques où le scoring précis des paires de phrases est nécessaire, comme dans les tâches de similarité sémantique ou la comparaison détaillée de contenu. Cependant, cette conception limite leur applicabilité dans les situations nécessitant une récupération rapide depuis de grands ensembles de données, où les embeddings pré-calculés et les calculs de similarité efficaces sont primordiaux.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg\" class=\"kg-image\" alt=\"Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum\" loading=\"lazy\" width=\"210\" height=\"150\"></figure><p>En revanche, le modèle d'interaction tardive de ColBERT permet le pré-calcul des embeddings de documents, accélérant significativement le processus de récupération sans compromettre la profondeur de l'analyse sémantique. Cette méthode, bien que contre-intuitive comparée à l'approche directe des cross-encoders, offre une solution évolutive pour les tâches de recherche d'information en temps réel et à grande échelle. Elle représente un compromis stratégique entre l'efficacité computationnelle et la qualité de la modélisation des interactions.</p><h2 id=\"finding-the-top-k-documents-using-colbert\">Trouver les K meilleurs documents avec ColBERT</h2><p>Une fois que nous avons les embeddings pour la requête et les documents, trouver les K documents les plus pertinents devient simple (mais pas aussi simple que le calcul du cosinus de deux vecteurs).</p><p>Les opérations clés incluent un produit scalaire par lots pour calculer les similarités terme à terme, un max-pooling sur les termes du document pour trouver la plus haute similarité par terme de requête, et une somme sur les termes de requête pour dériver le score total du document, suivi du tri des documents basé sur ces scores. Le pseudo-code PyTorch est décrit ci-dessous :</p><pre><code class=\"language-python\">import torch\n\ndef compute_relevance_scores(query_embeddings, document_embeddings, k):\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n    \n    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]\n    :param k: Number of top documents to re-rank\n    :return: Sorted document indices based on their relevance scores\n    \"\"\"\n    \n    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]\n    # Pad the k documents to their maximum length for batch operations\n    # Note: Assuming document_embeddings is already padded and moved to GPU\n    \n    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)\n    # Resulting shape: [k, num_query_terms, max_doc_length]\n    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n    \n    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term\n    # Shape after max-pool: [k, num_query_terms]\n    max_scores_per_query_term = scores.max(dim=2).values\n    \n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [k]\n    total_scores = max_scores_per_query_term.sum(dim=1)\n    \n    # Sort the documents based on their total scores\n    sorted_indices = total_scores.argsort(descending=True)\n    \n    return sorted_indices\n</code></pre><p>Notez que cette procédure est utilisée à la fois pendant l'entraînement et le reclassement lors de l'inférence. Le modèle ColBERT est entraîné en utilisant une perte de classement par paires, où les données d'entraînement consistent en triplets $(q, d^+, d^-)$, où $q$ représente une requête, $d^+$ est un document pertinent (positif) pour la requête, et $d^-$ est un document non pertinent (négatif). Le modèle vise à apprendre des représentations telles que le score de similarité entre $q$ et $d^+$ soit plus élevé que le score entre $q$ et $d^-$.</p><p>L'objectif d'entraînement peut être mathématiquement représenté comme la minimisation de la fonction de perte suivante : $$\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, où $S(q, d)$ dénote le score de similarité calculé par ColBERT entre une requête $q$ et un document $d$. Ce score est obtenu en agrégeant les scores de similarité maximale des embeddings les mieux appariés entre la requête et le document, suivant le motif d'interaction tardive décrit dans l'architecture du modèle. Cette approche assure que le modèle est entraîné à distinguer entre les documents pertinents et non pertinents pour une requête donnée, en encourageant une plus grande marge dans les scores de similarité pour les paires de documents positifs et négatifs.</p><h3 id=\"denoised-supervision-in-colbertv2\">Supervision débruitée dans ColBERTv2</h3><p>La supervision débruitée dans ColBERTv2 affine le processus d'entraînement original en sélectionnant des négatifs difficiles et en exploitant un cross-encoder pour la distillation. Cette méthode sophistiquée d'amélioration de la qualité des données d'entraînement implique plusieurs étapes :</p><ol><li><strong>Entraînement Initial</strong> : Utilisation des triplets officiels du jeu de données MS MARCO, comprenant une requête, un document pertinent et un document non pertinent.</li><li><strong>Indexation et Récupération</strong> : Emploi de la compression de ColBERTv2 pour indexer les passages d'entraînement, suivi de la récupération des k meilleurs passages pour chaque requête.</li><li><strong>Reclassement par Cross-Encoder</strong> : Amélioration de la sélection des passages par reclassement via un cross-encoder MiniLM, distillant ses scores dans ColBERTv2.</li><li><strong>Formation des Tuples d'Entraînement</strong> : Génération de tuples à w voies pour l'entraînement, incorporant à la fois des passages bien classés et moins bien classés pour créer des exemples difficiles.</li><li><strong>Raffinement Itératif</strong> : Répétition du processus pour améliorer continuellement la sélection des négatifs difficiles, améliorant ainsi les performances du modèle.</li></ol><p>Notez que ce processus représente une amélioration sophistiquée du régime d'entraînement de ColBERT plutôt qu'un changement fondamental de son architecture.</p><h3 id=\"hyperparameters-of-colbert\">Hyperparamètres de ColBERT</h3><p>Les hyperparamètres de ColBERT sont résumés ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Hyperparamètre</th>\n<th>Meilleur choix</th>\n<th>Raison</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning Rate</td>\n<td>3 x 10^{-6}</td>\n<td>Sélectionné pour le fine-tuning afin d'assurer des mises à jour stables et efficaces du modèle.</td>\n</tr>\n<tr>\n<td>Batch Size</td>\n<td>32</td>\n<td>Équilibre entre l'efficacité computationnelle et la capacité à capturer suffisamment d'informations par mise à jour.</td>\n</tr>\n<tr>\n<td>Number of Embeddings per Query (Nq)</td>\n<td>32</td>\n<td>Fixé pour assurer une taille de représentation cohérente entre les requêtes, facilitant un traitement efficace.</td>\n</tr>\n<tr>\n<td>Embedding Dimension (m)</td>\n<td>128</td>\n<td>Démontre un bon équilibre entre la puissance de représentation et l'efficacité computationnelle.</td>\n</tr>\n<tr>\n<td>Training Iterations</td>\n<td>200k (MS MARCO), 125k (TREC CAR)</td>\n<td>Choisi pour assurer un apprentissage approfondi tout en évitant le surapprentissage, avec des ajustements basés sur les caractéristiques du dataset.</td>\n</tr>\n<tr>\n<td>Bytes per Dimension in Embeddings</td>\n<td>4 (re-ranking), 2 (end-to-end ranking)</td>\n<td>Compromis entre précision et efficacité spatiale, en tenant compte du contexte d'application (re-ranking vs. end-to-end).</td>\n</tr>\n<tr>\n<td>Vector-Similarity Function</td>\n<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>\n<td>Sélectionné selon la performance et l'efficacité dans les contextes de recherche respectifs.</td>\n</tr>\n<tr>\n<td>FAISS Index Partitions (P)</td>\n<td>2000</td>\n<td>Détermine la granularité du partitionnement de l'espace de recherche, impactant l'efficacité de la recherche.</td>\n</tr>\n<tr>\n<td>Nearest Partitions Searched (p)</td>\n<td>10</td>\n<td>Équilibre entre l'étendue de la recherche et l'efficacité computationnelle.</td>\n</tr>\n<tr>\n<td>Sub-vectors per Embedding (s)</td>\n<td>16</td>\n<td>Affecte la granularité de la quantification, influençant la vitesse de recherche et l'utilisation de la mémoire.</td>\n</tr>\n<tr>\n<td>Index Representation per Dimension</td>\n<td>16-bit values</td>\n<td>Choisi pour la deuxième étape de la recherche end-to-end pour gérer le compromis entre précision et espace.</td>\n</tr>\n<tr>\n<td>Number of Layers in Encoders</td>\n<td>12-layer BERT</td>\n<td>Équilibre optimal entre la profondeur de la compréhension contextuelle et l'efficacité computationnelle.</td>\n</tr>\n  <tr>\n  <td>Max Query Length</td>\n<td>128</td>\n<td>Le nombre maximum de tokens traités par l'encodeur de requêtes. <b>Ceci est étendu dans le modèle Jina-ColBERT.</b></td>\n</tr>\n    <tr>\n  <td>Max Document Length</td>\n<td>512</td>\n<td>Le nombre maximum de tokens traités par l'encodeur de documents. <b>Ceci est étendu à 8192 dans le modèle Jina-ColBERT.</b></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"the-indexing-strategy-of-colbert\">La stratégie d'indexation de ColBERT</h2>\n<p>Contrairement aux approches basées sur la représentation qui encodent chaque document en un seul vecteur d'embedding, <strong>ColBERT encode les documents (et les requêtes) en ensembles d'embeddings, chaque token dans un document ayant son propre embedding.</strong> Cette approche signifie intrinsèquement que pour les documents plus longs, plus d'embeddings seront stockés, <strong>ce qui est un point faible du ColBERT original, et qui a été résolu plus tard par ColBERTv2.</strong></p>\n<p>La clé pour gérer cela efficacement réside dans l'utilisation par ColBERT d'une base de données vectorielle (par exemple <a href=\"https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io\">FAISS</a>) pour l'indexation et la recherche, et son processus d'indexation détaillé qui est conçu pour gérer efficacement de grands volumes de données. L'article original de ColBERT mentionne plusieurs stratégies pour améliorer l'efficacité de l'indexation et de la recherche, notamment :</p>\n<ul>\n<li><strong>Indexation hors ligne</strong> : Les représentations des documents sont calculées hors ligne, permettant le pré-calcul et le stockage des embeddings de documents. Ce processus utilise le traitement par lots et l'accélération GPU pour gérer efficacement de grandes collections de documents.</li>\n<li><strong>Stockage des embeddings</strong> : Les embeddings de documents peuvent être stockés en utilisant des valeurs 32-bit ou 16-bit pour chaque dimension, offrant un compromis entre précision et besoins en stockage. Cette flexibilité permet à ColBERT de maintenir un équilibre entre efficacité (en termes de performance de recherche) et efficience (en termes de coûts de stockage et de calcul).</li>\n</ul>\n<p>L'introduction de la <strong>compression résiduelle</strong> dans ColBERTv2, qui est une approche novatrice non présente dans le ColBERT original, joue un rôle clé dans la réduction de l'empreinte spatiale du modèle de 6 à 10 fois tout en préservant la qualité. Cette technique compresse davantage les embeddings en capturant et stockant efficacement uniquement les différences par rapport à un ensemble de centroïdes de référence fixes.</p>\n<h2 id=\"effectiveness-and-efficiency-of-colbert\">Efficacité et performance de ColBERT</h2>\n<p>On pourrait initialement supposer que l'incorporation de la compréhension contextuelle profonde de BERT dans la recherche nécessiterait intrinsèquement des ressources computationnelles importantes, rendant une telle approche moins viable pour les applications en temps réel en raison de la latence et des coûts computationnels élevés. Cependant, ColBERT remet en question et renverse cette supposition grâce à son utilisation innovante du mécanisme d'interaction tardive. Voici quelques points notables :</p>\n<ol>\n<li><strong>Gains d'efficacité significatifs</strong> : ColBERT réalise une réduction des coûts computationnels (FLOPs) et de la latence de plusieurs ordres de grandeur par rapport aux modèles de classement traditionnels basés sur BERT. Plus précisément, pour une taille de modèle donnée (par exemple, encodeur transformer \"base\" à 12 couches), ColBERT non seulement égale mais dans certains cas dépasse l'efficacité des modèles basés sur BERT avec des demandes computationnelles nettement inférieures. Par exemple, pour une profondeur de reclassement de <em>k</em>=10, BERT nécessite près de 180× plus de FLOPs que ColBERT ; cet écart s'élargit à mesure que <em>k</em> augmente, atteignant 13900× à <em>k</em>=1000 et même 23000× à <em>k</em>=2000.</li>\n<li><strong>Amélioration du Recall et MRR@10 dans la recherche de bout en bout</strong> : Contrairement à l'intuition initiale selon laquelle une interaction plus profonde entre les représentations des requêtes et des documents (comme observé dans les modèles d'interaction précoce) serait nécessaire pour une performance de recherche élevée, la configuration de recherche de bout en bout de ColBERT démontre une efficacité supérieure. Par exemple, son Recall@50 dépasse le Recall@1000 du BM25 officiel et presque tous les Recall@200 des autres modèles, soulignant la capacité remarquable du modèle à retrouver des documents pertinents dans une vaste collection sans comparaison directe de chaque paire requête-document.</li>\n<li><strong>Praticité pour les applications du monde réel</strong> : Les résultats expérimentaux soulignent l'applicabilité pratique de ColBERT pour les scénarios du monde réel. Son débit d'indexation et son efficacité mémoire le rendent adapté à l'indexation de grandes collections de documents comme MS MARCO en quelques heures, maintenant une haute efficacité avec une empreinte spatiale gérable. Ces qualités soulignent l'adéquation de ColBERT pour le déploiement dans des environnements de production où la performance et l'efficacité computationnelle sont primordiales.</li>\n<li><strong>Scalabilité avec la taille de la collection de documents</strong> : La conclusion peut-être la plus surprenante est la scalabilité et l'efficacité de ColBERT dans la gestion de collections de documents à grande échelle. L'architecture permet le pré-calcul des embeddings de documents et exploite un traitement par lots efficace pour l'interaction requête-document, permettant au système de s'adapter efficacement à la taille de la collection de documents. Cette scalabilité est contre-intuitive lorsqu'on considère la complexité et la profondeur de compréhension requises pour une recherche de documents efficace, démontrant l'approche innovante de ColBERT pour équilibrer l'efficacité computationnelle et l'efficacité de la recherche.</li>\n</ol>\n<h2 id=\"using-jina-colbert-v1-en-a-8192-length-colbertv2-model\">Utilisation de <code>jina-colbert-v1-en</code> : un modèle ColBERTv2 avec une longueur de 8192</h2>\n<p>Jina-ColBERT est conçu pour une recherche à la fois rapide et précise, supportant <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">des longueurs de contexte jusqu'à 8192, tirant parti des avancées de JinaBERT</a>, qui permet le traitement de séquences plus longues grâce à ses améliorations architecturales.</p>\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\">\n<div class=\"kg-callout-emoji\">💡</div>\n<div class=\"kg-callout-text\">Strictement parlant, Jina-ColBERT supporte une longueur de 8190 tokens. Rappelons que dans l'encodeur de documents ColBERT, chaque document est complété avec <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">[D],[CLS]</code> au début.</div>\n</div>\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-colbert-v1-en · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png\" alt=\"\"></div></a></figure>\n<h3 id=\"jinas-improvement-over-original-colbert\">Les améliorations de Jina par rapport au ColBERT original</h3>\n<p>La principale avancée de Jina-ColBERT est son architecture de base, <code>jina-bert-v2-base-en</code>, qui permet le traitement de contextes significativement plus longs (jusqu'à 8192 tokens) comparé au ColBERT original qui utilise <code>bert-base-uncased</code>. Cette capacité est cruciale pour gérer des documents avec un contenu extensif, fournissant des résultats de recherche plus détaillés et contextuels.</p>\n<h3 id=\"jina-colbert-v1-en-performance-comparison-vs-colbertv2\">Comparaison des performances de <code>jina-colbert-v1-en</code> vs. ColBERTv2</h3>\n<p>Nous avons évalué <code>jina-colbert-v1-en</code> sur les datasets BEIR et le nouveau benchmark LoCo qui favorise le contexte long, en le testant par rapport à l'implémentation originale de ColBERTv2 et aux modèles basés sur la non-interaction</p>modèle <code>jina-embeddings-v2-base-en</code>.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>ColBERTv2</th>\n<th>jina-colbert-v1-en</th>\n<th>jina-embeddings-v2-base-en</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Arguana</td>\n<td>46.5</td>\n<td><strong>49.4</strong></td>\n<td>44.0</td>\n</tr>\n<tr>\n<td>Climate-Fever</td>\n<td>18.1</td>\n<td>19.6</td>\n<td><strong>23.5</strong></td>\n</tr>\n<tr>\n<td>DBPedia</td>\n<td><strong>45.2</strong></td>\n<td>41.3</td>\n<td>35.1</td>\n</tr>\n<tr>\n<td>FEVER</td>\n<td>78.8</td>\n<td><strong>79.5</strong></td>\n<td>72.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td>35.4</td>\n<td>36.8</td>\n<td><strong>41.6</strong></td>\n</tr>\n<tr>\n<td>HotpotQA</td>\n<td><strong>67.5</strong></td>\n<td>65.9</td>\n<td>61.4</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>33.7</td>\n<td><strong>33.8</strong></td>\n<td>32.5</td>\n</tr>\n<tr>\n<td>NQ</td>\n<td>56.1</td>\n<td>54.9</td>\n<td><strong>60.4</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>85.5</td>\n<td>82.3</td>\n<td><strong>88.2</strong></td>\n</tr>\n<tr>\n<td>SCIDOCS</td>\n<td>15.4</td>\n<td>16.9</td>\n<td><strong>19.9</strong></td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td>68.9</td>\n<td><strong>70.1</strong></td>\n<td>66.7</td>\n</tr>\n<tr>\n<td>TREC-COVID</td>\n<td>72.6</td>\n<td><strong>75.0</strong></td>\n<td>65.9</td>\n</tr>\n<tr>\n<td>Webis-touch2020</td>\n<td>26.0</td>\n<td><strong>27.0</strong></td>\n<td>26.2</td>\n</tr>\n<tr>\n<td>LoCo</td>\n<td>74.3</td>\n<td>83.7</td>\n<td><strong>85.4</strong></td>\n</tr>\n<tr>\n<td>Moyenne</td>\n<td>51.7</td>\n<td><strong>52.6</strong></td>\n<td>51.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ce tableau démontre les performances supérieures de <code>jina-colbert-v1-en</code>, particulièrement dans les scénarios nécessitant des longueurs de contexte plus importantes par rapport au ColBERTv2 original. Notez que <code>jina-embeddings-v2-base-en</code> <a href=\"https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io\">utilise plus de données d'entraînement</a>, tandis que <code>jina-colbert-v1-en</code> n'utilise que MSMARCO, ce qui peut justifier les bonnes performances de <code>jina-embeddings-v2-base-en</code> sur certaines tâches.</p><h3 id=\"example-usage-of-jina-colbert-v1-en\">Exemple d'utilisation de <code>jina-colbert-v1-en</code></h3><p>Cet extrait de code décrit le processus d'indexation avec Jina-ColBERT, mettant en évidence sa prise en charge des documents longs.</p><pre><code class=\"language-python\">from colbert import Indexer\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\n\nn_gpu: int = 1  # Set your number of available GPUs\nexperiment: str = \"\"  # Name of the folder where the logs and created indices will be stored\nindex_name: str = \"\"  # The name of your index, i.e. the name of your vector database\n\nif __name__ == \"__main__\":\n    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):\n        config = ColBERTConfig(\n          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents\n        )\n        indexer = Indexer(\n          checkpoint=\"jinaai/jina-colbert-v1-en\",\n          config=config,\n        )\n        documents = [\n          \"ColBERT is an efficient and effective passage retrieval model.\",\n          \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.\",\n          \"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.\",\n          \"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.\",\n          \"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.\",\n          \"Jina is an easier way to build neural search systems.\",\n          \"You can use Jina-ColBERT to build neural search systems with ease.\",\n          # Add more documents here to ensure the clustering work correctly\n        ]\n        indexer.index(name=index_name, collection=documents)\n</code></pre><h3 id=\"use-jina-colbert-v1-en-in-ragatouille\">Utiliser <code>jina-colbert-v1-en</code> dans RAGatouille</h3><p>RAGatouille est une nouvelle bibliothèque Python qui facilite l'utilisation de méthodes de recherche avancées dans les pipelines RAG. Elle est conçue pour être modulaire et facilement intégrable, permettant aux utilisateurs d'exploiter la recherche de pointe de manière transparente. L'objectif principal de RAGatouille est de simplifier l'application de modèles complexes comme ColBERT dans les pipelines RAG, rendant accessibles ces méthodes aux développeurs sans nécessiter une expertise approfondie de la recherche sous-jacente. Grâce à <a href=\"https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io\">Benjamin Clavié</a>, vous pouvez maintenant utiliser facilement <code>jina-colbert-v1-en</code> :</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\n# Get your model &amp; collection of big documents ready\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v1-en\")\nmy_documents = [\n    \"very long document1\",\n    \"very long document2\",\n    # ... more documents\n]\n\n# And create an index with them at full length!\nRAG.index(collection=my_documents,\n          index_name=\"the_biggest_index\",\n          max_document_length=8190,)\n\n# or encode them in-memory with no truncation, up to your model's max length\nRAG.encode(my_documents)\n</code></pre><p>Pour plus d'informations détaillées et pour explorer davantage Jina-ColBERT, vous pouvez visiter la <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">page Hugging Face</a>.</p><h2 id=\"conclusion\">Conclusion</h2><p>ColBERT représente une avancée significative dans le domaine de la recherche d'information. En permettant des longueurs de contexte plus importantes avec Jina-ColBERT et en maintenant la compatibilité avec l'approche d'interaction tardive de ColBERT, il offre une alternative puissante pour les développeurs cherchant à implémenter des fonctionnalités de recherche à la pointe de la technologie.</p><p>Associé à la bibliothèque RAGatouille, qui simplifie l'intégration de modèles de recherche complexes dans les pipelines RAG, les développeurs peuvent désormais exploiter facilement la puissance de la recherche avancée, rationalisant leurs flux de travail et améliorant leurs applications. La synergie entre Jina-ColBERT et RAGatouille illustre un progrès remarquable dans l'accessibilité et l'efficacité des modèles de recherche IA avancés pour une utilisation pratique.</p>",
  "comment_id": "65d3a2134a32310001f5b71b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-02-19T19:46:43.000+01:00",
  "updated_at": "2024-08-30T23:11:22.000+02:00",
  "published_at": "2024-02-20T02:19:04.000+01:00",
  "custom_excerpt": "Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
  "excerpt": "Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche.",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon theater or concert hall marquee letters lit up at night with city lights and faint \"Adobe Sto\" visible.",
  "feature_image_caption": null
}
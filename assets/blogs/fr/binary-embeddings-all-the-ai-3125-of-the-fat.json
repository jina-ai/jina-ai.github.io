{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "Plongements binaires : Toute l'IA, 3,125 % du volume",
  "html": "<p>Les embeddings sont devenus la pierre angulaire d'une variété d'applications d'IA et de traitement du langage naturel, offrant un moyen de représenter le sens des textes sous forme de vecteurs de haute dimension. Cependant, avec l'augmentation de la taille des modèles et des quantités croissantes de données traitées par les modèles d'IA, les exigences en matière de calcul et de stockage pour les embeddings traditionnels ont augmenté. Les embeddings binaires ont été introduits comme une alternative compacte et efficace qui maintient des performances élevées tout en réduisant drastiquement les besoins en ressources.</p><p>Les embeddings binaires sont une façon d'atténuer ces exigences en ressources en réduisant la taille des vecteurs d'embedding jusqu'à 96 % (96,875 % dans le cas des Jina Embeddings). Les utilisateurs peuvent exploiter la puissance des embeddings binaires compacts dans leurs applications d'IA avec une perte minimale de précision.</p><h2 id=\"what-are-binary-embeddings\">Que Sont les Embeddings Binaires ?</h2><p>Les embeddings binaires sont une forme spécialisée de représentation des données où les vecteurs traditionnels à virgule flottante de haute dimension sont transformés en vecteurs binaires. Cela permet non seulement de compresser les embeddings mais aussi de conserver presque toute l'intégrité et l'utilité des vecteurs. L'essence de cette technique réside dans sa capacité à maintenir la sémantique et les distances relationnelles entre les points de données même après conversion.<br><br>La magie derrière les embeddings binaires est la quantification, une méthode qui transforme des nombres de haute précision en nombres de précision inférieure. Dans la modélisation IA, cela signifie souvent convertir les nombres à virgule flottante 32 bits des embeddings en représentations avec moins de bits, comme des entiers 8 bits.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">La binarisation est la transformation de toutes les valeurs scalaires en 0 ou 1, comme la conversion d'une image couleur en une image avec uniquement des pixels noirs ou blancs. Image : 神奈川沖浪裏 (1831) par 葛飾 (Hokusai)</span></figcaption></figure><p>Les embeddings binaires poussent cela à l'extrême, réduisant chaque valeur à 0 ou 1. La transformation des nombres à virgule flottante 32 bits en chiffres binaires réduit la taille des vecteurs d'embedding d'un facteur 32, soit une réduction de 96,875 %. Les opérations vectorielles sur les embeddings résultants sont beaucoup plus rapides. L'utilisation d'accélérations matérielles disponibles sur certaines puces peut augmenter la vitesse des comparaisons vectorielles bien au-delà d'un facteur 32 lorsque les vecteurs sont binarisés.</p><p>Certaines informations sont inévitablement perdues durant ce processus, mais cette perte est minimisée lorsque le modèle est très performant. Si les embeddings non quantifiés de différentes choses sont maximalement différents, alors la binarisation est plus susceptible de bien préserver cette différence. Sinon, il peut être difficile d'interpréter correctement les embeddings.</p><p>Les modèles Jina Embeddings sont entraînés pour être très robustes exactement de cette manière, les rendant bien adaptés à la binarisation.</p><p>De tels embeddings compacts rendent possibles de nouvelles applications d'IA, particulièrement dans des contextes aux ressources limitées comme les utilisations mobiles et sensibles au temps.</p><p>Ces avantages en termes de coûts et de temps de calcul s'accompagnent d'un coût de performance relativement faible, comme le montre le graphique ci-dessous.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10 : Scores calculés en utilisant le </em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">Normalized Discounted Cumulative Gain</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\"> pour les 10 premiers résultats.</em></i></figcaption></figure><p>Pour <code>jina-embeddings-v2-base-en</code>, la quantification binaire réduit la précision de récupération de 47,13 % à 42,05 %, une perte d'environ 10 %. Pour <code>jina-embeddings-v2-base-de</code>, cette perte n'est que de 4 %, passant de 44,39 % à 42,65 %.</p><p>Les modèles Jina Embeddings performent si bien lors de la production de vecteurs binaires car ils sont entraînés pour créer une distribution plus uniforme des embeddings. Cela signifie que deux embeddings différents seront probablement plus éloignés l'un de l'autre dans plus de dimensions que les embeddings d'autres modèles. Cette propriété garantit que ces distances sont mieux représentées par leurs formes binaires.</p><h2 id=\"how-do-binary-embeddings-work\">Comment Fonctionnent les Embeddings Binaires ?</h2><p>Pour comprendre comment cela fonctionne, considérons trois embeddings : <em>A</em>, <em>B</em>, et <em>C</em>. Ces trois sont tous des vecteurs à virgule flottante complets, non binarisés. Maintenant, disons que la distance de <em>A</em> à <em>B</em> est supérieure à la distance de <em>B</em> à <em>C</em>. Avec les embeddings, nous utilisons typiquement la <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">distance cosinus</a>, donc : </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>Si nous binarisons <em>A</em>, <em>B</em>, et <em>C</em>, nous pouvons mesurer la distance plus efficacement avec la <a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">distance de Hamming</a>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Distance de Hamming sur un cube. Gauche : Distance de A à B est 1. Droite : Distance de B à C est 2.</span></figcaption></figure><p>Appelons <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em> et <em>C<sub>bin</sub></em> les versions binarisées de <em>A</em>, <em>B</em> et <em>C</em>.</p>\n<p>Pour les vecteurs binaires, si la distance cosinus entre <em>A<sub>bin</sub></em> et <em>B<sub>bin</sub></em> est supérieure à celle entre <em>B<sub>bin</sub></em> et <em>C<sub>bin</sub></em>, alors la distance de Hamming entre <em>A<sub>bin</sub></em> et <em>B<sub>bin</sub></em> est supérieure ou égale à la distance de Hamming entre <em>B<sub>bin</sub></em> et <em>C<sub>bin</sub></em>.</p>\n<p>Donc si : </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>alors pour les distances de Hamming : </p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Idéalement, lorsque nous binarisons les embeddings, nous voulons que les mêmes relations avec les embeddings complets s'appliquent aux embeddings binaires. Cela signifie que si une distance est supérieure à une autre pour le cosinus à virgule flottante, elle devrait être supérieure pour la distance de Hamming entre leurs équivalents binarisés :</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Nous ne pouvons pas rendre cela vrai pour tous les triplets d'embeddings, mais nous pouvons le rendre vrai pour presque tous.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Les points bleus correspondent aux vecteurs à virgule flottante complets et les rouges à leurs équivalents binarisés. </span></figcaption></figure><p>Avec un vecteur binaire, nous pouvons traiter chaque dimension comme soit présente (un) soit absente (zéro). Plus deux vecteurs sont éloignés l'un de l'autre sous forme non binaire, plus la probabilité est élevée que dans une dimension donnée, l'un ait une valeur positive et l'autre une valeur négative. Cela signifie que sous forme binaire, il y aura très probablement plus de dimensions où l'un a un zéro et l'autre un un. Cela les rend plus éloignés selon la distance de Hamming.</p><p>L'inverse s'applique aux vecteurs qui sont plus proches : Plus les vecteurs non binaires sont proches, plus la probabilité est élevée que dans toute dimension, les deux aient des zéros ou des uns. Cela les rend plus proches selon la distance de Hamming.</p><p>Les modèles Jina Embeddings sont si bien adaptés à la binarisation car nous les entraînons en utilisant l'extraction négative et d'autres pratiques d'ajustement fin pour augmenter particulièrement la distance entre les choses dissemblables et réduire la distance entre les choses similaires. Cela rend les embeddings plus robustes, plus sensibles aux similitudes et aux différences, et rend la distance de Hamming entre les embeddings binaires plus proportionnelle à la distance cosinus entre les non binaires.</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">Combien Puis-je Économiser avec les Embeddings Binaires de Jina AI ?</h2><p>L'adoption des modèles d'embedding binaires de Jina AI ne réduit pas seulement la latence dans les applications sensibles au temps, mais génère également des avantages considérables en termes de coûts, comme le montre le tableau ci-dessous :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Modèle</th>\n<th>Mémoire pour<br/>250 millions<br/>d'embeddings</th>\n<th>Moyenne des<br/>benchmarks de<br/>récupération</th>\n<th>Prix estimé sur AWS<br/>(3,8 $ par GB/mois<br/>avec instances x2gb)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Embeddings à virgule flottante 32 bits</td>\n<td>715 GB</td>\n<td>47,13</td>\n<td>35 021 $</td>\n</tr>\n<tr>\n<td>Embeddings binaires</td>\n<td>22,3 GB</td>\n<td>42,05</td>\n<td>1 095 $</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Ces économies de plus de 95 % ne s'accompagnent que d'une réduction d'environ 10 % de la précision de recherche.</p><p>Ces économies sont encore plus importantes que l'utilisation de vecteurs binarisés du <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">modèle Ada 2 d'OpenAI</a> ou de <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Embed v3 de Cohere</a>, qui produisent tous deux des embeddings de sortie de 1024 dimensions ou plus. Les embeddings de Jina AI n'ont que 768 dimensions tout en offrant des performances comparables aux autres modèles, les rendant plus petits même avant la quantification pour une précision équivalente.</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Les vecteurs binaires économisent la mémoire, le temps de calcul, la bande passante de transmission et l'espace disque, offrant des avantages financiers dans plusieurs catégories</strong></b>. </div></div><p>Ces économies sont également environnementales, utilisant moins de matériaux rares et moins d'énergie.</p><h2 id=\"get-started\">Pour Commencer</h2><p>Pour obtenir des embeddings binaires en utilisant l'<a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">API Jina Embeddings</a>, il suffit d'ajouter le paramètre <code>encoding_type</code> à votre appel API, avec la valeur <code>binary</code> pour obtenir l'embedding binarisé encodé en entiers signés, ou <code>ubinary</code> pour les entiers non signés.</p><h3 id=\"directly-access-jina-embedding-api\">Accès Direct à l'API Jina Embedding</h3><p>Utilisation de <code>curl</code> :</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>Ou via l'API Python <code>requests</code> :</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>Avec la requête Python ci-dessus, vous obtiendrez la réponse suivante en inspectant <code>response.json()</code> :</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>Ce sont deux vecteurs d'embedding binaires stockés sous forme de 96 entiers signés sur 8 bits. Pour les décompresser en 768 valeurs de 0 et 1, vous devez utiliser la bibliothèque <code>numpy</code> :</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>Le résultat est un vecteur de 768 dimensions ne contenant que des 0 et des 1 :</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Utilisation de la Quantification Binaire dans Qdrant</h3><p>Vous pouvez également utiliser la <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">bibliothèque d'intégration de Qdrant</a> pour mettre directement les embeddings binaires dans votre base de vecteurs Qdrant. Comme Qdrant a implémenté en interne <code>BinaryQuantization</code>, vous pouvez l'utiliser comme configuration prédéfinie pour toute la collection de vecteurs, permettant de récupérer et stocker des vecteurs binaires sans autres modifications de votre code.</p><p>Voir le code d'exemple ci-dessous pour savoir comment faire :</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Fournir la clé API Jina et choisir l'un des modèles disponibles.\n# Vous pouvez obtenir une clé d'essai gratuite ici : https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # ou \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 pour la variante small\n\n# Obtenir les embeddings depuis l'API\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Indexer les embeddings dans Qdrant\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>Pour configurer la recherche, vous devez utiliser les paramètres <code>oversampling</code> et <code>rescore</code>:</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">Utilisation de LlamaIndex</h3><p>Pour utiliser les embeddings binaires Jina avec LlamaIndex, définissez le paramètre <code>encoding_queries</code> sur <code>binary</code> lors de l'instanciation de l'objet <code>JinaEmbedding</code> :</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# Vous pouvez obtenir une clé d'essai gratuite sur https://jina.ai/embeddings/\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">Autres bases de données vectorielles prenant en charge les embeddings binaires</h3><p>Les bases de données vectorielles suivantes fournissent une prise en charge native des vecteurs binaires :</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB par DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">Exemple</h2><p>Pour vous montrer les embeddings binaires en action, nous avons pris une sélection de résumés de <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a>, et obtenu à la fois des vecteurs en virgule flottante 32 bits et binaires en utilisant <code>jina-embeddings-v2-base-en</code>. Nous les avons ensuite comparés aux embeddings pour une requête exemple : « 3D segmentation ».</p><p>Vous pouvez voir dans le tableau ci-dessous que les trois premiers résultats sont les mêmes et quatre des cinq premiers correspondent. L'utilisation de vecteurs binaires produit des correspondances presque identiques.</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>Rang</th>\n<th>Distance<br/>Hamming</th>\n<th>Texte correspondant</th>\n<th>Cosinus</th>\n<th>Texte correspondant</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "32-bits, c'est beaucoup de précision pour quelque chose d'aussi robuste et imprécis qu'un modèle d'IA. Nous nous sommes donc débarrassés de 31 d'entre eux ! Les embeddings binaires sont plus petits, plus rapides et très performants.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
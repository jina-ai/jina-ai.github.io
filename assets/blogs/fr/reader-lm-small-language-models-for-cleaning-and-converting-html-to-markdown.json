{
  "slug": "reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown",
  "id": "66dff7eba241f5000155d851",
  "uuid": "49dc11d4-e792-49d6-b778-512add9024bc",
  "title": "Reader-LM : Petits modèles de langage pour nettoyer et convertir le HTML en Markdown",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/reader-lm-0.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/reader-lm-0.5b · Hugging Face</div><div class=\"kg-bookmark-description\">Nous sommes en route pour faire progresser et démocratiser l'intelligence artificielle grâce à l'open source et la science ouverte.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/reader-lm-0.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/reader-lm-1.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/reader-lm-1.5b · Hugging Face</div><div class=\"kg-bookmark-description\">Nous sommes en route pour faire progresser et démocratiser l'intelligence artificielle grâce à l'open source et la science ouverte.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/reader-lm-1.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>En avril 2024, nous avons lancé <a href=\"https://jina.ai/reader?ref=jina-ai-gmbh.ghost.io\">Jina Reader</a>, une API simple qui convertit n'importe quelle URL en markdown compatible LLM avec un simple préfixe : <code>r.jina.ai</code>. Malgré la programmation réseau sophistiquée en coulisses, la partie « lecture » principale est assez simple. D'abord, nous utilisons un navigateur Chrome headless pour récupérer la source de la page web. Ensuite, nous utilisons le package <a href=\"https://github.com/mozilla/readability?ref=jina-ai-gmbh.ghost.io\">Readability</a> de Mozilla pour extraire le contenu principal, en supprimant les éléments comme les en-têtes, les pieds de page, les barres de navigation et les barres latérales. Enfin, nous convertissons le HTML nettoyé en markdown en utilisant des <a href=\"https://x.com/JinaAI_/status/1823756993108304135?ref=jina-ai-gmbh.ghost.io\">expressions régulières</a> et la <a href=\"https://github.com/mixmark-io/turndown?ref=jina-ai-gmbh.ghost.io\">bibliothèque Turndown</a>. Le résultat est un fichier markdown bien structuré, prêt à être utilisé par les LLM pour l'ancrage, le résumé et le raisonnement.</p><p>Dans les premières semaines suivant la sortie de Jina Reader, nous avons reçu beaucoup de retours, particulièrement concernant la qualité du contenu. Certains utilisateurs l'ont trouvé trop détaillé, tandis que d'autres estimaient qu'il ne l'était pas assez. Il y a également eu des signalements indiquant que le filtre Readability supprimait le mauvais contenu ou que Turndown avait du mal à convertir certaines parties du HTML en markdown. Heureusement, beaucoup de ces problèmes ont été résolus avec succès en corrigeant le pipeline existant avec de nouveaux motifs regex ou heuristiques.</p><p>Depuis lors, nous nous sommes posé une question : au lieu de le corriger avec plus d'heuristiques et de regex (qui deviennent de plus en plus difficiles à maintenir et ne sont pas multilingues), pouvons-nous résoudre ce problème <em>de bout en bout</em> avec un modèle de langage ?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/Heading--48-.png\" class=\"kg-image\" alt=\"Flowchart illustrating the conversion of raw HTML to Markdown format using readability and turndown libraries, plus regex/heu\" loading=\"lazy\" width=\"1800\" height=\"945\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/Heading--48-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/Heading--48-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/09/Heading--48-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/09/Heading--48-.png 1800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Illustration de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>reader-lm</span></code><span style=\"white-space: pre-wrap;\">, remplaçant le pipeline readability+turndown+heuristiques regex par un petit modèle de langage.</span></figcaption></figure><p>À première vue, l'utilisation des LLM pour le nettoyage des données peut sembler excessive en raison de leur faible rapport coût-efficacité et de leur lenteur. Mais que se passe-t-il si nous envisageons un <strong>petit modèle de langage (SLM)</strong> — un modèle avec moins d'un milliard de paramètres qui peut fonctionner efficacement en périphérie ? Cela semble beaucoup plus attrayant, n'est-ce pas ? Mais est-ce vraiment faisable ou juste un vœu pieux ? Selon la loi d'échelle, moins de paramètres conduisent généralement à une réduction des capacités de raisonnement et de synthèse. Un SLM pourrait donc même avoir du mal à générer un contenu significatif si sa taille de paramètres est trop petite. Pour explorer cela plus en détail, examinons de plus près la tâche de conversion HTML vers Markdown :</p><ul><li>Premièrement, la tâche que nous envisageons <strong>n'est pas aussi créative ou complexe que les tâches LLM typiques</strong>. Dans le cas de la conversion HTML vers markdown, le modèle doit principalement <strong>copier sélectivement</strong> de l'entrée vers la sortie (c'est-à-dire ignorer le balisage HTML, les barres latérales, les en-têtes, les pieds de page), avec un effort minimal consacré à la génération de nouveau contenu (principalement l'insertion de syntaxe markdown). Cela contraste fortement avec les tâches plus larges que gèrent les LLM, comme la génération de poèmes ou l'écriture de code, où la sortie implique beaucoup plus de créativité et n'est pas une simple copie-colle de l'entrée. Cette observation suggère qu'un SLM pourrait fonctionner, car la tâche <em>semble</em> plus simple que la génération de texte plus générale.</li><li>Deuxièmement, nous devons <strong>prioriser le support du contexte long</strong>. Le HTML moderne contient souvent beaucoup plus de bruit que le simple balisage <code>&lt;div&gt;</code>. Le CSS en ligne et les scripts peuvent facilement faire gonfler le code à des centaines de milliers de tokens. Pour qu'un SLM soit pratique dans ce scénario, la longueur du contexte doit être suffisamment grande. Une longueur de tokens de 8K ou 16K n'est <em>pas du tout</em> utile.</li></ul><p>Il semble que ce dont nous ayons besoin soit un SLM <strong><em>peu profond mais large</em></strong>. \"Peu profond\" dans le sens où la tâche est principalement un simple \"copier-coller\", donc moins de blocs transformers sont nécessaires ; et \"large\" dans le sens où il nécessite un support de contexte long pour être pratique, donc le mécanisme d'attention nécessite une certaine attention. Des recherches antérieures ont montré que la longueur du contexte et la capacité de raisonnement sont étroitement liées. Pour un SLM, il est extrêmement difficile d'optimiser les deux dimensions tout en gardant la taille des paramètres petite.</p><p>Aujourd'hui, nous sommes ravis d'annoncer la première version de cette solution avec la sortie de <code>reader-lm-0.5b</code> et <code>reader-lm-1.5b</code>, deux SLM spécifiquement entraînés <strong>pour générer du markdown propre directement à partir de HTML brut bruité</strong>. Les deux modèles sont multilingues et supportent une longueur de contexte allant jusqu'à <strong>256K tokens</strong>. Malgré leur taille compacte, ces modèles atteignent des performances état de l'art sur cette tâche, surpassant leurs homologues LLM plus grands tout en ne faisant que 1/50ème de leur taille.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/Reader-LM-vs-LLMs-on-the-HTML2Markdown-task--1-.svg\" class=\"kg-image\" alt=\"Bar chart showing Reader-LM's superior HTML2Markdown task performance with the highest score at 0.72 against various LLMs.\" loading=\"lazy\" width=\"805\" height=\"514\"></figure><p>Voici les spécifications des deux modèles :</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th style=\"text-align:center\">reader-lm-0.5b</th>\n<th style=\"text-align:center\">reader-lm-1.5b</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td># Paramètres</td>\n<td style=\"text-align:center\">494M</td>\n<td style=\"text-align:center\">1.54B</td>\n</tr>\n<tr>\n<td>Longueur de contexte</td>\n<td style=\"text-align:center\">256K</td>\n<td style=\"text-align:center\">256K</td>\n</tr>\n<tr>\n<td>Hidden Size</td>\n<td style=\"text-align:center\">896</td>\n<td style=\"text-align:center\">1536</td>\n</tr>\n<tr>\n<td># Layers</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">28</td>\n</tr>\n<tr>\n<td># Query Heads</td>\n<td style=\"text-align:center\">14</td>\n<td style=\"text-align:center\">12</td>\n</tr>\n<tr>\n<td># KV Heads</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">2</td>\n</tr>\n<tr>\n<td>Head Size</td>\n<td style=\"text-align:center\">64</td>\n<td style=\"text-align:center\">128</td>\n</tr>\n<tr>\n<td>Intermediate Size</td>\n<td style=\"text-align:center\">4864</td>\n<td style=\"text-align:center\">8960</td>\n</tr>\n<tr>\n<td>Multilingue</td>\n<td style=\"text-align:center\">Oui</td>\n<td style=\"text-align:center\">Oui</td>\n</tr>\n<tr>\n<td>Dépôt HuggingFace</td>\n<td style=\"text-align:center\"><a href=\"https://huggingface.co/jinaai/reader-lm-0.5b/?ref=jina-ai-gmbh.ghost.io\">Lien</a></td>\n<td style=\"text-align:center\"><a href=\"https://huggingface.co/jinaai/reader-lm-1.5b/?ref=jina-ai-gmbh.ghost.io\">Lien</a></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"get-started-with-reader-lm\">Débuter avec Reader-LM</h2><h3 id=\"on-google-colab\">Sur Google Colab</h3><p>La façon la plus simple d'expérimenter <code>reader-lm</code> est d'exécuter notre notebook Colab, où nous démontrons comment utiliser <code>reader-lm-1.5b</code> pour convertir le site Hacker News en markdown. Le notebook est optimisé pour fonctionner sans problème sur le niveau GPU T4 gratuit de Google Colab. Vous pouvez également charger <code>reader-lm-0.5b</code> ou changer l'URL vers n'importe quel site web et explorer la sortie. Notez que l'entrée (c'est-à-dire le prompt) du modèle est le HTML brut — aucune instruction de préfixe n'est requise.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1wXWyj5hOxEHY6WeHbOwEzYAC0WB1I5uA?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Veuillez noter que le GPU T4 de la version gratuite présente des limitations qui peuvent empêcher l'utilisation d'optimisations avancées lors de l'exécution du modèle. Des fonctionnalités comme bfloat16 et flash attention ne sont pas disponibles sur le T4, ce qui peut entraîner une utilisation plus élevée de la VRAM et des performances plus lentes pour les entrées plus longues. <strong>Pour les environnements de production, nous recommandons d'utiliser un GPU haut de gamme comme le RTX 3090/4090 pour des performances nettement meilleures.</strong></p><h3 id=\"in-production-available-on-azure-aws-soon\">En Production : Bientôt Disponible sur Azure & AWS</h3><p>Reader-LM est disponible sur Azure Marketplace et AWS SageMaker. Si vous devez utiliser ces modèles au-delà de ces plateformes ou sur site au sein de votre entreprise, notez que les deux modèles sont sous licence CC BY-NC 4.0. <a href=\"https://jina.ai/contact-sales/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Pour toute demande d'utilisation commerciale, n'hésitez pas à nous contacter.</a></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-nli7b6dueo424?sr=0-1&ref_=beagle&applicationId=AWSMPContessa&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Reader-LM 0.5b</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-ms27ixcwq3wjk?sr=0-2&ref_=beagle&applicationId=AWSMPContessa&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Reader-LM 1.5b</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-500m?tab=Overview&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://azuremarketplace.microsoft.com/favicon.ico\" alt=\"\"></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-1500m?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://azuremarketplace.microsoft.com/favicon.ico\" alt=\"\"></div></div></a></figure><h2 id=\"benchmark\">Benchmark</h2><p>Pour évaluer quantitativement les performances de Reader-LM, nous l'avons comparé à plusieurs grands modèles de langage, notamment : GPT-4o, Gemini-1.5-Flash, Gemini-1.5-Pro, LLaMA-3.1-70B, Qwen2-7B-Instruct.</p><p>Les modèles ont été évalués selon les métriques suivantes :</p><ul><li><strong>ROUGE-L (plus élevé est meilleur)</strong> : Cette métrique, largement utilisée pour les tâches de résumé et de questions-réponses, mesure le chevauchement entre la sortie prédite et la référence au niveau des n-grammes.</li><li><strong>Taux d'Erreur de Token (TER, plus bas est meilleur)</strong> : Cette métrique calcule le taux auquel les tokens markdown générés n'apparaissent pas dans le contenu HTML d'origine. Nous avons conçu cette métrique pour évaluer le taux d'hallucination du modèle, nous aidant à identifier les cas où le modèle produit du contenu qui n'est pas ancré dans le HTML. D'autres améliorations seront apportées sur la base d'études de cas.</li><li><strong>Taux d'Erreur de Mots (WER, plus bas est meilleur)</strong> : Couramment utilisé dans les tâches OCR et ASR, le WER prend en compte la séquence de mots et calcule les erreurs telles que les insertions (ADD), les substitutions (SUB) et les suppressions (DEL). Cette métrique fournit une évaluation détaillée des écarts entre le markdown généré et la sortie attendue.</li></ul><p>Pour exploiter les LLM pour cette tâche, nous avons utilisé l'instruction uniforme suivante comme prompt de préfixe :</p><pre><code>Your task is to convert the content of the provided HTML file into the corresponding markdown file. You need to convert the structure, elements, and attributes of the HTML into equivalent representations in markdown format, ensuring that no important information is lost. The output should strictly be in markdown format, without any additional explanations.</code></pre><p>Les résultats se trouvent dans le tableau ci-dessous.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ROUGE-L</th>\n<th>WER</th>\n<th>TER</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>reader-lm-0.5b</td>\n<td>0.56</td>\n<td>3.28</td>\n<td>0.34</td>\n</tr>\n<tr>\n<td>reader-lm-1.5b</td>\n<td><strong>0.72</strong></td>\n<td><strong>1.87</strong></td>\n<td><strong>0.19</strong></td>\n</tr>\n<tr>\n<td>gpt-4o</td>\n<td>0.43</td>\n<td>5.88</td>\n<td>0.50</td>\n</tr>\n<tr>\n<td>gemini-1.5-flash</td>\n<td>0.40</td>\n<td>21.70</td>\n<td>0.55</td>\n</tr>\n<tr>\n<td>gemini-1.5-pro</td>\n<td>0.42</td>\n<td>3.16</td>\n<td>0.48</td>\n</tr>\n<tr>\n<td>llama-3.1-70b</td>\n<td>0.40</td>\n<td>9.87</td>\n<td>0.50</td>\n</tr>\n<tr>\n<td>Qwen2-7B-Instruct</td>\n<td>0.23</td>\n<td>2.45</td>\n<td>0.70</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"qualitative-study\">Étude Qualitative</h2><p>Nous avons mené une étude qualitative en inspectant visuellement la sortie markdown. <a href=\"https://docs.google.com/spreadsheets/d/1Wb2sMdiEoToPaXohcrEznFKStt_4alVOnJD3WKkiM7o/edit?gid=1576339853&ref=jina-ai-gmbh.ghost.io#gid=1576339853\">Nous avons sélectionné 22 sources HTML</a> comprenant des articles d'actualité, des articles de blog, des pages d'accueil, des pages e-commerce et des posts de forum en plusieurs langues : anglais, allemand, japonais et chinois. Nous avons également inclus l'API Jina Reader comme référence, qui s'appuie sur des expressions régulières, des heuristiques et des règles prédéfinies.</p><p>L'évaluation s'est concentrée sur quatre dimensions clés de la sortie, chaque modèle étant noté sur une échelle de 1 (plus bas) à 5 (plus haut) :</p><ol><li><strong>Extraction des en-têtes</strong> : Évalue la capacité de chaque modèle à identifier et formater les en-têtes h1, h2,..., h6 du document en utilisant la syntaxe markdown correcte.</li><li><strong>Extraction du contenu principal</strong> : Évalue la capacité des modèles à convertir avec précision le texte principal, en préservant les paragraphes, le formatage des listes et la cohérence de la présentation.</li><li><strong>Préservation de la structure riche</strong> : Analyse l'efficacité avec laquelle chaque modèle maintient la structure globale du document, y compris les titres, sous-titres, puces et listes ordonnées.</li><li><strong>Utilisation de la syntaxe Markdown</strong> : Évalue la capacité de chaque modèle à convertir correctement les éléments HTML tels que <code>&lt;a&gt;</code> (liens), <code>&lt;strong&gt;</code> (texte en gras) et <code>&lt;em&gt;</code> (italique) en leurs équivalents markdown appropriés.</li></ol><p>Les résultats se trouvent ci-dessous.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/Qualitative-Evaluation-of-Reader-LM-vs-LLMs-and-Jina-Reader-API--1-.svg\" class=\"kg-image\" alt=\"Graphique en barres comparant Reader-LM, LLMs et l'API Jina Reader sur des métriques comme l'extraction d'en-têtes et la préservation du contenu.\" loading=\"lazy\" width=\"863\" height=\"533\"></figure><p>Reader-LM-1.5B performe de manière constante dans toutes les dimensions, excellant particulièrement dans la préservation de la structure et l'utilisation de la syntaxe markdown. Bien qu'il ne surpasse pas toujours l'API Jina Reader, ses performances sont compétitives avec des modèles plus grands comme Gemini 1.5 Pro, en faisant une alternative très efficace aux LLM plus grands. Reader-LM-0.5B, bien que plus petit, offre toujours des performances solides, particulièrement dans la préservation de la structure.</p><h2 id=\"how-we-trained-reader-lm\">Comment Nous Avons Entraîné Reader-LM</h2><h3 id=\"data-preparation\">Préparation des Données</h3><p>Nous avons utilisé l'API Jina Reader pour générer des paires d'entraînement de HTML brut et leur markdown correspondant. Durant l'expérience, nous avons constaté que les SLM sont particulièrement sensibles à la qualité des données d'entraînement. Nous avons donc construit un pipeline de données qui garantit que seules les entrées markdown de haute qualité sont incluses dans l'ensemble d'entraînement.</p><p>De plus, nous avons ajouté du HTML synthétique et leurs équivalents markdown, générés par <code>GPT-4o</code>. Par rapport au HTML du monde réel, les données synthétiques ont tendance à être beaucoup plus courtes, avec des structures plus simples et plus prévisibles, et un niveau de bruit significativement plus bas.</p><p>Enfin, nous avons concaténé le HTML et le markdown en utilisant un modèle de chat. Les données d'entraînement finales sont formatées comme suit :</p><pre><code>&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{RAW_HTML}}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{MARKDOWN}}&lt;|im_end|&gt;\n</code></pre><p>Les données d'entraînement complètes représentent 2,5 milliards de tokens.</p><h3 id=\"two-stage-training\">Entraînement en Deux Étapes</h3><p>Nous avons expérimenté avec différentes tailles de modèles, allant de 65M et 135M jusqu'à 3B paramètres. Les spécifications pour chaque modèle sont présentées dans le tableau ci-dessous.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>reader-lm-65m</th>\n<th>reader-lm-135m</th>\n<th>reader-lm-360m</th>\n<th>reader-lm-0.5b</th>\n<th>reader-lm-1.5b</th>\n<th>reader-lm-1.7b</th>\n<th>reader-lm-3b</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hidden Size</td>\n<td>512</td>\n<td>576</td>\n<td>960</td>\n<td>896</td>\n<td>1536</td>\n<td>2048</td>\n<td>3072</td>\n</tr>\n<tr>\n<td># Layers</td>\n<td>8</td>\n<td>30</td>\n<td>32</td>\n<td>24</td>\n<td>28</td>\n<td>24</td>\n<td>32</td>\n</tr>\n<tr>\n<td># Query Heads</td>\n<td>16</td>\n<td>9</td>\n<td>15</td>\n<td>14</td>\n<td>12</td>\n<td>32</td>\n<td>32</td>\n</tr>\n<tr>\n<td># KV Heads</td>\n<td>8</td>\n<td>3</td>\n<td>5</td>\n<td>2</td>\n<td>2</td>\n<td>32</td>\n<td>32</td>\n</tr>\n<tr>\n<td>Head Size</td>\n<td>32</td>\n<td>64</td>\n<td>64</td>\n<td>64</td>\n<td>128</td>\n<td>64</td>\n<td>96</td>\n</tr>\n<tr>\n<td>Intermediate Size</td>\n<td>2048</td>\n<td>1536</td>\n<td>2560</td>\n<td>4864</td>\n<td>8960</td>\n<td>8192</td>\n<td>8192</td>\n</tr>\n<tr>\n<td>Attention Bias</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>True</td>\n<td>True</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>Embedding Tying</td>\n<td>False</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>False</td>\n</tr>\n<tr>\n<td>Vocabulary Size</td>\n<td>32768</td>\n<td>49152</td>\n<td>49152</td>\n<td>151646</td>\n<td>151646</td>\n<td>49152</td>\n<td>32064</td>\n</tr>\n<tr>\n<td>Base Model</td>\n<td>Lite-Oute-1-65M-Instruct</td>\n<td>SmolLM-135M</td>\n<td>SmolLM-360M-Instruct</td>\n<td>Qwen2-0.5B-Instruct</td>\n<td>Qwen2-1.5B-Instruct</td>\n<td>SmolLM-1.7B</td>\n<td>Phi-3-mini-128k-instruct</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>L'entraînement du modèle s'est déroulé en deux étapes :</p><ol><li><strong>HTML court et simple : </strong>Dans cette étape, la longueur maximale des séquences (HTML + markdown) était fixée à 32K tokens, avec un total de 1,5 milliard de tokens d'entraînement.</li><li><strong>HTML long et complexe</strong> : la longueur de séquence a été étendue à 128K tokens, avec 1,2 milliard de tokens d'entraînement. Nous avons implémenté le mécanisme zigzag-ring-attention de <a href=\"https://github.com/zhuzilin/ring-flash-attention?ref=jina-ai-gmbh.ghost.io\">« Ring Flash Attention » de Zilin Zhu (2024)</a> pour cette étape.</li></ol><p>Étant donné que les données d'entraînement incluaient des séquences allant jusqu'à 128K tokens, nous pensons que le modèle peut prendre en charge jusqu'à 256K tokens sans problème. Cependant, la gestion de 512K tokens pourrait être difficile, car l'extension des embeddings positionnels RoPE à quatre fois la longueur de la séquence d'entraînement pourrait entraîner une dégradation des performances.</p><p>Pour les modèles de 65M et 135M paramètres, nous avons observé qu'ils pouvaient atteindre un comportement de « copie » raisonnable, mais uniquement avec des séquences courtes (moins de 1K tokens). À mesure que la longueur d'entrée augmentait, ces modèles avaient du mal à produire une sortie raisonnable. Étant donné que le code source HTML moderne peut facilement dépasser 100K tokens, une limite de 1K tokens est loin d'être suffisante.</p><h3 id=\"degeneration-and-dull-loops\">Dégénération et boucles monotones</h3><p>L'un des principaux défis que nous avons rencontrés était la dégénération, particulièrement sous forme de répétition et de bouclage. Après avoir généré quelques tokens, le modèle commençait à générer le même token de manière répétée ou se retrouvait piégé dans une boucle, répétant continuellement une courte séquence de tokens jusqu'à atteindre la longueur maximale de sortie autorisée.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/image-1.png\" class=\"kg-image\" alt=\"Dark themed coding script with repeated structural programming comments about data types, functions, and mathematical operati\" loading=\"lazy\" width=\"2000\" height=\"1278\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/09/image-1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/09/image-1.png 2040w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Un exemple de dégénération se produit lorsque le modèle commence par une génération markdown normale mais se retrouve soudainement coincé dans des « boucles monotones », comme indiqué par les flèches rouges.</span></figcaption></figure><p>Pour résoudre ce problème :</p><ul><li>Nous avons appliqué la <a href=\"https://github.com/yxuansu/SimCTG?ref=jina-ai-gmbh.ghost.io\">recherche contrastive</a> comme méthode de décodage et incorporé une perte contrastive pendant l'entraînement. D'après nos expériences, cette méthode a effectivement réduit la génération répétitive en pratique.</li><li>Nous avons implémenté un critère simple d'arrêt des répétitions dans le pipeline transformer. Ce critère détecte automatiquement quand le modèle commence à répéter des tokens et arrête le décodage plus tôt pour éviter les boucles monotones. Cette idée s'est inspirée de cette <a href=\"https://github.com/huggingface/transformers/issues/32902?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">discussion</a>.</li></ul><h3 id=\"training-efficiency-on-long-inputs\">Efficacité d'entraînement sur les entrées longues</h3><p>Pour atténuer le risque d'erreurs de mémoire insuffisante (OOM) lors du traitement d'entrées longues, nous avons implémenté un transfert de modèle par morceaux. Cette approche encode l'entrée longue avec des morceaux plus petits, réduisant l'utilisation de la VRAM.</p><p>Nous avons amélioré l'implémentation du regroupement des données dans notre framework d'entraînement, qui est basé sur Transformers Trainer. Pour optimiser l'efficacité de l'entraînement, plusieurs textes courts (par exemple, 2K tokens) sont concaténés en une seule longue séquence (par exemple, 30K tokens), permettant un entraînement sans remplissage. Cependant, dans l'implémentation originale, certains exemples courts étaient divisés en deux sous-textes et inclus dans différentes séquences d'entraînement longues. Dans ces cas, le second sous-texte perdait son contexte (par exemple, le contenu HTML brut dans notre cas), conduisant à des données d'entraînement corrompues. Cela force le modèle à s'appuyer sur ses paramètres plutôt que sur le contexte d'entrée, ce que nous considérons comme une source majeure d'hallucination.</p><p>Au final, nous avons sélectionné les modèles 0.5B et 1.5B pour publication. <strong>Le modèle 0.5B est le plus petit capable d'atteindre le comportement de « copie sélective » souhaité sur des entrées à long contexte</strong>, tandis que le modèle 1.5B est le plus petit modèle plus grand qui améliore significativement les performances sans atteindre les rendements décroissants par rapport à la taille des paramètres.</p><h3 id=\"alternative-architecture-encoder-only-model\">Architecture alternative : Modèle Encoder-Only</h3><p>Au début de ce projet, nous avons également exploré l'utilisation d'une architecture encoder-only pour aborder cette tâche. Comme mentionné précédemment, la tâche de conversion HTML vers Markdown semble être principalement une tâche de « copie sélective ». Étant donné une paire d'entraînement (HTML brut et markdown), nous pouvons étiqueter les tokens qui existent à la fois dans l'entrée et la sortie comme <code>1</code>, et le reste comme <code>0</code>. Cela transforme le problème en une tâche de classification de tokens, similaire à ce qui est utilisé dans la Reconnaissance d'Entités Nommées (NER).</p><p>Bien que cette approche semblait logique, elle présentait des défis significatifs en pratique. Premièrement, le HTML brut provenant de sources réelles est extrêmement bruyant et long, rendant les étiquettes <code>1</code> extrêmement éparses et donc difficiles à apprendre pour le modèle. Deuxièmement, l'encodage de la syntaxe markdown spéciale dans un schéma <code>0-1</code> s'est avéré problématique, car des symboles comme <code>## title</code>, <code>*bold*</code>, et <code>| table |</code> n'existent pas dans l'entrée HTML brute. Troisièmement, les tokens de sortie ne suivent pas toujours strictement l'ordre de l'entrée. Des réorganisations mineures se produisent souvent, particulièrement avec les tableaux et les liens, rendant difficile la représentation de tels comportements de réorganisation dans un schéma simple <code>0-1</code>. La réorganisation à courte distance pourrait potentiellement être gérée avec la programmation dynamique ou des algorithmes d'alignement-warping en introduisant des étiquettes comme <code>-1, -2, +1, +2</code> pour représenter les décalages de distance, transformant le problème de classification binaire en une tâche de classification de tokens multi-classes.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/output--2-.png\" class=\"kg-image\" alt=\"Chart titled &quot;Token-Level DP Alignment (Horizontal)&quot; with tokens on the x-axis and alignment on the y-axis, highlighting best\" loading=\"lazy\" width=\"2000\" height=\"1436\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/output--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/output--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/09/output--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/09/output--2-.png 2179w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Utilisation de la programmation dynamique pour aligner le HTML brut (axe X) et le markdown (axe Y) pour créer des étiquettes d'entraînement au niveau des tokens.</span></figcaption></figure><p>En résumé, résoudre le problème avec une architecture encoder-only et le traiter comme une tâche de classification de tokens a son charme, d'autant plus que les séquences d'entraînement sont beaucoup plus courtes par rapport à un modèle decoder-only, ce qui le rend plus économe en VRAM. Cependant, <strong>le défi majeur réside dans la préparation de bonnes données d'entraînement.</strong> Lorsque nous avons réalisé que le temps et l'effort consacrés au prétraitement des données - utilisant la programmation dynamique et des heuristiques pour créer des séquences d'étiquetage parfaites au niveau des tokens - étaient écrasants, nous avons décidé d'abandonner cette approche.</p><h2 id=\"conclusion\">Conclusion</h2><p>Reader-LM est un nouveau petit modèle de langage (SLM) conçu pour l'extraction et le nettoyage de données sur le web ouvert. Inspiré par Jina Reader, notre objectif était de créer une solution de modèle de langage de bout en bout capable de convertir du HTML brut et bruité en markdown propre. En parallèle, nous nous sommes concentrés sur l'efficacité des coûts, en gardant la taille du modèle réduite pour garantir que Reader-LM reste pratique et utilisable. <strong>C'est également le premier modèle décodeur uniquement à contexte long entraîné chez Jina AI.</strong></p><p>Bien que la tâche puisse sembler initialement être un simple problème de \"copie sélective\", la conversion et le nettoyage du HTML vers markdown est loin d'être simple. Plus précisément, cela nécessite que le modèle excelle dans le raisonnement contextuel conscient de la position, ce qui exige une taille de paramètres plus importante, particulièrement dans les couches cachées. En comparaison, l'apprentissage de la syntaxe markdown est relativement simple.</p><p>Durant nos expériences, nous avons également constaté que l'entraînement d'un SLM à partir de zéro est particulièrement difficile. Commencer avec un modèle pré-entraîné et poursuivre avec un entraînement spécifique à la tâche a considérablement amélioré l'efficacité de l'entraînement. Il reste encore beaucoup de marge d'amélioration en termes d'efficacité et de qualité : l'extension de la longueur du contexte, l'accélération du décodage, et l'ajout du support des instructions dans l'entrée, ce qui permettrait à Reader-LM d'extraire des parties spécifiques d'une page web en markdown.</p>",
  "comment_id": "66dff7eba241f5000155d851",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/09/reader-lm-banner.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-09-10T09:40:27.000+02:00",
  "updated_at": "2024-09-20T14:09:30.000+02:00",
  "published_at": "2024-09-11T12:25:03.000+02:00",
  "custom_excerpt": "Reader-LM-0.5B and Reader-LM-1.5B are two novel small language models inspired by Jina Reader, designed to convert raw, noisy HTML from the open web into clean markdown.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/",
  "excerpt": "Reader-LM-0.5B et Reader-LM-1.5B sont deux nouveaux modèles de langage de petite taille inspirés par Jina Reader, conçus pour convertir le HTML brut et bruité du web ouvert en markdown propre.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Technical screenshot displaying \"REAPER-LM-0.5B/1.5B\" with HTML source code for Jina's search grounding feature.",
  "feature_image_caption": null
}
{
  "slug": "late-chunking-in-long-context-embedding-models",
  "id": "66c72e30da9a33000146d836",
  "uuid": "9eda87e2-a799-4360-bac9-6a1cd0193349",
  "title": "Chunking tardif dans les modÃ¨les d'embedding Ã  contexte long",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">ğŸ’¡</div><div class=\"kg-callout-text\">Le Late Chunking est maintenant disponible dans l'API <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>. <b><strong style=\"white-space: pre-wrap;\">Ordre de lecture recommandÃ© : partie I, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">partie II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">article de recherche</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><p></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What Late Chunking Really Is &amp; What It's Not: Part II</div><div class=\"kg-bookmark-description\">Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-4.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Nouveau ! Partie II : plongÃ©e profonde dans les indices de frontiÃ¨re et les idÃ©es reÃ§ues.</span></p></figcaption></figure><p>Il y a environ un an, en octobre 2023, nous avons publiÃ© <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">le premier modÃ¨le d'embedding open source au monde avec une longueur de contexte de 8K</a>, <code>jina-embeddings-v2-base-en</code>. Depuis lors, il y a eu beaucoup de dÃ©bats sur l'utilitÃ© du contexte long dans les modÃ¨les d'embedding. Pour de nombreuses applications, encoder un document de milliers de mots en une seule reprÃ©sentation d'embedding n'est pas idÃ©al. De nombreux cas d'utilisation nÃ©cessitent de rÃ©cupÃ©rer des portions plus petites du texte, et les systÃ¨mes de recherche basÃ©s sur des vecteurs denses fonctionnent souvent mieux avec des segments de texte plus petits, car la sÃ©mantique est moins susceptible d'Ãªtre Â« sur-compressÃ©e Â» dans les vecteurs d'embedding.</p><p>La GÃ©nÃ©ration AugmentÃ©e par Recherche (RAG) est l'une des applications les plus connues qui nÃ©cessite de diviser les documents en petits morceaux de texte (disons dans les 512 tokens). Ces chunks sont gÃ©nÃ©ralement stockÃ©s dans une base de donnÃ©es vectorielle, avec des reprÃ©sentations vectorielles gÃ©nÃ©rÃ©es par un modÃ¨le d'embedding de texte. Pendant l'exÃ©cution, le mÃªme modÃ¨le d'embedding encode une requÃªte en une reprÃ©sentation vectorielle, qui est ensuite utilisÃ©e pour identifier les chunks de texte pertinents stockÃ©s. Ces chunks sont ensuite transmis Ã  un grand modÃ¨le de langage (LLM), qui synthÃ©tise une rÃ©ponse Ã  la requÃªte basÃ©e sur les textes rÃ©cupÃ©rÃ©s.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--1-.svg\" class=\"kg-image\" alt=\"Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec\" loading=\"lazy\" width=\"1458\" height=\"307\"><figcaption><span style=\"white-space: pre-wrap;\">Un pipeline RAG typique de dÃ©coupage-embedding-rÃ©cupÃ©ration-gÃ©nÃ©ration.</span></figcaption></figure><p>En bref, l'embedding de plus petits chunks semble Ãªtre prÃ©fÃ©rable, en partie en raison des tailles d'entrÃ©e limitÃ©es des LLM en aval, mais aussi parce qu'il existe <strong>une prÃ©occupation que les informations contextuelles importantes dans un long contexte puissent Ãªtre diluÃ©es lorsqu'elles sont compressÃ©es en un seul vecteur.</strong></p><p>Mais si l'industrie n'a jamais besoin que de modÃ¨les d'embedding avec une longueur de contexte de 512, <em>quel est l'intÃ©rÃªt de former des modÃ¨les avec une longueur de contexte de 8192 ?</em></p><p>Dans cet article, nous revenons sur cette question importante, bien que dÃ©licate, en explorant les limites du pipeline naÃ¯f de dÃ©coupage-embedding dans RAG. Nous introduisons une nouvelle approche appelÃ©e <strong>Â« Late Chunking Â»</strong>, qui exploite les riches informations contextuelles fournies par les modÃ¨les d'embedding de longueur 8192 pour incorporer plus efficacement les chunks.</p><h2 id=\"the-lost-context-problem\">Le problÃ¨me du contexte perdu</h2><p>Le pipeline RAG simple de dÃ©coupage-embedding-rÃ©cupÃ©ration-gÃ©nÃ©ration n'est pas sans dÃ©fis. Plus prÃ©cisÃ©ment, <strong>ce processus peut dÃ©truire les dÃ©pendances contextuelles Ã  longue distance.</strong> En d'autres termes, lorsque des informations pertinentes sont rÃ©parties sur plusieurs chunks, sortir des segments de texte de leur contexte peut les rendre inefficaces, rendant cette approche particuliÃ¨rement problÃ©matique.</p><p>Dans l'image ci-dessous, un article WikipÃ©dia est divisÃ© en chunks de phrases. Vous pouvez voir que des expressions comme Â« its Â» et Â« the city Â» font rÃ©fÃ©rence Ã  Â« Berlin Â», qui n'est mentionnÃ© que dans la premiÃ¨re phrase. Cela rend plus difficile pour le modÃ¨le d'embedding de lier ces rÃ©fÃ©rences Ã  l'entitÃ© correcte, produisant ainsi une reprÃ©sentation vectorielle de moindre qualitÃ©.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"Comparative panels display Berlin's Wikipedia article and its chunked text to highlight clarity and readability benefits.\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png 1774w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Cela signifie que si nous divisons un long article en chunks de phrases, comme dans l'exemple ci-dessus, un systÃ¨me RAG pourrait avoir du mal Ã  rÃ©pondre Ã  une requÃªte comme Â« Quelle est la population de Berlin ? Â» Parce que le nom de la ville et la population n'apparaissent jamais ensemble dans un seul chunk, et sans contexte de document plus large, un LLM prÃ©sentÃ© avec l'un de ces chunks ne peut pas rÃ©soudre les rÃ©fÃ©rences anaphoriques comme Â« it Â» ou Â« the city Â».</p><p>Il existe certaines heuristiques pour attÃ©nuer ce problÃ¨me, comme le rÃ©Ã©chantillonnage avec une fenÃªtre glissante, l'utilisation de plusieurs longueurs de fenÃªtres de contexte et l'exÃ©cution de balayages de documents en plusieurs passes. Cependant, comme toutes les heuristiques, ces approches sont alÃ©atoires ; elles peuvent fonctionner dans certains cas, mais il n'y a aucune garantie thÃ©orique de leur efficacitÃ©.</p><h2 id=\"the-solution-late-chunking\">La solution : Late Chunking</h2><p>L'approche d'encodage naÃ¯ve (comme on le voit sur le cÃ´tÃ© gauche de l'image ci-dessous) implique d'utiliser des phrases, des paragraphes ou des limites de longueur maximale pour diviser le texte <em>a priori</em>. Ensuite, un modÃ¨le d'embedding est appliquÃ© de maniÃ¨re rÃ©pÃ©tÃ©e Ã  ces chunks rÃ©sultants. Pour gÃ©nÃ©rer un seul embedding pour chaque chunk, de nombreux modÃ¨les d'embedding utilisent le <em>mean pooling</em> sur ces embeddings au niveau des tokens pour produire un seul vecteur d'embedding.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--4-.svg\" class=\"kg-image\" alt=\"Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.\" loading=\"lazy\" width=\"1020\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Une illustration de la stratÃ©gie de dÃ©coupage naÃ¯ve (gauche) et de la stratÃ©gie de late chunking (droite).</span></figcaption></figure><p>En revanche, l'approche Â« Late Chunking Â» que nous proposons dans cet article applique d'abord la couche transformer du modÃ¨le d'embedding Ã  <em>l'ensemble du texte</em> ou autant que possible. Cela gÃ©nÃ¨re une sÃ©quence de reprÃ©sentations vectorielles pour chaque token qui englobe les informations textuelles de l'ensemble du texte. Ensuite, le mean pooling est appliquÃ© Ã  chaque chunk de cette sÃ©quence de vecteurs de tokens, produisant des embeddings pour chaque chunk qui prennent en compte le contexte du texte entier. Contrairement Ã  l'approche d'encodage naÃ¯ve, qui gÃ©nÃ¨re des embeddings de chunks indÃ©pendants et identiquement distribuÃ©s (i.i.d.), <strong>le late chunking crÃ©e un ensemble d'embeddings de chunks oÃ¹ chacun est Â« conditionnÃ© par Â» les prÃ©cÃ©dents, encodant ainsi plus d'informations contextuelles pour chaque chunk.</strong></p><p>Ã‰videmment, pour appliquer efficacement le late chunking, nous avons besoin de modÃ¨les d'embedding Ã  contexte long comme <code>jina-embeddings-v2-base-en</code>, qui supportent jusqu'Ã  8192 tokensâ€”environ dix pages standard de texte. Les segments de texte de cette taille sont beaucoup moins susceptibles d'avoir des dÃ©pendances contextuelles qui nÃ©cessitent un contexte encore plus long Ã  rÃ©soudre.</p><p>Il est important de souligner que le late chunking nÃ©cessite toujours des indices de frontiÃ¨re, mais ces indices ne sont utilisÃ©s qu'<em>aprÃ¨s</em> l'obtention des embeddings au niveau des tokensâ€”d'oÃ¹ le terme Â« late Â» dans sa dÃ©nomination.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>DÃ©coupage naÃ¯f</th>\n<th>Late Chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Le besoin d'indices de frontiÃ¨re</td>\n<td>Oui</td>\n<td>Oui</td>\n</tr>\n<tr>\n<td>L'utilisation des indices de frontiÃ¨re</td>\n<td>Directement en prÃ©traitement</td>\n<td>AprÃ¨s l'obtention des embeddings au niveau des tokens de la couche transformer</td>\n</tr>\n<tr>\n<td>Les embeddings de chunks rÃ©sultants</td>\n<td>i.i.d.</td>\n<td>Conditionnels</td>\n</tr>\n<tr>\n<td>Informations contextuelles des chunks voisins</td>\n<td>Perdues. Quelques heuristiques (comme l'Ã©chantillonnage avec chevauchement) pour attÃ©nuer cela</td>\n<td>Bien prÃ©servÃ©es par les modÃ¨les d'embedding Ã  contexte long</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"implementation-and-qualitative-evaluation\">ImplÃ©mentation et Ã©valuation qualitative</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/4c9d6ee1a7679cb6c4c106e58fabaf56/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>L'implÃ©mentation du late chunking peut Ãªtre trouvÃ©e dans le Google Colab liÃ© ci-dessus. Ici, nous utilisons notre rÃ©cente fonctionnalitÃ© publiÃ©e dans l'API Tokenizer, qui exploite tous les indices de frontiÃ¨re possibles pour segmenter un long document en chunks significatifs. Plus de discussions sur l'algorithme derriÃ¨re cette fonctionnalitÃ© peuvent Ãªtre trouvÃ©es sur X.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">API gratuite pour tokeniser du texte et segmenter un long texte en morceaux.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,â€¦ <a href=\"https://t.co/AtBCSrn7nI?ref=jina-ai-gmbh.ghost.io\">pic.twitter.com/AtBCSrn7nI</a></p>â€” Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1823756993108304135?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">14 aoÃ»t 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><p>En appliquant le chunking tardif Ã  l'exemple Wikipedia ci-dessus, on peut immÃ©diatement constater une amÃ©lioration de la similaritÃ© sÃ©mantique. Par exemple, dans le cas de \"la ville\" et \"Berlin\" dans un article Wikipedia, les vecteurs reprÃ©sentant \"la ville\" contiennent maintenant des informations la reliant Ã  la mention prÃ©cÃ©dente de \"Berlin\", ce qui en fait une bien meilleure correspondance pour les requÃªtes impliquant ce nom de ville.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Query</th>\n<th>Chunk</th>\n<th>Sim. on naive chunking</th>\n<th>Sim. on late chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Berlin</td>\n<td>Berlin is the capital and largest city of Germany, both by area and by population.</td>\n<td>0.849</td>\n<td>0.850</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.</td>\n<td>0.708</td>\n<td>0.825</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.</td>\n<td>0.753</td>\n<td>0.850</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Vous pouvez observer cela dans les rÃ©sultats numÃ©riques ci-dessus, qui comparent l'embedding du terme \"Berlin\" Ã  diverses phrases de l'article sur Berlin en utilisant la similaritÃ© cosinus. La colonne \"Sim. on IID chunk embeddings\" montre les valeurs de similaritÃ© entre l'embedding de la requÃªte \"Berlin\" et les embeddings utilisant le chunking <em>a priori</em>, tandis que \"Sim. under contextual chunk embedding\" reprÃ©sente les rÃ©sultats avec la mÃ©thode de chunking tardif.</p><h2 id=\"quantitative-evaluation-on-beir\">Ã‰valuation quantitative sur BEIR</h2><p>Pour vÃ©rifier l'efficacitÃ© du chunking tardif au-delÃ  d'un exemple simple, nous l'avons testÃ© en utilisant certains des benchmarks de rÃ©cupÃ©ration de <a href=\"https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">BeIR</a>. Ces tÃ¢ches de rÃ©cupÃ©ration consistent en un ensemble de requÃªtes, un corpus de documents textuels et un fichier QRels qui stocke les informations sur les ID des documents pertinents pour chaque requÃªte.</p><p>Pour identifier les documents pertinents pour une requÃªte, les documents sont dÃ©coupÃ©s, encodÃ©s dans un index d'embeddings, et les chunks les plus similaires sont dÃ©terminÃ©s pour chaque embedding de requÃªte en utilisant les k plus proches voisins (kNN). Puisque chaque chunk correspond Ã  un document, le classement kNN des chunks peut Ãªtre converti en un classement kNN des documents (en ne conservant que la premiÃ¨re occurrence pour les documents apparaissant plusieurs fois dans le classement). Ce classement rÃ©sultant est ensuite comparÃ© au classement fourni par le fichier QRels de rÃ©fÃ©rence, et les mÃ©triques de rÃ©cupÃ©ration comme nDCG@10 sont calculÃ©es. Cette procÃ©dure est illustrÃ©e ci-dessous, et le script d'Ã©valuation peut Ãªtre trouvÃ© dans ce rÃ©fÃ©rentiel pour la reproductibilitÃ©.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/late-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/late-chunking : Code pour expliquer et Ã©valuer le chunking tardif (chunked pooling)</div><div class=\"kg-bookmark-description\">Code pour expliquer et Ã©valuer le chunking tardif (chunked pooling) - jina-ai/late-chunking</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/bf0bb9d5ca928dc3fe25ae621398af0fdf5e34324e37cbeee6fa4189218c9b4d/jina-ai/late-chunking\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Nous avons effectuÃ© cette Ã©valuation sur diffÃ©rents jeux de donnÃ©es BeIR, en comparant le chunking naÃ¯f avec notre mÃ©thode de chunking tardif. Pour obtenir les indices de limite, nous avons utilisÃ© une regex qui divise les textes en chaÃ®nes d'environ 256 tokens. L'Ã©valuation du chunking naÃ¯f et tardif a utilisÃ© <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\"><code>jina-embeddings-v2-small-en</code></a> comme modÃ¨le d'embedding ; une version plus petite du modÃ¨le <code>v2-base-en</code> qui prend toujours en charge jusqu'Ã  8192 tokens. Les rÃ©sultats peuvent Ãªtre trouvÃ©s dans le tableau ci-dessous.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Avg. Document Length (characters)</th>\n<th>Naive Chunking (nDCG@10)</th>\n<th>Late Chunking (nDCG@10)</th>\n<th>No Chunking (nDCG@10)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td>1498.4</td>\n<td>64.20%</td>\n<td><strong>66.10%</strong></td>\n<td>63.89%</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td>1116.7</td>\n<td>63.36%</td>\n<td>64.70%</td>\n<td><strong>65.18%</strong></td>\n</tr>\n<tr>\n<td>FiQA2018</td>\n<td>767.2</td>\n<td>33.25%</td>\n<td><strong>33.84%</strong></td>\n<td>33.43%</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>1589.8</td>\n<td>23.46%</td>\n<td>29.98%</td>\n<td><strong>30.40%</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>62.2</td>\n<td>87.19%</td>\n<td>87.19%</td>\n<td>87.19%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Dans tous les cas, le chunking tardif a amÃ©liorÃ© les scores par rapport Ã  l'approche naÃ¯ve. Dans certains cas, il a mÃªme surpassÃ© l'encodage du document entier en un seul embedding, tandis que dans d'autres jeux de donnÃ©es, l'absence de chunking a donnÃ© les meilleurs rÃ©sultats (bien sÃ»r, l'absence de chunking n'a de sens que s'il n'est pas nÃ©cessaire de classer les chunks, ce qui est rare en pratique). Si nous traÃ§ons l'Ã©cart de performance entre l'approche naÃ¯ve et le chunking tardif en fonction de la longueur du document, il devient Ã©vident que la longueur moyenne des documents est corrÃ©lÃ©e avec de plus grandes amÃ©liorations des scores nDCG grÃ¢ce au chunking tardif. En d'autres termes, <strong>plus le document est long, plus la stratÃ©gie de chunking tardif devient efficace.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/chart--22-.svg\" class=\"kg-image\" alt=\"Graphique linÃ©aire montrant la diminution de l'amÃ©lioration relative avec l'augmentation de la longueur du document, de 0 Ã  1500 caractÃ¨res.\" loading=\"lazy\" width=\"582\" height=\"337\"><figcaption><span style=\"white-space: pre-wrap;\">L'amÃ©lioration du chunking tardif par rapport au chunking naÃ¯f est corrÃ©lÃ©e avec la longueur moyenne des documents.</span></figcaption></figure><h2 id=\"conclusion\">Conclusion</h2><p>Dans cet article, nous avons prÃ©sentÃ© une approche simple appelÃ©e \"chunking tardif\" pour intÃ©grer de courts chunks en exploitant la puissance des modÃ¨les d'embedding Ã  contexte long. Nous avons dÃ©montrÃ© comment l'embedding de chunk i.i.d. traditionnel ne prÃ©serve pas les informations contextuelles, conduisant Ã  une rÃ©cupÃ©ration sous-optimale ; et comment le chunking tardif offre une solution simple mais trÃ¨s efficace pour maintenir et conditionner les informations contextuelles dans chaque chunk. L'efficacitÃ© du chunking tardif devient de plus en plus significative sur les documents plus longs â€” une capacitÃ© rendue possible <em>uniquement</em> par les modÃ¨les d'embedding Ã  contexte long avancÃ©s comme <code>jina-embeddings-v2-base-en</code>. Nous espÃ©rons que ce travail non seulement valide l'importance des modÃ¨les d'embedding Ã  contexte long mais inspire Ã©galement des recherches supplÃ©mentaires sur ce sujet.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ce qu'est rÃ©ellement le Chunking Tardif et ce qu'il n'est pas : Partie II</div><div class=\"kg-bookmark-description\">Partie 2 de notre exploration du Chunking Tardif, une plongÃ©e approfondie dans les raisons pour lesquelles c'est la meilleure mÃ©thode pour les chunk embeddings et l'amÃ©lioration des performances de recherche/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-5.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Continuez la lecture de la partie II : plongÃ©e approfondie dans les indices de limite et les idÃ©es fausses.</span></p></figcaption></figure>",
  "comment_id": "66c72e30da9a33000146d836",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-22T14:25:20.000+02:00",
  "updated_at": "2024-10-06T16:29:02.000+02:00",
  "published_at": "2024-08-22T17:06:17.000+02:00",
  "custom_excerpt": "Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael GÃ¼nther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael GÃ¼nther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/late-chunking-in-long-context-embedding-models/",
  "excerpt": "Le dÃ©coupage de longs documents tout en prÃ©servant les informations contextuelles est un dÃ©fi. Nous prÃ©sentons le \"Late Chunking\" qui utilise des modÃ¨les d'embedding Ã  contexte long pour gÃ©nÃ©rer des embeddings contextuels de fragments permettant de meilleures applications de recherche.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating the 'Late Chunking' and 'Long Document Model' processes in machine learning on a black background.",
  "feature_image_caption": null
}
{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "Le Quoi et le Pourquoi de l'√âcart de Modalit√© Texte-Image dans les Mod√®les CLIP",
  "html": "<p>Les <a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">embeddings s√©mantiques</a> sont au c≈ìur des mod√®les d'IA modernes, y compris les chatbots et les mod√®les d'art IA. Ils sont parfois cach√©s aux utilisateurs, mais ils sont toujours l√†, tapis juste sous la surface.</p><p>La th√©orie des embeddings ne comporte que deux parties :</p><ol><li>Les choses ‚Äî les √©l√©ments externes au mod√®le d'IA, comme les textes et les images ‚Äî sont repr√©sent√©es par des vecteurs cr√©√©s par les mod√®les d'IA √† partir des donn√©es sur ces choses.</li><li>Les relations entre les choses externes au mod√®le d'IA sont repr√©sent√©es par des relations spatiales entre ces vecteurs. Nous entra√Ænons sp√©cifiquement les mod√®les d'IA pour cr√©er des vecteurs qui fonctionnent de cette mani√®re.</li></ol><p>Lorsque nous cr√©ons un mod√®le multimodal image-texte, nous entra√Ænons le mod√®le de sorte que les embeddings des images et les embeddings des textes d√©crivant ou li√©s √† ces images soient relativement proches les uns des autres. Les similarit√©s s√©mantiques entre les choses que ces deux vecteurs repr√©sentent ‚Äî une image et un texte ‚Äî se refl√®tent dans la relation spatiale entre les deux vecteurs.</p><p>Par exemple, nous pourrions raisonnablement nous attendre √† ce que les vecteurs d'embedding pour une image d'orange et le texte \"une orange fra√Æche\" soient plus proches l'un de l'autre que la m√™me image et le texte \"une pomme fra√Æche\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration sur fond noir montrant une orange et une pomme avec des fl√®ches entre elles et des citations indiquant &quot;Une orange fra√Æche&quot;\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>C'est le but d'un mod√®le d'embedding : G√©n√©rer des repr√©sentations o√π les caract√©ristiques qui nous int√©ressent ‚Äî comme le type de fruit repr√©sent√© dans une image ou nomm√© dans un texte ‚Äî sont pr√©serv√©es dans la distance entre eux.</p><p>Mais la multimodalit√© introduit autre chose. Nous pourrions d√©couvrir qu'une image d'orange est plus proche d'une image de pomme qu'elle ne l'est du texte \"une orange fra√Æche\", et que le texte \"une pomme fra√Æche\" est plus proche d'un autre texte que d'une image de pomme.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Fond noir pr√©sentant une pomme √† gauche et une orange √† droite avec des fl√®ches annot√©es marqu√©es &quot;Une pomme fra√Æche&quot; et &quot;\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Il s'av√®re que c'est exactement ce qui se passe avec les mod√®les multimodaux, y compris le mod√®le <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP</a> de Jina AI (<code>jina-clip-v1</code>).</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>Pour tester cela, nous avons √©chantillonn√© 1 000 paires texte-image du <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">jeu de test Flickr8k</a>. Chaque paire contient cinq textes de l√©gende (donc techniquement pas une paire) et une seule image, les cinq textes d√©crivant la m√™me image.</p><p>Par exemple, l'image suivante (<code>1245022983_fb329886dd.jpg</code> dans le dataset Flickr8k) :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"Une jeune fille en jupe rose jouant avec un frisbee dans un cadre urbain ext√©rieur avec des voitures et des v√©los pr√©sents.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>Ses cinq l√©gendes :</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>Nous avons utilis√© Jina CLIP pour int√©grer les images et les textes puis :</p><ol><li>Comparer les similarit√©s cosinus des embeddings d'images avec les embeddings de leurs textes de l√©gende.</li><li>Prendre les embeddings des cinq textes de l√©gende qui d√©crivent la m√™me image et comparer leurs similarit√©s cosinus entre eux.</li></ol><p>Le r√©sultat est un √©cart √©tonnamment important, visible dans la Figure 1 :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graphique avec deux courbes montrant la distribution de la similarit√© cosinus pour les paires Image2Text et Text2Text avec des axes √©tiquet√©s.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 1 : Distribution des valeurs de similarit√© cosinus entre les paires image-texte correspondantes et les paires texte-texte dans Jina CLIP.</span></figcaption></figure><p>√Ä quelques exceptions pr√®s, les paires de textes correspondantes sont beaucoup plus proches les unes des autres que les paires image-texte correspondantes. Cela indique fortement que Jina CLIP encode les textes dans une partie de l'espace d'embedding et les images dans une partie largement disjointe relativement √©loign√©e. Cet espace entre les textes et les images est le <em>foss√© multimodal</em>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagramme sur fond noir repr√©sentant 'Images' √† gauche, 'Texts' en bas, avec 'Foss√© Multimodal' √©tiquet√© au centre.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>Les mod√®les d'embedding multimodaux encodent plus que l'information s√©mantique qui nous int√©resse : ils encodent le m√©dium de leur entr√©e. Selon Jina CLIP, une image ne vaut pas, comme le dit le dicton, mille mots. Elle a un contenu qu'aucune quantit√© de mots ne pourra jamais vraiment √©galer. Elle encode le m√©dium d'entr√©e dans la s√©mantique de ses embeddings sans que personne ne l'ait jamais entra√Æn√©e √† le faire.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Tant que nous ne comparons que des images avec des textes et vice-versa, ce n'est pas un probl√®me, mais un mod√®le vraiment multimodal devrait pouvoir nous dire que, par exemple, le texte \"ceci est une pomme\" correspond mieux √† une image de pomme qu'√† un texte sur les oranges. Les mod√®les de type CLIP dans leur forme actuelle ne peuvent pas faire cela.</div></div><p>Ce ph√©nom√®ne a √©t√© √©tudi√© dans l'article <em>Mind the Gap : Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>] qui le d√©signe comme le \"foss√© modal\". Le foss√© modal est la s√©paration spatiale, dans l'espace d'embedding, entre les entr√©es dans un m√©dium et les entr√©es dans un autre. Bien que les mod√®les ne soient pas intentionnellement entra√Æn√©s √† avoir un tel foss√©, ils sont omnipr√©sents dans les mod√®les multimodaux.</p><p>Nos investigations sur le foss√© modal dans Jina CLIP s'appuient fortement sur <a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">D'o√π vient le foss√© modal ?</h2><p>Liang et al. [2022] identifient trois sources principales derri√®re le foss√© modal :</p><ul><li>Un biais d'initialisation qu'ils appellent \"l'effet c√¥ne\".</li><li>Des r√©ductions de temp√©rature (al√©atoire) pendant l'entra√Ænement qui rendent tr√®s difficile de \"d√©sapprendre\" ce biais.</li><li>Des proc√©dures d'apprentissage contrastif, largement utilis√©es dans les mod√®les multimodaux, qui renforcent involontairement le foss√©.</li></ul><p>Nous examinerons chacun √† tour de r√¥le.</p><h3 id=\"cone-effect\">Effet c√¥ne</h3><p>Un mod√®le construit avec une architecture CLIP ou de type CLIP est en r√©alit√© compos√© de deux mod√®les d'embedding distincts reli√©s ensemble. Pour les mod√®les multimodaux image-texte, cela signifie un mod√®le pour encoder les textes, et un autre compl√®tement s√©par√© pour encoder les images, comme dans le sch√©ma ci-dessous.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ces deux mod√®les sont entra√Æn√©s de mani√®re √† ce qu'un embedding d'image et un embedding de texte soient relativement proches lorsque le texte d√©crit bien l'image.</p><p>Vous pouvez entra√Æner un tel mod√®le en randomisant les poids dans les deux mod√®les, puis en lui pr√©sentant des paires image-texte ensemble, en l'entra√Ænant √† partir de z√©ro pour minimiser la distance entre les deux sorties. Le <a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">mod√®le CLIP original d'OpenAI</a> a √©t√© entra√Æn√© de cette mani√®re. Cependant, cela n√©cessite beaucoup de paires image-texte et un entra√Ænement tr√®s co√ªteux en calcul. Pour le premier mod√®le CLIP, OpenAI a extrait 400 millions de paires image-texte de contenus l√©gend√©s sur Internet.</p><p>Les mod√®les plus r√©cents de type CLIP utilisent des composants pr√©-entra√Æn√©s<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">.</a> Cela signifie entra√Æner chaque composant s√©par√©ment comme un bon mod√®le d'embedding unimodal, un pour les textes et un pour les images. Ces deux mod√®les sont ensuite entra√Æn√©s ensemble en utilisant des paires image-texte, un processus appel√© <em>contrastive tuning</em>. Des paires image-texte align√©es sont utilis√©es pour \"pousser\" progressivement les poids √† rapprocher les embeddings de texte et d'image correspondants, et √† √©loigner ceux qui ne correspondent pas.</p><p>Cette approche n√©cessite g√©n√©ralement moins de donn√©es de paires image-texte, qui sont difficiles et co√ªteuses √† obtenir, et de grandes quantit√©s de textes simples et d'images sans l√©gendes, qui sont beaucoup plus faciles √† obtenir. Jina CLIP (<code>jina-clip-v1</code>) a √©t√© entra√Æn√© en utilisant cette derni√®re m√©thode. Nous avons pr√©-entra√Æn√© un mod√®le <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2</a> pour l'encodage de texte en utilisant des donn√©es textuelles g√©n√©rales et utilis√© un <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">encodeur d'images EVA-02</a> pr√©-entra√Æn√©, puis les avons entra√Æn√©s davantage en utilisant diverses techniques d'entra√Ænement contrastif, comme d√©crit dans <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2 : Positions initiales des embeddings d'images et de textes avant l'entra√Ænement par paires dans Jina CLIP, projet√©es en deux dimensions.</span></figcaption></figure><p>Si nous prenons ces deux mod√®les pr√©-entra√Æn√©s et examinons leur sortie, avant de les entra√Æner avec des paires image-texte, nous remarquons quelque chose d'important. La Figure 2 (ci-dessus) est une projection <a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">UMAP</a> en deux dimensions des embeddings d'images produits par l'encodeur EVA-02 pr√©-entra√Æn√© et des embeddings de texte produits par JinaBERT v2 pr√©-entra√Æn√©, avec les lignes grises indiquant les paires image-texte correspondantes. Ceci est avant tout entra√Ænement intermodal.</p><p>Le r√©sultat est une sorte de \"c√¥ne\" tronqu√©, avec les embeddings d'images √† une extr√©mit√© et les embeddings de texte √† l'autre. Cette forme conique est mal traduite dans les projections bidimensionnelles mais vous pouvez globalement la voir dans l'image ci-dessus. Tous les textes se regroupent dans une partie de l'espace d'embedding, et toutes les images dans une autre partie. Si, apr√®s l'entra√Ænement, les textes restent plus similaires √† d'autres textes qu'aux images correspondantes, cet √©tat initial en est une raison majeure. L'objectif de mieux faire correspondre les images aux textes, les textes aux textes et les images aux images est totalement compatible avec cette forme conique.</p><p>Le mod√®le est biais√© d√®s sa naissance et ce qu'il apprend ne change pas cela. La Figure 3 (ci-dessous) est la m√™me analyse du mod√®le Jina CLIP tel que publi√©, apr√®s un entra√Ænement complet utilisant des paires image-texte. Si quelque chose, l'√©cart multimodal est encore plus prononc√©.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3 : Positions des embeddings d'images et de textes apr√®s l'entra√Ænement par paires dans Jina CLIP, projet√©es en deux dimensions.</span></figcaption></figure><p>M√™me apr√®s un entra√Ænement approfondi, Jina CLIP encode toujours le medium comme partie du message.</p><p>L'utilisation de l'approche plus co√ªteuse d'OpenAI, avec une initialisation purement al√©atoire, n'√©limine pas ce biais. Nous avons pris l'architecture CLIP originale d'OpenAI et compl√®tement randomis√© tous les poids, puis effectu√© la m√™me analyse que ci-dessus. Le r√©sultat est toujours une forme de c√¥ne tronqu√©, comme on le voit dans la Figure 4 :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4 : Positions initiales des embeddings d'images et de textes dans Jina CLIP avec des poids compl√®tement randomis√©s et sans aucun entra√Ænement, projet√©es en deux dimensions.</span></figcaption></figure><p>Ce biais est un probl√®me structurel, et peut ne pas avoir de solution. Si c'est le cas, nous ne pouvons que chercher des moyens de le corriger ou de l'att√©nuer pendant l'entra√Ænement.</p><h3 id=\"training-temperature\">Temp√©rature d'entra√Ænement</h3><p>Pendant l'entra√Ænement des mod√®les d'IA, nous ajoutons g√©n√©ralement un peu d'al√©atoire au processus. Nous calculons √† quel point un lot d'√©chantillons d'entra√Ænement devrait modifier les poids dans le mod√®le, puis ajoutons un petit facteur al√©atoire √† ces modifications avant de r√©ellement changer les poids. Nous appelons la quantit√© d'al√©atoire la <em>temp√©rature</em>, par analogie avec la fa√ßon dont nous utilisons l'al√©atoire en thermodynamique.</p><p>Les temp√©ratures √©lev√©es cr√©ent de grands changements dans les mod√®les tr√®s rapidement, tandis que les basses temp√©ratures r√©duisent la quantit√© de changement qu'un mod√®le peut subir chaque fois qu'il voit des donn√©es d'entra√Ænement. En cons√©quence, avec des temp√©ratures √©lev√©es, nous pouvons nous attendre √† ce que les embeddings individuels se d√©placent beaucoup dans l'espace d'embedding pendant l'entra√Ænement, et avec des temp√©ratures basses, ils se d√©placeront beaucoup plus lentement.</p><p>La meilleure pratique pour l'entra√Ænement des mod√®les d'IA est de commencer avec une temp√©rature √©lev√©e puis de la r√©duire progressivement. Cela aide le mod√®le √† faire de grands bonds dans l'apprentissage au d√©but lorsque les poids sont soit al√©atoires soit loin de l√† o√π ils doivent √™tre, puis lui permet d'apprendre les d√©tails de mani√®re plus stable.</p><p>L'entra√Ænement par paires image-texte de Jina CLIP commence avec une temp√©rature de 0,07 (c'est une temp√©rature relativement √©lev√©e) et la r√©duit exponentiellement au cours de l'entra√Ænement jusqu'√† 0,01, comme montr√© dans la Figure 5 ci-dessous, un graphique de la temp√©rature en fonction des √©tapes d'entra√Ænement :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5 : D√©croissance de la temp√©rature pendant l'entra√Ænement par paires dans Jina CLIP.</span></figcaption></figure><p>Nous voulions savoir si l'augmentation de la temp√©rature ‚Äî l'ajout d'al√©atoire ‚Äî r√©duirait l'effet de c√¥ne et rapprocherait globalement les embeddings d'images et de textes. Nous avons donc r√©entra√Æn√© Jina CLIP avec une temp√©rature fixe de 0,1 (une valeur tr√®s √©lev√©e). Apr√®s chaque √©poque d'entra√Ænement, nous avons v√©rifi√© la distribution des distances entre les paires image-texte et les paires texte-texte, comme dans la Figure 1. Les r√©sultats sont ci-dessous dans la Figure 6 :</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6 : L'√©cart entre les modalit√©s se r√©duit avec le temps lorsque la temp√©rature d'entra√Ænement est √©lev√©e.</span></figcaption></figure><p>Comme vous pouvez le voir, le maintien d'une temp√©rature √©lev√©e r√©duit consid√©rablement l'√©cart multimodal. Permettre aux embeddings de beaucoup bouger pendant l'entra√Ænement contribue grandement √† surmonter le biais initial dans leur distribution.</p><p>Cependant, cela a un co√ªt. Nous avons √©galement test√© les performances du mod√®le en utilisant six tests de r√©cup√©ration diff√©rents : trois tests de r√©cup√©ration texte-texte et trois tests de r√©cup√©ration texte-image, √† partir des jeux de donn√©es <a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, et <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a>. Dans tous les tests, nous observons une chute des performances au d√©but de l'entra√Ænement, suivie d'une remont√©e tr√®s lente, comme vous pouvez le voir dans la Figure 7 :</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7 : Performance pendant l'entra√Ænement. Au d√©but, on observe une forte baisse par rapport √† l'√©tat initial suivie d'une remont√©e tr√®s lente.</span></figcaption></figure><p>Il serait probablement extr√™mement long et co√ªteux d'entra√Æner un mod√®le comme Jina CLIP en utilisant cette temp√©rature √©lev√©e constante. Bien que th√©oriquement possible, ce n'est pas une solution pratique.</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">L'apprentissage contrastif et le probl√®me des faux n√©gatifs</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> ont √©galement d√©couvert que les pratiques standard d'apprentissage contrastif ‚Äî le m√©canisme que nous utilisons pour entra√Æner les mod√®les multimodaux de type CLIP ‚Äî tendent √† renforcer l'√©cart multimodal.</p><p>L'apprentissage contrastif est fondamentalement un concept simple. Nous avons un embedding d'image et un embedding de texte dont nous savons qu'ils devraient √™tre plus proches, nous ajustons donc les poids du mod√®le pendant l'entra√Ænement pour y parvenir. Nous proc√©dons lentement, en ajustant les poids par petites quantit√©s, et nous les ajustons proportionnellement √† l'√©cart entre les deux embeddings : plus ils sont proches, plus le changement est petit.</p><p>Cette technique fonctionne beaucoup mieux si nous ne nous contentons pas de rapprocher les embeddings lorsqu'ils correspondent, mais que nous les √©loignons √©galement lorsqu'ils ne correspondent pas. Nous voulons avoir non seulement des paires image-texte qui vont ensemble, mais aussi des paires dont nous savons qu'elles doivent √™tre s√©par√©es.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Cela pose quelques probl√®mes :</p><ol><li>Nos sources de donn√©es ne contiennent que des paires correspondantes. Personne ne cr√©erait une base de donn√©es de textes et d'images dont un humain a v√©rifi√© qu'ils ne sont pas li√©s, et on ne pourrait pas non plus en construire une facilement en explorant le web ou une autre technique non supervis√©e ou semi-supervis√©e.</li><li>M√™me les paires image-texte qui semblent superficiellement compl√®tement disjointes ne le sont pas n√©cessairement. Nous n'avons pas de th√©orie de la s√©mantique qui nous permette de faire objectivement de tels jugements n√©gatifs. Par exemple, une photo d'un chat allong√© sur un porche n'est pas totalement incompatible avec le texte \"un homme dormant sur un canap√©\". Les deux impliquent d'√™tre allong√© sur quelque chose.</li></ol><p>Id√©alement, nous voudrions entra√Æner avec des paires image-texte dont nous savons avec certitude qu'elles sont li√©es <em>et non li√©es</em>, mais il n'y a pas de moyen √©vident d'obtenir des paires connues comme non li√©es. Il est possible de demander aux gens \"Cette phrase d√©crit-elle cette image ?\" et d'attendre des r√©ponses coh√©rentes. Il est beaucoup plus difficile d'obtenir des r√©ponses coh√©rentes en demandant \"Cette phrase n'a-t-elle absolument rien √† voir avec cette image ?\"</p><p>√Ä la place, nous obtenons des paires image-texte non li√©es en s√©lectionnant al√©atoirement des images et des textes de nos donn√©es d'entra√Ænement, en supposant qu'ils seront pratiquement toujours de mauvaises correspondances. En pratique, nous divisons nos donn√©es d'entra√Ænement en lots. Pour entra√Æner Jina CLIP, nous avons utilis√© des lots contenant 32 000 paires image-texte correspondantes, mais pour cette exp√©rience, les tailles de lots n'√©taient que de 16.</p><p>Le tableau ci-dessous pr√©sente 16 paires image-texte √©chantillonn√©es al√©atoirement depuis Flickr8k :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Pour obtenir des paires non correspondantes, nous combinons chaque image du lot avec tous les textes <em>autres que celui qui lui correspond</em>. Par exemple, la paire suivante est une image et un texte non correspondants :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>L√©gende :</strong> Une fille en rose cueille des fleurs.</p><p>Mais cette proc√©dure suppose que tous les textes correspondant √† d'autres images sont √©galement de mauvaises correspondances. Ce n'est pas toujours vrai. Par exemple :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>L√©gende :</strong> Le chien est assis pr√®s d'un banc de neige.</p><p>Bien que le texte ne d√©crive pas cette image, ils ont un chien en commun. Traiter cette paire comme non correspondante aura tendance √† √©loigner le mot \"chien\" de toute image de chien.</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> montrent que ces paires non correspondantes imparfaites poussent toutes les images et les textes √† s'√©loigner les uns des autres.</p><p>Nous avons entrepris de v√©rifier leur affirmation avec un mod√®le d'image <code>vit-b-32</code> enti√®rement initialis√© al√©atoirement et un mod√®le de texte JinaBERT v2 similairement randomis√©, avec une temp√©rature d'entra√Ænement constante fix√©e √† 0,02 (une temp√©rature mod√©r√©ment basse). Nous avons construit deux ensembles de donn√©es d'entra√Ænement :</p><ul><li>Un avec des lots al√©atoires tir√©s de Flickr8k, avec des paires non correspondantes construites comme d√©crit ci-dessus.</li><li>Un o√π les lots sont intentionnellement construits avec plusieurs copies de la m√™me image avec diff√©rents textes dans chaque lot. Cela garantit qu'un nombre significatif de paires \"non correspondantes\" sont en fait des correspondances correctes les unes pour les autres.</li></ul><p>Nous avons ensuite entra√Æn√© deux mod√®les pendant une √©poque, un avec chaque jeu de donn√©es d'entra√Ænement, et mesur√© la distance cosinus moyenne entre 1 000 paires texte-image dans le jeu de donn√©es Flickr8k pour chaque mod√®le. Le mod√®le entra√Æn√© avec des lots al√©atoires avait une distance cosinus moyenne de 0,7521, tandis que celui entra√Æn√© avec beaucoup de paires \"non correspondantes\" intentionnellement correspondantes avait une distance cosinus moyenne de 0,7840. L'effet des paires \"non correspondantes\" incorrectes est assez significatif. √âtant donn√© que l'entra√Ænement r√©el du mod√®le est beaucoup plus long et utilise beaucoup plus de donn√©es, nous pouvons voir comment cet effet grandirait et accentuerait l'√©cart entre les images et les textes dans leur ensemble.</p><h2 id=\"the-medium-is-the-message\">Le medium est le message</h2><p>Le th√©oricien canadien des communications <a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Marshall McLuhan</a> a invent√© l'expression \"Le medium est le message\" dans son livre de 1964 <a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\"><em>Understanding Media: The Extensions of Man</em></a> pour souligner que les messages ne sont pas autonomes. Ils nous parviennent dans un contexte qui affecte fortement leur signification, et il affirmait c√©l√®brement que l'une des parties les plus importantes de ce contexte est la nature du medium de communication.</p><p>L'√©cart multimodal nous offre une opportunit√© unique d'√©tudier une classe de ph√©nom√®nes s√©mantiques √©mergents dans les mod√®les d'IA. Personne n'a dit √† Jina CLIP d'encoder le support des donn√©es sur lesquelles il a √©t√© entra√Æn√© ‚Äî il l'a fait de lui-m√™me. M√™me si nous n'avons pas r√©solu le probl√®me pour les mod√®les multimodaux, nous avons au moins une bonne compr√©hension th√©orique de l'origine du probl√®me.</p><p>Nous devrions supposer que nos mod√®les encodent d'autres choses que nous n'avons pas encore recherch√©es en raison du m√™me type de biais. Par exemple, nous avons probablement le m√™me probl√®me dans les mod√®les d'embedding multilingues. L'entra√Ænement conjoint sur deux langues ou plus conduit probablement au m√™me √©cart entre les langues, d'autant plus que des m√©thodes d'entra√Ænement similaires sont largement utilis√©es. Les solutions au probl√®me de l'√©cart peuvent avoir des implications tr√®s larges.</p><p>Une investigation sur le biais d'initialisation dans un plus large √©ventail de mod√®les m√®nera probablement aussi √† de nouvelles d√©couvertes. Si le support est le message pour un mod√®le d'embedding, qui sait quoi d'autre est encod√© dans nos mod√®les √† notre insu ?</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "On ne peut pas simplement utiliser un mod√®le CLIP pour r√©cup√©rer du texte et des images et trier les r√©sultats par score. Pourquoi ? √Ä cause de l'√©cart entre les modalit√©s. Qu'est-ce que c'est, et d'o√π vient-il ?",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
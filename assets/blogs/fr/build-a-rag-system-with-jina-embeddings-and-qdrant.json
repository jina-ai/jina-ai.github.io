{
  "slug": "build-a-rag-system-with-jina-embeddings-and-qdrant",
  "id": "65ddadd039a5b70001810ea3",
  "uuid": "e3419a3e-4a7c-43c3-ac69-6f65f00a7ad8",
  "title": "Construire un système RAG avec Jina Embeddings et Qdrant",
  "html": "<p>La génération augmentée par recherche (RAG) est un ensemble de techniques en pleine expansion qui ont été <a href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io\"><u>initialement conçues comme une extension des grands modèles de langage</u></a> (LLMs) mais sont de plus en plus considérées comme un paradigme alternatif qui réinvente entièrement le rôle des modèles d'IA.</p><p>Nous allons vous montrer comment créer un moteur RAG basique en utilisant le <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\">framework LlamaIndex</a>, un <a href=\"https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io\">LLM Mistral</a> open-source, <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2</a>, et <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\">la base de données vectorielle Qdrant prête pour l'IA</a>.</p><h2 id=\"rag-in-context\"><strong>Le RAG en Contexte</strong></h2><p>Dans un système RAG, le rôle du modèle d'IA reste central mais beaucoup plus limité. Au lieu de répondre aux prompts à partir de ses connaissances internes, dérivées de ses données d'entraînement, il reçoit les informations pertinentes dans son prompt. Le rôle du modèle est alors de comprendre et de synthétiser ces informations en une sortie.</p><p>Cela exploite les points forts des modèles d'IA et minimise leurs faiblesses.</p><p>Par exemple, nous pourrions donner au <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\"><u>modèle Mistral-Instruct</u></a> le prompt suivant :</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: Why is Taylor Swift planning to attend the Superbowl?\nAnswer:\n</code></pre><p>Quand j'ai fait cela, le modèle a répondu :</p><pre><code class=\"language-text\">I don't have real-time information. However, as of my last update,&nbsp;\nTaylor Swift is not officially confirmed to attend or perform at&nbsp;\nthe Superbowl. If there are any updates, I recommend checking the&nbsp;\nlatest news sources.</code></pre><p>Les grands modèles de langage sont coûteux à mettre à jour et à réentraîner, donc leurs informations internes ne peuvent jamais être totalement à jour.</p><p>De plus, les grands modèles de langage sont connus pour \"halluciner\", c'est-à-dire inventer des réponses plausibles qui peuvent être partiellement ou totalement incorrectes et qui doivent être vérifiées.</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: What was Weird Al Yankovic's first Taylor Swift parody?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" a spoof&nbsp;\nof Swift's song \"Blank Space.\" The parody was released in 2014&nbsp;\nas part of Yankovic's album \"Mandatory Fun.\" The song humorously&nbsp;\naddresses the obsession with celebrity gossip and scandals, as&nbsp;\nreported by the tabloid news website TMZ.</code></pre><p>Le <em>TMZ</em> de Weird Al Yankovic est sorti en 2011 sur l'album <em>Alpocalypse</em> et est une parodie de <a href=\"https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io\"><em><u>You Belong with Me</u></em></a> de Swift.</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" title=\"TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)\"></iframe></figure><p>La nature des réseaux neuronaux rend l'hallucination difficile, voire impossible, à prévenir complètement et compromet fortement tout projet d'utilisation des modèles d'IA comme sources d'information.</p><p>Selon les <a href=\"https://youtu.be/uaQZIK9gvNo?t=1130&ref=jina-ai-gmbh.ghost.io\"><u>mots de Sam Altman</u></a>, PDG d'OpenAI :</p><blockquote class=\"kg-blockquote-alt\">\"Je pense que les gens font une grande erreur quand ils considèrent ces modèles [LLMs] comme une base de données. [...] Ça marchera, plus ou moins. Mais c'est la base de données la plus chère, la plus lente et la moins précise au monde.\"</blockquote><p>Le RAG minimise la dépendance aux connaissances internes d'un modèle d'IA sur les faits du monde réel et tire parti de ses plus grands atouts : la compréhension du langage naturel et la génération fluide de langage.</p><p>Prenons un autre exemple de question à laquelle Mistral-Instruct ne peut pas répondre :</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\n\nQuery: What is the title of Taylor Swift's next album?\nAnswer:\n\nAs of the time I am providing this answer, Taylor Swift has&nbsp;\nnot yet announced the title of her next album. She has released&nbsp;\nseveral re-recorded albums, but the title of her next original&nbsp;\nalbum remains unknown. I recommend checking her official social&nbsp;\nmedia channels or website for the latest updates.</code></pre><p>Maintenant, un système RAG pourrait avoir une base de données d'articles d'actualité récents, ou peut-être rechercher sur Internet des textes liés au titre du prochain album de Taylor Swift. Ensuite, il insérerait les textes qu'il trouve dans le prompt.</p><p>Supposons qu'il identifie <a href=\"https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io\"><u>cet article AP</u></a> comme pertinent pour la requête et construit le prompt suivant :</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nFebruary 6, 2024\nLOS ANGELES (AP) — Forget her \"Reputation.\" Taylor Swift has a&nbsp;\nwhole new album coming out.\n\nAccepting the Grammy for best pop vocal album on Sunday night,&nbsp;\nTaylor said she'd been keeping a secret for two years.\n\n\"My brand-new album comes out April 19. It's called 'The Tortured&nbsp;\nPoets Department.' I'm going to go and post the cover right now&nbsp;\nbackstage,\" she announced.\n\nAnd so she did.\n\n---------------------\nGiven the context information and not prior knowledge, answer the query.&nbsp;\nPlease be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What is the title of Taylor Swift's next album?\nAnswer:</code></pre><p>Mistral-Instruct a répondu :</p><pre><code class=\"language-text\">The title of Taylor Swift's next album is 'The Tortured Poets Department'.</code></pre><p>Ce type de prompting réduit aussi drastiquement la probabilité d'hallucinations. Par exemple :</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nAlpocalypse is the thirteenth studio album by American parody&nbsp;\nmusician \"Weird Al\" Yankovic, released on June 21, 2011.\n\nTrack List\n\n1. Perform This Way (Parody of \"Born This Way\" by Lady Gaga)\n2. CNR\n3. TMZ (Parody of \"You Belong with Me\" by Taylor Swift)\n4. Skipper Dan\n5. Polka Face (Polka medley including songs by artists such as&nbsp;\n&nbsp;&nbsp;&nbsp;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)\n6. Craigslist\n7. Party in the CIA (Parody of \"Party in the U.S.A.\" by Miley&nbsp;\n&nbsp;&nbsp;&nbsp;Cyrus)\n8. Ringtone\n9. Another Tattoo (Parody of \"Nothin' on You\" by B.o.B featuring&nbsp;\n&nbsp;&nbsp;&nbsp;Bruno Mars)\n10. If That Isn't Love\n11. Whatever You Like (Parody of \"Whatever You Like\" by T.I.)\n12. Stop Forwarding That Crap to Me\n\n---------------------\nGiven the context information and not prior knowledge, answer the&nbsp;\nquery. Please be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What was Weird Al Yankovic's first Taylor Swift parody, what&nbsp;\nyear was it released, and what song was he parodying?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" and it was&nbsp;\nreleased in 2011. He was parodying \"You Belong with Me\" by Taylor&nbsp;\nSwift.\n</code></pre><h2 id=\"anatomy-of-a-rag-system\"><strong>Anatomie d'un système RAG</strong></h2><p>Un système RAG comporte, au minimum, deux composants :</p><ul><li>Un modèle d'IA génératif.</li><li>Un système de recherche d'information.</li></ul><p>Dans cet article, nous utiliserons <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io\">Mistral-Instruct</a> comme modèle d'IA génératif et y accéderons via l'<a href=\"https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io\">API d'inférence Hugging Face</a>. Nous utiliserons la <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\">base de données vectorielle Qdrant</a> et <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> ensemble pour le système de recherche d'information.</p><p>Comme source de données, nous utiliserons le <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io\"><em><u>Plan Stratégique National de Recherche et Développement en Intelligence Artificielle - Mise à jour 2023</u></em></a>, publié en mai 2023 par le <a href=\"https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io\">Conseil National des Sciences et de la Technologie</a>, un organisme qui conseille le Président américain sur les questions scientifiques et techniques. Ce document d'environ 25 000 mots est un document de politique et de planification stratégique pour la politique et le développement de l'IA américaine.</p><p>C'est aussi un exemple classique de document politique gouvernemental \"trop long ; pas lu\". La majeure partie est très ennuyeuse et, à moins d'avoir un intérêt direct ou un intérêt marqué dans ce domaine, vous ne le lirez probablement pas. Mais vous pourriez quand même vouloir savoir ce qu'il dit.</p><p>Cet article vous guidera à travers les étapes de création d'un système RAG capable de répondre aux questions sur le texte et son contenu.</p><h2 id=\"install-the-prerequisites\"><strong>Installer les Prérequis</strong></h2><p>Tout d'abord, assurez-vous d'avoir installé toutes les bibliothèques Python pertinentes. Dans votre environnement Python, exécutez ce qui suit :</p><pre><code class=\"language-shell\">pip install llama-index qdrant-client \npip install pdfminer.six llama-index-vector-stores-qdrant\npip install llama-index-llms-openai llama-index-embeddings-jinaai \npip install llama-index-llms-huggingface \"huggingface_hub[inference]\"</code></pre><p>Nous utiliserons LlamaIndex pour construire un système RAG autour du modèle de langage Mistral-Instruct et <a href=\"https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io\"><u>pdfminer.six</u></a> pour traiter le fichier PDF dans un magasin de vecteurs Qdrant.</p><h2 id=\"connect-to-mistral-instruct-via-the-hugging-face-inference-api\"><strong>Se connecter à Mistral-Instruct via l'API Inference de Hugging Face</strong></h2><p>Tout d'abord, configurez le LLM. Vous aurez besoin d'accéder à l'API Inference de Hugging Face, y compris un jeton d'accès. Si vous avez un compte Hugging Face, vous pouvez en obtenir un depuis <a href=\"https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io\">votre page de paramètres de compte</a>.</p><p>Si vous n'avez pas de compte, commencez par <a href=\"https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io\">en créer un</a>, puis créez un <a href=\"https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io\">jeton d'accès</a>.</p><p>Placez votre jeton dans une variable pour une utilisation future :</p><pre><code class=\"language-python\">hf_inference_api_key = '&lt;your HuggingFace Inference API token&gt;'</code></pre><p>Ensuite, nous construisons un modèle de prompt :</p><pre><code class=\"language-python\">from llama_index.core import PromptTemplate\n\nqa_prompt_tmpl = (\n&nbsp;&nbsp;&nbsp;&nbsp;\"Context information is below.\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"---------------------\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"{context_str}\\\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"---------------------\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Given the context information and not prior knowledge, \"\n&nbsp;&nbsp;&nbsp;&nbsp;\"answer the query. Please be brief, concise, and complete.\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"If the context information does not contain an answer to the query, \"\n&nbsp;&nbsp;&nbsp;&nbsp;\"respond with \\\"No information\\\".\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Query: {query_str}\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;\"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>Ce modèle demande au LLM d'utiliser uniquement les informations contextuelles fournies dans le prompt pour répondre aux questions. Ce prompt a bien fonctionné pour nous avec Mistral-Instruct, mais vous pouvez expérimenter d'autres formulations pour voir ce qui convient à votre cas d'utilisation.</p><pre><code class=\"language-python\">from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(\n    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n    token=hf_inference_api_key\n)</code></pre><p>Enfin, créez et initialisez un objet pour le framework LlamaIndex qui contient la connexion à Mistral-Instruct. Le code ci-dessous provient du <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\"><u>précédent article sur la création de systèmes RAG utilisant LlamaIndex</u></a>.</p><p>Cela place la connexion au LLM Mistral dans la variable <code>mistral_llm</code>.</p><h2 id=\"information-retrieval-with-qdrant-and-jina-embeddings\"><strong>Récupération d'informations avec Qdrant et Jina Embeddings</strong></h2><p>Pour configurer le système de récupération, vous aurez besoin d'une clé API Jina Embeddings. Vous pouvez en obtenir une gratuitement avec un budget prépayé d'un million de tokens sur le <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">site web de Jina Embeddings</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure><p>Placez votre clé Jina Embeddings dans une variable pour une utilisation future :</p><pre><code class=\"language-python\">jina_emb_api_key = \"&lt;your Jina Embeddings API key&gt;\"</code></pre><p>Ensuite, créez un objet connecteur en utilisant LlamaIndex pour le serveur Jina Embeddings, en sélectionnant spécifiquement le modèle monolingue anglais :</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_emb_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n)</code></pre><h3 id=\"load-text-data\"><strong>Charger les données textuelles</strong></h3><p>Ensuite, nous allons charger le document et le diviser en paragraphes. Tout d'abord, téléchargez le PDF depuis le site web de la Maison Blanche dans la variable <code>pdf_data</code> :</p><pre><code class=\"language-python\">import urllib.request\n\nuri = \"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"\npdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Ensuite, nous allons traiter le PDF page par page, extraire le texte puis le découper en paragraphes en séparant sur les doubles sauts de ligne. Ceux-ci sont stockés dans la liste <code>text_paras</code> :</p><pre><code class=\"language-python\">import regex as re\nfrom io import BytesIO, StringIO\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfparser import PDFParser\n\ntext_paras = []\nparser = PDFParser(BytesIO(pdf_data))\ndoc = PDFDocument(parser)\nrsrcmgr = PDFResourceManager()\nfor page in PDFPage.create_pages(doc):\n&nbsp;&nbsp;&nbsp;&nbsp;output_string = StringIO()\n&nbsp;&nbsp;&nbsp;&nbsp;device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n&nbsp;&nbsp;&nbsp;&nbsp;interpreter = PDFPageInterpreter(rsrcmgr, device)\n&nbsp;&nbsp;&nbsp;&nbsp;interpreter.process_page(page)\n&nbsp;&nbsp;&nbsp;&nbsp;page_text = output_string.getvalue()\n&nbsp;&nbsp;&nbsp;&nbsp;text_paras.extend(re.split(r'\\n\\s*\\n', page_text))\n</code></pre><p>Vérifiez que tout est chargé :</p><pre><code class=\"language-python\">assert len(text_paras) == 615</code></pre><p>Ensuite, nous allons convertir cette liste de textes courts en <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io\">objets Document LlamaIndex</a> :</p><pre><code class=\"language-python\">from llama_index.core.readers import StringIterableReader\n\nrag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>Et vous pouvez inspecter le texte :</p><pre><code class=\"language-python\">print(rag_docs[0].text)</code></pre><p>Résultat :</p><pre><code class=\"language-text\">NATIONAL ARTIFICIAL INTELLIGENCE&nbsp;\nRESEARCH AND DEVELOPMENT&nbsp;\nSTRATEGIC PLAN&nbsp;\n2023 UPDATE</code></pre><h3 id=\"set-up-a-qdrant-vector-database\"><strong>Configurer une base de données vectorielle Qdrant</strong></h3><p>Dans cet article, nous utiliserons le <a href=\"https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io\"><u>Qdrant Vector Search Cloud</u></a> pour implémenter la base de données du système RAG. L'offre gratuite de Qdrant inclut 1 Go de stockage, ce qui est plus que suffisant pour ce tutoriel.</p><p>Vous devrez <a href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><u>créer un compte sur le site web de Qdrant Cloud</u></a> avant de continuer.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Vector Search Database | Qdrant Cloud</div><div class=\"kg-bookmark-description\">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cloud.qdrant.io/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Qdrant Cloud</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cloud.qdrant.io/social.png\" alt=\"\"></div></a></figure><p>Une fois que vous avez un compte et que vous êtes connecté, vous devrez créer un cluster. Suivez les <a href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>instructions de \"démarrage rapide\" sur le site web de Qdrant</u></a> pour configurer un cluster gratuit et obtenir une API et le nom du serveur hôte Qdrant.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Quickstart - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Edit on GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png\" alt=\"\"></div></a></figure><p>Stockez la clé et le nom d'hôte dans des variables :</p><pre><code class=\"language-python\">qdrant_api_key = \"&lt;your API key&gt;\"\nqdrant_server = \"https://&lt;your server&gt;\"</code></pre><p>Ensuite, nous devrons importer les composants pertinents des packages <code>qdrant_client</code> et <code>llama_index</code> :</p><pre><code class=\"language-python\">import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nclient = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\nvector_store = QdrantVectorStore(client=client, collection_name=\"NTSC\")</code></pre><p>Ceci crée une collection nommée <code>NTSC</code> dans votre cluster gratuit.</p><p>Vous pouvez consulter la <a href=\"https://qdrant.tech/documentation/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>documentation Qdrant</u></a> pour plus d'informations sur la création d'un stockage de données local en mémoire ou sur disque et l'hébergement de votre propre instance de serveur Qdrant dans un conteneur docker via une API web.</p><h3 id=\"complete-the-rag-system\"><strong>Compléter le système RAG</strong></h3><p>Nous allons maintenant assembler ces composants en un système RAG complet en utilisant le code standard de LlamaIndex :</p><pre><code class=\"language-python\">from llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core import (\n\t\tVectorStoreIndex,\n\t\tServiceContext,\n\t\tget_response_synthesizer,\n)\n\n# set up the service and storage contexts\nservice_context = ServiceContext.from_defaults(\n&nbsp;&nbsp;&nbsp;&nbsp;llm=mixtral_llm, embed_model=jina_embedding_model\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# create an index\nindex = VectorStoreIndex.from_documents(\n&nbsp;&nbsp;&nbsp;&nbsp;rag_docs, storage_context=storage_context, service_context=service_context\n)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n&nbsp;&nbsp;&nbsp;&nbsp;service_context=service_context,\n&nbsp;&nbsp;&nbsp;&nbsp;text_qa_template=qa_prompt,\n&nbsp;&nbsp;&nbsp;&nbsp;response_mode=\"compact\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever,\n&nbsp;&nbsp;&nbsp;&nbsp;response_synthesizer=response_synthesizer,\n)\n</code></pre><p>Ce système utilisera Jina Embeddings pour indexer les paragraphes de notre source textuelle et placera les deux meilleures correspondances dans le contexte de chaque prompt. Pour passer aux trois meilleures correspondances par exemple, il suffit de modifier l'initialisation de <code>VectorIndexRetriever</code> ci-dessus pour avoir un paramètre <code>similarity_top_k</code> différent :</p><pre><code class=\"language-python\"># configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3,\n)</code></pre><p>Le système est maintenant prêt à être utilisé.</p><h3 id=\"query-a-document\"><strong>Interroger un Document</strong></h3><p>Essayons une requête simple :</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat is the Biden Administration's policy with regard to AI?&nbsp;\n\"\"\")\nprint(response.response)</code></pre><p>Le résultat (le vôtre peut être différent) :</p><pre><code class=\"language-text\">The Biden Administration prioritizes responsible AI systems that&nbsp;\nare ethical, trustworthy, and safe, and serve the public good. \nThe Administration has committed to substantial funding for AI \nR&amp;D, and has established the National AI Initiative Office to \ncoordinate AI activities across the federal government.&nbsp;\n\nThe Administration's AI policy focuses on developing evaluative\ntechniques for AI, workforce development, public-private\npartnerships, and international collaboration in AI research.</code></pre><p>Ou quelque chose de plus spécifique :</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat protections does the AI Bill of Rights propose to offer?\n\"\"\")\n\nprint(response.response)</code></pre><pre><code class=\"language-text\">The AI Bill of Rights proposes five core protections: Safe and&nbsp;\nEffective Systems; Algorithmic Discrimination Protections; Data&nbsp;\nPrivacy; Notice and Explanation; and Human Alternatives,&nbsp;\nConsideration, and Fallback.</code></pre><p>Ou même très spécifique :</p><pre><code class=\"language-python\">response = query_engine.query(\"Who is Kei Koizumi?\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">Kei Koizumi is the Principal Deputy Director for Policy at the Office&nbsp;\nof Science and Technology Policy (OSTP).</code></pre><p>Vous pouvez également poser des questions plus fantaisistes :</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat rights will AI's receive under President Biden's proposed&nbsp;\nAI Bill of Rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">The proposed AI Bill of Rights does not grant rights to AI systems, but&nbsp;\nrather outlines principles to ensure that AI technologies are&nbsp;\ndeveloped and used in a manner that respects human rights and values.&nbsp;\nThe principles include safety, transparency, fairness, disclosure,&nbsp;\ndata privacy, non-discrimination, and the ability for individuals to&nbsp;\nopt out of certain AI systems.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhy is President Biden's proposing an AI Bill of Rights?\nDoes AI really need rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">President Biden's proposed AI Bill of Rights aims to ensure that AI&nbsp;\ntechnologies are developed and used ethically, legally, and with&nbsp;\nsocietal considerations in mind. The Bill of Rights is not granting&nbsp;\n\"rights\" to AI, but rather setting guidelines for the responsible&nbsp;\ndevelopment and deployment of AI systems to protect individuals and&nbsp;\ncommunities from potential negative impacts.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nHas Donald Trump weighed in on AI?\nWill he Make Humans Great Again?\n\"\"\")\nprint(response.response)\n</code></pre><pre><code class=\"language-text\">No information. The context information does not mention Donald&nbsp;\nTrump's views on AI.</code></pre><h2 id=\"jina-ai-and-qdrant\"><strong>Jina AI et Qdrant</strong></h2><p>La fenêtre de contexte de 8k tokens de Jina Embeddings le rend particulièrement adapté à la génération augmentée par récupération car il peut travailler avec des blocs de texte beaucoup plus grands. Là où d'autres applications RAG doivent souvent stocker des phrases individuelles ou des paires de phrases, dans ce tutoriel, nous avons pu utiliser des paragraphes entiers sans nous soucier de leur taille.</p><p>Cela réduit le nombre d'éléments de texte qui doivent être stockés pour transporter la même information, réduisant les coûts de calcul, économisant la mémoire et rendant la récupération plus rapide.</p><p>La base de données vectorielle de Qdrant est simple à configurer, rapide et rentable, et comme vous l'avez vu, il ne faut que quelques lignes de Python pour l'intégrer dans un système RAG.</p><p>Pour en savoir plus sur la base de données prête pour l'IA et les offres cloud de Qdrant, vous pouvez <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>visiter leur site web</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qdrant - Vector Database</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Vector Database</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/images/previews/social-preview-P.png\" alt=\"\"></div></a></figure><h2 id=\"get-in-touch\"><strong>Contactez-nous</strong></h2><p>Jina AI s'engage à apporter des technologies d'IA fiables et abordables aux entreprises de toute taille et de tout type. Nous aimerions en savoir plus sur vos cas d'utilisation et vous aider à intégrer l'IA dans vos processus métier. Pour plus d'informations sur les offres de Jina AI et pour nous contacter, consultez le <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>site web de Jina AI</u></a> ou rejoignez notre <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>communauté sur Discord</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Best Embeddings and Perfect Prompts</div><div class=\"kg-bookmark-description\">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-author\">Best Embeddings and Perfect Prompts</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Join the Jina AI Discord Server!</div><div class=\"kg-bookmark-description\">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://discord.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Discord</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512\" alt=\"\"></div></a></figure>",
  "comment_id": "65ddadd039a5b70001810ea3",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-02-27T10:39:28.000+01:00",
  "updated_at": "2024-03-06T15:41:18.000+01:00",
  "published_at": "2024-03-04T16:00:13.000+01:00",
  "custom_excerpt": "Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/build-a-rag-system-with-jina-embeddings-and-qdrant/",
  "excerpt": "Créer un système RAG avec Jina Embeddings v2, la base de données vectorielle Qdrant, LlamaIndex et le LLM Mistral.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Promotional graphic with \"EMBEDDINGS 2\" and Drant logo separated by a red cross, symbolizing a partnership.",
  "feature_image_caption": null
}
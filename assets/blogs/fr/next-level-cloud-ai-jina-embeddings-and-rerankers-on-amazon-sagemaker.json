{
  "slug": "next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker",
  "id": "65fabb91502fd000011c667e",
  "uuid": "45cd5187-838d-46b7-a8a0-d890fcda9041",
  "title": "IA cloud de nouvelle génération : Jina Embeddings et Rerankers sur Amazon SageMaker",
  "html": "<p><a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> et <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io\">Jina Reranker</a> sont désormais disponibles sur <a href=\"https://aws.amazon.com/pm/sagemaker/?ref=jina-ai-gmbh.ghost.io\">Amazon SageMaker</a> via l'<a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>. Pour les utilisateurs enterprise qui accordent une grande importance à la sécurité, la fiabilité et la cohérence dans leurs opérations cloud, cela permet d'intégrer l'IA de pointe de Jina AI dans leurs déploiements AWS privés, où ils bénéficient de tous les avantages de l'infrastructure stable et établie d'AWS.</p><p>Avec notre gamme complète de modèles d'embedding et de reranking sur AWS Marketplace, les utilisateurs de SageMaker peuvent profiter de fenêtres de contexte révolutionnaires de 8k et d'embeddings multilingues de premier rang à la demande à des prix compétitifs. Vous n'avez pas à payer pour transférer des modèles vers ou depuis AWS, les prix sont transparents, et votre facturation est intégrée à votre compte AWS.</p><p>Les modèles actuellement disponibles sur Amazon SageMaker comprennent :</p><ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6w6k6ckusixpw?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Small - English</a></li><li>Modèles Bilingues Jina Embeddings v2 :<ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-dz3ubvmivnwry?ref=jina-ai-gmbh.ghost.io\">Allemand/Anglais</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-hxalozh37jka4?ref=jina-ai-gmbh.ghost.io\">Chinois/Anglais</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-rnb324fpie3n6?ref=jina-ai-gmbh.ghost.io\">Espagnol/Anglais</a></li></ul></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-tk7t7bz6fp5ng?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - Code</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\">Jina Reranker v1 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6kxbf5xqrluf4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT v1 - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-mgomngrh4c4k4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT Reranker v1 - English</a></li></ul><p>Pour la liste complète des modèles, consultez la <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">page vendeur de Jina AI sur AWS Marketplace</a>, et profitez d'un essai gratuit de sept jours.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Cet article vous guidera dans la création d'une application de <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\">Retrieval-augmented generation</a> (RAG) utilisant exclusivement des composants d'Amazon SageMaker. Les modèles que nous utiliserons sont <strong>Jina Embeddings v2 - English</strong>, <strong>Jina Reranker v1</strong>, et le modèle de langage <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\">Mistral-7B-Instruct</a>.</p><p>Vous pouvez également suivre avec un Notebook Python, que vous pouvez <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">télécharger</a> ou <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">exécuter sur Google Colab</a>.</p><h2 id=\"retrieval-augmented-generation\">Retrieval-Augmented Generation</h2><p>La Retrieval-augmented generation est un paradigme alternatif en IA générative. Au lieu d'utiliser des grands modèles de langage (LLM) pour répondre directement aux requêtes des utilisateurs avec ce qu'ils ont appris pendant l'entraînement, elle exploite leur production fluide du langage tout en déplaçant la logique et la recherche d'informations vers un appareil externe mieux adapté.</p><p>Avant d'invoquer un LLM, les systèmes RAG récupèrent activement les informations pertinentes d'une source de données externe puis les intègrent dans le prompt du LLM. Le rôle du LLM est de synthétiser les informations externes en une réponse cohérente aux demandes des utilisateurs, minimisant le risque d'hallucination et augmentant la pertinence et l'utilité du résultat.</p><p>Un système RAG comporte schématiquement au moins quatre composants :</p><ul><li>Une source de données, généralement une base de données vectorielle, adaptée à la recherche d'informations assistée par l'IA.</li><li>Un système de recherche d'informations qui traite la requête de l'utilisateur et récupère les données pertinentes pour y répondre.</li><li>Un système, incluant souvent un reranker basé sur l'IA, qui sélectionne certaines des données récupérées et les transforme en prompt pour un LLM.</li><li>Un LLM, par exemple l'un des modèles GPT ou un LLM open-source comme celui de Mistral, qui prend la requête de l'utilisateur et les données fournies pour générer une réponse.</li></ul><p>Les modèles d'embedding sont bien adaptés à la recherche d'informations et sont souvent utilisés à cette fin. Un modèle d'embedding de texte prend des textes en entrée et produit un <a href=\"https://jina.ai/news/how-embeddings-drive-ai-a-guide?ref=jina-ai-gmbh.ghost.io\">embedding</a> — un vecteur de haute dimension — dont la relation spatiale avec d'autres embeddings indique leur similarité sémantique, c'est-à-dire les sujets, contenus et significations apparentés. Ils sont souvent utilisés dans la recherche d'informations car plus les embeddings sont proches, plus l'utilisateur sera satisfait de la réponse. Ils sont également relativement faciles à affiner pour améliorer leurs performances dans des domaines spécifiques.</p><p>Les modèles de <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">reranking de texte</a> utilisent des principes d'IA similaires pour comparer des collections de textes à une requête et les trier par similarité sémantique. L'utilisation d'un modèle de reranking spécifique à la tâche, plutôt que de s'appuyer uniquement sur un modèle d'embedding, augmente souvent considérablement la précision des résultats de recherche. Dans une application RAG, le reranker sélectionne certains des résultats de la recherche d'informations afin de maximiser la probabilité que les bonnes informations se trouvent dans le prompt du LLM.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class=\"kg-bookmark-description\">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png\" alt=\"\"></div></a></figure><h2 id=\"benchmarking-performance-of-embedding-models-as-sagemaker-endpoints\"><strong>Tests de Performance des Modèles d'Embedding comme Points de Terminaison SageMaker</strong></h2><p>Nous avons testé les performances et la fiabilité du modèle <strong>Jina Embeddings v2 Base - English</strong> comme point de terminaison SageMaker, fonctionnant sur une instance <a href=\"https://aws.amazon.com/ec2/instance-types/g4/?ref=jina-ai-gmbh.ghost.io\">g4dn.xlarge</a>. Dans ces expériences, nous avons continuellement créé un nouvel utilisateur chaque seconde, chacun envoyant une requête, attendant sa réponse, et répétant l'opération après réception.</p><ul><li>Pour les requêtes de <em>moins de 100 tokens</em>, jusqu'à 150 utilisateurs simultanés, les temps de réponse <em>par requête</em> sont restés inférieurs à 100ms. Ensuite, les temps de réponse ont augmenté linéairement de 100ms à 1500ms avec l'ajout d'utilisateurs simultanés.<ul><li>À environ <em>300 utilisateurs simultanés</em>, nous avons reçu plus de 5 échecs de l'API et avons terminé le test.</li></ul></li><li>Pour les requêtes entre 1K et 8K tokens, jusqu'à 20 utilisateurs simultanés, les temps de réponse <em>par requête</em> sont restés inférieurs à 8s. Ensuite, les temps de réponse ont augmenté linéairement de 8s à 60s avec l'ajout d'utilisateurs simultanés.<ul><li>À environ <em>140 utilisateurs simultanés</em>, nous avons reçu plus de 5 échecs de l'API et avons terminé le test.</li></ul></li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png\" class=\"kg-image\" alt=\"Four comparative graphs displaying &quot;Small Context&quot; versus &quot;Large Context&quot; results over time, assessing performance metrics.\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/03/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Performance pendant les tests (gauche : petit contexte, droite : grand contexte), montrant l'effet de l'augmentation du nombre d'utilisateurs au fil du temps sur les temps de réponse et les taux d'échec.</span></figcaption></figure><p>Sur la base de ces résultats, nous pouvons conclure que pour la plupart des utilisateurs ayant des charges de travail d'embedding normales, les instances g4dn.xlarge ou g5.xlarge devraient répondre à leurs besoins quotidiens. Cependant, pour les tâches d'<em>indexation</em> importantes, qui sont généralement exécutées beaucoup moins souvent que les tâches de <em>recherche</em>, les utilisateurs pourraient préférer une option plus performante. Pour une liste de toutes les instances Sagemaker disponibles, veuillez consulter l'aperçu des instances <a href=\"https://aws.amazon.com/ec2/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">EC2</a> d'AWS.</p><h2 id=\"configure-your-aws-account\">Configurez votre compte AWS</h2><p>Tout d'abord, vous devez avoir un compte AWS. Si vous n'êtes pas déjà un utilisateur AWS, vous pouvez vous <a href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\">inscrire</a> pour un compte sur le site web d'AWS.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Console - Signup</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://portal.aws.amazon.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Signup</span></div></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Vous ne pourrez pas terminer ce tutoriel avec un compte Free Tier car Amazon ne fournit pas d'accès gratuit à SageMaker. Vous devez ajouter un moyen de paiement au compte pour vous abonner aux modèles de Jina AI, même si vous utilisez <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">notre essai gratuit de sept jours</a>.</div></div><h3 id=\"set-up-aws-tools-in-your-python-environment\">Configurez les outils AWS dans votre environnement Python</h3><p>Installez dans votre environnement Python les outils et bibliothèques AWS nécessaires pour ce tutoriel :</p><pre><code class=\"language-bash\">pip install awscli jina-sagemaker\n</code></pre><p>Vous devrez obtenir une clé d'accès et une clé d'accès secrète pour votre compte AWS. Pour ce faire, suivez les <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\">instructions sur le site web d'AWS</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Managing access keys for IAM users - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Create, modify, view, or update access keys (credentials) for programmatic calls to AWS.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/security-credentials-user.shared.console.png\" alt=\"\"></div></a></figure><p>Vous devrez également choisir une <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\">région AWS</a> dans laquelle travailler.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Regions, Availability Zones, and Local Zones - Amazon Relational Database Service</div><div class=\"kg-bookmark-description\">Learn how Amazon cloud computing resources are hosted in multiple locations world-wide, including AWS Regions and Availability Zones.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Relational Database Service</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/Con-AZ-Local.png\" alt=\"\"></div></a></figure><p>Ensuite, définissez les valeurs dans les variables d'environnement. En Python ou dans un notebook Python, vous pouvez le faire avec le code suivant :</p><pre><code class=\"language-bash\">import os\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = &lt;YOUR_ACCESS_KEY_ID&gt;\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = &lt;YOUR_SECRET_ACCESS_KEY&gt;\nos.environ[\"AWS_DEFAULT_REGION\"] = &lt;YOUR_AWS_REGION&gt;\nos.environ[\"AWS_DEFAULT_OUTPUT\"] = \"json\"\n</code></pre><p>Définissez la sortie par défaut sur <code>json</code>.</p><p>Vous pouvez également le faire via l'<a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html?ref=jina-ai-gmbh.ghost.io\">application en ligne de commande AWS</a> ou en configurant un <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html?ref=jina-ai-gmbh.ghost.io\">fichier de configuration AWS</a> sur votre système de fichiers local. Consultez la <a href=\"https://docs.aws.amazon.com/index.html?ref=jina-ai-gmbh.ghost.io\">documentation sur le site web d'AWS</a> pour plus de détails.</p><h3 id=\"create-a-role\">Créez un rôle</h3><p>Vous aurez également besoin d'un <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\">rôle AWS</a> avec des permissions suffisantes pour utiliser les ressources requises pour ce tutoriel.</p><p>Ce rôle doit :</p><ol><li>Avoir <strong>AmazonSageMakerFullAccess</strong> activé.</li><li>Soit :<ol><li>Avoir l'autorité pour faire des abonnements AWS Marketplace et avoir activé les trois éléments suivants :<ol><li><strong>aws-marketplace:ViewSubscriptions</strong></li><li><strong>aws-marketplace:Unsubscribe</strong></li><li><strong>aws-marketplace:Subscribe</strong></li></ol></li><li>Ou votre compte AWS a un abonnement à <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">jina-embedding-model</a>.</li></ol></li></ol><p>Stockez l'ARN (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html?ref=jina-ai-gmbh.ghost.io\">Amazon Resource Name</a>) du rôle dans la variable nommée <code>role</code> :</p><pre><code class=\"language-python\">role = &lt;YOUR_ROLE_ARN&gt;\n</code></pre><p>Consultez la documentation sur les rôles sur le site web d'AWS pour plus d'informations.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">IAM roles - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Learn how and when to use IAM roles.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div></a></figure><h3 id=\"subscribe-to-jina-ai-models-on-aws-marketplace\">Abonnez-vous aux modèles Jina AI sur AWS Marketplace</h3><p>Dans cet article, nous utiliserons le modèle Jina Embeddings v2 base English. Abonnez-vous sur <a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">l'AWS Marketplace</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Embeddings v2 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Vous verrez les informations de tarification en faisant défiler la page vers le bas. AWS facture à l'heure pour les modèles du marketplace, vous serez donc facturé pour le temps écoulé entre le démarrage du point de terminaison du modèle et son arrêt. Cet article vous montrera comment faire les deux.</p><p>Nous utiliserons également le modèle Jina Reranker v1 - English, auquel vous devrez vous abonner.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Reranker v1 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">Jina AI offre actuellement un essai gratuit de sept jours pour ses modèles. Vous devrez toujours payer pour les instances AWS qui les exécutent, mais pendant la période d'essai, vous n'avez pas à payer de frais supplémentaires pour les modèles.</div></div><p>Une fois que vous vous êtes abonné, récupérez les ARN des modèles pour votre région AWS et stockez-les dans les noms de variables <code>embedding_package_arn</code> et <code>reranker_package_arn</code> respectivement. Le code de ce tutoriel y fera référence en utilisant ces noms de variables.</p><p>Si vous ne savez pas comment obtenir les ARN, mettez le nom de votre région Amazon dans la variable <code>region</code> et utilisez le code suivant :</p><pre><code class=\"language-python\">region = os.environ[\"AWS_DEFAULT_REGION\"]\n\ndef get_arn_for_model(region_name, model_name):\n    model_package_map = {\n        \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n        \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n        \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n        \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n        \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n        \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n        \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n        \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n        \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n        \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n        \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n        \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n        \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n        \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n        \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n        \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n    }\n\n    return model_package_map[region_name]\n\nembedding_package_arn = get_arn_for_model(region, \"jina-embeddings-v2-base-en\")\nreranker_package_arn = get_arn_for_model(region, \"jina-reranker-v1-base-en\")\n</code></pre><h2 id=\"load-the-dataset\">Charger le jeu de données</h2><p>Dans ce tutoriel, nous allons utiliser une collection de vidéos fournie par la chaîne YouTube <a href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\">TU Delft Online Learning</a>. Cette chaîne produit divers supports pédagogiques dans les matières STEM. Sa programmation est sous <a href=\"https://creativecommons.org/licenses/by/3.0/legalcode?ref=jina-ai-gmbh.ghost.io\">licence CC-BY</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TU Delft Online Learning</div><div class=\"kg-bookmark-description\">Vous souhaitez faire carrière dans les sciences, le design ou l'ingénierie ? Alors rejoignez la communauté des apprenants en ligne de TU Delft !\nÀ TU Delft, l'apprentissage en ligne signifie l'apprentissage actif. Nos cours sont conçus pour vous offrir une expérience d'apprentissage engageante. Le contenu des cours est stimulant et exigeant, favorisant votre développement personnel et professionnel, tout en profitant de la flexibilité et de l'accessibilité qu'offrent nos cours en ligne pour que vous puissiez combiner l'apprentissage avec d'autres priorités de votre vie. Commencez à apprendre aujourd'hui : https://online-learning.tud…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/s/desktop/4feff1e2/img/favicon_144x144.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://yt3.googleusercontent.com/ytc/AIdro_kH5d18Xqqj-MKv9k_tf2KNFufCpMY8qEXdQzEy=s900-c-k-c0x00ffffff-no-rj\" alt=\"\"></div></a></figure><p>Nous avons téléchargé 193 vidéos de la chaîne et les avons traitées avec le modèle de reconnaissance vocale open-source <a href=\"https://openai.com/research/whisper?ref=jina-ai-gmbh.ghost.io\">Whisper</a> d'OpenAI. Nous avons utilisé le plus petit modèle <a href=\"https://huggingface.co/openai/whisper-tiny?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>openai/whisper-tiny</code></a> pour transformer les vidéos en transcriptions.</p><p>Les transcriptions ont été organisées dans un fichier CSV, que vous pouvez <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/tu_delft.csv?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">télécharger ici</a>.</p><p>Chaque ligne du fichier contient :</p><ul><li>Le titre de la vidéo</li><li>L'URL de la vidéo sur YouTube</li><li>Une transcription textuelle de la vidéo</li></ul><p>Pour charger ces données en Python, installez d'abord <code>pandas</code> et <code>requests</code> :</p><pre><code class=\"language-bash\">pip install requests pandas\n</code></pre><p>Chargez les données CSV directement dans un DataFrame Pandas nommé <code>tu_delft_dataframe</code> :</p><pre><code class=\"language-python\">import pandas\n\n# Load the CSV file\ntu_delft_dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/jina-ai/workshops/feat-sagemaker-post/notebooks/embeddings/sagemaker/tu_delft.csv\")\n</code></pre><p>Vous pouvez inspecter le contenu en utilisant la méthode <code>head()</code> du DataFrame. Dans un notebook, cela devrait ressembler à ceci :</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png\" class=\"kg-image\" alt=\"Data frame montrant les titres de webinaires comme &quot;Green Teams in Hospitals&quot;, avec leurs URLs YouTube et des extraits de texte d'introduction,\" loading=\"lazy\" width=\"1440\" height=\"580\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-15-at-14.30.35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1440w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Vous pouvez également regarder les vidéos en utilisant les URLs fournies dans ce jeu de données et vérifier que la reconnaissance vocale est imparfaite mais globalement correcte.</p><h2 id=\"start-the-jina-embeddings-v2-endpoint\">Démarrer le point de terminaison Jina Embeddings v2</h2><p>Le code ci-dessous lancera une instance <code>ml.g4dn.xlarge</code> sur AWS pour exécuter le modèle d'embedding. Cela peut prendre plusieurs minutes pour se terminer.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Choose a name for your embedding endpoint. It can be anything convenient.\nembeddings_endpoint_name = \"jina_embedding\"\n\nembedding_client = Client(region_name=boto3.Session().region_name)\nembedding_client.create_endpoint(\n    arn=embedding_package_arn,\n    role=role,\n    endpoint_name=embeddings_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nembedding_client.connect_to_endpoint(endpoint_name=embeddings_endpoint_name)\n</code></pre><p>Modifiez le <code>instance_type</code> pour sélectionner un autre type d'instance cloud AWS si nécessaire.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">AWS vous facturera votre temps dès que cette commande se termine. Vous serez facturé à l'heure jusqu'à ce que vous arrêtiez cette instance. Pour ce faire, suivez les instructions de la section <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Arrêt</strong></b></a>.</div></div><h2 id=\"build-and-index-the-dataset\">Construire et indexer le jeu de données</h2><p>Maintenant que nous avons chargé les données et que nous exécutons un modèle Jina Embeddings v2, nous pouvons préparer et indexer les données. Nous stockerons les données dans une base de données vectorielle <a href=\"https://faiss.ai/index.html?ref=jina-ai-gmbh.ghost.io\">FAISS</a>, une base de données vectorielle open-source spécialement conçue pour les applications d'IA.</p><p>Tout d'abord, installez les prérequis restants pour notre application RAG :</p><pre><code class=\"language-bash\">pip install tdqm numpy faiss-cpu\n</code></pre><h3 id=\"chunking\">Découpage</h3><p>Nous devrons prendre les transcriptions individuelles et les diviser en parties plus petites, c'est-à-dire des \"chunks\", afin de pouvoir intégrer plusieurs textes dans une invite pour le LLM. Le code ci-dessous découpe les transcriptions individuelles aux limites des phrases, en s'assurant que tous les chunks ne contiennent pas plus de 128 mots par défaut.</p><pre>```python\ndef chunk_text(text, max_words=128):\n    \"\"\"\n    Divise le texte en morceaux où chaque morceau contient le nombre maximal\n    de phrases complètes avec moins de mots que `max_words`.\n    \"\"\"\n    sentences = text.split(\".\")\n    chunk = []\n    word_count = 0\n\n    for sentence in sentences:\n        sentence = sentence.strip(\".\")\n        if not sentence:\n          continue\n\n        words_in_sentence = len(sentence.split())\n        if word_count + words_in_sentence <= max_words:\n            chunk.append(sentence) \n            word_count += words_in_sentence\n        else:\n            # Retourner le morceau actuel et en démarrer un nouveau\n            if chunk:\n              yield \". \".join(chunk).strip() + \".\"\n            chunk = [sentence]\n            word_count = words_in_sentence\n\n    # Retourner le dernier morceau s'il n'est pas vide\n    if chunk:\n        yield \" \".join(chunk).strip() + \".\"\n```\n\n### Obtenir les Embeddings pour chaque Morceau\n\nNous avons besoin d'un embedding pour chaque morceau afin de le stocker dans la base de données FAISS. Pour les obtenir, nous passons les morceaux de texte au point de terminaison du modèle d'embedding Jina AI, en utilisant la méthode `embedding_client.embed()`. Ensuite, nous ajoutons les morceaux de texte et les vecteurs d'embedding au dataframe pandas `tu_delft_dataframe` comme nouvelles colonnes `chunks` et `embeddings` :\n\n```python\nimport numpy as np\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\ndef generate_embeddings(text_df):\n    chunks = list(chunk_text(text_df[\"Text\"]))\n    embeddings = []\n\n    for i, chunk in enumerate(chunks):\n      response = embedding_client.embed(texts=[chunk])\n      chunk_embedding = response[0][\"embedding\"]\n      embeddings.append(np.array(chunk_embedding))\n\n    text_df[\"chunks\"] = chunks\n    text_df[\"embeddings\"] = embeddings\n    return text_df\n\nprint(\"Embedding des morceaux de texte ...\")\ntu_delft_dataframe = generate_embeddings(tu_delft_dataframe)\n## si vous utilisez Google Colab ou un notebook Python, vous pouvez\n## supprimer la ligne ci-dessus et décommenter la ligne suivante :\n# tu_delft_dataframe = tu_delft_dataframe.progress_apply(generate_embeddings, axis=1)\n```\n\n### Configurer la Recherche Sémantique avec Faiss\n\nLe code ci-dessous crée une base de données FAISS et insère les morceaux et les vecteurs d'embedding en itérant sur `tu_delft_pandas` :\n\n```python\nimport faiss\n\ndim = 768  # dimension des embeddings Jina v2\nindex_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\nk = 0\n\ndoc_ref = dict()\n\nfor idx, row in tu_delft_dataframe.iterrows():\n    embeddings = row[\"embeddings\"]\n    for i, embedding in enumerate(embeddings):\n        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype=\"float32\").reshape(1, -1))\n        faiss.normalize_L2(normalized_embedding)\n        index_with_ids.add_with_ids(normalized_embedding, k)\n        doc_ref[k] = (row[\"chunks\"][i], idx)\n        k += 1\n```\n\n## Démarrer le Point de Terminaison Jina Reranker v1\n\nComme pour le modèle Jina Embedding v2 ci-dessus, ce code lancera une instance de `ml.g4dn.xlarge` sur AWS pour exécuter le modèle de reranking. De même, cela peut prendre plusieurs minutes à exécuter.\n\n```python\nimport boto3\nfrom jina_sagemaker import Client\n\n# Choisissez un nom pour votre point de terminaison reranker. Il peut être n'importe quoi de pratique.\nreranker_endpoint_name = \"jina_reranker\"\n\nreranker_client = Client(region_name=boto3.Session().region_name)\nreranker_client.create_endpoint(\n    arn=reranker_package_arn,\n    role=role,\n    endpoint_name=reranker_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nreranker_client.connect_to_endpoint(endpoint_name=reranker_endpoint_name)\n```\n\n## Définir les Fonctions de Requête\n\nEnsuite, nous allons définir une fonction qui identifie les morceaux de transcription les plus similaires à n'importe quelle requête textuelle.\n\nC'est un processus en deux étapes :\n\n1. Convertir l'entrée utilisateur en un vecteur d'embedding en utilisant la méthode `embedding_client.embed()`, comme nous l'avons fait dans l'étape de préparation des données.\n2. Passer l'embedding à l'index FAISS pour récupérer les meilleures correspondances. Dans la fonction ci-dessous, la valeur par défaut est de retourner les 20 meilleures correspondances, mais vous pouvez contrôler cela avec le paramètre `n`.\n\nLa fonction `find_most_similar_transcript_segment` retournera les meilleures correspondances en comparant les cosinus des embeddings stockés à l'embedding de la requête.\n\n```python\ndef find_most_similar_transcript_segment(query, n=20):\n    query_embedding = embedding_client.embed(texts=[query])[0][\"embedding\"]  # En supposant que la requête est assez courte pour ne pas nécessiter de découpage\n    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype=\"float32\").reshape(1, -1))\n    faiss.normalize_L2(query_embedding)\n\n    D, I = index_with_ids.search(query_embedding, n)  # Obtenir les n meilleures correspondances\n\n    results = []\n    for i in range(n):\n        distance = D[0][i]\n        index_id = I[0][i]\n        transcript_segment, doc_idx = doc_ref[index_id]\n        results.append((transcript_segment, doc_idx, distance))\n\n    # Trier les résultats par distance\n    results.sort(key=lambda x: x[2])\n\n    return [(tu_delft_dataframe.iloc[r[1]][\"Title\"].strip(), r[0]) for r in results]\n```\n\nNous allons également définir une fonction qui accède au point de terminaison reranker `reranker_client`, lui passe les résultats de `find_most_similar_transcript_segment`, et retourne seulement les trois résultats les plus pertinents. Elle appelle le point de terminaison reranker avec la méthode `reranker_client.rerank()`.\n\n```python\ndef rerank_results(query_found, query, n=3):\n    ret = reranker_client.rerank(\n        documents=[f[1] for f in query_found], \n        query=query, \n        top_n=n,\n    )\n    return [query_found[r['index']] for r in ret[0]['results']]\n```\n\n## Utiliser JumpStart pour Charger Mistral-Instruct\n\nPour ce tutoriel, nous utiliserons le modèle `mistral-7b-instruct`, qui est [disponible via Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io), comme partie LLM du système RAG.\n\n```python\nfrom sagemaker.jumpstart.model import JumpStartModel\n\njumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\nmodel_predictor = jumpstart_model.deploy()\n```\n\nLe point de terminaison pour accéder à ce LLM est stocké dans la variable `model_predictor`.\n\n⚠️ L'utilisation de ce modèle est également un service facturé par AWS, alors n'oubliez pas de l'arrêter lorsque vous aurez terminé ce tutoriel. Voir la section [**Arrêt**](#shutting-down) pour arrêter ce déploiement une fois terminé.\n\n### Mistral-Instruct avec JumpStart\n\nVoici le code pour créer un modèle de prompt pour Mistral-Instruct pour cette application en utilisant [la classe de modèle de chaîne intégrée à Python](https://docs.python.org/3/library/string.html?ref=jina-ai-gmbh.ghost.io#template-strings). Il suppose que pour chaque requête, il y a trois morceaux de transcription correspondants qui seront présentés au modèle.\n\nVous pouvez expérimenter avec ce modèle vous-même pour modifier cette application ou voir si vous pouvez obtenir de meilleurs résultats.\n\n```python\nfrom string import Template\n\nprompt_template = Template(\"\"\"\n  <s>[INST] Répondez à la question ci-dessous en utilisant uniquement le contexte donné.\n  La question de l'utilisateur est basée sur des transcriptions de vidéos d'une chaîne\n    YouTube.\n  Le contexte est présenté sous forme d'une liste classée d'informations sous la forme\n    (titre-vidéo, segment-transcription), qui est pertinente pour répondre à la\n    question de l'utilisateur.\n  La réponse ne doit utiliser que le contexte présenté. Si la question ne peut pas être\n    répondue sur la base du contexte, dites-le.\n\n  Contexte :\n  1. Titre-vidéo : $title_1, segment-transcription : $segment_1\n  2. Titre-vidéo : $title_2, segment-transcription : $segment_2\n  3. Titre-vidéo : $title_3, segment-transcription : $segment_3\n\n  Question : $question\n\n  Réponse : [/INST]\n\"\"\")\n```\n\nAvec ce composant en place, nous avons maintenant toutes les parties d'une application RAG complète.\n\n## Interroger le Modèle\n\nL'interrogation du modèle est un processus en trois étapes.\n\n1. Rechercher les morceaux pertinents pour une requête donnée.\n2. Assembler le prompt.\n3. Envoyer le prompt au modèle Mistral-Instruct et retourner sa réponse.\n\nPour rechercher les morceaux pertinents, nous utilisons la fonction `find_most_similar_transcript_segment` que nous avons définie ci-dessus.\n\n```python\nquestion = \"Quand a été mise en service la première ferme éolienne offshore ?\"\nsearch_results = find_most_similar_transcript_segment(question)\nreranked_results = rerank_results(search_results, question)\n```<p>Vous pouvez examiner les résultats de recherche dans l'ordre reclassé :</p><pre><code class=\"language-python\">for title, text, _ in reranked_results:\n    print(title + \"\\n\" + text + \"\\n\")\n</code></pre><p>Résultat :</p><pre><code class=\"language-text\">Offshore Wind Farm Technology - Course Introduction\nSince the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n\nOffshore Wind Farm Technology - Course Introduction\nI'm a researcher and lecturer at the Wind Energy and Economics Department and I will be your moderator throughout this course.  That means I will answer any questions you may have.  I'll strengthen the interactions between the participants and also I'll get you in touch with the lecturers when needed.  The course is mainly developed for professionals in the field of offshore wind energy.  We want to broaden their knowledge of the relevant technical disciplines and their integration.  Professionals with a scientific background who are new to the field of offshore wind energy will benefit from a high-level insight into the engineering aspects of wind energy.  Overall, the course will help you make the right choices during the development and operation of offshore wind farms.\n\nOffshore Wind Farm Technology - Course Introduction\nDesigned wind turbines that better withstand wind, wave and current loads  Identify great integration strategies for offshore wind turbines and gain understanding of the operational and maintenance of offshore wind turbines and farms  We also hope that you will benefit from the course and from interaction with other learners who share your interest in wind energy  And therefore we look forward to meeting you online.\n</code></pre><p>Nous pouvons utiliser ces informations directement dans le modèle de prompt :</p><pre><code class=\"language-python\">prompt_for_llm = prompt_template.substitute(\n    question = question,\n    title_1 = search_results[0][0],\n    segment_1 = search_results[0][1],\n    title_2 = search_results[1][0],\n    segment_2 = search_results[1][1],\n    title_3 = search_results[2][0],\n    segment_3 = search_results[2][1],\n)\n</code></pre><p>Imprimez la chaîne résultante pour voir quel prompt est réellement envoyé au LLM :</p><pre><code class=\"language-python\">print(prompt_for_llm)\n</code></pre><pre><code class=\"language-text\">&lt;s&gt;[INST] Answer the question below only using the given context.\n  The question from the user is based on transcripts of videos from a YouTube\n    channel.\n  The context is presented as a ranked list of information in the form of\n    (video-title, transcript-segment), that is relevant for answering the\n    user's question.\n  The answer should only use the presented context. If the question cannot be\n    answered based on the context, say so.\n\n  Context:\n  1. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n  2. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: For example, we look at how to characterize the wind and wave conditions at a given location.  How to best place the wind turbines in a farm and also how to retrieve the electricity back to shore.  We look at the main design drivers for offshore wind turbines and their components.  We'll see how these aspects influence one another and the best choices to reduce the cost of energy.  This course is organized by the two-delfd wind energy institute, an interfaculty research organization focusing specifically on wind energy.  You will therefore benefit from the expertise of the lecturers in three different faculties of the university.  Aerospace engineering, civil engineering and electrical engineering.  Hi, my name is Ricardo Pareda.\n  3. Video-title: Systems Analysis for Problem Structuring part 1B the mono actor perspective example, transcript-segment: So let's assume the demarcation of the problem and the analysis of objectives has led to the identification of three criteria.  The security of supply, the percentage of offshore power generation and the costs of energy provision.  We now reason backwards to explore what factors have an influence on these system outcomes.  Really, the offshore percentage is positively influenced by the installed Wind Power capacity at sea, a key system factor.  Capacity at sea in turn is determined by both the size and the number of wind farms at sea.  The Ministry of Economic Affairs cannot itself invest in new wind farms but hopes to simulate investors and energy companies by providing subsidies and by expediting the granting process of licenses as needed.\n\n  Question: When was the first offshore wind farm commissioned?\n\n  Answer: [/INST]\n</code></pre><p>Passez ce prompt au point de terminaison LLM — <code>model_predictor</code> — via la méthode <code>model_predictor.predict()</code> :</p><pre><code class=\"language-python\">answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n</code></pre><p>Cela renvoie une liste, mais puisque nous n'avons passé qu'un seul prompt, ce sera une liste avec une seule entrée. Chaque entrée est un <code>dict</code> avec le texte de réponse sous la clé <code>generated_text</code> :</p><pre><code class=\"language-python\">answer = answer[0]['generated_text']\nprint(answer)\n</code></pre><p>Résultat :</p><pre><code class=\"language-text\">The first offshore wind farm was commissioned in 1991. (Context: Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, ...)\n</code></pre><p>Simplifions l'interrogation en écrivant une fonction pour effectuer toutes les étapes : prendre la question en chaîne comme paramètre et renvoyer la réponse sous forme de chaîne :</p><pre><code class=\"language-python\">def ask_rag(question):\n    search_results = find_most_similar_transcript_segment(question)\n    reranked_results = rerank_results(search_results, question)\n    prompt_for_llm = prompt_template.substitute(\n        question = question,\n        title_1 = search_results[0][0],\n        segment_1 = search_results[0][1],\n        title_2 = search_results[1][0],\n        segment_2 = search_results[1][1],\n        title_3 = search_results[2][0],\n        segment_3 = search_results[2][1],\n    )\n    answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n    return answer[0][\"generated_text\"]\n</code></pre><p>Maintenant, nous pouvons poser d'autres questions. Les réponses dépendront du contenu des transcriptions vidéo. Par exemple, nous pouvons poser des questions détaillées lorsque la réponse est présente dans les données et obtenir une réponse :</p><pre><code class=\"language-python\">ask_rag(\"What is a Kaplan Meyer estimator?\")\n</code></pre><pre><code class=\"language-text\">The Kaplan Meyer estimator is a non-parametric estimator for the survival \nfunction, defined for both censored and not censored data. It is represented \nas a series of declining horizontal steps that approaches the truths of the \nsurvival function if the sample size is sufficiently large enough. The value \nof the empirical survival function obtained is assumed to be constant between \ntwo successive distinct observations.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"Who is Reneville Solingen?\")\n</code></pre><pre><code class=\"language-text\">Reneville Solingen is a professor at Delft University of Technology in Global \nSoftware Engineering. She is also a co-author of the book \"The Power of Scrum.\"\n</code></pre><pre><code class=\"language-python\">answer = ask_rag(\"What is the European Green Deal?\")\nprint(answer)\n</code></pre><pre><code class=\"language-text\">The European Green Deal is a policy initiative by the European Union to combat \nclimate change and decarbonize the economy, with a goal to make Europe carbon \nneutral by 2050. It involves the use of green procurement strategies in various \nsectors, including healthcare, to reduce carbon emissions and promote corporate \nsocial responsibility.\n</code></pre><p>Nous pouvons également poser des questions qui sortent du cadre des informations disponibles :</p><pre><code class=\"language-python\">ask_rag(\"What countries export the most coffee?\")\n</code></pre><pre><code class=\"language-text\">Based on the context provided, there is no clear answer to the user's \nquestion about which countries export the most coffee as the context \nonly discusses the Delft University's cafeteria discounts and sustainable \ncoffee options, as well as lithium production and alternatives for use in \nelectric car batteries.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\")\n</code></pre><pre><code class=\"language-text\">The context does not provide sufficient information to answer the question. \nThe context is about thermit welding of rails, stress concentration factors, \nand a lyrics video. There is no mention of woodchucks or the ability of \nwoodchuck to chuck wood in the context.\n</code></pre><p>Essayez vos propres requêtes. Vous pouvez également modifier la façon dont le LLM est invité pour voir si cela améliore vos résultats.</p><h2 id=\"shutting-down\">Arrêt</h2><p>Puisque vous êtes facturé à l'heure pour les modèles que vous utilisez et pour l'infrastructure AWS pour les exécuter, il est très important d'arrêter les trois modèles d'IA lorsque vous terminez ce tutoriel :</p><ul><li>Le point de terminaison du modèle d'embedding <code>embedding_client</code></li><li>Le point de terminaison du modèle de reranking <code>reranker_client</code></li><li>Le point de terminaison du grand modèle de langage <code>model_predictor</code></li></ul><p>Pour arrêter les trois points de terminaison du modèle, exécutez le code suivant :</p><pre><code class=\"language-python\"># shut down the embedding endpoint\nembedding_client.delete_endpoint()\nembedding_client.close()\n# shut down the reranker endpoint\nreranker_client.delete_endpoint()\nreranker_client.close()\n# shut down the LLM endpoint\nmodel_predictor.delete_model()\nmodel_predictor.delete_endpoint()\n</code></pre><h2 id=\"get-started-now-with-jina-ai-models-on-aws-marketplace\">Commencez dès maintenant avec les modèles Jina AI sur AWS Marketplace</h2><p>Avec nos modèles d'embedding et de reranking sur SageMaker, les utilisateurs d'IA d'entreprise sur AWS ont maintenant un accès instantané à la proposition de valeur exceptionnelle de Jina AI sans compromettre les avantages de leurs opérations cloud existantes. Toute la sécurité, la fiabilité, la cohérence et la tarification prévisible d'AWS sont intégrées.</p><p>Chez Jina AI, nous travaillons dur pour apporter l'état de l'art aux entreprises qui peuvent bénéficier de l'intégration de l'IA dans leurs processus existants. Nous nous efforçons d'offrir des modèles solides, fiables et performants à des prix accessibles via des interfaces pratiques et pratiques, minimisant vos investissements en IA tout en maximisant vos retours.</p><p>Consultez <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">la page AWS Marketplace de Jina AI</a> pour une liste de tous les modèles d'embeddings et de reranking que nous proposons et pour essayer nos modèles gratuitement pendant sept jours.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace : Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Nous aimerions connaître vos cas d'utilisation et discuter de la façon dont les produits de Jina AI peuvent répondre aux besoins de votre entreprise. Contactez-nous via <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\">notre site web</a> ou notre <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\">canal Discord</a> pour partager vos commentaires et rester informé de nos derniers modèles.</p>",
  "comment_id": "65fabb91502fd000011c667e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/03/Blog-images--27-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-03-20T11:33:53.000+01:00",
  "updated_at": "2024-03-25T19:10:29.000+01:00",
  "published_at": "2024-03-25T16:00:51.000+01:00",
  "custom_excerpt": "Learn to use Jina Embeddings and Reranking models in a full-stack AI application on AWS, using only components available in Amazon SageMaker and the AWS Marketplace.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker/",
  "excerpt": "Apprenez à utiliser les modèles Jina Embeddings et Reranking dans une application IA full-stack sur AWS, en utilisant uniquement des composants disponibles dans Amazon SageMaker et l'AWS Marketplace.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract image with colorful wavy background featuring AWS, Embeddings, and Reranker logos.",
  "feature_image_caption": null
}
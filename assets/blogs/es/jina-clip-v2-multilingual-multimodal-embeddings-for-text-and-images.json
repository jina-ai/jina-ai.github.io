{
  "slug": "jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images",
  "id": "673cc4a7a7c46d00015cf1f5",
  "uuid": "6ca44950-b989-494a-b587-70847f24edd2",
  "title": "Jina CLIP v2: Embeddings multimodales y multilingües para texto e imágenes",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">Estamos en un viaje para avanzar y democratizar la inteligencia artificial a través del código abierto y la ciencia abierta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-11.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-clip-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?sui=&model=jina-clip-v2&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Tu Base de Búsqueda, Potenciada.</div><div class=\"kg-bookmark-description\">Los mejores embeddings, rerankers, LLM-reader, web scraper, clasificadores. La mejor IA de búsqueda para datos multilingües y multimodales.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-11.png\" alt=\"\"><span class=\"kg-bookmark-author\">Tu Base de Búsqueda, Potenciada.</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> API está disponible bajo la pestaña \"Embeddings\".</span></p></figcaption></figure><p>Los embeddings multimodales permiten buscar y entender datos a través de diferentes modalidades mediante una representación coherente. Sirven como columna vertebral para la recuperación de información neural y aplicaciones de GenAI multimodales. Hoy nos complace anunciar <code>jina-clip-v2</code>, unos nuevos embeddings multimodales multilingües de propósito general construidos sobre <code>jina-clip-v1</code> y nuestro recientemente lanzado <code>jina-embeddings-3</code>, que presenta varias mejoras clave:</p><ul><li><strong>Rendimiento Mejorado</strong>: v2 muestra una mejora del 3% sobre v1 tanto en tareas de recuperación texto-imagen como texto-texto. Similar a v1, el codificador de texto de v2 puede servir como un eficaz recuperador denso multilingüe de contexto largo. Tiene un rendimiento a la par con nuestro modelo frontera <code>jina-embeddings-v3</code> (actualmente los mejores embeddings multilingües con menos de 1B parámetros en MTEB).</li><li><strong>Soporte Multilingüe</strong>: Impulsado por <code>jina-embeddings-v3</code> como torre de texto, <code>jina-clip-v2</code> soporta 89 idiomas para recuperación multilingüe de imágenes, mostrando hasta un 4% de mejora comparado con <code>nllb-clip-large-siglip</code> en tareas de recuperación multilingüe de imágenes.</li><li><strong>Mayor Resolución de Imagen</strong>: v2 ahora soporta una resolución de imagen de entrada de 512x512, un aumento significativo desde los 224x224 de v1. Esta mayor resolución permite un mejor procesamiento de imágenes detalladas, mejor extracción de características y reconocimiento más preciso de elementos visuales detallados.</li><li><strong>Representaciones Matryoshka</strong>: v2 permite a los usuarios truncar las dimensiones de salida de los embeddings tanto de texto como de imagen desde 1024 hasta 64, reduciendo el almacenamiento y la sobrecarga de procesamiento mientras mantiene un fuerte rendimiento.</li></ul><h2 id=\"model-architecture\">Arquitectura del Modelo</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--35-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"></figure><p><code>jina-clip-v2</code> es un modelo estilo CLIP de 0.9B que combina dos potentes codificadores: el codificador de texto <code>Jina XLM-RoBERTa</code> (la columna vertebral de <code>jina-embeddings-v3</code>) y el codificador de visión <code>EVA02-L14</code> (un Transformer de visión eficiente desarrollado por BAAI). Estos codificadores son entrenados conjuntamente para crear representaciones alineadas de imágenes y texto.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Text Encoder</th>\n<th>Image Encoder</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Base Model</td>\n<td>Jina XLM-RoBERTa</td>\n<td>EVA02-L</td>\n</tr>\n<tr>\n<td>Parameters</td>\n<td>561M</td>\n<td>304M</td>\n</tr>\n<tr>\n<td>Input Specification</td>\n<td>8,192 tokens (max)</td>\n<td>512×512 pixels</td>\n</tr>\n<tr>\n<td>Min Output Dimensions</td>\n<td>64</td>\n<td>64</td>\n</tr>\n<tr>\n<td>Max Output Dimensions</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Attention Mechanism</td>\n<td>FlashAttention2</td>\n<td>xFormers</td>\n</tr>\n<tr>\n<td>Pooling Strategy</td>\n<td>Mean pooling</td>\n<td>CLS pooling</td>\n</tr>\n<tr>\n<td>Additional Features</td>\n<td>89 languages supported</td>\n<td>Patch size 14x14</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"cross-modal-retrieval-performance\">Rendimiento de Recuperación Cross-Modal</h2><p>Jina CLIP v2 proporciona soporte multilingüe para 89 idiomas y con rendimiento superior en los principales idiomas, incluyendo árabe, chino, inglés, francés, alemán, japonés, ruso y español. En benchmarks de recuperación de imágenes multilingües, muestra un rendimiento que iguala o supera a <a href=\"https://huggingface.co/visheratin/nllb-clip-large-siglip?ref=jina-ai-gmbh.ghost.io\">NLLB-CLIP-SigLIP</a>, un modelo estilo CLIP ligeramente más grande (1.3B, 44% más grande que <code>jina-clip-v2</code>) de última generación que utiliza un codificador de texto pre-entrenado de los modelos NLLB.</p><h3 id=\"english-only-text-and-images\">Texto e Imágenes Solo en Inglés</h3><p>En benchmarks estándar de recuperación cross-modal (Flickr30k y COCO), <code>jina-clip-v2</code> demuestra mejoras importantes en todos los aspectos. Logra un rendimiento estado del arte del 98.0% en recuperación imagen-a-texto en Flickr30k, superando tanto a su predecesor como a NLLB-CLIP-SigLIP. El modelo muestra ganancias consistentes en todos los escenarios de recuperación, con mejoras notables de hasta 3.3% sobre v1 en recuperación imagen-a-texto en COCO, mientras mantiene un rendimiento competitivo con NLLB-CLIP-SigLIP a través de diferentes benchmarks y direcciones de modalidad.</p><p><strong>Rendimiento Recall@5 en Flickr30k:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>98.0</td>\n<td>+1.7%</td>\n<td>+0.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>96.4</td>\n<td>-</td>\n<td>-0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>97.1</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>89.8</td>\n<td>+0.9%</td>\n<td>-2.6%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>89.0</td>\n<td>-</td>\n<td>-3.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>92.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Rendimiento Recall@5 en COCO:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>81.5</td>\n<td>+3.3%</td>\n<td>+2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>78.9</td>\n<td>-</td>\n<td>-0.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>79.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>68.4</td>\n<td>+2.9%</td>\n<td>-3.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>66.5</td>\n<td>-</td>\n<td>-6.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>70.8</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"multilingual-text-and-images\">Texto e Imágenes Multilingües</h3><p>En benchmarks cross-modales multilingües, <code>jina-clip-v2</code> demuestra un rendimiento robusto, destacando particularmente en la recuperación imagen-a-texto donde supera a NLLB-SigLIP en todos los conjuntos de datos, con una mejora de hasta +3.8% en Crossmodal 3600. Aunque NLLB-SigLIP muestra capacidades de recuperación texto-a-imagen ligeramente más fuertes, la brecha de rendimiento permanece pequeña, típicamente dentro del 3%.</p><p><strong>Rendimiento Image2Text Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>83.23</td>\n<td>+3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>80.16</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>86.03</td>\n<td>+0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.37</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.98</td>\n<td>+0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.41</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Rendimiento Text2Image Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>81.43</td>\n<td>-0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>82.07</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>84.87</td>\n<td>-3.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.60</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.03</td>\n<td>-3.0%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.63</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"text-only-dense-retriever-performance\">Rendimiento del Recuperador Denso de Solo Texto</h2><p>Similar a su predecesor, el codificador de texto de <code>jina-clip-v2</code> puede servir como un recuperador multilingüe denso efectivo. En los exhaustivos benchmarks Multilingual MTEB, logra un rendimiento sólido, alcanzando 69.86% en recuperación y 67.77% en tareas de similitud semántica. Estos resultados demuestran su versatilidad, compitiendo eficazmente con nuestro modelo especializado de incrustaciones de texto <code>jina-embeddings-v3</code>:</p><table>\n<thead>\n<tr>\n<th>Tarea</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>69.86</td>\n<td>-3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>72.59</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Semantic Similarity</td>\n<td>jina-clip-v2</td>\n<td>67.77</td>\n<td>-2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>69.81</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>En tareas en inglés, <code>jina-clip-v2</code> muestra mejoras consistentes sobre su predecesor y NLLB-SigLIP, con ventajas particularmente fuertes en rendimiento de recuperación (casi el doble del puntaje de NLLB-SigLIP).</p><table>\n<thead>\n<tr>\n<th>Tarea</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS</td>\n<td>jina-clip-v2</td>\n<td>81.29</td>\n<td>+0.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>80.92</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>74.65</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>49.33</td>\n<td>+2.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>48.33</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>24.92</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"matryoshka-representation-performance\">Rendimiento de la Representación Matryoshka</h2><p>Tanto los codificadores de texto como de imagen son compatibles con MRL, y sus dimensiones de salida pueden truncarse a 64 mientras mantienen un rendimiento sólido. Nuestra evaluación de truncamiento de incrustaciones reveló un notable potencial de compresión. Incluso una reducción dimensional agresiva del 75% mantuvo más del 99% del rendimiento en tareas de texto, imagen y multimodales.</p><h3 id=\"image-classification\">Clasificación de Imágenes</h3><p>A través de 37 diversos benchmarks de clasificación de imágenes, el codificador de imagen muestra una fuerte resistencia a las dimensiones truncadas. La compresión de 1024 a 64 dimensiones (reducción del 94%) resulta en solo una caída del 8% en la precisión top-5 y del 12.5% en top-1, destacando su potencial para una implementación eficiente con una pérdida mínima de rendimiento.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/accuracy_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Para </span><b><strong style=\"white-space: pre-wrap;\">clasificación de imágenes</strong></b><span style=\"white-space: pre-wrap;\">, utilizamos los 19 benchmarks del conjunto de datos </span><a href=\"https://github.com/google-research/task_adaptation?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VTAB</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VOC 2007</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/sun397?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">SUN397</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://cs.stanford.edu/~acoates/stl10/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">STL10</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Rendered SST2</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://objectnet.dev/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ObjectNet</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/cvdfoundation/mnist?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MNIST</span></a><span style=\"white-space: pre-wrap;\">, German Traffic Sign Recognition Benchmark (</span><a href=\"https://benchmark.ini.rub.de/gtsrb_dataset.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">GTSRB</span></a><span style=\"white-space: pre-wrap;\">), Fine-Grained Visual Classification of Aircraft (</span><a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FGVC-Aircraft</span></a><span style=\"white-space: pre-wrap;\">), </span><a href=\"https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FER 2013</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Country211</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/cars196?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Cars196</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/hendrycks/natural-adv-examples?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet-A, ImageNet-O,</span></a><a href=\"https://huggingface.co/datasets/ILSVRC/imagenet-1k?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet1k</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/HaohanWang/ImageNet-Sketch?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet Sketch</span></a><span style=\"white-space: pre-wrap;\"> e </span><a href=\"https://imagenetv2.org/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet v2</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h3 id=\"cross-modal-retrieval\">Recuperación Multimodal</h3><p>A pesar de una dramática reducción del 94% a solo 64 dimensiones, la recuperación multimodal utilizando embeddings truncados tanto de imagen como de texto se mantuvo notablemente robusta, preservando el 93% del rendimiento de imagen a texto y el 90% de texto a imagen.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/crossmodal_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Utilizamos seis benchmarks, tres de los cuales son multilingües: </span><a href=\"https://google.github.io/crossmodal-3600/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Crossmodal-3600</span></a><span style=\"white-space: pre-wrap;\"> (36 idiomas), </span><a href=\"https://shannon.cs.illinois.edu/DenotationGraph/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr30k</span></a><span style=\"white-space: pre-wrap;\"> (solo inglés), </span><a href=\"https://hockenmaier.cs.illinois.edu/8k-pictures.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr8k</span></a><span style=\"white-space: pre-wrap;\"> (solo inglés), </span><a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (solo inglés), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Multilingual MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (10 idiomas), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">XTD 200</span></a><span style=\"white-space: pre-wrap;\"> (27 idiomas)</span></figcaption></figure><h3 id=\"text-only-retrieval\">Recuperación Solo de Texto</h3><p>En los <strong>benchmarks MTEB solo en inglés</strong>, los embeddings de texto de 64 dimensiones (comprimidos desde 1024) preservaron la similitud semántica notablemente bien, cayendo solo un 2.1%, mientras que la recuperación vio una modesta disminución del 17.5%.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/mteb_performance.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"></figure><h2 id=\"getting-started\">Primeros Pasos</h2><h3 id=\"via-api\">Vía API</h3><p>El código demuestra cómo generar embeddings usando <code>requests</code> de Python. Pasa una cadena de texto con una imagen en base64 o URL, más el tamaño de dimensión deseado (por defecto 1024, mostrado como 768 abajo).</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-Python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v2',\n  'encoding_type': 'float',\n  'dimensions': '768' \n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Recuerda reemplazar &lt;YOUR_JINA_AI_API_KEY&gt; con una clave API de Jina activada. Puedes obtener </span><a href=\"https://jina.ai/?sui=apikey&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><span style=\"white-space: pre-wrap;\">una clave API gratuita con un millón de tokens gratis aquí.</span></a></p></figcaption></figure><h3 id=\"image-tokens-pricing\">Precios de Tokens de Imagen</h3><p>Nuestra API cuenta tanto tokens de texto como de imagen. Para las imágenes, el consumo de tokens se basa en el número de mosaicos de 512x512 píxeles necesarios para cubrir toda el área de la imagen. Cada mosaico cuesta 4,000 tokens para procesar, incluyendo mosaicos parcialmente llenos. <strong>Para una mejor eficiencia de costos, recomendamos que los usuarios de la API redimensionen sus imágenes a 512x512 antes de enviar las solicitudes.</strong></p><table>\n<thead>\n<tr>\n<th>Resolución de Imagen</th>\n<th>Mosaicos Requeridos</th>\n<th>Costo en Tokens</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>512x512</td>\n<td>1</td>\n<td>4,000</td>\n</tr>\n<tr>\n<td>720x720</td>\n<td>4</td>\n<td>16,000</td>\n</tr>\n<tr>\n<td>1080x1080</td>\n<td>9</td>\n<td>36,000</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--37-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Para imágenes cuadradas, redimensionar a 512x512 para mejor eficiencia de costos. Para tareas sensibles a la relación de aspecto, escalar el borde más largo a 512, centrar la imagen y rellenar con negro. Para propósitos generales, el redimensionamiento directo a 512x512 funciona bien.</span></figcaption></figure><h3 id=\"via-csp-marketplaces\">A través de Marketplaces CSP</h3><p>Jina CLIP v2 está disponible directamente en AWS, Azure y GCP a los precios listados allí.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-bfbctuqmky676?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina CLIP v2</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-10.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/socialPreview-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-9.ico\" alt=\"\"></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://console.cloud.google.com/marketplace/browse?q=jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Cloud console</div><div class=\"kg-bookmark-description\">Gasta de manera inteligente, adquiere más rápido y rentabiliza el gasto comprometido en Google Cloud con Google Cloud Marketplace. Explora el catálogo de más de 2000 aplicaciones SaaS, VMs, stacks de desarrollo y aplicaciones Kubernetes optimizadas para ejecutarse en Google Cloud.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/default.png\" alt=\"\"></div></div></a></figure><h3 id=\"via-vectordb\"><strong>A través de VectorDB</strong></h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pinecone.io/models/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">La base de datos vectorial para construir IA con conocimiento | Pinecone</div><div class=\"kg-bookmark-description\">Busca entre miles de millones de elementos coincidencias similares para cualquier objeto, en milisegundos. Es la siguiente generación de búsqueda, a una llamada API de distancia.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-3.png\" alt=\"\"><span class=\"kg-bookmark-author\">Pinecone Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/docs_og_image.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://weaviate.io/developers/weaviate/model-providers/jinaai/embeddings-multimodal?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embeddings Multimodales | Weaviate</div><div class=\"kg-bookmark-description\">La integración de Weaviate con las APIs de Jina AI te permite acceder a las capacidades de sus modelos directamente desde Weaviate.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-12.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Weaviate</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/provider_integrations_jinaai.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant es una Base de Datos Vectorial y Motor de Búsqueda Vectorial de código abierto escrito en Rust. Proporciona un servicio de búsqueda de similitud vectorial rápido y escalable con una API conveniente.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-4.png\" alt=\"\"><span class=\"kg-bookmark-author\">edit</span><span class=\"kg-bookmark-publisher\">Qdrant</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-social-preview-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Conclusión</h2><p>Basándonos en nuestro lanzamiento de <code>jina-clip-v1</code> en junio, que extendió el modelo CLIP de OpenAI con entrada de texto de hasta 8.192 tokens, y el innovador <code>jina-embeddings-v3</code> multilingüe, <code>jina-clip-v2</code> trae tres avances principales: soporte multilingüe para 89 idiomas, mayor resolución de imagen a 512x512, y aprendizaje de representación Matryoshka para embeddings más truncados.</p><p>Los modelos tipo CLIP se han establecido como la columna vertebral para aplicaciones multimodales de propósito general. Con <code>jina-clip-v2</code>, estamos llevando estas capacidades al siguiente nivel, derribando barreras lingüísticas para ofrecer una comprensión y recuperación multimodal más precisa. Creemos que este lanzamiento cumple la promesa de hacer que la búsqueda y recuperación multimodal sean más potentes y accesibles para desarrolladores en todo el mundo.</p>",
  "comment_id": "673cc4a7a7c46d00015cf1f5",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/11/clipv2.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-11-19T18:02:31.000+01:00",
  "updated_at": "2024-11-21T17:29:45.000+01:00",
  "published_at": "2024-11-21T17:29:45.000+01:00",
  "custom_excerpt": "Jina-CLIP v2, a 0.9B multimodal embedding model with multilingual support of 89 languages, high image resolution at 512x512, and Matryoshka representations.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/",
  "excerpt": "Jina-CLIP v2, un modelo de embeddings multimodal de 0.9B con soporte multilingüe para 89 idiomas, alta resolución de imagen de 512x512 y representaciones Matryoshka.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
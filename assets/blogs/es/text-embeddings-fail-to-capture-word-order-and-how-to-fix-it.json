{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "Los Text Embeddings fallan al capturar el orden de las palabras y cómo solucionarlo",
  "html": "<p>Recientemente, Christoph Schuhmann, fundador de <a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a> compartió una observación interesante sobre los modelos de embeddings de texto:</p><blockquote>Cuando las palabras dentro de una oración se mezclan aleatoriamente, la similitud coseno entre sus embeddings de texto permanece sorprendentemente alta en comparación con la oración original.</blockquote><p>Por ejemplo, veamos dos oraciones: <code>Berlin is the capital of Germany</code> y <code>the Germany Berlin is capital of</code>. Aunque la segunda oración no tiene sentido, los modelos de embeddings de texto realmente no pueden distinguirlas. Usando <code>jina-embeddings-v3</code>, estas dos oraciones tienen una puntuación de similitud coseno de 0.9295.</p><p>El orden de las palabras no es lo único para lo que los embeddings parecen no ser muy sensibles. Las transformaciones gramaticales pueden cambiar dramáticamente el significado de una oración pero tienen poco impacto en la distancia de embedding. Por ejemplo, <code>She ate dinner before watching the movie</code> y <code>She watched the movie before eating dinner</code> tienen una similitud coseno de 0.9833, a pesar de tener el orden opuesto de acciones.</p><p>La negación también es notoriamente difícil de codificar consistentemente sin <a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">entrenamiento especial</a> — <code>This is a useful model</code> y <code>This is not a useful model</code> se ven prácticamente igual en el espacio de embeddings. A menudo, reemplazar las palabras en un texto con otras de la misma clase, como cambiar \"hoy\" por \"ayer\", o alterar el tiempo verbal, no cambia los embeddings tanto como uno pensaría que debería.</p><p>Esto tiene serias implicaciones. Consideremos dos consultas de búsqueda: <code>Flight from Berlin to Amsterdam</code> y <code>Flight from Amsterdam to Berlin</code>. Tienen embeddings casi idénticos, con <code>jina-embeddings-v3</code> asignándoles una similitud coseno de 0.9884. Para una aplicación del mundo real como búsqueda de viajes o logística, esta deficiencia es fatal.</p><p>En este artículo, analizamos los desafíos que enfrentan los modelos de embeddings, examinando sus persistentes dificultades con el orden y la elección de palabras. Analizamos los principales modos de fallo en categorías lingüísticas —incluyendo contextos direccionales, temporales, causales, comparativos y de negación— mientras exploramos estrategias para mejorar el rendimiento del modelo.</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">¿Por qué las oraciones mezcladas tienen puntuaciones coseno sorprendentemente cercanas?</h2><p>Al principio, pensamos que esto podría deberse a cómo el modelo combina los significados de las palabras - crea un embedding para cada palabra (6-7 palabras en cada una de nuestras oraciones de ejemplo anteriores) y luego promedia estos embeddings juntos con mean pooling. Esto significa que hay muy poca información sobre el orden de las palabras disponible para el embedding final. Un promedio es el mismo sin importar el orden de los valores.</p><p>Sin embargo, incluso los modelos que usan CLS pooling (que mira una primera palabra especial para entender toda la oración y debería ser más sensible al orden de las palabras) tienen el mismo problema. Por ejemplo, <code>bge-1.5-base-en</code> todavía da una puntuación de similitud coseno de 0.9304 para las oraciones <code>Berlin is the capital of Germany</code> y <code>the Germany Berlin is capital of</code>.</p><p>Esto señala una limitación en cómo se entrenan los modelos de embeddings. Mientras que los modelos de lenguaje inicialmente aprenden la estructura de la oración durante el pre-entrenamiento, parecen perder algo de esta comprensión durante el entrenamiento contrastivo — el proceso que usamos para crear modelos de embeddings.</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">¿Cómo impactan la longitud del texto y el orden de las palabras en la similitud de embeddings?</h2><p>¿Por qué los modelos tienen problemas con el orden de las palabras en primer lugar? Lo primero que viene a la mente es la longitud (en tokens) del texto. Cuando el texto se envía a la función de codificación, el modelo primero genera una lista de embeddings de tokens (es decir, cada palabra tokenizada tiene un vector dedicado que representa su significado), luego los promedia.</p><p>Para ver cómo la longitud del texto y el orden de las palabras impactan la similitud de embeddings, generamos un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">conjunto de datos de 180 oraciones sintéticas</a> de diferentes longitudes, como 3, 5, 10, 15, 20 y 30 tokens. También mezclamos aleatoriamente los tokens para formar una variación de cada oración:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Aquí hay algunos ejemplos:</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">Length (tokens)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">Original sentence</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">Shuffled sentence</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Codificaremos el conjunto de datos usando nuestro propio modelo <code>jina-embeddings-v3</code> y el modelo de código abierto <code>bge-base-en-v1.5</code>, luego calcularemos la similitud coseno entre la oración original y la mezclada:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Length (tokens)</th>\n<th>Mean cosine similarity</th>\n<th>Standard deviation in cosine similarity</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.947</td>\n<td>0.053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.909</td>\n<td>0.052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.924</td>\n<td>0.031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.918</td>\n<td>0.019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.899</td>\n<td>0.021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.874</td>\n<td>0.025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ahora podemos generar un diagrama de caja, que hace más clara la tendencia en la similitud coseno:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Distribución de similitud por longitud de oración para oraciones mezcladas con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> y </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (sin ajuste fino)</span></figcaption></figure><p>Como podemos ver, hay una clara relación lineal en la similitud coseno promedio de los embeddings. Cuanto más largo es el texto, menor es la puntuación promedio de similitud coseno entre las oraciones originales y las mezcladas aleatoriamente. Esto probablemente ocurre debido al \"desplazamiento de palabras\", es decir, qué tan lejos se han movido las palabras de sus posiciones originales después de la mezcla aleatoria. En un texto más corto, simplemente hay menos \"espacios\" a los que un token puede ser mezclado por lo que no puede moverse tan lejos, mientras que un texto más largo tiene un mayor número de permutaciones potenciales y las palabras pueden moverse una distancia mayor.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Combinaciones de oraciones por número de palabras</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Detendremos la tabla aquí, ya que el número de combinaciones es el factorial del número de palabras. Cuando llegamos a treinta palabras, obtenemos 265 nonillones (2.652528598 E+32) combinaciones.</div></div><p>Como se muestra en la figura siguiente (Similitud del Coseno vs Desplazamiento Promedio de Palabras), cuanto más largo es el texto, mayor es el desplazamiento de palabras:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Similitud del Coseno vs Desplazamiento Promedio de Palabras con conjunto de datos de oraciones mezcladas mostrando la correlación entre el desplazamiento promedio de palabras y la disimilitud del coseno.</span></figcaption></figure><p>Las incrustaciones de tokens dependen del contexto local, es decir, de las palabras más cercanas a ellos. En un texto corto, reorganizar las palabras no puede cambiar mucho ese contexto. Sin embargo, en un texto más largo, una palabra puede moverse muy lejos de su contexto original y eso puede cambiar significativamente su incrustación de token. Como resultado, mezclar las palabras en un texto más largo produce una incrustación más distante que en uno más corto. La figura anterior muestra que tanto para <code>jina-embeddings-v3</code>, usando agrupación media, como para <code>bge-base-en-v1.5</code>, usando agrupación CLS, se mantiene la misma relación: mezclar textos más largos y desplazar palabras más lejos resulta en puntuaciones de similitud más pequeñas.</p><h2 id=\"do-bigger-models-solve-the-problem\">¿Los Modelos Más Grandes Resuelven el Problema?</h2><p>Normalmente, cuando nos enfrentamos a este tipo de problema, una táctica común es simplemente usar un modelo más grande. Pero, ¿puede un modelo de incrustación de texto más grande realmente capturar mejor la información del orden de las palabras? Según la ley de escalado de los modelos de incrustación de texto (<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">referenciada en nuestra publicación de lanzamiento de <code>jina-embeddings-v3</code></a>), los modelos más grandes generalmente proporcionan un mejor rendimiento:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Ley de Escalado de Modelos de Incrustación, Mostrando el Escalado de Rendimiento MTEB con Número de Parámetros.</span></figcaption></figure><p>Pero ¿puede un modelo más grande capturar la información del orden de las palabras de manera más efectiva? Probamos tres variaciones del modelo BGE: <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>, <a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a>, y <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>, con tamaños de parámetros de 33 millones, 110 millones y 335 millones, respectivamente.</p><p>Usaremos las mismas 180 oraciones que antes, pero ignoraremos la información de longitud. Codificaremos tanto las oraciones originales como sus mezclas aleatorias usando las tres variantes del modelo y graficaremos la similitud del coseno promedio:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Impacto del Tamaño del Modelo en la Sensibilidad al Orden de las Palabras con Conjunto de Datos de Oraciones Mezcladas usando </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, y </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Aunque podemos ver que los modelos más grandes son más sensibles a la variación del orden de las palabras, la diferencia es pequeña. Incluso el mucho más grande <code>bge-large-en-v1.5</code> es apenas un poco mejor para distinguir las oraciones mezcladas de las no mezcladas. Otros factores entran en juego para determinar qué tan sensible es un modelo de incrustación a las reordenaciones de palabras, particularmente las diferencias en el régimen de entrenamiento. Además, la similitud del coseno es una herramienta muy limitada para medir la capacidad de un modelo para hacer distinciones. Sin embargo, podemos ver que el tamaño del modelo no es una consideración importante. No podemos simplemente hacer nuestro modelo más grande y resolver este problema.</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">Orden de Palabras y Elección de Palabras en el Mundo Real</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Durante gran parte de esta publicación estamos usando <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v2</code></a> (<i><em class=\"italic\" style=\"white-space: pre-wrap;\">no</em></i> nuestro modelo más reciente, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>) ya que <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v2</code> es mucho más pequeño y por lo tanto más rápido para experimentar en nuestras GPUs locales, con 137m parámetros frente a los 580m de <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v3</code>.</div></div><p>Como mencionamos en la introducción, el orden de las palabras no es el único desafío para los modelos de incrustación. Un desafío más realista del mundo real es sobre la <em>elección</em> de palabras. Hay muchas formas de cambiar las palabras en una oración — formas que no se reflejan bien en las incrustaciones. Podemos tomar \"Ella voló de París a Tokio\" y alterarla para obtener \"Ella condujo de Tokio a París\", y las incrustaciones permanecen similares. Hemos mapeado esto a través de varias categorías de alteración:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categoría</th>\n<th>Ejemplo - Izquierda</th>\n<th>Ejemplo - Derecha</th>\n<th>Similitud del Coseno (<code>jina</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Direccional</td>\n<td>Ella voló de París a Tokio</td>\n<td>Ella condujo de Tokio a París</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td>Ella cenó antes de ver la película</td>\n<td>Ella vio la película antes de cenar</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>Causal</td>\n<td>La temperatura en aumento derritió la nieve</td>\n<td>La nieve derretida enfrió la temperatura</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>Comparativa</td>\n<td>El café sabe mejor que el té</td>\n<td>El té sabe mejor que el café</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>Negación</td>\n<td>Él está de pie junto a la mesa</td>\n<td>Él está de pie lejos de la mesa</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Ten en cuenta que estos son casos comunes que observamos durante nuestro trabajo y no necesariamente representan una taxonomía completa de categorías.</div></div><p>La tabla anterior muestra una lista de \"casos de fallo\" donde un modelo de embeddings de texto falla al capturar alteraciones sutiles de palabras. Esto coincide con nuestras expectativas: los modelos de embeddings de texto carecen de la capacidad de razonar. Por ejemplo, el modelo no comprende la relación entre \"desde\" y \"hacia\". Los modelos de embeddings de texto realizan coincidencias semánticas, con semántica típicamente capturada a nivel de token y luego comprimida en un único vector denso después del pooling. En contraste, <a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">los LLM (modelos autorregresivos) entrenados en conjuntos de datos más grandes, a escala de billones de tokens, están comenzando a demostrar capacidades emergentes de razonamiento</a>.</p><p>Esto nos hizo preguntarnos, ¿podemos ajustar el modelo de embeddings con aprendizaje contrastivo usando tripletas para acercar la consulta y el positivo, mientras alejamos la consulta del negativo?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: El Efecto del Aprendizaje Contrastivo: Acercar la Consulta y el Positivo, Alejar el Negativo.</span></figcaption></figure><p>Por ejemplo, \"Vuelo desde Ámsterdam a Berlín\" podría considerarse el par negativo de \"Vuelo desde Berlín a Ámsterdam\". De hecho, en el <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\">informe técnico de <code>jina-embeddings-v1</code></a> (Michael Guenther, et al.), abordamos brevemente este problema a pequeña escala: ajustamos el modelo <code>jina-embeddings-v1</code> en un <a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">conjunto de datos de negación</a> de 10.000 ejemplos generados por modelos de lenguaje grandes.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Los resultados, reportados en el enlace del informe anterior, fueron prometedores:</p><blockquote>Observamos que en todos los tamaños de modelo, el ajuste fino en datos de tripletas (que incluye nuestro conjunto de datos de entrenamiento de negación) mejora dramáticamente el rendimiento, particularmente en la tarea HardNegation.</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Tabla mostrando puntajes de EasyNegation y HardNegation en varios tamaños de modelos </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> con entrenamiento por pares y combinado de tripletas/pares.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Comparación de Rendimiento de Estrategias de Entrenamiento entre diferentes versiones de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">Ajuste Fino de Modelos de Embeddings de Texto con Conjuntos de Datos Curados</h2><p>En las secciones anteriores, exploramos varias observaciones clave sobre los embeddings de texto:</p><ol><li>Los textos más cortos son más propensos a errores en la captura del orden de las palabras.</li><li>Aumentar el tamaño del modelo de embeddings de texto no necesariamente mejora la comprensión del orden de las palabras.</li><li>El aprendizaje contrastivo podría ofrecer una solución potencial a estos problemas.</li></ol><p>Con esto en mente, ajustamos <code>jina-embeddings-v2-base-en</code> y <code>bge-base-en-1.5</code> en nuestros conjuntos de datos de negación y orden de palabras (aproximadamente 11.000 muestras de entrenamiento en total):</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Para ayudar a evaluar el ajuste fino, generamos un <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">conjunto de datos</a> de 1.000 tripletas que consisten en un caso <code>query</code>, <code>positive (pos)</code>, y <code>negative (neg)</code>:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Aquí hay una fila de ejemplo:</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Estas tripletas están diseñadas para cubrir varios casos de fallo, incluyendo cambios de significado <strong>direccionales</strong>, <strong>temporales</strong> y <strong>causales</strong> debido a cambios en el orden de las palabras.</p><p>Ahora podemos evaluar los modelos en tres conjuntos de evaluación diferentes:</p><ol><li>El conjunto de 180 oraciones sintéticas (del principio de este post), mezcladas aleatoriamente.</li><li>Cinco ejemplos revisados manualmente (de la tabla direccional/causal/etc anterior).</li><li>94 tripletas curadas de nuestro <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">conjunto de datos de tripletas</a> que acabamos de generar.</li></ol><p>Aquí está la diferencia para oraciones mezcladas antes y después del ajuste fino:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Longitud de Oración (tokens)</th>\n<th>Similitud Coseno Media (<code>jina</code>)</th>\n<th>Similitud Coseno Media (<code>jina-ft</code>)</th>\n<th>Similitud Coseno Media (<code>bge</code>)</th>\n<th>Similitud Coseno Media (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>El resultado parece claro: a pesar de que el proceso de fine-tuning solo tomó cinco minutos, vemos una mejora dramática en el rendimiento en el conjunto de datos de oraciones aleatorias:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Distribución de Similitud por Longitud de Oración para Oraciones Mezcladas con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> y </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (ajustado).</span></figcaption></figure><p>También vemos mejoras en los casos direccionales, temporales, causales y comparativos. El modelo muestra una mejora sustancial en el rendimiento reflejada por una caída en la similitud coseno promedio. La mayor ganancia de rendimiento se da en el caso de negación, debido a que nuestro conjunto de datos de fine-tuning contiene 10,000 ejemplos de entrenamiento de negación.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categoría</th>\n<th>Ejemplo - Izquierda</th>\n<th>Ejemplo - Derecha</th>\n<th>Similitud Coseno Media (<code>jina</code>)</th>\n<th>Similitud Coseno Media (<code>jina-ft</code>)</th>\n<th>Similitud Coseno Media (<code>bge</code>)</th>\n<th>Similitud Coseno Media (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Direccional</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>Causal</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>Comparativo</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>Negación</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Conclusión</h2><p>En esta publicación, profundizamos en los desafíos que enfrentan los modelos de embedding de texto, especialmente su dificultad para manejar efectivamente el orden de las palabras. Para desglosarlo, hemos identificado cinco tipos principales de fallas: <strong>Direccional</strong>, <strong>Temporal</strong>, <strong>Causal</strong>, <strong>Comparativo</strong> y <strong>Negación</strong>. Estos son los tipos de consultas donde el orden de las palabras realmente importa, y si tu caso de uso involucra alguno de estos, vale la pena conocer las limitaciones de estos modelos.</p><p>También realizamos un experimento rápido, expandiendo un conjunto de datos centrado en la negación para cubrir las cinco categorías de fallas. Los resultados fueron prometedores: el fine-tuning con \"negativos difíciles\" cuidadosamente seleccionados mejoró la capacidad del modelo para reconocer qué elementos pertenecen juntos y cuáles no. Dicho esto, aún hay más trabajo por hacer. Los próximos pasos incluyen profundizar en cómo el tamaño y la calidad del conjunto de datos afectan el rendimiento.</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "Los modelos de incrustación de texto tienen dificultades para captar sutiles matices lingüísticos como el orden de las palabras, las relaciones direccionales, las secuencias temporales, las conexiones causales, las comparaciones y la negación. Comprender estos desafíos es clave para mejorar el rendimiento del modelo.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
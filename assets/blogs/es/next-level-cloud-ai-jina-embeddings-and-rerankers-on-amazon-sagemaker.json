{
  "slug": "next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker",
  "id": "65fabb91502fd000011c667e",
  "uuid": "45cd5187-838d-46b7-a8a0-d890fcda9041",
  "title": "IA en la Nube de Siguiente Nivel: Jina Embeddings y Rerankers en Amazon SageMaker",
  "html": "<p><a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> y <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io\">Jina Reranker</a> están ahora disponibles para su uso con <a href=\"https://aws.amazon.com/pm/sagemaker/?ref=jina-ai-gmbh.ghost.io\">Amazon SageMaker</a> desde el <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>. Para los usuarios empresariales que valoran altamente la seguridad, fiabilidad y consistencia en sus operaciones en la nube, esto pone la IA de vanguardia de Jina AI en sus despliegues privados de AWS, donde disfrutan de todos los beneficios de la infraestructura establecida y estable de AWS.</p><p>Con nuestra gama completa de modelos de embeddings y reranking en AWS Marketplace, los usuarios de SageMaker pueden aprovechar las revolucionarias ventanas de contexto de entrada de 8k y los embeddings multilingües de primera categoría bajo demanda a precios competitivos. No tiene que pagar por transferir modelos hacia o desde AWS, los precios son transparentes y su facturación está integrada con su cuenta de AWS.</p><p>Los modelos disponibles actualmente en Amazon SageMaker incluyen:</p><ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6w6k6ckusixpw?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Small - English</a></li><li>Modelos Bilingües Jina Embeddings v2:<ul><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-dz3ubvmivnwry?ref=jina-ai-gmbh.ghost.io\">Alemán/Inglés</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-hxalozh37jka4?ref=jina-ai-gmbh.ghost.io\">Chino/Inglés</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-rnb324fpie3n6?ref=jina-ai-gmbh.ghost.io\">Español/Inglés</a></li></ul></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-tk7t7bz6fp5ng?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 Base - Code</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\">Jina Reranker v1 Base - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-6kxbf5xqrluf4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT v1 - English</a></li><li><a href=\"https://aws.amazon.com/marketplace/pp/prodview-mgomngrh4c4k4?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina ColBERT Reranker v1 - English</a></li></ul><p>Para ver la lista completa de modelos, visite la <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">página de vendedor de Jina AI en AWS Marketplace</a>, y aproveche una prueba gratuita de siete días.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Este artículo le guiará a través de la creación de una aplicación de <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\">Generación aumentada por recuperación</a> (RAG) utilizando exclusivamente componentes de Amazon SageMaker. Los modelos que usaremos son <strong>Jina Embeddings v2 - English</strong>, <strong>Jina Reranker v1</strong>, y el modelo de lenguaje grande <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\">Mistral-7B-Instruct</a>.</p><p>También puede seguir con un Notebook de Python, que puede <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">descargar</a> o <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/embeddings/sagemaker/sagemaker.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ejecutar en Google Colab</a>.</p><h2 id=\"retrieval-augmented-generation\">Generación Aumentada por Recuperación</h2><p>La generación aumentada por recuperación es un paradigma alternativo en IA generativa. En lugar de usar modelos de lenguaje grandes (LLMs) para responder directamente a las solicitudes de los usuarios con lo que ha aprendido en el entrenamiento, aprovecha su fluida producción de lenguaje mientras reubica la lógica y la recuperación de información en un aparato externo más adecuado para ello.</p><p>Antes de invocar un LLM, los sistemas RAG recuperan activamente información relevante de alguna fuente de datos externa y luego la alimentan al LLM como parte de su prompt. El papel del LLM es sintetizar la información externa en una respuesta coherente a las solicitudes del usuario, minimizando el riesgo de alucinación y aumentando la relevancia y utilidad del resultado.</p><p>Un sistema RAG esquemáticamente tiene al menos cuatro componentes:</p><ul><li>Una fuente de datos, típicamente una base de datos vectorial de algún tipo, adecuada para la recuperación de información asistida por IA.</li><li>Un sistema de recuperación de información que trata la solicitud del usuario como una consulta y recupera datos relevantes para responderla.</li><li>Un sistema, a menudo incluyendo un reranker basado en IA, que selecciona algunos de los datos recuperados y los procesa en un prompt para un LLM.</li><li>Un LLM, por ejemplo, uno de los modelos GPT o un LLM de código abierto como el de Mistral, que toma la solicitud del usuario y los datos proporcionados y genera una respuesta para el usuario.</li></ul><p>Los modelos de embedding son adecuados para la recuperación de información y se utilizan frecuentemente para ese propósito. Un modelo de embedding de texto toma textos como entradas y produce un <a href=\"https://jina.ai/news/how-embeddings-drive-ai-a-guide?ref=jina-ai-gmbh.ghost.io\">embedding</a> — un vector de alta dimensión — cuya relación espacial con otros embeddings indica su similitud semántica, es decir, temas similares, contenidos y significados relacionados. Se utilizan a menudo en la recuperación de información porque cuanto más cercanos están los embeddings, más probable es que el usuario esté satisfecho con la respuesta. También son relativamente fáciles de afinar para mejorar su rendimiento en dominios específicos.</p><p>Los modelos de <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">reranking de texto</a> utilizan principios de IA similares para comparar colecciones de textos con una consulta y ordenarlos por su similitud semántica. Usar un modelo de reranker específico para la tarea, en lugar de confiar solo en un modelo de embedding, a menudo aumenta dramáticamente la precisión de los resultados de búsqueda. El reranker en una aplicación RAG selecciona algunos de los resultados de la recuperación de información para maximizar la probabilidad de que la información correcta esté en el prompt para el LLM.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Maximizing Search Relevance and RAG Accuracy with Jina Reranker</div><div class=\"kg-bookmark-description\">Boost your search and RAG accuracy with Jina Reranker. Our new model improves the accuracy and relevance by 20% over simple vector search. Try it now for free!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Reranker1.png\" alt=\"\"></div></a></figure><h2 id=\"benchmarking-performance-of-embedding-models-as-sagemaker-endpoints\"><strong>Evaluación del Rendimiento de los Modelos de Embedding como Endpoints de SageMaker</strong></h2><p>Probamos el rendimiento y la fiabilidad del modelo <strong>Jina Embeddings v2 Base - English</strong> como endpoint de SageMaker, ejecutándose en una instancia <a href=\"https://aws.amazon.com/ec2/instance-types/g4/?ref=jina-ai-gmbh.ghost.io\">g4dn.xlarge</a>. En estos experimentos, generamos continuamente un nuevo usuario cada segundo, cada uno de los cuales enviaba una solicitud, esperaba su respuesta y repetía al recibirla.</p><ul><li>Para solicitudes de <em>menos de 100 tokens</em>, para hasta 150 usuarios concurrentes, los tiempos de respuesta <em>por solicitud</em> se mantuvieron por debajo de 100ms. Luego, los tiempos de respuesta aumentaron linealmente de 100ms a 1500ms con la generación de más usuarios concurrentes.<ul><li>Con aproximadamente <em>300 usuarios concurrentes</em>, recibimos más de 5 fallos de la API y terminamos la prueba.</li></ul></li><li>Para solicitudes entre 1K y 8K tokens, para hasta 20 usuarios concurrentes, los tiempos de respuesta <em>por solicitud</em> se mantuvieron por debajo de 8s. Luego, los tiempos de respuesta aumentaron linealmente de 8s a 60s con la generación de más usuarios concurrentes.<ul><li>Con aproximadamente <em>140 usuarios concurrentes</em>, recibimos más de 5 fallos de la API y terminamos la prueba.</li></ul></li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png\" class=\"kg-image\" alt=\"Four comparative graphs displaying &quot;Small Context&quot; versus &quot;Large Context&quot; results over time, assessing performance metrics.\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/03/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/image-3.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Rendimiento durante las pruebas (izquierda: contexto pequeño, derecha: contexto grande), mostrando el efecto del aumento de usuarios a lo largo del tiempo en los tiempos de respuesta y tasas de fallo. </span></figcaption></figure><p>Basándonos en estos resultados, podemos concluir que para la mayoría de los usuarios con cargas de trabajo de embeddings normales, las instancias g4dn.xlarge o g5.xlarge deberían satisfacer sus necesidades diarias. Sin embargo, para trabajos de <em>indexación</em> grandes, que típicamente se ejecutan con mucha menos frecuencia que las tareas de <em>búsqueda</em>, los usuarios podrían preferir una opción más potente. Para una lista de todas las instancias Sagemaker disponibles, consulte la descripción general de <a href=\"https://aws.amazon.com/ec2/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">EC2</a> de AWS.</p><h2 id=\"configure-your-aws-account\">Configura tu cuenta de AWS</h2><p>Primero, necesitarás tener una cuenta de AWS. Si aún no eres usuario de AWS, puedes <a href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\">registrarte</a> para obtener una cuenta en el sitio web de AWS.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://portal.aws.amazon.com/billing/signup?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Console - Signup</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://portal.aws.amazon.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Signup</span></div></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">No podrás completar este tutorial con una cuenta de Free Tier porque Amazon no proporciona acceso gratuito a SageMaker. Debes agregar un método de pago a la cuenta para suscribirte a los modelos de Jina AI, incluso si usas <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">nuestra prueba gratuita de siete días</a>.</div></div><h3 id=\"set-up-aws-tools-in-your-python-environment\">Configura las herramientas de AWS en tu entorno Python</h3><p>Instala en tu entorno Python las herramientas y bibliotecas de AWS necesarias para este tutorial:</p><pre><code class=\"language-bash\">pip install awscli jina-sagemaker\n</code></pre><p>Necesitarás obtener una clave de acceso y una clave de acceso secreta para tu cuenta de AWS. Para hacerlo, sigue las <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\">instrucciones en el sitio web de AWS</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Managing access keys for IAM users - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Create, modify, view, or update access keys (credentials) for programmatic calls to AWS.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/security-credentials-user.shared.console.png\" alt=\"\"></div></a></figure><p>También necesitarás elegir una <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\">región de AWS</a> para trabajar.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Regions, Availability Zones, and Local Zones - Amazon Relational Database Service</div><div class=\"kg-bookmark-description\">Learn how Amazon cloud computing resources are hosted in multiple locations world-wide, including AWS Regions and Availability Zones.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Relational Database Service</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.aws.amazon.com/images/AmazonRDS/latest/UserGuide/images/Con-AZ-Local.png\" alt=\"\"></div></a></figure><p>Luego, establece los valores en variables de entorno. En Python o en un notebook de Python, puedes hacerlo con el siguiente código:</p><pre><code class=\"language-bash\">import os\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = &lt;YOUR_ACCESS_KEY_ID&gt;\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = &lt;YOUR_SECRET_ACCESS_KEY&gt;\nos.environ[\"AWS_DEFAULT_REGION\"] = &lt;YOUR_AWS_REGION&gt;\nos.environ[\"AWS_DEFAULT_OUTPUT\"] = \"json\"\n</code></pre><p>Establece la salida predeterminada como <code>json</code>.</p><p>También puedes hacer esto a través de la <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html?ref=jina-ai-gmbh.ghost.io\">aplicación de línea de comandos de AWS</a> o configurando un <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html?ref=jina-ai-gmbh.ghost.io\">archivo de configuración de AWS</a> en tu sistema de archivos local. Consulta la <a href=\"https://docs.aws.amazon.com/index.html?ref=jina-ai-gmbh.ghost.io\">documentación en el sitio web de AWS</a> para más detalles.</p><h3 id=\"create-a-role\">Crea un Rol</h3><p>También necesitarás un <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\">rol de AWS</a> con permisos suficientes para usar los recursos requeridos para este tutorial.</p><p>Este rol debe:</p><ol><li>Tener habilitado <strong>AmazonSageMakerFullAccess</strong>.</li><li>Ya sea:<ol><li>Tener autoridad para hacer suscripciones en AWS Marketplace y tener habilitados los tres:<ol><li><strong>aws-marketplace:ViewSubscriptions</strong></li><li><strong>aws-marketplace:Unsubscribe</strong></li><li><strong>aws-marketplace:Subscribe</strong></li></ol></li><li>O tu cuenta de AWS debe tener una suscripción a <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">jina-embedding-model</a>.</li></ol></li></ol><p>Almacena el ARN (<a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html?ref=jina-ai-gmbh.ghost.io\">Amazon Resource Name</a>) del rol en la variable <code>role</code>:</p><pre><code class=\"language-python\">role = &lt;YOUR_ROLE_ARN&gt;\n</code></pre><p>Consulta la documentación sobre roles en el sitio web de AWS para más información.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">IAM roles - AWS Identity and Access Management</div><div class=\"kg-bookmark-description\">Learn how and when to use IAM roles.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.aws.amazon.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">AWS Identity and Access Management</span></div></div></a></figure><h3 id=\"subscribe-to-jina-ai-models-on-aws-marketplace\">Suscríbete a los modelos de Jina AI en AWS Marketplace</h3><p>En este artículo, usaremos el modelo Jina Embeddings v2 base English. Suscríbete a él en el <a href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\">AWS Marketplace</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-5iljbegvoi66w?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Embeddings v2 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Verás la información de precios desplazándote hacia abajo en la página. AWS cobra por hora por los modelos del marketplace, por lo que se te facturará por el tiempo desde que inicias el endpoint del modelo hasta que lo detienes. Este artículo te mostrará cómo hacer ambas cosas.</p><p>También usaremos el modelo Jina Reranker v1 - English, al que necesitarás suscribirte.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-avmxk2wxbygd6?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Reranker v1 Base - en</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">en</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-text\">Jina AI está ofreciendo actualmente una prueba gratuita de siete días de sus modelos. Aún deberás pagar por las instancias de AWS que los ejecuten, pero durante el período de prueba, no tendrás que pagar adicionalmente por los modelos.</div></div><p>Cuando te hayas suscrito a ellos, obtén los ARN de los modelos para tu región de AWS y guárdalos en las variables <code>embedding_package_arn</code> y <code>reranker_package_arn</code> respectivamente. El código en este tutorial hará referencia a ellos usando esos nombres de variables.</p><p>Si no sabes cómo obtener los ARN, coloca el nombre de tu región de Amazon en la variable <code>region</code> y usa el siguiente código:</p><pre><code class=\"language-python\">region = os.environ[\"AWS_DEFAULT_REGION\"]\n\ndef get_arn_for_model(region_name, model_name):\n    model_package_map = {\n        \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:253352124568:model-package/{model_name}\",\n        \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{model_name}\",\n        \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{model_name}\",\n        \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{model_name}\",\n        \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{model_name}\",\n        \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{model_name}\",\n        \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{model_name}\",\n        \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{model_name}\",\n        \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{model_name}\",\n        \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{model_name}\",\n        \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{model_name}\",\n        \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{model_name}\",\n        \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{model_name}\",\n        \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{model_name}\",\n        \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{model_name}\",\n        \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{model_name}\",\n    }\n\n    return model_package_map[region_name]\n\nembedding_package_arn = get_arn_for_model(region, \"jina-embeddings-v2-base-en\")\nreranker_package_arn = get_arn_for_model(region, \"jina-reranker-v1-base-en\")\n</code></pre><h2 id=\"load-the-dataset\">Cargar el Dataset</h2><p>En este tutorial, vamos a utilizar una colección de videos proporcionados por el canal de YouTube <a href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\">TU Delft Online Learning</a>. Este canal produce una variedad de materiales educativos en materias STEM. Su programación está bajo <a href=\"https://creativecommons.org/licenses/by/3.0/legalcode?ref=jina-ai-gmbh.ghost.io\">licencia CC-BY</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/@tudelftonlinelearning1226?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">TU Delft Online Learning</div><div class=\"kg-bookmark-description\">Are you looking to make your career in science, design or engineering? Then join the community of online learners at TU Delft!\nAt TU Delft, online learning means active learning. Our courses are designed to provide you with an engaging learning experience. Course content is challenging and demanding, promoting your personal growth and professional development, while enjoying the flexibility and accessibility that our online courses offers so you can combine learning with other priorities of your life. Start learning today: https://online-learning.tud…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/s/desktop/4feff1e2/img/favicon_144x144.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://yt3.googleusercontent.com/ytc/AIdro_kH5d18Xqqj-MKv9k_tf2KNFufCpMY8qEXdQzEy=s900-c-k-c0x00ffffff-no-rj\" alt=\"\"></div></a></figure><p>Descargamos 193 videos del canal y los procesamos con el modelo de reconocimiento de voz <a href=\"https://openai.com/research/whisper?ref=jina-ai-gmbh.ghost.io\">Whisper de OpenAI</a> de código abierto. Utilizamos el modelo más pequeño <a href=\"https://huggingface.co/openai/whisper-tiny?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>openai/whisper-tiny</code></a> para procesar los videos en transcripciones.</p><p>Las transcripciones se han organizado en un archivo CSV, que puedes <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/embeddings/sagemaker/tu_delft.csv?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">descargar desde aquí</a>.</p><p>Cada fila del archivo contiene:</p><ul><li>El título del video</li><li>La URL del video en YouTube</li><li>Una transcripción del texto del video</li></ul><p>Para cargar estos datos en Python, primero instala <code>pandas</code> y <code>requests</code>:</p><pre><code class=\"language-bash\">pip install requests pandas\n</code></pre><p>Carga los datos CSV directamente en un DataFrame de Pandas llamado <code>tu_delft_dataframe</code>:</p><pre><code class=\"language-python\">import pandas\n\n# Load the CSV file\ntu_delft_dataframe = pandas.read_csv(\"https://raw.githubusercontent.com/jina-ai/workshops/feat-sagemaker-post/notebooks/embeddings/sagemaker/tu_delft.csv\")\n</code></pre><p>Puedes inspeccionar el contenido usando el método <code>head()</code> del DataFrame. En un notebook, debería verse algo así:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png\" class=\"kg-image\" alt=\"Data frame mostrando títulos de webinars como &quot;Green Teams in Hospitals,&quot; con sus URLs de YouTube y extractos de texto introductorios,\" loading=\"lazy\" width=\"1440\" height=\"580\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/03/Screenshot-2024-03-15-at-14.30.35.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/03/Screenshot-2024-03-15-at-14.30.35.png 1440w\" sizes=\"(min-width: 720px) 720px\"></figure><p>También puedes ver los videos usando las URLs proporcionadas en este dataset y verificar que el reconocimiento de voz es imperfecto pero básicamente correcto.</p><h2 id=\"start-the-jina-embeddings-v2-endpoint\">Iniciar el Endpoint de Jina Embeddings v2</h2><p>El código siguiente lanzará una instancia de <code>ml.g4dn.xlarge</code> en AWS para ejecutar el modelo de embedding. Esto puede tardar varios minutos en completarse.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Choose a name for your embedding endpoint. It can be anything convenient.\nembeddings_endpoint_name = \"jina_embedding\"\n\nembedding_client = Client(region_name=boto3.Session().region_name)\nembedding_client.create_endpoint(\n    arn=embedding_package_arn,\n    role=role,\n    endpoint_name=embeddings_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nembedding_client.connect_to_endpoint(endpoint_name=embeddings_endpoint_name)\n</code></pre><p>Cambia el <code>instance_type</code> para seleccionar un tipo diferente de instancia en la nube de AWS si es apropiado.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">AWS te facturará por tu tiempo comenzando tan pronto como este comando retorne. Se te facturará por hora hasta que detengas esta instancia. Para hacerlo, sigue las instrucciones en la sección <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Apagando</strong></b></a>.</div></div><h2 id=\"build-and-index-the-dataset\">Construir e Indexar el Dataset</h2><p>Ahora que hemos cargado los datos y estamos ejecutando un modelo Jina Embeddings v2, podemos preparar e indexar los datos. Almacenaremos los datos en un <a href=\"https://faiss.ai/index.html?ref=jina-ai-gmbh.ghost.io\">almacén de vectores FAISS</a>, una base de datos de vectores de código abierto diseñada específicamente para aplicaciones de IA.</p><p>Primero, instala los requisitos restantes para nuestra aplicación RAG:</p><pre><code class=\"language-bash\">pip install tdqm numpy faiss-cpu\n</code></pre><h3 id=\"chunking\">Fragmentación</h3><p>Necesitaremos tomar las transcripciones individuales y dividirlas en partes más pequeñas, es decir, \"fragmentos\", para que podamos ajustar múltiples textos en un prompt para el LLM. El código siguiente divide las transcripciones individuales en los límites de las oraciones, asegurando que todos los fragmentos no tengan más de 128 palabras por defecto.</p><pre><code class=\"language-python\">def chunk_text(text, max_words=128):\n    \"\"\"\n    Divide text into chunks where each chunk contains the maximum number \n    of full sentences with fewer words than `max_words`.\n    \"\"\"\n    sentences = text.split(\".\")\n    chunk = []\n    word_count = 0\n\n    for sentence in sentences:\n        sentence = sentence.strip(\".\")\n        if not sentence:\n          continue\n\n        words_in_sentence = len(sentence.split())\n        if word_count + words_in_sentence &lt;= max_words:\n            chunk.append(sentence)\n            word_count += words_in_sentence\n        else:\n            # Yield the current chunk and start a new one\n            if chunk:\n              yield \". \".join(chunk).strip() + \".\"\n            chunk = [sentence]\n            word_count = words_in_sentence\n\n    # Yield the last chunk if it's not empty\n    if chunk:\n        yield \" \".join(chunk).strip() + \".\"</code></pre><h3 id=\"get-embeddings-for-each-chunk\">Obtener Embeddings para Cada Fragmento</h3><p>Necesitamos un embedding para cada fragmento para almacenarlo en la base de datos FAISS. Para obtenerlos, pasamos los fragmentos de texto al endpoint del modelo de embedding de Jina AI, usando el método <code>embedding_client.embed()</code>. Luego, agregamos los fragmentos de texto y los vectores de embedding al dataframe de pandas <code>tu_delft_dataframe</code> como las nuevas columnas <code>chunks</code> y <code>embeddings</code>:</p><pre><code class=\"language-python\">import numpy as np\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\ndef generate_embeddings(text_df):\n    chunks = list(chunk_text(text_df[\"Text\"]))\n    embeddings = []\n\n    for i, chunk in enumerate(chunks):\n      response = embedding_client.embed(texts=[chunk])\n      chunk_embedding = response[0][\"embedding\"]\n      embeddings.append(np.array(chunk_embedding))\n\n    text_df[\"chunks\"] = chunks\n    text_df[\"embeddings\"] = embeddings\n    return text_df\n\nprint(\"Embedding text chunks ...\")\ntu_delft_dataframe = generate_embeddings(tu_delft_dataframe)\n## si estás usando Google Colab o un notebook de Python, puedes \n## eliminar la línea anterior y descomentar la siguiente línea:\n# tu_delft_dataframe = tu_delft_dataframe.progress_apply(generate_embeddings, axis=1)\n</code></pre><h3 id=\"set-up-semantic-search-using-faiss\">Configurar Búsqueda Semántica Usando Faiss</h3><p>El código siguiente crea una base de datos FAISS e inserta los fragmentos y vectores de embedding iterando sobre <code>tu_delft_pandas</code>:</p><pre><code class=\"language-python\">import faiss\n\ndim = 768  # dimensión de los embeddings de Jina v2\nindex_with_ids = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\nk = 0\n\ndoc_ref = dict()\n\nfor idx, row in tu_delft_dataframe.iterrows():\n    embeddings = row[\"embeddings\"]\n    for i, embedding in enumerate(embeddings):\n        normalized_embedding = np.ascontiguousarray(np.array(embedding, dtype=\"float32\").reshape(1, -1))\n        faiss.normalize_L2(normalized_embedding)\n        index_with_ids.add_with_ids(normalized_embedding, k)\n        doc_ref[k] = (row[\"chunks\"][i], idx)\n        k += 1\n</code></pre><h2 id=\"start-the-jina-reranker-v1-endpoint\">Iniciar el Endpoint de Jina Reranker v1</h2><p>Al igual que con el modelo Jina Embedding v2 anterior, este código lanzará una instancia de <code>ml.g4dn.xlarge</code> en AWS para ejecutar el modelo reranker. De manera similar, puede tardar varios minutos en ejecutarse.</p><pre><code class=\"language-python\">import boto3\nfrom jina_sagemaker import Client\n\n# Elige un nombre para tu endpoint reranker. Puede ser cualquier nombre conveniente.\nreranker_endpoint_name = \"jina_reranker\"\n\nreranker_client = Client(region_name=boto3.Session().region_name)\nreranker_client.create_endpoint(\n    arn=reranker_package_arn,\n    role=role,\n    endpoint_name=reranker_endpoint_name,\n    instance_type=\"ml.g4dn.xlarge\",\n    n_instances=1,\n)\n\nreranker_client.connect_to_endpoint(endpoint_name=reranker_endpoint_name)\n</code></pre><h2 id=\"define-query-functions\">Definir Funciones de Consulta</h2><p>A continuación, definiremos una función que identifica los fragmentos de transcripción más similares a cualquier consulta de texto.</p><p>Este es un proceso de dos pasos:</p><ol><li>Convertir la entrada del usuario en un vector de embedding usando el método <code>embedding_client.embed()</code>, igual que hicimos en la etapa de preparación de datos.</li><li>Pasar el embedding al índice FAISS para recuperar las mejores coincidencias. En la función siguiente, el valor predeterminado es devolver las 20 mejores coincidencias, pero puedes controlarlo con el parámetro <code>n</code>.</li></ol><p>La función <code>find_most_similar_transcript_segment</code> devolverá las mejores coincidencias comparando los cosenos de los embeddings almacenados con el embedding de la consulta.</p><pre><code class=\"language-python\">def find_most_similar_transcript_segment(query, n=20):\n    query_embedding = embedding_client.embed(texts=[query])[0][\"embedding\"]  # Asumiendo que la consulta es lo suficientemente corta para no necesitar fragmentación\n    query_embedding = np.ascontiguousarray(np.array(query_embedding, dtype=\"float32\").reshape(1, -1))\n    faiss.normalize_L2(query_embedding)\n\n    D, I = index_with_ids.search(query_embedding, n)  # Obtener las n mejores coincidencias\n\n    results = []\n    for i in range(n):\n        distance = D[0][i]\n        index_id = I[0][i]\n        transcript_segment, doc_idx = doc_ref[index_id]\n        results.append((transcript_segment, doc_idx, distance))\n\n    # Ordenar los resultados por distancia\n    results.sort(key=lambda x: x[2])\n\n    return [(tu_delft_dataframe.iloc[r[1]][\"Title\"].strip(), r[0]) for r in results]\n</code></pre><p>También definiremos una función que accede al endpoint reranker <code>reranker_client</code>, le pasa los resultados de <code>find_most_similar_transcript_segment</code> y devuelve solo los tres resultados más relevantes. Llama al endpoint reranker con el método <code>reranker_client.rerank()</code>.</p><pre><code class=\"language-python\">def rerank_results(query_found, query, n=3):\n    ret = reranker_client.rerank(\n        documents=[f[1] for f in query_found], \n        query=query, \n        top_n=n,\n    )\n    return [query_found[r['index']] for r in ret[0]['results']]\n</code></pre><h2 id=\"use-jumpstart-to-load-mistral-instruct\">Usar JumpStart para Cargar Mistral-Instruct</h2><p>Para este tutorial, usaremos el modelo <code>mistral-7b-instruct</code>, que está <a href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\">disponible a través de Amazon SageMaker JumpStart</a>, como la parte LLM del sistema RAG.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/blogs/machine-learning/mistral-7b-foundation-models-from-mistral-ai-are-now-available-in-amazon-sagemaker-jumpstart/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Los modelos base Mistral 7B de Mistral AI ya están disponibles en Amazon SageMaker JumpStart | Amazon Web Services</div><div class=\"kg-bookmark-description\">Hoy, nos complace anunciar que los modelos base Mistral 7B, desarrollados por Mistral AI, están disponibles para los clientes a través de Amazon SageMaker JumpStart para implementarlos con un solo clic para ejecutar inferencias. Con 7 mil millones de parámetros, Mistral 7B se puede personalizar fácilmente e implementar rápidamente. Puedes probar este modelo con SageMaker JumpStart, un [...]</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://a0.awsstatic.com/main/images/site/touch-icon-ipad-144-smile.png\" alt=\"\"><span class=\"kg-bookmark-author\">Amazon Web Services</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/10/09/mistral-7b-sagemaker-jumpstart.jpg\" alt=\"\"></div></a></figure><p>Ejecuta el siguiente código para cargar e implementar Mistral-Instruct:</p><pre><code class=\"language-python\">from sagemaker.jumpstart.model import JumpStartModel\n\njumpstart_model = JumpStartModel(model_id=\"huggingface-llm-mistral-7b-instruct\", role=role)\nmodel_predictor = jumpstart_model.deploy()\n</code></pre><p>El endpoint para acceder a este LLM se almacena en la variable <code>model_predictor</code>.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">El uso de este modelo también es un servicio facturable para AWS, así que no olvides apagarlo cuando termines con este tutorial. Consulta la sección <a href=\"#shutting-down\" rel=\"noreferrer\"><b><strong style=\"white-space: pre-wrap;\">Apagando</strong></b></a> para detener esta implementación cuando hayas terminado.</div></div><h3 id=\"mistral-instruct-with-jumpstart\">Mistral-Instruct con JumpStart</h3><p>A continuación se muestra el código para crear una plantilla de prompt para Mistral-Instruct para esta aplicación usando <a href=\"https://docs.python.org/3/library/string.html?ref=jina-ai-gmbh.ghost.io#template-strings\">la clase template de cadenas incorporada de Python</a>. Asume que para cada consulta hay tres fragmentos de transcripción coincidentes que se presentarán al modelo.</p><p>Puedes experimentar con esta plantilla por tu cuenta para modificar esta aplicación o ver si puedes obtener mejores resultados.</p><pre><code class=\"language-python\">from string import Template\n\nprompt_template = Template(\"\"\"\n  &lt;s&gt;[INST] Responde la pregunta siguiente usando solo el contexto proporcionado.\n  La pregunta del usuario está basada en transcripciones de videos de un canal\n    de YouTube.\n  El contexto se presenta como una lista ordenada de información en forma de\n    (título-del-video, segmento-de-transcripción), que es relevante para responder\n    la pregunta del usuario.\n  La respuesta debe usar solo el contexto presentado. Si la pregunta no puede\n    responderse basándose en el contexto, dilo.\n\n  Contexto:\n  1. Título-del-video: $title_1, segmento-de-transcripción: $segment_1\n  2. Título-del-video: $title_2, segmento-de-transcripción: $segment_2\n  3. Título-del-video: $title_3, segmento-de-transcripción: $segment_3\n\n  Pregunta: $question\n\n  Respuesta: [/INST]\n\"\"\")\n</code></pre><p>Con este componente en su lugar, ahora tenemos todas las partes de una aplicación RAG completa.</p><h2 id=\"querying-the-model\">Consultando el Modelo</h2><p>Consultar el modelo es un proceso de tres pasos.</p><ol><li>Buscar fragmentos relevantes dada una consulta.</li><li>Ensamblar el prompt.</li><li>Enviar el prompt al modelo Mistral-Instruct y devolver su respuesta.</li></ol><p>Para buscar fragmentos relevantes, usamos la función <code>find_most_similar_transcript_segment</code> que definimos anteriormente.</p><pre><code class=\"language-python\">question = \"¿Cuándo se puso en servicio el primer parque eólico marino?\"\nsearch_results = find_most_similar_transcript_segment(question)\nreranked_results = rerank_results(search_results, question)\n</code></pre><p>Puedes inspeccionar los resultados de búsqueda en orden reordenado:</p><pre><code class=\"language-python\">for title, text, _ in reranked_results:\n    print(title + \"\\n\" + text + \"\\n\")\n</code></pre><p>Resultado:</p><pre><code class=\"language-text\">Offshore Wind Farm Technology - Course Introduction\nSince the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n\nOffshore Wind Farm Technology - Course Introduction\nI'm a researcher and lecturer at the Wind Energy and Economics Department and I will be your moderator throughout this course.  That means I will answer any questions you may have.  I'll strengthen the interactions between the participants and also I'll get you in touch with the lecturers when needed.  The course is mainly developed for professionals in the field of offshore wind energy.  We want to broaden their knowledge of the relevant technical disciplines and their integration.  Professionals with a scientific background who are new to the field of offshore wind energy will benefit from a high-level insight into the engineering aspects of wind energy.  Overall, the course will help you make the right choices during the development and operation of offshore wind farms.\n\nOffshore Wind Farm Technology - Course Introduction\nDesigned wind turbines that better withstand wind, wave and current loads  Identify great integration strategies for offshore wind turbines and gain understanding of the operational and maintenance of offshore wind turbines and farms  We also hope that you will benefit from the course and from interaction with other learners who share your interest in wind energy  And therefore we look forward to meeting you online.\n</code></pre><p>Podemos usar esta información directamente en la plantilla del prompt:</p><pre><code class=\"language-python\">prompt_for_llm = prompt_template.substitute(\n    question = question,\n    title_1 = search_results[0][0],\n    segment_1 = search_results[0][1],\n    title_2 = search_results[1][0],\n    segment_2 = search_results[1][1],\n    title_3 = search_results[2][0],\n    segment_3 = search_results[2][1],\n)\n</code></pre><p>Imprime la cadena resultante para ver qué prompt se envía realmente al LLM:</p><pre><code class=\"language-python\">print(prompt_for_llm)\n</code></pre><pre><code class=\"language-text\">&lt;s&gt;[INST] Answer the question below only using the given context.\n  The question from the user is based on transcripts of videos from a YouTube\n    channel.\n  The context is presented as a ranked list of information in the form of\n    (video-title, transcript-segment), that is relevant for answering the\n    user's question.\n  The answer should only use the presented context. If the question cannot be\n    answered based on the context, say so.\n\n  Context:\n  1. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, scientists and engineers have adapted and improved the technology of wind energy to offshore conditions.  This is a rapidly evolving field with installation of increasingly larger wind turbines in deeper waters.  At sea, the challenges are indeed numerous, with combined wind and wave loads, reduced accessibility and uncertain-solid conditions.  My name is Axel Vire, I'm an assistant professor in Wind Energy at U-Delf and specializing in offshore wind energy.  This course will touch upon the critical aspect of wind energy, how to integrate the various engineering disciplines involved in offshore wind energy.  Each week we will focus on a particular discipline and use it to design and operate a wind farm.\n  2. Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: For example, we look at how to characterize the wind and wave conditions at a given location.  How to best place the wind turbines in a farm and also how to retrieve the electricity back to shore.  We look at the main design drivers for offshore wind turbines and their components.  We'll see how these aspects influence one another and the best choices to reduce the cost of energy.  This course is organized by the two-delfd wind energy institute, an interfaculty research organization focusing specifically on wind energy.  You will therefore benefit from the expertise of the lecturers in three different faculties of the university.  Aerospace engineering, civil engineering and electrical engineering.  Hi, my name is Ricardo Pareda.\n  3. Video-title: Systems Analysis for Problem Structuring part 1B the mono actor perspective example, transcript-segment: So let's assume the demarcation of the problem and the analysis of objectives has led to the identification of three criteria.  The security of supply, the percentage of offshore power generation and the costs of energy provision.  We now reason backwards to explore what factors have an influence on these system outcomes.  Really, the offshore percentage is positively influenced by the installed Wind Power capacity at sea, a key system factor.  Capacity at sea in turn is determined by both the size and the number of wind farms at sea.  The Ministry of Economic Affairs cannot itself invest in new wind farms but hopes to simulate investors and energy companies by providing subsidies and by expediting the granting process of licenses as needed.\n\n  Question: When was the first offshore wind farm commissioned?\n\n  Answer: [/INST]\n</code></pre><p>Pasa este prompt al endpoint del LLM — <code>model_predictor</code> — a través del método <code>model_predictor.predict()</code>:</p><pre><code class=\"language-python\">answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n</code></pre><p>Esto devuelve una lista, pero como solo pasamos un prompt, será una lista con una entrada. Cada entrada es un <code>dict</code> con el texto de respuesta bajo la clave <code>generated_text</code>:</p><pre><code class=\"language-python\">answer = answer[0]['generated_text']\nprint(answer)\n</code></pre><p>Resultado:</p><pre><code class=\"language-text\">The first offshore wind farm was commissioned in 1991. (Context: Video-title: Offshore Wind Farm Technology - Course Introduction, transcript-segment: Since the first offshore wind farm commissioned in 1991 in Denmark, ...)\n</code></pre><p>Vamos a simplificar las consultas escribiendo una función que haga todos los pasos: tomando la pregunta como cadena de texto como parámetro y devolviendo la respuesta como cadena:</p><pre><code class=\"language-python\">def ask_rag(question):\n    search_results = find_most_similar_transcript_segment(question)\n    reranked_results = rerank_results(search_results, question)\n    prompt_for_llm = prompt_template.substitute(\n        question = question,\n        title_1 = search_results[0][0],\n        segment_1 = search_results[0][1],\n        title_2 = search_results[1][0],\n        segment_2 = search_results[1][1],\n        title_3 = search_results[2][0],\n        segment_3 = search_results[2][1],\n    )\n    answer = model_predictor.predict({\"inputs\": prompt_for_llm})\n    return answer[0][\"generated_text\"]\n</code></pre><p>Ahora podemos hacer algunas preguntas más. Las respuestas dependerán del contenido de las transcripciones de video. Por ejemplo, podemos hacer preguntas detalladas cuando la respuesta está presente en los datos y obtener una respuesta:</p><pre><code class=\"language-python\">ask_rag(\"What is a Kaplan Meyer estimator?\")\n</code></pre><pre><code class=\"language-text\">The Kaplan Meyer estimator is a non-parametric estimator for the survival \nfunction, defined for both censored and not censored data. It is represented \nas a series of declining horizontal steps that approaches the truths of the \nsurvival function if the sample size is sufficiently large enough. The value \nof the empirical survival function obtained is assumed to be constant between \ntwo successive distinct observations.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"Who is Reneville Solingen?\")\n</code></pre><pre><code class=\"language-text\">Reneville Solingen is a professor at Delft University of Technology in Global \nSoftware Engineering. She is also a co-author of the book \"The Power of Scrum.\"\n</code></pre><pre><code class=\"language-python\">answer = ask_rag(\"What is the European Green Deal?\")\nprint(answer)\n</code></pre><pre><code class=\"language-text\">The European Green Deal is a policy initiative by the European Union to combat \nclimate change and decarbonize the economy, with a goal to make Europe carbon \nneutral by 2050. It involves the use of green procurement strategies in various \nsectors, including healthcare, to reduce carbon emissions and promote corporate \nsocial responsibility.\n</code></pre><p>También podemos hacer preguntas que están fuera del alcance de la información disponible:</p><pre><code class=\"language-python\">ask_rag(\"What countries export the most coffee?\")\n</code></pre><pre><code class=\"language-text\">Based on the context provided, there is no clear answer to the user's \nquestion about which countries export the most coffee as the context \nonly discusses the Delft University's cafeteria discounts and sustainable \ncoffee options, as well as lithium production and alternatives for use in \nelectric car batteries.\n</code></pre><pre><code class=\"language-python\">ask_rag(\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\")\n</code></pre><pre><code class=\"language-text\">The context does not provide sufficient information to answer the question. \nThe context is about thermit welding of rails, stress concentration factors, \nand a lyrics video. There is no mention of woodchucks or the ability of \nwoodchuck to chuck wood in the context.\n</code></pre><p>Prueba tus propias consultas. También puedes cambiar la forma en que se hace el prompt al LLM para ver si eso mejora tus resultados.</p><h2 id=\"shutting-down\">Apagado</h2><p>Debido a que se te factura por hora por los modelos que usas y por la infraestructura de AWS para ejecutarlos, es muy importante detener los tres modelos de IA cuando termines este tutorial:</p><ul><li>El endpoint del modelo de embedding <code>embedding_client</code></li><li>El endpoint del modelo de reranking <code>reranker_client</code></li><li>El endpoint del modelo de lenguaje grande <code>model_predictor</code></li></ul><p>Para apagar los tres endpoints de modelos, ejecuta el siguiente código:</p><pre><code class=\"language-python\"># shut down the embedding endpoint\nembedding_client.delete_endpoint()\nembedding_client.close()\n# shut down the reranker endpoint\nreranker_client.delete_endpoint()\nreranker_client.close()\n# shut down the LLM endpoint\nmodel_predictor.delete_model()\nmodel_predictor.delete_endpoint()\n</code></pre><h2 id=\"get-started-now-with-jina-ai-models-on-aws-marketplace\">Empieza ahora con los modelos de Jina AI en AWS Marketplace</h2><p>Con nuestros modelos de embedding y reranking en SageMaker, los usuarios empresariales de IA en AWS ahora tienen acceso instantáneo a la excepcional propuesta de valor de Jina AI sin comprometer los beneficios de sus operaciones en la nube existentes. Toda la seguridad, confiabilidad, consistencia y precios predecibles de AWS vienen incluidos.</p><p>En Jina AI, trabajamos arduamente para llevar el estado del arte a las empresas que pueden beneficiarse de incorporar IA en sus procesos existentes. Nos esforzamos por ofrecer modelos sólidos, confiables y de alto rendimiento a precios accesibles a través de interfaces convenientes y prácticas, minimizando sus inversiones en IA mientras maximiza sus retornos.</p><p>Visita <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">la página de Jina AI en AWS Marketplace</a> para ver una lista de todos los modelos de embeddings y reranking que ofrecemos y para probar nuestros modelos gratis durante siete días.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina AI</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\"></div></a></figure><p>Nos encantaría conocer sus casos de uso y hablar sobre cómo los productos de Jina AI pueden adaptarse a las necesidades de su negocio. Contáctenos a través de <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\">nuestro sitio web</a> o nuestro <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\">canal de Discord</a> para compartir sus comentarios y mantenerse al día con nuestros últimos modelos.</p>",
  "comment_id": "65fabb91502fd000011c667e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/03/Blog-images--27-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-03-20T11:33:53.000+01:00",
  "updated_at": "2024-03-25T19:10:29.000+01:00",
  "published_at": "2024-03-25T16:00:51.000+01:00",
  "custom_excerpt": "Learn to use Jina Embeddings and Reranking models in a full-stack AI application on AWS, using only components available in Amazon SageMaker and the AWS Marketplace.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/next-level-cloud-ai-jina-embeddings-and-rerankers-on-amazon-sagemaker/",
  "excerpt": "Aprende a usar los modelos de Jina Embeddings y Reranking en una aplicación de IA full-stack en AWS, utilizando únicamente componentes disponibles en Amazon SageMaker y AWS Marketplace.",
  "reading_time": 21,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract image with colorful wavy background featuring AWS, Embeddings, and Reranker logos.",
  "feature_image_caption": null
}
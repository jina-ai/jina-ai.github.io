{
  "slug": "ai-explainability-made-easy-how-late-interaction-makes-jina-colbert-transparent",
  "id": "6672af263ce1950001eed6a7",
  "uuid": "44371108-082d-4fb0-a28d-4f86fc02ac14",
  "title": "Explicabilidad de la IA simplificada: Cómo la interacción tardía hace transparente a Jina-ColBERT",
  "html": "<p>Uno de los problemas persistentes de los modelos de IA es que las redes neuronales no explican cómo producen sus resultados. No siempre está claro cuánto es esto un problema real para la inteligencia artificial. Cuando pedimos a los humanos que expliquen su razonamiento, habitualmente racionalizan, típicamente sin ser conscientes de que lo están haciendo, dando las explicaciones más plausibles sobre sí mismos sin ninguna indicación de lo que realmente está sucediendo en sus mentes.</p><p>Ya sabemos cómo hacer que los modelos de IA inventen respuestas plausibles. Quizás la inteligencia artificial es más parecida a los humanos en ese aspecto de lo que nos gustaría admitir.</p><p>Hace cincuenta años, el filósofo estadounidense Thomas Nagel escribió un influyente ensayo llamado <em>¿Qué se siente ser un murciélago?</em> Sostenía que debe haber algo que se siente al ser un murciélago: Ver el mundo como lo ve un murciélago y percibir la existencia como lo hace un murciélago. Sin embargo, según Nagel, incluso si conociéramos cada hecho cognoscible sobre cómo funcionan los cerebros, sentidos y cuerpos de los murciélagos, aún no sabríamos qué se siente ser un murciélago.</p><p>La explicabilidad de la IA es el mismo tipo de problema. Conocemos cada hecho que hay que saber sobre un modelo de IA dado. Es solo una gran cantidad de números de precisión finita organizados en una secuencia de matrices. Podemos verificar trivialmente que cada salida del modelo es el resultado de una aritmética correcta, pero esa información es inútil como explicación.</p><p>No hay una solución general para este problema en la IA más que la que hay para los humanos. Sin embargo, la arquitectura ColBERT, y particularmente cómo utiliza la \"interacción tardía\" cuando se usa como reordenador, te permite obtener perspectivas significativas de tus modelos sobre por qué da resultados específicos en casos particulares.</p><p>Este artículo te muestra cómo la interacción tardía permite la explicabilidad, usando el modelo Jina-ColBERT <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><code>jina-colbert-v1-en</code></a> y la <a href=\"https://matplotlib.org/?ref=jina-ai-gmbh.ghost.io\">biblioteca Python Matplotlib</a>.</p><h2 id=\"a-brief-overview-of-colbert\">Una Breve Descripción de ColBERT</h2><p>ColBERT fue introducido en <a href=\"https://doi.org/10.1145/3397271.3401075?ref=jina-ai-gmbh.ghost.io\">Khattab & Zaharia (2020)</a> como una extensión del <a href=\"https://doi.org/10.18653/v1/N19-1423?ref=jina-ai-gmbh.ghost.io\">modelo BERT presentado inicialmente en 2018</a> por Google. Los modelos <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/?ref=jina-ai-gmbh.ghost.io\">Jina-ColBERT de Jina AI</a> se basan en este trabajo y en la arquitectura posterior ColBERT v2 propuesta en <a href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\">Santhanam, et al. (2021)</a>. Los modelos tipo ColBERT pueden usarse para crear embeddings, pero tienen algunas características adicionales cuando se usan como modelo de reordenamiento. El principal beneficio es la <em>interacción tardía</em>, que es una forma de estructurar el problema de similitud semántica de texto diferente a los modelos de embedding estándar.</p><h3 id=\"embedding-models\">Modelos de Embedding</h3><p>En un modelo de embedding tradicional, comparamos dos textos generando vectores representativos para ellos llamados <em>embeddings</em>, y luego comparamos esos embeddings mediante métricas de distancia como coseno o distancia de Hamming. Cuantificar la similitud semántica de dos textos generalmente sigue un procedimiento común.</p><p>Primero, creamos embeddings para los dos textos por separado. Para cada texto:</p><ol><li>Un tokenizador divide el texto en fragmentos aproximadamente del tamaño de palabras.</li><li>Cada token se mapea a un vector.</li><li>Los vectores de tokens interactúan a través del sistema de atención y capas de convolución, añadiendo información de contexto a la representación de cada token.</li><li>Una capa de pooling transforma estos vectores de tokens modificados en un único vector de embedding.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings_pooling_dark_small-1.png\" class=\"kg-image\" alt=\"Diagram of text classification model with convolutional, attention, pooling layers, and text tokens on a black background.\" loading=\"lazy\" width=\"550\" height=\"900\"><figcaption><span style=\"white-space: pre-wrap;\">Un modelo de embedding esquematizado que crea un único embedding para un texto.</span></figcaption></figure><p>Luego, cuando hay un embedding para cada texto, los comparamos entre sí, típicamente usando la métrica del coseno o la distancia de Hamming.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings2_simpler_dark_small.png\" class=\"kg-image\" alt=\"Flowchart describing a text similarity calculation process with tokenization, embedding models, and scoring.\" loading=\"lazy\" width=\"775\" height=\"825\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Embeddings2_simpler_dark_small.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Embeddings2_simpler_dark_small.png 775w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">En un modelo de embedding convencional, los documentos se comparan comparando directamente sus embeddings.</span></figcaption></figure><p>La puntuación ocurre comparando los dos embeddings completos entre sí, sin ninguna información específica sobre los tokens. Toda la interacción entre tokens es \"temprana\" ya que ocurre antes de que los dos textos se comparen entre sí.</p><h3 id=\"reranking-models\">Modelos de Reordenamiento</h3><p>Los modelos de reordenamiento funcionan de manera diferente.</p><p>Primero, en lugar de crear un embedding para cualquier texto, toma un texto, llamado <em>consulta</em>, y una colección de otros textos que llamaremos <em>documentos objetivo</em> y luego puntúa cada documento objetivo con respecto al texto de consulta. Estos números no están normalizados y no son como comparar embeddings, pero se pueden ordenar. Los documentos objetivo que obtienen la puntuación más alta con respecto a la consulta son los textos que están más relacionados semánticamente con la consulta según el modelo.</p><p>Veamos cómo funciona esto concretamente con el modelo reordenador <code>jina-colbert-v1-en</code>, usando la API de Jina Reranker y Python.</p><p>El código siguiente también está en un notebook que puedes <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io\">descargar</a> o <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io\">ejecutar en Google Colab</a>.</p><p>Primero deberías instalar la versión más reciente de la biblioteca <code>requests</code> en tu entorno Python. Puedes hacerlo con el siguiente comando:</p><pre><code class=\"language-bash\">pip install requests -U\n</code></pre><p>A continuación, visita la <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\">página de la API de Jina Reranker</a> y obtén un token API gratuito, válido para hasta un millón de tokens de procesamiento de texto. Copia la clave del token API de la parte inferior de la página, como se muestra a continuación:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/jina_reranker_api.png\" class=\"kg-image\" alt=\"Screenshot of Reranker API's interface with explanatory text and red-highlighted code segment for search optimization.\" loading=\"lazy\" width=\"1650\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/jina_reranker_api.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/jina_reranker_api.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/jina_reranker_api.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/jina_reranker_api.png 1650w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Cómo obtener tu clave API personal desde la página de la API de Jina Reranker.</span></figcaption></figure><p>Usaremos el siguiente texto de consulta:</p><ul><li>\"Elephants eat 150 kg of food per day.\"</li></ul><p>Y compararemos esta consulta con tres textos:</p><ul><li>\"Elephants eat 150 kg of food per day.\"</li><li>\"Every day, the average elephant consumes roughly 150 kg of plants.\"</li><li>\"The rain in Spain falls mainly on the plain.\"</li></ul><p>El primer documento es idéntico a la consulta, el segundo es una reformulación del primero, y el último texto es completamente no relacionado.</p><p>Usa el siguiente código Python para obtener las puntuaciones, asignando tu token API de Jina Reranker a la variable <code>jina_api_key</code>:</p><pre><code class=\"language-Python\">import requests\n\nurl = \"&lt;https://api.jina.ai/v1/rerank&gt;\"\njina_api_key = \"&lt;YOUR JINA RERANKER API TOKEN HERE&gt;\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {jina_api_key}\"\n}\ndata = {\n    \"model\": \"jina-colbert-v1-en\",\n    \"query\": \"Elephants eat 150 kg of food per day.\",\n    \"documents\": [\n        \"Elephants eat 150 kg of food per day.\",\n        \"Every day, the average elephant consumes roughly 150 kg of food.\",\n        \"The rain in Spain falls mainly on the plain.\",\n    ],\n    \"top_n\": 3\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nfor item in response.json()['results']:\n    print(f\"{item['relevance_score']} : {item['document']['text']}\")\n</code></pre><p>Ejecutar este código desde un archivo Python o en un notebook debería producir el siguiente resultado:</p><pre><code class=\"language-Text\">11.15625 : Elephants eat 150 kg of food per day.\n9.6328125 : Every day, the average elephant consumes roughly 150 kg of food.\n1.568359375 : The rain in Spain falls mainly on the plain.\n</code></pre><p>La coincidencia exacta tiene la puntuación más alta, como esperaríamos, mientras que la reformulación tiene la segunda más alta, y un texto completamente no relacionado tiene una puntuación mucho más baja.</p><h3 id=\"scoring-using-colbert\">Puntuación usando ColBERT</h3><p>Lo que hace que el reordenamiento ColBERT sea diferente de la puntuación basada en embeddings es que los tokens de los dos textos se comparan entre sí durante el proceso de puntuación. Los dos textos nunca tienen sus propios embeddings.</p><p>Primero, usamos la misma arquitectura que los modelos de embedding para crear nuevas representaciones para cada token que incluyen información de contexto del texto. Luego, comparamos cada token de la consulta con cada token del documento.</p><p>Para cada token en la consulta, identificamos el token en el documento que tiene la interacción más fuerte con él, y sumamos esos puntajes de interacción para calcular un valor numérico final.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/ColBERT_dual_dark_small.png\" class=\"kg-image\" alt=\"Detailed diagram showing computational model with tokens, scored and categorized into &quot;Early&quot; and &quot;Late&quot; interactions.\" loading=\"lazy\" width=\"1325\" height=\"1200\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/ColBERT_dual_dark_small.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/ColBERT_dual_dark_small.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/ColBERT_dual_dark_small.png 1325w\" sizes=\"(min-width: 1200px) 1200px\"></figure><p>Esta interacción es \"tardía\": los tokens interactúan entre los dos textos cuando los comparamos entre sí. Pero recuerda, la interacción \"tardía\" no excluye la interacción \"temprana\". Los pares de vectores de tokens que se comparan ya contienen información sobre sus contextos específicos.</p><p>Este esquema de interacción tardía preserva la información a nivel de token, incluso si esa información es específica del contexto. Esto nos permite ver, en parte, cómo el modelo ColBERT calcula su puntuación porque podemos identificar qué pares de tokens contextualizados contribuyen a la puntuación final.</p><h2 id=\"explaining-rankings-with-heat-maps\">Explicando los Rankings con Mapas de Calor</h2><p>Los mapas de calor son una técnica de visualización útil para ver qué está sucediendo en Jina-ColBERT cuando crea puntuaciones. En esta sección, usaremos las bibliotecas <a href=\"https://seaborn.pydata.org/?ref=jina-ai-gmbh.ghost.io\"><code>seaborn</code></a> y <a href=\"https://matplotlib.org/?ref=jina-ai-gmbh.ghost.io\"><code>matplotlib</code></a> para crear mapas de calor desde la capa de interacción tardía de <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><code>jina-colbert-v1-en</code></a>, mostrando cómo los tokens de consulta interactúan con cada token del texto objetivo.</p><h3 id=\"set-up\">Configuración</h3><p>Hemos creado un archivo de biblioteca Python que contiene el código para acceder al modelo <code>jina-colbert-v1-en</code> y usar <code>seaborn</code>, <code>matplotlib</code> y <code>Pillow</code> para crear mapas de calor. Puedes <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">descargar esta biblioteca directamente desde GitHub</a>, o <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">usar el notebook proporcionado</a> en tu propio sistema, o en <a href=\"https://colab.research.google.com/github/jina-ai/workshops/blob/main/notebooks/heatmaps/colbert_heatmaps.ipynb?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Google Colab</a>.</p><p>Primero, instala los requisitos. Necesitarás la última versión de la biblioteca <code>requests</code> en tu entorno Python. Así que, si aún no lo has hecho, ejecuta:</p><pre><code class=\"language-bash\">pip install requests -U \n</code></pre><p>Luego, instala las bibliotecas principales:</p><pre><code class=\"language-bash\">pip install matplotlib seaborn torch Pillow\n</code></pre><p>A continuación, descarga <code>jina_colbert_heatmaps.py</code> desde GitHub. Puedes hacerlo <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">a través de un navegador web</a> o desde la línea de comandos si tienes instalado <code>wget</code>:</p><pre><code class=\"language-bash\">wget https://raw.githubusercontent.com/jina-ai/workshops/main/notebooks/heatmaps/jina_colbert_heatmaps.py\n</code></pre><p>Con las bibliotecas instaladas, solo necesitamos declarar una función para el resto de este artículo:</p><pre><code class=\"language-Python\">from jina_colbert_heatmaps import JinaColbertHeatmapMaker\n\ndef create_heatmap(query, document, figsize=None):\n    heat_map_maker = JinaColbertHeatmapMaker(jina_api_key=jina_api_key)\n    # get token embeddings for the query\n    query_emb = heat_map_maker.embed(query, is_query=True)\n    # get token embeddings for the target document\n    document_emb = heat_map_maker.embed(document, is_query=False)\n    return heat_map_maker.compute_heatmap(document_emb[0], query_emb[0], figsize)\n</code></pre><h3 id=\"results\">Resultados</h3><p>Ahora que podemos crear mapas de calor, hagamos algunos y veamos qué nos dicen.</p><p>Ejecuta el siguiente comando en Python:</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", \"Elephants eat 150 kg of food per day.\")</code></pre><p>El resultado será un mapa de calor que se verá así:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--68-.png\" class=\"kg-image\" alt=\"Heatmap visualizing relationships between phrases like &quot;elephants eat 150 kg of food per day&quot; with color gradients indicating\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--68-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--68-.png 640w\"></figure><p>Este es un mapa de calor de los niveles de activación entre pares de tokens cuando comparamos dos textos idénticos. Cada cuadrado muestra la interacción entre dos tokens, uno de cada texto. Los tokens adicionales <code>[CLS]</code> y <code>[SEP]</code> indican el principio y el final del texto respectivamente, y <code>q</code> y <code>d</code> se insertan justo después del token <code>[CLS]</code> en consultas y documentos objetivo respectivamente. Esto permite que el modelo tenga en cuenta las interacciones entre tokens y el principio y final de los textos, pero también permite que las representaciones de tokens sean sensibles a si están en consultas u objetivos.</p><p>Cuanto más brillante sea el cuadrado, más interacción hay entre los dos tokens, lo que indica que están semánticamente relacionados. La puntuación de interacción de cada par de tokens está en el rango de -1.0 a 1.0. Los cuadrados resaltados por un marco rojo son los que cuentan para la puntuación final: Para cada token en la consulta, su nivel de interacción más alto con cualquier token del documento es el valor que cuenta.</p><p>Las mejores coincidencias — los puntos más brillantes — y los valores máximos enmarcados en rojo están casi todos exactamente en la diagonal, y tienen una interacción muy fuerte. Las únicas excepciones son los tokens \"técnicos\" <code>[CLS]</code>, <code>q</code>, y <code>d</code>, así como la palabra \"of\" que es una \"palabra vacía\" de alta frecuencia en inglés que lleva muy poca información independiente.</p><p>Tomemos una oración estructuralmente similar — \"Cats eat 50 g of food per day.\" — y veamos cómo interactúan los tokens:</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", \"Cats eat 50 g of food per day.\")</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/download.png\" class=\"kg-image\" alt=\"Heatmap visualizing the relevance of keywords like &quot;elephants&quot;, &quot;food&quot;, and &quot;kg&quot; with varying intensity colors, indicating da\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/download.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/download.png 640w\"></figure><p>Una vez más, las mejores coincidencias están principalmente en la diagonal porque las palabras son frecuentemente las mismas y la estructura de la oración es casi idéntica. Incluso \"cats\" y \"elephants\" coinciden, debido a sus contextos comunes, aunque no muy bien.</p><p>Cuanto menos similar es el contexto, peor es la coincidencia. Considera el texto \"Employees eat at the company canteen.\"</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", \"Employees eat at the company canteen.\")</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--69-.png\" class=\"kg-image\" alt=\"Heatmap visualization showing word correlations from news articles, including topics like food, elephants, and work environme\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--69-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--69-.png 640w\"></figure><p>Aunque estructuralmente similar, la única coincidencia fuerte aquí es entre las dos instancias de \"eat\". Temáticamente, estas son oraciones muy diferentes, incluso si sus estructuras son altamente paralelas.</p><p>Observando la oscuridad de los colores en los cuadrados enmarcados en rojo, podemos ver cómo el modelo las clasificaría como coincidencias para \"Elephants eat 150 kg of food per day\", y <code>jina-colbert-v1-en</code> confirma esta intuición:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>Text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>11.15625</td>\n<td>Elephants eat 150 kg of food per day.</td>\n</tr>\n<tr>\n<td>8.3671875</td>\n<td>Cats eat 50 g of food per day.</td>\n</tr>\n<tr>\n<td>3.734375</td>\n<td>Employees eat at the company canteen.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ahora, comparemos \"Elephants eat 150 kg of food per day.\" con una oración que tiene esencialmente el mismo significado pero una formulación diferente: \"Every day, the average elephant consumes roughly 150 kg of food.\"</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", \"Every day, the average elephant consumes roughly 150 kg of food.\")</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--70-.png\" class=\"kg-image\" alt=\"Colorful heatmap visualizing the relationship between elephant consumption metrics and other variables.\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--70-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--70-.png 640w\"></figure><p>Observe la fuerte interacción entre \"eat\" en la primera oración y \"consume\" en la segunda. La diferencia en el vocabulario no impide que Jina-ColBERT reconozca el significado común.</p><p>Además, \"every day\" coincide fuertemente con \"per day\", aunque estén en lugares completamente diferentes. Solo la palabra de bajo valor \"of\" es una coincidencia anómala.</p><p>Ahora, comparemos la misma consulta con un texto totalmente no relacionado: \"The rain in Spain falls mainly on the plain.\"</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", \"The rain in Spain falls mainly on the plain.\")</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/download-1.png\" class=\"kg-card kg-image\" alt=\"Mapa de calor Seaborn que visualiza las frecuencias de discusiones de temas durante meses, sombreado de rojo a azul oscuro.\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/download-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/download-1.png 640w\"></figure><p>Puede ver que las interacciones de \"mejor coincidencia\" tienen una puntuación mucho más baja para este par, y hay muy poca interacción entre las palabras de los dos textos. Intuitivamente, esperaríamos que obtuviera una puntuación baja en comparación con \"Every day, the average elephant consumes roughly 150 kg of food\", y <code>jina-colbert-v1-en</code> está de acuerdo:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Score</th>\n<th>Text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>9.6328125</td>\n<td>Every day, the average elephant consumes roughly 150 kg of food.</td>\n</tr>\n<tr>\n<td>1.568359375</td>\n<td>The rain in Spain falls mainly on the plain.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"long-texts\">Textos largos</h3><p>Estos son ejemplos sencillos para demostrar el funcionamiento de los modelos de reclasificación estilo ColBERT. En contextos de recuperación de información, como la generación aumentada por recuperación, las consultas tienden a ser textos cortos mientras que los documentos candidatos a coincidir tienden a ser más largos, a menudo tan largos como la ventana de contexto de entrada del modelo.</p><p>Los modelos Jina-ColBERT admiten contextos de entrada de 8192 tokens, equivalentes a aproximadamente 16 páginas estándar de texto a espacio simple.</p><p>También podemos generar mapas de calor para estos casos asimétricos. Por ejemplo, tomemos la primera sección de la <a href=\"https://en.wikipedia.org/wiki/Indian_elephant?ref=jina-ai-gmbh.ghost.io\">página de Wikipedia sobre Elefantes Indios</a>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png\" class=\"kg-image\" alt=\"Captura de pantalla de la página de Wikipedia sobre elefantes indios, con artículos, tres imágenes de elefantes y estado de conservación.\" loading=\"lazy\" width=\"2000\" height=\"1870\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Screenshot-2024-06-13-at-14.12.36--1-.png 2188w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Para ver esto como texto plano, como se pasa a <code>jina-colbert-v1-en</code>, haga clic en <a href=\"https://raw.githubusercontent.com/jina-ai/workshops/docs-heatmaps/notebooks/heatmaps/wikipedia_indian_elephant.txt?ref=jina-ai-gmbh.ghost.io\">este enlace</a>.</p><p>Este texto tiene 364 palabras, por lo que nuestro mapa de calor no se verá muy cuadrado:</p><pre><code class=\"language-Python\">create_heatmap(\"Elephants eat 150 kg of food per day.\", wikipedia_elephants, figsize=(50,7))</code></pre><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--71--2.png\" class=\"kg-image\" alt=\"Mapa de calor gráfico que muestra datos genéticos, con puntos rojos y naranjas que indican varios niveles de expresión a través de pares de bases\" loading=\"lazy\" width=\"2000\" height=\"378\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--71--2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--71--2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--71--2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/06/Untitled--71--2.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure><p>Vemos que \"elephants\" coincide en muchos lugares del texto. Esto no es sorprendente en un texto sobre elefantes. Pero también podemos ver un área donde hay una interacción mucho más fuerte:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--72--1.png\" class=\"kg-image\" alt=\"Mapa de calor genómico con patrones rojos y negros, eje etiquetado 'XNTY', y regiones resaltadas que indican puntos de datos.\" loading=\"lazy\" width=\"2000\" height=\"443\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--72--1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Untitled--72--1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/06/Untitled--72--1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/06/Untitled--72--1.png 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure><p>¿Qué está pasando aquí? Con Jina-ColBERT, podemos encontrar la parte del texto más largo que corresponde a esto. Resulta que es la cuarta oración del segundo párrafo:</p><blockquote>The species is classified as a megaherbivore and consume up to 150 kg (330 lb) of plant matter per day.</blockquote><p>Esto reafirma la misma información que en el texto de consulta. Si miramos el mapa de calor solo para esta oración, podemos ver las coincidencias fuertes:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--74-.png\" class=\"kg-image\" alt=\"Mapa de calor que muestra la co-ocurrencia de palabras con enfoque en &quot;elephants&quot;, &quot;food&quot; y &quot;day&quot;, con intensidad de color indicando la fuerza\" loading=\"lazy\" width=\"640\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Untitled--74-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Untitled--74-.png 640w\"></figure><p>Jina-ColBERT te proporciona los medios para ver exactamente qué áreas en un texto largo causaron que coincidiera con la consulta. Esto lleva a una mejor depuración, pero también a una mayor explicabilidad. No se necesita ninguna sofisticación para ver cómo se hace una coincidencia.</p><h2 id=\"explaining-ai-outcomes-with-jina-colbert\">Explicando los resultados de IA con Jina-ColBERT</h2><p>Los embeddings son una tecnología central en la IA moderna. Casi todo lo que hacemos se basa en la idea de que las relaciones complejas y aprendibles en los datos de entrada pueden expresarse en la geometría de espacios de alta dimensión. Sin embargo, es muy difícil para los simples humanos dar sentido a las relaciones espaciales en miles a millones de dimensiones.</p><p>ColBERT es un paso atrás desde ese nivel de abstracción. No es una respuesta completa al problema de explicar lo que hace un modelo de IA, pero nos señala directamente qué partes de nuestros datos son responsables de nuestros resultados.</p><p>A veces, la IA tiene que ser una caja negra. Las matrices gigantes que hacen todo el trabajo pesado son demasiado grandes para que cualquier humano las mantenga en su cabeza. Pero la arquitectura ColBERT ilumina un poco la caja y demuestra que es posible más.</p><p>El modelo Jina-ColBERT está actualmente disponible solo para inglés (<code>jina-colbert-v1-en</code>) pero más idiomas y contextos de uso están en camino. Esta línea de modelos, que no solo realiza recuperación de información de última generación sino que también puede decirte por qué coincidió algo, demuestra el compromiso de Jina AI de hacer que las tecnologías de IA sean tanto accesibles como útiles.</p>",
  "comment_id": "6672af263ce1950001eed6a7",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/Search-acc--3-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-06-19T12:12:54.000+02:00",
  "updated_at": "2024-07-08T21:08:21.000+02:00",
  "published_at": "2024-06-19T16:01:36.000+02:00",
  "custom_excerpt": "AI explainability and transparency are hot topics. How can we trust AI if we can't see how it works? Jina-ColBERT shows you how, with the right model architecture, you can easily make your AI spill its secrets.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e8495e0f6e004d70bd9e",
      "name": "Maximilian Werk",
      "slug": "maximilian",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/Profile-Picture.jpg",
      "cover_image": null,
      "bio": "I love bringing business value with ML powered solutions as well as broad strategic and deep technical discussions. I also care a lot about our company culture and enjoy pair programming.",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/maximilian/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e8495e0f6e004d70bd9e",
    "name": "Maximilian Werk",
    "slug": "maximilian",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/Profile-Picture.jpg",
    "cover_image": null,
    "bio": "I love bringing business value with ML powered solutions as well as broad strategic and deep technical discussions. I also care a lot about our company culture and enjoy pair programming.",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/maximilian/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/ai-explainability-made-easy-how-late-interaction-makes-jina-colbert-transparent/",
  "excerpt": "La explicabilidad y transparencia de la IA son temas candentes. ¿Cómo podemos confiar en la IA si no podemos ver cómo funciona? Jina-ColBERT te muestra cómo, con la arquitectura de modelo adecuada, puedes hacer fácilmente que tu IA revele sus secretos.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Digital representation of a golden building seen through a blue and yellow mesh pattern, evoking a technological vibe.",
  "feature_image_caption": null
}
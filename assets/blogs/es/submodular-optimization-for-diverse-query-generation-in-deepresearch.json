{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "Optimización submodular para la generación de consultas diversas en DeepResearch",
  "html": "<p>Al implementar DeepResearch, hay al menos dos lugares donde necesitas generar diversas consultas. Primero, debes <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">generar consultas de búsqueda web basadas en la entrada del usuario</a> (lanzar directamente la entrada del usuario al motor de búsqueda no es una buena idea). En segundo lugar, muchos sistemas DeepResearch incluyen <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">un \"planificador de investigación\" que descompone el problema original en subproblemas</a>, llama concurrentemente a agentes para resolverlos de forma independiente y luego fusiona sus resultados. Ya sea que se trate de consultas o subproblemas, nuestras expectativas siguen siendo las mismas: deben ser relevantes para la entrada original y lo suficientemente diversas como para proporcionar perspectivas únicas sobre ella. A menudo, necesitamos limitar el número de consultas para evitar gastar dinero innecesariamente solicitando motores de búsqueda o utilizando tokens de agente.</p><p>Si bien comprenden la importancia de la generación de consultas, la mayoría de las implementaciones de código abierto de DeepResearch no se toman esta optimización en serio. Simplemente solicitan estas restricciones directamente. Algunos podrían pedirle al LLM un turno adicional para evaluar y diversificar las consultas. Aquí hay un ejemplo de cómo la mayoría de las implementaciones abordan esto básicamente:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Dos prompts diferentes para generar diversas consultas usando LLMs. El prompt superior usa instrucciones simples. El inferior es más sofisticado y estructurado. Dada la consulta original y el número de consultas que se generarán, esperamos que las consultas generadas sean suficientemente diversas. En este ejemplo, usamos </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> como el LLM y la consulta original es </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>En este artículo, quiero demostrar un enfoque más riguroso para resolver la generación óptima de consultas utilizando modelos de vectores de frases y <strong>optimización submodular</strong>. En mis días de doctorado, la optimización submodular era una de mis técnicas favoritas junto con L-BFGS. Mostraré cómo aplicarla para generar un conjunto de diversas consultas bajo una restricción de cardinalidad, lo que puede mejorar significativamente la calidad general de los sistemas DeepResearch.</p><h2 id=\"query-generation-via-prompting\">Generación de Consultas mediante el uso de Prompt</h2><p>Primero, queremos comprobar si el uso de prompts es un enfoque eficaz para generar diversas consultas. También queremos entender si un prompt sofisticado es más eficaz que un prompt simple. Ejecutemos un experimento comparando los dos prompts siguientes para averiguarlo:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Prompt simple</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Prompt estructurado</span></p></figcaption></figure><p>Usamos <code>gemini-2.5-flash</code> como el LLM con la consulta original <code>\"embeddings and rerankers\"</code> y probamos tanto el prompt simple como el estructurado para generar iterativamente de una a 20 consultas. Luego, usamos <code>jina-embeddings-v3</code> con la tarea <code>text-matching</code> para medir la similitud de frases entre la consulta original y las consultas generadas, así como la similitud dentro de las propias consultas generadas. Aquí están las visualizaciones.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Ambos prompts muestran patrones similares en el análisis \"Dentro de las consultas generadas\" (los dos gráficos de la derecha), con similitudes cosenales medianas que se mantienen altas (rango de 0.4-0.6) en diferentes recuentos de consultas. El prompt simple parece ser incluso mejor para diversificar las consultas cuando el número de consultas es grande, mientras que el prompt estructurado mantiene una relevancia ligeramente mejor con respecto a la consulta original, manteniendo la relevancia alrededor de 0.6.</span></figcaption></figure><p>Si observa los dos gráficos del lado derecho, se puede ver que tanto el prompt simple como el estructurado exhiben una gran varianza en las puntuaciones de similitud cosenales, y muchas alcanzan una similitud de 0.7-0.8, lo que sugiere que algunas consultas generadas son casi idénticas. Además, ambos métodos tienen dificultades para mantener la diversidad a medida que se generan más consultas. En lugar de ver una clara tendencia a la baja en la similitud al aumentar el recuento de consultas, observamos niveles de similitud relativamente estables (y altos), lo que indica que las consultas adicionales a menudo duplican las perspectivas existentes.</p><p>Una explicación es lo que Wang et al. (2025) encontraron que los LLM a menudo reflejan las opiniones de los grupos dominantes de manera desproporcionada, incluso con la dirección del prompt, lo que indica un sesgo hacia las perspectivas comunes. Esto se debe a que los datos de entrenamiento del LLM pueden sobrerrepresentar ciertos puntos de vista, lo que hace que el modelo genere variaciones que se alinean con estas perspectivas prevalentes. Abe et al. (2025) también encontraron que la expansión de consultas basada en LLM favorece las interpretaciones populares y pasa por alto otras. Por ejemplo, \"¿Cuáles son los beneficios de la IA?\" podría producir beneficios comunes como la automatización, la eficiencia, la ética, pero perderse otros menos obvios como el descubrimiento de fármacos.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">Formulación del Problema</h2><p>Uno podría pensar que nuestro experimento anterior no es concluyente y que deberíamos mejorar el prompt e intentarlo de nuevo. Si bien el uso de prompts ciertamente puede cambiar los resultados hasta cierto punto, lo que es más importante es que hemos aprendido algo: simplemente aumentar el número de consultas generadas hace que sea más probable que obtengamos diversas consultas. La mala noticia es que también estamos obteniendo un montón de consultas duplicadas como producto secundario.</p><p>Pero dado que es barato generar una gran cantidad de consultas, lo que eventualmente produce <em>algunas</em> buenas, ¿por qué no tratamos esto como un problema de selección de subconjuntos?</p><p>En matemáticas, así es como podemos formular este problema: dado una entrada original $q_0$, un conjunto de consultas candidatas $V=\\{q_1, q_2, \\cdots, q_n\\}$ generadas por un LLM usando ingeniería de Prompt. Seleccionar un subconjunto $X\\subseteq V$ de $k$ consultas que maximicen la cobertura y minimicen la redundancia.</p><p>Desafortunadamente, encontrar el subconjunto óptimo de $k$ consultas de $n$ candidatos requiere verificar $\\binom{n}{k}$ combinaciones - complejidad exponencial. Para solo 20 candidatos y $k=5$, eso es 15,504 combinaciones. </p><h3 id=\"submodular-function\">Función Submodular</h3><p>Antes de intentar resolver brutalmente el problema de selección de subconjuntos, permítanme presentarles a los lectores el término <strong>submodularidad</strong> y <strong>función submodular</strong>. Puede que les suene desconocido a muchos, pero es posible que hayan oído hablar de la idea de \"rendimientos decrecientes\"; pues bien, la submodularidad es la representación matemática de eso.</p><p>Consideremos la colocación de routers Wi-Fi para proporcionar cobertura de Internet en un edificio grande. El primer router que instalas da un valor tremendo: cubre un área significativa que antes no tenía cobertura. El segundo router también añade un valor sustancial, pero parte de su área de cobertura se solapa con el primer router, por lo que el beneficio marginal es menor que el primero. A medida que sigues añadiendo routers, cada router adicional cubre cada vez menos área nueva porque la mayoría de los espacios ya están cubiertos por los routers existentes. Eventualmente, el décimo router podría proporcionar muy poca cobertura adicional, ya que el edificio ya está bien cubierto.</p><p>Esta intuición captura la esencia de la submodularidad. Matemáticamente, una función de conjunto $f: 2^V \\rightarrow \\mathbb{R}$ es <strong>submodular</strong> si para todo $A \\subseteq B \\subseteq V$ y cualquier elemento $v \\notin B$:</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>En lenguaje sencillo: añadir un elemento a un conjunto más pequeño da al menos tanto beneficio como añadir el mismo elemento a un conjunto más grande que contiene el conjunto más pequeño.</p><p>Ahora apliquemos este concepto a nuestro problema de generación de consultas. Uno puede darse cuenta inmediatamente de que la selección de consultas exhibe <strong>rendimientos decrecientes</strong> naturales:</p><ul><li>La primera consulta que seleccionamos cubre un espacio semántico completamente nuevo</li><li>La segunda consulta debería cubrir diferentes aspectos, pero es inevitable que haya alguna superposición</li><li>A medida que añadimos más consultas, cada consulta adicional cubre cada vez menos terreno nuevo</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">De </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">una de mis viejas diapositivas de AAAI 2013</span></a><span style=\"white-space: pre-wrap;\">, donde expliqué la submodularidad usando una bolsa de bolas. Añadir más bolas a la bolsa mejora la \"facilidad\", pero la mejora relativa se hace cada vez más pequeña, como se ve en los valores delta decrecientes en el eje y de la derecha.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">Diseño de función submodular basada en Vectores Modelo</h2><p>Sea $\\mathbf{e}_i \\in \\mathbb{R}^d$ el vector de Vectores Modelo para la consulta $q_i$, obtenido usando un modelo de Vectores Modelo de frases (por ejemplo, <code>jina-embeddings-v3</code>). Hay dos enfoques principales para diseñar nuestra función objetivo:</p><h3 id=\"approach-1-facility-location-coverage-based\">Enfoque 1: Ubicación de instalaciones (basado en la cobertura)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>Esta función mide lo bien que el conjunto seleccionado $X$ \"cubre\" todas las consultas candidatas, donde:</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ es la similitud coseno</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ asegura la relevancia para la consulta original</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ mide la cobertura del candidato $j$ por el conjunto seleccionado $X$</li></ul><p>Una advertencia es que esta función solo fomenta la diversidad de forma <em>implícita</em>. No penaliza explícitamente la similitud dentro del conjunto seleccionado $X$. La diversidad surge porque la selección de consultas similares proporciona rendimientos de cobertura decrecientes.</p><h3 id=\"approach-2-explicit-coverage-diversity\">Enfoque 2: Cobertura explícita + Diversidad</h3><p>Para un control más directo sobre la diversidad, podemos combinar la cobertura y un término de diversidad explícito:</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>donde el componente de diversidad se puede formular como:</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>Este término de diversidad mide la similitud total entre las consultas seleccionadas y las consultas no seleccionadas; se maximiza cuando seleccionamos consultas que son diferentes de los candidatos restantes (una forma de función de corte de grafo).</p><h3 id=\"difference-between-two-approaches\">Diferencia entre los dos enfoques</h3><p>Ambas formulaciones mantienen la submodularidad.</p><p>La función de ubicación de instalaciones es una función submodular bien conocida. Exhibe submodularidad debido a la operación max: cuando añadimos una nueva consulta $q$ a nuestro conjunto seleccionado, cada consulta candidata $j$ es cubierta por la consulta \"mejor\" en nuestro conjunto (la que tiene la mayor similitud). Añadir $q$ a un conjunto más pequeño $A$ es más probable que mejore la cobertura de varios candidatos que añadirlo a un conjunto más grande $B \\supseteq A$ donde muchos candidatos ya están bien cubiertos.</p><p>En la función de diversidad de corte de grafo<strong>,</strong> el término de diversidad $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ es submodular porque mide el \"corte\" entre los conjuntos seleccionados y no seleccionados. Añadir una nueva consulta a un conjunto seleccionado más pequeño crea más conexiones nuevas con consultas no seleccionadas que añadirla a un conjunto seleccionado más grande.</p><p>El enfoque de ubicación de instalaciones se basa en la diversidad <em>implícita</em> a través de la competencia de cobertura, mientras que el enfoque explícito mide y optimiza directamente la diversidad. Así que ambos son válidos, pero el enfoque explícito te da un control más directo sobre el equilibrio entre relevancia y diversidad.</p><h2 id=\"implementations\">Implementaciones</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">La implementación completa se puede encontrar aquí en Github.</span></p></figcaption></figure><p>Dado que nuestra función es submodular, podemos usar <strong>el algoritmo voraz</strong> que proporciona una garantía de aproximación de $(1-1/e) \\approx 0.63$:</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{sujeto a} \\quad |X| \\leq k$$</p><p>Aquí está el código para optimizar la ubicación de instalaciones (basado en la cobertura) - el que tiene diversidad implícita.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Algoritmo voraz para la selección de consultas submodulares\n    \n    Args:\n        candidates: Lista de cadenas de consulta candidatas\n        embeddings: Matriz de Vectores Modelo de consulta (n x d)\n        original_embedding: Vectores Modelo de la consulta original (d,)\n        k: Número de consultas para seleccionar\n        alpha: Parámetro de ponderación de relevancia\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precalcular las puntuaciones de relevancia\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calcular la ganancia marginal de añadir la consulta i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Calcular la ganancia marginal de añadir new_idx al conjunto seleccionado\"\"\"\n    if not selected:\n        # Primera consulta: la ganancia es la suma de todas las puntuaciones de relevancia\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Calcular la cobertura actual\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Calcular la nueva cobertura con la consulta adicional\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>El parámetro de equilibrio $\\alpha$ controla el equilibrio entre relevancia y diversidad:</p><ul><li><strong>$\\alpha$ alto (por ejemplo, 0.8)</strong>: Prioriza la relevancia para la consulta original, puede sacrificar la diversidad</li><li><strong>$\\alpha$ bajo (por ejemplo, 0.2)</strong>: Prioriza la diversidad entre las consultas seleccionadas, puede desviarse de la intención original</li><li><strong>$\\alpha$ moderado (por ejemplo, 0.4-0.6)</strong>: Enfoque equilibrado, a menudo funciona bien en la práctica</li></ul><h3 id=\"lazy-greedy-algorithm\">Algoritmo Voraz Perezoso</h3><p>Uno puede notar en el código anterior:</p><pre><code class=\"language-python\">for i in remaining:\n    # Calcular la ganancia marginal de añadir la consulta i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>Estamos calculando la ganancia marginal para <strong>todos</strong> los candidatos restantes en cada iteración. Podemos hacerlo mejor que esto.</p><p>El <strong>algoritmo voraz perezoso</strong> es una optimización inteligente que explota la submodularidad para evitar cálculos innecesarios. La idea clave es: si el elemento A tenía una ganancia marginal mayor que el elemento B en la iteración $t$, entonces A seguirá teniendo una ganancia marginal mayor que B en la iteración $t+1$ (debido a la propiedad de submodularidad).</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>El método greedy perezoso funciona así:</p><ol><li>Mantener una cola de prioridad de elementos ordenados por sus ganancias marginales.</li><li>Solo volver a calcular la ganancia marginal del elemento superior.</li><li>Si sigue siendo la más alta después del recálculo, selecciónela.</li><li>De lo contrario, vuelva a insertarla en la posición correcta y compruebe el siguiente elemento superior.</li></ol><p>Esto puede proporcionar aceleraciones significativas porque evitamos volver a calcular las ganancias marginales para los elementos que claramente no se seleccionarán.</p><h3 id=\"results\">Resultados</h3><p>Volvamos a ejecutar el experimento. Utilizamos el mismo *Prompt* simple para generar de una a 20 consultas diversas y realizamos las mismas mediciones de similitud coseno que antes. Para la optimización submodular, seleccionamos consultas de los 20 candidatos generados utilizando diferentes valores de k y medimos la similitud como antes. Los resultados muestran que las consultas seleccionadas mediante la optimización submodular son más diversas y muestran una menor similitud en el conjunto.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Consulta original = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Consulta original = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Consulta original = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Consulta original = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">Pregunta final: Por qué es importante la formulación submodular</h2><p>Puede que se pregunte: ¿por qué tomarse la molestia de formular esto como un problema de optimización submodular? ¿Por qué no utilizar simplemente heurísticas u otros enfoques de optimización?</p><p>En resumen, la formulación submodular transforma una heurística *ad-hoc* de \"seleccionar consultas diversas\" en un problema de optimización riguroso con **garantías demostrables**, **algoritmos eficientes** y objetivos medibles.</p><h3 id=\"guaranteed-efficiency\">Eficiencia garantizada</h3><p>Una vez que demostramos que nuestra función objetivo es submodular, obtenemos potentes garantías teóricas y un algoritmo eficiente. El algoritmo *greedy* que se ejecuta en tiempo $O(nk)$ en comparación con la comprobación de combinaciones $\\binom{n}{k}$ logra una aproximación de $(1-1/e) \\approx 0.63$ a la solución óptima. Esto significa que nuestra solución *greedy* es siempre al menos un 63% tan buena como la mejor solución posible. **Ninguna heurística puede prometer esto.**</p><p>Además, el algoritmo *greedy* perezoso es drásticamente más rápido en la práctica debido a la estructura matemática de las funciones submodulares. La aceleración proviene de **rendimientos decrecientes**: es poco probable que los elementos que fueron malas elecciones en iteraciones anteriores se conviertan en buenas elecciones más adelante. Por lo tanto, en lugar de comprobar los $n$ candidatos, el *greedy* perezoso normalmente solo necesita volver a calcular las ganancias para los pocos candidatos principales.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">No hay necesidad de heurísticas hechas a mano</h3><p>Sin un marco de trabajo basado en principios, podría recurrir a reglas *ad-hoc* como \"asegurarse de que las consultas tengan una similitud coseno &lt; 0.7\" o \"equilibrar diferentes categorías de palabras clave\". Estas reglas son difíciles de ajustar y no se generalizan. La optimización submodular le proporciona un enfoque basado en principios y con fundamentos matemáticos. Puede ajustar los hiperparámetros sistemáticamente utilizando conjuntos de validación y supervisar la calidad de la solución en los sistemas de producción. Cuando el sistema produce resultados deficientes, tiene métricas claras para depurar lo que salió mal.</p><p>Por último, la optimización submodular es un campo bien estudiado con décadas de investigación, lo que le permite aprovechar algoritmos avanzados más allá del *greedy* (como el *greedy* acelerado o la búsqueda local), conocimientos teóricos sobre cuándo ciertas formulaciones funcionan mejor y extensiones para manejar restricciones adicionales, como límites de presupuesto o requisitos de equidad.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Para aquellos que estén interesados en la optimización submodular, recomiendo este sitio para obtener más información.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "Muchos conocen la importancia de la diversidad de consultas en DeepResearch, pero pocos saben cómo resolverla rigurosamente a través de la optimización submodular.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "scaling-test-time-compute-for-embedding-models",
  "id": "675a84f80ce9930001b86f09",
  "uuid": "49f876f3-0d50-4555-8f9e-136473f720ac",
  "title": "Escalando el C√≥mputo en Tiempo de Prueba para Modelos de Embeddings",
  "html": "<p>Desde el lanzamiento del <a href=\"https://openai.com/o1/?ref=jina-ai-gmbh.ghost.io\">modelo O1</a> de OpenAI, uno de los temas m√°s discutidos en la comunidad de IA ha sido el <strong>escalado del c√≥mputo en tiempo de prueba</strong>. Esto se refiere a asignar recursos computacionales adicionales durante la inferencia‚Äîla fase donde un modelo de IA genera salidas en respuesta a entradas‚Äîen lugar de durante el preentrenamiento. Un ejemplo bien conocido es el razonamiento multi-paso de \"cadena de pensamiento\", que permite a los modelos realizar deliberaciones internas m√°s extensas, como evaluar m√∫ltiples respuestas potenciales, planificaci√≥n m√°s profunda y auto-reflexi√≥n antes de llegar a una respuesta final. Esta estrategia mejora la calidad de las respuestas, particularmente en tareas de razonamiento complejo. El modelo <a href=\"https://huggingface.co/Qwen/QwQ-32B-Preview?ref=jina-ai-gmbh.ghost.io\">QwQ-32B-Preview</a> recientemente lanzado por Alibaba sigue esta tendencia de mejorar el razonamiento de IA a trav√©s del incremento del c√≥mputo en tiempo de prueba.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">En este contexto, \"escalado\" principalmente significa aumentar la capacidad computacional (como poder de procesamiento o tiempo) disponible durante la inferencia. No se refiere a <b><strong style=\"white-space: pre-wrap;\">scaling out</strong></b> (distribuir tareas entre m√∫ltiples sistemas) o lograr un <b><strong style=\"white-space: pre-wrap;\">speedup</strong></b> (reducir el tiempo de procesamiento).</div></div><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1.mp4\" poster=\"https://img.spacergif.org/v1/900x432/0a/spacer.png\" width=\"900\" height=\"432\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:10</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1√ó</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Cuando se usa el modelo O1 de OpenAI, los usuarios pueden notar claramente que la inferencia de m√∫ltiples pasos requiere tiempo adicional mientras el modelo construye cadenas de razonamiento para resolver problemas.</span></p></figcaption>\n        </figure><p>En Jina AI, nos enfocamos m√°s en embeddings y rerankers que en LLMs, as√≠ que para nosotros es natural considerar el escalado del c√≥mputo en tiempo de prueba en este contexto: <em>¬øC√≥mo se puede aplicar la \"cadena de pensamiento\" a los modelos de embedding?</em> Aunque inicialmente pueda no parecer intuitivo, este art√≠culo explora una perspectiva novedosa y demuestra c√≥mo se puede aplicar el escalado del c√≥mputo en tiempo de prueba a <code>jina-clip</code> para clasificar im√°genes fuera de distribuci√≥n (OOD)‚Äîresolviendo tareas que de otro modo ser√≠an imposibles.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Nuestro experimento se centr√≥ en el reconocimiento de Pokemon, que presenta un desaf√≠o interesante para los modelos de embedding. Mientras que los modelos tipo CLIP sobresalen en la coincidencia general de imagen-texto, pueden tener dificultades con dominios espec√≠ficos o im√°genes OOD sin ajuste fino. Al dar a los modelos m√°s tiempo para \"pensar\", descubrimos que la clasificaci√≥n multi-objetivo‚Äîan√°loga a una \"cadena de pensamiento\"‚Äîpodr√≠a mejorar la precisi√≥n sin ning√∫n ajuste del modelo de embedding en s√≠.</span></figcaption></figure><h2 id=\"case-study\">Caso de Estudio</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z?ref=jina-ai-gmbh.ghost.io#scrollTo=CJt5zwA9E2jB\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-15.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Nuestro experimento se centr√≥ en la clasificaci√≥n de Pokemon usando el <a href=\"https://huggingface.co/datasets/TheFusion21/PokemonCards?ref=jina-ai-gmbh.ghost.io\">dataset TheFusion21/PokemonCards</a>, que contiene miles de im√°genes de cartas de Pokemon. <strong>La tarea es clasificaci√≥n de im√°genes</strong> donde la entrada es una ilustraci√≥n recortada de carta Pokemon (con todo el texto/descripciones eliminados) y la salida es el nombre correcto del Pokemon de un conjunto predefinido de nombres. Esta tarea presenta un desaf√≠o particularmente interesante para los modelos de embedding CLIP porque:</p><ul><li>Los nombres y visuales de Pokemon representan conceptos espec√≠ficos fuera de distribuci√≥n para el modelo, haciendo que la clasificaci√≥n directa sea desafiante</li><li>Cada Pokemon tiene rasgos visuales claros que pueden descomponerse en elementos b√°sicos (formas, colores, poses) que CLIP podr√≠a entender mejor</li><li>La ilustraci√≥n de la carta proporciona un formato visual consistente mientras introduce complejidad a trav√©s de fondos, poses y estilos art√≠sticos variables</li><li>La tarea requiere integrar m√∫ltiples caracter√≠sticas visuales simult√°neamente, similar a las cadenas de razonamiento complejas en modelos de lenguaje</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"835\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Recortamos las im√°genes de las cartas Pokemon para eliminar toda la informaci√≥n textual (encabezado, pie de p√°gina, descripci√≥n) para evitar cualquier adivinanza trivial debido a que los nombres de Pokemon aparezcan en esos textos. Las etiquetas de clase de estos Pokemon son [</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol G</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Weedle</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Azumarill</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Bulbasaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Venusaur</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aggron</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Beedrill Œ¥</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Alakazam</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Dratini</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Arcanine</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Blaine's Moltres</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Celebi & Venusaur-GX</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">]</span></figcaption></figure><h3 id=\"baseline\">L√≠nea base</h3><p>El enfoque base utiliza una simple comparaci√≥n directa entre la ilustraci√≥n de las cartas Pok√©mon y los nombres. Primero, recortamos cada imagen de carta Pok√©mon para eliminar toda la informaci√≥n textual (encabezado, pie de p√°gina, descripci√≥n) para evitar cualquier conjetura trivial del modelo CLIP debido a nombres de Pok√©mon que aparecen en esos textos. Luego codificamos tanto las im√°genes recortadas como los nombres de Pok√©mon usando el modelo <code>jina-clip-v1</code> y <code>jina-clip-v2</code> para obtener sus respectivos embeddings. La clasificaci√≥n se realiza calculando la similitud coseno entre estos embeddings de imagen y texto - cada imagen se empareja con el nombre que tiene el puntaje de similitud m√°s alto. Esto crea una correspondencia uno a uno directa entre la ilustraci√≥n de la carta y los nombres de Pok√©mon, sin ning√∫n contexto adicional o informaci√≥n de atributos. El pseudo c√≥digo a continuaci√≥n resume el m√©todo base.</p><pre><code class=\"language-python\"># Preprocessing\ncropped_images = [crop_artwork(img) for img in pokemon_cards]  # Remove text, keep only art\npokemon_names = [\"Absol\", \"Aerodactyl\", ...]  # Raw Pokemon names\n\n# Get embeddings using jina-clip-v1\nimage_embeddings = model.encode_image(cropped_images)\ntext_embeddings = model.encode_text(pokemon_names)\n\n# Classification by cosine similarity\nsimilarities = cosine_similarity(image_embeddings, text_embeddings)\npredicted_names = [pokemon_names[argmax(sim)] for sim in similarities]\n\n# Evaluate\naccuracy = mean(predicted_names == ground_truth_names)</code></pre><h3 id=\"chain-of-thoughts-for-classification\">\"Cadena de Pensamientos\" para Clasificaci√≥n</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--10-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>En lugar de emparejar directamente im√°genes con nombres, descomponemos el reconocimiento de Pok√©mon en un sistema estructurado de atributos visuales. Definimos cinco grupos de atributos clave: color dominante (por ejemplo, \"blanco\", \"azul\"), forma primaria (por ejemplo, \"un lobo\", \"un reptil alado\"), rasgo caracter√≠stico (por ejemplo, \"un solo cuerno blanco\", \"grandes alas\"), forma del cuerpo (por ejemplo, \"similar a un lobo en cuatro patas\", \"alado y esbelto\"), y escena de fondo (por ejemplo, \"espacio exterior\", \"bosque verde\").</p><p>Para cada grupo de atributos, creamos indicaciones de texto espec√≠ficas (por ejemplo, \"El cuerpo de este Pok√©mon es principalmente de color {}\") emparejadas con opciones relevantes. Luego usamos el modelo para calcular puntuaciones de similitud entre la imagen y cada opci√≥n de atributo. Estas puntuaciones se convierten en probabilidades usando softmax para obtener una medida m√°s calibrada de confianza.</p><p>La estructura completa de Cadena de Pensamiento (CoT) consiste en dos partes: <code>classification_groups</code> que describe grupos de indicaciones, y <code>pokemon_rules</code> que define qu√© opciones de atributos debe coincidir cada Pok√©mon. Por ejemplo, Absol debe coincidir con \"blanco\" para el color y \"similar a un lobo\" para la forma. La CoT completa se muestra a continuaci√≥n (explicaremos c√≥mo se construye m√°s adelante):</p><pre><code class=\"language-python\">pokemon_system = {\n    \"classification_cot\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pok√©mon's body is mainly {} in color.\",\n            \"options\": [\n                \"white\",    # Absol, Absol G\n                \"gray\",     # Aggron\n                \"brown\",    # Aerodactyl, Weedle, Beedrill Œ¥\n                \"blue\",     # Azumarill\n                \"green\",    # Bulbasaur, Venusaur, Celebi&Venu, Caterpie\n                \"yellow\",   # Alakazam, Ampharos\n                \"red\",      # Blaine's Moltres\n                \"orange\",   # Arcanine\n                \"light blue\"# Dratini\n            ]\n        },\n        \"primary_form\": {\n            \"prompt\": \"It looks like {}.\",\n            \"options\": [\n                \"a wolf\",         # Absol, Absol G\n                \"an armored dinosaur\",  # Aggron\n                \"a winged reptile\",     # Aerodactyl\n                \"a rabbit-like creature\", # Azumarill\n                \"a toad-like creature\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"a caterpillar larva\",    # Weedle, Caterpie\n                \"a wasp-like insect\",     # Beedrill Œ¥\n                \"a fox-like humanoid\",     # Alakazam\n                \"a sheep-like biped\",      # Ampharos\n                \"a dog-like beast\",        # Arcanine\n                \"a flaming bird\",          # Blaine's Moltres\n                \"a serpentine dragon\"      # Dratini\n            ]\n        },\n        \"key_trait\": {\n            \"prompt\": \"Its most notable feature is {}.\",\n            \"options\": [\n                \"a single white horn\", # Absol, Absol G\n                \"metal armor plates\",  # Aggron\n                \"large wings\",         # Aerodactyl, Beedrill Œ¥\n                \"rabbit ears\",         # Azumarill\n                \"a green plant bulb\",  # Bulbasaur, Venusaur, Celebi&Venu\n                \"a small red spike\",   # Weedle\n                \"big green eyes\",      # Caterpie\n                \"a mustache and spoons\", # Alakazam\n                \"a glowing tail orb\",  # Ampharos\n                \"a fiery mane\",        # Arcanine\n                \"flaming wings\",       # Blaine's Moltres\n                \"a tiny white horn on head\" # Dratini\n            ]\n        },\n        \"body_shape\": {\n            \"prompt\": \"The body shape can be described as {}.\",\n            \"options\": [\n                \"wolf-like on four legs\",   # Absol, Absol G\n                \"bulky and armored\",        # Aggron\n                \"winged and slender\",       # Aerodactyl, Beedrill Œ¥\n                \"round and plump\",          # Azumarill\n                \"sturdy and four-legged\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"long and worm-like\",       # Weedle, Caterpie\n                \"upright and humanoid\",     # Alakazam, Ampharos\n                \"furry and canine\",         # Arcanine\n                \"bird-like with flames\",    # Blaine's Moltres\n                \"serpentine\"                # Dratini\n            ]\n        },\n        \"background_scene\": {\n            \"prompt\": \"The background looks like {}.\",\n            \"options\": [\n                \"outer space\",      # Absol G, Beedrill Œ¥\n                \"green forest\",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&Venu\n                \"a rocky battlefield\", # Absol, Aggron, Aerodactyl\n                \"a purple psychic room\", # Alakazam\n                \"a sunny field\",     # Ampharos\n                \"volcanic ground\",   # Arcanine\n                \"a red sky with embers\", # Blaine's Moltres\n                \"a calm blue lake\"   # Dratini\n            ]\n        }\n    },\n    \n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,      \n            \"body_shape\": 0,    \n            \"background_scene\": 2   \n        },\n        \"Absol G\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,       \n            \"body_shape\": 0,     \n            \"background_scene\": 0    \n        },\n        // ...\n    }\n}\n</code></pre><p>La clasificaci√≥n final combina estas probabilidades de atributos - en lugar de una √∫nica comparaci√≥n de similitud, ahora estamos haciendo m√∫ltiples comparaciones estructuradas y agregando sus probabilidades para tomar una decisi√≥n m√°s informada.</p><pre><code class=\"language-python\"># Classification process\ndef classify_pokemon(image):\n   # Generate all text prompts\n   all_prompts = []\n   for group in classification_cot:\n       for option in group[\"options\"]:\n           prompt = group[\"prompt\"].format(option)\n           all_prompts.append(prompt)\n\n   # Get embeddings and similarities\n   image_embedding = model.encode_image(image)\n   text_embeddings = model.encode_text(all_prompts)\n   similarities = cosine_similarity(image_embedding, text_embeddings)\n\n   # Convert to probabilities per attribute group\n   probabilities = {}\n   for group_name, group_sims in group_similarities:\n       probabilities[group_name] = softmax(group_sims)\n\n   # Score each Pokemon based on matching attributes\n   scores = {}\n   for pokemon, rules in pokemon_rules.items():\n       score = 0\n       for group, target_idx in rules.items():\n           score += probabilities[group][target_idx]\n       scores[pokemon] = score\n\n   return max(scores, key=scores.get)</code></pre><h3 id=\"complexity-analysis\">An√°lisis de Complejidad</h3><p>Supongamos que queremos clasificar una imagen en uno de los <code>N</code> nombres de Pok√©mon. El enfoque base requiere calcular <code>N</code> embeddings de texto (uno para cada nombre de Pok√©mon). En contraste, nuestro enfoque de c√°lculo escalado en tiempo de prueba requiere calcular <code>Q</code> embeddings de texto, donde</p><code>Q</code> es el n√∫mero total de combinaciones de preguntas y opciones en todas las preguntas. Ambos m√©todos requieren calcular una incrustaci√≥n de imagen y realizar un paso final de clasificaci√≥n, por lo que excluimos estas operaciones comunes de nuestra comparaci√≥n. En este caso de estudio, nuestro <code>N=13</code> y <code>Q=52</code>.</p><p>En un caso extremo donde <code>Q = N</code>, nuestro enfoque esencialmente se reducir√≠a al punto de referencia. Sin embargo, la clave para escalar efectivamente el c√≥mputo en tiempo de prueba es: </p><ul><li>Construir preguntas cuidadosamente elegidas que aumenten <code>Q</code> </li><li>Asegurar que cada pregunta proporcione pistas distintas e informativas sobre la respuesta final </li><li>Dise√±ar preguntas para que sean lo m√°s ortogonales posible para maximizar su ganancia de informaci√≥n conjunta.</li></ul><p>Este enfoque es an√°logo al juego de \"Veinte Preguntas\", donde cada pregunta se elige estrat√©gicamente para reducir eficazmente las posibles respuestas.</p><h3 id=\"evaluation\">Evaluaci√≥n</h3><p>Nuestra evaluaci√≥n se realiz√≥ en 117 im√°genes de prueba que abarcan 13 clases diferentes de Pok√©mon. Y el resultado es el siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>jina-clip-v1</th>\n<th>jina-clip-v2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>31.36%</td>\n<td>16.10%</td>\n</tr>\n<tr>\n<td>CoT</td>\n<td>46.61%</td>\n<td>38.14%</td>\n</tr>\n<tr>\n<td>Improvement</td>\n<td>+15.25%</td>\n<td>+22.04%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Se puede ver que la misma clasificaci√≥n CoT ofrece mejoras significativas para ambos modelos (+15.25% y +22.04% respectivamente) en esta tarea poco com√∫n o OOD. Esto tambi√©n sugiere que una vez que se construye el <code>pokemon_system</code>, <strong>el mismo sistema CoT puede transferirse efectivamente entre diferentes modelos; y no se requiere ajuste fino ni entrenamiento posterior.</strong></p><p>El rendimiento relativamente fuerte de la l√≠nea base de v1 (31.36%) en la clasificaci√≥n de Pok√©mon es notable. Este modelo fue entrenado en <a href=\"https://www.youtube.com/watch?v=HsGyxVUN1SA&ref=jina-ai-gmbh.ghost.io\">LAION-400M, que inclu√≠a contenido relacionado con Pok√©mon</a>. En contraste, v2 fue entrenado en DFN-2B (submuestreando 400M instancias), un conjunto de datos de mayor calidad pero m√°s filtrado que puede haber excluido contenido relacionado con Pok√©mon, lo que explica el menor rendimiento base de V2 (16.10%) en esta tarea espec√≠fica.</p><h3 id=\"constructing-pokemonsystem-effectively\">Construyendo <code>pokemon_system</code> Efectivamente</h3><p>La efectividad de nuestro enfoque de c√≥mputo escalado en tiempo de prueba depende en gran medida de qu√© tan bien construyamos el <code>pokemon_system</code>. Hay diferentes enfoques para construir este sistema, desde manual hasta completamente automatizado.</p><h4 id=\"manual-construction\">Construcci√≥n Manual</h4><p>El enfoque m√°s directo es analizar manualmente el conjunto de datos de Pok√©mon y crear grupos de atributos, prompts y reglas. Un experto en el dominio necesitar√≠a identificar atributos visuales clave como color, forma y caracter√≠sticas distintivas. Luego escribir√≠a prompts en lenguaje natural para cada atributo, enumerar√≠a las opciones posibles para cada grupo de atributos y mapear√≠a cada Pok√©mon a sus opciones de atributos correctas. Si bien esto proporciona reglas de alta calidad, consume mucho tiempo y no escala bien para <code>N</code> m√°s grandes.</p><h4 id=\"llm-assisted-construction\">Construcci√≥n Asistida por LLM</h4><p>Podemos aprovechar los LLMs para acelerar este proceso pidi√©ndoles que generen el sistema de clasificaci√≥n. Un prompt bien estructurado solicitar√≠a grupos de atributos basados en caracter√≠sticas visuales, plantillas de prompt en lenguaje natural, opciones completas y mutuamente excluyentes, y reglas de mapeo para cada Pok√©mon. El LLM puede generar r√°pidamente un primer borrador, aunque su salida puede necesitar verificaci√≥n.</p><pre><code class=\"language-txt\">I need help creating a structured system for Pokemon classification. For each Pokemon in this list: [Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...], create a classification system with:\n\n1. Classification groups that cover these visual attributes:\n   - Dominant color of the Pokemon\n   - What type of creature it appears to be (primary form)\n   - Its most distinctive visual feature\n   - Overall body shape\n   - What kind of background/environment it's typically shown in\n\n2. For each group:\n   - Create a natural language prompt template using \"{}\" for the option\n   - List all possible options that could apply to these Pokemon\n   - Make sure options are mutually exclusive and comprehensive\n\n3. Create rules that map each Pokemon to exactly one option per attribute group, using indices to reference the options\n\nPlease output this as a Python dictionary with two main components:\n- \"classification_groups\": containing prompts and options for each attribute\n- \"pokemon_rules\": mapping each Pokemon to its correct attribute indices\n\nExample format:\n{\n    \"classification_groups\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokemon's body is mainly {} in color\",\n            \"options\": [\"white\", \"gray\", ...]\n        },\n        ...\n    },\n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,  # index for \"white\"\n            ...\n        },\n        ...\n    }\n}</code></pre><p>Un enfoque m√°s robusto combina la generaci√≥n por LLM con la validaci√≥n humana. Primero, el LLM genera un sistema inicial. Luego, los expertos humanos revisan y corrigen las agrupaciones de atributos, la completitud de las opciones y la precisi√≥n de las reglas. El LLM refina el sistema bas√°ndose en esta retroalimentaci√≥n, y el proceso se itera hasta alcanzar una calidad satisfactoria. Este enfoque equilibra la eficiencia con la precisi√≥n.</p><h4 id=\"automated-construction-with-dspy\">Construcci√≥n Automatizada con DSPy</h4><p>Para un enfoque completamente automatizado, podemos usar DSPy para optimizar iterativamente <code>pokemon_system</code>. El proceso comienza con un <code>pokemon_system</code> simple escrito ya sea manualmente o por LLMs como prompt inicial. Cada versi√≥n se eval√∫a en un conjunto de retenci√≥n, usando la precisi√≥n como se√±al de retroalimentaci√≥n para DSPy. Bas√°ndose en este rendimiento, se generan prompts optimizados (es decir, nuevas versiones de <code>pokemon_system</code>). Este ciclo se repite hasta la convergencia, y durante todo el proceso, el modelo de incrustaci√≥n permanece completamente fijo.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Usando DSPy para encontrar el mejor dise√±o CoT de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>pokemon_system</span></code><span style=\"white-space: pre-wrap;\">; el proceso de ajuste necesita hacerse solo una vez para cada tarea.</span></figcaption></figure><h2 id=\"why-scale-test-time-compute-for-embedding-models\">¬øPor qu√© Escalar el C√≥mputo en Tiempo de Prueba para Modelos de Incrustaci√≥n?</h2><p>Porque escalar el preentrenamiento eventualmente se vuelve econ√≥micamente intratable. </p><p>Desde el lanzamiento de la suite de incrustaciones de Jina‚Äîincluyendo <code>jina-embeddings-v1</code>, <code>v2</code>, <code>v3</code>, <code>jina-clip-v1</code>, <code>v2</code>, y <code>jina-ColBERT-v1</code>, <code>v2</code>‚Äîcada actualizaci√≥n de modelo a trav√©s del preentrenamiento escalado ha venido con m√°s costos. Por ejemplo, nuestro primer modelo, <code>jina-embeddings-v1</code>, lanzado en junio de 2023 con 110M par√°metros. Entrenarlo en ese momento cost√≥ entre $5,000 y $10,000 dependiendo de c√≥mo se mida. Con <code>jina-embeddings-v3</code>, las mejoras son significativas, pero provienen principalmente de los mayores recursos invertidos. La trayectoria de costos para modelos de frontera ha pasado de miles a decenas de miles de d√≥lares y, para empresas de IA m√°s grandes, incluso cientos de millones hoy. Si bien aportar m√°s dinero, recursos y datos al preentrenamiento produce mejores modelos, los retornos marginales eventualmente hacen que un mayor escalamiento sea econ√≥micamente insostenible.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/plot--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/plot--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/plot--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/plot--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/plot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Ley de escalamiento de modelos de incrustaci√≥n. El rendimiento promedio MTEB en tareas en ingl√©s est√° graficado contra el n√∫mero de par√°metros del modelo. Cada punto representa un modelo de incrustaci√≥n. La l√≠nea de tendencia, que representa todos los modelos, est√° resaltada, con modelos multiling√ºes en puntos cian. Este gr√°fico fue creado seleccionando los 100 mejores modelos de incrustaci√≥n del tablero de clasificaci√≥n MTEB, excluyendo aquellos sin informaci√≥n de tama√±o, t√≠picamente modelos cerrados o propietarios. Tambi√©n se filtraron las presentaciones identificadas como trolleo obvio. </span></figcaption></figure><p>Por otro lado, los modelos de incrustaci√≥n modernos se est√°n volviendo cada vez m√°s poderosos: multiling√ºes, multitarea, multimodales y capaces de un fuerte rendimiento zero-shot y de seguimiento de instrucciones. Esta versatilidad deja mucho espacio para mejoras algor√≠tmicas y escalamiento del c√≥mputo en tiempo de prueba.</p><p>La pregunta entonces se convierte en: ¬øcu√°l es el costo que los usuarios est√°n dispuestos a pagar por una consulta que les importa profundamente? Si tolerar tiempos de inferencia m√°s largos para modelos de preentrenamiento fijos mejora significativamente la calidad de los resultados, muchos lo encontrar√≠an valioso. En nuestra opini√≥n, hay un potencial sustancial sin explotar en el escalamiento del c√≥mputo en tiempo de prueba para modelos de incrustaci√≥n. Esto representa un cambio de simplemente aumentar el tama√±o del modelo durante el entrenamiento a mejorar el esfuerzo computacional durante la fase de inferencia para lograr un mejor rendimiento.</p><h2 id=\"conclusion\">Conclusi√≥n</h2><p>Nuestro caso de estudio sobre el c√≥mputo en tiempo de prueba de <code>jina-clip-v1/v2</code> muestra varios hallazgos clave:</p><ol><li>Logramos un mejor rendimiento en datos poco comunes o fuera de distribuci√≥n (OOD) sin ning√∫n ajuste fino o post-entrenamiento en las incrustaciones.</li><li>El sistema hizo distinciones m√°s matizadas al refinar iterativamente las b√∫squedas de similitud y los criterios de clasificaci√≥n.</li><li>Al incorporar ajustes din√°micos de prompts y razonamiento iterativo, transformamos el proceso de inferencia del modelo de incrustaci√≥n de una sola consulta en una cadena de pensamiento m√°s sofisticada.</li></ol><p>Este caso de estudio apenas rasca la superficie de lo que es posible con el c√≥mputo en tiempo de prueba. Queda espacio sustancial para escalar algor√≠tmicamente. Por ejemplo, podr√≠amos desarrollar m√©todos para seleccionar iterativamente preguntas que reduzcan m√°s eficientemente el espacio de respuestas, similar a la estrategia √≥ptima en el juego de \"Veinte Preguntas\". Al escalar el c√≥mputo en tiempo de prueba, podemos empujar los modelos de incrustaci√≥n m√°s all√° de sus limitaciones actuales y permitirles abordar tareas m√°s complejas y matizadas que una vez parec√≠an fuera de alcance.</p>",
  "comment_id": "675a84f80ce9930001b86f09",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-12T07:38:48.000+01:00",
  "updated_at": "2024-12-12T17:54:17.000+01:00",
  "published_at": "2024-12-12T17:54:17.000+01:00",
  "custom_excerpt": "Better results scale with compute‚Äîmore on learning, more on search. A good pretrained model takes you far, but test-time compute takes you further. It's important to recognize this new paradigm of scaling test-time compute, even for embedding models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/scaling-test-time-compute-for-embedding-models/",
  "excerpt": "Los mejores resultados escalan con la capacidad de c√≥mputo‚Äîm√°s en aprendizaje, m√°s en b√∫squeda. Un buen modelo pre-entrenado te lleva lejos, pero el c√≥mputo en tiempo de prueba te lleva a√∫n m√°s lejos. Es importante reconocer este nuevo paradigma de escalar el c√≥mputo en tiempo de prueba, incluso para modelos de embeddings.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
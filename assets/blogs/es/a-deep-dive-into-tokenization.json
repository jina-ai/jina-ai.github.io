{
  "slug": "a-deep-dive-into-tokenization",
  "id": "65afb3ee8da8040001e17061",
  "uuid": "02d119e4-ed5f-4edf-8b66-65aea1386d96",
  "title": "Un an√°lisis profundo de la tokenizaci√≥n",
  "html": "<p>Hay muchas barreras para entender los modelos de IA, algunas de ellas bastante grandes, y pueden obstaculizar la implementaci√≥n de procesos de IA. Pero la primera que muchas personas encuentran es entender a qu√© nos referimos cuando hablamos de <strong>tokens</strong>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">Free API to tokenize texts, count and get first/last-N tokens.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\"></div></a></figure><p>Uno de los par√°metros pr√°cticos m√°s importantes al elegir un modelo de lenguaje de IA es el tama√±o de su ventana de contexto ‚Äî el tama√±o m√°ximo del texto de entrada ‚Äî que se da en tokens, no en palabras o caracteres ni ninguna otra unidad autom√°ticamente reconocible.</p><p>Adem√°s, los servicios de embeddings t√≠picamente se calculan \"por token\", lo que significa que los tokens son importantes para entender tu factura.</p><p>Esto puede ser muy confuso si no tienes claro qu√© es un token.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png\" class=\"kg-image\" alt=\"Tabla de precios actual de Jina Embeddings (a febrero de 2024).\" loading=\"lazy\" width=\"2000\" height=\"1036\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Tabla de precios actual de Jina Embeddings (a febrero de 2024). N√≥tese que los precios se indican por \"1M tokens\".</span></figcaption></figure><p>Pero de todos los aspectos confusos de la IA moderna, los tokens son probablemente los menos complicados. Este art√≠culo intentar√° aclarar qu√© es la tokenizaci√≥n, qu√© hace y por qu√© lo hacemos de esta manera.</p><h2 id=\"tldr\">tl;dr</h2><p>Para aquellos que quieren o necesitan una respuesta r√°pida para calcular cu√°ntos tokens comprar de Jina Embeddings o una estimaci√≥n de cu√°ntos necesitar√°n comprar, las siguientes estad√≠sticas son lo que est√°n buscando.</p><h3 id=\"tokens-per-english-word\">Tokens por Palabra en Ingl√©s</h3><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Una llamada a la API de Jina Embeddings v2 para modelos en ingl√©s usar√° <b><strong style=\"white-space: pre-wrap;\">aproximadamente</strong></b> <b><strong style=\"white-space: pre-wrap;\">10% m√°s</strong></b> tokens que el n√∫mero de palabras en tu texto, <b><strong style=\"white-space: pre-wrap;\">m√°s dos tokens por embedding</strong></b>.</div></div><p>Durante pruebas emp√≠ricas, descritas m√°s adelante en este art√≠culo, una variedad de textos en ingl√©s se convirtieron en tokens a una tasa de aproximadamente 10% m√°s tokens que palabras, usando los modelos solo en ingl√©s de Jina Embeddings. Este resultado fue bastante robusto.</p><p>Los modelos Jina Embeddings v2 tienen una ventana de contexto de 8192 tokens. Esto significa que si pasas a un modelo Jina un texto en ingl√©s m√°s largo que 7,400 palabras, hay una buena probabilidad de que sea truncado.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">El tama√±o m√°ximo de entrada para <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 para ingl√©s</strong></b> es aproximadamente <b><strong style=\"white-space: pre-wrap;\">7,400 palabras</strong></b>.</div></div><h3 id=\"tokens-per-chinese-character\">Tokens por Car√°cter Chino</h3><p>Para el chino, los resultados son m√°s variables. Dependiendo del tipo de texto, las proporciones variaron de 0.6 a 0.75 tokens por car√°cter chino (Ê±âÂ≠ó). Los textos en ingl√©s dados a Jina Embeddings v2 para chino producen aproximadamente el mismo n√∫mero de tokens que Jina Embeddings v2 para ingl√©s: aproximadamente 10% m√°s que el n√∫mero de palabras.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">El tama√±o m√°ximo de entrada en chino para <b><strong style=\"white-space: pre-wrap;\">Jina Embeddings v2 para chino e ingl√©s</strong></b> es aproximadamente <b><strong style=\"white-space: pre-wrap;\">10,500 caracteres</strong></b> (<b><strong style=\"white-space: pre-wrap;\">Â≠óÊï∞</strong></b>), o <b><strong style=\"white-space: pre-wrap;\">0.6 a 0.75 tokens por car√°cter chino, m√°s dos por embedding.</strong></b></div></div><h3 id=\"tokens-per-german-word\">Tokens por Palabra en Alem√°n</h3><p>Las proporciones de palabra a token en alem√°n son m√°s variables que en ingl√©s pero menos que en chino. Dependiendo del g√©nero del texto, obtuve en promedio entre 20% y 30% m√°s tokens que palabras. Dar textos en ingl√©s a Jina Embeddings v2 para alem√°n e ingl√©s usa algunos tokens m√°s que los modelos solo en ingl√©s y chino/ingl√©s: 12% a 15% m√°s tokens que palabras.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Jina Embeddings v2 para alem√°n e ingl√©s contar√° <b><strong style=\"white-space: pre-wrap;\">20% a 30% m√°s tokens que palabras, m√°s dos por embedding</strong></b>. El tama√±o m√°ximo del contexto de entrada es aproximadamente <b><strong style=\"white-space: pre-wrap;\">6,300 palabras en alem√°n</strong></b>.</div></div><h3 id=\"caution\">¬°Precauci√≥n!</h3><p>Estos son c√°lculos simples, pero deber√≠an ser aproximadamente correctos para la mayor√≠a de los textos en lenguaje natural y la mayor√≠a de los usuarios. En √∫ltima instancia, solo podemos prometer que el n√∫mero de tokens siempre ser√° no m√°s que el n√∫mero de caracteres en tu texto, m√°s dos. Pr√°cticamente siempre ser√° mucho menos que eso, pero no podemos prometer ning√∫n conteo espec√≠fico por adelantado.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">¬°Los Resultados Pueden Variar! </strong></b><br><br>Estas son estimaciones basadas en c√°lculos estad√≠sticamente ingenuos. No garantizamos cu√°ntos tokens requerir√° una solicitud particular.</div></div><p>Si todo lo que necesitas es consejo sobre cu√°ntos tokens comprar para Jina Embeddings, puedes detenerte aqu√≠. Otros modelos de embeddings, de compa√±√≠as diferentes a Jina AI, pueden no tener las mismas proporciones de token a palabra y token a car√°cter chino que tienen los modelos Jina, pero generalmente no ser√°n muy diferentes en general.</p><p>Si quieres entender por qu√©, el resto de este art√≠culo es una inmersi√≥n m√°s profunda en la tokenizaci√≥n para modelos de lenguaje.</p><h2 id=\"words-tokens-numbers\">Palabras, Tokens, N√∫meros</h2><p>La tokenizaci√≥n ha sido parte del procesamiento del lenguaje natural durante m√°s tiempo que la existencia de los modelos modernos de IA.</p><p>Es un poco clich√© decir que todo en una computadora es solo un n√∫mero, pero tambi√©n es mayormente cierto. El lenguaje, sin embargo, no es naturalmente solo un mont√≥n de n√∫meros. Puede ser habla, hecha de ondas sonoras, o escritura, hecha de marcas en papel, o incluso una imagen de un texto impreso o un video de alguien usando lenguaje de se√±as. Pero la mayor√≠a de las veces, cuando hablamos de usar computadoras para procesar lenguaje natural, nos referimos a textos compuestos de secuencias de caracteres: letras (a, b, c, etc.), n√∫meros (0, 1, 2‚Ä¶), puntuaci√≥n y espacios, en diferentes idiomas y codificaciones textuales.</p><p>Los ingenieros de computaci√≥n los llaman \"strings\".</p><p>Los modelos de lenguaje de IA toman secuencias de n√∫meros como entrada. As√≠ que, podr√≠as escribir la oraci√≥n:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Pero, despu√©s de la tokenizaci√≥n, el modelo de IA recibe como entrada:</p><pre><code class=\"language-python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>La tokenizaci√≥n es el proceso de convertir una cadena de entrada en una secuencia espec√≠fica de n√∫meros que tu modelo de IA puede entender.</p><p>Cuando usas un modelo de IA a trav√©s de una API web que cobra a los usuarios por token, cada solicitud se convierte en una secuencia de n√∫meros como la anterior. El n√∫mero de tokens en la solicitud es la longitud de esa secuencia de n√∫meros. As√≠, pedir a Jina Embeddings v2 para ingl√©s que te d√© un embedding para \"<em>What is today's weather in Berlin?</em>\" te costar√° 11 tokens porque convirti√≥ esa oraci√≥n en una secuencia de 11 n√∫meros antes de pasarla al modelo de IA.</p><p>Los modelos de IA basados en la <a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?ref=jina-ai-gmbh.ghost.io\">arquitectura Transformer</a> tienen una <strong>ventana de contexto</strong> de tama√±o fijo cuyo tama√±o se mide en tokens. A veces esto se llama \"ventana de entrada\", \"tama√±o de contexto\" o \"longitud de secuencia\" (especialmente en el <a href=\"https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io\">leaderboard MTEB de Hugging Face</a>). Significa el tama√±o m√°ximo de texto que el modelo puede ver a la vez.</p><p>As√≠ que, si quieres usar un modelo de embeddings, este es el tama√±o m√°ximo de entrada permitido.</p><p>Los modelos Jina Embeddings v2 tienen todos una ventana de contexto de 8,192 tokens. Otros modelos tendr√°n diferentes (t√≠picamente m√°s peque√±as) ventanas de contexto. Esto significa que sin importar cu√°nto texto le introduzcas, el tokenizador asociado con ese modelo Jina Embeddings debe convertirlo en no m√°s de 8,192 tokens.</p><h2 id=\"mapping-language-to-numbers\">Mapeando Lenguaje a N√∫meros</h2><p>La forma m√°s simple de explicar la l√≥gica de los tokens es esta:</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Un token es un n√∫mero que representa una parte de una cadena.</div></div><p>Para modelos de lenguaje natural, la parte de una cadena que un token representa es una palabra, una parte de una palabra, o una pieza de puntuaci√≥n. Los espacios generalmente no reciben ninguna representaci√≥n expl√≠cita en la salida del tokenizador.</p><p>La tokenizaci√≥n es parte de un grupo de t√©cnicas en procesamiento de lenguaje natural llamadas <a href=\"https://en.wikipedia.org/wiki/Text_segmentation?ref=jina-ai-gmbh.ghost.io\"><em>segmentaci√≥n de texto</em></a>, y el m√≥dulo que realiza la tokenizaci√≥n se llama, muy l√≥gicamente, un <strong>tokenizador</strong>.</p><p>Para mostrar c√≥mo funciona la tokenizaci√≥n, vamos a tokenizar algunas oraciones usando el modelo m√°s peque√±o de Jina Embeddings v2 para ingl√©s: <code>jina-embeddings-v2-small-en</code>. El otro modelo solo en ingl√©s de Jina Embeddings ‚Äî <code>jina-embeddings-v2-base-en</code> ‚Äî usa el mismo tokenizador, as√≠ que no tiene sentido descargar megabytes extra de modelo de IA que no usaremos en este art√≠culo.</p><p>Primero, instala el m√≥dulo <code>transformers</code> en tu entorno Python o notebook. Usa elLa bandera <code>-U</code> para asegurarse de actualizar a la √∫ltima versi√≥n ya que este modelo no funcionar√° con algunas versiones anteriores:</p><pre><code class=\"language-bash\">pip install -U transformers\n</code></pre><p>Luego, descarga <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><code>jina-embeddings-v2-small-en</code></a> usando <code>AutoModel.from_pretrained</code>:</p><pre><code class=\"language-Python\">from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n</code></pre><p>Para tokenizar una cadena, usa el m√©todo <code>encode</code> del objeto miembro <code>tokenizer</code> del modelo:</p><pre><code class=\"language-Python\">model.tokenizer.encode(\"What is today's weather in Berlin?\")\n</code></pre><p>El resultado es una lista de n√∫meros:</p><pre><code class=\"language-Python\">[101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102]\n</code></pre><p>Para convertir estos n√∫meros de vuelta a forma de cadenas, usa el m√©todo <code>convert_ids_to_tokens</code> del objeto <code>tokenizer</code>:</p><pre><code class=\"language-Python\">model.tokenizer.convert_ids_to_tokens([101, 2054, 2003, 2651, 1005, 1055, 4633, 1999, 4068, 1029, 102])\n</code></pre><p>El resultado es una lista de cadenas:</p><pre><code class=\"language-Python\">['[CLS]', 'what', 'is', 'today', \"'\", 's', 'weather', 'in',\n 'berlin', '?', '[SEP]']\n</code></pre><p>Ten en cuenta que el tokenizador del modelo ha:</p><ol><li>Agregado <code>[CLS]</code> al inicio y <code>[SEP]</code> al final. Esto es necesario por razones t√©cnicas y significa que <strong>cada solicitud de embedding costar√° dos tokens extra</strong>, adem√°s de los tokens que requiera el texto.</li><li>Separado la puntuaci√≥n de las palabras, convirtiendo \"<em>Berlin?</em>\" en: <code>berlin</code> y <code>?</code>, y \"<em>today's</em>\" en <code>today</code>, <code>'</code>, y <code>s</code>.</li><li>Puesto todo en min√∫sculas. No todos los modelos hacen esto, pero puede ayudar con el entrenamiento cuando se usa ingl√©s. Puede ser menos √∫til en idiomas donde la capitalizaci√≥n tiene un significado diferente.</li></ol><p>Diferentes algoritmos de conteo de palabras en diferentes programas pueden contar las palabras en esta oraci√≥n de manera diferente. OpenOffice la cuenta como seis palabras. El algoritmo de segmentaci√≥n de texto Unicode (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) cuenta siete palabras. Otro software puede llegar a otros n√∫meros, dependiendo de c√≥mo manejen la puntuaci√≥n y los cl√≠ticos como \"'s\".</p><p>El tokenizador para este modelo produce nueve tokens para esas seis o siete palabras, m√°s los dos tokens extra necesarios con cada solicitud.</p><p>Ahora, probemos con un nombre de lugar menos com√∫n que Berl√≠n:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"I live in Kinshasa.\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>El resultado:</p><pre><code class=\"language-Python\">['[CLS]', 'i', 'live', 'in', 'kin', '##sha', '##sa', '.', '[SEP]']\n</code></pre><p>El nombre \"Kinshasa\" se divide en tres tokens: <code>kin</code>, <code>##sha</code>, y <code>##sa</code>. El <code>##</code> indica que este token no es el comienzo de una palabra.</p><p>Si le damos al tokenizador algo completamente extra√±o, el n√∫mero de tokens sobre el n√∫mero de palabras aumenta a√∫n m√°s:</p><pre><code class=\"language-Python\">token_ids = model.tokenizer.encode(\"Klaatu barada nikto\")\ntokens = model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n\n['[CLS]', 'k', '##la', '##at', '##u', 'bar', '##ada', 'nik', '##to', '[SEP]']\n</code></pre><p>Tres palabras se convierten en ocho tokens, m√°s los tokens <code>[CLS]</code> y <code>[SEP]</code>.</p><p>La tokenizaci√≥n en alem√°n es similar. Con el modelo <a href=\"https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Jina Embeddings v2 para alem√°n</a>, podemos tokenizar una traducci√≥n de \"What is today's weather in Berlin?\" de la misma manera que con el modelo en ingl√©s.</p><pre><code class=\"language-Python\">german_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-de', trust_remote_code=True)\ntoken_ids = german_model.tokenizer.encode(\"Wie wird das Wetter heute in Berlin?\")\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>El resultado:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Wie', 'wird', 'das', 'Wetter', 'heute', 'in', 'Berlin', '?', '&lt;/s&gt;']\n</code></pre><p>Este tokenizador es un poco diferente del ingl√©s en que <code>&lt;s&gt;</code> y <code>&lt;/s&gt;</code> reemplazan a <code>[CLS]</code> y <code>[SEP]</code> pero cumplen la misma funci√≥n. Adem√°s, el texto no est√° normalizado en cuanto a may√∫sculas y min√∫sculas ‚Äî las may√∫sculas y min√∫sculas permanecen como est√°n escritas ‚Äî porque la capitalizaci√≥n es significativa en alem√°n de manera diferente al ingl√©s.</p><p>(Para simplificar esta presentaci√≥n, he eliminado un car√°cter especial que indica el comienzo de una palabra.)</p><p>Ahora, probemos con una oraci√≥n m√°s compleja <a href=\"https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html?ref=jina-ai-gmbh.ghost.io\">de un texto period√≠stico</a>:</p><blockquote>Ein Gro√üteil der milliardenschweren Bauern-Subventionen bleibt liegen ‚Äì zu genervt sind die Landwirte von b√ºrokratischen G√§ngelungen und Regelwahn.</blockquote><pre><code>sentence = \"\"\"\nEin Gro√üteil der milliardenschweren Bauern-Subventionen\nbleibt liegen ‚Äì zu genervt sind die Landwirte von \nb√ºrokratischen G√§ngelungen und Regelwahn.\n\"\"\"\ntoken_ids = german_model.tokenizer.encode(sentence)\ntokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)</code></pre><p>El resultado tokenizado:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Ein', 'Gro√üteil', 'der', 'mill', 'iarden', 'schwer', \n 'en', 'Bauern', '-', 'Sub', 'ventionen', 'bleibt', 'liegen', \n '‚Äì', 'zu', 'gen', 'ervt', 'sind', 'die', 'Landwirte', 'von', \n 'b√ºro', 'krat', 'ischen', 'G√§n', 'gel', 'ungen', 'und', 'Regel', \n 'wahn', '.', '&lt;/s&gt;']\n</code></pre><p>Aqu√≠, puedes ver que muchas palabras alemanas fueron divididas en piezas m√°s peque√±as y no necesariamente siguiendo las reglas gramaticales alemanas. El resultado es que una palabra larga en alem√°n que contar√≠a como una sola palabra para un contador de palabras podr√≠a ser cualquier n√∫mero de tokens para el modelo de IA de Jina.</p><p>Hagamos lo mismo en chino, traduciendo \"What is today's weather in Berlin?\" como:</p><blockquote>ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü</blockquote><pre><code>chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)\ntoken_ids = chinese_model.tokenizer.encode(\"ÊüèÊûó‰ªäÂ§©ÁöÑÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü\")\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>El resultado tokenizado:</p><pre><code class=\"language-Python\">['&lt;s&gt;', 'ÊüèÊûó', '‰ªäÂ§©ÁöÑ', 'Â§©Ê∞î', 'ÊÄé‰πàÊ†∑', 'Ôºü', '&lt;/s&gt;']\n</code></pre><p>En chino, normalmente no hay separaciones de palabras en el texto escrito, pero el tokenizador de Jina Embeddings frecuentemente une m√∫ltiples caracteres chinos:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Token string</th>\n<th>Pinyin</th>\n<th>Meaning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ÊüèÊûó</td>\n<td>B√≥l√≠n</td>\n<td>Berlin</td>\n</tr>\n<tr>\n<td>‰ªäÂ§©ÁöÑ</td>\n<td>jƒ´ntiƒÅn de</td>\n<td>today's</td>\n</tr>\n<tr>\n<td>Â§©Ê∞î</td>\n<td>tiƒÅnq√¨</td>\n<td>weather</td>\n</tr>\n<tr>\n<td>ÊÄé‰πàÊ†∑</td>\n<td>zƒõnmey√†ng</td>\n<td>how</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Usemos una oraci√≥n m√°s compleja <a href=\"https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7?ref=jina-ai-gmbh.ghost.io\">de un peri√≥dico de Hong Kong</a>:</p><pre><code class=\"language-Python\">sentence = \"\"\"\nÊñ∞Ë¶èÂÆöÂü∑Ë°åÈ¶ñÊó•ÔºåË®òËÄÖÂú®‰∏ãÁè≠È´òÂ≥∞ÂâçÁöÑ‰∏ãÂçà5ÊôÇ‰æÜÂà∞Âª£Â∑ûÂú∞Èêµ3ËôüÁ∑öÔºå\nÂæûÁπÅÂøôÁöÑÁè†Ê±üÊñ∞ÂüéÁ´ôÂïüÁ®ãÔºåÂêëÊ©üÂ†¥ÂåóÊñπÂêëÂá∫Áôº„ÄÇ\n\"\"\"\ntoken_ids = chinese_model.tokenizer.encode(sentence)\ntokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens)\n</code></pre><p>(Traducci√≥n: <em>\"El primer d√≠a que las nuevas regulaciones entraron en vigor, este reportero lleg√≥ a la L√≠nea 3 del Metro de Guangzhou a las 5 p.m., durante la hora pico, habiendo partido de la Estaci√≥n Zhujiang New Town en direcci√≥n norte hacia el aeropuerto.\"</em>)</p><p>El resultado:</p><pre><code class=\"language-python\">['&lt;s&gt;', 'Êñ∞', 'Ë¶èÂÆö', 'Âü∑Ë°å', 'È¶ñ', 'Êó•', 'Ôºå', 'Ë®òËÄÖ', 'Âú®‰∏ã', 'Áè≠', \n 'È´òÂ≥∞', 'ÂâçÁöÑ', '‰∏ãÂçà', '5', 'ÊôÇ', '‰æÜÂà∞', 'Âª£Â∑û', 'Âú∞', 'Èêµ', '3', \n 'Ëôü', 'Á∑ö', 'Ôºå', 'Âæû', 'ÁπÅÂøô', 'ÁöÑ', 'Áè†Ê±ü', 'Êñ∞Âüé', 'Á´ô', 'Âïü', \n 'Á®ã', 'Ôºå', 'Âêë', 'Ê©üÂ†¥', 'Âåó', 'ÊñπÂêë', 'Âá∫Áôº', '„ÄÇ', '&lt;/s&gt;']\n</code></pre><p>Estos tokens no se corresponden con ning√∫n diccionario espec√≠fico de palabras chinas (ËØçÂÖ∏). Por ejemplo, \"ÂïüÁ®ã\" - <em>q«êch√©ng</em> (partir, emprender) normalmente se categorizar√≠a como una sola palabra pero aqu√≠ est√° dividida en sus dos caracteres constituyentes. De manera similar, \"Âú®‰∏ãÁè≠\" usualmente ser√≠a reconocido como dos palabras, con la divisi√≥n entre \"Âú®\" - <em>z√†i</em> (en, durante) y \"‰∏ãÁè≠\" - <em>xi√†bƒÅn</em> (final de la jornada laboral, hora pico), no entre \"Âú®‰∏ã\" y \"Áè≠\" como lo ha hecho el tokenizador aqu√≠.</p><p>En los tres idiomas, los lugares donde el tokenizador divide el texto no est√°n directamente relacionados con los lugares l√≥gicos donde un lector humano los dividir√≠a.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">El algoritmo tokenizador no usa un diccionario convencional consciente del lenguaje, por lo que su comportamiento no coincide con la forma en que los humanos cuentan las palabras.</div></div><p>Esto no es una caracter√≠stica espec√≠fica de los modelos Jina Embeddings. Este enfoque de tokenizaci√≥n es casi universal en el desarrollo de modelos de IA. Aunque dos modelos de IA diferentes pueden no tener tokenizadores id√©nticos, en el estado actual de desarrollo, pr√°cticamente todos usar√°n tokenizadores con este tipo de comportamiento.</p><p>La siguiente secci√≥n discutir√° el algoritmo espec√≠fico usado en la tokenizaci√≥n y la l√≥gica detr√°s de √©l.</p><h2 id=\"why-do-we-tokenize-and-why-this-way\">¬øPor qu√© Tokenizamos? ¬øY Por qu√© de Esta Manera?</h2><p>Los modelos de lenguaje de IA toman como entrada secuencias de n√∫meros que representan secuencias de texto, pero ocurren m√°s cosas antes de ejecutar la red neuronal subyacente y crear un embedding. Cuando se presenta una lista de n√∫meros que representan peque√±as secuencias de texto, el modelo busca cada n√∫mero en un diccionario interno que almacena un vector √∫nico para cada n√∫mero. Luego los combina, y eso se convierte en la entrada de la red neuronal.</p><p>Esto significa que el tokenizador <strong>debe</strong> poder convertir <strong><em>cualquier</em></strong> texto de entrada que le demos en tokens que aparezcan en el diccionario de vectores de tokens del modelo. Si tom√°ramos nuestros tokens de un diccionario convencional, la primera vez que encontr√°ramos un error ortogr√°fico o un nombre propio raro o una palabra extranjera, todo el modelo se detendr√≠a. No podr√≠a procesar esa entrada.</p><p>En el procesamiento del lenguaje natural, esto se llama el problema del vocabulario fuera de vocabulario (OOV), y est√° presente en todos los tipos de texto y todos los idiomas. Hay algunas estrategias para abordar el problema OOV:</p><ol><li>Ignorarlo. Reemplazar todo lo que no est√° en el diccionario con un token \"desconocido\".</li><li>Evitarlo. En lugar de usar un diccionario que mapee secuencias de texto a vectores, usar uno que mapee <em>caracteres individuales</em> a vectores. El ingl√©s solo usa 26 letras la mayor√≠a del tiempo, por lo que esto debe ser m√°s peque√±o y m√°s robusto contra problemas OOV que cualquier diccionario.</li><li>Encontrar subsecuencias frecuentes en el texto, ponerlas en el diccionario y usar caracteres (tokens de una sola letra) para lo que quede.</li></ol><p>La primera estrategia significa que se pierde mucha informaci√≥n importante. El modelo ni siquiera puede aprender sobre los datos que ha visto si toman la forma de algo que no est√° en el diccionario. Muchas cosas en el texto ordinario simplemente no est√°n presentes ni siquiera en los diccionarios m√°s grandes.</p><p>La segunda estrategia es posible, y los investigadores la han estudiado. Sin embargo, significa que el modelo tiene que aceptar muchas m√°s entradas y tiene que aprender mucho m√°s. Esto significa un modelo mucho m√°s grande y muchos m√°s datos de entrenamiento para un resultado que nunca ha demostrado ser mejor que la tercera estrategia.</p><p>Los modelos de lenguaje de IA pr√°cticamente todos implementan la tercera estrategia de alguna forma. La mayor√≠a usa alguna variante del <a href=\"https://huggingface.co/learn/nlp-course/chapter6/6?ref=jina-ai-gmbh.ghost.io\">algoritmo Wordpiece</a> <a href=\"https://ieeexplore.ieee.org/document/6289079?ref=jina-ai-gmbh.ghost.io\">[Schuster y Nakajima 2012]</a> o una t√©cnica similar llamada <a href=\"https://en.wikipedia.org/wiki/Byte_pair_encoding?ref=jina-ai-gmbh.ghost.io\">Codificaci√≥n por Pares de Bytes</a> (BPE). [<a href=\"https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829?ref=jina-ai-gmbh.ghost.io\">Gage 1994</a>, <a href=\"https://aclanthology.org/P16-1162/?ref=jina-ai-gmbh.ghost.io\">Senrich et al. 2016</a>] Estos algoritmos son <em>agn√≥sticos al lenguaje</em>. Eso significa que funcionan igual para todos los lenguajes escritos sin ning√∫n conocimiento m√°s all√° de una lista exhaustiva de posibles caracteres. Fueron dise√±ados para modelos multiling√ºes como BERT de Google que toman cualquier entrada del raspado de Internet ‚Äî cientos de idiomas y textos que no son lenguaje humano como programas de computadora ‚Äî para que pudieran ser entrenados sin hacer ling√º√≠stica complicada.</p><p>Algunas investigaciones muestran mejoras significativas usando tokenizadores m√°s espec√≠ficos y conscientes del lenguaje. [<a href=\"https://aclanthology.org/2021.acl-long.243/?ref=jina-ai-gmbh.ghost.io\">Rust et al. 2021</a>] Pero construir tokenizadores de esa manera requiere tiempo, dinero y experiencia. Implementar una estrategia universal como BPE o Wordpiece es mucho m√°s barato y f√°cil.</p><p>Sin embargo, como consecuencia, no hay manera de saber cu√°ntos tokens representa un texto espec√≠fico m√°s que ejecutarlo a trav√©s de un tokenizador y luego contar el n√∫mero de tokens que salen de √©l. Debido a que la subsecuencia m√°s peque√±a posible de un texto es una letra, puedes estar seguro de que el n√∫mero de tokens no ser√° mayor que el n√∫mero de caracteres (menos espacios) m√°s dos.</p><p>Para obtener una buena estimaci√≥n, necesitamos enviar mucho texto a nuestro tokenizador y calcular emp√≠ricamente cu√°ntos tokens obtenemos en promedio, en comparaci√≥n con cu√°ntas palabras o caracteres ingresamos. En la siguiente secci√≥n, haremos algunas mediciones emp√≠ricas no muy sistem√°ticas para todos los modelos Jina Embeddings v2 actualmente disponibles.</p><h2 id=\"empirical-estimates-of-token-output-sizes\">Estimaciones Emp√≠ricas de los Tama√±os de Salida de Tokens</h2><p>Para ingl√©s y alem√°n, us√© el algoritmo de segmentaci√≥n de texto Unicode (<a href=\"https://unicode.org/reports/tr29/?ref=jina-ai-gmbh.ghost.io\">Unicode Standard Annex #29</a>) para obtener el conteo de palabras de los textos. Este algoritmo es ampliamente utilizado para seleccionar fragmentos de texto cuando haces doble clic en algo. Es lo m√°s cercano disponible a un contador de palabras objetivo universal.</p><p>Instal√© la <a href=\"https://pypi.org/project/polyglot/?ref=jina-ai-gmbh.ghost.io\">biblioteca polyglot</a> en Python, que implementa este segmentador de texto:</p><pre><code class=\"language-bash\">pip install -U polyglot\n</code></pre><p>Para obtener el conteo de palabras de un texto, puedes usar c√≥digo como este fragmento:</p><pre><code class=\"language-python\">from polyglot.text import Text\n\ntxt = \"What is today's weather in Berlin?\"\nprint(len(Text(txt).words))\n</code></pre><p>El resultado deber√≠a ser <code>7</code>.</p><p>Para obtener un conteo de tokens, se pasaron segmentos del texto a los tokenizadores de varios modelos Jina Embeddings, como se describe a continuaci√≥n, y cada vez, rest√© dos del n√∫mero de tokens devueltos.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚ö†Ô∏è</div><div class=\"kg-callout-text\">Los conteos de tokens listados aqu√≠ <b><strong style=\"white-space: pre-wrap;\">no incluyen</strong></b> los dos tokens extra al principio y al final de cada texto tokenizado.</div></div><h3 id=\"english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en\">Ingl√©s<br>(<code>jina-embeddings-v2-small-en</code> y <code>jina-embeddings-v2-base-en</code>)</h3><p>Para calcular promedios, descargu√© dos corpus de texto en ingl√©s de <a href=\"https://wortschatz.uni-leipzig.de/en?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Wortschatz Leipzig</a>, una colecci√≥n de corpus descargables gratuitamente en varios idiomas y configuraciones alojada por la Universidad de Leipzig:</p><ul><li>Un corpus de un mill√≥n de oraciones de datos de noticias en ingl√©s de 2020 (<code>eng_news_2020_1M</code>)</li><li>Un corpus de un mill√≥n de oraciones de datos de <a href=\"https://en.wikipedia.org/?ref=jina-ai-gmbh.ghost.io\">Wikipedia en ingl√©s</a> de 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Ambos se pueden encontrar en <a href=\"https://wortschatz.uni-leipzig.de/en/download/English?ref=jina-ai-gmbh.ghost.io\">su p√°gina de descargas en ingl√©s</a>.</p><p>Para diversidad, tambi√©n descargu√© la <a href=\"https://www.gutenberg.org/ebooks/135?ref=jina-ai-gmbh.ghost.io\">traducci√≥n de Hapgood de <em>Los Miserables</em> de Victor Hugo</a> del Proyecto Gutenberg, y una copia de la Versi√≥n King James de la Biblia, traducida al ingl√©s en 1611.</p><p>Para los cuatro textos, cont√© las palabras usando el segmentador Unicode implementado en <code>polyglot</code>, luego cont√© los tokens generados por <code>jina-embeddings-v2-small-en</code>, restando dos tokens por cada solicitud de tokenizaci√≥n. Los resultados son los siguientes:</p>\n<!--kg-card-begin: html-->\n<table id=\"6f07d5d4-ca08-466e-92fc-e784a932e4d0\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b\"><th id=\"wvl`\" class=\"simple-table-header-color simple-table-header\">Texto</th><th id=\"|<X;\" class=\"simple-table-header-color simple-table-header\">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id=\"GHal\" class=\"simple-table-header-color simple-table-header\">Conteo de tokens<br>(Jina Embeddings v2 <br>para ingl√©s)<br></th><th id=\"h]mu\" class=\"simple-table-header-color simple-table-header\">Proporci√≥n de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id=\"7e9eda1b-54b6-40f3-be6f-b233f161e2b5\"><td id=\"wvl`\" class=\"\"><code>eng_news_2020_1M</code></td><td id=\"|<X;\" class=\"\">22.825.712</td><td id=\"GHal\" class=\"\">25.270.581</td><td id=\"h]mu\" class=\"\">1,107</td></tr><tr id=\"a81dfe1d-9143-4306-9bf3-4891ca8fb019\"><td id=\"wvl`\" class=\"\"><code>eng_wikipedia_2016_1M</code></td><td id=\"|<X;\" class=\"\">24.243.607</td><td id=\"GHal\" class=\"\">26.813.877</td><td id=\"h]mu\" class=\"\">1,106</td></tr><tr id=\"d2fff413-6e0d-4ab2-9626-4d618d99af91\"><td id=\"wvl`\" class=\"\"><code>les_miserables_en</code></td><td id=\"|<X;\" class=\"\">688.911</td><td id=\"GHal\" class=\"\">764.121</td><td id=\"h]mu\" class=\"\">1,109</td></tr><tr id=\"eb304e43-4fd3-4e02-9993-13fb0307f544\"><td id=\"wvl`\" class=\"\"><code>kjv_bible</code></td><td id=\"|<X;\" class=\"\">1.007.651</td><td id=\"GHal\" class=\"\">1.099.335</td><td id=\"h]mu\" class=\"\">1,091</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>El uso de n√∫meros precisos no significa que este sea un resultado exacto. El hecho de que documentos de g√©neros tan diferentes tengan entre 9% y 11% m√°s tokens que palabras indica que probablemente puedes esperar alrededor de 10% m√°s tokens que palabras, seg√∫n medido por el segmentador Unicode. Los procesadores de texto a menudo no cuentan la puntuaci√≥n, mientras que el segmentador Unicode s√≠ lo hace, por lo que no puedes esperar que los conteos de palabras del software de oficina coincidan necesariamente con esto.</p><h3 id=\"german-jina-embeddings-v2-base-de\">Alem√°n<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>Para el alem√°n, descargu√© tres corpus de la <a href=\"https://wortschatz.uni-leipzig.de/en/download/German?ref=jina-ai-gmbh.ghost.io\">p√°gina alemana de Wortschatz Leipzig</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> ‚Äî Un mill√≥n de oraciones de una mezcla equilibrada de textos en diferentes g√©neros, de 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> ‚Äî Un mill√≥n de oraciones de texto period√≠stico de 2019.</li><li><code>deu_wikipedia_2021_1M</code> ‚Äî Un mill√≥n de oraciones extra√≠das de la Wikipedia alemana en 2021.</li></ul><p>Y para diversidad, tambi√©n descargu√© los <a href=\"https://deutschestextarchiv.de/search?q=Kapital&in=metadata&ref=jina-ai-gmbh.ghost.io\">tres vol√∫menes de <em>El Capital</em> de Karl Marx</a> del <a href=\"https://www.deutschestextarchiv.de/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Deutsches Textarchiv</a>.</p><p>Luego segu√≠ el mismo procedimiento que para el ingl√©s:</p>\n<!--kg-card-begin: html-->\n<table id=\"ad695a91-f35b-4215-bd4d-5d1415bb9812\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"7786decb-f68d-433d-8f58-3861d0350027\"><th id=\"UGp`\" class=\"simple-table-header-color simple-table-header\" style=\"width:234.2265625px\">Texto</th><th id=\"|qln\" class=\"simple-table-header-color simple-table-header\">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id=\"YXZX\" class=\"simple-table-header-color simple-table-header\">Conteo de tokens<br>(Jina Embeddings v2 <br>para alem√°n e ingl√©s)<br></th><th id=\"oEoQ\" class=\"simple-table-header-color simple-table-header\">Ratio de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id=\"9cb48640-64db-4783-8bfe-c78412022a21\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_mixed-typical_2011_1M</code></td><td id=\"|qln\" class=\"\">7.924.024</td><td id=\"YXZX\" class=\"\">9.772.652</td><td id=\"oEoQ\" class=\"\">1,234</td></tr><tr id=\"32fee905-17dc-4c2c-a32d-5e6508b033bc\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_newscrawl-public_2019_1M</code></td><td id=\"|qln\" class=\"\">17.949.120</td><td id=\"YXZX\" class=\"\">21.711.555</td><td id=\"oEoQ\" class=\"\">1,210</td></tr><tr id=\"35d0c8c4-7912-4d61-829a-bb39b643aa1c\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>deu_wikipedia_2021_1M</code></td><td id=\"|qln\" class=\"\">17.999.482</td><td id=\"YXZX\" class=\"\">22.654.901</td><td id=\"oEoQ\" class=\"\">1,259</td></tr><tr id=\"19e10367-e070-4dcc-8cbe-cfc75c43e0f9\"><td id=\"UGp`\" class=\"\" style=\"width:234.2265625px\"><code>marx_kapital</code></td><td id=\"|qln\" class=\"\">784.336</td><td id=\"YXZX\" class=\"\">1.011.377</td><td id=\"oEoQ\" class=\"\">1,289</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Estos resultados tienen una dispersi√≥n mayor que el modelo solo en ingl√©s, pero aun as√≠ sugieren que el texto en alem√°n producir√°, en promedio, entre 20% y 30% m√°s tokens que palabras.</p><p>Los textos en ingl√©s producen m√°s tokens con el tokenizador alem√°n-ingl√©s que con el de solo ingl√©s:</p>\n<!--kg-card-begin: html-->\n<table id=\"c31b2079-e921-4e06-a24b-8ed60ae63d8d\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5\"><th id=\"Nc<l\" class=\"simple-table-header-color simple-table-header\" style=\"width:187.78125px\">Texto</th><th id=\"R@A^\" class=\"simple-table-header-color simple-table-header\">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id=\"UUfl\" class=\"simple-table-header-color simple-table-header\">Conteo de tokens<br>(Jina Embeddings v2 <br>para alem√°n e ingl√©s)<br></th><th id=\"iTZS\" class=\"simple-table-header-color simple-table-header\">Ratio de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id=\"3461fd8c-ca39-4670-8f0e-e38a4958464a\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_news_2020_1M</code></td><td id=\"R@A^\" class=\"\">24243607</td><td id=\"UUfl\" class=\"\">27758535</td><td id=\"iTZS\" class=\"\">1,145</td></tr><tr id=\"48770d4d-5855-4f5f-934f-5b2900aa56c3\"><td id=\"Nc<l\" class=\"\" style=\"width:187.78125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"R@A^\" class=\"\">22825712</td><td id=\"UUfl\" class=\"\">25566921</td><td id=\"iTZS\" class=\"\">1,120</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Deber√≠as esperar necesitar entre 12% y 15% m√°s tokens que palabras para embeber textos en ingl√©s con el modelo biling√ºe alem√°n/ingl√©s que con el de solo ingl√©s.</p><h3 id=\"chinese-jina-embeddings-v2-base-zh\">Chino<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>El chino t√≠picamente se escribe sin espacios y no ten√≠a una noci√≥n tradicional de \"palabras\" antes del siglo XX. En consecuencia, el tama√±o de un texto en chino se mide t√≠picamente en caracteres (<strong>Â≠óÊï∞</strong>). As√≠ que, en lugar de usar el segmentador Unicode, med√≠ la longitud de los textos en chino eliminando todos los espacios y luego simplemente obteniendo la longitud de caracteres.</p><p>Descargu√© tres corpus de la <a href=\"https://wortschatz.uni-leipzig.de/en/download/Chinese?ref=jina-ai-gmbh.ghost.io\">p√°gina de corpus chino de Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> ‚Äî Un mill√≥n de oraciones de la Wikipedia en chino, extra√≠das en 2018.</li><li><code>zho_news_2007-2009_1M</code> ‚Äî Un mill√≥n de oraciones de fuentes period√≠sticas chinas, recopiladas entre 2007 y 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> ‚Äî Un mill√≥n de oraciones de fuentes period√≠sticas que usan exclusivamente caracteres chinos tradicionales (ÁπÅÈ´îÂ≠ó).</li></ul><p>Adem√°s, para mayor diversidad, tambi√©n us√© <em>La verdadera historia de A Q</em> (ÈòøQÊ≠£ÂÇ≥), una novela corta de Lu Xun (È≠ØËøÖ) escrita a principios de la d√©cada de 1920. Descargu√© la <a href=\"https://www.gutenberg.org/ebooks/25332?ref=jina-ai-gmbh.ghost.io\">versi√≥n en caracteres tradicionales de Project Gutenberg</a>.</p>\n<!--kg-card-begin: html-->\n<table id=\"dace0ca3-97c0-481e-98e2-d2724b7bbe66\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"adc6e6ff-8afd-4915-8884-0894546a13dc\"><th id=\"bCvb\" class=\"simple-table-header-color simple-table-header\" style=\"width:223.6953125px\">Texto</th><th id=\"CaUc\" class=\"simple-table-header-color simple-table-header\">Conteo de caracteres<br>(Â≠óÊï∞)<br></th><th id=\"CQ{d\" class=\"simple-table-header-color simple-table-header\">Conteo de tokens<br>(Jina Embeddings v2 <br>para chino e ingl√©s)<br></th><th id=\"_};C\" class=\"simple-table-header-color simple-table-header\">Ratio de tokens a caracteres<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id=\"e75154ce-a33e-4af1-a983-4c4213f93c0e\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_wikipedia_2018_1M</code></td><td id=\"CaUc\" class=\"\">45.116.182</td><td id=\"CQ{d\" class=\"\">29.193.028</td><td id=\"_};C\" class=\"\">0,647</td></tr><tr id=\"605560a8-5c77-4add-a3e4-4615779b571a\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho_news_2007-2009_1M</code></td><td id=\"CaUc\" class=\"\">44.295.314</td><td id=\"CQ{d\" class=\"\">28.108.090</td><td id=\"_};C\" class=\"\">0,635</td></tr><tr id=\"6e23944e-a480-4978-8550-a83404b218c4\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>zho-trad_newscrawl_2011_1M</code></td><td id=\"CaUc\" class=\"\">54,585,819</td><td id=\"CQ{d\" class=\"\">40,290,982</td><td id=\"_};C\" class=\"\">0.738</td></tr><tr id=\"50abbb96-06f7-4308-9c66-7c18f2a67721\"><td id=\"bCvb\" class=\"\" style=\"width:223.6953125px\"><code>Ah_Q</code></td><td id=\"CaUc\" class=\"\">41,268</td><td id=\"CQ{d\" class=\"\">25,346</td><td id=\"_};C\" class=\"\">0.614</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Esta dispersi√≥n en las proporciones de tokens a caracteres es inesperada, y especialmente el valor at√≠pico para el corpus de caracteres tradicionales merece m√°s investigaci√≥n. No obstante, podemos concluir que para el chino, debes esperar necesitar <em>menos</em> tokens que caracteres hay en tu texto. Dependiendo de tu contenido, puedes esperar necesitar entre un 25% y un 40% menos.</p><p>Los textos en ingl√©s en Jina Embeddings v2 para chino e ingl√©s produjeron aproximadamente el mismo n√∫mero de tokens que en el modelo solo para ingl√©s:</p>\n<!--kg-card-begin: html-->\n<table id=\"061e7c3f-d109-476d-85fb-db3b369e4f35\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"1200d074-3353-4815-ab66-a90e93ec349d\"><th id=\"v\\xv\" class=\"simple-table-header-color simple-table-header\" style=\"width:184.53125px\">Text</th><th id=\"qlUV\" class=\"simple-table-header-color simple-table-header\" style=\"width:165.3125px\">Word count<br>(Unicode Segmenter)<br></th><th id=\"=]?F\" class=\"simple-table-header-color simple-table-header\">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id=\"<rlw\" class=\"simple-table-header-color simple-table-header\">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id=\"2fe4e02d-94fd-4513-bfcb-7f85d66b6883\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_news_2020_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">24,243,607</td><td id=\"=]?F\" class=\"\">26,890,176</td><td id=\"<rlw\" class=\"\">1.109</td></tr><tr id=\"e7f937f4-b156-4f5d-9e0b-3041d07b1b20\"><td id=\"v\\xv\" class=\"\" style=\"width:184.53125px\"><code>eng_wikipedia_2016_1M</code></td><td id=\"qlUV\" class=\"\" style=\"width:165.3125px\">22,825,712</td><td id=\"=]?F\" class=\"\">25,060,352</td><td id=\"<rlw\" class=\"\">1.097</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h2 id=\"taking-tokens-seriously\">Tomando los Tokens en Serio</h2><p>Los tokens son un andamiaje importante para los modelos de lenguaje de IA, y la investigaci√≥n en esta √°rea contin√∫a.</p><p>Uno de los aspectos donde los modelos de IA han demostrado ser revolucionarios es el descubrimiento de que son muy robustos frente a datos ruidosos. Incluso si un modelo particular no utiliza la estrategia de tokenizaci√≥n √≥ptima, si la red es lo suficientemente grande, tiene suficientes datos y est√° adecuadamente entrenada, puede aprender a hacer lo correcto a partir de una entrada imperfecta.</p><p>En consecuencia, se dedica mucho menos esfuerzo a mejorar la tokenizaci√≥n que en otras √°reas, pero esto podr√≠a cambiar.</p><p>Como usuario de embeddings, que los compras a trav√©s de una <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">API como Jina Embeddings</a>, no puedes saber exactamente cu√°ntos tokens necesitar√°s para una tarea espec√≠fica y es posible que tengas que hacer algunas pruebas propias para obtener n√∫meros s√≥lidos. Pero las estimaciones proporcionadas aqu√≠ ‚Äî aproximadamente 110% del recuento de palabras para ingl√©s, aproximadamente 125% del recuento de palabras para alem√°n, y aproximadamente 70% del recuento de caracteres para chino ‚Äî deber√≠an ser suficientes para un presupuesto b√°sico.</p>",
  "comment_id": "65afb3ee8da8040001e17061",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-01-23T13:41:18.000+01:00",
  "updated_at": "2024-08-14T11:38:01.000+02:00",
  "published_at": "2024-01-31T16:10:14.000+01:00",
  "custom_excerpt": "Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/a-deep-dive-into-tokenization/",
  "excerpt": "La tokenizaci√≥n, en los LLMs, significa dividir los textos de entrada en partes m√°s peque√±as para su procesamiento. Entonces, ¬øpor qu√© los embeddings se cobran por token?",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect.",
  "feature_image_caption": null
}
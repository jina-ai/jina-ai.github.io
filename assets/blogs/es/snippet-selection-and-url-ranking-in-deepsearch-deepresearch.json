{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "Selecci√≥n de Fragmentos y Clasificaci√≥n de URLs en DeepSearch/DeepResearch",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Una Gu√≠a Pr√°ctica para Implementar DeepSearch/DeepResearch</div><div class=\"kg-bookmark-description\">QPS fuera, profundidad dentro. DeepSearch es la nueva norma. Encuentra respuestas a trav√©s de ciclos de lectura-b√∫squeda-razonamiento. Aprende qu√© es y c√≥mo construirlo.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Si ya has le√≠do nuestra gu√≠a de implementaci√≥n de DeepSearch/DeepResearch, profundicemos en algunos detalles que pueden mejorar <em>enormemente</em> la calidad. En esta publicaci√≥n, nos centraremos en dos desaf√≠os clave: <strong>aprovechar los embeddings para la selecci√≥n de fragmentos de p√°ginas web extensas</strong> y <strong>usar rerankers para priorizar URLs para el rastreo.</strong></p><p>Algunos recordar√°n nuestra conclusi√≥n anterior que indicaba que \"los embeddings solo eran √∫tiles para la deduplicaci√≥n de consultas como tareas STS (similitud textual sem√°ntica), mientras que los rerankers ni siquiera formaban parte de nuestra implementaci√≥n original de DeepSearch\". Bueno, resulta que ambos siguen siendo bastante valiosos, solo que no de la manera convencional que uno podr√≠a esperar. Siempre hemos seguido el camino m√°s <em>eficiente</em> posible. No agregamos componentes solo para justificar su existencia o nuestro valor como proveedor de embeddings y rerankers. <strong>Nos basamos en lo que la b√∫squeda realmente necesita en su fundamento.</strong></p><p>As√≠ que despu√©s de semanas de experimentos e iteraciones, hemos descubierto usos poco comunes pero efectivos para ambos en sistemas DeepSearch/DeepResearch. Al aplicarlos, hemos mejorado significativamente la calidad de <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> (si√©ntete libre de probarlo). Nos gustar√≠a compartir estos conocimientos con otros profesionales que trabajan en este campo.</p><h2 id=\"select-snippet-from-long-content\">Seleccionar Fragmentos de Contenido Largo</h2><p>El problema es este: despu√©s de <a href=\"https://jina.ai/reader\">usar Jina Reader para leer el contenido de la p√°gina web</a>, necesitamos agregarlo como un elemento de conocimiento al contexto del agente para el razonamiento. Si bien volcar todo el contenido en la ventana de contexto del LLM es la forma m√°s simple, no es √≥ptima cuando se consideran los costos de tokens y la velocidad de generaci√≥n. En la pr√°ctica, necesitamos identificar qu√© partes del contenido son m√°s relevantes para la pregunta y agregar selectivamente solo esas partes como conocimiento al contexto del agente.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Estamos hablando de los casos donde el contenido sigue siendo demasiado largo incluso despu√©s de la limpieza markdown de Jina Reader. Esto ocurre a menudo con p√°ginas largas como issues de GitHub, hilos de Reddit, discusiones en foros y publicaciones de blog (incluidas muchas de las nuestras en jina.ai/news).</div></div><p>El filtrado basado en LLM tiene los mismos problemas de costo y latencia, as√≠ que busquemos soluciones de modelos m√°s peque√±os: necesitamos modelos m√°s peque√±os y econ√≥micos, <strong>pero a√∫n multiling√ºes</strong> ‚Äì un factor crucial ya que no podemos garantizar que tanto la consulta como los documentos siempre est√©n en ingl√©s.</p><p>Tenemos una pregunta de un lado (ya sea la consulta original o una pregunta de brecha) y un gran contenido markdown del otro lado, donde la mayor√≠a del contenido es irrelevante. Necesitamos seleccionar los fragmentos m√°s relevantes para la consulta. Esto se asemeja al problema de chunking con el que la comunidad RAG ha lidiado desde 2023 - recuperar solo fragmentos relevantes usando modelos de recuperaci√≥n para colocarlos en la ventana de contexto para la summarizaci√≥n. Sin embargo, hay dos diferencias clave en nuestro caso:</p><ol><li>Fragmentos limitados de un n√∫mero limitado de documentos. Si cada fragmento contiene aproximadamente 500 tokens, entonces un documento web largo t√≠pico tiene alrededor de 200,000 tokens (p50) a 1,000,000 tokens (p99), y usamos Jina Reader para obtener 4-5 URLs en cada paso, esto producir√≠a aproximadamente cientos de fragmentos - lo que significa cientos de vectores de embedding y cientos de similitudes de coseno. Esto es f√°cilmente manejable con JavaScript en memoria sin una base de datos vectorial.</li><li>Necesitamos fragmentos consecutivos para formar snippets de conocimiento efectivos. No podemos aceptar snippets que combinen oraciones dispersas como <code>[1-2, 6-7, 9, 14, 17, ...].</code> Un snippet de conocimiento m√°s √∫til seguir√≠a patrones como <code>[3-15, 17-24, ...]</code> - manteniendo siempre texto consecutivo. Esto facilita que el LLM copie y cite desde la fuente de conocimiento y reduce la alucinaci√≥n.</li></ol><p>El resto son todas las advertencias de las que se quejan los profesionales: cada fragmento no puede ser demasiado largo ya que los modelos de embedding no pueden manejar bien contextos largos; el chunking introduce p√©rdida de contexto y hace que los embeddings de fragmentos sean i.i.d; y ¬øc√≥mo encontrar las mejores se√±ales de l√≠mite que mantengan tanto la legibilidad como la sem√°ntica? Si sabes de lo que estamos hablando, probablemente te han perseguido estos problemas en tus implementaciones de RAG.</p><p>Pero para resumir - <strong>el late-chunking con <code>jina-embeddings-v3</code> resuelve bellamente los tres problemas.</strong> El late chunking mantiene la informaci√≥n de contexto para cada fragmento, es <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">insensible a las se√±ales de l√≠mite</a>, y <code>jina-embeddings-v3</code> en s√≠ es SOTA en tareas de recuperaci√≥n multiling√ºe <em>asim√©trica</em>. Los lectores interesados pueden seguir nuestras publicaciones de blog o papers para m√°s detalles, pero aqu√≠ est√° la implementaci√≥n general.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">Este diagrama ilustra el algoritmo de selecci√≥n de snippets, que funciona de manera similar a </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\">. El proceso comienza dividiendo un documento largo en fragmentos de longitud fija, que luego se embeben con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> con la opci√≥n de late-chunking activada. Despu√©s de calcular los puntajes de similitud entre cada fragmento y la consulta, una ventana deslizante se mueve a trav√©s de los puntajes de similitud para encontrar la ventana con el valor promedio m√°s alto.</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qu√© es Realmente el Late Chunking y Qu√© No Es: Parte II</div><div class=\"kg-bookmark-description\">Parte 2 de nuestra exploraci√≥n del Late Chunking, una inmersi√≥n profunda en por qu√© es el mejor m√©todo para embeddings de fragmentos y mejora del rendimiento de b√∫squeda/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Embeddings Multiling√ºes con Task LoRA</div><div class=\"kg-bookmark-description\">Presentamos jina-embeddings-v3, un nuevo modelo de embedding de texto con 570 millones de par√°metros, que logra un rendimiento estado del arte en datos multiling√ºes y tareas de recuperaci√≥n de contexto largo, soportando longitudes de contexto de hasta 8192 tokens. El modelo incluye un conjunto de adaptadores de Adaptaci√≥n de Bajo Rango (LoRA) espec√≠ficos para tareas para generar embeddings de alta calidad para recuperaci√≥n de consultas-documentos, agrupamiento, clasificaci√≥n y coincidencia de texto. La evaluaci√≥n en el benchmark MTEB muestra que jina-embeddings-v3 supera a los √∫ltimos embeddings propietarios de OpenAI y Cohere en tareas en ingl√©s, mientras logra un rendimiento superior comparado con multilingual-e5-large-instruct en todas las tareas multiling√ºes. Con una dimensi√≥n de salida predeterminada de 1024, los usuarios pueden reducir flexiblemente las dimensiones de embedding hasta 32 sin comprometer el rendimiento, habilitado por el Aprendizaje de Representaci√≥n Matryoshka.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Embeddings de Fragmentos Contextuales Usando Modelos de Embedding de Contexto Largo</div><div class=\"kg-bookmark-description\">Muchos casos de uso requieren recuperar porciones m√°s peque√±as de texto, y los sistemas de recuperaci√≥n basados en vectores densos a menudo funcionan mejor con segmentos de texto m√°s cortos, ya que es menos probable que la sem√°ntica se sobre-comprima en los embeddings. En consecuencia, los profesionales a menudo dividen los documentos de texto en fragmentos m√°s peque√±os y los codifican por separado. Sin embargo, los embeddings de fragmentos creados de esta manera pueden perder informaci√≥n contextual de los fragmentos circundantes, resultando en representaciones sub√≥ptimas. En este art√≠culo, introducimos un nuevo m√©todo llamado late chunking, que aprovecha los modelos de embedding de contexto largo para primero embeber todos los tokens del texto largo, con el chunking aplicado despu√©s del modelo transformer y justo antes del mean pooling - de ah√≠ el t√©rmino \"late\" en su nombre. Los embeddings de fragmentos resultantes capturan la informaci√≥n contextual completa, llevando a resultados superiores en varias tareas de recuperaci√≥n. El m√©todo es lo suficientemente gen√©rico para ser aplicado a una amplia gama de modelos de embedding de contexto largo y funciona sin entrenamiento adicional. Para aumentar a√∫n m√°s la efectividad del late chunking, proponemos un enfoque de fine-tuning dedicado para modelos de embedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Usando chunking tard√≠o y mean pooling tipo Conv1D para seleccionar el mejor fragmento con respecto a la pregunta.</span></p></figcaption></figure><p>Aseg√∫rate de llamar a la API de Jina Embeddings con los par√°metros de recuperaci√≥n <code>task</code>, <code>late_chunking</code> y <code>truncate</code> configurados como se muestra a continuaci√≥n:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Para incrustar la pregunta, aseg√∫rate de cambiar </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> a </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> y desactivar </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code></p></figcaption></figure><p>La implementaci√≥n completa se puede encontrar en Github:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">Clasificar URLs para la siguiente lectura</h2><p>El problema es este: durante una sesi√≥n de DeepSearch, probablemente recopilar√°s muchas URLs de las p√°ginas de resultados del motor de b√∫squeda (SERP) y descubrir√°s a√∫n m√°s cada vez que leas p√°ginas web individuales (esos enlaces en la p√°gina). El recuento total de URLs √∫nicas puede alcanzar f√°cilmente los cientos. Nuevamente, simplemente volcar todas las URLs directamente en el contexto del LLM es ineficiente - desperdicia valioso espacio de ventana de contexto y, m√°s problem√°ticamente, <strong>descubrimos que los LLMs esencialmente eligen URLs al azar.</strong> Es crucial guiar al LLM hacia URLs que tengan la mayor probabilidad de contener la respuesta que necesitas.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Mejor opci√≥n para usar Jina Reader para rastrear una p√°gina en DeepSearch. Esto recopilar√° todos los enlaces de la p√°gina en un campo separado </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> y los eliminar√° del campo </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\">.</span></p></figcaption></figure><p>Piensa en este problema como un PageRank en contexto donde necesitamos ponderar cientos de URLs durante una sesi√≥n. Clasificamos las URLs bas√°ndonos en m√∫ltiples factores que combinan tiempo de √∫ltima actualizaci√≥n, frecuencia de dominio, estructura de ruta y, lo m√°s importante, relevancia sem√°ntica para la consulta para crear una puntuaci√≥n compuesta. Recuerda que solo podemos usar la informaci√≥n disponible <em>antes</em> de visitar realmente la URL:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>Se√±ales de Frecuencia</strong>: Las URLs que aparecen m√∫ltiples veces en diferentes fuentes reciben peso adicional. Las URLs de dominios que aparecen frecuentemente en los resultados de b√∫squeda reciben un impulso, ya que los dominios populares suelen contener contenido autoritativo.</p><p><strong>Estructura de Ruta</strong>: Analizamos las rutas de URL para identificar clusters de contenido. Las URLs dentro de jerarqu√≠as de ruta comunes reciben puntuaciones m√°s altas, con un factor de decaimiento aplicado a rutas m√°s profundas.</p><p><strong>Relevancia Sem√°ntica</strong>: Usamos <code>jina-reranker-v2-base-multilingual</code> para evaluar la relevancia sem√°ntica entre la pregunta y la informaci√≥n textual de cada URL, que es <a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">un problema cl√°sico de reclasificaci√≥n</a>. La informaci√≥n textual de cada URL proviene de:</p><ul><li>T√≠tulo y fragmentos de resultados de la API SERP (<code>https://s.jina.ai/</code> con <code>'X-Respond-With': 'no-content'</code>)</li><li>Texto de anclaje de URLs en la p√°gina (<code>https://r.jina.ai</code> con <code>'X-With-Links-Summary': 'all'</code>)</li></ul><p><strong>Tiempo de √öltima Actualizaci√≥n</strong>: Algunas consultas de DeepSearch son sensibles al tiempo, por lo que las URLs actualizadas recientemente son m√°s valiosas que las antiguas. Sin ser un motor de b√∫squeda importante como Google, determinar de manera confiable el tiempo de √∫ltima actualizaci√≥n es un desaf√≠o. Hemos implementado un enfoque de m√∫ltiples capas que combina la siguiente se√±al y proporciona una marca de tiempo con puntuaci√≥n de confianza que prioriza el contenido m√°s reciente cuando es necesario.</p><ul><li>Filtros de API SERP (como el par√°metro <code>tbs</code> de s.jina.ai para filtrar por antig√ºedad)</li><li>An√°lisis de encabezados HTTP (Last-Modified, ETag)</li><li>Extracci√≥n de metadatos (meta tags, marcas de tiempo Schema.org)</li><li>Reconocimiento de patrones de contenido (fechas visibles en HTML)</li><li>Indicadores espec√≠ficos de CMS para plataformas como WordPress, Drupal y Ghost</li></ul><p><strong>Contenido Restringido:</strong> Algunos contenidos en plataformas de redes sociales est√°n restringidos o simplemente detr√°s de muros de pago, y sin iniciar sesi√≥n o violar sus ToS, no hay una manera leg√≠tima de obtener este contenido. Debemos mantener activamente una lista de URLs y nombres de host problem√°ticos para reducir sus clasificaciones, evitando perder tiempo en contenido inaccesible.</p><p><strong>Diversidad de Dominio:</strong> En algunos casos, las URLs con mayor peso provienen todas de los mismos nombres de host, lo que puede atrapar a DeepSearch en un √≥ptimo local y reducir la calidad final de los resultados. Observa los ejemplos anteriores donde todas las URLs principales son de StackOverflow. Para mejorar la diversidad, podemos implementar un enfoque de exploraci√≥n-explotaci√≥n seleccionando las k URLs mejor clasificadas de cada nombre de host.</p><p>La implementaci√≥n completa de la clasificaci√≥n de URLs se puede encontrar en nuestro Github.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main ¬∑ jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Faceü§ó Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset ¬∑ Issue #7175 ¬∑ huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Recuerda poner los pesos de las URL en el contexto del agente e instruir a los LLM para que respeten los pesos.</span></p></figcaption></figure><h2 id=\"conclusion\">Conclusi√≥n</h2><p>Desde el lanzamiento de nuestro sistema DeepSearch el 2 de febrero de 2025, hemos descubierto dos detalles de implementaci√≥n que mejoraron sustancialmente la calidad. Sorprendentemente, ambos utilizan embeddings y rerankers multiling√ºes de manera \"in-context\" - operando a una escala mucho menor que los √≠ndices precomputados que estos modelos t√≠picamente requieren. Esto explica nuestra omisi√≥n inicial.</p><p>Esto apunta a una fascinante polarizaci√≥n en el futuro de la tecnolog√≠a de b√∫squeda. Consideremos un marco an√°logo a la teor√≠a del proceso dual de Kahneman:</p><ul><li>Pensamiento r√°pido (grep, BM25, SQL): Coincidencia de patrones r√°pida y gobernada por reglas con demandas computacionales m√≠nimas.</li><li>Pensamiento lento (LLM): Razonamiento integral con comprensi√≥n contextual profunda, que requiere una computaci√≥n significativa.</li><li>Pensamiento medio (embeddings, rerankers): ¬øAtrapado en el limbo? Demasiado \"avanzado\"/sem√°ntico para la simple coincidencia de patrones pero sin verdaderas capacidades de razonamiento.</li></ul><p>Podr√≠amos estar presenciando la popularidad de una arquitectura bifurcada donde SQL/BM25 ligero y eficiente maneja la recuperaci√≥n inicial de contenido, alimentando directamente a LLM poderosos para el procesamiento profundo. Estos LLM incorporan cada vez m√°s las funciones sem√°nticas que anteriormente requer√≠an modelos especializados de nivel medio. El papel restante para los modelos de pensamiento medio se desplaza hacia tareas in-context especializadas: filtrado, deduplicaci√≥n y operaciones de alcance limitado donde el razonamiento completo ser√≠a ineficiente.</p><p>Sin embargo, la selecci√≥n de fragmentos cr√≠ticos y la clasificaci√≥n de URL siguen siendo componentes fundamentales con impacto directo en la calidad del sistema DeepSearch/DeepResearch. Esperamos que nuestras ideas inspiren mejoras en sus propias implementaciones.</p><p>La expansi√≥n de consultas contin√∫a siendo otro determinante crucial de la calidad. Estamos evaluando activamente m√∫ltiples enfoques, desde reescrituras b√°sicas basadas en prompts hasta modelos de lenguaje peque√±os y m√©todos basados en razonamiento. Busque nuestros pr√≥ximos hallazgos sobre este frente pronto. Est√©n atentos.</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "Dominar estos dos detalles transforma tu DeepSearch de mediocre a legendario: seleccionar los mejores fragmentos de p√°ginas web extensas y clasificar las URLs antes de rastrearlas.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
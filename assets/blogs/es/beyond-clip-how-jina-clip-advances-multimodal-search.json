{
  "id": "671b96784821eb000165d2de",
  "uuid": "ec571b8c-d111-4d49-bad8-2836bd885f1c",
  "title": "Más allá de CLIP: Cómo Jina-CLIP avanza en la búsqueda multimodal",
  "slug": "beyond-clip-how-jina-clip-advances-multimodal-search",
  "html": "<p>La búsqueda multimodal, que combina texto e imágenes en una experiencia de búsqueda fluida, ha ganado impulso gracias a modelos como <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP de OpenAI</a>. Estos modelos conectan efectivamente los datos visuales y textuales, permitiéndonos relacionar imágenes con texto relevante y viceversa.</p><p>Si bien CLIP y modelos similares son potentes, tienen limitaciones notables, particularmente al procesar textos más largos o manejar relaciones textuales complejas. Aquí es donde entra <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image</div><div class=\"kg-bookmark-description\">Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Diseñado para abordar estos desafíos, <code>jina-clip-v1</code> ofrece una mejor comprensión del texto mientras mantiene sólidas capacidades de correspondencia texto-imagen. Proporciona una solución más eficiente para aplicaciones que utilizan ambas modalidades, simplificando el proceso de búsqueda y eliminando la necesidad de alternar entre modelos separados para texto e imágenes.</p><p>En esta publicación, exploraremos lo que <code>jina-clip-v1</code> aporta a las aplicaciones de búsqueda multimodal, mostrando experimentos que demuestran cómo mejora tanto la precisión como la variedad de resultados a través de embeddings integrados de texto e imagen.</p><h2 id=\"what-is-clip\">¿Qué es CLIP?</h2><p>CLIP (Contrastive Language–Image Pretraining) es una arquitectura de modelo de IA desarrollada por OpenAI que conecta texto e imágenes mediante el aprendizaje de representaciones conjuntas. CLIP es esencialmente un modelo de texto y un modelo de imagen unidos — transforma ambos tipos de entrada en un espacio de embedding compartido, donde textos e imágenes similares se posicionan cerca unos de otros. CLIP fue entrenado con un vasto conjunto de datos de pares imagen-texto, lo que le permite comprender la relación entre contenido visual y textual. Esto le permite generalizar bien a través de diferentes dominios, haciéndolo altamente efectivo en escenarios de aprendizaje zero-shot, como generar subtítulos o recuperación de imágenes.</p><p>Desde el lanzamiento de CLIP, otros modelos como <a href=\"https://arxiv.org/abs/2303.15343?ref=jina-ai-gmbh.ghost.io\">SigLiP</a>, <a href=\"https://arxiv.org/abs/2111.07991?ref=jina-ai-gmbh.ghost.io\">LiT</a>, y <a href=\"https://arxiv.org/abs/2303.15389?ref=jina-ai-gmbh.ghost.io\">EvaCLIP</a> han expandido sus fundamentos, mejorando aspectos como la eficiencia del entrenamiento, el escalado y la comprensión multimodal. Estos modelos a menudo aprovechan conjuntos de datos más grandes, arquitecturas mejoradas y técnicas de entrenamiento más sofisticadas para expandir los límites de la alineación texto-imagen, avanzando aún más en el campo de los modelos imagen-lenguaje.</p><p>Si bien CLIP <em>puede</em> trabajar solo con texto, enfrenta limitaciones significativas. Primero, fue entrenado solo con subtítulos cortos, no textos largos, manejando un máximo de aproximadamente 77 palabras. Segundo, CLIP sobresale en conectar texto con imágenes pero tiene dificultades al comparar texto con otro texto, como reconocer que las cadenas <code>a crimson fruit</code> y <code>a red apple</code> pueden referirse a lo mismo. Aquí es donde destacan los modelos de texto especializados, como <code>jina-embeddings-v3</code>.</p><p>Estas limitaciones complican las tareas de búsqueda que involucran tanto texto como imágenes, por ejemplo, una tienda online \"shop the look\" donde un usuario puede buscar productos de moda usando tanto una cadena de texto como una imagen. Al indexar tus productos, necesitas procesar cada uno múltiples veces - una vez para la imagen, una vez para el texto, y una vez más con un modelo específico de texto. De igual manera, cuando un usuario busca un producto, tu sistema necesita buscar al menos dos veces para encontrar tanto objetivos de texto como de imagen:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png\" class=\"kg-image\" alt=\"Flowchart outlining &quot;Offline Indexing&quot; and &quot;Online Querying&quot; processes with labeled blocks and arrows for XML data interactio\" loading=\"lazy\" width=\"970\" height=\"1255\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png 970w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-jina-clip-v1-solves-clip%E2%80%99s-shortcomings\"><strong>Cómo </strong><code>jina-clip-v1</code><strong> Resuelve las Limitaciones de CLIP</strong></h2><p>Para superar las limitaciones de CLIP, creamos <code>jina-clip-v1</code> para entender textos más largos y hacer coincidir más efectivamente las consultas de texto tanto con textos como con imágenes. ¿Qué hace que <code>jina-clip-v1</code> sea tan especial? En primer lugar, utiliza un modelo de comprensión de texto más inteligente (JinaBERT), ayudándole a entender textos más largos y complicados (como descripciones de productos), no solo subtítulos cortos (como nombres de productos). En segundo lugar, entrenamos <code>jina-clip-v1</code> para ser bueno en dos cosas a la vez: tanto en hacer coincidir texto con imágenes como en hacer coincidir texto con otros textos.</p><p>Con OpenAI CLIP, ese no es el caso: tanto para indexar como para consultar, necesitas invocar dos modelos (CLIP para imágenes y textos cortos como subtítulos, otro embedding de texto para textos más largos como descripciones). Esto no solo añade sobrecarga, sino que ralentiza la búsqueda, una operación que <em>debería</em> ser realmente rápida. <code>jina-clip-v1</code> hace todo eso en un solo modelo, sin sacrificar velocidad:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-22.png\" class=\"kg-image\" alt=\"Flowchart of JaclinQ's offline indexing and online querying processes, involving imagery and text analysis.\" loading=\"lazy\" width=\"2000\" height=\"2785\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/10/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Este enfoque unificado abre nuevas posibilidades que eran desafiantes con modelos anteriores, potencialmente remodelando cómo abordamos la búsqueda. En esta publicación, realizamos dos experimentos:</p><ul><li><strong>Mejorando los resultados de búsqueda combinando texto e imagen</strong>: ¿Podemos combinar lo que <code>jina-clip-v1</code> entiende del texto con lo que entiende de las imágenes? ¿Qué sucede cuando mezclamos estos dos tipos de comprensión? ¿Agregar información visual cambia nuestros resultados de búsqueda? En resumen, ¿podemos obtener mejores resultados si buscamos con texto e imágenes al mismo tiempo?</li><li><strong>Usando imágenes para diversificar los resultados de búsqueda</strong>: La mayoría de los motores de búsqueda maximizan las coincidencias de texto. Pero ¿podemos usar la comprensión de imágenes de <code>jina-clip-v1</code> como un \"shuffle visual\"? En lugar de mostrar solo los resultados más relevantes, podríamos incluir algunos visualmente diversos. No se trata de encontrar más resultados relacionados – se trata de mostrar una gama más amplia de perspectivas, incluso si están menos estrechamente relacionadas. Al hacer esto, podemos descubrir aspectos de un tema que no habíamos considerado antes. Por ejemplo, en el contexto de búsqueda de moda, si un usuario busca \"vestido de cóctel multicolor\", ¿quieren que los primeros resultados se vean todos iguales (es decir, coincidencias <em>muy</em> cercanas), o una mayor variedad para elegir (mediante shuffle visual)?</li></ul><p>Ambos enfoques son valiosos en una variedad de casos de uso donde los usuarios podrían buscar con texto o imágenes, como en comercio electrónico, medios, arte y diseño, imágenes médicas y más allá.</p><h2 id=\"averaging-text-and-image-embeddings-for-above-average-performance\">Promediando Embeddings de Texto e Imagen para un Rendimiento Superior al Promedio</h2><p>Cuando un usuario envía una consulta (generalmente como una cadena de texto), podemos usar la torre de texto de <code>jina-clip-v1</code> para codificar la consulta en un embedding de texto. La fortaleza de <code>jina-clip-v1</code> radica en su capacidad para entender tanto texto como imágenes al alinear señales texto-a-texto y texto-a-imagen en el mismo espacio semántico.</p><p>¿Podemos mejorar los resultados de recuperación si combinamos los embeddings preindexados de texto e imagen de cada producto promediándolos?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png\" class=\"kg-image\" alt=\"Flowchart on a black background detailing text and image embedding processes with a black knit midi dress photo example.\" loading=\"lazy\" width=\"995\" height=\"359\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-28.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png 995w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Esto crea una representación única que incluye tanto información textual (por ejemplo, descripción del producto) como información visual (por ejemplo, imagen del producto). Luego podemos usar el embedding de la consulta de texto para buscar estas representaciones combinadas. ¿Cómo afecta esto a nuestros resultados de búsqueda?</p><p>Para averiguarlo, utilizamos el conjunto de datos <a href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\">Fashion200k</a>, un conjunto de datos a gran escala creado específicamente para tareas relacionadas con la recuperación de imágenes de moda y la comprensión multimodal. Consiste en más de 200,000 imágenes de artículos de moda, como ropa, zapatos y accesorios, junto con sus correspondientes descripciones de producto y metadatos.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - xthan/fashion-200k: Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\"</div><div class=\"kg-bookmark-description\">Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\" - xthan/fashion-200k</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">xthan</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2116651d448aec6ea0508f5fdb123e6292fa00bfb1cf8fb6f3468cbe761da769/xthan/fashion-200k\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Clasificamos cada elemento en una categoría amplia (por ejemplo, <code>dress</code>) y una categoría detallada (como <code>knit midi dress</code>).</p><h3 id=\"analyzing-three-retrieval-methods\"><strong>Análisis de Tres Métodos de Recuperación</strong></h3><p>Para ver si promediar los embeddings de texto e imagen producía mejores resultados de recuperación, experimentamos con tres tipos de búsqueda, cada uno utilizando una cadena de texto (por ejemplo, <code>red dress</code>) como consulta:</p><ul><li><strong>Consulta a Descripción usando embeddings de texto:</strong> Buscar descripciones de productos basadas en embeddings de texto.</li><li><strong>Consulta a Imagen usando búsqueda cross-modal:</strong> Buscar imágenes de productos basadas en embeddings de imagen.</li><li><strong>Consulta a Embedding Promedio:</strong> Buscar embeddings promediados tanto de descripciones como de imágenes de productos.</li></ul><p>Primero indexamos todo el conjunto de datos y luego generamos aleatoriamente 1,000 consultas para evaluar el rendimiento. Codificamos cada consulta en un embedding de texto y realizamos la coincidencia por separado, según los métodos descritos anteriormente. Medimos la precisión por qué tan bien las categorías de los productos devueltos coincidían con la consulta de entrada.</p><p>Cuando usamos la consulta <code>multicolor henley t-shirt dress</code>, la búsqueda de <strong>Consulta a Descripción</strong> logró la mayor precisión top-5, pero los últimos tres vestidos mejor clasificados eran visualmente idénticos. Esto es menos que ideal, ya que una búsqueda efectiva debería equilibrar relevancia y diversidad para captar mejor la atención del usuario.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png\" class=\"kg-card kg-image\" alt=\"Array of five unique dresses, categorized as casual and day, arranged in a row on a white background with named tags for easy\" loading=\"lazy\" width=\"2000\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La búsqueda cross-modal de <strong>Consulta a Imagen</strong> usó la misma consulta y tomó el enfoque opuesto, presentando una colección altamente diversa de vestidos. Si bien coincidió con dos de cinco resultados en la categoría amplia correcta, ninguno coincidió con la categoría detallada.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png\" class=\"kg-card kg-image\" alt=\"Variety of women's clothing items including short and long-sleeved tops and casual to maxi dresses with color swatches.\" loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La <strong>búsqueda con embeddings promediados de texto e imagen</strong> produjo el mejor resultado: los cinco resultados coincidieron con la categoría amplia, y dos de cinco coincidieron con la categoría detallada. Además, se eliminaron los elementos visualmente duplicados, proporcionando una selección más variada. Usar embeddings de texto para buscar en embeddings promediados de texto e imagen parece mantener la calidad de la búsqueda mientras incorpora señales visuales, lo que lleva a resultados más diversos y completos.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png\" class=\"kg-card kg-image\" alt=\"Showcase of various women's dresses, including a multicolor henley t-shirt dress and a pink Missoni dress, labeled with categ\" loading=\"lazy\" width=\"2000\" height=\"513\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"scaling-up-evaluating-with-more-queries\"><strong>Escalando: Evaluación con Más Consultas</strong></h3><p>Para ver si esto funcionaría a mayor escala, continuamos ejecutando el experimento con categorías amplias y detalladas adicionales. Ejecutamos varias iteraciones, recuperando un número diferente de resultados (\"valores k\") cada vez.</p><p>Tanto en categorías amplias como detalladas, la <strong>Consulta a Embedding Promedio</strong> logró consistentemente la mayor precisión en todos los valores k (10, 20, 50, 100). Esto muestra que combinar embeddings de texto e imagen proporciona los resultados más precisos para recuperar elementos relevantes, independientemente de si la categoría es amplia o específica:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png\" class=\"kg-card kg-image\" alt=\"Comparative chart of 'Broad Precision@K' and 'Fine-grained Precision@K' showing different precision values for query-related \" loading=\"lazy\" width=\"2000\" height=\"836\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>k</strong></th>\n<th><strong>Search Type</strong></th>\n<th><strong>Broad Category Precision (cosine similarity)</strong></th>\n<th><strong>Fine-grained Category Precision (cosine similarity)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>Query to Description</td>\n<td>0.9026</td>\n<td>0.2314</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Image</td>\n<td>0.7614</td>\n<td>0.2037</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9230</strong></td>\n<td><strong>0.2711</strong></td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Description</td>\n<td>0.9150</td>\n<td>0.2316</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Image</td>\n<td>0.7523</td>\n<td>0.1964</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9229</strong></td>\n<td><strong>0.2631</strong></td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Description</td>\n<td>0.9134</td>\n<td>0.2254</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Image</td>\n<td>0.7418</td>\n<td>0.1750</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9226</strong></td>\n<td><strong>0.2390</strong></td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Description</td>\n<td>0.9092</td>\n<td>0.2139</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Image</td>\n<td>0.7258</td>\n<td>0.1675</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9150</strong></td>\n<td><strong>0.2286</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<ul><li>La <strong>Consulta a Descripción usando embeddings de texto</strong> funcionó bien en ambas categorías pero quedó ligeramente por detrás del enfoque de embedding promediado. Esto sugiere que las descripciones textuales por sí solas proporcionan información valiosa, particularmente para categorías más amplias como \"dress\", pero pueden carecer de la sutileza necesaria para una clasificación detallada precisa (por ejemplo, distinguir entre diferentes tipos de vestidos).</li><li>La <strong>Consulta a Imagen usando búsqueda cross-modal</strong> tuvo consistentemente la precisión más baja en ambas categorías. Esto sugiere que mientras las características visuales pueden ayudar a identificar categorías amplias, son menos efectivas para capturar las distinciones detalladas de elementos específicos de moda. El desafío de distinguir categorías detalladas puramente desde características visuales es particularmente evidente, donde las diferencias visuales pueden ser sutiles y requerir contexto adicional proporcionado por el texto.</li><li>En general, combinar información textual y visual (mediante <strong>embeddings promediados</strong>) logró alta precisión tanto en tareas de recuperación de moda amplias como detalladas. Las descripciones textuales juegan un papel importante, especialmente en la identificación de categorías amplias, mientras que las imágenes por sí solas son menos efectivas en ambos casos.</li></ul><p>En general, la precisión fue mucho más alta para categorías amplias en comparación con categorías detalladas, principalmente debido a que los elementos en categorías amplias (por ejemplo, <code>dress</code>) están más representados en el conjunto de datos que las categorías detalladas (por ejemplo, <code>henley dress</code>), simplemente porque la última es un subconjunto de la primera. Por su propia naturaleza, una categoría amplia es más fácil de generalizar que una categoría detallada. Fuera del ejemplo de la moda, es sencillo identificar que algo, en general, es un pájaro. Es mucho más difícil identificarlo como un <a href=\"https://www.youtube.com/watch?v=nPhVOZiPokA&ref=jina-ai-gmbh.ghost.io\">Vogelkop Superb Bird of Paradise</a>.</p><p>Otro aspecto a tener en cuenta es que la información en una consulta de texto coincide más fácilmente con otros textos (como nombres de productos o descripciones), que con características visuales. Por lo tanto, si se usa un texto como entrada, los textos son una salida más probable que las imágenes. Obtenemos los mejores resultados combinando tanto imágenes como texto (mediante el promedio de los embeddings) en nuestro índice.</p><h2 id=\"retrieve-results-with-text-diversify-them-with-images\">Recuperar Resultados con Texto; Diversificarlos con Imágenes</h2><p>En la sección anterior, tocamos el tema de los resultados de búsqueda visualmente duplicados. En la búsqueda, <em>la precisión por sí sola no siempre es suficiente</em>. En muchos casos, mantener una lista clasificada concisa pero altamente relevante y diversa es más efectivo, especialmente cuando la consulta del usuario es ambigua (por ejemplo, si un usuario busca<code>black jacket</code> — ¿se refieren a una chaqueta de motociclista negra, una bomber, un blazer u otro tipo?)</p><p>Ahora, en lugar de aprovechar la capacidad multimodal de <code>jina-clip-v1</code>, usaremos los embeddings de texto de su torre de texto para la búsqueda inicial, y luego aplicaremos los embeddings de imagen de la torre de imagen como un \"reordenador visual\" para diversificar los resultados de búsqueda. Esto se ilustra en el diagrama siguiente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png\" class=\"kg-image\" alt=\"Flowchart detailing multimodal document text processing, with branches for text and image embedding and various processing pa\" loading=\"lazy\" width=\"975\" height=\"476\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png 975w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><ol><li>Primero, recuperar los k primeros resultados de búsqueda basados en embeddings de texto.</li><li>Para cada resultado principal de búsqueda, extraer características visuales y agruparlas usando embeddings de imagen.</li><li>Reordenar los resultados de búsqueda seleccionando un elemento de cada grupo y presentar una lista diversificada al usuario.</li></ol><p>Después de recuperar los cincuenta primeros resultados, aplicamos un agrupamiento k-means ligero (k=5) a los embeddings de imagen, luego seleccionamos elementos de cada grupo. La precisión de categoría se mantuvo consistente con el rendimiento de Consulta-a-Descripción, ya que usamos la categoría de consulta-a-producto como métrica de medición. Sin embargo, los resultados clasificados comenzaron a cubrir más aspectos diferentes (como tela, corte y patrón) con la diversificación basada en imágenes. Como referencia, aquí está el ejemplo del vestido tipo camiseta henley multicolor de antes:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png\" class=\"kg-image\" alt=\"Collection of t-shirt dresses categorized into casual and day, short and long sleeves, displayed in two rows.\" loading=\"lazy\" width=\"2000\" height=\"1484\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ahora veamos cómo la diversificación afecta los resultados de búsqueda usando la búsqueda por embedding de texto combinada con embedding de imagen como reordenador de diversificación:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png\" class=\"kg-image\" alt=\"Five diverse dresses arranged in a row, categorized as various types including casual and day dresses, mini and short, and ma\" loading=\"lazy\" width=\"2000\" height=\"465\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Los resultados clasificados provienen de la búsqueda basada en texto pero comienzan a cubrir \"aspectos\" más diversos dentro de los cinco primeros ejemplos. Esto logra un efecto similar al promedio de embeddings sin realmente promediarlos.</p><p>Sin embargo, esto tiene un costo: tenemos que aplicar un paso adicional de agrupamiento después de recuperar los k primeros resultados, lo que añade algunos milisegundos extra, dependiendo del tamaño de la clasificación inicial. Además, determinar el valor de k para el agrupamiento k-means implica algunas conjeturas heurísticas. ¡Ese es el precio que pagamos por una mejor diversificación de resultados!</p><h2 id=\"conclusion\">Conclusión</h2><p><code>jina-clip-v1</code> cierra efectivamente la brecha entre la búsqueda de texto e imagen unificando ambas modalidades en un solo modelo eficiente. Nuestros experimentos han demostrado que su capacidad para procesar textos más largos y complejos junto con imágenes ofrece un rendimiento de búsqueda superior en comparación con modelos tradicionales como CLIP.</p><p>Nuestras pruebas cubrieron varios métodos, incluyendo la coincidencia de texto con descripciones, imágenes y embeddings promediados. Los resultados mostraron consistentemente que la combinación de embeddings de texto e imagen produjo los mejores resultados, mejorando tanto la precisión como la diversidad de los resultados de búsqueda. También descubrimos que usar embeddings de imagen como un \"reordenador visual\" mejoró la variedad de resultados mientras mantenía la relevancia.</p><p>Estos avances tienen implicaciones significativas para aplicaciones del mundo real donde los usuarios buscan usando tanto descripciones de texto como imágenes. Al entender ambos tipos de datos simultáneamente, <code>jina-clip-v1</code> optimiza el proceso de búsqueda, entregando resultados más relevantes y permitiendo recomendaciones de productos más diversas. Esta capacidad de búsqueda unificada se extiende más allá del comercio electrónico para beneficiar la gestión de activos multimedia, bibliotecas digitales y curación de contenido visual, facilitando el descubrimiento de contenido relevante en diferentes formatos.</p><p>Si bien <code>jina-clip-v1</code> actualmente solo admite inglés, estamos trabajando en <code>jina-clip-v2</code>. Siguiendo los pasos de <code>jina-embeddings-v3</code> y <code>jina-colbert-v2</code>, esta nueva versión será un recuperador multimodal multilingüe de última generación que admitirá 89 idiomas. Esta actualización abrirá nuevas posibilidades para tareas de búsqueda y recuperación en diferentes mercados e industrias, convirtiéndolo en un modelo de embedding más potente para aplicaciones globales en comercio electrónico, medios y más allá.</p>",
  "comment_id": "671b96784821eb000165d2de",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-25T15:00:40.000+02:00",
  "updated_at": "2024-10-30T19:14:11.000+01:00",
  "published_at": "2024-10-29T11:51:40.000+01:00",
  "custom_excerpt": "Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/beyond-clip-how-jina-clip-advances-multimodal-search/",
  "excerpt": "Aprende cómo Jina-CLIP mejora el CLIP de OpenAI con una mayor precisión en la recuperación y resultados más diversos a través de embeddings unificados de texto e imagen.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere.",
  "feature_image_caption": null
}
{
  "slug": "bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning",
  "id": "67066bd652567c0001d0f2cd",
  "uuid": "1130051f-f343-4eb2-9956-9b574c212704",
  "title": "Cerrando las brechas ling√º√≠sticas en embeddings multiling√ºes mediante aprendizaje contrastivo",
  "html": "<p>En los modelos multiling√ºes, uno de los desaf√≠os clave es el \"<strong>language gap</strong>\" ‚Äî un fen√≥meno donde frases con el mismo significado en diferentes idiomas no est√°n tan estrechamente alineadas o agrupadas como deber√≠an. Idealmente, un texto en un idioma y su equivalente en otro deber√≠an tener representaciones similares ‚Äî es decir, embeddings que est√©n muy cerca uno del otro ‚Äî permitiendo que las aplicaciones multiling√ºes operen de manera id√©ntica en textos de diferentes idiomas. Sin embargo, los modelos a menudo representan sutilmente el idioma de un texto, creando un \"language gap\" que lleva a un rendimiento sub√≥ptimo entre idiomas.</p><p>En esta publicaci√≥n, exploraremos este language gap y c√≥mo impacta el rendimiento en los modelos de embeddings de texto. Hemos realizado experimentos para evaluar el alineamiento sem√°ntico de par√°frasis en el mismo idioma y para traducciones entre diferentes pares de idiomas, usando nuestro modelo <code>jina-xlm-roberta</code> y el m√°s reciente <code>jina-embeddings-v3</code>. Estos experimentos revelan qu√© tan bien se agrupan las frases con significados similares o id√©nticos bajo diferentes condiciones de entrenamiento.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-6.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Tambi√©n hemos experimentado con t√©cnicas de entrenamiento para mejorar el alineamiento sem√°ntico entre idiomas, espec√≠ficamente la introducci√≥n de <strong>datos multiling√ºes paralelos</strong> durante el aprendizaje contrastivo. En este art√≠culo, compartiremos nuestros hallazgos y resultados.</p><h2 id=\"multilingual-model-training-creates-and-reduces-the-language-gap\"><strong>El Entrenamiento de Modelos Multiling√ºes Crea y Reduce el Language Gap</strong></h2><p>El entrenamiento de modelos de embeddings de texto t√≠picamente involucra un proceso de m√∫ltiples etapas con dos partes principales:</p><ol><li><a href=\"https://aclanthology.org/2023.acl-long.49/?ref=jina-ai-gmbh.ghost.io\"><strong>Masked Language Modeling</strong></a> (MLM): El pre-entrenamiento t√≠picamente involucra grandes cantidades de texto en las que algunos tokens son enmascarados aleatoriamente. El modelo es entrenado para predecir estos tokens enmascarados. Este procedimiento ense√±a al modelo los patrones del idioma o idiomas en los datos de entrenamiento, incluyendo dependencias de selecci√≥n entre tokens que pueden surgir de la sintaxis, la sem√°ntica l√©xica y las restricciones pragm√°ticas del mundo real.</li><li><a href=\"https://paperswithcode.com/task/contrastive-learning?ref=jina-ai-gmbh.ghost.io\"><strong>Contrastive Learning</strong></a>: Despu√©s del pre-entrenamiento, el modelo se entrena adicionalmente con datos curados o semi-curados para acercar los embeddings de textos sem√°nticamente similares y (opcionalmente) alejar los dis√≠miles. Este entrenamiento puede usar pares, tripletas o incluso grupos de textos cuya similitud sem√°ntica ya es conocida o al menos estimada de manera confiable. Puede tener varias subetapas y hay una variedad de estrategias de entrenamiento para esta parte del proceso, con nueva investigaci√≥n publicada frecuentemente y sin un consenso claro sobre el enfoque √≥ptimo.</li></ol><p>Para entender c√≥mo surge el language gap y c√≥mo puede cerrarse, necesitamos examinar el papel de ambas etapas.</p><h3 id=\"masked-language-pretraining\"><strong>Pre-entrenamiento de Masked Language</strong></h3><p>Parte de la capacidad multiling√ºe de los modelos de embeddings de texto se adquiere durante el pre-entrenamiento.</p><p>Los cognados y las palabras prestadas hacen posible que el modelo aprenda cierto alineamiento sem√°ntico entre idiomas a partir de grandes cantidades de datos de texto. Por ejemplo, la palabra en ingl√©s <em>banana</em> y la palabra en franc√©s <em>banane</em> (y en alem√°n <em>Banane</em>) son frecuentes y suficientemente similares en su escritura para que un modelo de embeddings pueda aprender que las palabras que se parecen a \"banan-\" tienen patrones de distribuci√≥n similares entre idiomas. Puede aprovechar esa informaci√≥n para aprender, hasta cierto punto, que otras palabras que no se parecen entre idiomas tambi√©n tienen significados similares, e incluso descubrir c√≥mo se traducen algunas estructuras gramaticales.</p><p>Sin embargo, esto ocurre sin entrenamiento expl√≠cito.</p><p>Probamos el modelo <code>jina-xlm-roberta</code>, la base pre-entrenada de <code>jina-embeddings-v3</code>, para ver qu√© tan bien aprendi√≥ las equivalencias entre idiomas durante el pre-entrenamiento MLM. Graficamos representaciones de oraciones bidimensionales <a href=\"https://pair-code.github.io/understanding-umap/?ref=jina-ai-gmbh.ghost.io\">UMAP</a> de un conjunto de oraciones en ingl√©s traducidas al alem√°n, holand√©s, chino simplificado y japon√©s. Los resultados est√°n en la figura siguiente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png\" class=\"kg-image\" alt=\"Multilingual scatterplot showing word embeddings' alignment across five languages on UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_roberta_mlm_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_roberta_mlm_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Proyecci√≥n bidimensional UMAP de una selecci√≥n de oraciones en ingl√©s y sus traducciones en alem√°n, holand√©s, chino y japon√©s. Las l√≠neas grises conectan las oraciones en idiomas no ingleses con las oraciones en ingl√©s de las que fueron traducidas.<br><br>Estas oraciones tienden fuertemente a formar grupos espec√≠ficos por idioma en el espacio de embeddings de <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-xlm-roberta</code>, aunque se pueden ver algunos valores at√≠picos en esta proyecci√≥n que pueden ser un efecto secundario de la proyecci√≥n bidimensional.</div></div><p>Puede verse que el pre-entrenamiento ha agrupado muy fuertemente los embeddings de oraciones en el mismo idioma. Esta es una proyecci√≥n en dos dimensiones de una distribuci√≥n en un espacio de dimensiones mucho m√°s alto, por lo que todav√≠a es posible que, por ejemplo, una oraci√≥n en alem√°n que sea una buena traducci√≥n de una en ingl√©s pueda seguir siendo la oraci√≥n en alem√°n cuyo embedding est√° m√°s cerca del embedding de su fuente en ingl√©s. Pero s√≠ muestra que un embedding de una oraci√≥n en ingl√©s probablemente est√° m√°s cerca de otra oraci√≥n en ingl√©s que de una sem√°nticamente id√©ntica o casi id√©ntica en alem√°n.</p><p>Note tambi√©n c√≥mo el alem√°n y el holand√©s forman grupos mucho m√°s cercanos que otros pares de idiomas. Esto no es sorprendente para dos idiomas relativamente cercanos. El alem√°n y el holand√©s son lo suficientemente similares que a veces son parcialmente mutuamente comprensibles.</p><p>El japon√©s y el chino tambi√©n aparecen m√°s cercanos entre s√≠ que con otros idiomas. Aunque no est√°n relacionados entre s√≠ de la misma manera, el japon√©s escrito t√≠picamente usa <em>kanji</em> (Êº¢Â≠ó), o <em>h√†nz√¨</em> en chino<em>. </em>El japon√©s comparte la mayor√≠a de estos caracteres escritos con el chino, y los dos idiomas comparten muchas palabras escritas con uno o varios kanji/h√†nz√¨ juntos. Desde la perspectiva del MLM, este es el mismo tipo de similitud visible que entre el holand√©s y el alem√°n.</p><p>Podemos ver este \"language gap\" de una manera m√°s simple observando solo dos idiomas con dos oraciones cada uno:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png\" class=\"kg-image\" alt=\"Graph illustrating linguistic relationships with color-coded lines, data points for English and German phrases, and an &quot;MLM P\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--8-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image--8-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image--8-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--8-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Dado que MLM parece agrupar naturalmente los textos por idioma, \"my dog is blue\" y \"my cat is red\" est√°n agrupados juntos, lejos de sus contrapartes en alem√°n. A diferencia del \"<a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/?ref=jina-ai-gmbh.ghost.io\">modality gap</a>\" discutido en una publicaci√≥n anterior del blog, creemos que esto surge de similitudes y diferencias superficiales entre idiomas: ortograf√≠as similares, uso de las mismas secuencias de caracteres en la escritura, y posiblemente similitudes en morfolog√≠a y estructura sint√°ctica ‚Äî √≥rdenes comunes de palabras y formas comunes de construir palabras.</p><p>En resumen, hasta el grado en que un modelo est√° aprendiendo equivalencias entre idiomas en el pre-entrenamiento MLM, no es suficiente para superar un fuerte sesgo hacia agrupar textos por idioma. Deja un gran language gap.</p><h3 id=\"contrastive-learning\"><strong>Contrastive Learning</strong></h3><p>Idealmente, queremos que un modelo de embeddings sea indiferente al idioma y solo codifique significados generales en sus embeddings. En un modelo as√≠, no ver√≠amos agrupamiento por idioma y no tendr√≠amos language gap. Las oraciones en un idioma deber√≠an estar muy cerca de buenas traducciones y lejos de otras oraciones que significan algo diferente, incluso en el mismo idioma, como en la figura siguiente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png\" class=\"kg-image\" alt=\"Graph displays &quot;Clustering by Meaning&quot; with multilingual labels, emphasizing abstract concepts on a dark backdrop.\" loading=\"lazy\" width=\"1815\" height=\"1014\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-10---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-10---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-10---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-10---1-.png 1815w\" sizes=\"(min-width: 720px) 720px\"></figure><p>El pre-entrenamiento MLM no logra eso, as√≠ que usamos t√©cnicas adicionales de <em>contrastive learning</em> para mejorar la representaci√≥n sem√°ntica de textos en embeddings.</p><p>El contrastive learning involucra usar pares de textos que se sabe son similares o diferentes en significado, y tripletas donde se sabe que un par es m√°s similar que el otro. Los pesos se ajustan durante el entrenamiento para reflejar esta relaci√≥n conocida entre pares y tripletas de texto.</p><p>Hay 30 idiomas representados en nuestro conjunto de datos de contrastive learning, pero el 97% de los pares y tripletas est√°n en un solo idioma, con solo 3% involucrando pares o tripletas entre idiomas. Pero este 3% es suficiente para producir un resultado dram√°tico: Los embeddings muestran muy poco agrupamiento por idioma y los textos sem√°nticamente similares producen embeddings cercanos independientemente de su idioma, como se muestra en la proyecci√≥n UMAP de embeddings de <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png\" class=\"kg-image\" alt=\"Scatter plot on black background showing language distribution post-contrastive training with UMAP dimensions.\" loading=\"lazy\" width=\"1000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/jina_xlm_contrastive_representation.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/jina_xlm_contrastive_representation.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Para confirmarlo, medimos la Correlaci√≥n de Spearman de las representaciones generadas por <code>jina-xlm-roberta</code> y <code>jina-embeddings-v3</code> en el conjunto de datos STS17.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Correlaci√≥n de Spearman</strong></b></a> mide la correlaci√≥n de rangos, es decir, qu√© tan similares son dos listas ordenadas. Este es un buen mecanismo para comparar modelos de embeddings entre s√≠ y con puntuaciones humanas porque la puntuaci√≥n real es mucho menos importante que qu√© elementos est√°n clasificados por encima o por debajo de otros.</div></div><p>La tabla siguiente muestra la Correlaci√≥n de Spearman entre clasificaciones de similitud sem√°ntica para textos traducidos en diferentes idiomas. Tomamos un conjunto de oraciones en ingl√©s y luego medimos la similitud de sus embeddings con un embedding de una oraci√≥n de referencia espec√≠fica y los ordenamos desde el m√°s similar al menos similar. Luego traducimos todas esas oraciones a otro idioma y repetimos el proceso de clasificaci√≥n. En un modelo de embedding multiling√ºe ideal, las dos listas ordenadas ser√≠an id√©nticas, y la Correlaci√≥n de Spearman ser√≠a 1.0.</p><p>El gr√°fico y la tabla a continuaci√≥n muestran nuestros resultados comparando el ingl√©s y los otros seis idiomas en el benchmark STS17, usando tanto <code>jina-xlm-roberta</code> como <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png\" class=\"kg-image\" alt=\"Bar chart comparing Spearman Correlation for English paired with AR, DE, ES, FR, IT, NL, colored in red and blue by alphabet \" loading=\"lazy\" width=\"2000\" height=\"1056\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-4---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-4---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-4---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-4---1-.png 2085w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong><code>jina-xlm-roberta</code></strong></th>\n<th><strong><code>jina-embeddings-v3</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English ‚Üî Arabic</td>\n<td>0.1581</td>\n<td><strong>0.7977</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî German</td>\n<td>0.2136</td>\n<td><strong>0.8366</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Spanish</td>\n<td>0.1049</td>\n<td><strong>0.8509</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî French</td>\n<td>0.1659</td>\n<td><strong>0.8378</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Italian</td>\n<td>0.2293</td>\n<td><strong>0.8674</strong></td>\n</tr>\n<tr>\n<td>English ‚Üî Dutch</td>\n<td>0.2387</td>\n<td><strong>0.8398</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Aqu√≠ se puede ver la enorme diferencia que hace el aprendizaje contrastivo, comparado con el pre-entrenamiento original. A pesar de tener solo un 3% de datos multiling√ºes en su mezcla de entrenamiento, el modelo <code>jina-embeddings-v3</code> ha aprendido suficiente sem√°ntica multiling√ºe para casi eliminar la brecha ling√º√≠stica que adquiri√≥ en el pre-entrenamiento.</p><h2 id=\"english-vs-the-world-can-other-languages-keep-up-in-alignment\">Ingl√©s vs. El Mundo: ¬øPueden otros idiomas mantenerse al d√≠a en el alineamiento?</h2><p>Entrenamos <code>jina-embeddings-v3</code> en 89 idiomas, con un enfoque particular en 30 idiomas escritos muy utilizados. A pesar de nuestros esfuerzos para construir un corpus de entrenamiento multiling√ºe a gran escala, el ingl√©s todav√≠a representa casi la mitad de los datos que usamos en el entrenamiento contrastivo. Otros idiomas, incluyendo lenguajes globales ampliamente utilizados para los que hay abundante material textual disponible, siguen estando relativamente subrepresentados en comparaci√≥n con la enorme cantidad de datos en ingl√©s en el conjunto de entrenamiento.</p><p>Dado este predominio del ingl√©s, ¬øest√°n las representaciones en ingl√©s m√°s alineadas que las de otros idiomas? Para explorar esto, realizamos un experimento de seguimiento.</p><p>Construimos un conjunto de datos, <a href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><code>parallel-sentences</code></a>, que consiste en 1,000 pares de textos en ingl√©s, un \"ancla\" y un \"positivo\", donde el texto positivo est√° l√≥gicamente implicado por el texto ancla.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/parallel-sentences?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/parallel-sentences ¬∑ Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-3.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/parallel-sentences.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Por ejemplo, la primera fila de la tabla siguiente. Estas oraciones no son id√©nticas en significado, pero tienen significados compatibles. Describen informativamente la misma situaci√≥n.</p><p>Luego tradujimos estos pares a cinco idiomas usando GPT-4: alem√°n, holand√©s, chino (simplificado), chino (tradicional) y japon√©s. Finalmente, los inspeccionamos manualmente para asegurar la calidad.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Anchor</strong></th>\n<th><strong>Positive</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>Two young girls are playing outside in a non-urban environment.</td>\n<td>Two girls are playing outside.</td>\n</tr>\n<tr>\n<td>German</td>\n<td>Zwei junge M√§dchen spielen drau√üen in einer nicht urbanen Umgebung.</td>\n<td>Zwei M√§dchen spielen drau√üen.</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>Twee jonge meisjes spelen buiten in een niet-stedelijke omgeving.</td>\n<td>Twee meisjes spelen buiten.</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>‰∏§‰∏™Âπ¥ËΩªÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁéØÂ¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>‰∏§‰∏™Â•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>ÂÖ©ÂÄãÂπ¥ËºïÂ•≥Â≠©Âú®ÈùûÂüéÂ∏ÇÁí∞Â¢É‰∏≠Áé©ËÄç„ÄÇ</td>\n<td>ÂÖ©ÂÄãÂ•≥Â≠©Âú®Â§ñÈù¢Áé©„ÄÇ</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>2‰∫∫„ÅÆËã•„ÅÑÂ•≥„ÅÆÂ≠ê„ÅåÈÉΩÂ∏ÇÁí∞Â¢É„Åß„ÅØ„Å™„ÅÑÂ†¥ÊâÄ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n<td>‰∫å‰∫∫„ÅÆÂ∞ëÂ•≥„ÅåÂ§ñ„ÅßÈÅä„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Luego codificamos cada par de textos con <code>jina-embeddings-v3</code> y calculamos la similitud del coseno entre ellos. La figura y la tabla siguiente muestran la distribuci√≥n de las puntuaciones de similitud del coseno para cada idioma, y la similitud promedio:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png\" class=\"kg-image\" alt=\"Graph showing cosine similarity distributions for textual pairs in English, German, Dutch, Chinese, and Japanese against dens\" loading=\"lazy\" width=\"1060\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/1_monolingual_distribution_triplets.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/1_monolingual_distribution_triplets.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/1_monolingual_distribution_triplets.png 1060w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>English</td>\n<td>0.9078</td>\n</tr>\n<tr>\n<td>German</td>\n<td>0.8949</td>\n</tr>\n<tr>\n<td>Dutch</td>\n<td>0.8844</td>\n</tr>\n<tr>\n<td>Chinese (Simplified)</td>\n<td>0.8876</td>\n</tr>\n<tr>\n<td>Chinese (Traditional)</td>\n<td>0.8933</td>\n</tr>\n<tr>\n<td>Japanese</td>\n<td>0.8895</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>A pesar del predominio del ingl√©s en los datos de entrenamiento, <code>jina-embeddings-v3</code> reconoce la similitud sem√°ntica en alem√°n, holand√©s, japon√©s y ambas formas de chino casi tan bien como lo hace en ingl√©s.</p><h2 id=\"breaking-language-barriers-cross-lingual-alignment-beyond-english\"><strong>Rompiendo las Barreras Ling√º√≠sticas: Alineamiento Multiling√ºe M√°s All√° del Ingl√©s</strong></h2><p>Los estudios de alineaci√≥n de representaciones entre idiomas t√≠picamente estudian pares de idiomas que incluyen el ingl√©s. Este enfoque podr√≠a, en teor√≠a, ocultar lo que realmente est√° sucediendo. Un modelo podr√≠a simplemente optimizarse para representar todo lo m√°s cerca posible a su equivalente en ingl√©s, sin examinar si otros pares de idiomas est√°n adecuadamente soportados.</p><p>Para explorar esto, realizamos algunos experimentos usando el dataset <code>parallel-sentences</code>, enfoc√°ndonos en la alineaci√≥n entre idiomas m√°s all√° de solo pares biling√ºes con ingl√©s.</p><p>La tabla siguiente muestra la distribuci√≥n de similitudes de coseno entre textos equivalentes en diferentes pares de idiomas ‚Äî textos que son traducciones de una fuente com√∫n en ingl√©s. Idealmente, todos los pares deber√≠an tener un coseno de 1 ‚Äî es decir, embeddings sem√°nticos id√©nticos. En la pr√°ctica, esto nunca podr√≠a ocurrir, pero esperar√≠amos que un buen modelo tenga valores de coseno muy altos para pares de traducci√≥n.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png\" class=\"kg-image\" alt=\"Density graph charting cross-lingual cosine similarities for language pairs using jina-embeddings-v3 model.\" loading=\"lazy\" width=\"978\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--9-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--9-.png 978w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Language Pair</strong></th>\n<th><strong>Average Cosine Similarity</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>German ‚Üî Dutch</td>\n<td>0.8779</td>\n</tr>\n<tr>\n<td>German ‚Üî Japanese</td>\n<td>0.8664</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Japanese</td>\n<td>0.8534</td>\n</tr>\n<tr>\n<td>Dutch ‚Üî Chinese (Simplified)</td>\n<td>0.8479</td>\n</tr>\n<tr>\n<td>Chinese (Simplified) ‚Üî Chinese (Traditional)</td>\n<td>0.8758</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Aunque las puntuaciones de similitud entre diferentes idiomas son un poco m√°s bajas que para textos compatibles en el mismo idioma, siguen siendo muy altas. La similitud de coseno de las traducciones entre holand√©s y alem√°n es casi tan alta como entre textos compatibles en alem√°n.</p><p>Esto podr√≠a no ser sorprendente porque el alem√°n y el holand√©s son idiomas muy similares. De manera similar, las dos variedades de chino probadas aqu√≠ no son realmente dos idiomas diferentes, sino formas estil√≠sticamente diferentes del mismo idioma. Pero se puede ver que incluso pares de idiomas muy diferentes como el holand√©s y el chino o el alem√°n y el japon√©s siguen mostrando una similitud muy fuerte entre textos sem√°nticamente equivalentes.</p><p>Consideramos la posibilidad de que estos valores de similitud tan altos pudieran ser un efecto secundario del uso de ChatGPT como traductor. Para probarlo, descargamos <a href=\"https://help.ted.com/hc/en-us/articles/360018572954-How-do-I-find-transcripts-for-TED-and-TEDx-talks?ref=jina-ai-gmbh.ghost.io\">transcripciones traducidas por humanos de charlas TED</a> en ingl√©s y alem√°n y verificamos si las oraciones traducidas alineadas tendr√≠an la misma alta correlaci√≥n.</p><p>El resultado fue incluso m√°s fuerte que para nuestros datos traducidos por m√°quina, como se puede ver en la figura siguiente.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png\" class=\"kg-image\" alt=\"Graph of cross-lingual alignment density EN-DE with peak around cosine similarity 1.0, titled &quot;jina-embeddings-v3: Cross-ling\" loading=\"lazy\" width=\"988\" height=\"590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image--10-.png 988w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-much-does-cross-language-data-contribute-to-cross-language-alignment\">¬øCu√°nto Contribuyen los Datos Multiling√ºes a la Alineaci√≥n Entre Idiomas?</h2><p>La brecha ling√º√≠stica desvaneciente y el alto nivel de rendimiento entre idiomas parecen desproporcionados en comparaci√≥n con la muy peque√±a parte de los datos de entrenamiento que era expl√≠citamente multiling√ºe. Solo el 3% de los datos de entrenamiento contrastivo espec√≠ficamente ense√±a al modelo c√≥mo hacer alineaciones entre idiomas.</p><p>As√≠ que hicimos una prueba para ver si los datos multiling√ºes estaban haciendo alguna contribuci√≥n.</p><p>Reentrenar completamente <code>jina-embeddings-v3</code> sin ning√∫n dato multiling√ºe ser√≠a prohibitivamente costoso para un peque√±o experimento, as√≠ que descargamos el modelo <a href=\"https://huggingface.co/FacebookAI/xlm-roberta-base?ref=jina-ai-gmbh.ghost.io\"><code>xlm-roberta-base</code> de Hugging Face</a> y lo entrenamos m√°s con aprendizaje contrastivo, usando un subconjunto de los datos que usamos para entrenar <code>jina-embeddings-v3</code>. Espec√≠ficamente ajustamos la cantidad de datos multiling√ºes para probar dos casos: Uno sin datos multiling√ºes, y otro donde el 20% de los pares eran multiling√ºes. Puedes ver los metapar√°metros de entrenamiento en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table id=\"e30425bb-015e-4956-8872-b1b64cdd7ad0\" class=\"simple-table\"><tbody><tr id=\"daa3cfcc-9012-411b-8da3-05c7c6f4b371\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><strong>Backbone</strong></td><td id=\"DU<d\" class=\"\"><strong>% Cross-Language</strong></td><td id=\"@Feo\" class=\"\"><strong>Learning Rate</strong></td><td id=\"fZNx\" class=\"\"><strong>Loss Function</strong></td><td id=\"Rv}\\\" class=\"\"><strong>Temperature</strong></td></tr><tr id=\"f3a2d068-d902-4fc1-8c89-269a5ebbb135\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base </code><strong>without</strong> X-language data</td><td id=\"DU<d\" class=\"\">0%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr><tr id=\"52887e22-326c-46cd-b79c-d6dcd110c1d2\"><td id=\"<j<o\" class=\"\" style=\"width:187.9375px\"><code>xlm-roberta-base</code><strong> with </strong>X-language data</td><td id=\"DU<d\" class=\"\">20%</td><td id=\"@Feo\" class=\"\">5e-4</td><td id=\"fZNx\" class=\"\">InfoNCE</td><td id=\"Rv}\\\" class=\"\">0.05</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Luego evaluamos el rendimiento multiling√ºe de ambos modelos usando los <a href=\"https://github.com/embeddings-benchmark/mteb?ref=jina-ai-gmbh.ghost.io\">benchmarks STS17 y STS22 del MTEB</a> y la Correlaci√≥n de Spearman. Presentamos los resultados a continuaci√≥n:</p><h3 id=\"sts17\"><strong>STS17</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png\" class=\"kg-image\" alt=\"Bar graph showing Spearman correlation for language pairs on STS17 with and without parallel corpus.\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-6---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-6---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-6---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-6---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"de30bf7f-d1a9-43f4-8e5f-15bf65c59674\" class=\"simple-table\"><tbody><tr id=\"5c6440a6-eba9-404a-a5f1-88099bc6702d\"><td id=\"{N[x\" class=\"\"><strong>Language Pair</strong></td><td id=\"jGmJ\" class=\"\"><strong>With parallel corpora</strong></td><td id=\"p<ZH\" class=\"\"><strong>Without parallel corpora</strong></td></tr><tr id=\"33f08461-58aa-43f3-9ed1-577f8676e99d\"><td id=\"{N[x\" class=\"\">English ‚Üî Arabic</td><td id=\"jGmJ\" class=\"\"><strong>0.6418</strong></td><td id=\"p<ZH\" class=\"\">0.5875</td></tr><tr id=\"9875386d-2043-4d9d-8252-e53ec525ec29\"><td id=\"{N[x\" class=\"\">English ‚Üî German</td><td id=\"jGmJ\" class=\"\">0.7364</td><td id=\"p<ZH\" class=\"\"><strong>0.7390</strong></td></tr><tr id=\"15d28a12-3a80-4176-9984-69b5d8a7d8ff\"><td id=\"{N[x\" class=\"\">English ‚Üî Spanish</td><td id=\"jGmJ\" class=\"\"><strong>0.6968</strong></td><td id=\"p<ZH\" class=\"\">0.6799</td></tr><tr id=\"21821558-c8b9-4c34-8ec5-9db3ca7d9328\"><td id=\"{N[x\" class=\"\">English ‚Üî French</td><td id=\"jGmJ\" class=\"\"><strong>0.7066</strong></td><td id=\"p<ZH\" class=\"\">0.6944</td></tr><tr id=\"a2e3b5e5-8e4a-4270-abff-5d059ff6be72\"><td id=\"{N[x\" class=\"\">English ‚Üî Italian</td><td id=\"jGmJ\" class=\"\"><strong>0.7232</strong></td><td id=\"p<ZH\" class=\"\">0.7070</td></tr><tr id=\"95daf20f-2a82-4431-8581-a4ce24d81462\"><td id=\"{N[x\" class=\"\">English ‚Üî Dutch</td><td id=\"jGmJ\" class=\"\"><strong>0.7597</strong></td><td id=\"p<ZH\" class=\"\">0.7468</td></tr><tr id=\"4fd4c45a-a69e-4e33-b057-9c5f10b99fdb\"><td id=\"{N[x\" class=\"\">English ‚Üî Turkish</td><td id=\"jGmJ\" class=\"\"><strong>0.6933</strong></td><td id=\"p<ZH\" class=\"\">0.6050</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h3 id=\"sts22\"><strong>STS22</strong></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png\" class=\"kg-image\" alt=\"Chart comparing models of language alignment, showing Spearman correlation scores for eight language pairs with and without p\" loading=\"lazy\" width=\"2000\" height=\"1032\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/output-7---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/output-7---1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/output-7---1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/output-7---1-.png 2133w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table id=\"e53757df-8a0e-42ec-ba05-715baa3c77cd\" class=\"simple-table\"><tbody><tr id=\"45c43b8c-b1b7-4ac0-91b0-1e07025f1b92\"><td id=\"OF=p\" class=\"\"><strong>Par de idiomas</strong></td><td id=\"P<\\i\" class=\"\"><strong>Con corpus paralelos</strong></td><td id=\"LAtp\" class=\"\"><strong>Sin corpus paralelos</strong></td></tr><tr id=\"696ebe11-3eda-49ef-8dfe-608f9b71430b\"><td id=\"OF=p\" class=\"\">Ingl√©s ‚Üî Espa√±ol</td><td id=\"P<\\i\" class=\"\"><strong>0.7710</strong></td><td id=\"LAtp\" class=\"\">0.7675</td></tr><tr id=\"eb4c1d81-7d98-453d-af3f-95b2adccfb55\"><td id=\"OF=p\" class=\"\">Chino simplificado ‚Üî Ingl√©s</td><td id=\"P<\\i\" class=\"\"><strong>0.6885</strong></td><td id=\"LAtp\" class=\"\">0.6860</td></tr><tr id=\"533cefd3-b30e-4d6a-9350-8f5d28b17ba6\"><td id=\"OF=p\" class=\"\">Espa√±ol ‚Üî Italiano</td><td id=\"P<\\i\" class=\"\"><strong>0.6829</strong></td><td id=\"LAtp\" class=\"\">0.6814</td></tr><tr id=\"d3ecdd71-44bb-4a3b-9ac2-8cc90a785d5f\"><td id=\"OF=p\" class=\"\">Alem√°n ‚Üî Franc√©s</td><td id=\"P<\\i\" class=\"\"><strong>0.5763</strong></td><td id=\"LAtp\" class=\"\">0.5496</td></tr><tr id=\"c6242853-4da7-4369-b1f1-1a27262a487a\"><td id=\"OF=p\" class=\"\">Alem√°n ‚Üî Ingl√©s</td><td id=\"P<\\i\" class=\"\">0.5439</td><td id=\"LAtp\" class=\"\"><strong>0.5566</strong></td></tr><tr id=\"31a4a5ba-199c-4904-b926-ff0561aac1b5\"><td id=\"OF=p\" class=\"\">Polaco ‚Üî Ingl√©s</td><td id=\"P<\\i\" class=\"\">0.6966</td><td id=\"LAtp\" class=\"\"><strong>0.7156</strong></td></tr><tr id=\"4f529d81-e8c9-4e5d-a705-36e357abebc3\"><td id=\"OF=p\" class=\"\">Alem√°n ‚Üî Ingl√©s</td><td id=\"P<\\i\" class=\"\"><strong>0.5832</strong></td><td id=\"LAtp\" class=\"\">0.5478</td></tr><tr id=\"cd1429f7-c810-4a0e-9dca-88e2c83157bc\"><td id=\"OF=p\" class=\"\">Franc√©s ‚Üî Polaco</td><td id=\"P<\\i\" class=\"\">0.8451</td><td id=\"LAtp\" class=\"\">0.8451</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Nos sorprendi√≥ ver que para la mayor√≠a de los pares de idiomas que probamos, los datos de entrenamiento interling√º√≠stico aportaron poca o ninguna mejora. Es dif√≠cil asegurar que esto seguir√≠a siendo cierto para modelos completamente entrenados con conjuntos de datos m√°s grandes, pero ciertamente ofrece evidencia de que el entrenamiento expl√≠cito entre idiomas no aporta mucho.</p><p>Sin embargo, hay que tener en cuenta que STS17 incluye pares de ingl√©s/√°rabe e ingl√©s/turco. Estos son idiomas mucho menos representados en nuestros datos de entrenamiento. El modelo XML-RoBERTa que utilizamos fue pre-entrenado con datos que conten√≠an un 2,25% en √°rabe y un 2,32% en turco, mucho menos que los otros idiomas que probamos. El peque√±o conjunto de datos de aprendizaje contrastivo que utilizamos en este experimento solo conten√≠a un 1,7% de √°rabe y un 1,8% de turco.</p><p>Esos dos pares de idiomas son los √∫nicos probados donde el entrenamiento con datos interling√º√≠sticos marc√≥ una clara diferencia. Creemos que los datos interling√º√≠sticos expl√≠citos son m√°s efectivos para idiomas que est√°n menos representados en los datos de entrenamiento, pero necesitamos explorar m√°s esta √°rea antes de sacar una conclusi√≥n. El papel y la efectividad de los datos interling√º√≠sticos en el entrenamiento contrastivo es un √°rea donde Jina AI est√° realizando investigaci√≥n activa.</p><h2 id=\"conclusion\">Conclusi√≥n</h2><p>Los m√©todos convencionales de pre-entrenamiento de lenguaje, como el Modelado de Lenguaje Enmascarado, dejan una \"brecha ling√º√≠stica\", donde textos sem√°nticamente similares en diferentes idiomas no se alinean tan estrechamente como deber√≠an. Hemos demostrado que el r√©gimen de aprendizaje contrastivo de Jina Embeddings es muy efectivo para reducir o incluso eliminar esta brecha.</p><p>Las razones por las que esto funciona no est√°n completamente claras. Utilizamos pares de texto expl√≠citamente interling√º√≠sticos en el entrenamiento contrastivo, pero solo en cantidades muy peque√±as, y no est√° claro qu√© papel juegan realmente en asegurar resultados interling√º√≠sticos de alta calidad. Nuestros intentos de mostrar un efecto claro usando condiciones m√°s controladas no produjeron un resultado inequ√≠voco.</p><p>Sin embargo, <strong>est√° claro que <code>jina-embeddings-v3</code> ha superado la brecha ling√º√≠stica del pre-entrenamiento, convirti√©ndose en una herramienta poderosa para aplicaciones multiling√ºes.</strong> Est√° listo para usar en cualquier tarea que requiera un rendimiento fuerte e id√©ntico en m√∫ltiples idiomas.</p><p>Puedes usar <code>jina-embeddings-v3</code> a trav√©s de nuestra <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">API de Embeddings</a> (con un mill√≥n de tokens gratuitos) o a trav√©s de AWS o Azure. Si quieres usarlo fuera de estas plataformas o en las instalaciones de tu empresa, ten en cuenta que est√° licenciado bajo CC BY-NC 4.0. <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io\">Cont√°ctanos</a> si est√°s interesado en el uso comercial.</p>",
  "comment_id": "67066bd652567c0001d0f2cd",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/gap-blog-1.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-09T13:41:10.000+02:00",
  "updated_at": "2024-10-10T20:15:26.000+02:00",
  "published_at": "2024-10-09T14:42:22.000+02:00",
  "custom_excerpt": "Multilingual models often face a \"language gap,\" where similar phrases in different languages don't align. We show how contrastive learning can bridge this gap, enhancing cross-language performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/",
  "excerpt": "Los modelos multiling√ºes a menudo enfrentan una \"brecha ling√º√≠stica\", donde frases similares en diferentes idiomas no se alinean. Mostramos c√≥mo el aprendizaje contrastivo puede cerrar esta brecha, mejorando el rendimiento entre idiomas.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon green squares form intricate patterns on a black digital background, creating a dynamic, abstract design.",
  "feature_image_caption": null
}
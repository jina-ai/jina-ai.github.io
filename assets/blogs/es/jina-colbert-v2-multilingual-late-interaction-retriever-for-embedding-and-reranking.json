{
  "slug": "jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking",
  "id": "66cd8fc6e84873000133d63d",
  "uuid": "e995c4d9-1832-4e2a-8108-e8453f5c82c5",
  "title": "Jina ColBERT v2: Recuperador de interacción tardía multilingüe para incrustación y reordenamiento",
  "html": "<p>Hoy, nos complace lanzar Jina ColBERT v2 (<code>jina-colbert-v2</code>), un avanzado modelo de recuperación de interacción tardía basado en la arquitectura ColBERT. Este nuevo modelo de lenguaje mejora el rendimiento de <code>jina-colbert-v1-en</code> y añade soporte multilingüe y dimensiones de salida dinámicas.</p><p>Este nuevo lanzamiento destaca las siguientes características:</p><ul><li><strong>Rendimiento superior en recuperación</strong> comparado con el ColBERT-v2 original (+6,5%) y nuestro lanzamiento anterior, <code>jina-colbert-v1-en</code> (+5,4%).</li><li><strong>Soporte multilingüe</strong> para 89 idiomas, ofreciendo un fuerte rendimiento en los principales idiomas globales.</li><li><strong>Tamaños de embedding de salida controlados por el usuario</strong> a través del aprendizaje de representación Matryoshka, permitiendo a los usuarios equilibrar flexiblemente entre eficiencia y precisión.</li></ul><h2 id=\"technical-summary-of-jina-colbert-v2\">Resumen Técnico de <code>jina-colbert-v2</code></h2><p>El informe técnico completo se puede encontrar en arXiv:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2408.16672?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever</div><div class=\"kg-bookmark-description\">Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Rohan Jha</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th><code>jina-colbert-v2</code></th>\n<th><code>jina-colbert-v1-en</code></th>\n<th>Original ColBERTv2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Average of 14 English<br/>BEIR tasks</td>\n<td><b>0.521</b></td>\n<td>0.494</td>\n<td>0.489</td>\n</tr>\n<tr>\n<td>Multilingual</td>\n<td><b>89 languages</b></td>\n<td>English-only</td>\n<td>English-only</td>\n</tr>\n<tr>\n<td>Output dimensions</td>\n<td><b>128, 96, or 64</b></td>\n<td>Fixed 128</td>\n<td>Fixed 128</td>\n</tr>\n<tr>\n<td>Max query length</td>\n<td>32 tokens</td>\n<td>32 tokens</td>\n<td>32 tokens</td>\n</tr>\n<tr>\n<td>Max document length</td>\n<td>8192 tokens</td>\n<td>8192 tokens</td>\n<td>512 tokens</td>\n</tr>  \n\n<tr>\n<td>Parameters</td>\n<td>560M</td>\n<td>137M</td>\n<td>110M</td>\n</tr>\n<tr>\n<td>Model size</td>\n<td>1.1GB</td>\n<td>550MB</td>\n<td>438MB</td>\n</tr>\n\n\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"asymmetric-embedding-in-colbert\">Embedding Asimétrico en ColBERT</h2><p>ColBERT se basa en la arquitectura BERT añadiendo <strong>interacción tardía</strong> y codificación <strong>asimétrica</strong> de consulta-documento.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>La naturaleza asimétrica de ColBERT significa que al usar modelos como <code>jina-colbert-v2</code> o <code>jina-colbert-v1-en</code>, necesitas especificar si estás embebiendo una consulta, un documento o ambos (para propósitos de reordenamiento). Esta flexibilidad adicional mejora el rendimiento sobre los modelos de embedding homogéneos en tareas de recuperación.</p><h2 id=\"multilingual-support-for-over-89-languages\">Soporte Multilingüe Para Más de 89 Idiomas</h2><p>Jina ColBERT v2 tiene amplias capacidades multilingües, diseñadas para satisfacer las demandas de recuperación de información y aplicaciones de IA modernas y globalizadas. El corpus de entrenamiento para <code>jina-colbert-v2</code> incorpora 89 idiomas, con etapas adicionales de entrenamiento para los principales idiomas internacionales incluyendo <strong>árabe, chino, inglés, francés, alemán, japonés, ruso y español</strong>, así como <strong>lenguajes de programación</strong>. El entrenamiento también incluyó un corpus de textos bilingües alineados para desbloquear potenciales interlingüísticos, permitiendo que las consultas y documentos en diferentes idiomas se emparejen en tareas de reordenamiento/recuperación.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Distribution-of-the-languages-in-the-training-corpus-at-the-pretrained-stage--3-.svg\" class=\"kg-image\" alt=\"Chart of language distribution in training data, highlighting dominance of English and Chinese.\" loading=\"lazy\" width=\"1456\" height=\"743\"><figcaption><span style=\"white-space: pre-wrap;\">Distribución de datos del conjunto de pre-entrenamiento por idioma (especificado por código </span><a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ISO-639</span></a><span style=\"white-space: pre-wrap;\">) en escala logarítmica.</span></figcaption></figure><p>Actualmente, Jina ColBERT v2 destaca como <strong>el único modelo tipo ColBERT multilingüe</strong> que genera embeddings compactos, superando significativamente la recuperación basada en BM25 en todos los idiomas probados en los benchmarks MIRACL.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-Multilingual-Data--1-.svg\" class=\"kg-image\" alt=\"Bar chart comparing jina-colbert-v2 and BM25 performance across 20 languages on multilingual tasks.\" loading=\"lazy\" width=\"691\" height=\"426\"><figcaption><span style=\"white-space: pre-wrap;\">Rendimiento de Jina ColBERT v2 en 16 idiomas, comparado con BM25, en benchmarks MIRACL.</span></figcaption></figure><p>Además, en tareas de recuperación en inglés, Jina ColBERT v2 supera el rendimiento de su predecesor <code>jina-colbert-v1-en</code> y el modelo ColBERT v2 original, con un rendimiento comparable al modelo altamente especializado solo en inglés <a href=\"https://huggingface.co/answerdotai/answerai-colbert-small-v1?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">AnswerAI-ColBERT-small</a>.</p>\n<!--kg-card-begin: html-->\n<table class=\"simple-table\">\n  <tbody>\n<thead>\n<tr>\n      <th><strong>Model Name</strong></th>\n      <th><strong>Average score<br>(14 BEIR English-only benchmarks)<br></strong></th>\n      <th><strong>Multilingual Support</strong></th>\n  </tr>\n    </thead>\n    <tr>\n      <td><code>jina-colbert-v2</code></td>\n      <td>0.521</td>\n      <td>Multilingual</td>\n    </tr>\n    <tr>\n      <td><code>jina-colbert-v1-en</code></td>\n      <td>0.494</td>\n      <td>English-only</td>\n    </tr>\n    <tr>\n      <td>ColBERT v2.0</td>\n      <td>0.489</td>\n      <td>English-only</td>\n    </tr>\n    <tr>\n      <td>AnswerAI-ColBERT-small</td>\n      <td>0.549</td>\n      <td>English-only</td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Evaluation-on-English-only-datasets-from-BEIR--2-.svg\" class=\"kg-image\" alt=\"Bar chart showing model evaluations on English BEIR datasets, with several models like 'jina-colbert' and 'BM25'.\" loading=\"lazy\" width=\"1088\" height=\"712\"><figcaption><span style=\"white-space: pre-wrap;\">Evaluación de jina-colbert-v2 en una selección de conjuntos de datos solo en inglés del benchmark BEIR.</span></figcaption></figure><h2 id=\"matryoshka-representation-learning\">Aprendizaje de Representación Matryoshka</h2><p><a href=\"https://arxiv.org/abs/2205.13147?ref=jina-ai-gmbh.ghost.io\">El Aprendizaje de Representación Matryoshka</a> es una técnica para entrenar modelos que soporten diferentes tamaños de vectores de salida mientras minimizan cualquier pérdida en precisión. Entrenamos las capas ocultas de la red con varias cabezas de proyección lineal diferentes — las capas finales de una red neuronal — cada una soportando un tamaño de salida diferente. <strong>Jina ColBERT v2 soporta vectores de salida de 128, 96 y 64 dimensiones.</strong></p><p>Jina ColBERT v2 produce embeddings de salida de 128 dimensiones por defecto, pero puede producir 96 y 64 dimensiones que son casi idénticas en rendimiento pero son 25% y 50% más cortas respectivamente.</p><p>La tabla siguiente muestra el rendimiento <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\">nDGC</a> de<code>jina-colbert-v2</code> para los diez primeros resultados (<em>nDGC@10</em>) en seis conjuntos de datos del benchmark BEIR. Aquí puede verse que la diferencia en rendimiento entre 128 dimensiones y 96 es apenas del 1% y menos del 1,5% entre 128 y 64 dimensiones.</p>\n<!--kg-card-begin: html-->\n<table id=\"b838dc78-1321-499e-98e7-63e3b5c8e910\" class=\"simple-table\"><tbody><thead id=\"177f4349-0620-4947-a3ce-01e598395ed7\"><tr><th id=\"<\\ml\" class=\"\"><strong>Dimensiones de Salida</strong></th><th id=\"<NYX\" class=\"\"><strong>Puntuación </strong><strong>Promedio</strong><strong><br>(nDGC@10 para 6 benchmarks)<br></strong></th></tr></thead><tr id=\"9199b56b-0513-4c99-a2a7-29cde915c3b9\"><td id=\"<\\ml\" class=\"\">128</td><td id=\"<NYX\" class=\"\">0.565</td></tr><tr id=\"af4d45fc-ebf4-4e1f-b5b0-1807a1cb889b\"><td id=\"<\\ml\" class=\"\">96</td><td id=\"<NYX\" class=\"\">0.558</td></tr><tr id=\"ecf7eac9-5c56-47e6-ab27-0ddb4659e263\"><td id=\"<\\ml\" class=\"\">64</td><td id=\"<NYX\" class=\"\">0.556</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Performance-on-selected-BEIR-benchmarks.svg\" class=\"kg-image\" alt=\"Gráfico de barras de benchmarks BEIR, destacando puntuaciones de conjuntos de datos desde nfcorpus hasta msmarco, donde jina-colbert-v2.64 sobresale.\" loading=\"lazy\" width=\"732\" height=\"538\"><figcaption><span style=\"white-space: pre-wrap;\">Rendimiento de Jina ColBERT v2 con diferentes dimensiones de salida.</span></figcaption></figure><p>Reducir el tamaño de los vectores de salida ahorra espacio y acelera aplicaciones como la recuperación de información basada en vectores que tienen que comparar diferentes vectores o medir la distancia entre ellos.</p><p>Esto tiene consecuencias significativas en los costos, incluso solo en términos de almacenamiento reducido. Por ejemplo, usando el <a href=\"https://cloud.qdrant.io/calculator?ref=jina-ai-gmbh.ghost.io\">calculador de costos de Qdrant cloud</a>, almacenar 100 millones de documentos en AWS con vectores de 128 dimensiones para cada uno tiene un <a href=\"https://cloud.qdrant.io/calculator?provider=aws&region=eu-central-1&replicas=1&quantization=None&vectors=100000000&dimension=128&ref=jina-ai-gmbh.ghost.io\">costo estimado de US$1.319,24 por mes</a>. Con 64 dimensiones, esto <a href=\"https://cloud.qdrant.io/calculator?provider=aws&region=eu-central-1&replicas=1&quantization=None&vectors=100000000&dimension=64&ref=jina-ai-gmbh.ghost.io\">se reduce a US$659,62</a>.</p><h2 id=\"getting-started-with-jina-colbert-v2\">Comenzando con Jina ColBERT v2</h2><p>Jina ColBERT v2 está disponible a través de la API Jina Search Foundation, el <a href=\"https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy&ref=jina-ai-gmbh.ghost.io\">mercado de AWS</a>, y <a href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\">en Azure</a>. También está disponible para <em>uso no comercial solamente</em> (<a href=\"https://www.creativecommons.org/licenses/by-nc/4.0/deed.en?ref=jina-ai-gmbh.ghost.io\">CC BY-NC-4.0</a>) a través de <a href=\"https://huggingface.co/jinaai/jina-colbert-v2?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>.</p><h3 id=\"via-jina-search-foundation-api\">A través de la API Jina Search Foundation</h3><h4 id=\"for-embedding\">Para Embeddings</h4><p>El siguiente comando <code>curl</code> muestra cómo especificar la entrada y las opciones para obtener embeddings de documentos de <code>jina-colbert-v2</code> a través de la API de Embeddings de Jina. Para obtener vectores del tamaño deseado, especifique 128 o 64 para el parámetro <code>dimensions</code>. Este parámetro es opcional y el valor predeterminado es 128.</p><p>Los documentos de entrada serán truncados si son más largos de 8192 tokens.</p><p>Especifique su clave API de Jina en el encabezado de autorización <code>Authorization: Bearer &lt;TU CLAVE API DE JINA&gt;</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # O 64 para vectores de mitad de tamaño\n\t\"input_type\": \"document\", # Para embeddings de consulta ver abajo\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your document text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each text can be up to 8192 tokens long\"\n    ]}'\n</code></pre><p>Para obtener embeddings de consulta, establezca el parámetro <code>input_type</code> a <code>query</code> en lugar de <code>document</code>. Tenga en cuenta que las consultas tienen límites de tamaño mucho más estrictos que los documentos. Se truncarán a 32 tokens. La codificación de consultas <em>siempre</em> devolverá 32 tokens, incluyendo embeddings para el relleno si hay menos de 32 tokens.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/multi-vector \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n\t\"model\": \"jina-colbert-v2\",\n\t\"dimensions\": 128, # O 64 para vectores de mitad de tamaño\n\t\"input_type\": \"query\", # Esto debe especificarse para embeddings de consulta\n\t\"embedding_type\": \"float\",\n\t\"input\": [\n\t\t\"Your query text string goes here\", \n\t\t\"You can send multiple texts\", \n\t\t\"Each query text can be up to 32 tokens long\"\n    ]}'\n</code></pre><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Embeddings multimodales y bilingües de contexto largo para tu búsqueda y RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"for-reranking\">Para Reordenamiento</h4><p>Para usar <code>jina-colbert-v2</code> a través de la API Jina Reranker, pasando una consulta y varios documentos y obteniendo puntuaciones de coincidencia clasificables, construya su solicitud como la siguiente:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/rerank \\\\\n\t -H \"Content-Type: application/json\" \\\\\n\t -H \"Authorization: Bearer &lt;YOUR JINA API KEY&gt;\" \\\\\n\t -d '{\n      \"model\": \"jina-colbert-v2\",\n      \"query\": \"What is the population of Berlin?\",\n      \"top_n\": 3,\n      \"documents\": [\n        \"Berlin's population grew by 0.7 percent in 2023 compared with the previous year. Accordingly, around 27,300 more residents lived in Berlin at the end of the last year than in 2022. Those of 30 to under 40 years old form the numerically largest age group. With roughly 881,000 foreign residents from around 170 nations and an average age of the population of 42.5 years old.\",\n        \"Mount Berlin is a glacier-covered volcano in Marie Byrd Land, Antarctica, 100 kilometres (62 mi) from the Amundsen Sea. It is a roughly 20-kilometre-wide (12 mi) mountain with parasitic vents that consists of two coalesced volcanoes: Berlin proper with the 2-kilometre-wide (1.2 mi) Berlin Crater and Merrem Peak with a 2.5-by-1-kilometre-wide (1.55 mi × 0.62 mi) crater, 3.5 kilometres (2.2 mi) away from Berlin.\",\n        \"Population as of 31.12.2023 by nationality and federal states Land\\\\tTotal\\\\tGermans\\\\tForeigners\\\\tincluding EU-states number\\\\t%\\\\tnumber\\\\t%\",\n        \"The urban area of Berlin has a population of over 4.5 million and is therefore the most populous urban area in Germany. The Berlin-Brandenburg capital region has around 6.2 million inhabitants and is Germany's second-largest metropolitan region after the Rhine-Ruhr region, and the sixth-biggest metropolitan region by GDP in the European Union.\",\n        \"Irving Berlin (born Israel Beilin) was an American composer and songwriter. His music forms a large part of the Great American Songbook. Berlin received numerous honors including an Academy Award, a Grammy Award, and a Tony Award.\",\n        \"Berlin is a town in the Capitol Planning Region, Connecticut, United States. The population was 20,175 at the 2020 census.\",\n        \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\",\n        \"Berlin, Berlin ist eine für die ARD produzierte Fernsehserie, die von 2002 bis 2005 im Vorabendprogramm des Ersten ausgestrahlt wurde. Regie führten unter anderem Franziska Meyer Price, Christoph Schnee, Sven Unterwaldt Jr. und Titus Selge.\"\n        ]\n    }'</code></pre><p>Tenga en cuenta el argumento <code>top_n</code>, que especifica el número de documentos que desea recuperar. Por ejemplo, si su aplicación solo usa la coincidencia superior, establezca <code>top_n</code> a 1.</p><p>Para fragmentos de código en Python y otros lenguajes de programación y frameworks, vaya a la <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">página de la API de Embeddings de Jina AI</a>, o seleccione <code>jina-colbert-v2</code> del menú desplegable en la <a href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\">página de la API Jina Reranker</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker/?ref=jina-ai-gmbh.ghost.io#apiform\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Maximice la relevancia de búsqueda y la precisión RAG con facilidad.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"via-stanford-colbert\">A través de Stanford ColBERT</h3><p>También puedes usar Jina ColBERT v2 como reemplazo directo de <a href=\"https://github.com/stanford-futuredata/ColBERT?ref=jina-ai-gmbh.ghost.io\">ColBERT v2</a> en la biblioteca Stanford ColBERT. Simplemente especifica <code>jinaai/jina-colbert-v2</code> como fuente del modelo:</p><pre><code class=\"language-python\">from colbert.infra import ColBERTConfig\nfrom colbert.modeling.checkpoint import Checkpoint\n\nckpt = Checkpoint(\"jinaai/jina-colbert-v2\", colbert_config=ColBERTConfig())\ndocs = [\"Your list of texts\"] \nquery_vectors = ckpt.queryFromText(docs)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Debes instalar <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> y <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> para usar el código anterior.</div></div><h3 id=\"via-ragatouille\">Vía RAGatouille</h3><p>Jina ColBERT v2 está integrado de manera similar en <a href=\"https://github.com/AnswerDotAI/RAGatouille?ref=jina-ai-gmbh.ghost.io\">RAGatouille</a>. Puedes descargarlo y usarlo mediante el método <code>RAGPretrainedModel.from_pretrained()</code>:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v2\")\ndocs = [\"Your list of texts\"]\nRAG.index(docs, index_name=\"your_index_name\")\nquery = \"Your query\"\nresults = RAG.search(query)\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Debes instalar <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">einops</code> y <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">flash_attn</code> para usar el código anterior.</div></div><h3 id=\"via-qdrant\">Vía Qdrant</h3><p>Desde la versión 1.10, Qdrant ha añadido <a href=\"https://qdrant.tech/blog/qdrant-1.10.x/?ref=jina-ai-gmbh.ghost.io\">soporte</a> para multi-vectores y modelos de interacción tardía. Los usuarios existentes de motores Qdrant, ya sean versiones locales o gestionadas en la nube, pueden beneficiarse integrando directamente <code>jina-colbert-v2</code> usando el cliente de Qdrant.</p><p><strong>Creando una nueva Colección usando la operación MAX_SIM</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nqdrant_client.create_collection(\n    collection_name=\"{collection_name}\",\n    vectors_config={\n        \"colbert\": models.VectorParams(\n            size=128,\n            distance=models.Distance.COSINE,\n            multivector_config=models.MultiVectorConfig(\n                comparator=models.MultiVectorComparator.MAX_SIM\n            ),\n        )\n    }\n)</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">⚠️</div><div class=\"kg-callout-text\">Es esencial configurar correctamente el parámetro <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">multivector_config</code> para usar modelos tipo ColBERT en Qdrant.</div></div><p><strong>Insertando Documentos en Colecciones Multi-vector</strong></p><pre><code class=\"language-Python\">import requests\nfrom qdrant_client import QdrantClient, models\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\ndata = {\n    'model': 'jina-colbert-v2',\n    'input_type': 'query',\n    'embedding_type': 'float',\n    'input': [\n        'Your text string goes here',\n        'You can send multiple texts',\n        'Each text can be up to 8192 tokens long'\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nrows = response.json()[\"data\"]\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nfor i, row in enumerate(rows):\n    qdrant_client.upsert(\n        collection_name=\"{collection_name}\",\n        points=[\n            models.PointStruct(\n                id=i,  \n                vector=row[\"embeddings\"],  \n                payload={\"text\": data[\"input\"][i]} \n            )\n        ],\n    )</code></pre><p><strong>Consultando Colecciones</strong></p><pre><code class=\"language-Python\">from qdrant_client import QdrantClient, models\nimport requests\n\nurl = 'https://api.jina.ai/v1/multi-vector'\n\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer &lt;YOUR BEARER&gt;'\n}\n\n\ndata = {\n    'model': 'jina-colbert-v2',\n    \"input_type\": \"query\",\n    \"embedding_type\": \"float\",\n    \"input\": [\n        \"how many tokens in an input do Jina AI's embedding models support?\"\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nvector = response.json()[\"data\"][0][\"embeddings\"]\n\n\nqdrant_client = QdrantClient(\n    url=\"&lt;YOUR_ENDPOINT&gt;\",\n    api_key=\"&lt;YOUR_API_KEY&gt;\",\n)\n\nresults = qdrant_client.query_points(\n    collection_name=\"{collection_name}\",\n    query=vector,\n)\n\nprint(results)</code></pre><h3 id=\"summary\">Resumen</h3><p>Jina ColBERT v2 (<code>jina-colbert-v2</code>) se basa en el alto rendimiento de <code>jina-colbert-v1-en</code> y expande sus capacidades a una amplia gama de idiomas globales. Con soporte para múltiples tamaños de embeddings, <code>jina-colbert-v2</code> permite a los usuarios ajustar el equilibrio entre precisión y eficiencia para adaptarse a sus casos de uso específicos, potencialmente ofreciendo ahorros significativos en tiempo y costos de computación.</p><p>Este modelo combina todas estas características en un solo paquete a precio competitivo, accesible a través de una API web intuitiva y compatible con cualquier marco de computación que soporte solicitudes HTTP. <a href=\"https://jina.ai/?sui=&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">Pruébalo tú mismo</a> con 1 millón de tokens gratuitos para ver cómo puede mejorar tus aplicaciones y procesos.</p>",
  "comment_id": "66cd8fc6e84873000133d63d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-27T10:35:18.000+02:00",
  "updated_at": "2024-09-09T07:43:38.000+02:00",
  "published_at": "2024-08-30T09:19:58.000+02:00",
  "custom_excerpt": "Jina ColBERT v2 supports 89 languages with superior retrieval performance, user-controlled output dimensions, and 8192 token-length. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking/",
  "excerpt": "ColBERT v2 de Jina admite 89 idiomas con un rendimiento de recuperación superior, dimensiones de salida controladas por el usuario y una longitud de token de 8192.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Dark-themed coding interface displaying English and Japanese characters with \"JINA COLBERT V2\" highlighted in the center.",
  "feature_image_caption": null
}
{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "¿Qué deberíamos aprender de ModernBERT?",
  "html": "<p>Allá por 2018, Google lanzó BERT y fue un cambio revolucionario para NLP, mucho antes de la actual ola de LLM. Incluso ahora, muchos Modelos de Lenguaje Pequeños están construidos sobre BERT. En diciembre de 2024, <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> toma lo que hemos aprendido de los recientes desarrollos de LLM y lo aplica a estos modelos más pequeños. ¿Las claves? Mejor eficiencia de parámetros, comprensión de código y manejo de contextos largos.</p><p>En esta publicación, analizaremos cómo se compara ModernBERT con dos modelos que conocemos a fondo: <code>jina-XLM-RoBERTa</code> (la columna vertebral multilingüe detrás de <code>jina-embeddings-v3</code>) y <code>RobERTa-large</code>. Veamos cada modelo:</p><ul><li><strong>ModernBERT</strong> (dic. 2024) es un SLM recientemente lanzado, desarrollado en colaboración por Answer.AI, LightOn y HuggingFace. Aprovecha optimizaciones modernas como RoPE para una ventana de contexto de 8,192 tokens y capas <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">GeGLU</a>, mejorando el rendimiento mientras mantiene la eficiencia.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong> </strong>(sept. 2024) es un modelo de embeddings de texto multilingüe basado en <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> de Meta. Mientras que el <code>XLM-RoBERTa</code> original mejora <code>RoBERTa</code> usando el conjunto de datos multilingüe XLM, <code>jina-XLM-RoBERTa</code> va más allá con entrenamiento de contexto extendido, implementación de <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> y soporte para <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a>. Este modelo sirve como base para <code>jina-embeddings-v3</code>.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a> (julio 2019) desarrollado por Meta, es una versión mejorada de BERT con 355 millones de parámetros. A través de entrenamiento extendido, conjuntos de datos más grandes e innovaciones como el enmascaramiento dinámico, ha logrado resultados impresionantes en benchmarks clave incluyendo <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a> y <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a>. Esto lo hace adecuado para varias tareas de NLP, desde clasificación de texto hasta respuesta de preguntas.</li></ul><p>Al comparar estos modelos en tres aspectos centrales, buscamos destacar las efectivas decisiones de diseño de ModernBERT para otros desarrolladores de modelos e identificar insights clave de desarrollo para futuros modelos tipo BERT. También compartiremos nuestros aprendizajes del desarrollo de <code>jina-embeddings-v3</code> y discutiremos mejoras planeadas para <code>jina-embeddings-v4</code> y <code>jina-reranker-v3</code>.</p><h2 id=\"modernberts-parameter-efficiency\">Eficiencia de Parámetros de ModernBERT</h2><p>Primero examinemos el enfoque de ModernBERT hacia la eficiencia de parámetros - incorpora varios insights clave de desarrollos recientes de LLM. ModernBERT aprovecha tres estrategias principales: una arquitectura más profunda pero más delgada, tamaño de vocabulario controlado, y escalamiento progresivo del modelo comenzando desde modelos más pequeños.</p><h3 id=\"deep-and-thin-architecture\">Arquitectura Profunda y Delgada</h3><p>ModernBERT-large se hace más profundo con 28 capas, mientras que <code>jina-XLM-RoBERTa</code> y <code>RoBERTa-large</code> funcionan con 24. Pero aquí está lo interesante - iguala a <code>RoBERTa-large</code> en cantidad de parámetros a pesar de esas capas extra. <code>jina-XLM-RoBERTa</code> necesita más parámetros ya que maneja 89 idiomas, mientras que los otros dos se enfocan solo en inglés.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">La profundidad (número de capas) es más importante que el ancho (número de unidades ocultas) para LLMs pequeños. Una estructura de modelo profunda y delgada sobresale en capturar conceptos abstractos, resultando en un rendimiento final superior.</span></figcaption></figure><p>La mayoría de los parámetros de un transformer provienen de las capas de atención y completamente conectadas. ModernBERT se mantiene competitivo en tamaño haciéndose \"más delgado\" - ejecuta 2,624 unidades ocultas a través de 28 capas, comparado con las 4,096 unidades de RoBERTa-large a través de 24 capas. Esta configuración más \"profunda\" pero delgada les permite alcanzar sus objetivos de rendimiento sin inflar el modelo.</p>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parámetros</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Estados ocultos</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Dimensiones intermedias</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Cabezas de atención</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Capas</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Tamaño de vocabulario</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Este enfoque se alinea con la investigación <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> de Meta, que encontró que para modelos más pequeños, la profundidad importa más que el ancho cuando se trata de capturar patrones complejos y mejorar el rendimiento. Esencialmente, la capacidad de procesar información a través de más capas transformer resulta más valiosa que tener capas más anchas para procesamiento paralelo.</p><p>Veamos los datos sobre cómo se desempeña esta arquitectura profunda y delgada.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">Comparado con modelos similares que usan la arquitectura tradicional superficial-gruesa, ModernBERT está entregando mejores resultados en tareas clave como recuperación y STS - todo mientras mantiene un conteo similar de parámetros.</span></figcaption></figure>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Promedio</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Tome <code>jina-XLM-RoBERTa</code> - se basa en la arquitectura superficial-gruesa de <code>RoBERTa-large</code> pero aumenta el vocabulario de 50K a 250K tokens y entrena con más datos. Sin embargo, ModernBERT aún lo supera, sugiriendo que el cambio arquitectónico está marcando una diferencia real en eficiencia.</p><h3 id=\"vocabulary-size-matters\">El Tamaño del Vocabulario Importa</h3><p>Primero, veamos cómo se cuentan los parámetros de vocabulario en transformers. Para cualquier transformer, <code>parámetros de vocabulario = número de tokens distintos × tamaño oculto</code>. Tome <code>jina-XLM-RoBERTa</code>: con 250K tokens y 1,024 dimensiones, necesita 256M parámetros solo para codificación de vocabulario - ¡antes de manejar cualquier tarea de lenguaje real!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">En los transformers, la primera capa mapea tokens a estados ocultos usando una matriz de pesos, específicamente los pesos del vocabulario. Considerando usar todos los puntos de código UTF-8 (1.112.064) con 1.024 dimensiones ocultas - necesitarías masivos </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> parámetros solo para la conversión de tokens. Mientras que los LLM más grandes (100B+ parámetros) pueden manejar esta sobrecarga, es una restricción seria para modelos más pequeños. Es exactamente por esto que usamos tokenizadores como BPE, que fusionan eficientemente puntos de código UTF-8 comunes en tokens únicos.</span></figcaption></figure><p>Pero aquí está el asunto: <strong>los pesos del vocabulario no contribuyen a los mecanismos de atención - son solo tablas de búsqueda.</strong> Para los SLM que trabajan con presupuestos fijos de parámetros, un vocabulario más grande significa menos parámetros disponibles para las capas de atención, que realizan el procesamiento real del lenguaje. Esto explica por qué ModernBERT-large en inglés supera al multilingüe <code>jina-XLM-RoBERTa</code> a pesar de ser más pequeño - <code>jina-XLM-RoBERTa</code> asigna más parámetros (¡47%!) para soportar múltiples idiomas. El vocabulario enfocado de ModernBERT no solo mejora el rendimiento sino que también acelera la inferencia, haciéndolo particularmente efectivo para aplicaciones con recursos limitados.</p><p>Así que ahora si miramos <em>solo</em> los parámetros del modelo central (excluyendo los pesos del vocabulario), ModernBERT en realidad tiene más poder computacional que sus pares: ModernBERT dedica 19% más parámetros al modelado de lenguaje <em>real</em> que <code>jina-XLM-RoBERTa</code> y 15% más que <code>RoBERTa-large</code>!</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Especificaciones del Modelo</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Soporte de Idiomas</td>\n<td>Solo Inglés</td>\n<td>89 Idiomas</td>\n<td>Solo Inglés</td>\n</tr>\n<tr>\n<td>Tamaño del Vocabulario</td>\n<td>50.4K</td>\n<td>250K</td>\n<td>50.3K</td>\n</tr>\n<tr>\n<td>Parámetros Totales</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Parámetros de Vocabulario</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>Ratio de Parámetros de Vocabulario</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>Parámetros del Modelo Central</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">Escalado del Modelo mediante \"Weight Tiling\"</h3><p>Al construir el backbone de <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a>, descubrimos que entrenar SLM desde cero era intensivo en recursos y complejo. ModernBERT aborda esto con un enfoque inteligente de inicialización llamado <strong>weight tiling</strong> - esencialmente inicializando ModernBERT-large desde los pesos de su versión base más pequeña.</p><p>Esta técnica no es completamente nueva - se basa en el trabajo de DeepMind con <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> y también aparece en los modelos <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2</a> de Microsoft. Pero su aplicación aquí es particularmente efectiva para abordar el cuello de botella del entrenamiento de SLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT escala de 22 a 28 capas usando la estrategia de inicialización de profundidad del equipo de Gopher. Para esas capas extra (23-28), inicializan cada una usando pesos de las 22 capas originales de ModernBERT-base. Para las matrices de pesos de cada capa, usan el enfoque de tiling central de Phi-2. Funciona así: toman los pesos de ModernBERT-base y los colocan justo en el medio de las matrices de ModernBERT-large. ¿Para los bordes que siguen vacíos? Envuelven los pesos originales cíclicamente para llenarlos.</span></figcaption></figure><p>Esta estrategia de inicialización le da a ModernBERT-large una ventaja significativa - en lugar de comenzar desde cero, aprovecha los patrones pre-aprendidos de su contraparte más pequeña. Ha demostrado ser particularmente <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">efectiva para escalar modelos de lenguaje en este rango de tamaño</a>.</p><blockquote>Encontramos que un modelo con inicio en caliente se recupera rápidamente de una pérdida inicial alta (debido a los parámetros añadidos) a una pérdida bastante cercana a la del modelo base. Podemos expandir 417M parámetros por más de 3× en tamaño y mantener un rendimiento superior al de un modelo equivalente entrenado desde cero hasta la convergencia, lo que implica que las ganancias no se limitaron al inicio del entrenamiento. Sin embargo, en tamaños más grandes, las ganancias relativas logradas en la convergencia disminuyen, especialmente con expansiones en anchura.</blockquote><p>El envolvimiento cíclico de pesos no es solo por conveniencia - se alinea bien con cómo las matrices de atención naturalmente exhiben patrones periódicos. La investigación de Gopher muestra que este enfoque realmente brilla para SLM (menos de 9B parámetros), aunque los beneficios comienzan a disminuir a medida que te mueves hacia territorios de modelos más grandes.</p><h2 id=\"modernberts-code-modeling\">Modelado de Código de ModernBERT</h2><p>ModernBERT trae un enfoque especializado para la comprensión de código con su tokenizador optimizado para código y datos de entrenamiento. Esta optimización para el procesamiento de código rinde frutos tanto en tareas de comprensión como de recuperación.</p><p>Ejecutamos un benchmark usando el corpus <code>jina-embeddings-v2-code</code>, comparando tres modelos como backbones: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, y <code>RoBERTa-large</code>. ¿La prueba? <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - emparejar descripciones de texto con fragmentos de código. ModernBERT superó a ambas alternativas en toda la línea.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">La brecha tiene sentido - ni </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> ni </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> vieron lenguajes de programación durante el entrenamiento. Mientras tanto, ModernBERT-large se entrenó con dos billones de tokens, incluyendo una cantidad sustancial de código. Esta exposición a la sintaxis y patrones de programación le da una clara ventaja en tareas relacionadas con código. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> supera ligeramente a </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">, probablemente debido a sus datos de entrenamiento multilingües más grandes - misma arquitectura, más exposición. Sin embargo, ambos quedan significativamente por detrás de ModernBERT-large.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Tarea</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">La ventaja del tokenizador</h3><p>Veamos por qué ModernBERT maneja tan bien el código - utiliza el <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">tokenizador OLMo</a>, que fue entrenado específicamente en código, en lugar de los tokenizadores estándar de BERT/RoBERTa.</p><p>Un tokenizador divide el texto UTF-8 en tokens que se mapean a vectores - estos son los que el modelo realmente procesa. Durante el entrenamiento, aprende a combinar secuencias de caracteres que ocurren frecuentemente en tokens únicos. ¿La diferencia? Un tokenizador estándar podría dividir <code>init</code> en <code>in</code> + <code>it</code>, perdiendo el contexto de programación. Pero el tokenizador de ModernBERT, consciente del código, lo obtiene sin dividirlo.</p><p>Aquí es donde se pone interesante con el manejo de espacios: ModernBERT preserva los espacios iniciales de Python como tokens únicos y diferencia entre 4 y 8 espacios - crucial para la estructura del código. Mientras tanto, <strong><code>jina-XLM-RoBERTa</code> colapsa todos los espacios continuos en un solo <code>_</code>, y RoBERTa-large trata cada espacio como su propio token.</strong> Esto significa que el codificador de ModernBERT recibe una entrada más limpia y significativa al procesar código, mientras que los otros trabajan con tokens fracturados y menos coherentes.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT preserva los espacios iniciales de Python como tokens únicos y diferencia entre 4 y 8 espacios - crucial para la estructura del código; mientras que los otros trabajan con tokens fracturados y menos coherentes.</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">Manejo de contexto largo de ModernBERT</h2><p>ModernBERT ha logrado avances significativos en el procesamiento de texto largo, gracias a su extenso corpus de entrenamiento (300B tokens con muestras de 8,192 tokens) y técnicas avanzadas como la atención combinada global y local.</p><p>Para evaluar las capacidades de manejo de documentos largos, usamos el <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">conjunto de datos MLDR</a> - un benchmark exhaustivo de texto largo que abarca 13 idiomas. Dado que ModernBERT actualmente solo admite inglés, nos centramos en el subconjunto en inglés de MLDR para comparar ModernBERT con <code>jina-XLM-RoBERTa</code>. Si bien ambos modelos pueden manejar entradas de 8K tokens, <code>RoBERTa-large</code> fue excluido de este benchmark debido a su límite de 512 tokens, que es insuficiente para el análisis de texto largo.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>El rendimiento superior de ModernBERT no se debe solo a su extenso entrenamiento en texto largo - se debe en gran parte a su innovadora combinación de mecanismos de atención global y local. A diferencia de <code>jina-XLM-RoBERTa</code>, que aplica una atención global computacionalmente costosa a cada capa, ModernBERT adopta un enfoque más eficiente. Alterna entre atención global (usada cada tercera capa con un <code>theta</code> de 160,000) y atención local (usando una ventana deslizante de 128 tokens con un <code>theta</code> de 100,000). Esta estrategia híbrida mantiene un alto rendimiento mientras reduce dramáticamente el tiempo de entrenamiento.</p><blockquote>En ModernBERT, cada tercera capa emplea atención global con un theta RoPE de 160,000 y las capas restantes usan una ventana deslizante de atención local de 128 tokens con un theta RoPE de 10,000. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">¿La lección amarga?</h2><p>La ley de escalado y <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">la lección amarga</a> sugieren que las mejoras significativas de rendimiento provienen principalmente del aumento en el número de parámetros y datos de entrenamiento. Este principio guió nuestro enfoque de expandir el corpus y usar LoRA para adaptaciones específicas de tareas.</p><p>Sin embargo, el éxito de ModernBERT ha revelado que subestimamos el poder de la optimización arquitectónica. Demuestra que los SLMs pueden lograr resultados excepcionales a través de una mejor eficiencia datos-modelo, sin necesariamente aumentar los parámetros. Un reciente <a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">informe técnico de Stella Embeddings</a> refuerza este hallazgo, indicando que los métodos actuales de entrenamiento de modelos de embedding pueden mejorarse sin aumentar el corpus o el tamaño del modelo.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Gráfico que muestra la Ley de Escalado de Modelos de Embedding con 'Tamaño de Parámetros' en el eje x y 'Rendimiento MTEB' en el eje y\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Ley de escalado de modelos de embedding. El rendimiento promedio MTEB en tareas en inglés se grafica contra el número de parámetros del modelo. Cada punto representa un modelo de embedding. La línea de tendencia, que representa todos los modelos, está resaltada, con los modelos multilingües enfatizados en cian. Se puede ver que </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> demuestra un rendimiento superior comparado con modelos de tamaño similar, mostrando también una mejora superlineal sobre su predecesor, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">. Este gráfico fue creado seleccionando los 100 mejores modelos de embedding del ranking MTEB, excluyendo aquellos sin información de tamaño, típicamente modelos cerrados o propietarios. También se filtraron las presentaciones identificadas como trolleo obvio.</span></figcaption></figure><p>Avanzando, anticipamos costos computacionales más bajos y tamaños de modelo más pequeños a medida que obtenemos conocimientos más profundos sobre la utilización de datos e implementamos las técnicas de ModernBERT. A corto plazo, podemos implementar mejoras directas descritas en el paper de ModernBERT - específicamente integrando más datos relacionados con código y adoptando un tokenizador amigable con el código. Cambios más complejos, como cambiar a una arquitectura profunda y delgada o arrancar modelos grandes desde otros más pequeños, requerirán construir modelos base desde cero - una iniciativa a mediano plazo.</p><p>Si bien la eficiencia de ModernBERT es notable, su limitación a solo texto apunta a desafíos futuros. A medida que los modelos de embedding multimodales ganan popularidad, nuestro próximo desafío es desarrollar modelos base de búsqueda más inteligentes, rápidos y capaces que puedan manejar entradas para aplicaciones multimodales. Estas aplicaciones demandan ventanas de contexto aún más largas - un desafío de eficiencia que queda por resolver.</p><h2 id=\"conclusion\">Conclusión</h2><p>A lo largo de esta publicación, hemos explorado cómo ModernBERT hace avanzar los modelos de la familia BERT a través de tres innovaciones clave: su arquitectura profunda y delgada, tokenizador optimizado, y escalado eficiente usando mosaico de pesos. Estas mejoras permiten a ModernBERT ofrecer un rendimiento sobresaliente en un tamaño relativamente compacto, superando tanto a <code>RoBERTa-large</code> como a <code>jina-XLM-RoBERTa</code> en varias tareas. ModernBERT demuestra que las mejoras arquitectónicas pueden importar más que el tamaño de los parámetros, abriendo puertas para modelos más eficientes. Su uso exitoso del mosaico de pesos muestra cómo el escalado progresivo puede reducir los costos de entrenamiento mientras preserva o incluso mejora el rendimiento. Además, su vocabulario compacto y optimizaciones dirigidas sugieren oportunidades crecientes para SLMs especializados en entornos con recursos limitados.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
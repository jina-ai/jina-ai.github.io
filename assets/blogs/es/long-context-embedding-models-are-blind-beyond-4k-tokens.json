{
  "slug": "long-context-embedding-models-are-blind-beyond-4k-tokens",
  "id": "67c868baf1c5780001164330",
  "uuid": "a9f711ab-651e-4587-8a49-793d15b21380",
  "title": "Los modelos de embedding de contexto largo son ciegos m√°s all√° de los 4K tokens",
  "html": "En febrero de 2025, un equipo de investigadores de IA public√≥ el <a href=\"https://arxiv.org/abs/2502.05167\">paper NoLiMA</a>, que introduce un nuevo benchmark para evaluar la capacidad de los modelos de lenguaje grandes para manejar contextos largos.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2502.05167\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">NoLiMa: Long-Context Evaluation Beyond Literal Matching</div><div class=\"kg-bookmark-description\">Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-8.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Ali Modarressi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Este paper introduce un cambio significativo al benchmark tradicional Needle-in-a-Haystack (NIAH) al eliminar las coincidencias literales entre las preguntas y la aguja (informaci√≥n relevante) oculta en el pajar (texto irrelevante).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/niah-vs-nolima.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"240\" height=\"150\"><figcaption><span style=\"white-space: pre-wrap;\">Por ejemplo, en el NIAH tradicional, si la pregunta es \"¬øEn qu√© a√±o visit√≥ John Par√≠s?\", la aguja podr√≠a contener directamente \"John visit√≥ Par√≠s en 2019\". En NOLIMA, la pregunta podr√≠a ser \"¬øQu√© personaje ha estado en Francia?\" mientras que la aguja contiene \"De hecho, Yuki vive junto a la Casa de la √ìpera Semper\" - requiriendo que el modelo sepa que la Casa de la √ìpera Semper est√° en Dresde, Alemania, no en Francia.</span></figcaption></figure><p>Esto destaca una limitaci√≥n cr√≠tica en los LLM actuales: dependen en gran medida de la coincidencia de patrones superficiales, y su capacidad para realizar razonamiento asociativo profundo se deteriora r√°pidamente con el aumento de la longitud del contexto.</p><p>Bas√°ndonos en estos hallazgos, buscamos investigar si patrones de rendimiento similares ocurren en modelos de embedding, espec√≠ficamente enfoc√°ndonos en <code>jina-embeddings-v3</code>. Dado que la efectividad de los sistemas RAG depende cr√≠ticamente de la calidad de los modelos de recuperaci√≥n, buscamos extender la investigaci√≥n de NoLiMA a trav√©s de experimentos controlados que aborden dos preguntas fundamentales:</p><ul><li>¬øC√≥mo manejan los modelos de embedding la recuperaci√≥n de aguja en un pajar a trav√©s de diferentes longitudes de contexto cuando se ven forzados a hacer saltos sem√°nticos m√°s all√° de las coincidencias literales de palabras clave?</li><li>¬øPuede la ampliaci√≥n estrat√©gica de consultas con contenido sem√°nticamente similar mitigar esta brecha de rendimiento?</li></ul><p>El marcado contraste observado en los LLM ‚Äîrobustos con coincidencias l√©xicas pero vulnerables con variaciones sem√°nticas‚Äî sugiere que los sistemas de recuperaci√≥n basados en embeddings podr√≠an enfrentar desaf√≠os similares al ir m√°s all√° de la coincidencia de t√©rminos superficiales, potencialmente revelando limitaciones fundamentales en las tecnolog√≠as actuales de b√∫squeda sem√°ntica.</p><h2 id=\"needles-and-haystacks-construction\">Construcci√≥n de Agujas y Pajares</h2><h3 id=\"needles-construction\">Construcci√≥n de Agujas</h3><p>Las pruebas tradicionales de aguja en un pajar usan agujas que reflejan la redacci√≥n de la pregunta que se est√° buscando. Por ejemplo:</p><ul><li>Pregunta: \"¬øQu√© personaje ha estado en Dresde?\"</li><li>Aguja: \"Yuki vive en Dresde.\"</li></ul><p>Pero como NoLiMA, queremos probar la comprensi√≥n sem√°ntica en lugar de la simple coincidencia de palabras clave, as√≠ que creamos variaciones de un salto (usando palabras espec√≠ficamente no presentes en los documentos) con dos ordenamientos de palabras diferentes:</p><ul><li>Pregunta: \"¬øQu√© personaje ha estado en Dresde?\"</li><li>Aguja (predeterminada): \"De hecho, Yuki vive junto a la Casa de la √ìpera Semper.\"</li><li>Aguja (invertida): \"La Casa de la √ìpera Semper est√° junto a donde vive Yuki.\"</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">La <a href=\"https://en.wikipedia.org/wiki/Semperoper\">Casa de la √ìpera Semper</a> est√° en Dresde, proporcionando el contexto para esta aguja de un salto.</div></div><p>Siguiendo la metodolog√≠a del paper, generamos estos grupos de aguja-pregunta (consistentes en una pregunta, <strong>una aguja de un salto</strong>, y <strong>una aguja de un salto invertida</strong>) a trav√©s de varias categor√≠as, como los ejemplos a continuaci√≥n:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Categor√≠a</th>\n<th>Pregunta</th>\n<th>Aguja original (como referencia)</th>\n<th>Aguja de un salto</th>\n<th>Aguja de un salto invertida</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Restricciones diet√©ticas</td>\n<td>¬øQu√© personaje no puede comer comidas a base de pescado?</td>\n<td>Alice no puede comer comidas a base de pescado.</td>\n<td>Entonces, Alice mencion√≥ ser vegana durante a√±os.</td>\n<td>Ser vegana era importante para Alice durante a√±os.</td>\n</tr>\n<tr>\n<td>Condiciones m√©dicas</td>\n<td>¬øQu√© personaje no puede beber leche?</td>\n<td>Bob no puede beber leche.</td>\n<td>Bob explic√≥ que era intolerante a la lactosa.</td>\n<td>Ser intolerante a la lactosa afectaba a Bob diariamente.</td>\n</tr>\n<tr>\n<td>Dominio de idiomas</td>\n<td>¬øQu√© personaje habla franc√©s?</td>\n<td>Charlie habla franc√©s.</td>\n<td>De hecho, Charlie estudi√≥ en la Sorbona.</td>\n<td>En la Sorbona, Charlie complet√≥ su t√≠tulo.</td>\n</tr>\n<tr>\n<td>Antecedentes profesionales</td>\n<td>¬øQu√© personaje es m√∫sico?</td>\n<td>Diane es m√∫sica.</td>\n<td>En 2013, Diane dirigi√≥ en la √ìpera de S√≠dney.</td>\n<td>La presentaci√≥n en la √ìpera de S√≠dney fue dirigida por Diane.</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Los nombres anteriores son solo para referencia. En las agujas reales se extraen aleatoriamente de una lista de nombres culturalmente diversos.<br><br>N√≥tese que las agujas originales (coincidencias literales de palabras clave) se proporcionan como referencia y no se utilizan en nuestros experimentos.</div></div><h3 id=\"haystacks-construction\">Construcci√≥n de Pajares</h3><p>Comenzamos con diez libros de dominio p√∫blico, cada uno conteniendo al menos 50,000 tokens, concatenando aleatoriamente fragmentos cortos (menos de 250 tokens) de ellos en pajares de diferentes longitudes, espec√≠ficamente 128, 256, 512, 1024, 2048, 4096 y 8192 tokens. Luego incrustamos una aguja en cada pajar:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"896\" height=\"415\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-21.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-21.png 896w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Construcci√≥n del pajar a partir de fragmentos cortos de libros y una sola aguja por pajar.</span></figcaption></figure><p>Para un ejemplo m√°s concreto, tomaremos la aguja \"De hecho, Yuki vive junto a la Casa de la √ìpera Semper\" y la colocaremos en un pajar de 128 tokens en la posici√≥n 50:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1570\" height=\"508\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/text2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/text2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/text2.png 1570w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Un ejemplo de aguja en un pajar.</span></figcaption></figure><p>Usando <code>jina-embeddings-v3</code> para incrustar los textos, la puntuaci√≥n de similitud entre el texto de la aguja y el texto del pajar es:</p><pre><code class=\"language-bash\">Question-Haystack similarity = 0.2391\n</code></pre><p>Luego normalizamos la puntuaci√≥n dividiendo este n√∫mero por la puntuaci√≥n de similitud de la pregunta y la aguja predeterminada (sin creaci√≥n de pajar, solo comparaci√≥n directa):</p><pre><code class=\"language-bash\">Question-Needle similarity = 0.3598\nNormalized Query-Haystack similarity = 0.2391 / 0.3598 = 0.6644\n</code></pre><p>Esta normalizaci√≥n es necesaria porque no todos los modelos producen las mismas puntuaciones de similitud entre dos textos, y <code>jina-embeddings-v3</code> tiende a subcalcular la similitud entre dos textos.</p><p>Para cada aguja (incluyendo todas las predeterminadas e invertidas) generamos diez pajares por longitud de contexto, incrustando una aguja por pajar en una ubicaci√≥n diferente. Para una aguja y longitud de contexto dadas, los pajares se ver√≠an algo as√≠:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"290\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-7.png 800w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Agujas colocadas a intervalos regulares a lo largo de diez pajares.</span></figcaption></figure><p>Como control, tambi√©n generamos un pajar para cada condici√≥n de prueba sin ninguna aguja. En total, son 3,234 pajares. Codificamos cada pajar con <code>jina-embeddings-v3</code> (usando el LoRA predeterminado de coincidencia de texto), luego para cada pajar lo truncamos (si el total de tokens exced√≠a 8,192, el l√≠mite para<code>jina-embeddings-v3</code>) luego codific√≥ su pregunta correspondiente.</p><h2 id=\"evaluation-metrics\">M√©tricas de Evaluaci√≥n</h2><p>Nuestro marco de evaluaci√≥n utiliza varias m√©tricas para evaluar el rendimiento del modelo de embeddings a trav√©s de diferentes longitudes de contexto:</p><h3 id=\"primary-metrics\">M√©tricas Primarias</h3><p><strong>Puntuaci√≥n de Similitud Normalizada</strong><br>La m√©trica principal es una puntuaci√≥n de similitud normalizada que tiene en cuenta tanto la similitud sem√°ntica entre la pregunta y el contexto completo (similitud pregunta-pajar), como la similitud base entre la pregunta y su aguja predeterminada correspondiente (similitud pregunta-aguja). Esta normalizaci√≥n asegura que el rendimiento del modelo se eval√∫e en relaci√≥n con un punto de referencia significativo en lugar de solo puntuaciones de similitud absolutas. El proceso de normalizaci√≥n implica calcular la puntuaci√≥n de similitud de coseno directa entre preguntas y sus agujas correspondientes (nuestra l√≠nea base), y dividir la similitud pregunta-pajar por esta puntuaci√≥n base:<br></p><p>$\\text{Similitud Normalizada} = \\frac{\\cos{(q,h)}}{\\cos{(q,n)}}$</p><p><strong>Ratio Comparativo con el Azar</strong><br>Para cualquier modelo de embeddings, las puntuaciones de similitud de coseno entre diferentes pares de consulta-documento solo son directamente comparables cuando la consulta permanece igual. Por lo tanto, m√°s all√° de usar puntuaciones de similitud normalizadas, tambi√©n medimos con qu√© frecuencia la pregunta es m√°s similar al pajar completo que a un pasaje aleatorio de la misma longitud sin aguja.</p><h3 id=\"secondary-metrics\">M√©tricas Secundarias</h3><p><strong>An√°lisis de Separaci√≥n</strong><br>Esta m√©trica eval√∫a qu√© tan bien el modelo distingue entre contenido relevante e irrelevante. Incluye la <strong>separaci√≥n media</strong>, que representa la diferencia entre ejemplos positivos (pasajes que contienen la respuesta) y ejemplos negativos (pasajes que no contienen la respuesta), y la <strong>puntuaci√≥n AUC (√Årea Bajo la Curva)</strong>, que mide la capacidad de discriminaci√≥n basada en el √°rea bajo la curva ROC (Caracter√≠stica Operativa del Receptor).</p><p><strong>Efectos de Posici√≥n</strong><br>Analizamos c√≥mo la ubicaci√≥n de la aguja afecta el rendimiento a trav√©s del <strong>coeficiente de correlaci√≥n</strong> entre posici√≥n y puntuaci√≥n de similitud, <strong>pendiente de regresi√≥n</strong> que muestra el cambio de rendimiento a trav√©s de las posiciones, y <strong>an√°lisis de rendimiento agrupado por posici√≥n</strong>.</p><h2 id=\"findings\">Hallazgos</h2><h3 id=\"degradation-of-similarity-score-and-correctness\">Degradaci√≥n de la Puntuaci√≥n de Similitud y Correcci√≥n</h3><p>Nuestros resultados muestran claramente que el rendimiento se degrada a medida que aumenta la longitud del contexto, con la puntuaci√≥n de similitud media cayendo de 0.37 en 128 tokens a 0.10 en 8K tokens, siguiendo una tendencia no lineal con una ca√≠da pronunciada entre 128 y 1K tokens.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-9.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Rendimiento normalizado vs longitud del contexto.</span></figcaption></figure><p>En la siguiente figura, demostramos que invertir la aguja tiene poca diferencia en la puntuaci√≥n de similitud normalizada. Tanto la aguja predeterminada (por ejemplo, \"En realidad, Yuki vive cerca de la √ìpera Semper\") como la aguja invertida (por ejemplo, \"La √ìpera Semper est√° junto a donde vive Yuki\") muestran un rendimiento casi id√©ntico:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-10.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Rendimiento de orden predeterminado vs invertido.</span></figcaption></figure><p>Las diferentes conexiones sem√°nticas del conjunto de datos muestran rendimientos variables, con los pares ubicaci√≥n-punto de referencia manteniendo los resultados m√°s fuertes, mientras que las conexiones diet√©ticas y de condiciones m√©dicas se degradan m√°s r√°pidamente:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"993\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-11.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Rendimiento normalizado por grupo vs longitud del contexto.</span></figcaption></figure><p>La comparaci√≥n de los resultados con el azar respalda nuestros hallazgos, mostrando que cuanto m√°s grande es el pajar, m√°s se acercan los resultados al azar, es decir, es casi tan probable seleccionar un pasaje aleatorio sin aguja (respuesta correcta) como el pajar para una pregunta dada:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-12.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-12.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Rendimiento del modelo vs azar (0.5).</span></figcaption></figure><p>Nuevamente, vemos rendimientos variables basados en diferentes conexiones sem√°nticas, con algunas (como restricciones diet√©ticas) cayendo bien por debajo del azar incluso en contextos relativamente cortos, mientras que otras (como ubicaciones y puntos de referencia) muestran un rendimiento mucho mejor independientemente de la longitud del contexto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Rendimiento por grupo vs azar.</span></figcaption></figure><p>Invertir la aguja tiene poco efecto en el rendimiento. En el siguiente gr√°fico, mostramos la ratio comparativa de preferir el pajar correcto al azar, dividido por si la aguja colocada conten√≠a la respuesta en orden predeterminado u orden invertido: </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Orden predeterminado vs invertido - rendimiento vs azar.</span></figcaption></figure><p>Como podemos ver que los resultados para agujas en orden predeterminado e invertido siguen la misma tendencia, no continuaremos el an√°lisis dividido respecto a este criterio.</p><h3 id=\"can-we-separate-positive-from-negative-results\">¬øPodemos Separar Resultados Positivos de Negativos?</h3><p>Uno de nuestros hallazgos m√°s importantes proviene del an√°lisis de qu√© tan bien los modelos de embeddings pueden distinguir contenido relevante de irrelevante a trav√©s de diferentes longitudes de contexto. Este \"an√°lisis de separaci√≥n\" revela que la correcci√≥n de la recuperaci√≥n cae r√°pidamente entre longitudes de contexto de 128 y 1000 tokens, y luego contin√∫a cayendo, aunque a una tasa m√°s lenta:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1091\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 10: An√°lisis de separaci√≥n vs longitud del contexto.</span></figcaption></figure><p>Para contextos cortos (128 tokens), el modelo muestra una fuerte separaci√≥n con una diferencia media de 0.1 y una discriminaci√≥n clara, alcanzando un AUC de 0.81 (lo que significa que el 81% del tiempo, el modelo clasifica un pasaje relevante m√°s alto que uno irrelevante). Esto indica que en contextos m√°s cortos, el modelo puede distinguir de manera confiable los pasajes que contienen la respuesta de aquellos que no la contienen.</p><p>Sin embargo, esto se deteriora r√°pidamente a medida que aumenta la longitud del contexto. En 1.000 tokens, la separaci√≥n cae un 60% a 0,040, y el AUC disminuye a 0,66, se√±alando una notable ca√≠da en el rendimiento. A los 8.000 tokens, hay una separaci√≥n m√≠nima (0,001) y una discriminaci√≥n casi aleatoria, con un AUC de solo 0,50. Este patr√≥n revela una perspectiva crucial: incluso cuando los modelos pueden calcular puntuaciones de similitud razonables en contextos m√°s largos, apenas pueden usar estas puntuaciones para distinguir informaci√≥n relevante de irrelevante. A los 8.000 tokens, la capacidad del modelo para diferenciar contenido relevante es esencialmente aleatoria.</p><p>La velocidad de esta degradaci√≥n a medida que crece el contexto es sorprendente. Las puntuaciones de similitud brutas caen aproximadamente un 75% de 128 a 8.000 tokens, pero las m√©tricas de separaci√≥n disminuyen casi un 99% en el mismo intervalo. M√°s preocupante a√∫n, el tama√±o del efecto muestra una disminuci√≥n a√∫n m√°s pronunciada, cayendo un 98,6%. Esto sugiere que las dificultades de los modelos de embedding con contextos largos van m√°s all√° de simplemente reducir las puntuaciones de similitud‚Äîsu capacidad fundamental para identificar informaci√≥n relevante se deteriora mucho m√°s severamente de lo que se entend√≠a anteriormente.</p><h3 id=\"how-does-the-needle-position-affect-the-core-metrics\">¬øC√≥mo Afecta la Posici√≥n de la Aguja a las M√©tricas Principales?</h3><p>Si bien las m√©tricas de rendimiento principales suelen ser mejores cuando la aguja est√° al principio del pajar, la degradaci√≥n del rendimiento no siempre se correlaciona con la ubicaci√≥n en el medio del contexto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1052\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 11: Rendimiento por posici√≥n relativa a trav√©s de longitudes de contexto.</span></figcaption></figure><p>Tambi√©n vemos que el rendimiento es mejor cuando la aguja est√° al inicio de un contexto dado, y en contextos cortos vemos un peque√±o aumento en el rendimiento cuando la aguja se coloca hacia el final. Sin embargo, en todos los contextos vemos una ca√≠da en el rendimiento cuando la aguja est√° en posiciones intermedias:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-17.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 12: Ratios comparativos por posici√≥n.</span></figcaption></figure><h2 id=\"what-effect-does-query-expansion-have-on-the-results\">¬øQu√© Efecto Tiene la Expansi√≥n de Consultas en los Resultados?</h2><p>Recientemente publicamos un post sobre expansi√≥n de consultas, una t√©cnica utilizada en sistemas de b√∫squeda para mejorar el rendimiento de b√∫squeda agregando t√©rminos relevantes a las consultas.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Query Expansion with LLMs: Searching Better by Saying More</div><div class=\"kg-bookmark-description\">Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-21.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/query-expansion-with-llms-searching-better-by-saying-more.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>En el post, usamos un LLM para generar t√©rminos de expansi√≥n, que luego se agregaron a los embeddings de consulta para mejorar el rendimiento de recuperaci√≥n. Los resultados mostraron mejoras significativas. Ahora, queremos examinar c√≥mo (o si) la t√©cnica mejorar√° los resultados para la b√∫squeda de aguja en un pajar. Por ejemplo, dada una consulta:</p><pre><code class=\"language-bash\">Which character has been to Dresden?\n</code></pre><p>Usamos un LLM (Gemini 2.0) para expandirla y agregar 100 t√©rminos adicionales que se ven as√≠:</p><pre><code class=\"language-bash\">Which character has been to Dresden? Character: fictional character literary character protagonist antagonist figure persona role dramatis personae\\\\n\\\\nDresden: Dresden Germany; bombing of Dresden World War II historical fiction Kurt Vonnegut Slaughterhouse-Five city in Saxony Elbe River cultural landmark\\\\n\\\\nHas been to: visited traveled to journeyed to presence in appears in features in set in takes place in location setting\n\n</code></pre><h3 id=\"how-much-does-query-expansion-help-match-the-needle-to-the-haystack\">¬øCu√°nto Ayuda la Expansi√≥n de Consultas a Emparejar la Aguja con el Pajar?</h3><p>Para nuestro experimento, generamos tres conjuntos de t√©rminos de consulta expandidos (como se describe en el <a href=\"https://jina.ai/news/query-expansion-with-llms-searching-better-by-saying-more/\">post original</a>) - 100, 150 y 250 t√©rminos. Luego ejecutamos el mismo conjunto de experimentos que antes, repetidos tres veces, una vez con cada conjunto de t√©rminos de consulta expandidos.</p><p>Los resultados con todos los conjuntos de expansi√≥n mostraron una clara degradaci√≥n a medida que aumentaba la longitud del contexto, con un efecto similar a no usar expansi√≥n de consulta en absoluto (Figuras 4 y 7):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1071\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 13: Rendimiento normalizado combinado: todos los tama√±os de expansi√≥n.</span></figcaption></figure><p>En comparaci√≥n con las consultas no expandidas, todas las condiciones de expansi√≥n de consulta mostraron el mismo patr√≥n de rendimiento degradado a medida que crec√≠a el contexto. La tendencia de degradaci√≥n tambi√©n sigue siendo no lineal con una fuerte disminuci√≥n entre 128 y 1K tokens:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1081\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 14: Ratio comparativo combinado: todos los tama√±os de expansi√≥n.</span></figcaption></figure><p>Sin embargo, al examinar el ratio comparativo se muestra que la expansi√≥n de consultas tiene claros beneficios: El modelo es mucho m√°s propenso a seleccionar el pajar con la aguja sobre el que no la tiene. En contraste, sin expansi√≥n de consulta la probabilidad de seleccionar el pasaje correcto cay√≥ tanto que, con un tama√±o de pajar de 8K tokens, era casi lo mismo que elegir un pasaje al azar.</p><h3 id=\"how-do-we-explain-needle-matching-results-with-query-expansion\">¬øC√≥mo Explicamos los Resultados de Emparejamiento de Agujas con Expansi√≥n de Consultas?</h3><p>Estos resultados se alinean con los hallazgos tanto del paper NoLiMa como de la investigaci√≥n de expansi√≥n de consultas, y pueden explicarse de la siguiente manera:</p><ol><li><strong>Compensaci√≥n entre calidad y cantidad</strong>: El mejor rendimiento de la expansi√≥n de 100 t√©rminos, en comparaci√≥n con 150 y 250 t√©rminos, sugiere que hay un punto √≥ptimo donde t√©rminos adicionales comienzan a agregar m√°s ruido que se√±al. La expansi√≥n de 250 t√©rminos probablemente introduce t√©rminos con relaciones sem√°nticas m√°s d√©biles con la consulta original, que se vuelven contraproducentes en contextos m√°s largos.</li><li><strong>La longitud del contexto sigue siendo el desaf√≠o principal</strong>: A pesar de los beneficios de la expansi√≥n de consultas, el rendimiento a√∫n se degrada significativamente con el aumento de la longitud del contexto. Esto sugiere que incluso con expansi√≥n, persiste la limitaci√≥n arquitect√≥nica fundamental de los modelos basados en atenci√≥n en contextos largos.</li><li><strong>Identificaci√≥n del umbral pr√°ctico</strong>: El ratio comparativo que se mantiene por encima de 0,5 indica que la expansi√≥n mantiene un rendimiento superior al azar incluso a 8K tokens, proporcionando una forma pr√°ctica de extender la <em>ventana de contexto efectiva</em> para modelos de embedding. La comparaci√≥n con el azar muestra que, incluso cuando se presentan documentos con contexto largo, expandir la consulta hace m√°s probable encontrar la respuesta correcta (es decir, la aguja) que una incorrecta. Esto es una mejora en comparaci√≥n con las consultas no expandidas, donde la probabilidad de encontrar la respuesta correcta se aproxima al azar a medida que aumenta la longitud del contexto.</li></ol><h2 id=\"diagnosis-what-role-does-lexical-matching-play-in-embeddings\">Diagn√≥stico: ¬øQu√© Papel Juega la Coincidencia L√©xica en los Embeddings?</h2><p>En los experimentos anteriores, medimos la efectividad de los modelos de embedding para hacer inferencias sem√°nticas de \"un salto\" en pasajes de contexto largo, eliminando toda posibilidad de coincidencia literal. Encontramos que, incluso con expansi√≥n de consultas, la capacidad del modelo de embedding para encontrar pasajes relevantes se deteriora a medida que crece la longitud del contexto. Este efecto es significativo, y el hallazgo es notable porque normalmente esperar√≠amos que un modelo de embedding pudiera hacer las inferencias relevantes sin asistencia adicional. Cuando reemplazamos coincidencias literales con variaciones de un salto (por ejemplo, \"Dresden\" ‚Üí \"Semper Opera House\"), todo lo que estamos haciendo es reemplazar un concepto por otro cercano.</p><p>Ahora, agarremos el toro por los cuernos y hagamos la pregunta directamente: ¬øLa coincidencia literal realmente juega un papel lo suficientemente significativo en la coincidencia sem√°ntica, o el efecto de la longitud del contexto la supera? Para responder esta pregunta, rehicicimos nuestras pruebas con agujas que contienen coincidencias literales, por ejemplo:</p><ul><li>Pregunta: \"Which character has been to Dresden?\"</li><li>Aguja (predeterminada): \"Actually, Yuki lives in Dresden.\"</li><li>Aguja (invertida): \"Dresden is where Yuki lives.\"</li></ul><p>Observe que, en lugar de una variaci√≥n de un paso para inferir que la √ìpera Semper est√° en Dresde y, por lo tanto, un personaje que vive junto a ella deber√≠a haber sido el que visit√≥ Dresde, estas pistas indican directamente el nombre del personaje que vive en Dresde.</p><p>Despu√©s de reformular los 22 pares de pregunta-pista de esta manera, volvimos a ejecutar nuestros experimentos con todas las longitudes de contexto incluidas y las ubicaciones de las pistas, utilizando el mismo modelo de embeddings <code>jina-embeddings-v3</code>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-22.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1078\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 15: Rendimiento normalizado vs longitud del contexto.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-23.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-23.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-23.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-23.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 16: Rendimiento del modelo vs azar (0.5).</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/image-24.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/03/image-24.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/03/image-24.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/03/image-24.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/03/image-24.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 17: Ratios comparativos por posici√≥n</span></figcaption></figure><p>Los resultados son sorprendentes. Incluso con coincidencias literales en el contexto, la capacidad del modelo para distinguir la respuesta correcta de una aleatoria se deteriora r√°pidamente a medida que crece la longitud del contexto, aunque mantiene una ligera ventaja sobre la ausencia completa de cualquier coincidencia literal.</p><p>Esto demuestra finalmente que la capacidad de un modelo de embeddings para encontrar una aguja en un pajar se ve afectada mucho m√°s por el tama√±o del pajar (y la ubicaci√≥n de la aguja en √©l) que por la formulaci√≥n sem√°ntica de la aguja.</p><h2 id=\"conclusion\">Conclusi√≥n</h2><p>Nuestros hallazgos con modelos de embeddings se alinean con el art√≠culo de NoLiMA sobre LLMs: El tama√±o del contexto es altamente determinante para la coincidencia y recuperaci√≥n correcta. Demostramos que esto es cierto incluso cuando hay una coincidencia exacta letra por letra.</p><p>El problema no es la capacidad de un embedding para realizar coincidencias sem√°nticas. Los modelos de embeddings como <code>jina-embeddings-v3</code> manejan bien los contextos cortos, pero su efectividad disminuye a medida que aumenta la longitud del contexto. La expansi√≥n de consultas puede reducir este efecto hasta cierto punto, pero la calidad de recuperaci√≥n a√∫n se degrada en contextos m√°s largos. Adem√°s, la expansi√≥n de consultas plantea problemas adicionales, ya que es crucialmente importante identificar t√©rminos de expansi√≥n que mejoren la recuperaci√≥n sin agregar ruido sem√°ntico. Estamos investigando y buscando formas de abordar directamente la recuperaci√≥n de aguja en un pajar y mejorar el rendimiento futuro de <code>jina-embeddings-v4</code>.</p>",
  "comment_id": "67c868baf1c5780001164330",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-05T16:07:38.000+01:00",
  "updated_at": "2025-03-07T03:56:34.000+01:00",
  "published_at": "2025-03-07T03:56:34.000+01:00",
  "custom_excerpt": "We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "641c23a2f4d50d003d590474",
      "name": "Saahil Ognawala",
      "slug": "saahil",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
      "cover_image": null,
      "bio": "Senior Product Manager at Jina AI",
      "website": "http://www.saahilognawala.com/",
      "location": "Munich, DE",
      "facebook": null,
      "twitter": "@saahil",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "641c23a2f4d50d003d590474",
    "name": "Saahil Ognawala",
    "slug": "saahil",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg",
    "cover_image": null,
    "bio": "Senior Product Manager at Jina AI",
    "website": "http://www.saahilognawala.com/",
    "location": "Munich, DE",
    "facebook": null,
    "twitter": "@saahil",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/saahil/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/long-context-embedding-models-are-blind-beyond-4k-tokens/",
  "excerpt": "Investigamos los modelos de embedding en nuevas tareas de \"aguja en un pajar\" y descubrimos que m√°s all√° de 4K tokens, simplemente est√°n tirando los dados - incluso con coincidencias l√©xicas exactas o expansi√≥n de consultas, no pueden distinguir la se√±al del ruido en contextos largos.",
  "reading_time": 14,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
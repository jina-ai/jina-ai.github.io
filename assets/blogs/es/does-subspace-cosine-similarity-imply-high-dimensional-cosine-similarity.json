{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "¬øLa similitud del coseno en subespacios implica similitud del coseno en altas dimensiones?",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">El 25 de enero de 2024, OpenAI lanz√≥ <a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">un nuevo modelo de embeddings</a> con una nueva caracter√≠stica llamada <i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">\"shortening\"</strong></b></i>, que permite a los desarrolladores recortar embeddings ‚Äîesencialmente cortando n√∫meros desde el final de la secuencia‚Äî sin comprometer la capacidad del embedding para representar conceptos de manera efectiva. Profundiza en este post para obtener una s√≥lida base te√≥rica sobre la viabilidad y el fundamento detr√°s de esta innovaci√≥n.</div></div><p>Considera esto: al medir la similitud del coseno de vectores de embeddings en espacios de alta dimensionalidad, ¬øc√≥mo implica su similitud en subespacios de menor dimensi√≥n la similitud general? ¬øExiste una relaci√≥n directa y proporcional, o la realidad es m√°s compleja con datos de alta dimensionalidad?</p><p>M√°s concretamente, <strong>¬øuna alta similitud entre vectores en sus primeras 256 dimensiones asegura una alta similitud en sus 768 dimensiones completas?</strong> Por el contrario, si los vectores difieren significativamente en algunas dimensiones, ¬øesto significa una baja similitud general? Estas no son meras reflexiones te√≥ricas; son consideraciones cruciales para la recuperaci√≥n eficiente de vectores, la indexaci√≥n de bases de datos y el rendimiento de los sistemas RAG.</p><p>Los desarrolladores a menudo conf√≠an en heur√≠sticas, asumiendo que una alta similitud en el subespacio equivale a una alta similitud general o que diferencias notables en una dimensi√≥n afectan significativamente la similitud general. La pregunta es: ¬øestos m√©todos heur√≠sticos est√°n construidos sobre una base te√≥rica s√≥lida, o son simplemente suposiciones por conveniencia?</p><p>Este post profundiza en estas preguntas, examinando la teor√≠a y las implicaciones pr√°cticas de la similitud en subespacios en relaci√≥n con la similitud general de vectores.</p><h2 id=\"bounding-the-cosine-similarity\">Acotando la Similitud del Coseno</h2><p>Dados los vectores $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$, los descomponemos como $\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ y $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$, donde $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ y $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$, con $m+n=d$.</p><p>La similitud del coseno en el subespacio $\\mathbb{R}^m$ est√° dada por $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$; similarmente, la similitud en el subespacio $\\mathbb{R}^n$ es $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$.</p><p>En el espacio original $\\mathbb{R}^d$, la similitud del coseno se define como:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>Ahora, sea $s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$. Entonces, tenemos:$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>Fin de la demostraci√≥n.</p><p>N√≥tese que en el paso final de la demostraci√≥n, aprovechamos que la similitud del coseno siempre es menor o igual a 1. Esto forma nuestro l√≠mite superior. De manera similar, podemos mostrar que el l√≠mite inferior de \\(\\cos(\\mathbf{A},\\mathbf{B})\\) est√° dado por:</p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\], donde $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$.</p><p>N√≥tese que para el l√≠mite inferior, no podemos concluir apresuradamente que \\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\). Esto se debe al rango de la funci√≥n coseno, que abarca entre \\([-1, 1]\\). Debido a este rango, es imposible establecer un l√≠mite inferior m√°s ajustado que el valor trivial de -1.</p><p>As√≠ que en conclusi√≥n, tenemos la siguiente cota amplia: $$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)).$$ y una cota m√°s ajustada \\[\\begin{align*}  \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\], donde $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $.</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">Conexi√≥n con el Lema de Johnson‚ÄìLindenstrauss</h3><p>El lema JL afirma que para cualquier \\(0 &lt; \\epsilon &lt; 1\\) y cualquier conjunto finito de puntos \\( S \\) en \\( \\mathbb{R}^d \\), existe un mapeo \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\) (con \\( k = O(\\epsilon^{-2} \\log |S|) \\)) tal que para todos \\( \\mathbf{u}, \\mathbf{v} \\in S \\), las distancias euclidianas se preservan aproximadamente:<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson‚ÄìLindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>Para hacer que $f$ funcione como una selecci√≥n de subespacio, podemos usar una matriz diagonal para la proyecci√≥n, como una matriz \\(5 \\times 3\\) \\(f\\), aunque no aleatoria (nota, la formulaci√≥n t√≠pica del lema JL involucra transformaciones lineales que a menudo utilizan matrices aleatorias extra√≠das de una distribuci√≥n gaussiana). Por ejemplo, si queremos retener la 1¬™, 3¬™ y 5¬™ dimensiones de un espacio vectorial de 5 dimensiones, la matriz \\(f\\) podr√≠a dise√±arse de la siguiente manera: \\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>Sin embargo, al especificar que $f$ sea diagonal, limitamos la clase de funciones que pueden usarse para la proyecci√≥n. El lema JL garantiza la existencia de una $f$ adecuada dentro de la clase m√°s amplia de transformaciones lineales, pero cuando restringimos $f$ a ser diagonal, puede que no exista tal $f$ adecuada dentro de esta clase restringida para aplicar las cotas del lema JL.</p><h2 id=\"validating-the-bounds\">Validando las Cotas</h2><p>Para explorar emp√≠ricamente las cotas te√≥ricas de la similitud del coseno en espacios vectoriales de alta dimensionalidad, podemos emplear una simulaci√≥n de Monte Carlo. Este m√©todo nos permite generar un gran n√∫mero de pares de vectores aleatorios, calcular sus similitudes tanto en el espacio original como en los subespacios, y luego evaluar qu√© tan bien se mantienen en la pr√°ctica los l√≠mites te√≥ricos superior e inferior.</p><p>El siguiente fragmento de c√≥digo Python implementa este concepto. Genera aleatoriamente pares de vectores en un espacio de alta dimensionalidad y calcula su similitud del coseno. Luego, divide cada vector en dos subespacios, calcula la similitud del coseno dentro de cada subespacio y eval√∫a los l√≠mites superior e inferior de la similitud del coseno en dimensi√≥n completa bas√°ndose en las similitudes de los subespacios.</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),\n          np.all(gamma * t &lt;= cos_sim)))\n</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Un validador Monte Carlo para validar los l√≠mites de similitud del coseno</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Una muestra de salida de nuestro validador Monte Carlo. Es importante tener en cuenta que la condici√≥n </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>lower/upper bound satisfied</span></code><span style=\"white-space: pre-wrap;\"> se verifica para cada vector individualmente. Mientras tanto, el </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>avg. lower/upper bound</span></code><span style=\"white-space: pre-wrap;\"> proporciona una visi√≥n general m√°s intuitiva de las estad√≠sticas relacionadas con estos l√≠mites, pero no influye directamente en el proceso de validaci√≥n.</span></p></figcaption></figure><h2 id=\"understanding-the-bounds\">Entendiendo los L√≠mites</h2><p>En resumen, al comparar dos vectores de alta dimensionalidad, la similitud general se encuentra entre las mejores y peores similitudes de sus subespacios, ajustadas seg√∫n qu√© tan grandes o importantes son esos subespacios en el esquema general. Esto es lo que los l√≠mites para la similitud del coseno en dimensiones superiores representan intuitivamente: el equilibrio entre las partes m√°s y menos similares, ponderadas por sus tama√±os o importancia relativa.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Illustrative comparison of two stylus pen caps and bodies with labeled sections on a black background\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Cada bol√≠grafo tiene dos componentes principales: el cuerpo y la tapa.</span></figcaption></figure><p>Imagina que est√°s tratando de comparar dos objetos de m√∫ltiples partes (digamos, dos bol√≠grafos elegantes) bas√°ndote en su similitud general. Cada bol√≠grafo tiene dos componentes principales: el cuerpo y la tapa. La similitud del bol√≠grafo completo (tanto cuerpo como tapa) es lo que estamos tratando de determinar:</p><h3 id=\"upper-bound-gamma-cdot-s\">L√≠mite Superior ($\\gamma \\cdot s$)</h3><p>Piensa en $s$ como la mejor coincidencia entre las partes correspondientes de los bol√≠grafos. Si las tapas son muy similares pero los cuerpos no, $s$ es la similitud de las tapas.</p><p>Ahora, $\\gamma$ es como un factor de escala basado en el tama√±o (o importancia) de cada parte. Si un bol√≠grafo tiene un cuerpo muy largo y una tapa corta, mientras que el otro tiene un cuerpo corto y una tapa larga, $\\gamma$ ajusta la similitud general para tener en cuenta estas diferencias en las proporciones.</p><p>El l√≠mite superior nos dice que sin importar qu√© tan similares sean algunas partes, la similitud general no puede exceder esta \"similitud de la mejor parte\" escalada por el factor de proporci√≥n.</p><h3 id=\"lower-bound-gamma-cdot-t\">L√≠mite Inferior ($\\gamma \\cdot t$)</h3><p>Aqu√≠, $t$ es la similitud de las partes que menos coinciden. Si los cuerpos de los bol√≠grafos son bastante diferentes pero las tapas son similares, $t$ refleja la similitud del cuerpo.</p><p>Nuevamente, $\\gamma$ escala esto basado en la proporci√≥n de cada parte.</p><p>El l√≠mite inferior significa que la similitud general no puede ser peor que esta \"similitud de la peor parte\" despu√©s de tener en cuenta la proporci√≥n de cada parte.</p><h2 id=\"implications-of-the-bounds\">Implicaciones de los L√≠mites</h2><p>Para los ingenieros de software que trabajan con embeddings, b√∫squeda vectorial, recuperaci√≥n o bases de datos, entender estos l√≠mites tiene implicaciones pr√°cticas, particularmente cuando se trata de datos de alta dimensionalidad. La b√∫squeda vectorial a menudo implica encontrar los vectores m√°s cercanos (m√°s similares) en una base de datos para un vector de consulta dado, t√≠picamente usando la similitud del coseno como medida de cercan√≠a. Los l√≠mites que discutimos pueden proporcionar informaci√≥n sobre la efectividad y las limitaciones de usar similitudes de subespacios para tales tareas.</p><h3 id=\"using-subspace-similarity-for-ranking\">Uso de la Similitud de Subespacios para la Clasificaci√≥n</h3><p><strong>Seguridad y Precisi√≥n</strong>: Usar la similitud de subespacios para clasificar y recuperar los k mejores resultados puede ser efectivo, pero con precauci√≥n. El l√≠mite superior indica que la similitud general no puede exceder la similitud m√°xima de los subespacios. Por lo tanto, si un par de vectores es altamente similar en un subespacio particular, es un fuerte candidato para ser similar en el espacio de alta dimensionalidad.</p><p><strong>Posibles Errores</strong>: Sin embargo, el l√≠mite inferior sugiere que dos vectores con baja similitud en un subespacio a√∫n podr√≠an ser bastante similares en general. Por lo tanto, confiar √∫nicamente en la similitud de subespacios podr√≠a perder algunos resultados relevantes.</p><h3 id=\"misconceptions-and-cautions\">Conceptos Err√≥neos y Precauciones</h3><p><strong>Sobreestimar la Importancia del Subespacio</strong>: Un concepto err√≥neo com√∫n es sobreestimar la importancia de un subespacio particular. Si bien la alta similitud en un subespacio es un buen indicador, no garantiza una alta similitud general debido a la influencia de otros subespacios.</p><p><strong>Ignorar Similitudes Negativas</strong>: En casos donde la similitud del coseno en un subespacio es negativa, indica una relaci√≥n opuesta en esa dimensi√≥n. Los ingenieros deben ser cautelosos sobre c√≥mo estas similitudes negativas impactan la similitud general.</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "¬øUna alta similitud en el subespacio garantiza una alta similitud general entre vectores? Esta publicaci√≥n examina la teor√≠a y las implicaciones pr√°cticas de la similitud en subespacios.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
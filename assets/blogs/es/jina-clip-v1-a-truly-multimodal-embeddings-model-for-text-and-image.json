{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1: Un modelo de embeddings verdaderamente multimodal para texto e imagen",
  "html": "<p>Jina CLIP v1 (<code>jina-clip-v1</code>) es un nuevo modelo de embedding multimodal que extiende las capacidades del <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">modelo CLIP original</a> de OpenAI. Con este nuevo modelo, los usuarios tienen un √∫nico modelo de embedding que ofrece un rendimiento de √∫ltima generaci√≥n tanto en la recuperaci√≥n de solo texto como en la recuperaci√≥n multimodal texto-imagen. Jina AI ha mejorado el rendimiento de OpenAI CLIP en un 165% en la recuperaci√≥n de solo texto y en un 12% en la recuperaci√≥n de imagen a imagen, con un rendimiento id√©ntico o levemente mejor en las tareas de texto a imagen e imagen a texto. Este rendimiento mejorado hace que Jina CLIP v1 sea indispensable para trabajar con entradas multimodales.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> mejora OpenAI CLIP en <a href=\"#compare_table\" rel=\"noreferrer\">todas las categor√≠as de recuperaci√≥n</a>.</div></div><p>En este art√≠culo, primero discutiremos las limitaciones del modelo CLIP original y c√≥mo las hemos abordado usando un m√©todo √∫nico de co-entrenamiento. Luego, demostraremos la efectividad de nuestro modelo en varios benchmarks de recuperaci√≥n. Finalmente, proporcionaremos instrucciones detalladas sobre c√≥mo los usuarios pueden comenzar con Jina CLIP v1 a trav√©s de nuestra API de Embeddings y Hugging Face.</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">La Arquitectura CLIP para IA Multimodal</h2><p>En enero de 2021, OpenAI lanz√≥ el modelo <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a> (Contrastive Language‚ÄìImage Pretraining). CLIP tiene una arquitectura sencilla pero ingeniosa: combina dos modelos de embedding, uno para textos y otro para im√°genes, en un √∫nico modelo con un √∫nico espacio de embedding de salida. Sus embeddings de texto e imagen son directamente comparables entre s√≠, haciendo que la distancia entre un embedding de texto y uno de imagen sea proporcional a qu√© tan bien ese texto describe la imagen, y viceversa.</p><p>Esto ha demostrado ser muy √∫til en la recuperaci√≥n de informaci√≥n multimodal y en la clasificaci√≥n de im√°genes zero-shot. Sin entrenamiento especial adicional, CLIP tuvo un buen desempe√±o al colocar im√°genes en categor√≠as con etiquetas en lenguaje natural.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"Diagram illustrating image to text translation using an astronaut on Mars with a red moon as an example.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>El modelo de embedding de texto en el CLIP original era una red neuronal personalizada con solo 63 millones de par√°metros. En el lado de la imagen, OpenAI lanz√≥ CLIP con una selecci√≥n de modelos <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> y <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT</a>. Cada modelo fue pre-entrenado para su modalidad individual y luego entrenado con im√°genes con subt√≠tulos para producir embeddings similares para pares preparados de imagen-texto.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"Flowchart with text &quot;Embedding Space&quot;, linked to &quot;Image Encoder&quot; and &quot;Text Encoder&quot;, with a &quot;Distracted boyfriend&quot; label.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Este enfoque produjo resultados impresionantes. Particularmente notable es su rendimiento en clasificaci√≥n zero-shot. Por ejemplo, aunque los datos de entrenamiento no inclu√≠an im√°genes etiquetadas de <a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">astronautas</a>, CLIP pod√≠a identificar correctamente im√°genes de astronautas bas√°ndose en su comprensi√≥n de conceptos relacionados en textos e im√°genes.</p><p>Sin embargo, CLIP de OpenAI tiene dos importantes desventajas:</p><ul><li>La primera es su capacidad muy limitada de entrada de texto. Puede tomar un m√°ximo de 77 tokens de entrada, pero el <a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">an√°lisis emp√≠rico muestra</a> que en la pr√°ctica no usa m√°s de 20 tokens para producir sus embeddings. Esto es porque CLIP fue entrenado con im√°genes con subt√≠tulos, y los subt√≠tulos tienden a ser muy cortos. Esto contrasta con los modelos actuales de embedding de texto que soportan varios miles de tokens.</li><li>Segundo, el rendimiento de sus embeddings de texto en escenarios de recuperaci√≥n de solo texto es muy pobre. Los subt√≠tulos de im√°genes son un tipo muy limitado de texto y no reflejan la amplia gama de casos de uso que se esperar√≠a que un modelo de embedding de texto soporte.</li></ul><p>En la mayor√≠a de los casos de uso reales, la recuperaci√≥n de solo texto y texto-imagen se combinan o al menos ambos est√°n disponibles para las tareas. Mantener un segundo modelo de embeddings para tareas de solo texto efectivamente duplica el tama√±o y la complejidad de tu marco de IA.</p><p>El nuevo modelo de Jina AI aborda estos problemas directamente, y <code>jina-clip-v1</code> aprovecha el progreso realizado en los √∫ltimos a√±os para proporcionar un rendimiento de √∫ltima generaci√≥n en tareas que involucran todas las combinaciones de modalidades de texto e imagen.</p><h2 id=\"introducing-jina-clip-v1\">Presentando Jina CLIP v1</h2><p>Jina CLIP v1 mantiene el esquema original de OpenAI CLIP: dos modelos co-entrenados para producir salidas en el mismo espacio de embedding.</p><p>Para la codificaci√≥n de texto, adaptamos la arquitectura <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> utilizada en los <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">modelos Jina Embeddings v2</a>. Esta arquitectura soporta una ventana de entrada de 8k tokens de √∫ltima generaci√≥n y produce vectores de 768 dimensiones, generando embeddings m√°s precisos a partir de textos m√°s largos. Esto es m√°s de 100 veces los 77 tokens de entrada soportados en el modelo CLIP original.</p><p>Para los embeddings de im√°genes, estamos usando el √∫ltimo modelo de la Academia de Inteligencia Artificial de Beijing: el modelo <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code></a>. Hemos comparado emp√≠ricamente varios modelos de IA de im√°genes, prob√°ndolos en contextos multimodales con pre-entrenamiento similar, y <code>EVA-02</code> super√≥ claramente a los dem√°s. Tambi√©n es comparable a la arquitectura Jina BERT en tama√±o de modelo, por lo que las cargas de c√≥mputo para tareas de procesamiento de imagen y texto son aproximadamente id√©nticas.</p><p>Estas elecciones producen beneficios importantes para los usuarios:</p><ul><li>Mejor rendimiento en todos los benchmarks y todas las combinaciones modales, y especialmente grandes mejoras en el rendimiento de embedding de solo texto.</li><li>El rendimiento emp√≠ricamente superior de <code>EVA-02</code> tanto en tareas de imagen-texto como de solo imagen, con el beneficio adicional del entrenamiento adicional de Jina AI, mejorando el rendimiento de solo imagen.</li><li>Soporte para entradas de texto mucho m√°s largas. El soporte de entrada de <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">8k tokens de Jina Embeddings</a> hace posible procesar informaci√≥n textual detallada y correlacionarla con im√°genes.</li><li>Un gran ahorro neto en espacio, c√≥mputo, mantenimiento de c√≥digo y complejidad porque este modelo multimodal es altamente eficiente incluso en escenarios no multimodales.</li></ul><h3 id=\"training\">Entrenamiento</h3><p>Parte de nuestra receta para una IA multimodal de alto rendimiento son nuestros datos y procedimiento de entrenamiento. Notamos que la longitud muy corta de los textos utilizados en los subt√≠tulos de im√°genes es la causa principal del pobre rendimiento de solo texto en los modelos tipo CLIP, y nuestro entrenamiento est√° expl√≠citamente dise√±ado para remediar esto.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"Flowchart illustrating optimization of text and caption-image similarity in three tasks, using a model and encoders, ending i\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>El entrenamiento se realiza en tres pasos:</p><ol><li>Usar datos de im√°genes con subt√≠tulos para aprender a alinear embeddings de imagen y texto, intercalados con pares de texto con significados similares. Este co-entrenamiento optimiza conjuntamente para los dos tipos de tareas. El rendimiento de solo texto del modelo disminuye durante esta fase, pero no tanto como si hubi√©ramos entrenado solo con pares de imagen-texto.</li><li>Entrenar usando datos sint√©ticos que alinean im√°genes con textos m√°s largos, generados por un modelo de IA, que describe la imagen. Continuar entrenando con pares de solo texto al mismo tiempo. Durante esta fase, el modelo aprende a atender textos m√°s largos en conjunto con im√°genes.</li><li>Usar tripletes de texto con <a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">negativos dif√≠ciles</a> para mejorar a√∫n m√°s el rendimiento de solo texto aprendiendo a hacer distinciones sem√°nticas m√°s finas. Al mismo tiempo, continuar entrenando usando pares sint√©ticos de im√°genes y textos largos. Durante esta fase, el rendimiento de solo texto mejora dram√°ticamente sin que el modelo pierda ninguna capacidad de imagen-texto.</li></ol><p>Para m√°s informaci√≥n sobre los detalles del entrenamiento y la arquitectura del modelo, por favor lee <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">nuestro art√≠culo reciente</a>:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">El Preentrenamiento Contrastivo de Lenguaje-Imagen (CLIP) se utiliza ampliamente para entrenar modelos que alinean im√°genes y textos en un espacio de embedding com√∫n, mape√°ndolos a vectores de tama√±o fijo. Estos modelos son clave para la recuperaci√≥n de informaci√≥n multimodal y tareas relacionadas. Sin embargo, los modelos CLIP generalmente tienen un rendimiento inferior en tareas de solo texto en comparaci√≥n con modelos especializados de texto. Esto crea ineficiencias para los sistemas de recuperaci√≥n de informaci√≥n que mantienen embeddings y modelos separados para tareas de solo texto y multimodales. Proponemos un nuevo m√©todo de entrenamiento contrastivo multi-tarea para abordar este problema, que usamos para entrenar el modelo jina-clip-v1 y lograr un rendimiento estado del arte tanto en tareas de recuperaci√≥n texto-imagen como texto-texto.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">Nuevo Estado del Arte en Embeddings Multimodales</h2><p>Evaluamos el rendimiento de Jina CLIP v1 en tareas de solo texto, solo imagen y tareas multimodales que involucran ambas modalidades de entrada. Utilizamos el <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">benchmark de recuperaci√≥n MTEB</a> para evaluar el rendimiento de solo texto. Para tareas de solo imagen, usamos el benchmark <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a>. Para tareas multimodales, evaluamos en <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a>, y <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a>, que est√°n incluidos en el <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">Benchmark CLIP</a>.</p><p>Los resultados se resumen en la siguiente tabla:</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Como puede verse en estos resultados, <code>jina-clip-v1</code> supera al CLIP original de OpenAI en todas las categor√≠as, y es dram√°ticamente mejor en recuperaci√≥n de solo texto y solo imagen. Promediando todas las categor√≠as, esto representa una mejora del 46% en rendimiento.</p><p>Puede encontrar una evaluaci√≥n m√°s detallada en <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">nuestro art√≠culo reciente</a>.</p><h2 id=\"getting-started-with-embeddings-api\">Comenzando con la API de Embeddings</h2><p>Puede integrar f√°cilmente Jina CLIP v1 en sus aplicaciones usando la <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">API de Embeddings de Jina</a>.</p><p>El c√≥digo a continuaci√≥n muestra c√≥mo llamar a la API para obtener embeddings de textos e im√°genes, usando el paquete <code>requests</code> en Python. Pasa una cadena de texto y una URL a una imagen al servidor de Jina AI y devuelve ambas codificaciones.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">‚òùÔ∏è</div><div class=\"kg-callout-text\">Recuerde reemplazar <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> con una clave API de Jina activada. Puede obtener una clave de prueba con un mill√≥n de tokens gratuitos desde la <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">p√°gina web de Jina Embeddings</a>.</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">Integraci√≥n con Principales Frameworks de LLM</h3><p>Jina CLIP v1 ya est√° disponible para <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> y <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a>:</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>: Use <code>JinaEmbedding</code> con la clase base <code>MultimodalEmbedding</code>, y llame a <code>get_image_embeddings</code> o <code>get_text_embeddings</code>.</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>: Use <code>JinaEmbeddings</code>, y llame a <code>embed_images</code> o <code>embed_documents</code>.</li></ul><h3 id=\"pricing\">Precios</h3><p>Tanto las entradas de texto como de imagen se cobran por consumo de tokens.</p><p>Para texto en ingl√©s, <a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">hemos calculado emp√≠ricamente</a> que en promedio necesitar√° 1.1 tokens por cada palabra.</p><p>Para im√°genes, contamos el n√∫mero de mosaicos de 224x224 p√≠xeles necesarios para cubrir su imagen. Algunos de estos mosaicos pueden estar parcialmente en blanco pero cuentan igual. Cada mosaico cuesta 1,000 tokens para procesar.</p><p><strong>Ejemplo</strong></p><p>Para una imagen con dimensiones de 750x500 p√≠xeles:</p><ol><li>La imagen se divide en mosaicos de 224x224 p√≠xeles.<ol><li>Para calcular el n√∫mero de mosaicos, tome el ancho en p√≠xeles y divida por 224, luego redondee al entero m√°s cercano. <br>     750/224 ‚âà 3.35 ‚Üí 4</li><li>Repita para la altura en p√≠xeles: <br>     500/224 ‚âà 2.23 ‚Üí 3</li></ol></li><li>El n√∫mero total de mosaicos requeridos en este ejemplo es: <br>           4 (horizontal) x 3 (vertical) = 12 mosaicos</li><li>El costo ser√° 12 x 1,000 = 12,000 tokens </li></ol><h3 id=\"enterprise-support\">Soporte Empresarial</h3><p>Estamos introduciendo un nuevo beneficio para usuarios que compren el plan de Despliegue en Producci√≥n con <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">11 mil millones de tokens</a>. Esto incluye:</p><ul><li>Tres horas de consultor√≠a con nuestros equipos de producto e ingenier√≠a para discutir sus casos de uso espec√≠ficos y requisitos.</li><li>Un notebook Python personalizado dise√±ado para su caso de uso de RAG (Generaci√≥n Aumentada por Recuperaci√≥n) o b√∫squeda vectorial, demostrando c√≥mo integrar los modelos de Jina AI en su aplicaci√≥n.</li><li>Asignaci√≥n a un ejecutivo de cuenta y soporte prioritario por email para asegurar que sus necesidades sean atendidas de manera r√°pida y eficiente.</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Jina CLIP v1 de C√≥digo Abierto en Hugging Face</h2><p>Jina AI est√° comprometida con una base de b√∫squeda de c√≥digo abierto, y por ese prop√≥sito, estamos haciendo este modelo disponible gratuitamente bajo una <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">licencia Apache 2.0</a>, en <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a>.</p><p>Puede encontrar c√≥digo de ejemplo para descargar y ejecutar este modelo en su propio sistema o instalaci√≥n en la nube en la p√°gina del modelo en Hugging Face para <code>jina-clip-v1</code>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">Estamos en un viaje para avanzar y democratizar la inteligencia artificial a trav√©s del c√≥digo abierto y la ciencia abierta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">Resumen</h2><p>El √∫ltimo modelo de Jina AI ‚Äî <code>jina-clip-v1</code> ‚Äî representa un avance significativo en modelos de embedding multimodales, ofreciendo mejoras sustanciales de rendimiento sobre CLIP de OpenAI. Con mejoras notables en tareas de recuperaci√≥n de solo texto y solo imagen, as√≠ como un rendimiento competitivo en tareas de texto a imagen e imagen a texto, se presenta como una soluci√≥n prometedora para casos de uso complejos de embeddings.</p><p>Actualmente, este modelo solo admite textos en ingl√©s debido a limitaciones de recursos. Estamos trabajando para expandir sus capacidades a m√°s idiomas.</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzm√°n",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "El nuevo modelo de embedding multimodal de Jina AI no solo supera a OpenAI CLIP en la recuperaci√≥n de texto e im√°genes, sino que es a la vez un s√≥lido modelo de embedding de im√°genes y un modelo de embedding de texto de √∫ltima generaci√≥n. Ya no necesitas diferentes modelos para diferentes modalidades.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
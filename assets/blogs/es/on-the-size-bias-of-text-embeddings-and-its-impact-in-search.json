{
  "slug": "on-the-size-bias-of-text-embeddings-and-its-impact-in-search",
  "id": "67e52df15dcba60001c30ebe",
  "uuid": "bae3e1b8-3bf2-4dbc-a553-b26ea64aeb60",
  "title": "Sobre el sesgo de tamaño en los embeddings de texto y su impacto en la búsqueda",
  "html": "<p>La similitud semántica es lo que los modelos de embeddings están construidos para medir, pero esas mediciones están influenciadas por muchos factores de sesgo. En este artículo, vamos a analizar una fuente generalizada de sesgo en los modelos de embeddings de texto: el tamaño de entrada.</p><p><strong>Los embeddings de textos más largos generalmente muestran puntuaciones de similitud más altas cuando se comparan con otros embeddings de texto, independientemente de qué tan similar sea el contenido real.</strong> Si bien los textos verdaderamente similares seguirán teniendo puntuaciones de similitud más altas que los no relacionados, los textos más largos introducen un sesgo: haciendo que sus embeddings parezcan más similares en promedio simplemente debido a su longitud.</p><p>Esto tiene consecuencias reales. Significa que los modelos de embeddings, por sí solos, no pueden medir muy bien la relevancia. Con la búsqueda basada en embeddings, siempre hay una mejor coincidencia, pero el sesgo de tamaño significa que no se puede usar la puntuación de similitud para decidir si la mejor coincidencia, o cualquier coincidencia menor, son realmente relevantes. No se puede decir que, por ejemplo, cualquier coincidencia con un coseno superior a 0.75 es relevante porque puede haber fácilmente un documento largo que coincida a ese nivel a pesar de ser completamente irrelevante.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Comparar vectores de embeddings solo puede informarte sobre la similitud relativa, no sobre la relevancia.</div></div><p>Vamos a demostrar esto con algunos ejemplos simples y mostrar cómo la similitud del coseno entre embeddings de texto no puede servir como una forma general de evaluar</p><h2 id=\"visualizing-size-bias\">Visualizando el Sesgo de Tamaño</h2><p>Para mostrar cómo se manifiesta el sesgo de tamaño, vamos a usar el último modelo de embeddings de Jina AI <code>jina-embeddings-v3</code> con la opción de tarea <code>text-matching</code>. También usaremos documentos de texto de un conjunto de datos de IR ampliamente utilizado: El <a href=\"https://ir.dcs.gla.ac.uk/resources/test_collections/cisi/\">conjunto de datos CISI</a>, que puedes <a href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\">descargar desde Kaggle</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">CISI (a dataset for Information Retrieval)</div><div class=\"kg-bookmark-description\">A public dataset from the University of Glasgow's Information Retrieval Group</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-31.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Kaggle</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/dataset-card.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Este conjunto de datos se usa para entrenar sistemas de IR, por lo que contiene tanto consultas como documentos para emparejarlas. Solo vamos a usar los documentos, que están todos en el archivo <code>CISI.ALL</code>. Puedes descargarlo desde la línea de comandos en una <a href=\"https://github.com/GianRomani/CISI-project-MLOps\">fuente alternativa en GitHub</a> con el comando:</p><pre><code class=\"language-bash\">wget https://raw.githubusercontent.com/GianRomani/CISI-project-MLOps/refs/heads/main/CISI.ALL\n</code></pre><p>CISI contiene 1,460 documentos. Las estadísticas básicas sobre los tamaños de los textos y sus distribuciones de tamaño se resumen en la tabla e histogramas a continuación:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>en Palabras</th>\n<th>en Oraciones</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tamaño promedio del documento</td>\n<td>119.2</td>\n<td>4.34</td>\n</tr>\n<tr>\n<td>Desviación estándar</td>\n<td>63.3</td>\n<td>2.7</td>\n</tr>\n<tr>\n<td>Tamaño máximo</td>\n<td>550</td>\n<td>38</td>\n</tr>\n<tr>\n<td>Tamaño mínimo</td>\n<td>8</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-8.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-9.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Vamos a leer los documentos en Python y obtener embeddings para ellos. El código siguiente asume que el archivo <code>CISI.ALL</code> está en el directorio local:</p><pre><code class=\"language-python\">with open(\"CISI.ALL\", \"r\", encoding=\"utf-8\") as inp:\n    cisi_raw = inp.readlines()\n\ndocs = []\ncurrent_doc = \"\"\nin_text = False\nfor line in cisi_raw:\n    if line.startswith(\".\"):\n        in_text = False\n        if current_doc:\n            docs.append(current_doc.strip())\n            current_doc = \"\"\n        if line.startswith(\".W\"):\n            in_text = True\n    else:\n        if in_text:\n            current_doc += line\n</code></pre><p>Esto llenará la lista <code>docs</code> con 1,460 documentos. Puedes inspeccionarlos:</p><pre><code class=\"language-text\">print(docs[0])\n\nThe present study is a history of the DEWEY Decimal\nClassification.  The first edition of the DDC was published\nin 1876, the eighteenth edition in 1971, and future editions\nwill continue to appear as needed.  In spite of the DDC's\nlong and healthy life, however, its full story has never\nbeen told.  There have been biographies of Dewey\nthat briefly describe his system, but this is the first\nattempt to provide a detailed history of the work that\nmore than any other has spurred the growth of\nlibrarianship in this country and abroad.</code></pre><p>Ahora, vamos a construir embeddings para cada texto usando <code>jina-embeddings-v3</code>. Para esto, necesitarás <a href=\"https://jina.ai/embeddings/#apiform\">una clave API del sitio web de Jina AI</a>. Puedes obtener una clave gratuita para hasta 1 millón de tokens de embeddings, lo cual es suficiente para este artículo.</p><p>Coloca tu clave en una variable:</p><pre><code class=\"language-python\">api_key = \"&lt;Your Key&gt;\"\n</code></pre><p>Ahora, genera embeddings usando la tarea <code>text-matching</code> con <code>jina-embeddings-v3</code>. Este código procesa los textos en <code>docs</code> en lotes de 10.</p><pre><code class=\"language-python\">import requests\nimport json\nfrom numpy import array\n\nembeddings  = []\n\nurl = \"https://api.jina.ai/v1/embeddings\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\n\ni = 0\nwhile i &lt; len(docs):\n    print(f\"Got {len(embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": docs[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>Para cada texto, habrá un embedding de 1024 dimensiones en la lista <code>embeddings</code>. Puedes ver cómo se ve:</p><pre><code class=\"language-python\">print(embeddings[0])\n\narray([ 0.0352382 , -0.00594871,  0.03808545, ..., -0.01147173,\n         -0.01710563,  0.01109511], shape=(1024,))),\n</code></pre><p>Ahora, calculamos los cosenos entre todos los pares de embeddings. Primero, definamos la función del coseno <code>cos_sim</code> usando <code>numpy</code>:</p><pre><code class=\"language-python\">from numpy import dot\nfrom numpy.linalg import norm\n\ndef cos_sim(a, b): \n    return float((a @ b.T) / (norm(a)*norm(b)))\n</code></pre><p>Luego, calculamos los cosenos de cada uno de los 1,460 embeddings comparados con los otros 1,459:</p><pre><code class=\"language-python\">all_cosines = []\nfor i, emb1 in enumerate(embeddings):\n    for j, emb2 in enumerate(embeddings):\n        if i != j:\n            all_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>El resultado es una lista de 2,130,140 valores. Su distribución debería aproximarse a los cosenos entre documentos \"aleatorios\" en el mismo idioma y registro. La tabla e histograma a continuación resumen los resultados.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Número de textos</th>\n<th>1,460</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Número de cosenos</td>\n<td>2,130,140</td>\n</tr>\n<tr>\n<td>Promedio</td>\n<td>0.343</td>\n</tr>\n<tr>\n<td>Desviación estándar</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-10.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>Estos documentos, aunque no están relacionados entre sí, típicamente tienen cosenos muy por encima de cero. Podríamos estar tentados a establecer un umbral de 0.459 (promedio + 1 desviación estándar), o tal vez redondearlo a 0.5, y decir que cualquier par de documentos con un coseno menor que ese debe estar en gran parte no relacionado.</p><p>Pero hagamos el mismo experimento con textos más pequeños. Usaremos la biblioteca <a href=\"https://www.nltk.org/\" rel=\"noreferrer\"><code>nltk</code></a> para dividir cada documento en oraciones:</p><pre><code class=\"language-python\">import nltk\n\nsentences = []\nfor doc in docs:\n    sentences.extend(nltk.sent_tokenize(doc)) \n</code></pre><p>Esto produce 6,331 oraciones con una longitud promedio de 27.5 palabras y una desviación estándar de 16.6. En el histograma siguiente, la distribución de tamaño de las oraciones está en rojo, y para documentos completos, está en azul, para que puedas compararlos.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-12.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"567\" height=\"455\"></figure><p>Usaremos el mismo modelo y métodos para obtener embeddings para cada oración:</p><pre><code class=\"language-python\">sentence_embeddings = []\n\ni = 0\nwhile i &lt; len(sentences):\n    print(f\"Got {len(sentence_embeddings)}...\")\n    data = {\n        \"model\": \"jina-embeddings-v3\",\n        \"task\": \"text-matching\",\n        \"late_chunking\": False,\n        \"dimensions\": 1024,\n        \"embedding_type\": \"float\",\n        \"input\": sentences[i:i+10]\n    }\n    \n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    for emb in response.json()['data']:\n        sentence_embeddings.append(array(emb['embedding']))\n    i += 10\n</code></pre><p>Y luego calcular el coseno del embedding de cada oración con cada una de las otras oraciones:</p><pre><code class=\"language-python\">sent_cosines = []\nfor i, emb1 in enumerate(sentence_embeddings):\n    for j, emb2 in enumerate(sentence_embeddings):\n        if i != j:\n            sent_cosines.append(cos_sim(emb1, emb2))\n</code></pre><p>El resultado es una cantidad bastante mayor de valores de coseno: 40.075.230, como se resume en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of sentences</th>\n<th>6,331</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>40,075,230</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.254</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Los cosenos entre oraciones son considerablemente más bajos en promedio que los cosenos entre documentos completos. El histograma siguiente compara sus distribuciones, y se puede ver claramente que los pares de oraciones forman una distribución casi idéntica a la de los pares de documentos pero desplazada hacia la izquierda.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-14.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Para comprobar que esta dependencia del tamaño es robusta, calculemos todos los cosenos entre oraciones y documentos y añadámoslos al histograma. Su información se resume en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>6,331 sentences &amp; 1,460 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>9,243,260</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.276</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.119</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La línea verde de abajo muestra la distribución de los cosenos entre oraciones y documentos. Podemos ver que esta distribución se ajusta perfectamente entre los cosenos documento-documento y los cosenos oración-oración, mostrando que el efecto del tamaño involucra a <em>ambos</em> textos que se están comparando, tanto el más grande como el más pequeño.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-16.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Hagamos otra prueba concatenando los documentos en grupos de diez, creando 146 documentos mucho más grandes y midiendo sus cosenos. El resultado se resume a continuación:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Number of texts</th>\n<th>146 documents</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cosines</td>\n<td>21,170</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>0.658</td>\n</tr>\n<tr>\n<td>Std. Deviation</td>\n<td>0.09</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-17.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"587\" height=\"455\"></figure><p>Esto está <em>muy</em> a la derecha de las otras distribuciones. Un umbral de coseno de 0.5 nos diría que casi todos estos documentos están relacionados entre sí. Para excluir documentos irrelevantes de este tamaño, tendríamos que establecer el umbral mucho más alto, tal vez hasta 0.9, lo que sin duda excluiría buenas coincidencias entre los documentos más pequeños.</p><p>Esto demuestra que no podemos usar umbrales mínimos de coseno para estimar qué tan buena es una coincidencia, al menos no sin tener en cuenta de alguna manera el tamaño del documento.</p><h2 id=\"what-causes-size-bias\">¿Qué Causa el Sesgo por Tamaño?</h2><p>El sesgo por tamaño en los embeddings no es como los <a href=\"https://jina.ai/news/long-context-embedding-models-are-blind-beyond-4k-tokens/\">sesgos posicionales en modelos de contexto largo</a>. No es causado por las arquitecturas. Tampoco se trata inherentemente del tamaño. Por ejemplo, si hubiéramos creado documentos más largos simplemente concatenando copias del mismo documento una y otra vez, no mostraría un sesgo por tamaño.</p><p>El problema es que los textos largos dicen más cosas. Incluso si están limitados por un tema y propósito, el objetivo de escribir más palabras es decir más cosas.</p><p>Los textos más largos, al menos del tipo que la gente normalmente crea, naturalmente producirán embeddings que se \"dispersan\" sobre más espacio semántico. Si un texto dice más cosas, su embedding tendrá un ángulo menor con otros vectores en promedio, independientemente del tema del texto.</p><h2 id=\"measuring-relevance\">Midiendo la Relevancia</h2><p>La lección de este post es que no se pueden usar cosenos entre vectores semánticos <em>por sí solos</em> para determinar si algo es una buena coincidencia, solo que es la mejor coincidencia entre las disponibles. Hay que hacer algo más además de calcular cosenos para verificar la utilidad y validez de las mejores coincidencias.</p><p>Podrías intentar la normalización. Si puedes medir el sesgo por tamaño empíricamente, puede ser posible compensarlo. Sin embargo, este enfoque podría no ser muy robusto. Lo que funciona para un conjunto de datos probablemente no funcionará para otro.</p><p>La <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/\">codificación asimétrica consulta-documento</a>, proporcionada en <code>jina-embeddings-v3</code>, reduce el sesgo por tamaño en los modelos de embedding pero no lo elimina. El propósito de la codificación asimétrica es codificar los documentos para que estén menos \"dispersos\" y codificar las consultas para que lo estén más.</p><p>La línea roja en el histograma siguiente es la distribución de los cosenos documento-documento usando codificación asimétrica con <code>jina-embeddings-v3</code> – cada documento se codifica usando las banderas <code>retrieval.query</code> y <code>retrieval.passage</code>, y cada embedding de consulta de documento se compara con cada embedding de pasaje de documento que no sea del mismo documento. El coseno promedio es 0.200, con una desviación estándar de 0.124.</p><p>Estos cosenos son considerablemente más pequeños que los que encontramos arriba para los mismos documentos usando la bandera <code>text-matching</code>, como se muestra en el histograma siguiente.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-25.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"591\" height=\"455\"></figure><p>Sin embargo, la codificación asimétrica no ha eliminado el sesgo por tamaño. El histograma siguiente compara los cosenos para documentos completos y oraciones usando codificación asimétrica.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/image-23.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"576\" height=\"455\"></figure><p>El promedio para los cosenos de oraciones es 0.124, por lo que usando codificación asimétrica, la diferencia entre el coseno promedio de oraciones y el coseno promedio de documentos es 0.076. La diferencia en promedios para la codificación simétrica es 0.089. El cambio en el sesgo por tamaño es insignificante.</p><p>Aunque la codificación asimétrica mejora los embeddings para la recuperación de información, no es mejor para medir la relevancia de las coincidencias.</p><h2 id=\"future-possibilities\">Posibilidades Futuras</h2><p>El enfoque del reranker, por ejemplo <code>jina-reranker-v2-base-multilingual</code> y <code>jina-reranker-m0</code>, es una forma alternativa de puntuar las coincidencias consulta-documento que ya sabemos que <a href=\"https://jina.ai/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/\">mejora la precisión de las consultas</a>. Las puntuaciones del reranker no están normalizadas, por lo que tampoco funcionan como medidas de similitud objetivas. Sin embargo, se calculan de manera diferente, y podría ser posible normalizar las puntuaciones del reranker de manera que sean buenos estimadores de relevancia.</p><p>Otra alternativa es usar modelos de lenguaje grandes, preferiblemente con fuertes capacidades de razonamiento, para evaluar directamente si un candidato es una buena coincidencia para una consulta. De manera simplista, podríamos preguntarle a un modelo de lenguaje grande específico para la tarea: \"¿En una escala del 1 al 10, este documento es una buena coincidencia para esta consulta?\" Los modelos existentes podrían no estar bien adaptados para la tarea, pero el entrenamiento focalizado y técnicas de prompt más sofisticadas son prometedores.</p><p>No es imposible que los modelos midan la relevancia, pero requiere un paradigma diferente de los modelos de embedding.</p><h2 id=\"use-your-models-for-what-its-good-for\">Usa tus Modelos para lo que son Buenos</h2><p>El efecto de sesgo por tamaño que hemos documentado arriba muestra una de las limitaciones fundamentales de los modelos de embedding: Son excelentes para comparar cosas pero poco fiables para medir la relevancia absoluta. Esta limitación no es un defecto en el diseño—es una característica inherente de cómo funcionan estos modelos.</p><p>Entonces, ¿qué significa esto para ti?</p><p>Primero, sé escéptico con los umbrales de coseno. Simplemente no funcionan. Las medidas de similitud por coseno producen números de punto flotante que parecen tentadoramente objetivos. Pero el hecho de que algo produzca números no significa que esté midiendo algo objetivamente.</p><p>Segundo, considera soluciones híbridas. Los embeddings pueden reducir eficientemente un gran conjunto de elementos a candidatos prometedores, después de lo cual puedes aplicar técnicas más sofisticadas (y computacionalmente intensivas) como rerankers o LLMs, o incluso evaluadores humanos para determinar la relevancia real.</p><p>Tercero, al diseñar sistemas, piensa en términos de tareas en lugar de capacidades. El modelo objetivamente más inteligente, con las puntuaciones más altas en benchmarks sigue siendo un desperdicio de dinero si no puede hacer el trabajo para el que lo conseguiste.</p><p>Entender las limitaciones de nuestros modelos no es pesimista – refleja un principio más amplio en las aplicaciones: Entender para qué son buenos tus modelos, y para qué no, es crítico para construir sistemas confiables y efectivos. Al igual que no usaríamos un martillo para apretar un tornillo, no deberíamos usar modelos de embedding para tareas que no pueden manejar. Respeta para qué son buenos tus herramientas.</p>",
  "comment_id": "67e52df15dcba60001c30ebe",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading---2025-04-16T094756.687.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-27T11:52:33.000+01:00",
  "updated_at": "2025-04-16T03:48:15.000+02:00",
  "published_at": "2025-04-16T03:40:03.000+02:00",
  "custom_excerpt": "Size bias refers to how the length of text inputs affects similarity, regardless of semantic relevance. It explains why search systems sometimes return long, barely-relevant documents instead of shorter, more precise matches to your query.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/on-the-size-bias-of-text-embeddings-and-its-impact-in-search/",
  "excerpt": "El sesgo de tamaño se refiere a cómo la longitud de los textos de entrada afecta la similitud, independientemente de la relevancia semántica. Explica por qué los sistemas de búsqueda a veces devuelven documentos largos y apenas relevantes en lugar de coincidencias más cortas y precisas para tu consulta.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
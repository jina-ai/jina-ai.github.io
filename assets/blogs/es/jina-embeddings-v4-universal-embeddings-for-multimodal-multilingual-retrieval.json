{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4：向量模型 (Embeddings) 通用模型，适用于多模态多语言检索",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Modelos de Búsqueda Fundacionales</div><div class=\"kg-bookmark-description\">Modelo de \"embedding\" universal para la recuperación multimodal y multilingüe</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Modelos de Búsqueda Fundacionales</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: \"Embeddings\" Universales para la Recuperación Multimodal Multilingüe</div><div class=\"kg-bookmark-description\">Presentamos jina-embeddings-v4, un modelo de \"embedding\" multimodal de 3.8 mil millones de parámetros que unifica representaciones de texto e imagen a través de una arquitectura novedosa que admite \"embeddings\" de un solo vector y de múltiples vectores en el estilo de interacción tardía. El modelo incorpora adaptadores de Adaptación de Bajo Rango (LoRA) específicos de la tarea para optimizar el rendimiento en diversos escenarios de recuperación, incluida la recuperación de información basada en consultas, la similitud semántica entre modalidades y la búsqueda de código de programación. Las evaluaciones exhaustivas demuestran que jina-embeddings-v4 logra un rendimiento de vanguardia tanto en tareas de recuperación unimodal como intermodal, con una fortaleza particular en el procesamiento de contenido visualmente rico, como tablas, gráficos, diagramas y formatos multimedia mixtos. Para facilitar la evaluación de esta capacidad, también presentamos Jina-VDR, un nuevo punto de referencia diseñado específicamente para la recuperación de imágenes visualmente ricas.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">Estamos en un viaje para avanzar y democratizar la inteligencia artificial a través del código abierto y la ciencia abierta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Hoy lanzamos <code>jina-embeddings-v4</code>, nuestro nuevo modelo de \"embedding\" universal de 3.8 mil millones de parámetros para texto e imágenes. Incluye un conjunto de adaptadores LoRA específicos de la tarea que optimizan el rendimiento para las tareas de recuperación más populares, incluida la recuperación de consulta-documento, la coincidencia semántica y la búsqueda de código. <code>jina-embeddings-v4</code> logra un rendimiento de recuperación de vanguardia en tareas multimodales y multilingües en los puntos de referencia MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP y ViDoRe, con una fortaleza particular en el procesamiento de contenido visualmente rico, como tablas, gráficos, diagramas y mezclas de ellos. El modelo admite \"embeddings\" de un solo vector y de múltiples vectores.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Rendimiento de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> en la recuperación de documentos visuales y puntos de referencia multimodales. Las distribuciones de diagramas de caja muestran las puntuaciones medias y la variabilidad del rendimiento de los modelos de \"embedding\" en seis categorías de puntos de referencia: ViDoRe (recuperación de documentos de visión), Jina-VDR (recuperación integral de documentos visuales), Recuperación de Wikimedia Commons (coincidencia multilingüe de documento-descripción), Recuperación de GitHub README (recuperación de documentación de código), Recuperación de Tweet Stock (análisis de gráficos financieros) y CLIP Benchmark (recuperación general de texto a imagen). Las variantes de Jina-embeddings-v4 (resaltadas en cian) demuestran un rendimiento de vanguardia en tareas de documentos visualmente ricos, con la versión multivectorial logrando las puntuaciones más altas en puntos de referencia de documentos visuales especializados (90.2 en ViDoRe, 80.2 en Jina-VDR), al tiempo que mantiene un rendimiento competitivo en tareas generales de recuperación multimodal (84.1 en CLIP Benchmark). Los modelos se clasifican por rendimiento medio dentro de cada categoría de punto de referencia, con puntos de datos individuales que muestran distribuciones de puntuaciones en múltiples tareas de evaluación.</span></figcaption></figure><p><code>jina-embeddings-v4</code> es nuestro modelo de \"embedding\" más ambicioso hasta el momento. Como modelo de código abierto, <code>jina-embeddings-v4</code> supera a los principales modelos de \"embedding\" de código cerrado de los principales proveedores, ofreciendo un rendimiento un 12% mejor que <code>text-embedding-3-large</code> de OpenAI en la recuperación multilingüe (66.49 frente a 59.27), una mejora del 28% en las tareas de documentos largos (67.11 frente a 52.42), un 15% mejor que <code>voyage-3</code> en la recuperación de código (71.59 frente a 67.23) e igualando el rendimiento de <code>gemini-embedding-001</code> de Google. Esto convierte a v4 en el modelo de \"embedding\" universal de código abierto más capaz disponible en la actualidad, que ofrece a los investigadores y desarrolladores capacidades de \"embedding\" multimodal de nivel empresarial con total transparencia en el proceso de entrenamiento, las decisiones arquitectónicas y los pesos del modelo a través de <a href=\"https://arxiv.org/abs/2506.18902\">nuestro informe técnico completo.</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Rendimiento de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> en cinco puntos de referencia de recuperación. El gráfico muestra distribuciones de diagramas de caja con puntuaciones medias para cada modelo en los puntos de referencia de Recuperación de texto, Recuperación de código, Recuperación multilingüe, Recuperación de contexto largo y Similitud textual semántica (STS). </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (resaltado en cian) demuestra un rendimiento competitivo o de vanguardia en todas las categorías de evaluación, con resultados particularmente sólidos en la recuperación de texto y STS. Los modelos se clasifican por rendimiento medio dentro de cada categoría de punto de referencia, con puntos de datos individuales que muestran distribuciones de puntuaciones en múltiples tareas de evaluación.</span></figcaption></figure><h2 id=\"new-architecture\">Nueva Arquitectura</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Arquitectura de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">. El modelo está construido sobre la base de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> (3.8B parámetros). Las entradas de texto e imagen se procesan a través de una vía compartida: las imágenes primero se convierten en secuencias de \"tokens\" a través de un codificador de visión, luego ambas modalidades se procesan conjuntamente mediante el decodificador del modelo de lenguaje con capas de atención contextual. Tres adaptadores LoRA específicos de la tarea (60 millones de parámetros cada uno) proporcionan una optimización especializada para las tareas de recuperación, coincidencia de texto y código sin modificar los pesos de la base congelada. La arquitectura admite modos de salida dual: (1) \"embeddings\" de un solo vector (2048 dimensiones, truncables a 128) generados a través de la agrupación media para una búsqueda de similitud eficiente, y (2) \"embeddings\" de múltiples vectores (128 dimensiones por \"token\") a través de capas de proyección para estrategias de recuperación de interacción tardía.</span></figcaption></figure><p>La actualización de <code>jina-embeddings-v3</code> a<code>jina-embeddings-v4</code> representa un cambio de paradigma desde los modelos de 向量模型 (Embeddings) que solo utilizaban texto, hacia los modelos multimodales. Mientras que v3 se centraba en optimizar los 向量模型 (Embeddings) de texto con adaptadores LoRA específicos para cada tarea, v4 aborda la creciente necesidad de integrar contenido textual y visual en representaciones unificadas.\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Aspecto</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Modelo Base</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>Parámetros (Base)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>Parámetros (con adaptadores)</td>\n<td>572M</td>\n<td>3.8B + 60M por adaptador</td>\n</tr>\n<tr>\n<td>Modalidades</td>\n<td>Solo texto</td>\n<td>Texto + Imágenes (multimodal)</td>\n</tr>\n<tr>\n<td>Longitud Máxima de Entrada</td>\n<td>8,192 词元 (Tokens)</td>\n<td>32,768 词元 (Tokens)</td>\n</tr>\n<tr>\n<td>Procesamiento de Imágenes</td>\n<td>Ninguno</td>\n<td>Hasta 20 megapíxeles, documentos visualmente ricos</td>\n  </tr>\n<tr>\n<td>Soporte Multilingüe</td>\n<td>89 idiomas</td>\n<td>29+ idiomas</td>\n</tr>\n<tr>\n<td>Tipos de Vectores</td>\n<td>Solo vector único</td>\n<td>Vector único + Multi-vector (interacción tardía)</td>\n</tr>\n<tr>\n<td>Dimensiones del Vector Único</td>\n<td>1024 (MRL truncable a 32)</td>\n<td>2048 (MRL truncable a 128)</td>\n</tr>\n<tr>\n<td>Dimensiones del Multi-vector</td>\n<td>No disponible</td>\n<td>128 por 词元 (Token)</td>\n</tr>\n<tr>\n<td>Especializaciones LoRA por Tarea</td>\n<td>• Recuperación asimétrica<br>• Similitud semántica<br>• Clasificación<br>• Separación</td>\n<td>• Recuperación asimétrica<br>• Similitud semántica<br>• Recuperación de código</td>\n</tr>\n<tr>\n<td>Etapas de Entrenamiento</td>\n<td>3 etapas: Pre-entrenamiento → Ajuste fino del 向量模型 (Embedding) → Entrenamiento del adaptador</td>\n<td>2 etapas: Entrenamiento de pares conjuntos → Entrenamiento del adaptador específico para la tarea</td>\n</tr>\n<tr>\n<td>Funciones de Pérdida</td>\n<td>InfoNCE, CoSent, Pérdida de tripletes extendida</td>\n<td>InfoNCE conjunto + Divergencia KL para vector único/múltiple</td>\n</tr>\n<tr>\n<td>Codificación Posicional</td>\n<td>RoPE (ajuste de frecuencia base rotatoria)</td>\n<td>M-RoPE (Codificación Posicional Rotatoria Multimodal)</td>\n</tr>\n<tr>\n<td>Procesamiento Intermodal</td>\n<td>N/A</td>\n<td>Codificador unificado (brecha de modalidad reducida)</td>\n</tr>\n<tr>\n<td>Soporte MRL</td>\n<td>Sí</td>\n<td>Sí</td>\n</tr>\n<tr>\n<td>Implementación de Atención</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"backbone\">Backbone</h3><p>El cambio arquitectónico más significativo en v4 es el cambio del backbone de <code>XLM-RoBERTa</code> a <code>Qwen2.5-VL-3B-Instruct</code>. Esta decisión fue impulsada por el objetivo central de v4 de crear un modelo de 向量模型 (Embedding) universal que permita el \"verdadero procesamiento multimodal\" donde las imágenes se convierten en secuencias de 词元 (Tokens) y se procesan junto con el texto, eliminando la <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">brecha de modalidad</a> presente en las arquitecturas de codificador dual.</p><p>La selección del backbone se alinea con varios objetivos de diseño clave: la excelencia de Qwen2.5-VL en la comprensión de documentos apoya directamente la fortaleza de v4 en el procesamiento de contenido visualmente rico como tablas, gráficos y capturas de pantalla. Las capacidades de resolución dinámica permiten a v4 manejar imágenes redimensionadas a 20 megapíxeles como se especifica en la arquitectura. La codificación posicional avanzada proporciona la base que permite a v4 lograr una alineación intermodal superior con una puntuación de alineación de 0.71 en comparación con 0.15 para OpenAI CLIP.</p><h3 id=\"lora-adapters\">Adaptadores LoRA</h3><p>V4 se simplifica de las cinco tareas de v3 a tres tareas enfocadas, lo que refleja las lecciones aprendidas sobre la efectividad y la adopción por parte del usuario:</p><ul><li><strong>Recuperación asimétrica</strong> (consolidando los adaptadores de consulta/pasaje de v3)</li><li><strong>Similitud simétrica</strong> (el equivalente de coincidencia de texto de v3 para las tareas STS)</li><li><strong>Recuperación de código</strong> (aprendido de v2-code, ausente en v3)</li></ul><p>Esta consolidación elimina los adaptadores de clasificación y separación de v3, enfocando v4 en los casos de uso de 向量模型 (Embedding) de mayor impacto: la recuperación y STS.</p><h3 id=\"output-embeddings\">Output Embeddings</h3><p>V4 introduce un sistema de salida dual que soporta tanto 向量模型 (Embeddings) de vector único como multi-vector, mientras que v3 solo proporcionaba salidas de vector único. Esto aborda diferentes escenarios de recuperación:</p><ul><li><strong>Modo de vector único</strong>: 向量模型 (Embeddings) de 2048 dimensiones (truncables a 128 a través de MRL) para una búsqueda de similitud eficiente</li><li><strong>Modo multi-vector</strong>: 128 dimensiones por 词元 (Token) para la <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">recuperación de interacción tardía</a></li></ul><p>Este enfoque dual proporciona una mayor eficacia con representaciones multi-vectoriales, particularmente en la recuperación de documentos visualmente ricos, manteniendo la eficiencia para las tareas de similitud estándar. La ventaja de rendimiento constante del 7-10% del modo multi-vector sobre el modo de vector único en las tareas visuales sugiere que la interacción tardía proporciona una coincidencia semántica fundamentalmente mejor para el contenido multimodal.</p><h3 id=\"parameter-size\">Parameter Size</h3><p>Si bien v4 es 6.7 veces más grande que v3 (3.8B vs 570M parámetros), las mejoras de rendimiento solo en texto son en realidad modestas, lo que sugiere que el escalamiento de parámetros fue impulsado principalmente por los requisitos multimodales en lugar de la mejora de texto. En los puntos de referencia de texto centrales, v4 alcanza 66.49 en MMTEB en comparación con los 58.58 de v3 (mejora del 14%) y 55.97 en MTEB-EN versus los 54.33 de v3 (mejora del 3%). Para la recuperación de código, v4 obtiene 71.59 en CoIR en comparación con los 55.07 de v3 (mejora del 30%), mientras que el rendimiento de documentos largos muestra v4 en 67.11 versus los 55.66 de v3 en LongEmbed (mejora del 21%). El escalamiento sustancial se justifica al considerar las capacidades multimodales de v4: alcanzar 84.11 nDCG@5 en la recuperación de documentos visuales (Jina-VDR) y 90.17 en los puntos de referencia de ViDoRe, capacidades totalmente ausentes en v3. El aumento de parámetros representa así nuestra inversión en la funcionalidad multimodal manteniendo un rendimiento de texto competitivo, con la arquitectura unificada eliminando la necesidad de modelos separados de texto y visión al tiempo que se logra una alineación intermodal de 0.71 en comparación con 0.15 para los enfoques tradicionales de codificador dual.</p><h2 id=\"getting-started\">Empezando</h2><p>Para una verificación rápida, pruebe nuestra demostración de texto a imagen en la caja de herramientas de Search Foundation. Hemos preparado una colección de imágenes de documentos de nuestro sitio web, y también puede agregar sus propias URL de imágenes. Simplemente escriba su consulta y presione enter para ver los resultados clasificados. Puede retirarlo ya sea como OCR o recuperación de imágenes basada en contenido; también siéntase libre de probar consultas en idiomas que no sean inglés.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">La demostración está disponible en: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> Tenga en cuenta que el uso de esta demostración consumirá los 词元 (Tokens) de su clave API principal. Además, la demostración puede parecer un poco lenta, ya que necesita descargar todas las imágenes en el servidor desde esas URL, y no se implementa el almacenamiento en caché para las imágenes.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">A través de la API</h3><p>El código a continuación muestra cómo usar <code>jina-embeddings-v4</code>. Puede pasar una cadena de texto, una imagen codificada en base64 o una URL de imagen. Los nuevos usuarios pueden obtener una clave API de Jina con 10 millones de 词元 (Tokens) gratuitos.</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>Debido a los recursos limitados de la GPU, nuestra API de Vectorización (Embedding API) actualmente admite documentos de hasta 8K *tokens* (Tokens) de longitud, a pesar de la capacidad nativa de <code>jina-embeddings-v4</code> para manejar hasta 32K *tokens* (Tokens). Para las aplicaciones que requieren contextos más largos de 8K *tokens* (Tokens) (como <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>), recomendamos implementar nuestros modelos a través de CSP o auto alojar el modelo.</p><h3 id=\"via-csp-marketplaces\">A través de los mercados de CSP</h3><p><code>jina-embeddings-v4</code> pronto estará disponible directamente en AWS, Azure y GCP a los precios que se indican allí.</p><h3 id=\"via-huggingface\">A través de HuggingFace</h3><p>Para fines de investigación y experimentación, puede utilizar el modelo localmente desde nuestra página de Hugging Face. Hemos preparado un cuaderno de Google Colab que demuestra cómo funciona.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Conclusión</h2><p><code>jina-embeddings-v4</code> representa nuestro salto más significativo hasta el momento: un modelo de *vectorización* (embedding) universal de 3.8 mil millones de parámetros que procesa texto e imágenes a través de una vía unificada, que admite la recuperación densa y de interacción tardía, al tiempo que supera a los modelos propietarios de Google, OpenAI y Voyage AI, especialmente en la recuperación de documentos visualmente ricos. Pero esta capacidad no surgió de forma aislada; es la culminación de cuatro generaciones de resolución de limitaciones fundamentales.</p><p>Cuando comenzamos con <code>jina-embeddings-v1</code> a principios de 2022, todos asumieron que más datos significaban un mejor rendimiento. Demostramos lo contrario: filtrar 1.5 mil millones de pares a 385 millones de ejemplos de alta calidad superó a conjuntos de datos mucho más grandes. La lección: la selección supera a la recopilación.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constituye un conjunto de modelos de *vectorización* (embedding) de oraciones de alto rendimiento, expertos en la traducción de entradas textuales en representaciones numéricas, capturando la semántica del texto. Estos modelos sobresalen en aplicaciones como la recuperación densa y la similitud textual semántica. Este documento detalla el desarrollo de Jina Embeddings, comenzando con la creación de conjuntos de datos pareados y triplete de alta calidad. Subraya el papel crucial de la limpieza de datos en la preparación del conjunto de datos, ofrece información detallada sobre el proceso de entrenamiento del modelo y concluye con una evaluación exhaustiva del rendimiento utilizando el Massive Text Embedding Benchmark (MTEB). Además, para aumentar el conocimiento del modelo sobre la negación gramatical, construimos un nuevo conjunto de datos de entrenamiento y evaluación de declaraciones negadas y no negadas, que ponemos a disposición del público.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Pero los usuarios seguían topándose con el muro de 512 *tokens* (Tokens) de BERT. El entrenamiento en secuencias más largas parecía costoso, hasta que <code>jina-embeddings-v2</code> reveló una solución elegante: entrenar corto, implementar largo. Los sesgos de atención lineal de ALiBi permiten que los modelos entrenados en 512 *tokens* (Tokens) manejen sin problemas 8,192 *tokens* (Tokens) en la inferencia. Obtuvimos más capacidad por menos cómputo.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Los modelos de *vectorización* (embedding) de texto han surgido como herramientas poderosas para transformar oraciones en vectores de características de tamaño fijo que encapsulan información semántica. Si bien estos modelos son esenciales para tareas como la recuperación de información, la agrupación semántica y la *re-clasificación* (re-ranking) de texto, la mayoría de los modelos de código abierto existentes, especialmente los construidos sobre arquitecturas como BERT, luchan por representar documentos extensos y, a menudo, recurren al truncamiento. Un enfoque común para mitigar este desafío implica dividir los documentos en párrafos más pequeños para la *vectorización* (embedding). Sin embargo, esta estrategia da como resultado un conjunto mucho mayor de vectores, lo que en consecuencia conduce a un mayor consumo de memoria y búsquedas de vectores computacionalmente intensivas con una latencia elevada. Para abordar estos desafíos, presentamos Jina Embeddings 2, un modelo de *vectorización* (embedding) de texto de código abierto capaz de acomodar hasta 8192 *tokens* (Tokens). Este modelo está diseñado para trascender el límite convencional de 512 *tokens* (Tokens) y procesar hábilmente documentos largos. Jina Embeddings 2 no solo logra un rendimiento de vanguardia en una variedad de tareas relacionadas con *vectorización* (embedding) en el benchmark MTEB, sino que también coincide con el rendimiento del modelo propietario ada-002 de OpenAI. Además, nuestros experimentos indican que un contexto extendido puede mejorar el rendimiento en tareas como NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>El éxito de <code>jina-embeddings-v2</code> expuso otra limitación: diferentes tareas necesitaban diferentes optimizaciones. En lugar de construir modelos separados, <code>jina-embeddings-v3</code> utilizó pequeños adaptadores LoRA de 60M para personalizar un modelo base de 570M para cualquier tarea. Un modelo se convirtió en cinco modelos especializados.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">Presentamos jina-embeddings-v3, un nuevo modelo de *vectorización* (embedding) de texto con 570 millones de parámetros, que logra un rendimiento de vanguardia en datos multilingües y tareas de recuperación de contexto largo, que admite longitudes de contexto de hasta 8192 *tokens* (Tokens). El modelo incluye un conjunto de adaptadores de Adaptación de Bajo Rango (LoRA) específicos para cada tarea para generar *vectores* (embeddings) de alta calidad para la recuperación de documentos de consulta, la agrupación, la clasificación y la coincidencia de texto. La evaluación en el benchmark MTEB muestra que jina-embeddings-v3 supera a los últimos *vectores* (embeddings) propietarios de OpenAI y Cohere en tareas en inglés, al tiempo que logra un rendimiento superior en comparación con multilingual-e5-large-instruct en todas las tareas multilingües. Con una dimensión de salida predeterminada de 1024, los usuarios pueden reducir de forma flexible las dimensiones de *vectorización* (embedding) hasta 32 sin comprometer el rendimiento, lo que permite el aprendizaje de representación de Matryoshka.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Incluso con la especialización de tareas, seguimos siendo solo texto, mientras que los usuarios necesitaban comprensión visual. Los modelos estándar basados en CLIP como <code>jina-clip-v1</code> y <code>jina-clip-v2</code> utilizan codificadores separados, creando una \"brecha de modalidad\" donde el contenido similar en diferentes formatos termina muy separado. Al igual que nuestro recientemente lanzado <code>jina-reranker-m0</code>, <code>jina-embeddings-v4</code> eliminó esto por completo: una vía unificada procesa todo, eliminando la brecha en lugar de salvarla.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">Presentamos jina-embeddings-v4, un modelo de *vectorización* (embedding) multimodal de 3.8 mil millones de parámetros que unifica las representaciones de texto e imagen a través de una nueva arquitectura que admite *vectores* (embeddings) de un solo vector y de múltiples vectores en el estilo de interacción tardía. El modelo incorpora adaptadores de Adaptación de Bajo Rango (LoRA) específicos para cada tarea para optimizar el rendimiento en diversos escenarios de recuperación, incluida la recuperación de información basada en consultas, la similitud semántica entre modalidades y la búsqueda de código de programación. Las evaluaciones integrales demuestran que jina-embeddings-v4 logra un rendimiento de vanguardia tanto en tareas de recuperación unimodales como intermodales, con una fuerza particular en el procesamiento de contenido visualmente rico, como tablas, gráficos, diagramas y formatos de medios mixtos. Para facilitar la evaluación de esta capacidad, también presentamos Jina-VDR, un nuevo benchmark diseñado específicamente para la recuperación de imágenes visualmente ricas.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Tanto <code>jina-embeddings-v4</code> como <code>jina-reranker-m0</code> comparten un cambio fundamental: el uso de *LLM* (LLM) como base en lugar de modelos de solo codificador. Esto no es una coincidencia, refleja una profunda ventaja que la mayoría pasa por alto: los modelos de solo codificador crean \"brechas de modalidad\" donde las imágenes se agrupan por separado del texto. Los modelos de solo decodificador abren posibilidades que no eran alcanzables con arquitecturas de solo codificador, incluida la verdadera representación de modalidad mixta y la explicabilidad.</p><p>Nuestra principal conclusión: los modelos de 向量模型 (embeddings) y la generación tratan sobre la comprensión de la semántica. Los 大模型 (LLM) que sobresalen en la generación, naturalmente, sobresalen en la representación. Creemos que el futuro reside en arquitecturas unificadas donde los modelos de 向量模型 (embedding) y el 重排器 (reranking) surgen del <strong>mismo modelo de base de búsqueda</strong>, y eso es exactamente hacia lo que Jina AI está construyendo.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina Embeddings v4 es un modelo de \"向量模型 (Embeddings)\" universal de 3.8 mil millones de parámetros para la recuperación multimodal y multilingüe que admite salidas de \"向量模型 (Embeddings)\" de un solo vector y de múltiples vectores.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
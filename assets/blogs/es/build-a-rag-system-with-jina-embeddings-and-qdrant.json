{
  "slug": "build-a-rag-system-with-jina-embeddings-and-qdrant",
  "id": "65ddadd039a5b70001810ea3",
  "uuid": "e3419a3e-4a7c-43c3-ac69-6f65f00a7ad8",
  "title": "Construye un sistema RAG con Jina Embeddings y Qdrant",
  "html": "<p>La generación aumentada por recuperación es un conjunto de técnicas en rápido crecimiento que fueron <a href=\"https://dl.acm.org/doi/abs/10.5555/3495724.3496517?ref=jina-ai-gmbh.ghost.io\"><u>originalmente concebidas como una extensión de los modelos de lenguaje grandes</u></a> (LLM), pero cada vez más se ven como un paradigma alternativo que reimagina completamente el papel de los modelos de IA.</p><p>Te mostraremos cómo crear un motor RAG básico usando el <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\">framework LlamaIndex</a>, un <a href=\"https://docs.mistral.ai/?ref=jina-ai-gmbh.ghost.io\">LLM Mistral</a> de código abierto, <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2</a> y la <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\">base de datos vectorial preparada para IA de Qdrant</a>.</p><h2 id=\"rag-in-context\"><strong>RAG en Contexto</strong></h2><p>En un sistema RAG, el papel del modelo de IA sigue siendo central pero mucho más limitado. En lugar de responder a las preguntas desde su conocimiento interno, derivado de sus datos de entrenamiento, se le presenta la información relevante en su prompt. El papel del modelo es entonces entender y sintetizar la información en una respuesta.</p><p>Esto aprovecha las fortalezas de los modelos de IA y minimiza sus debilidades.</p><p>Por ejemplo, podríamos darle al <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1?ref=jina-ai-gmbh.ghost.io\"><u>modelo Mistral-Instruct</u></a> el siguiente prompt:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: Why is Taylor Swift planning to attend the Superbowl?\nAnswer:\n</code></pre><p>Cuando hice eso, el modelo respondió:</p><pre><code class=\"language-text\">I don't have real-time information. However, as of my last update,&nbsp;\nTaylor Swift is not officially confirmed to attend or perform at&nbsp;\nthe Superbowl. If there are any updates, I recommend checking the&nbsp;\nlatest news sources.</code></pre><p>Los modelos de lenguaje grandes son costosos de actualizar y reentrenar, por lo que su información interna nunca puede estar completamente actualizada.</p><p>Además, los modelos de lenguaje grandes son conocidos por \"alucinar\", es decir, confabular respuestas plausibles que pueden ser parcial o totalmente incorrectas y que deben ser verificadas.</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\nQuery: What was Weird Al Yankovic's first Taylor Swift parody?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" a spoof&nbsp;\nof Swift's song \"Blank Space.\" The parody was released in 2014&nbsp;\nas part of Yankovic's album \"Mandatory Fun.\" The song humorously&nbsp;\naddresses the obsession with celebrity gossip and scandals, as&nbsp;\nreported by the tabloid news website TMZ.</code></pre><p>El <em>TMZ</em> de Weird Al Yankovic fue lanzado en 2011 en el álbum <em>Alpocalypse</em> y es una parodia de <a href=\"https://youtu.be/vwp8Ur6tO-8?ref=jina-ai-gmbh.ghost.io\"><em><u>You Belong with Me</u></em></a> de Swift.</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/BwqFgnl4rMs?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" title=\"TMZ (Parody of &quot;You Belong With Me&quot; by Taylor Swift) (Official 4K Video)\"></iframe></figure><p>La naturaleza de las redes neuronales hace que las alucinaciones sean difíciles, si no imposibles, de prevenir completamente y socava fuertemente cualquier plan de usar modelos de IA como fuentes de información.</p><p>En <a href=\"https://youtu.be/uaQZIK9gvNo?t=1130&ref=jina-ai-gmbh.ghost.io\"><u>palabras de Sam Altman</u></a>, CEO de OpenAI:</p><blockquote class=\"kg-blockquote-alt\">\"Creo que la gente comete un gran error cuando piensa en estos modelos [LLM] como una base de datos. [...] Funcionará, más o menos. Pero es como la base de datos más cara, más lenta y menos precisa del mundo.\"</blockquote><p>RAG minimiza la dependencia del conocimiento interno de hechos sobre el mundo de un modelo de IA y aprovecha sus mayores fortalezas: la comprensión del lenguaje natural y la generación fluida de lenguaje.</p><p>Veamos otro ejemplo de una pregunta que Mistral-Instruct no puede responder:</p><pre><code class=\"language-text\">Answer the query. Please be brief, concise, and complete.\n\nQuery: What is the title of Taylor Swift's next album?\nAnswer:\n\nAs of the time I am providing this answer, Taylor Swift has&nbsp;\nnot yet announced the title of her next album. She has released&nbsp;\nseveral re-recorded albums, but the title of her next original&nbsp;\nalbum remains unknown. I recommend checking her official social&nbsp;\nmedia channels or website for the latest updates.</code></pre><p>Ahora, un sistema RAG podría tener una base de datos de artículos de noticias recientes, o tal vez buscar en Internet textos relacionados con el título del próximo álbum de Taylor Swift. Luego, insertaría los textos que encuentra en el prompt.</p><p>Supongamos que identifica <a href=\"https://apnews.com/article/taylor-swift-new-album-5e219107982b1498a3656caeea85e0eb?ref=jina-ai-gmbh.ghost.io\"><u>este artículo de AP</u></a> como relevante para la consulta y construye el siguiente prompt:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nFebruary 6, 2024\nLOS ANGELES (AP) — Forget her \"Reputation.\" Taylor Swift has a&nbsp;\nwhole new album coming out.\n\nAccepting the Grammy for best pop vocal album on Sunday night,&nbsp;\nTaylor said she'd been keeping a secret for two years.\n\n\"My brand-new album comes out April 19. It's called 'The Tortured&nbsp;\nPoets Department.' I'm going to go and post the cover right now&nbsp;\nbackstage,\" she announced.\n\nAnd so she did.\n\n---------------------\nGiven the context information and not prior knowledge, answer the query.&nbsp;\nPlease be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What is the title of Taylor Swift's next album?\nAnswer:</code></pre><p>Mistral-Instruct respondió:</p><pre><code class=\"language-text\">The title of Taylor Swift's next album is 'The Tortured Poets Department'.</code></pre><p>Este tipo de prompting también reduce drásticamente la probabilidad de alucinaciones. Por ejemplo:</p><pre><code class=\"language-text\">Context information is below.\n---------------------\nAlpocalypse is the thirteenth studio album by American parody&nbsp;\nmusician \"Weird Al\" Yankovic, released on June 21, 2011.\n\nTrack List\n\n1. Perform This Way (Parody of \"Born This Way\" by Lady Gaga)\n2. CNR\n3. TMZ (Parody of \"You Belong with Me\" by Taylor Swift)\n4. Skipper Dan\n5. Polka Face (Polka medley including songs by artists such as&nbsp;\n&nbsp;&nbsp;&nbsp;Lady Gaga, Justin Bieber, Britney Spears, Ke$ha, and others)\n6. Craigslist\n7. Party in the CIA (Parody of \"Party in the U.S.A.\" by Miley&nbsp;\n&nbsp;&nbsp;&nbsp;Cyrus)\n8. Ringtone\n9. Another Tattoo (Parody of \"Nothin' on You\" by B.o.B featuring&nbsp;\n&nbsp;&nbsp;&nbsp;Bruno Mars)\n10. If That Isn't Love\n11. Whatever You Like (Parody of \"Whatever You Like\" by T.I.)\n12. Stop Forwarding That Crap to Me\n\n---------------------\nGiven the context information and not prior knowledge, answer the&nbsp;\nquery. Please be brief, concise, and complete.\nIf the context information does not contain an answer to the query,&nbsp;\nrespond with \"No information\".\nQuery: What was Weird Al Yankovic's first Taylor Swift parody, what&nbsp;\nyear was it released, and what song was he parodying?\nAnswer:\n\nWeird Al Yankovic's first Taylor Swift parody was \"TMZ,\" and it was&nbsp;\nreleased in 2011. He was parodying \"You Belong with Me\" by Taylor&nbsp;\nSwift.\n</code></pre><h2 id=\"anatomy-of-a-rag-system\"><strong>Anatomía de un sistema RAG</strong></h2><p>Un sistema RAG tiene, como mínimo, dos componentes:</p><ul><li>Un modelo de IA generativo.</li><li>Un sistema de recuperación de información.</li></ul><p>En este artículo, usaremos <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2?ref=jina-ai-gmbh.ghost.io\">Mistral-Instruct</a> como el modelo de IA generativo y accederemos a él a través de la <a href=\"https://huggingface.co/docs/api-inference/index?ref=jina-ai-gmbh.ghost.io\">API de Inferencia de Hugging Face</a>. Usaremos la <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\">base de datos vectorial Qdrant</a> y <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings</a> juntos para el sistema de recuperación de información.</p><p>Como fuente de datos, usaremos el <a href=\"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf?ref=jina-ai-gmbh.ghost.io\"><em><u>Plan Estratégico Nacional de Investigación y Desarrollo de Inteligencia Artificial Actualización 2023</u></em></a>, publicado en mayo de 2023 por el <a href=\"https://www.whitehouse.gov/ostp/ostps-teams/nstc/?ref=jina-ai-gmbh.ghost.io\">Consejo Nacional de Ciencia y Tecnología</a>, un organismo que asesora al Presidente de los Estados Unidos en asuntos científicos y técnicos. Este documento de aproximadamente 25,000 palabras es un documento de política y planificación estratégica para la política y el desarrollo de IA estadounidense.</p><p>También es un clásico documento de política gubernamental \"muy largo; no lo leí\". La mayor parte es muy aburrida, y a menos que tengas un interés o una participación cercana en esta área, probablemente no lo vas a leer. Pero aún podrías querer saber qué dice.</p><p>Este artículo te guiará a través de los pasos para crear un sistema RAG capaz de responder preguntas sobre el texto y su contenido.</p><h2 id=\"install-the-prerequisites\"><strong>Instalar los Prerrequisitos</strong></h2><p>Primero, asegúrate de tener todas las bibliotecas de Python relevantes instaladas. En tu entorno de Python, ejecuta lo siguiente:</p><pre><code class=\"language-shell\">pip install llama-index qdrant-client \npip install pdfminer.six llama-index-vector-stores-qdrant\npip install llama-index-llms-openai llama-index-embeddings-jinaai \npip install llama-index-llms-huggingface \"huggingface_hub[inference]\"</code></pre><p>Usaremos LlamaIndex para construir un sistema RAG alrededor del modelo de lenguaje Mistral-Instruct y <a href=\"https://pypi.org/project/pdfminer.six/?ref=jina-ai-gmbh.ghost.io\"><u>pdfminer.six</u></a> para procesar el archivo PDF en un almacén vectorial Qdrant.</p><h2 id=\"connect-to-mistral-instruct-via-the-hugging-face-inference-api\"><strong>Conectarse a Mistral-Instruct a través de la API de inferencia de Hugging Face</strong></h2><p>Primero, configuremos el LLM. Necesitarás acceso a la API de inferencia de Hugging Face, incluyendo un token de acceso. Si tienes una cuenta de Hugging Face, puedes obtenerlo desde <a href=\"https://huggingface.co/settings/tokens.?ref=jina-ai-gmbh.ghost.io\">tu página de configuración de cuenta</a>.</p><p>Si no tienes una cuenta, primero <a href=\"https://huggingface.co/join?ref=jina-ai-gmbh.ghost.io\">crea una</a>, luego genera un <a href=\"https://huggingface.co/settings/tokens?ref=jina-ai-gmbh.ghost.io\">token de acceso</a>.</p><p>Coloca tu token en una variable para uso futuro:</p><pre><code class=\"language-python\">hf_inference_api_key = '&lt;your HuggingFace Inference API token&gt;'</code></pre><p>A continuación, construimos una plantilla de prompt:</p><pre><code class=\"language-python\">from llama_index.core import PromptTemplate\n\nqa_prompt_tmpl = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query. Please be brief, concise, and complete.\\n\"\n    \"If the context information does not contain an answer to the query, \"\n    \"respond with \\\"No information\\\".\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)</code></pre><p>Esta plantilla instruye al LLM a usar solo la información de contexto proporcionada en el prompt para responder preguntas. Este prompt ha funcionado bien para nosotros al trabajar con Mistral-Instruct, pero puedes experimentar con otras redacciones para ver qué funciona en tu caso de uso.</p><pre><code class=\"language-python\">from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n\nmixtral_llm = HuggingFaceInferenceAPI(\n    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n    token=hf_inference_api_key\n)</code></pre><p>Finalmente, crea e inicializa un objeto para el framework LlamaIndex que mantiene la conexión con Mistral-Instruct. El código siguiente proviene de la <a href=\"https://jina.ai/news/full-stack-rag-with-jina-embeddings-v2-and-llamaindex/?ref=jina-ai-gmbh.ghost.io\"><u>publicación anterior sobre crear sistemas RAG usando LlamaIndex</u></a>.</p><p>Esto coloca la conexión al LLM Mistral en la variable <code>mistral_llm</code>.</p><h2 id=\"information-retrieval-with-qdrant-and-jina-embeddings\"><strong>Recuperación de información con Qdrant y Jina Embeddings</strong></h2><p>Para configurar el sistema de recuperación, necesitarás una clave API de Jina Embeddings. Puedes obtener una gratis con un presupuesto prepagado de 1 millón de tokens en el <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">sitio web de Jina Embeddings</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Embedding API</div><div class=\"kg-bookmark-description\">Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-embedding-api.png\" alt=\"\"></div></a></figure><p>Coloca tu clave de Jina Embeddings en una variable para uso futuro:</p><pre><code class=\"language-python\">jina_emb_api_key = \"&lt;your Jina Embeddings API key&gt;\"</code></pre><p>Luego, crea un objeto conector usando LlamaIndex para el servidor de Jina Embeddings, seleccionando específicamente el modelo monolingüe en inglés:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_emb_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n)</code></pre><h3 id=\"load-text-data\"><strong>Cargar datos de texto</strong></h3><p>A continuación, cargaremos el documento y lo dividiremos en párrafos. Primero, descarga el PDF del sitio web de la Casa Blanca en la variable <code>pdf_data</code>:</p><pre><code class=\"language-python\">import urllib.request\n\nuri = \"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"\npdf_data = urllib.request.urlopen(uri).read()</code></pre><p>Luego, procesaremos el PDF página por página, extrayendo el texto y dividiéndolo en párrafos separando por dobles saltos de línea. Estos se almacenan en la lista <code>text_paras</code>:</p><pre><code class=\"language-python\">import regex as re\nfrom io import BytesIO, StringIO\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.pdfparser import PDFParser\n\ntext_paras = []\nparser = PDFParser(BytesIO(pdf_data))\ndoc = PDFDocument(parser)\nrsrcmgr = PDFResourceManager()\nfor page in PDFPage.create_pages(doc):\n    output_string = StringIO()\n    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    interpreter.process_page(page)\n    page_text = output_string.getvalue()\n    text_paras.extend(re.split(r'\\n\\s*\\n', page_text))\n</code></pre><p>Verifica que todo esté cargado:</p><pre><code class=\"language-python\">assert len(text_paras) == 615</code></pre><p>A continuación, convertiremos esta lista de textos cortos en <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html?ref=jina-ai-gmbh.ghost.io\">objetos Document de LlamaIndex</a>:</p><pre><code class=\"language-python\">from llama_index.core.readers import StringIterableReader\n\nrag_docs = StringIterableReader().load_data(text_paras)</code></pre><p>Y puedes inspeccionar el texto:</p><pre><code class=\"language-python\">print(rag_docs[0].text)</code></pre><p>Resultado:</p><pre><code class=\"language-text\">NATIONAL ARTIFICIAL INTELLIGENCE \nRESEARCH AND DEVELOPMENT \nSTRATEGIC PLAN \n2023 UPDATE</code></pre><h3 id=\"set-up-a-qdrant-vector-database\"><strong>Configurar una base de datos vectorial Qdrant</strong></h3><p>En este artículo, usaremos <a href=\"https://cloud.qdrant.io/?ref=jina-ai-gmbh.ghost.io\"><u>Qdrant Vector Search Cloud</u></a> para implementar la base de datos del sistema RAG. El nivel gratuito de Qdrant incluye 1GB de almacenamiento, que es más que suficiente para este tutorial.</p><p>Necesitarás <a href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><u>crear una cuenta en el sitio web de Qdrant Cloud</u></a> antes de continuar.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://cloud.qdrant.io/login?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Vector Search Database | Qdrant Cloud</div><div class=\"kg-bookmark-description\">Managed cloud solution of the Qdrant vector search engine. Cloud-native vector database for high performant vector similarity search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cloud.qdrant.io/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Qdrant Cloud</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cloud.qdrant.io/social.png\" alt=\"\"></div></a></figure><p>Una vez que tengas una cuenta y hayas iniciado sesión, necesitarás crear un cluster. Sigue las <a href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>instrucciones de \"inicio rápido\" en el sitio web de Qdrant</u></a> para configurar un cluster gratuito y obtener una API y el nombre del servidor host de Qdrant.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/cloud/quickstart-cloud/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Quickstart - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Edit on GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/documentation/cloud/quickstart-cloud-social-preview.png\" alt=\"\"></div></a></figure><p>Almacena la clave y el nombre del host en variables:</p><pre><code class=\"language-python\">qdrant_api_key = \"&lt;your API key&gt;\"\nqdrant_server = \"https://&lt;your server&gt;\"</code></pre><p>A continuación, necesitaremos importar los componentes relevantes de los paquetes <code>qdrant_client</code> y <code>llama_index</code>:</p><pre><code class=\"language-python\">import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nclient = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\nvector_store = QdrantVectorStore(client=client, collection_name=\"NTSC\")</code></pre><p>Esto crea una colección llamada <code>NTSC</code> en tu clúster gratuito.</p><p>Puedes leer la <a href=\"https://qdrant.tech/documentation/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>documentación de Qdrant</u></a> para obtener información sobre cómo construir un almacén de datos local en memoria o en disco y hospedar tu propia instancia del servidor Qdrant en un contenedor docker a través de una API web.</p><h3 id=\"complete-the-rag-system\"><strong>Completar el sistema RAG</strong></h3><p>Ahora ensamblaremos estos componentes en un sistema RAG completo utilizando código base para LlamaIndex:</p><pre><code class=\"language-python\">from llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core import (\n\t\tVectorStoreIndex,\n\t\tServiceContext,\n\t\tget_response_synthesizer,\n)\n\n# set up the service and storage contexts\nservice_context = ServiceContext.from_defaults(\n&nbsp;&nbsp;&nbsp;&nbsp;llm=mixtral_llm, embed_model=jina_embedding_model\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# create an index\nindex = VectorStoreIndex.from_documents(\n&nbsp;&nbsp;&nbsp;&nbsp;rag_docs, storage_context=storage_context, service_context=service_context\n)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n&nbsp;&nbsp;&nbsp;&nbsp;service_context=service_context,\n&nbsp;&nbsp;&nbsp;&nbsp;text_qa_template=qa_prompt,\n&nbsp;&nbsp;&nbsp;&nbsp;response_mode=\"compact\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n&nbsp;&nbsp;&nbsp;&nbsp;retriever=retriever,\n&nbsp;&nbsp;&nbsp;&nbsp;response_synthesizer=response_synthesizer,\n)\n</code></pre><p>Este sistema utilizará Jina Embeddings para indexar los párrafos de nuestra fuente de texto y colocará las dos mejores coincidencias en el contexto de cada prompt. Para cambiar a las tres mejores, por ejemplo, solo modifica la inicialización de <code>VectorIndexRetriever</code> anterior para tener un parámetro <code>similarity_top_k</code> diferente:</p><pre><code class=\"language-python\"># configure retriever\nretriever = VectorIndexRetriever(\n&nbsp;&nbsp;&nbsp;&nbsp;index=index,\n&nbsp;&nbsp;&nbsp;&nbsp;similarity_top_k=3,\n)</code></pre><p>Ahora el sistema está listo para usar.</p><h3 id=\"query-a-document\"><strong>Consultar un Documento</strong></h3><p>Probemos una consulta sencilla:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat is the Biden Administration's policy with regard to AI?&nbsp;\n\"\"\")\nprint(response.response)</code></pre><p>El resultado (el tuyo puede no ser idéntico):</p><pre><code class=\"language-text\">The Biden Administration prioritizes responsible AI systems that&nbsp;\nare ethical, trustworthy, and safe, and serve the public good. \nThe Administration has committed to substantial funding for AI \nR&amp;D, and has established the National AI Initiative Office to \ncoordinate AI activities across the federal government.&nbsp;\n\nThe Administration's AI policy focuses on developing evaluative\ntechniques for AI, workforce development, public-private\npartnerships, and international collaboration in AI research.</code></pre><p>O algo más específico:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat protections does the AI Bill of Rights propose to offer?\n\"\"\")\n\nprint(response.response)</code></pre><pre><code class=\"language-text\">The AI Bill of Rights proposes five core protections: Safe and&nbsp;\nEffective Systems; Algorithmic Discrimination Protections; Data&nbsp;\nPrivacy; Notice and Explanation; and Human Alternatives,&nbsp;\nConsideration, and Fallback.</code></pre><p>O incluso muy específico:</p><pre><code class=\"language-python\">response = query_engine.query(\"Who is Kei Koizumi?\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">Kei Koizumi is the Principal Deputy Director for Policy at the Office&nbsp;\nof Science and Technology Policy (OSTP).</code></pre><p>También puedes hacer preguntas más imaginativas:</p><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhat rights will AI's receive under President Biden's proposed&nbsp;\nAI Bill of Rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">The proposed AI Bill of Rights does not grant rights to AI systems, but&nbsp;\nrather outlines principles to ensure that AI technologies are&nbsp;\ndeveloped and used in a manner that respects human rights and values.&nbsp;\nThe principles include safety, transparency, fairness, disclosure,&nbsp;\ndata privacy, non-discrimination, and the ability for individuals to&nbsp;\nopt out of certain AI systems.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nWhy is President Biden's proposing an AI Bill of Rights?\nDoes AI really need rights?\n\"\"\")\nprint(response.response)</code></pre><pre><code class=\"language-text\">President Biden's proposed AI Bill of Rights aims to ensure that AI&nbsp;\ntechnologies are developed and used ethically, legally, and with&nbsp;\nsocietal considerations in mind. The Bill of Rights is not granting&nbsp;\n\"rights\" to AI, but rather setting guidelines for the responsible&nbsp;\ndevelopment and deployment of AI systems to protect individuals and&nbsp;\ncommunities from potential negative impacts.</code></pre><pre><code class=\"language-python\">response = query_engine.query(\"\"\"\nHas Donald Trump weighed in on AI?\nWill he Make Humans Great Again?\n\"\"\")\nprint(response.response)\n</code></pre><pre><code class=\"language-text\">No information. The context information does not mention Donald&nbsp;\nTrump's views on AI.</code></pre><h2 id=\"jina-ai-and-qdrant\"><strong>Jina AI y Qdrant</strong></h2><p>La ventana de contexto de 8k tokens de Jina Embeddings lo hace especialmente adecuado para la generación aumentada por recuperación porque puede trabajar con bloques de texto mucho más grandes. Mientras que otras aplicaciones RAG a menudo tienen que almacenar oraciones individuales o pares de oraciones, en este tutorial, pudimos usar párrafos completos sin preocuparnos por su tamaño.</p><p>Esto reduce el número de elementos de texto que deben almacenarse para llevar la misma información, reduciendo los costos de computación, ahorrando memoria y haciendo la recuperación más rápida.</p><p>La base de datos vectorial de Qdrant es simple de configurar, rápida y rentable, y como has visto, solo se necesitan unas pocas líneas de Python para integrarla en un sistema RAG.</p><p>Para más información sobre la base de datos preparada para IA y las ofertas en la nube de Qdrant, puedes <a href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\" rel=\"noreferrer\"><u>visitar su sitio web</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/?utm_source=jinaai&utm_medium=partner&utm_campaign=qdrant-jinaai-rag-blog\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qdrant - Vector Database</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://qdrant.tech/images/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">Vector Database</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://qdrant.tech/images/previews/social-preview-P.png\" alt=\"\"></div></a></figure><h2 id=\"get-in-touch\"><strong>Contacta con Nosotros</strong></h2><p>Jina AI está comprometida a llevar tecnologías de IA confiables y asequibles a empresas de todos los tamaños y tipos. Nos encantaría conocer tus casos de uso y ayudarte a integrar la IA en tus procesos de negocio. Para más información sobre las ofertas de Jina AI y para contactarnos, visita el <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>sitio web de Jina AI</u></a> o únete a nuestra <a href=\"https://discord.jina.ai/?ref=jina-ai-gmbh.ghost.io\"><u>comunidad en Discord</u></a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Best Embeddings and Perfect Prompts</div><div class=\"kg-bookmark-description\">Jina AI provides best-in-class embedding API and prompt optimizer, easing the development of multimodal AI applications.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-author\">Best Embeddings and Perfect Prompts</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://discord.com/invite/AWXCCC6G2P?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Join the Jina AI Discord Server!</div><div class=\"kg-bookmark-description\">Check out the Jina AI community on Discord - hang out with 4493 other members and enjoy free voice and text chat.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://discord.com/assets/images/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Discord</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn.discordapp.com/splashes/1106542220112302130/80f2c2128aefeb55209a5bdb2130bb92.jpg?size=512\" alt=\"\"></div></a></figure>",
  "comment_id": "65ddadd039a5b70001810ea3",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Blog-images--26-.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-02-27T10:39:28.000+01:00",
  "updated_at": "2024-03-06T15:41:18.000+01:00",
  "published_at": "2024-03-04T16:00:13.000+01:00",
  "custom_excerpt": "Create a RAG system with Jina Embeddings v2, Qdrant vector database, LlamaIndex, and Mistral LLM.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ae7353e4e55003d52598e",
    "name": "Scott Martens",
    "slug": "scott",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
    "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
    "website": "https://jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/build-a-rag-system-with-jina-embeddings-and-qdrant/",
  "excerpt": "Crea un sistema RAG con Jina Embeddings v2, base de datos vectorial Qdrant, LlamaIndex y Mistral LLM.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Promotional graphic with \"EMBEDDINGS 2\" and Drant logo separated by a red cross, symbolizing a partnership.",
  "feature_image_caption": null
}
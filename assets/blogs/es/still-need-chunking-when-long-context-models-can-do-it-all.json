{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "¿Aún Se Necesita el Chunking Cuando los Modelos de Contexto Largo Pueden Hacerlo Todo?",
  "html": "<p>En octubre de 2023, presentamos <code>jina-embeddings-v2</code>, la primera familia de modelos de embeddings de código abierto capaz de manejar entradas de hasta 8.192 tokens. Sobre esta base, este año lanzamos <code>jina-embeddings-v3</code>, ofreciendo el mismo amplio soporte de entrada con mejoras adicionales.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>En esta publicación profundizaremos en los embeddings de contexto largo y responderemos algunas preguntas: ¿Cuándo es práctico consolidar un volumen tan grande de texto en un solo vector? ¿La segmentación mejora la recuperación y, si es así, ¿cómo? ¿Cómo podemos preservar el contexto de diferentes partes de un documento mientras segmentamos el texto?</p><p>Para responder estas preguntas, compararemos varios métodos para generar embeddings:</p><ul><li>Embedding de contexto largo (codificación de hasta 8.192 tokens en un documento) vs contexto corto (es decir, truncando a 192 tokens).</li><li>Sin fragmentación vs. fragmentación simple vs. <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">fragmentación tardía</a>.</li><li>Diferentes tamaños de fragmentos con fragmentación tanto simple como tardía.</li></ul><h2 id=\"is-long-context-even-useful\">¿Es Útil el Contexto Largo?</h2><p>Con la capacidad de codificar hasta diez páginas de texto en un solo embedding, los modelos de embedding de contexto largo abren posibilidades para la representación de texto a gran escala. Sin embargo, ¿es realmente útil? Según mucha gente... no.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Fuentes: </span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Cita de Nils Reimer en el podcast How AI Is Built</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">tweet de brainlag</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">comentario de egorfine en Hacker News</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">comentario de andy99 en Hacker News</span></a></p></figcaption></figure><p>Vamos a abordar todas estas preocupaciones con una investigación detallada de las capacidades de contexto largo, cuándo el contexto largo es útil y cuándo deberías (y no deberías) usarlo. Pero primero, escuchemos a estos escépticos y veamos algunos de los problemas que enfrentan los modelos de embedding de contexto largo.</p><h2 id=\"problems-with-long-context-embeddings\">Problemas con los Embeddings de Contexto Largo</h2><p>Imaginemos que estamos construyendo un sistema de búsqueda de documentos para artículos, como los de nuestro <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">blog de Jina AI</a>. A veces un solo artículo puede cubrir múltiples temas, como el <a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">informe sobre nuestra visita a la conferencia ICML 2024</a>, que contiene:</p><ul><li>Una introducción, capturando información general sobre ICML (número de participantes, ubicación, alcance, etc).</li><li>La presentación de nuestro trabajo (<code>jina-clip-v1</code>).</li><li>Resúmenes de otros trabajos de investigación interesantes presentados en ICML.</li></ul><p>Si creamos un solo embedding para este artículo, ese embedding representa una mezcla de tres temas dispares:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Al generar el embedding de un documento que cubre múltiples temas, el vector resultante representa una mezcla de todos los párrafos, potencialmente perdiendo la información distinta y específica contenida en cada párrafo individual.</span></figcaption></figure><p>Esto lleva a varios problemas:</p><ul><li><strong>Dilución de la Representación:</strong> Si bien todos los temas en un texto dado <em>podrían</em> estar relacionados, solo uno puede ser relevante para la consulta de búsqueda del usuario. Sin embargo, un solo embedding (en este caso, el del post completo) es solo un punto en el espacio vectorial. A medida que se agrega más texto a la entrada del modelo, el embedding se desplaza para capturar el tema general del artículo, haciéndolo menos efectivo en representar el contenido cubierto en párrafos específicos.</li><li><strong>Capacidad Limitada:</strong> Los modelos de embedding producen vectores de tamaño fijo, independientemente de la longitud de entrada. A medida que se agrega más contenido a la entrada, se vuelve más difícil para el modelo representar toda esta información en el vector. Piensa en ello como reducir una imagen a 16×16 píxeles — Si reduces una imagen de algo simple, como una manzana, aún puedes derivar significado de la imagen escalada. ¿Reducir un mapa callejero de Berlín? No tanto.</li><li><strong>Pérdida de Información:</strong> En algunos casos, incluso los modelos de embedding de contexto largo alcanzan sus límites; Muchos modelos admiten codificación de texto de hasta 8.192 tokens. Los documentos más largos necesitan ser truncados antes del embedding, lo que lleva a pérdida de información. Si la información relevante para el usuario se encuentra al final del documento, no será capturada por el embedding en absoluto.</li><li><strong>Podrías <em>Necesitar</em> Segmentación de Texto:</strong> Algunas aplicaciones requieren embeddings para segmentos específicos del texto pero no para todo el documento, como identificar el pasaje relevante en un texto.</li></ul><h2 id=\"long-context-vs-truncation\">Contexto Largo vs. Truncamiento</h2><p>Para ver si el contexto largo vale la pena, veamos el rendimiento de dos escenarios de recuperación:</p><ul><li>Codificación de documentos de hasta 8.192 tokens (aproximadamente 10 páginas de texto).</li><li>Truncamiento de documentos a 192 tokens y codificación hasta ahí.</li></ul><p>Compararemos resultados usando<code>jina-embeddings-v3</code> con la métrica de recuperación nDCG@10. Probamos los siguientes conjuntos de datos:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Descripción</th>\n<th>Ejemplo de Consulta</th>\n<th>Ejemplo de Documento</th>\n<th>Longitud Media del Documento (caracteres)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>Un conjunto de datos de recuperación médica de texto completo con 3,244 consultas y documentos principalmente de PubMed.</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326,753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>Un conjunto de datos de resúmenes de reuniones basado en consultas que requiere resumir segmentos relevantes de reuniones.</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37,445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>Conjunto de datos de preguntas y respuestas con historias largas y preguntas correspondientes sobre contenido específico.</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53,336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>Un conjunto de datos de preguntas y respuestas multi-paso con hasta 5 pasos de razonamiento, diseñado con plantillas para evitar atajos.</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30,854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>Un conjunto de datos de resúmenes de guiones con transcripciones y resúmenes de series de TV que requieren integración dispersa de la trama.</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1,613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Como podemos ver, codificar más de 192 tokens puede dar mejoras notables de rendimiento:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Comparación del Rendimiento de Embedding de Contexto Largo y Embedding de Texto Corto</span></figcaption></figure><p>Sin embargo, en algunos conjuntos de datos, vemos mejoras más grandes que en otros:</p><ul><li>Para <strong>NFCorpus</strong>, el truncamiento apenas hace diferencia. Esto se debe a que los títulos y resúmenes están justo al comienzo de los documentos, y estos son altamente relevantes para los términos típicos de búsqueda del usuario. Ya sea truncado o no, los datos más pertinentes permanecen dentro del límite de tokens.</li><li><strong>QMSum</strong> y <strong>NarrativeQA</strong> son consideradas tareas de \"comprensión lectora\", donde los usuarios típicamente buscan hechos específicos dentro de un texto. Estos hechos a menudo están dispersos en detalles a lo largo del documento y pueden caer fuera del límite truncado de 192 tokens. Por ejemplo, en el documento NarrativeQA <em>Percival Keene</em>, la pregunta \"¿Quién es el matón que roba el almuerzo de Percival?\" se responde mucho más allá de este límite. De manera similar, en <strong>2WikiMultiHopQA</strong>, la información relevante está dispersa a lo largo de documentos completos, requiriendo que los modelos naveguen y sinteticen conocimiento de múltiples secciones para responder consultas efectivamente.</li><li><strong>SummScreenFD</strong> es una tarea dirigida a identificar el guion correspondiente a un resumen dado. Debido a que el resumen abarca información distribuida a lo largo del guion, codificar más texto mejora la precisión de hacer coincidir el resumen con el guion correcto.</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">Segmentando Texto para un Mejor Rendimiento de Recuperación</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Avanzando, discutimos tres conceptos similares. Para evitar confusiones, nos referimos a ellos como sigue:<br>• <b><strong style=\"white-space: pre-wrap;\">Segmentación</strong></b>: Detectar señales de límite en un texto de entrada, por ejemplo, oraciones o un número fijo de tokens.<br>• <b><strong style=\"white-space: pre-wrap;\">Fragmentación ingenua</strong></b>: Dividir el texto en fragmentos basados en señales de segmentación, antes de codificarlo.<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Fragmentación tardía</strong></b></a>: Codificar el documento primero y luego segmentarlo (preservando el contexto entre fragmentos).</div></div><p>En lugar de incrustar un documento completo en un vector, podemos usar varios métodos para primero segmentar el documento asignando señales de límite:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Aplicando los métodos de fragmentación \"Tamaño Fijo\", \"Basado en Oraciones\" y \"Semántico\" a un pasaje de texto</span></figcaption></figure><p>Algunos métodos comunes incluyen:</p><ul><li><strong>Segmentación por tamaño fijo:</strong> El documento se divide en segmentos de un número fijo de tokens, determinado por el tokenizador del modelo de embedding. Esto asegura que la tokenización de los segmentos corresponda a la tokenización del documento completo (segmentar por un número específico de caracteres podría llevar a una tokenización diferente).</li><li><strong>Segmentación por oración:</strong> El documento se segmenta en oraciones, y cada fragmento consiste en <em>n</em> número de oraciones.</li><li><strong>Segmentación por semántica:</strong> Cada segmento corresponde a múltiples oraciones y un modelo de embedding determina la similitud de oraciones consecutivas. Las oraciones con altas similitudes de embedding se asignan al mismo fragmento.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Puedes realizar fácilmente la segmentación con <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a>, nuestra API gratuita para segmentar texto largo en fragmentos y tokenización basada en la estructura del documento.</div></div><p>Por simplicidad, usamos segmentación de tamaño fijo en este artículo.</p><h3 id=\"document-retrieval-using-naive-chunking\">Recuperación de Documentos Usando Fragmentación Ingenua</h3><p>Una vez que hemos realizado la segmentación de tamaño fijo, podemos fragmentar ingenuamente el documento de acuerdo con esos segmentos:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Fragmentación ingenua basada en señales de límite detectadas durante la segmentación.</span></figcaption></figure><p>Usando <code>jina-embeddings-v3</code>, codificamos cada fragmento en un embedding que captura con precisión su semántica, luego almacenamos esos embeddings en una base de datos vectorial.</p><p>En tiempo de ejecución, el modelo codifica la consulta del usuario en un vector de consulta. Comparamos esto contra nuestra base de datos vectorial de embeddings de fragmentos para encontrar el fragmento con la mayor similitud del coseno, y luego devolvemos el documento correspondiente al usuario:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Recuperación de documentos implementada con segmentación ingenua: (1) Los documentos en la colección se dividen en fragmentos basados en señales de límites, (2) el modelo de embedding codifica todos los fragmentos y almacenamos los embeddings resultantes en una base de datos, (3) cuando llega una consulta, el modelo de embedding la codifica y la base de datos determina el fragmento más similar. Al final, identificamos el documento relevante a partir del ID del documento almacenado para el fragmento en la base de datos y lo devolvemos al usuario.</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">Problemas con la Segmentación Ingenua</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: Cuando se segmenta un texto en oraciones, las referencias a partes anteriores del texto no pueden resolverse.</span></figcaption></figure><p>Si bien la segmentación ingenua aborda algunas de las limitaciones de los modelos de embedding de contexto largo, también tiene sus desventajas:</p><ul><li><strong>Pérdida de la Visión General:</strong> En cuanto a la recuperación de documentos, múltiples embeddings de fragmentos más pequeños pueden fallar en capturar el tema general del documento. Es como no ver el bosque por los árboles.</li><li><strong>Problema de Contexto Faltante:</strong> Los fragmentos no pueden interpretarse con precisión ya que falta información de contexto, como se ilustra en la Figura 6.</li><li><strong>Eficiencia:</strong> Más fragmentos requieren más almacenamiento y aumentan el tiempo de recuperación.</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">La Segmentación Tardía Resuelve el Problema del Contexto</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Para resolver el problema del contexto faltante, introdujimos un nuevo método llamado \"segmentación tardía\", descrito en nuestros posts anteriores del blog: <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a>, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a>, <a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte III</strong></b></a>, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper de investigación</strong></b></a>.</div></div><p>La segmentación tardía funciona en dos pasos principales:</p><ol><li>Primero, utiliza las capacidades de contexto largo del modelo para codificar el documento completo en embeddings de tokens. Esto preserva el contexto completo del documento.</li><li>Luego, crea embeddings de fragmentos aplicando mean pooling a secuencias específicas de embeddings de tokens, correspondientes a las señales de límites identificadas durante la segmentación.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Segmentación tardía vs segmentación ingenua.</span></figcaption></figure><p>La ventaja clave de este enfoque es que los embeddings de tokens están contextualizados - lo que significa que naturalmente capturan referencias y relaciones con otras partes del documento. Dado que el proceso de embedding ocurre antes de la segmentación, cada fragmento mantiene la conciencia del contexto más amplio del documento, resolviendo el problema del contexto faltante que afecta a los enfoques de segmentación ingenua.</p><p>Para documentos que exceden el tamaño máximo de entrada del modelo, podemos usar \"segmentación tardía larga\":</p><ol><li>Primero, dividimos el documento en \"macro-fragmentos\" superpuestos. Cada macro-fragmento tiene un tamaño que cabe dentro de la longitud máxima de contexto del modelo (por ejemplo, 8.192 tokens).</li><li>El modelo procesa estos macro-fragmentos para crear embeddings de tokens.</li><li>Una vez que tenemos los embeddings de tokens, procedemos con la segmentación tardía estándar - aplicando mean pooling para crear los embeddings finales de los fragmentos.</li></ol><p>Este enfoque nos permite manejar documentos de cualquier longitud mientras se preservan los beneficios de la segmentación tardía. Piensa en ello como un proceso de dos etapas: primero hacer el documento digerible para el modelo, luego aplicar el procedimiento regular de segmentación tardía.</p><p>En resumen:</p><ul><li><strong>Segmentación ingenua:</strong> Segmentar el documento en fragmentos pequeños, luego codificar cada fragmento por separado.</li><li><strong>Segmentación tardía:</strong> Codificar el documento completo de una vez para crear embeddings de tokens, luego crear embeddings de fragmentos mediante pooling de los embeddings de tokens basados en los límites de segmentos.</li><li><strong>Segmentación tardía larga:</strong> Dividir documentos grandes en macro-fragmentos superpuestos que se ajusten a la ventana de contexto del modelo, codificarlos para obtener embeddings de tokens, luego aplicar la segmentación tardía normalmente.</li></ul><p>Para una descripción más extensa de la idea, consulta nuestro <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">paper</a> o los posts del blog mencionados anteriormente.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">¿Segmentar o No Segmentar?</h2><p>Ya hemos visto que el embedding de contexto largo generalmente supera a los embeddings de texto más cortos, y hemos dado una visión general de las estrategias de segmentación ingenua y tardía. La pregunta ahora es: ¿Es la segmentación mejor que el embedding de contexto largo?</p><p>Para realizar una comparación justa, truncamos los valores de texto a la longitud máxima de secuencia del modelo (8.192 tokens) antes de comenzar a segmentarlos. Usamos segmentación de tamaño fijo con 64 tokens por segmento (tanto para segmentación ingenua como para segmentación tardía). Comparemos tres escenarios:</p><ul><li><strong>Sin segmentación:</strong> Codificamos cada texto en un solo embedding. Esto lleva a los mismos puntajes que el experimento anterior (ver Figura 2), pero los incluimos aquí para compararlos mejor.</li><li><strong>Segmentación ingenua:</strong> Segmentamos los textos, luego aplicamos segmentación ingenua basada en las señales de límites.</li><li><strong>Segmentación tardía:</strong> Segmentamos los textos, luego usamos segmentación tardía para determinar los embeddings.</li></ul><p>Tanto para la segmentación tardía como para la segmentación ingenua, usamos recuperación de fragmentos para determinar el documento relevante (como se muestra en la Figura 5, anteriormente en este post).</p><p>Los resultados no muestran un claro ganador:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 8: Sin segmentación vs segmentación ingenua vs segmentación tardía</span></figcaption></figure><ul><li><strong>Para la recuperación de hechos, la segmentación ingenua funciona mejor:</strong> Para los conjuntos de datos QMSum, NarrativeQA y 2WikiMultiHopQA, el modelo tiene que identificar pasajes relevantes en el documento. Aquí, la segmentación ingenua es claramente mejor que codificar todo en un solo embedding, ya que probablemente solo unos pocos fragmentos incluyen información relevante, y dichos fragmentos la capturan mucho mejor que un solo embedding de todo el documento.</li><li><strong>El chunking tardío funciona mejor con documentos coherentes y contexto relevante:</strong> Para documentos que cubren un tema coherente donde los usuarios buscan temas generales en lugar de hechos específicos (como en NFCorpus), el chunking tardío supera ligeramente al no chunking, ya que equilibra el contexto general del documento con el detalle local. Sin embargo, mientras que el chunking tardío generalmente funciona mejor que el chunking ingenuo al preservar el contexto, esta ventaja puede convertirse en una desventaja cuando se buscan hechos aislados dentro de documentos que contienen información mayormente irrelevante - como se ve en las regresiones de rendimiento para NarrativeQA y 2WikiMultiHopQA, donde el contexto adicional se vuelve más distractor que útil.</li></ul><h3 id=\"does-chunk-size-make-a-difference\">¿El Tamaño del Chunk Hace la Diferencia?</h3><p>La efectividad de los métodos de chunking realmente depende del dataset, destacando cómo la estructura del contenido juega un papel crucial:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 9: Comparación de tamaños de chunk con chunking ingenuo y tardío.</span></figcaption></figure><p>Como podemos ver, el chunking tardío generalmente supera al chunking ingenuo en tamaños de chunk más pequeños, ya que los chunks ingenuos pequeños son demasiado reducidos para contener mucho contexto, mientras que los chunks tardíos pequeños retienen el contexto del documento completo, haciéndolos más significativos semánticamente. La excepción a esto es el dataset NarrativeQA donde hay simplemente tanto contexto irrelevante que el chunking tardío se queda atrás. Con tamaños de chunk más grandes, el chunking ingenuo muestra una mejora notable (ocasionalmente superando al chunking tardío) debido al mayor contexto, mientras que el rendimiento del chunking tardío disminuye gradualmente.</p><h2 id=\"takeaways-when-to-use-what\">Conclusiones: ¿Cuándo Usar Qué?</h2><p>En esta publicación, hemos examinado diferentes tipos de tareas de recuperación de documentos para entender mejor cuándo usar la segmentación y cuándo ayuda el chunking tardío. Entonces, ¿qué hemos aprendido?</p><h3 id=\"when-should-i-use-long-context-embedding\">¿Cuándo Debería Usar Embedding de Contexto Largo?</h3><p>En general, no perjudica la precisión de recuperación incluir tanto texto de tus documentos como puedas en la entrada de tu modelo de embedding. Sin embargo, los modelos de embedding de contexto largo a menudo se enfocan en el inicio de los documentos, ya que contienen contenido como títulos e introducción que son más importantes para juzgar la relevancia, pero los modelos pueden perder contenido en la mitad del documento.</p><h3 id=\"when-should-i-use-naive-chunking\">¿Cuándo Debería Usar Chunking Ingenuo?</h3><p>Cuando los documentos cubren múltiples aspectos, o las consultas de usuarios apuntan a información específica dentro de un documento, el chunking generalmente mejora el rendimiento de recuperación.</p><p>Finalmente, las decisiones de segmentación dependen de factores como la necesidad de mostrar texto parcial a los usuarios (por ejemplo, como Google presenta los pasajes relevantes en las vistas previas de los resultados de búsqueda), lo que hace que la segmentación sea esencial, o las limitaciones de cómputo y memoria, donde la segmentación puede ser menos favorable debido al incremento en la sobrecarga de recuperación y uso de recursos.</p><h3 id=\"when-should-i-use-late-chunking\">¿Cuándo Debería Usar Chunking Tardío?</h3><p>Al codificar el documento completo antes de crear chunks, el chunking tardío resuelve el problema de que los segmentos de texto pierdan su significado debido a la falta de contexto. Esto funciona particularmente bien con documentos coherentes, donde cada parte se relaciona con el todo. Nuestros experimentos muestran que el chunking tardío es especialmente efectivo cuando se divide el texto en chunks más pequeños, como se demuestra en nuestro <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">paper</a>. Sin embargo, hay una advertencia: si partes del documento no están relacionadas entre sí, incluir este contexto más amplio puede realmente empeorar el rendimiento de recuperación, ya que agrega ruido a los embeddings.</p><h2 id=\"conclusion\">Conclusión</h2><p>La elección entre embedding de contexto largo, chunking ingenuo y chunking tardío depende de los requisitos específicos de tu tarea de recuperación. Los embeddings de contexto largo son valiosos para documentos coherentes con consultas generales, mientras que el chunking sobresale en casos donde los usuarios buscan hechos o información específica dentro de un documento. El chunking tardío mejora aún más la recuperación al mantener la coherencia contextual dentro de segmentos más pequeños. En última instancia, entender tus datos y objetivos de recuperación guiará el enfoque óptimo, equilibrando precisión, eficiencia y relevancia contextual.</p><p>Si estás explorando estas estrategias, considera probar <code>jina-embeddings-v3</code>—sus capacidades avanzadas de contexto largo, chunking tardío y flexibilidad lo convierten en una excelente opción para diversos escenarios de recuperación.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "Comparación de cómo se comportan los modelos de embeddings de contexto largo con diferentes estrategias de segmentación para encontrar el enfoque óptimo según tus necesidades.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
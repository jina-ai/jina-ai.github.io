{
  "slug": "what-late-chunking-really-is-and-what-its-not-part-ii",
  "id": "66fe70236ca44300014cabe4",
  "uuid": "a27b0f3c-a533-422c-9d37-3ed3e2130539",
  "title": "Qu√© es realmente el Chunking Tard√≠o y qu√© no lo es: Parte II",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Muy recomendable <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models?ref=jina-ai-gmbh.ghost.io\">leer primero la Parte I</a>, ya que este art√≠culo ofrece una visi√≥n m√°s profunda, centr√°ndose en malentendidos comunes y comparaciones. <b><strong style=\"white-space: pre-wrap;\">Orden de lectura recomendado: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, parte II, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">art√≠culo de investigaci√≥n</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-1.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael G√ºnther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>La segmentaci√≥n de un documento largo tiene dos problemas: primero, <strong>determinar los puntos de corte</strong>, es decir, c√≥mo segmentar el documento. Se pueden considerar longitudes fijas de tokens, un n√∫mero fijo de oraciones o t√©cnicas m√°s avanzadas como <a href=\"https://jina.ai/segmenter?ref=jina-ai-gmbh.ghost.io\">expresiones regulares o modelos de segmentaci√≥n sem√°ntica</a>. Los l√≠mites precisos de los segmentos no solo mejoran la legibilidad de los resultados de b√∫squeda, sino que tambi√©n aseguran que los segmentos alimentados a un LLM en un sistema RAG sean precisos y suficientes, ni m√°s ni menos.</p><p>El segundo problema es la <strong>p√©rdida de contexto</strong> dentro de cada segmento. Una vez que el documento est√° segmentado, el siguiente paso l√≥gico para la mayor√≠a de las personas es incrustar cada segmento por separado en un proceso por lotes. Sin embargo, esto lleva a una p√©rdida del contexto global del documento original. Muchos trabajos anteriores han abordado primero el primer problema, argumentando que una mejor detecci√≥n de l√≠mites mejora la representaci√≥n sem√°ntica. Por ejemplo, la \"segmentaci√≥n sem√°ntica\" agrupa oraciones con alta similitud de coseno en el espacio de incrustaci√≥n para minimizar la disrupci√≥n de unidades sem√°nticas.</p><p>Desde nuestro punto de vista, estos dos problemas son <em>casi</em> ortogonales y pueden abordarse por separado. Si tuvi√©ramos que priorizar, <strong>dir√≠amos que el segundo problema es m√°s cr√≠tico.</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th style=\"text-align:center\">Problema 2: <b>Informaci√≥n contextual</b></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td style=\"text-align:center\">Preservada</td>\n<td>Perdida</td>\n</tr>\n<tr>\n<td><b>Problema 1: Puntos de corte</b></td>\n<td>Buenos</td>\n<td style=\"text-align:center\">Escenario ideal</td>\n<td>Resultados de b√∫squeda pobres</td>\n</tr>\n<tr>\n<td></td>\n<td>Pobres</td>\n<td style=\"text-align:center\">Buenos resultados de b√∫squeda, pero los resultados pueden no ser legibles para humanos o para razonamiento LLM</td>\n<td>Peor escenario posible</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"late-chunking-for-context-loss\">Late Chunking para la P√©rdida de Contexto</h2><p>El <strong>late chunking</strong> comienza abordando el segundo problema: la <strong>p√©rdida de contexto</strong>. No se trata de encontrar los puntos de corte ideales o l√≠mites sem√°nticos. A√∫n necesitas usar expresiones regulares, heur√≠sticas u otras t√©cnicas para dividir un documento largo en peque√±os segmentos. Pero en lugar de incrustar cada segmento tan pronto como se segmenta, el late chunking primero codifica todo el documento en una ventana de contexto (para <code>jina-embeddings-v3</code> es de 8192 tokens). Luego, sigue las se√±ales de l√≠mites para aplicar el agrupamiento medio para cada segmento, de ah√≠ el t√©rmino \"late\" en late chunking.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Diagram--Blog-images--6-.svg\" class=\"kg-image\" alt=\"Diagram comparing &quot;Naive Chunking&quot; and &quot;Late Chunking&quot; methods for processing long documents with labeled steps.\" loading=\"lazy\" width=\"1200\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">El late chunking a√∫n requiere se√±ales de l√≠mites, pero la diferencia clave est√° en cu√°ndo se usan esas se√±ales. En el late chunking, las se√±ales solo se aplican despu√©s de que todo el documento ha sido incrustado, y se utilizan para determinar el alcance del agrupamiento.</span></figcaption></figure><h2 id=\"late-chunking-is-resilient-to-poor-boundary-cues\">El Late Chunking es Resistente a Se√±ales de L√≠mites Deficientes</h2><p>Lo realmente interesante es que los experimentos muestran que el late chunking elimina la necesidad de l√≠mites sem√°nticos perfectos, lo que aborda parcialmente el primer problema mencionado anteriormente. De hecho, el late chunking aplicado a l√≠mites de tokens fijos supera al chunking ingenuo con se√±ales de l√≠mites sem√°nticos. Los modelos de segmentaci√≥n simples, como los que usan l√≠mites de longitud fija, funcionan a la par de los algoritmos avanzados de detecci√≥n de l√≠mites cuando se combinan con late chunking. Probamos tres tama√±os diferentes de modelos de incrustaci√≥n, y los resultados muestran que todos ellos se benefician consistentemente del late chunking en todos los conjuntos de datos de prueba. Dicho esto, el modelo de incrustaci√≥n en s√≠ sigue siendo el factor m√°s significativo en el rendimiento: <strong>no hay un solo caso en el que un modelo m√°s d√©bil con late chunking supere a un modelo m√°s fuerte sin √©l.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/plot--7-.svg\" class=\"kg-image\" alt=\"Scatter plot chart showing the percentage of relative improvements across various models against a baseline, with a vertical \" loading=\"lazy\" width=\"950\" height=\"756\"><figcaption><span style=\"white-space: pre-wrap;\">Mejora relativa de recuperaci√≥n sobre la l√≠nea base (es decir, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> con se√±ales de l√≠mite de longitud de token fija y chunking ingenuo). Como parte de un estudio de ablaci√≥n, probamos late chunking con diferentes se√±ales de l√≠mites (longitud de token fija, l√≠mites de oraciones y l√≠mites sem√°nticos) y diferentes modelos (</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\">, y </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">). Basado en su rendimiento en MTEB, la clasificaci√≥n de estos tres modelos de incrustaci√≥n es: </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-small</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>nomic-v1</span></code><span style=\"white-space: pre-wrap;\"> &lt; </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">. Sin embargo, el objetivo de este experimento no es evaluar el rendimiento de los modelos de embedding en s√≠, sino entender c√≥mo un mejor modelo de embedding interact√∫a con el chunking tard√≠o y las se√±ales de l√≠mites. Para los detalles del experimento, por favor consulte nuestro paper de investigaci√≥n.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Combo</th>\n<th>SciFact</th>\n<th>NFCorpus</th>\n<th>FiQA</th>\n<th>TRECCOVID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baseline</td>\n<td>64.2</td>\n<td>23.5</td>\n<td>33.3</td>\n<td>63.4</td>\n</tr>\n<tr>\n<td>Late</td>\n<td>66.1</td>\n<td>30.0</td>\n<td>33.8</td>\n<td>64.7</td>\n</tr>\n<tr>\n<td>Nomic</td>\n<td>70.7</td>\n<td>35.3</td>\n<td>37.0</td>\n<td>72.9</td>\n</tr>\n<tr>\n<td>Jv3</td>\n<td>71.8</td>\n<td>35.6</td>\n<td>46.3</td>\n<td>73.0</td>\n</tr>\n<tr>\n<td>Late + Nomic</td>\n<td>70.6</td>\n<td>35.3</td>\n<td>38.3</td>\n<td>75.0</td>\n</tr>\n<tr>\n<td>Late + Jv3</td>\n<td><strong>73.2</strong></td>\n<td><strong>36.7</strong></td>\n<td><strong>47.6</strong></td>\n<td><strong>77.2</strong></td>\n</tr>\n<tr>\n<td>SentBound</td>\n<td>64.7</td>\n<td>28.3</td>\n<td>30.4</td>\n<td>66.5</td>\n</tr>\n<tr>\n<td>Late + SentBound</td>\n<td>65.2</td>\n<td>30.0</td>\n<td>33.9</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td>Nomic + SentBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SentBound</td>\n<td>71.4</td>\n<td>35.8</td>\n<td>43.7</td>\n<td>72.4</td>\n</tr>\n<tr>\n<td>Late + Nomic + SentBound</td>\n<td>70.5</td>\n<td>35.3</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SentBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n<tr>\n<td>SemanticBound</td>\n<td>64.3</td>\n<td>27.4</td>\n<td>30.3</td>\n<td>66.2</td>\n</tr>\n<tr>\n<td>Late + SemanticBound</td>\n<td>65.0</td>\n<td>29.3</td>\n<td>33.7</td>\n<td>66.3</td>\n</tr>\n<tr>\n<td>Nomic + SemanticBound</td>\n<td>70.4</td>\n<td>35.3</td>\n<td>34.8</td>\n<td>74.3</td>\n</tr>\n<tr>\n<td>Jv3 + SemanticBound</td>\n<td>71.2</td>\n<td>36.1</td>\n<td>44.0</td>\n<td>74.7</td>\n</tr>\n<tr>\n<td>Late + Nomic + SemanticBound</td>\n<td>70.5</td>\n<td>36.9</td>\n<td>36.9</td>\n<td>76.1</td>\n</tr>\n<tr>\n<td>Late + Jv3 + SemanticBound</td>\n<td>72.4</td>\n<td>36.6</td>\n<td>47.6</td>\n<td>76.2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Ten en cuenta que ser resiliente a l√≠mites deficientes no significa que podamos ignorarlos‚Äîsiguen siendo importantes tanto para la legibilidad humana como para los LLM. As√≠ es como lo vemos: al optimizar la segmentaci√≥n, es decir, el primer problema mencionado anteriormente, podemos enfocarnos completamente en la legibilidad sin preocuparnos por la p√©rdida sem√°ntica/contextual. El Chunking Tard√≠o maneja bien o mal los puntos de corte, as√≠ que la legibilidad es todo lo que necesitas considerar.</p><h2 id=\"late-chunking-is-bidirectional\">El Chunking Tard√≠o es Bidireccional</h2><p>Otro malentendido com√∫n sobre el chunking tard√≠o es que sus embeddings condicionales de chunks dependen solo de los chunks anteriores sin \"mirar adelante\". Esto es incorrecto. La dependencia condicional en el <strong>chunking tard√≠o es realmente bidireccional</strong>, no unidireccional. Esto se debe a que la matriz de atenci√≥n en el modelo de embedding‚Äîun transformador de solo codificador‚Äîest√° completamente conectada, a diferencia de la matriz triangular enmascarada utilizada en modelos autorregresivos. Formalmente, el embedding del chunk $k$, $v_k \\sim Q(c_k|D)$, en lugar de $v_k \\sim Q(c_k | c_1, c_2, \\cdots, c_{k-1})$, donde $Q$ denota una factorizaci√≥n del modelo de lenguaje. Esto tambi√©n explica por qu√© el chunking tard√≠o no depende de la ubicaci√≥n precisa de los l√≠mites.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/Heading--27-.svg\" class=\"kg-image\" alt=\"Diagramas de un modelo transformador con codificador detallado a la izquierda y decodificador a la derecha, etiquetados con tokens, embeddings, \" loading=\"lazy\" width=\"1033\" height=\"560\"><figcaption><span style=\"white-space: pre-wrap;\">A diferencia de los modelos de solo decodificador con auto-atenci√≥n enmascarada, los modelos de embedding son t√≠picamente de solo codificador con una matriz de atenci√≥n completa. Esto significa que cada embedding de token est√° condicionado por todos los otros tokens dentro de la misma ventana de contexto, que, en el caso de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">, incluye hasta 8191 otros tokens. Como resultado, el embedding del chunk lleva informaci√≥n de contexto global en ambas direcciones.</span></figcaption></figure><h2 id=\"late-chunking-can-be-trained\">El Chunking Tard√≠o Puede Ser Entrenado</h2><p>El chunking tard√≠o <em>no</em> requiere entrenamiento adicional para los modelos de embedding. Puede aplicarse a cualquier modelo de embedding de contexto largo que use agrupaci√≥n media, haci√©ndolo muy atractivo para los profesionales. Dicho esto, si est√°s trabajando en tareas como respuesta a preguntas o recuperaci√≥n de documentos por consulta, el rendimiento a√∫n puede mejorarse m√°s con algo de ajuste fino. Espec√≠ficamente, los datos de entrenamiento consisten en tuplas que contienen:</p><ul><li>Una <strong>consulta</strong> (por ejemplo, una pregunta o t√©rmino de b√∫squeda).</li><li>Un <strong>documento</strong> que contiene informaci√≥n relevante para responder la consulta.</li><li>Un <strong>segmento relevante</strong> dentro del documento, que es el fragmento espec√≠fico de texto que responde directamente a la consulta.</li></ul><p>El modelo se entrena emparejando consultas con sus segmentos relevantes, usando una funci√≥n de p√©rdida contrastiva como InfoNCE. Esto asegura que los segmentos relevantes est√©n estrechamente alineados con la consulta en el espacio de embeddings, mientras que los segmentos no relacionados se alejan. Como resultado, el modelo aprende a enfocarse en las partes m√°s relevantes del documento al generar embeddings de chunks. <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">Para m√°s detalles, consulte nuestro paper de investigaci√≥n.</a></p><h2 id=\"late-chunking-vs-contextual-retrieval\">Chunking Tard√≠o vs. Recuperaci√≥n Contextual</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.anthropic.com/news/contextual-retrieval?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Presentando la Recuperaci√≥n Contextual</div><div class=\"kg-bookmark-description\">Anthropic es una empresa de seguridad e investigaci√≥n en IA que trabaja para construir sistemas de IA confiables, interpretables y dirigibles.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-2.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Poco despu√©s de que se introdujo el chunking tard√≠o, Anthropic present√≥ una estrategia separada llamada <strong>Recuperaci√≥n Contextual</strong>. El m√©todo de Anthropic es un enfoque de fuerza bruta para abordar el problema del contexto perdido, y funciona de la siguiente manera:</p><ol><li>Cada chunk se env√≠a al LLM junto con el documento completo.</li><li>El LLM agrega contexto relevante a cada chunk.</li><li>Esto resulta en embeddings m√°s ricos e informativos.</li></ol><p>En nuestra opini√≥n, esto es esencialmente <strong>enriquecimiento de contexto</strong>, donde el contexto global se codifica expl√≠citamente en cada chunk usando un LLM, lo cual es costoso en t√©rminos de <strong>costo</strong>, <strong>tiempo</strong> y <strong>almacenamiento</strong>. Adem√°s, no est√° claro si este enfoque es resiliente a los l√≠mites de los chunks, ya que el LLM depende de chunks precisos y legibles para enriquecer el contexto de manera efectiva. En contraste, el chunking tard√≠o es altamente resiliente a las se√±ales de l√≠mites, como se demostr√≥ anteriormente. No requiere almacenamiento adicional ya que el tama√±o del embedding permanece igual. A pesar de aprovechar la longitud completa del contexto del modelo de embedding, <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io#parameter-latechunking\">sigue siendo significativamente m√°s r√°pido que usar un LLM para generar enriquecimiento</a>. En el estudio cualitativo de nuestro paper de investigaci√≥n, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">mostramos que la recuperaci√≥n contextual de Anthropic tiene un rendimiento similar al chunking tard√≠o.</a> Sin embargo, el chunking tard√≠o proporciona una soluci√≥n m√°s de bajo nivel, gen√©rica y natural al aprovechar la mec√°nica inherente del transformador de solo codificador.</p><h2 id=\"which-embedding-models-support-late-chunking\">¬øQu√© Modelos de Embedding Soportan el Chunking Tard√≠o?</h2><p>El chunking tard√≠o no es exclusivo de <code>jina-embeddings-v3</code> o <code>v2</code>. Es un enfoque bastante gen√©rico que puede aplicarse a cualquier modelo de embedding de contexto largo que use agrupaci√≥n media. Por ejemplo, en esta publicaci√≥n, mostramos que <code>nomic-v1</code> tambi√©n lo soporta. Damos una c√°lida bienvenida a todos los proveedores de embeddings para que implementen soporte para el chunking tard√≠o en sus soluciones.</p><p>Como usuario de modelos, al evaluar un nuevo modelo de embedding o API, puedes seguir estos pasos para verificar si podr√≠a soportar chunking tard√≠o:</p><ol><li><strong>Salida √önica</strong>: ¬øEl modelo/API te da solo un embedding final por oraci√≥n en lugar de embeddings a nivel de token? Si es as√≠, probablemente no pueda soportar late chunking (especialmente para esas APIs web).</li><li><strong>Soporte de Contexto Largo</strong>: ¬øEl modelo/API maneja contextos de al menos 8192 tokens? Si no, el late chunking no ser√° aplicable‚Äîo m√°s precisamente, no tiene sentido adaptar late chunking para un modelo de contexto corto. Si es as√≠, aseg√∫rate de que realmente funcione bien con contextos largos, no solo que afirme soportarlos. Normalmente puedes encontrar esta informaci√≥n en el informe t√©cnico del modelo, como evaluaciones en LongMTEB u otros benchmarks de contexto largo.</li><li><strong>Mean Pooling</strong>: Para modelos autohospedados o APIs que proporcionan embeddings a nivel de token antes del pooling, verifica si el m√©todo de pooling predeterminado es mean pooling. Los modelos que usan CLS o max pooling no son compatibles con late chunking.</li></ol><p>En resumen, si un modelo de embedding soporta contexto largo y usa mean pooling por defecto, puede soportar f√°cilmente late chunking. Consulta nuestro <a href=\"https://github.com/jina-ai/late-chunking/issues/?ref=jina-ai-gmbh.ghost.io\">repositorio de GitHub para detalles de implementaci√≥n y m√°s discusi√≥n</a>.</p><h2 id=\"conclusion\">Conclusi√≥n</h2><p>Entonces, ¬øqu√© es late chunking? Late chunking es un m√©todo sencillo para generar embeddings de chunks utilizando modelos de embedding de contexto largo. Es r√°pido, resistente a las se√±ales de l√≠mites y altamente efectivo. No es una heur√≠stica ni una sobreingenier√≠a‚Äîes un dise√±o reflexivo basado en una comprensi√≥n profunda del mecanismo transformer.</p><p>Hoy en d√≠a, el bombo publicitario alrededor de los LLMs es innegable. En muchos casos, problemas que podr√≠an abordarse eficientemente con modelos m√°s peque√±os como BERT son en cambio delegados a LLMs, impulsados por el atractivo de soluciones m√°s grandes y complejas. No es sorprendente que los grandes proveedores de LLM impulsen una mayor adopci√≥n de sus modelos, mientras que los proveedores de embeddings aboguen por los embeddings ‚Äî ambos juegan seg√∫n sus fortalezas comerciales. Pero al final, no se trata del bombo publicitario, sino de la acci√≥n, de lo que realmente funciona. Dejemos que la comunidad, la industria y, lo m√°s importante, el tiempo revelen qu√© enfoque es verdaderamente m√°s eficiente, m√°s ligero y construido para perdurar.</p><p>Aseg√∫rate de leer <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">nuestro paper de investigaci√≥n</a>, y te animamos a que eval√∫es late chunking en varios escenarios y compartas tus comentarios con nosotros.</p>",
  "comment_id": "66fe70236ca44300014cabe4",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/lc2.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-03T12:21:23.000+02:00",
  "updated_at": "2024-10-07T15:29:00.000+02:00",
  "published_at": "2024-10-03T19:19:16.000+02:00",
  "custom_excerpt": "Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-late-chunking-really-is-and-what-its-not-part-ii/",
  "excerpt": "Parte 2 de nuestra exploraci√≥n del Late Chunking (fragmentaci√≥n tard√≠a), un an√°lisis profundo de por qu√© es el mejor m√©todo para incrustar fragmentos y mejorar el rendimiento de b√∫squeda/RAG.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Slide depicting the \"Late Chunking\" process, with flow charts and a model highlighting the transition from a \"Long Document\" ",
  "feature_image_caption": null
}
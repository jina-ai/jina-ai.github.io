{
  "slug": "the-what-and-why-of-text-image-modality-gap-in-clip-models",
  "id": "66c8431bda9a33000146d97d",
  "uuid": "52a3f4ec-9f1b-4a34-8f37-2810925c85f1",
  "title": "El Qu√© y Por qu√© de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP",
  "html": "<p>Los <a href=\"https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai?ref=jina-ai-gmbh.ghost.io\">embeddings sem√°nticos</a> son el n√∫cleo de los modelos modernos de IA, incluso de los chatbots y modelos de arte con IA. A veces est√°n ocultos para los usuarios, pero siguen ah√≠, acechando justo bajo la superficie.</p><p>La teor√≠a de los embeddings tiene solo dos partes:</p><ol><li>Las cosas ‚Äî elementos fuera del modelo de IA, como textos e im√°genes ‚Äî est√°n representadas por vectores creados por modelos de IA a partir de datos sobre esas cosas.</li><li>Las relaciones entre las cosas fuera del modelo de IA est√°n representadas por relaciones espaciales entre esos vectores. Entrenamos modelos de IA espec√≠ficamente para crear vectores que funcionen de esa manera.</li></ol><p>Cuando creamos un modelo multimodal de imagen-texto, entrenamos el modelo para que los embeddings de las im√°genes y los embeddings de los textos que describen o se relacionan con esas im√°genes est√©n relativamente cerca entre s√≠. Las similitudes sem√°nticas entre las cosas que esos dos vectores representan ‚Äî una imagen y un texto ‚Äî se reflejan en la relaci√≥n espacial entre los dos vectores.</p><p>Por ejemplo, podr√≠amos esperar razonablemente que los vectores de embedding para una imagen de una naranja y el texto \"una naranja fresca\" est√©n m√°s cerca entre s√≠ que la misma imagen y el texto \"una manzana fresca\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png\" class=\"kg-image\" alt=\"Illustration on a black background showing an orange and an apple with arrows between them and quotes reading &quot;A fresh orange\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ese es el prop√≥sito de un modelo de embedding: generar representaciones donde las caracter√≠sticas que nos importan ‚Äî como qu√© tipo de fruta se muestra en una imagen o se nombra en un texto ‚Äî se preserven en la distancia entre ellas.</p><p>Pero la multimodalidad introduce algo m√°s. Podr√≠amos encontrar que una imagen de una naranja est√° m√°s cerca de una imagen de una manzana que del texto \"una naranja fresca\", y que el texto \"una manzana fresca\" est√° m√°s cerca de otro texto que de una imagen de una manzana.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png\" class=\"kg-image\" alt=\"Black background featuring an apple on the left and an orange on the right with annotated arrows marked &quot;A fresh apple.&quot; and \" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Resulta que esto es exactamente lo que sucede con los modelos multimodales, incluido el modelo <a href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image?ref=jina-ai-gmbh.ghost.io\">Jina CLIP</a> (<code>jina-clip-v1</code>) de Jina AI.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class=\"kg-bookmark-description\">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><p>Para probarlo, tomamos una muestra de 1,000 pares de texto-imagen del <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">conjunto de prueba Flickr8k</a>. Cada par contiene cinco textos de leyenda (as√≠ que t√©cnicamente no es un par) y una sola imagen, con los cinco textos describiendo la misma imagen.</p><p>Por ejemplo, la siguiente imagen (<code>1245022983_fb329886dd.jpg</code> en el conjunto de datos Flickr8k):</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg\" class=\"kg-image\" alt=\"A young girl in a pink skirt playing with a frisbee in an urban outdoor setting with cars and bikes present.\" loading=\"lazy\" width=\"334\" height=\"500\"></figure><p>Sus cinco leyendas:</p><pre><code class=\"language-Text\">A child in all pink is posing nearby a stroller with buildings in the distance.\nA little girl in pink dances with her hands on her hips.\nA small girl wearing pink dances on the sidewalk.\nThe girl in a bright pink skirt dances near a stroller.\nThe little girl in pink has her hands on her hips.\n</code></pre><p>Usamos Jina CLIP para incrustar las im√°genes y textos y luego:</p><ol><li>Comparamos las similitudes de coseno de los embeddings de im√°genes con los embeddings de sus textos de leyenda.</li><li>Tomamos los embeddings de los cinco textos de leyenda que describen la misma imagen y comparamos sus similitudes de coseno entre s√≠.</li></ol><p>El resultado es una brecha sorprendentemente grande, visible en la Figura 1:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png\" class=\"kg-image\" alt=\"Graph with two curves showing the distribution of Cosine Similarity for Image2Text and Text2Text pairs with labeled axes.\" loading=\"lazy\" width=\"1870\" height=\"1130\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Distribuci√≥n de valores de similitud de coseno entre pares coincidentes de imagen-texto y texto-texto en Jina CLIP.</span></figcaption></figure><p>Con pocas excepciones, los pares de texto coincidentes est√°n mucho m√°s cerca entre s√≠ que los pares de imagen-texto coincidentes. Esto indica fuertemente que Jina CLIP est√° codificando textos en una parte del espacio de embedding e im√°genes en una parte mayormente disjunta relativamente alejada de ella. Este espacio entre los textos y las im√°genes es la <em>brecha multimodal</em>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png\" class=\"kg-image\" alt=\"Diagram on black background depicting 'Images' on left, 'Texts' on bottom, with labeled 'Multimodal Gap' in the center.\" loading=\"lazy\" width=\"493\" height=\"479\"></figure><p>Los modelos de embedding multimodales est√°n codificando m√°s que la informaci√≥n sem√°ntica que nos importa: est√°n codificando el medio de su entrada. Seg√∫n Jina CLIP, una imagen no vale, como dice el dicho, mil palabras. Tiene un contenido que ninguna cantidad de palabras puede igualar realmente. Codifica el medio de entrada en la sem√°ntica de sus embeddings sin que nadie lo haya entrenado para hacerlo.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Mientras solo comparemos im√°genes con textos y viceversa, esto no es un problema, pero un modelo verdaderamente multimodal deber√≠a poder decirnos que, por ejemplo, el texto \"esto es una manzana\" coincide mejor con una imagen de una manzana que con un texto sobre naranjas. Los modelos estilo CLIP en su forma actual no pueden hacer eso.</div></div><p>Este fen√≥meno ha sido investigado en el art√≠culo <em>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al., 2022</a>] que se refiere a √©l como la \"brecha de modalidad\". La brecha de modalidad es la separaci√≥n espacial, en el espacio de embedding, entre entradas en un medio y entradas en otro. Aunque los modelos no est√°n intencionalmente entrenados para tener tal brecha, son omnipresentes en los modelos multimodales.</p><p>Nuestras investigaciones sobre la brecha de modalidad en Jina CLIP se basan fuertemente en <a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a>.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://papers.neurips.cc/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">NeurIPS Proceedings</span></div></div></a></figure><h2 id=\"where-does-the-modality-gap-come-from\">¬øDe d√≥nde viene la brecha de modalidad?</h2><p>Liang et al. [2022] identifican tres fuentes principales detr√°s de la brecha de modalidad:</p><ul><li>Un sesgo de inicializaci√≥n que llaman el \"efecto cono\".</li><li>Reducciones en la temperatura (aleatoriedad) durante el entrenamiento que hacen muy dif√≠cil \"desaprender\" este sesgo.</li><li>Procedimientos de aprendizaje contrastivo, que son ampliamente utilizados en modelos multimodales, que involuntariamente refuerzan la brecha.</li></ul><p>Examinaremos cada uno por turno.</p><h3 id=\"cone-effect\">Efecto Cono</h3><p>Un modelo construido con una arquitectura CLIP o tipo CLIP es en realidad dos modelos de embedding separados conectados entre s√≠. Para modelos multimodales de imagen-texto, esto significa un modelo para codificar textos y otro completamente separado para codificar im√°genes, como se muestra en el esquema a continuaci√≥n.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png\" class=\"kg-image\" alt=\"Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di\" loading=\"lazy\" width=\"1025\" height=\"750\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Estos dos modelos son entrenados de manera que un embedding de imagen y un embedding de texto est√©n relativamente cerca cuando el texto describe bien la imagen.</p><p>Puedes entrenar un modelo como este aleatorizando los pesos en ambos modelos, luego presentando pares de imagen y texto juntos, entren√°ndolo desde cero para minimizar la distancia entre las dos salidas. El <a href=\"https://arxiv.org/abs/2103.00020?ref=jina-ai-gmbh.ghost.io\">modelo CLIP original de OpenAI</a> fue entrenado de esta manera. Sin embargo, esto requiere muchos pares de imagen-texto y un entrenamiento computacionalmente costoso. Para el primer modelo CLIP, OpenAI extrajo 400 millones de pares de imagen-texto de materiales con subt√≠tulos en Internet.</p><p>Los modelos m√°s recientes tipo CLIP utilizan componentes pre-entrenados<a href=\"https://doi.org/10.1109/CVPR52688.2022.01759?ref=jina-ai-gmbh.ghost.io\">.</a> Esto significa entrenar cada componente por separado como un buen modelo de embedding de modo √∫nico, uno para textos y otro para im√°genes. Estos dos modelos luego se entrenan juntos usando pares de imagen-texto, un proceso llamado <em>ajuste contrastivo</em>. Los pares alineados de imagen-texto se utilizan para \"empujar\" lentamente los pesos para hacer que los embeddings de texto e imagen coincidentes est√©n m√°s cerca entre s√≠, y los no coincidentes m√°s separados.</p><p>Este enfoque generalmente requiere menos datos de pares imagen-texto, que son dif√≠ciles y costosos de obtener, y grandes cantidades de textos e im√°genes sin subt√≠tulos, que son mucho m√°s f√°ciles de obtener. Jina CLIP (<code>jina-clip-v1</code>) fue entrenado usando este √∫ltimo m√©todo. Pre-entrenamos un modelo <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">JinaBERT v2</a> para codificaci√≥n de texto usando datos de texto generales y utilizamos un <a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\">codificador de im√°genes EVA-02</a> pre-entrenado, luego los entrenamos m√°s usando una variedad de t√©cnicas de entrenamiento contrastivo, como se describe en <a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">Koukounas et al. [2024]</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png\" class=\"kg-image\" alt=\"UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Ubicaciones iniciales de los embeddings de imagen y texto antes del entrenamiento por pares en Jina CLIP, proyectadas en dos dimensiones.</span></figcaption></figure><p>Si tomamos estos dos modelos pre-entrenados y observamos su salida, antes de entrenarlos con pares de imagen-texto, notamos algo importante. La Figura 2 (arriba) es una <a href=\"https://umap-learn.readthedocs.io/en/latest/?ref=jina-ai-gmbh.ghost.io\">proyecci√≥n UMAP</a> en dos dimensiones de los embeddings de imagen producidos por el codificador EVA-02 pre-entrenado y los embeddings de texto producidos por JinaBERT v2 pre-entrenado, con las l√≠neas grises indicando pares imagen-texto coincidentes. Esto es antes de cualquier entrenamiento cross-modal.</p><p>El resultado es una especie de \"cono\" truncado, con embeddings de imagen en un extremo y embeddings de texto en el otro. Esta forma de cono se traduce pobremente a proyecciones bidimensionales, pero se puede ver ampliamente en la imagen de arriba. Todos los textos se agrupan en una parte del espacio de embedding, y todas las im√°genes en otra parte. Si, despu√©s del entrenamiento, los textos siguen siendo m√°s similares a otros textos que a las im√°genes coincidentes, este estado inicial es una gran raz√≥n por la que sucede. El objetivo de mejor coincidencia de im√°genes con textos, textos con textos e im√°genes con im√°genes, es completamente compatible con esta forma de cono.</p><p>El modelo tiene prejuicios de nacimiento y lo que aprende no cambia eso. La Figura 3 (abajo) es el mismo an√°lisis del modelo Jina CLIP tal como se lanz√≥, despu√©s del entrenamiento completo usando pares de imagen-texto. Si acaso, la brecha multimodal es a√∫n m√°s pronunciada.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png\" class=\"kg-image\" alt=\"UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings.\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Ubicaciones de los embeddings de imagen y texto despu√©s del entrenamiento por pares en Jina CLIP, proyectadas en dos dimensiones.</span></figcaption></figure><p>Incluso despu√©s de un entrenamiento extensivo, Jina CLIP a√∫n codifica el medio como parte del mensaje.</p><p>Usar el enfoque m√°s costoso de OpenAI, con inicializaci√≥n puramente aleatoria, no elimina este sesgo. Tomamos la arquitectura original de OpenAI CLIP y aleatorizamos completamente todos los pesos, luego hicimos el mismo an√°lisis que arriba. El resultado sigue siendo una forma de cono truncado, como se ve en la Figura 4:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png\" class=\"kg-image\" alt=\"Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding\" loading=\"lazy\" width=\"2000\" height=\"1333\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Ubicaciones iniciales de los embeddings de imagen y texto en Jina CLIP con pesos completamente aleatorios y sin entrenamiento, proyectadas en dos dimensiones.</span></figcaption></figure><p>Este sesgo es un problema estructural y puede no tener soluci√≥n. Si es as√≠, solo podemos buscar formas de corregirlo o mitigarlo durante el entrenamiento.</p><h3 id=\"training-temperature\">Temperatura de Entrenamiento</h3><p>Durante el entrenamiento de modelos de IA, t√≠picamente agregamos algo de aleatoriedad al proceso. Calculamos cu√°nto deber√≠a cambiar un lote de muestras de entrenamiento los pesos en el modelo, luego agregamos un peque√±o factor aleatorio a esos cambios antes de realmente cambiar los pesos. Llamamos a la cantidad de aleatoriedad la <em>temperatura</em>, por analog√≠a con la forma en que usamos la aleatoriedad en termodin√°mica.</p><p>Las temperaturas altas crean cambios grandes en los modelos muy r√°pido, mientras que las temperaturas bajas reducen la cantidad que un modelo puede cambiar cada vez que ve algunos datos de entrenamiento. El resultado es que con temperaturas altas, podemos esperar que los embeddings individuales se muevan mucho en el espacio de embedding durante el entrenamiento, y con temperaturas bajas, se mover√°n mucho m√°s lentamente.</p><p>La mejor pr√°ctica para entrenar modelos de IA es comenzar con una temperatura alta y luego reducirla progresivamente. Esto ayuda al modelo a hacer grandes saltos en el aprendizaje al principio cuando los pesos son aleatorios o est√°n lejos de donde necesitan estar y luego le permite aprender los detalles de manera m√°s estable.</p><p>El entrenamiento de pares imagen-texto de Jina CLIP comienza con una temperatura de 0.07 (esta es una temperatura relativamente alta) y la reduce exponencialmente durante el curso del entrenamiento a 0.01, como se muestra en la Figura 5 a continuaci√≥n, un gr√°fico de temperatura vs. pasos de entrenamiento:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png\" class=\"kg-image\" alt=\"Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating \" loading=\"lazy\" width=\"1000\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Disminuci√≥n de temperatura durante el entrenamiento por pares en Jina CLIP.</span></figcaption></figure><p>Quer√≠amos saber si aumentar la temperatura ‚Äî a√±adiendo aleatoriedad ‚Äî reducir√≠a el efecto cono y acercar√≠a los embeddings de imagen y texto en general. As√≠ que reentrenamos Jina CLIP con una temperatura fija de 0.1 (un valor muy alto). Despu√©s de cada √©poca de entrenamiento, verificamos la distribuci√≥n de distancias entre pares de imagen-texto y pares de texto-texto, al igual que en la Figura 1. Los resultados se muestran a continuaci√≥n en la Figura 6:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets.\" loading=\"lazy\" width=\"1999\" height=\"1999\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: La brecha entre modalidades se reduce con el tiempo cuando la temperatura de entrenamiento es alta.</span></figcaption></figure><p>Como puede verse, mantener una temperatura alta reduce dram√°ticamente la brecha multimodal. Permitir que los embeddings se muevan mucho durante el entrenamiento ayuda significativamente a superar el sesgo inicial en la distribuci√≥n de embeddings.</p><p>Sin embargo, esto tiene un costo. Tambi√©n probamos el rendimiento del modelo utilizando seis pruebas de recuperaci√≥n diferentes: Tres pruebas de recuperaci√≥n texto-texto y tres de texto-imagen, de los conjuntos de datos <a href=\"https://huggingface.co/datasets/HuggingFaceM4/COCO?ref=jina-ai-gmbh.ghost.io\">MS-COCO</a>, <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a> y <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30k</a>. En todas las pruebas, vemos que el rendimiento cae al principio del entrenamiento y luego sube muy lentamente, como puede verse en la Figura 7:</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png\" class=\"kg-image\" alt=\"Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions.\" loading=\"lazy\" width=\"2000\" height=\"735\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w\" sizes=\"(min-width: 1200px) 1200px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Rendimiento durante el entrenamiento. Al principio, hay una fuerte ca√≠da desde el estado inicial y luego una subida muy lenta.</span></figcaption></figure><p>Probablemente ser√≠a extremadamente largo y costoso entrenar un modelo como Jina CLIP usando esta temperatura alta constante. Aunque te√≥ricamente es factible, no es una soluci√≥n pr√°ctica.</p><h3 id=\"contrastive-learning-and-the-false-negative-problem\">Aprendizaje Contrastivo y el Problema de los Falsos Negativos</h3><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> tambi√©n descubrieron que las pr√°cticas est√°ndar de aprendizaje contrastivo ‚Äî el mecanismo que usamos para entrenar modelos multimodales tipo CLIP ‚Äî tienden a reforzar la brecha multimodal.</p><p>El aprendizaje contrastivo es fundamentalmente un concepto simple. Tenemos un embedding de imagen y un embedding de texto y sabemos que deber√≠an estar m√°s cerca entre s√≠, as√≠ que ajustamos los pesos en el modelo durante el entrenamiento para lograr esto. Vamos despacio, ajustando los pesos en peque√±as cantidades, y los ajustamos en proporci√≥n a qu√© tan separados est√°n los dos embeddings: M√°s cerca significa un cambio m√°s peque√±o que m√°s lejos.</p><p>Esta t√©cnica funciona mucho mejor si no solo acercamos los embeddings cuando coinciden, sino que tambi√©n los alejamos cuando no coinciden. Queremos tener no solo pares de imagen-texto que pertenezcan juntos, sino tambi√©n pares que sabemos que deben estar separados.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png\" class=\"kg-image\" alt=\"Black background with an illustration of a red apple and an orange, associated with arrows and quotes \"A fresh apple\" and \"A \" loading=\"lazy\" width=\"1020\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Esto plantea algunos problemas:</p><ol><li>Nuestras fuentes de datos consisten enteramente en pares coincidentes. Nadie har√≠a una base de datos de textos e im√°genes que un humano haya verificado como no relacionados, ni tampoco se podr√≠a construir f√°cilmente una mediante web scraping u otra t√©cnica no supervisada o semi-supervisada.</li><li>Incluso los pares de imagen-texto que superficialmente parecen completamente disjuntos no necesariamente lo son. No tenemos una teor√≠a de la sem√°ntica que nos permita hacer objetivamente tales juicios negativos. Por ejemplo, una imagen de un gato acostado en un porche no es una coincidencia completamente negativa para el texto \"un hombre durmiendo en un sof√°\". Ambos involucran estar acostado sobre algo.</li></ol><p>Idealmente, querr√≠amos entrenar con pares de imagen-texto que supi√©ramos con certeza que est√°n relacionados <em>y no relacionados</em>, pero no hay una manera obvia de obtener pares conocidos no relacionados. Es posible preguntarle a la gente \"¬øEsta frase describe esta imagen?\" y esperar respuestas consistentes. Es mucho m√°s dif√≠cil obtener respuestas consistentes preguntando \"¬øEsta frase no tiene nada que ver con esta imagen?\"</p><p>En su lugar, obtenemos pares de imagen-texto no relacionados seleccionando aleatoriamente im√°genes y textos de nuestros datos de entrenamiento, esperando que pr√°cticamente siempre sean malas coincidencias. En la pr√°ctica, esto funciona dividiendo nuestros datos de entrenamiento en lotes. Para entrenar Jina CLIP, usamos lotes que conten√≠an 32,000 pares coincidentes de imagen-texto, pero para este experimento, los tama√±os de lote fueron solo de 16.</p><p>La tabla siguiente muestra 16 pares de imagen-texto muestreados aleatoriamente de Flickr8k:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png\" class=\"kg-image\" alt=\"Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with\" loading=\"lazy\" width=\"1827\" height=\"1245\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Para obtener pares no coincidentes, combinamos cada imagen en el lote con cada texto <em>excepto con el que coincide</em>. Por ejemplo, el siguiente par es una imagen y texto que no coinciden:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png\" class=\"kg-image\" alt=\"Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Descripci√≥n:</strong> Una ni√±a de rosa recoge flores.</p><p>Pero este procedimiento asume que todos los textos que coinciden con otras im√°genes son igualmente malas coincidencias. Esto no siempre es cierto. Por ejemplo:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png\" class=\"kg-image\" alt=\"Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation.\" loading=\"lazy\" width=\"500\" height=\"500\"></figure><p><strong>Descripci√≥n:</strong> El perro se sienta junto a un mont√≥n de nieve.</p><p>Aunque el texto no describe esta imagen, tienen un perro en com√∫n. Tratar este par como no coincidente tender√° a alejar la palabra \"perro\" de cualquier imagen de un perro.</p><p><a href=\"https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html?ref=jina-ai-gmbh.ghost.io\">Liang et al. [2022]</a> muestran que estos pares no coincidentes imperfectos empujan todas las im√°genes y textos a alejarse entre s√≠.</p><p>Nos propusimos verificar su afirmaci√≥n con un modelo de imagen <code>vit-b-32</code> completamente inicializado aleatoriamente y un modelo de texto JinaBERT v2 similarmente aleatorizado, con la temperatura de entrenamiento establecida en una constante de 0.02 (una temperatura moderadamente baja). Construimos dos conjuntos de datos de entrenamiento:</p><ul><li>Uno con lotes aleatorios extra√≠dos de Flickr8k, con pares no coincidentes construidos como se describi√≥ anteriormente.</li><li>Otro donde los lotes se construyen intencionalmente con m√∫ltiples copias de la misma imagen con diferentes textos en cada lote. Esto garantiza que un n√∫mero significativo de pares \"no coincidentes\" son en realidad coincidencias correctas entre s√≠.</li></ul><p>Luego entrenamos dos modelos durante una √©poca, uno con cada conjunto de datos de entrenamiento, y medimos la distancia coseno promedio entre 1,000 pares de texto-imagen en el conjunto de datos Flickr8k para cada modelo. El modelo entrenado con lotes aleatorios tuvo una distancia coseno promedio de 0.7521, mientras que el entrenado con muchos pares \"no coincidentes\" intencionalmente coincidentes tuvo una distancia coseno promedio de 0.7840. El efecto de los pares \"no coincidentes\" incorrectos es bastante significativo. Dado que el entrenamiento real del modelo es mucho m√°s largo y usa muchos m√°s datos, podemos ver c√≥mo este efecto crecer√≠a y aumentar√≠a la brecha entre im√°genes y textos en su conjunto.</p><h2 id=\"the-medium-is-the-message\">El Medio es el Mensaje</h2><p>El te√≥rico canadiense de las comunicaciones <a href=\"https://en.wikipedia.org/wiki/The_medium_is_the_message?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Marshall McLuhan</a> acu√±√≥ la frase \"El medio es el mensaje\" en su libro de 1964 <a href=\"https://en.wikipedia.org/wiki/Understanding_Media?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\"><em>Understanding Media: The Extensions of Man</em></a> para enfatizar que los mensajes no son aut√≥nomos. Nos llegan en un contexto que afecta fuertemente su significado, y √©l afirm√≥ famosamente que una de las partes m√°s importantes de ese contexto es la naturaleza del medio de comunicaci√≥n.</p><p>La brecha de multimodalidad nos ofrece una oportunidad √∫nica para estudiar una clase de fen√≥menos sem√°nticos emergentes en modelos de IA. Nadie le dijo a Jina CLIP que codificara el medio de los datos con los que fue entrenado ‚Äî simplemente lo hizo de todos modos. Incluso si no hemos resuelto el problema para modelos multimodales, al menos tenemos una buena comprensi√≥n te√≥rica de d√≥nde proviene el problema.</p><p>Debemos asumir que nuestros modelos est√°n codificando otras cosas que a√∫n no hemos buscado debido al mismo tipo de sesgo. Por ejemplo, probablemente tenemos el mismo problema en modelos de embedding multiling√ºes. El entrenamiento conjunto en dos o m√°s idiomas probablemente conduce a la misma brecha entre idiomas, especialmente porque se utilizan ampliamente m√©todos de entrenamiento similares. Las soluciones al problema de la brecha pueden tener implicaciones muy amplias.</p><p>Una investigaci√≥n sobre el sesgo de inicializaci√≥n en una gama m√°s amplia de modelos probablemente tambi√©n conducir√° a nuevos hallazgos. Si el medio es el mensaje para un modelo de embedding, ¬øqui√©n sabe qu√© m√°s se est√° codificando en nuestros modelos sin que nos demos cuenta?</p>",
  "comment_id": "66c8431bda9a33000146d97d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-08-23T10:06:51.000+02:00",
  "updated_at": "2024-08-27T20:10:53.000+02:00",
  "published_at": "2024-08-26T15:56:36.000+02:00",
  "custom_excerpt": "You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/the-what-and-why-of-text-image-modality-gap-in-clip-models/",
  "excerpt": "No puedes simplemente usar un modelo CLIP para recuperar texto e im√°genes y ordenar los resultados por puntuaci√≥n. ¬øPor qu√©? Por la brecha de modalidad. ¬øQu√© es y de d√≥nde proviene?",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic black image with \"modality gap\" in 3D purple letters, additional text, and a dynamic glass sphere effect.",
  "feature_image_caption": null
}
{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "Embeddings binarios: Toda la IA, 3.125% del peso",
  "html": "<p>Los embeddings se han convertido en la piedra angular de una variedad de aplicaciones de IA y procesamiento de lenguaje natural, ofreciendo una forma de representar los significados de los textos como vectores multidimensionales. Sin embargo, entre el creciente tamaño de los modelos y las cantidades cada vez mayores de datos que procesan los modelos de IA, las demandas computacionales y de almacenamiento para los embeddings tradicionales han aumentado. Los embeddings binarios se han introducido como una alternativa compacta y eficiente que mantiene un alto rendimiento mientras reduce drásticamente los requisitos de recursos.</p><p>Los embeddings binarios son una forma de mitigar estos requisitos de recursos reduciendo el tamaño de los vectores de embedding hasta en un 96% (96.875% en el caso de Jina Embeddings). Los usuarios pueden aprovechar el poder de los embeddings binarios compactos en sus aplicaciones de IA con una pérdida mínima de precisión.</p><h2 id=\"what-are-binary-embeddings\">¿Qué Son los Embeddings Binarios?</h2><p>Los embeddings binarios son una forma especializada de representación de datos donde los vectores tradicionales de punto flotante de alta dimensión se transforman en vectores binarios. Esto no solo comprime los embeddings sino que también retiene casi toda la integridad y utilidad de los vectores. La esencia de esta técnica radica en su capacidad para mantener la semántica y las distancias relacionales entre los puntos de datos incluso después de la conversión.<br><br>La magia detrás de los embeddings binarios es la cuantización, un método que convierte números de alta precisión en números de menor precisión. En el modelado de IA, esto a menudo significa convertir los números de punto flotante de 32 bits en representaciones con menos bits, como enteros de 8 bits.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">La binarización es la transformación de todos los valores escalares a 0 o 1, como convertir una imagen en color a una con solo píxeles blancos o negros. Imagen: 神奈川沖浪裏 (1831) por 葛飾 (Hokusai)</span></figcaption></figure><p>Los embeddings binarios llevan esto a su extremo último, reduciendo cada valor a 0 o 1. Transformar números de punto flotante de 32 bits a dígitos binarios reduce el tamaño de los vectores de embedding 32 veces, una reducción del 96.875%. Las operaciones vectoriales en los embeddings resultantes son mucho más rápidas como resultado. El uso de aceleraciones por hardware disponibles en algunos microchips puede aumentar la velocidad de las comparaciones vectoriales mucho más de 32 veces cuando los vectores están binarizados.</p><p>Inevitablemente se pierde algo de información durante este proceso, pero esta pérdida se minimiza cuando el modelo es muy eficiente. Si los embeddings no cuantizados de diferentes cosas son máximamente diferentes, entonces la binarización tiene más probabilidades de preservar bien esa diferencia. De lo contrario, puede ser difícil interpretar los embeddings correctamente.</p><p>Los modelos de Jina Embeddings están entrenados para ser muy robustos exactamente de esa manera, haciéndolos muy adecuados para la binarización.</p><p>Tales embeddings compactos hacen posibles nuevas aplicaciones de IA, particularmente en contextos con recursos limitados como usos móviles y sensibles al tiempo.</p><p>Estos beneficios en costos y tiempo de cómputo vienen con un costo de rendimiento relativamente pequeño, como muestra el gráfico a continuación.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10: Puntuaciones calculadas usando </em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">Normalized Discounted Cumulative Gain</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\"> para los 10 primeros resultados.</em></i></figcaption></figure><p>Para <code>jina-embeddings-v2-base-en</code>, la cuantización binaria reduce la precisión de recuperación del 47.13% al 42.05%, una pérdida de aproximadamente 10%. Para <code>jina-embeddings-v2-base-de</code>, esta pérdida es solo del 4%, del 44.39% al 42.65%.</p><p>Los modelos de Jina Embeddings funcionan tan bien al producir vectores binarios porque están entrenados para crear una distribución más uniforme de embeddings. Esto significa que dos embeddings diferentes probablemente estarán más alejados entre sí en más dimensiones que los embeddings de otros modelos. Esta propiedad asegura que esas distancias estén mejor representadas por sus formas binarias.</p><h2 id=\"how-do-binary-embeddings-work\">¿Cómo Funcionan los Embeddings Binarios?</h2><p>Para ver cómo funciona esto, consideremos tres embeddings: <em>A</em>, <em>B</em> y <em>C</em>. Estos tres son vectores completos de punto flotante, no binarizados. Ahora, digamos que la distancia de <em>A</em> a <em>B</em> es mayor que la distancia de <em>B</em> a <em>C</em>. Con embeddings, típicamente usamos la <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">distancia coseno</a>, entonces: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>Si binarizamos <em>A</em>, <em>B</em> y <em>C</em>, podemos medir la distancia más eficientemente con la <a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">distancia de Hamming</a>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Distancia de Hamming en un cubo. Izquierda: La distancia de A a B es 1. Derecha: La distancia de B a C es 2.</span></figcaption></figure><p>Llamemos <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em> y <em>C<sub>bin</sub></em> a las versiones binarizadas de <em>A</em>, <em>B</em> y <em>C</em>.</p>\n<p>Para vectores binarios, si la distancia coseno entre <em>A<sub>bin</sub></em> y <em>B<sub>bin</sub></em> es mayor que entre <em>B<sub>bin</sub></em> y <em>C<sub>bin</sub></em>, entonces la distancia de Hamming entre <em>A<sub>bin</sub></em> y <em>B<sub>bin</sub></em> es mayor o igual que la distancia de Hamming entre <em>B<sub>bin</sub></em> y <em>C<sub>bin</sub></em>.</p>\n<p>Entonces si: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>entonces para distancias de Hamming: </p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Idealmente, cuando binarizamos embeddings, queremos que las mismas relaciones con embeddings completos se mantengan para los embeddings binarios como para los completos. Esto significa que si una distancia es mayor que otra para el coseno de punto flotante, debería ser mayor para la distancia de Hamming entre sus equivalentes binarizados:</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>No podemos hacer que esto sea verdadero para todos los tripletes de embeddings, pero podemos hacerlo verdadero para casi todos ellos.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Los puntos azules corresponden a vectores completos de punto flotante y los rojos a sus equivalentes binarizados. </span></figcaption></figure><p>Con un vector binario, podemos tratar cada dimensión como presente (un uno) o ausente (un cero). Cuanto más distantes estén dos vectores entre sí en forma no binaria, mayor será la probabilidad de que en cualquier dimensión, uno tenga un valor positivo y el otro un valor negativo. Esto significa que en forma binaria, probablemente habrá más dimensiones donde uno tenga un cero y el otro un uno. Esto los hace más distantes por distancia de Hamming.</p><p>Lo opuesto se aplica a vectores que están más cerca entre sí: Cuanto más cerca estén los vectores no binarios, mayor será la probabilidad de que en cualquier dimensión ambos tengan ceros o ambos tengan unos. Esto los hace más cercanos por distancia de Hamming.</p><p>Los modelos de Jina Embeddings son tan adecuados para la binarización porque los entrenamos usando minería negativa y otras prácticas de ajuste fino para aumentar especialmente la distancia entre cosas diferentes y reducir la distancia entre las similares. Esto hace que los embeddings sean más robustos, más sensibles a las similitudes y diferencias, y hace que la distancia de Hamming entre embeddings binarios sea más proporcional a la distancia coseno entre los no binarios.</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">¿Cuánto Puedo Ahorrar con los Embeddings Binarios de Jina AI?</h2><p>Adoptar los modelos de embedding binario de Jina AI no solo reduce la latencia en aplicaciones sensibles al tiempo, sino que también produce beneficios considerables en costos, como se muestra en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Modelo</th>\n<th>Memoria por<br/>250 millones<br/>de embeddings</th>\n<th>Promedio de<br/>benchmark de<br/>recuperación</th>\n<th>Precio estimado en AWS<br/>($3.8 por GB/mes<br/>con instancias x2gb)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Embeddings de punto flotante de 32 bits</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>Embeddings binarios</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Este ahorro de más del 95% viene acompañado de solo ~10% de reducción en la precisión de recuperación.</p><p>Estos ahorros son incluso mayores que usando vectores binarizados de <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">el modelo Ada 2 de OpenAI</a> o <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Embed v3 de Cohere</a>, ambos producen embeddings de salida de 1024 dimensiones o más. Los embeddings de Jina AI tienen solo 768 dimensiones y aun así tienen un rendimiento comparable a otros modelos, haciéndolos más pequeños incluso antes de la cuantización para la misma precisión.</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Los vectores binarios ahorran memoria, tiempo de cómputo, ancho de banda de transmisión y almacenamiento en disco, proporcionando beneficios financieros en varias categorías</strong></b>. </div></div><p>Estos ahorros también son ambientales, usando menos materiales raros y menos energía.</p><h2 id=\"get-started\">Empezar</h2><p>Para obtener embeddings binarios usando la <a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">API de Jina Embeddings</a>, solo agregue el parámetro <code>encoding_type</code> a su llamada API, con el valor <code>binary</code> para obtener el embedding binarizado codificado como enteros con signo, o <code>ubinary</code> para enteros sin signo.</p><h3 id=\"directly-access-jina-embedding-api\">Acceder Directamente a la API de Jina Embedding</h3><p>Usando <code>curl</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>O mediante la API de Python <code>requests</code>:</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>Con la <code>request</code> de Python anterior, obtendrá la siguiente respuesta al inspeccionar <code>response.json()</code>:</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>Estos son dos vectores de embedding binarios almacenados como 96 enteros de 8 bits con signo. Para desempaquetarlos a 768 0's y 1's, necesita usar la biblioteca <code>numpy</code>:</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>El resultado es un vector de 768 dimensiones con solo 0's y 1's:</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Usando Cuantización Binaria en Qdrant</h3><p>También puede usar la <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">biblioteca de integración de Qdrant</a> para poner embeddings binarios directamente en su almacén de vectores Qdrant. Como Qdrant ha implementado internamente <code>BinaryQuantization</code>, puede usarlo como una configuración preestablecida para toda la colección de vectores, permitiéndole recuperar y almacenar vectores binarios sin ningún otro cambio en su código.</p><p>Vea el código de ejemplo a continuación para saber cómo:</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Proporciona la clave API de Jina y elige uno de los modelos disponibles.\n# Puedes obtener una clave de prueba gratuita aquí: https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # o \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 para la variante pequeña\n\n# Obtener embeddings desde la API\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Tu texto va aquí\", \"Puedes enviar múltiples textos\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Indexar los embeddings en Qdrant\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>Para configurar la búsqueda, debes usar los parámetros <code>oversampling</code> y <code>rescore</code>:</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">Uso de LlamaIndex</h3><p>Para usar embeddings binarios de Jina con LlamaIndex, establece el parámetro <code>encoding_queries</code> como <code>binary</code> al instanciar el objeto <code>JinaEmbedding</code>:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# Puedes obtener una clave de prueba gratuita en https://jina.ai/embeddings/\nJINA_API_KEY = \"&lt;TU CLAVE API&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Texto de consulta aquí')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">Otras Bases de Datos Vectoriales que Soportan Embeddings Binarios</h3><p>Las siguientes bases de datos vectoriales proporcionan soporte nativo para vectores binarios:</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB by DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">Ejemplo</h2><p>Para mostrarte los embeddings binarios en acción, tomamos una selección de resúmenes de <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a>, y obtuvimos tanto vectores de punto flotante de 32 bits como vectores binarios usando <code>jina-embeddings-v2-base-en</code>. Luego los comparamos con los embeddings de una consulta de ejemplo: \"3D segmentation\".</p><p>Puedes ver en la tabla siguiente que las tres primeras respuestas son las mismas y cuatro de las cinco principales coinciden. El uso de vectores binarios produce coincidencias casi idénticas en los primeros resultados.</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>Rank</th>\n<th>Hamming<br/>dist.</th>\n<th>Matching Text</th>\n<th>Cosine</th>\n<th>Matching text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "32 bits es mucha precisión para algo tan robusto e inexacto como un modelo de AI. ¡Así que nos deshicimos de 31 de ellos! Los embeddings binarios son más pequeños, más rápidos y altamente eficientes.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
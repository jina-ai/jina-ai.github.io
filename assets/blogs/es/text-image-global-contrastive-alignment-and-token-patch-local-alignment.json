{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "Alineación contrastiva global de texto-imagen y alineación local de token-parche",
  "html": "<p>Mientras experimentaba con modelos al estilo <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali</a>, uno de nuestros ingenieros creó una visualización usando nuestro modelo <code>jina-clip-v2</code> recientemente lanzado. Mapeó la similitud entre los embeddings de tokens y los embeddings de parches para pares dados de imagen-texto, creando superposiciones de mapas de calor que produjeron algunas perspectivas visuales intrigantes.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Desafortunadamente, <strong>esto es solo una visualización heurística</strong> - no un mecanismo explícito o garantizado. Si bien la alineación contrastiva global tipo CLIP puede (y a menudo lo hace) crear <em>incidentalmente</em> alineaciones locales aproximadas entre parches y tokens, esto es un <strong>efecto secundario no intencionado</strong> en lugar de un objetivo deliberado del modelo. Permítanme explicar por qué.</p><h2 id=\"understand-the-code\">Entender el Código</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Analicemos lo que hace el código a alto nivel. Ten en cuenta que <code>jina-clip-v2</code> en realidad no expone ninguna API para acceder a embeddings a nivel de token o parche por defecto - esta visualización requirió algunas modificaciones posteriores para hacerla funcionar.</p><p><strong>Calcular embeddings a nivel de palabra</strong></p><p>Al establecer <code>model.text_model.output_tokens = True</code>, llamar a <code>text_model(x=...,)[1]</code> devolverá un segundo elemento <code>(batch_size, seq_len, embed_dim)</code> para los embeddings de tokens. Así que toma una oración de entrada, la tokeniza con el tokenizador Jina CLIP, y luego agrupa los subtokens de nuevo en \"palabras\" promediando los embeddings de tokens correspondientes. Detecta el inicio de una nueva palabra verificando si la cadena del token comienza con el carácter <code>_</code> (típico en tokenizadores basados en SentencePiece). Produce una lista de embeddings a nivel de palabra y una lista de palabras (de modo que \"Perro\" es un embedding, \"y\" es un embedding, etc.).</p><p><strong>Calcular embeddings a nivel de parche</strong></p><p>Para la torre de imágenes, <code>vision_model(..., return_all_features=True)</code> devolverá <code>(batch_size, n_patches+1, embed_dim)</code>, donde el primer token es el token <code>[CLS]</code>. A partir de eso, el código extrae los embeddings para cada parche (es decir, los tokens de parche del transformador de visión). Luego reshapea estos embeddings de parche en una cuadrícula 2D, <code>patch_side × patch_side</code>, que luego se escala para coincidir con la resolución de la imagen original.</p><p><strong>Visualizar similitud palabra-parche</strong></p><p>El cálculo de similitud y la posterior generación del mapa de calor son técnicas de interpretabilidad \"post-hoc\" estándar: seleccionas un embedding de texto, calculas la similitud del coseno con cada embedding de parche, y luego generas un mapa de calor que muestra qué parches tienen la mayor similitud con ese embedding de token específico. Finalmente, recorre cada token en la oración, resalta ese token en negrita a la izquierda, y superpone el mapa de calor basado en similitud sobre la imagen original a la derecha. Todos los frames se compilan en un GIF animado.</p><h2 id=\"is-it-meaningful-explainability\">¿Es una Explicabilidad Significativa?</h2><p>Desde un punto de vista de <em>código puro</em>, sí, la lógica es coherente y producirá un mapa de calor para cada token. Obtendrás una serie de frames que resaltan las similitudes de parches, así que el script \"hace lo que dice que hace\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Mirando los ejemplos anteriores, vemos que palabras como <code>moon</code> y <code>branches</code> parecen alinearse bien con sus parches visuales correspondientes en la imagen original. Pero aquí está la pregunta clave: ¿es esta una alineación significativa, o solo estamos viendo una coincidencia afortunada?</p><p>Esta es una pregunta más profunda. Para entender las advertencias, recordemos <strong>cómo se entrena CLIP</strong>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagrama del modelo JINA-CLIP-V2 mostrando etapas desde la entrada hasta la salida para el procesamiento de texto en inglés y multilingüe.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 combina un codificador de texto (Jina XLM-RoBERTa, 561M parámetros) y un codificador de visión (EVA02-L14, 304M parámetros). Cada cuadrado de color a la derecha representa una oración o imagen completa en el lote - no tokens o parches individuales.</span></figcaption></figure><ul><li>CLIP usa alineación contrastiva <strong>global</strong> entre una imagen completa y un texto completo. Durante el entrenamiento, el codificador de imagen produce un único vector (representación agrupada), y el codificador de texto produce otro vector único; CLIP está entrenado para que estos coincidan para pares texto-imagen correspondientes y no coincidan en caso contrario.</li><li><strong>No hay supervisión explícita a nivel de 'el parche X corresponde al token Y'.</strong> El modelo no está entrenado directamente para resaltar \"esta región de la imagen es el perro, esa región es el gato\", etc. En su lugar, se le enseña que la representación completa de la imagen debe coincidir con la representación completa del texto.</li><li>Debido a que la arquitectura de CLIP es un Vision Transformer en el lado de la imagen y un transformador de texto en el lado del texto—ambos formando codificadores separados—no hay un módulo de atención cruzada que alinee nativamente parches con tokens. En su lugar, obtienes puramente <strong>auto-atención</strong> en cada torre, más una proyección final para los embeddings globales de imagen o texto.</li></ul><p>En resumen, esta es una visualización heurística. El hecho de que cualquier embedding de parche pueda estar cerca o lejos de un embedding de token particular es algo emergente. Es más un <em>truco de interpretabilidad post-hoc</em> que una \"atención\" robusta u oficial del modelo.</p><h2 id=\"why-might-local-alignment-emerge\">¿Por Qué Podría Emerger la Alineación Local?</h2><p>Entonces, ¿por qué podríamos a veces detectar alineaciones locales a nivel palabra-parche? Aquí está el asunto: aunque CLIP está entrenado en un objetivo contrastivo <em>global</em> de imagen-texto, aún usa auto-atención (en codificadores de imagen basados en ViT) y capas transformer (para texto). Dentro de estas capas de auto-atención, diferentes partes de las representaciones de imagen pueden interactuar entre sí, al igual que lo hacen las palabras en las representaciones de texto. A través del entrenamiento en conjuntos de datos masivos de imagen-texto, el modelo naturalmente desarrolla estructuras latentes internas que lo ayudan a coincidir imágenes generales con sus descripciones de texto correspondientes.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>La <strong>alineación local</strong> puede aparecer en estas representaciones latentes por al menos dos razones:</p><ol><li><strong>Patrones de co-ocurrencia</strong>: Si un modelo ve muchas imágenes de \"perros\" junto a muchas imágenes de \"gatos\" (a menudo etiquetadas o descritas con esas palabras), puede aprender características latentes que corresponden aproximadamente a estos conceptos. Así, el embedding para \"perro\" podría volverse cercano a parches locales que muestran una forma o textura similar a un perro. Esto <em>no</em> está supervisado explícitamente a nivel de parche, pero emerge de la asociación repetida entre pares de imágenes/texto de perros.</li><li><strong>Self-attention</strong>: En los Vision Transformers, los parches prestan atención entre sí. Los parches distintivos (como el rostro de un perro) pueden terminar con una \"firma\" latente consistente, porque el modelo está tratando de producir una única representación globalmente precisa de toda la escena. Si esto ayuda a minimizar la pérdida contrastiva general, se reforzará.</li></ol><h2 id=\"theoretical-analysis\">Análisis Teórico</h2><p>El objetivo de aprendizaje contrastivo de CLIP busca maximizar la similitud del coseno entre pares de imagen-texto coincidentes mientras la minimiza para pares no coincidentes. Asumamos que los codificadores de texto e imagen producen embeddings de tokens y parches respectivamente:</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>La similitud global puede representarse como un agregado de similitudes locales:</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Cuando pares específicos de token-parche co-ocurren frecuentemente a través de los datos de entrenamiento, el modelo refuerza su similitud mediante actualizaciones acumulativas del gradiente:</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, donde $C$ es el número de co-ocurrencias. Esto lleva a que $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$ aumente significativamente, promoviendo una alineación local más fuerte para estos pares. Sin embargo, la pérdida contrastiva distribuye las actualizaciones del gradiente entre todos los pares token-parche, limitando la fuerza de las actualizaciones para cualquier par específico:</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>Esto evita el refuerzo significativo de similitudes individuales entre token-parche.</p><h2 id=\"conclusion\">Conclusión</h2><p>Las visualizaciones token-parche de CLIP aprovechan una alineación incidental y emergente entre las representaciones de texto e imagen. Esta alineación, aunque intrigante, proviene del <strong>entrenamiento contrastivo global</strong> de CLIP y carece de la robustez estructural necesaria para una explicabilidad precisa y confiable. Las visualizaciones resultantes a menudo exhiben <strong>ruido e inconsistencia</strong>, limitando su utilidad para aplicaciones interpretativas en profundidad.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Los modelos de interacción tardía como <strong>ColBERT</strong> y <strong>ColPali</strong> abordan estas limitaciones mediante la <strong>incorporación arquitectónica de alineaciones explícitas y detalladas</strong> entre tokens de texto y parches de imagen. Al procesar las modalidades de forma independiente y realizar cálculos de similitud específicos en una etapa posterior, estos modelos aseguran que cada token de texto esté significativamente asociado con las regiones relevantes de la imagen.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP puede visualizar similitudes entre tokens y parches, sin embargo, se trata más de un truco de interpretabilidad post-hoc que de una \"atención\" robusta u oficial del modelo. Aquí la razón.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
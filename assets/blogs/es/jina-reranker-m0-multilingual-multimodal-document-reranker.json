{
  "slug": "jina-reranker-m0-multilingual-multimodal-document-reranker",
  "id": "67ea5eb45dcba60001c30f0a",
  "uuid": "b710acf7-f8e7-4588-bc54-30a1fbe42eca",
  "title": "jina-reranker-m0: Reranqueador multilingüe y multimodal de documentos",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-reranker-m0\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-reranker-m0 · Hugging Face</div><div class=\"kg-bookmark-description\">Estamos en un viaje para avanzar y democratizar la inteligencia artificial a través del código abierto y la ciencia abierta.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-34.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-reranker-m0.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Hoy lanzamos <code>jina-reranker-m0</code>, nuestro nuevo modelo de reordenamiento multilingüe y multimodal para <strong>clasificar documentos visuales en múltiples idiomas: </strong>acepta una consulta junto con una colección de imágenes de documentos visualmente ricos, incluyendo páginas con texto, figuras, tablas, infografías y varios diseños a través de múltiples dominios y más de 29 idiomas. Produce una lista ordenada de documentos según su relevancia para la consulta de entrada. En comparación con <code>jina-reranker-v2-base-multilingual</code>, <code>jina-reranker-m0</code> también mejora la reordenación de texto para contenido multilingüe, documentos largos y tareas de búsqueda de código.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/all-benchmarks--6-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1714\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/04/all-benchmarks--6-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/04/all-benchmarks--6-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/04/all-benchmarks--6-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/04/all-benchmarks--6-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">El rendimiento de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> en los benchmarks de recuperación visual ViDoRe, MBEIR y Winoground muestra sus capacidades en diversas tareas de recuperación multimodal que abarcan múltiples dominios e idiomas. Cada punto representa puntuaciones de rendimiento para diferentes tipos/tareas de documentos visuales. Los diagramas de caja ilustran la distribución de estas puntuaciones, con los números resaltados indicando el rendimiento promedio (media). Para resultados completos de los benchmarks, consulte el apéndice de esta publicación.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/model-perf-boxplot--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2338\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/04/model-perf-boxplot--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/04/model-perf-boxplot--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/04/model-perf-boxplot--13-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/04/model-perf-boxplot--13-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Este diagrama de caja muestra el rendimiento de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> en cuatro benchmarks de reordenamiento de solo texto. Cada benchmark puede incluir múltiples conjuntos de datos, idiomas o tareas, representados por puntos individuales dentro del diagrama de caja. El diagrama muestra la distribución de estas puntuaciones, con el número resaltado mostrando el rendimiento promedio (media). Mientras que la mayoría de los benchmarks usan NDCG@10 como métrica de rendimiento, MKQA usa recall@10 en su lugar, ya que los datos de anotación de MKQA no admiten el cálculo de NDCG (la evaluación oficial usa recall, que determina la relevancia del documento mediante heurísticas). Los resultados completos de los benchmarks están disponibles en el apéndice de esta publicación.</span></figcaption></figure><h2 id=\"new-architecture\">Nueva Arquitectura</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">La arquitectura de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> está construida sobre Qwen2-VL-2B y consta de 2.1 mil millones de parámetros. Este modelo clasifica eficientemente los documentos evaluando tanto sus elementos visuales como textuales en relación con las consultas, utilizando comparación por pares.</span></figcaption></figure><p>A diferencia de <code>jina-reranker-v2-base-multilingual</code>, <code>jina-reranker-m0</code> cambia de la arquitectura clásica de codificador cruzado a un modelo de lenguaje visual basado únicamente en decodificador. Aprovecha el codificador y proyector de visión preentrenado de Qwen2-VL, ajustó su LLM con LoRA, y post-entrenó un MLP para generar logits de clasificación que miden la relevancia consulta-documento. Esto proporciona un <strong>modelo discriminativo</strong> optimizado para tareas de clasificación.</p>\n<!--kg-card-begin: html-->\n<table><thead>\n  <tr>\n    <th></th>\n    <th><code>jina-reranker-m0</code></th>\n    <th><code>jina-reranker-v2</code></th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td>Arquitectura</td>\n    <td>Vision Language Model</td>\n    <td>Cross-Encoder</td>\n  </tr>\n  <tr>\n    <td>Modelo base</td>\n    <td>Qwen2-VL-2B</td>\n    <td>Jina-XLM-RoBERTa</td>\n  </tr>\n  <tr>\n    <td>Parámetros</td>\n    <td>2.4 B</td>\n    <td>278 M</td>\n  </tr>\n  <tr>\n    <td>Longitud máxima de contexto (consulta + documento)</td>\n    <td>10,240</td>\n    <td>8,192</td>\n  </tr>\n  <tr>\n    <td>Máximo de parches de imagen (resolución dinámica)</td>\n    <td>768 × 28 × 28</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Soporte multilingüe</td>\n    <td>✅</td>\n    <td>✅</td>\n  </tr>\n  <tr>\n    <td>Tareas soportadas</td>\n    <td>Text2Text, Text2Image, Image2Text, Text2Mixed</td>\n    <td>Text2Text</td>\n  </tr>\n</tbody></table>\n<!--kg-card-end: html-->\n<p>Esta nueva arquitectura permite a <code>jina-reranker-m0</code> manejar hasta 32K tokens, combinando perfectamente entradas visuales y textuales. El modelo admite imágenes desde un tamaño mínimo de 56×56 píxeles hasta resolución 4K. Al procesar imágenes, el ViT y el proyector condensan tokens adyacentes de 2×2 en tokens visuales únicos para la entrada del LLM. Tokens especiales como <code>&lt;|vision_start|&gt;</code> y <code>&lt;|vision_end|&gt;</code> marcan claramente los límites de los tokens visuales, permitiendo que el modelo de lenguaje procese adecuadamente la información visual y realice razonamiento multimodal sofisticado que integra elementos tanto visuales como textuales.</p><p>Esta arquitectura también resuelve efectivamente <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">el problema de la brecha de modalidad</a> que plagó modelos anteriores como <code>jina-clip-v1</code> y <code>jina-clip-v2</code>. Anteriormente, las imágenes se agrupaban cerca de otras imágenes mientras que el texto se agrupaba cerca de otro texto en el espacio de representación, creando una desconexión. Esto significaba que cuando tus documentos candidatos contenían tanto imágenes como texto, recuperar imágenes usando consultas de texto era problemático. Con <code>jina-reranker-m0</code>, ahora puedes clasificar imágenes y documentos juntos sin preocuparte por esta brecha, creando una experiencia de búsqueda multimodal verdaderamente unificada.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">En sistemas de recuperación multimodal, una \"brecha de modalidad\" se refiere a la diferencia en cómo el modelo califica la similitud texto-a-texto versus texto-a-imagen. Mirando la imagen de la izquierda (</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">), hay una clara separación entre las dos distribuciones: La distribución de similitud texto-a-texto (roja) alcanza su pico alrededor de 0.35. La similitud texto-a-imagen (azul) alcanza su pico alrededor de 0.65-0.7. Esta separación significativa indica una gran brecha de modalidad - el modelo califica los pares texto-a-texto y texto-a-imagen en rangos fundamentalmente diferentes. Esto hace difícil comparar directamente puntuaciones entre modalidades. En un sistema sin brecha de modalidad (por ejemplo, `, esperaríamos que las distribuciones se superpongan en gran medida, lo que significa que el modelo califica ambos tipos de pares en rangos similares basándose puramente en la relevancia, no en el tipo de modalidad.</span></figcaption></figure><p>Vale la pena señalar que nuestro entrenamiento se limitó a un máximo de 10K tokens de entrada, con hasta 768 tokens por imagen (entre los marcadores <code>&lt;|vision_start|&gt;</code> y <code>&lt;|vision_end|&gt;</code>). Además, no entrenamos específicamente el modelo para tareas de reordenamiento <code>image-to-image</code>, <code>image-to-multimodal</code>, o <code>text-to-multimodal</code>. En este contexto, \"multimodal\" se refiere a un solo documento que contiene tanto tokens de imagen como de texto en la entrada. Mirando todas las posibles combinaciones de tokens de imagen y texto tanto en consultas como en documentos, podemos resumir el rango completo de tareas soportadas por <code>jina-reranker-m0</code> en la tabla siguiente.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading--96-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/04/Heading--96-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/04/Heading--96-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/04/Heading--96-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> admite una amplia gama de combinaciones de consultas y documentos de entrada para fines de reordenamiento. Logra un rendimiento de vanguardia en tareas de texto a texto, texto a imagen, imagen a texto y texto a contenido unimodal mixto, gracias a un extenso entrenamiento. El modelo también maneja otras combinaciones de entrada de manera zero-shot - la arquitectura admite estas combinaciones de tokens, aunque no hemos entrenado específicamente para estas tareas.</span></figcaption></figure><p>En nuestras pruebas, encontramos evidencia que sugiere que el modelo puede extrapolar a estas tareas de clasificación no entrenadas, pero cualquier efectividad en estas áreas debe verse como resultado de la transferibilidad zero-shot del modelo o efectos secundarios no intencionados del entrenamiento. No hemos realizado evaluaciones serias del rendimiento del modelo en estas tareas, y planeamos explorar estas capacidades más a fondo en investigaciones futuras.</p><h2 id=\"getting-started\">Primeros Pasos</h2><h3 id=\"via-api\">A través de API</h3><p>El código a continuación muestra cómo calcular puntuaciones de relevancia entre la consulta <code>\"small language model data extraction\"</code> y una colección de imágenes y documentos de texto. Puedes pasar una cadena de texto, una imagen codificada en base64 o una URL de imagen. Los nuevos usuarios pueden obtener una clave API de Jina con 1 millón de tokens gratuitos. Si bien nuestra API no admite el uso de imágenes como consultas, puedes usar imágenes como consultas cuando accedes al modelo a través de la biblioteca Hugging Face Transformers.</p><pre><code class=\"language-bash\">curl -X POST \\\n  https://api.jina.ai/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d '{\n  \"model\": \"jina-reranker-m0\",\n  \"query\": \"small language model data extraction\",\n  \"documents\": [\n    {\n      \"image\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/handelsblatt-preview.png\"\n    },\n    {\n      \"image\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/paper-11.png\"\n    },\n    {\n      \"image\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/wired-preview.png\"\n    },\n    {\n      \"text\": \"We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The models effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.\"\n    },\n    {\n      \"image\": \"https://jina.ai/blog-banner/using-deepseek-r1-reasoning-model-in-deepsearch.webp\"\n    },\n    {\n      \"text\": \"数据提取么？为什么不用正则啊，你用正则不就全解决了么？\"\n    },\n    {\n      \"text\": \"During the California Gold Rush, some merchants made more money selling supplies to miners than the miners made finding gold.\"\n    },\n    {\n      \"text\": \"Die wichtigsten Beiträge unserer Arbeit sind zweifach: Erstens führen wir eine neuartige dreistufige Datensynthese-Pipeline namens Draft-Refine-Critique ein, die durch iterative Verfeinerung hochwertige Trainingsdaten generiert; und zweitens schlagen wir eine umfassende Trainingsstrategie vor, die kontinuierliches Vortraining zur Längenerweiterung, überwachtes Feintuning mit spezialisierten Kontrollpunkten, direkte Präferenzoptimierung (DPO) und iteratives Self-Play-Tuning kombiniert. Um die weitere Forschung und Anwendung der strukturierten Inhaltsextraktion zu erleichtern, ist das Modell auf Hugging Face öffentlich verfügbar.\"\n    }\n  ],\n  \"return_documents\": false\n}'</code></pre><p>La respuesta se muestra a continuación, donde el primer resultado <code>index=1</code> corresponde a nuestra captura de pantalla del paper de ReaderLM-v2 <a href=\"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/paper-11.png\">paper screenshot</a>. </p><pre><code class=\"language-json\">{\"model\":\"jina-reranker-m0\",\"usage\":{\"total_tokens\":2829},\"results\":[{\"index\":1,\"relevance_score\":0.9587112551898949},{\"index\":3,\"relevance_score\":0.9337408271911014},{\"index\":7,\"relevance_score\":0.8922925217195924},{\"index\":2,\"relevance_score\":0.8891905997562045},{\"index\":0,\"relevance_score\":0.8827516945848907},{\"index\":4,\"relevance_score\":0.8701035914834407},{\"index\":6,\"relevance_score\":0.8676828987527296},{\"index\":5,\"relevance_score\":0.8455347349164652}]}</code></pre><h3 id=\"via-csp-marketplaces\">A través de Marketplaces CSP</h3><p><code>jina-reranker-m0</code> estará pronto disponible directamente en AWS, Azure y GCP a los precios listados allí.</p><h3 id=\"via-huggingface\">A través de HuggingFace</h3><p>También puedes usar el modelo localmente desde nuestra página de Hugging Face. Hemos preparado un notebook de Google Colab que demuestra cómo funciona. En comparación con nuestra API web, usar el modelo localmente ofrece mayor flexibilidad, como la capacidad de usar imágenes como consultas y trabajar con documentos multimodales.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1gNTJHbdYSdgOEAea7kB6XaW56Zala0vk?usp=sharing\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-33.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-8.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"evaluation\">Evaluación</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://docs.google.com/spreadsheets/d/1KrCD7l0lhzMkyg3z-gEDmymxe4Eun9Z-C0kU3_cxw7Q/edit?usp=sharing\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">[public]-jina-reranker-m0-evaluation-results</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/spreadsheets_2023q4-1.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Google Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/AHkbwyKUKD8mc67_5eRgiiBYvZFKdug_jMKmBHiEesOvQ3bVWmAAKMu-afa1748S_WJXuc4UdNjKMqEsIQnlnf2X5OQsA0dmDJirjwvEkrgBMhQlJwXS8VM-w1200-h630-p\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Las evaluaciones completas se pueden encontrar en esta hoja de cálculo de Google.</span></p></figcaption></figure><h3 id=\"beir-text2text-english-only\">BEIR (Texto a Texto, solo inglés)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2104.08663\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</div><div class=\"kg-bookmark-description\">Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-13.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Nandan Thakur</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>BEIR es un benchmark heterogéneo para la recuperación de información, diseñado para evaluar la versatilidad y robustez de los modelos IR. Contiene un conjunto diverso de datasets de varios dominios y se centra en la evaluación zero-shot. Se utilizan métricas de evaluación estandarizadas como NDCG, Recall@K y MRR.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th style=\"text-align: right\">AVG (NDCG@10)</th>\n<th style=\"text-align: right\">TREC-COVID</th>\n<th style=\"text-align: right\">NFCorpus</th>\n<th style=\"text-align: right\">NQ</th>\n<th style=\"text-align: right\">HotpotQA</th>\n<th style=\"text-align: right\">FiQA</th>\n<th style=\"text-align: right\">ArguAna</th>\n<th style=\"text-align: right\">Touche-2020</th>\n<th style=\"text-align: right\">DBPedia</th>\n<th style=\"text-align: right\">SCIDOCS</th>\n<th style=\"text-align: right\">FEVER</th>\n<th style=\"text-align: right\">Climate-FEVER</th>\n<th style=\"text-align: right\">SciFact</th>\n<th style=\"text-align: right\">Quora</th>\n</tr>\n</thead>\n<tbody>\n  <tr>\n<td>jina-reranker-m0</td>\n<td style=\"text-align: right\"><strong>58.95</strong></td>\n<td style=\"text-align: right\"><strong>84.17</strong></td>\n<td style=\"text-align: right\"><strong>41.03</strong></td>\n<td style=\"text-align: right\"><strong>72.25</strong></td>\n<td style=\"text-align: right\">76.99</td>\n<td style=\"text-align: right\"><strong>51.62</strong></td>\n<td style=\"text-align: right\">40.69</td>\n<td style=\"text-align: right\">31.79</td>\n<td style=\"text-align: right\"><strong>49.34</strong></td>\n<td style=\"text-align: right\"><strong>22.91</strong></td>\n<td style=\"text-align: right\">91.14</td>\n<td style=\"text-align: right\">36.42</td>\n<td style=\"text-align: right\"><strong>79.94</strong></td>\n<td style=\"text-align: right\">88.01</td>\n</tr>\n<tr>\n<td>jina-embeddings-v3 (1024 tokens)</td>\n<td style=\"text-align: right\">55.81</td>\n<td style=\"text-align: right\">77.81</td>\n<td style=\"text-align: right\">36.65</td>\n<td style=\"text-align: right\">64.31</td>\n<td style=\"text-align: right\">64.63</td>\n<td style=\"text-align: right\">47.47</td>\n<td style=\"text-align: right\"><strong>54.31</strong></td>\n<td style=\"text-align: right\">26.55</td>\n<td style=\"text-align: right\">41.07</td>\n<td style=\"text-align: right\">19.91</td>\n<td style=\"text-align: right\">89.00</td>\n<td style=\"text-align: right\"><strong>42.33</strong></td>\n<td style=\"text-align: right\">72.4</td>\n<td style=\"text-align: right\"><strong>89.06</strong></td>\n</tr>\n<tr>\n<td>bge-reranker-v2-m3</td>\n<td style=\"text-align: right\">56.51</td>\n<td style=\"text-align: right\">82.19</td>\n<td style=\"text-align: right\">34.33</td>\n<td style=\"text-align: right\">69.52</td>\n<td style=\"text-align: right\"><strong>77.89</strong></td>\n<td style=\"text-align: right\">45.45</td>\n<td style=\"text-align: right\">36.21</td>\n<td style=\"text-align: right\"><strong>33.12</strong></td>\n<td style=\"text-align: right\">46.72</td>\n<td style=\"text-align: right\">17.79</td>\n<td style=\"text-align: right\">91.03</td>\n<td style=\"text-align: right\">38.69</td>\n<td style=\"text-align: right\">72.64</td>\n<td style=\"text-align: right\">89.10</td>\n</tr>\n<tr>\n<td>jina-reranker-v2-multilingual</td>\n<td style=\"text-align: right\">57.06</td>\n<td style=\"text-align: right\">80.53</td>\n<td style=\"text-align: right\">37.17</td>\n<td style=\"text-align: right\">67.39</td>\n<td style=\"text-align: right\">76.17</td>\n<td style=\"text-align: right\">46.48</td>\n<td style=\"text-align: right\">39.28</td>\n<td style=\"text-align: right\">32.35</td>\n<td style=\"text-align: right\">47.81</td>\n<td style=\"text-align: right\">20.03</td>\n<td style=\"text-align: right\"><strong>93.02</strong></td>\n<td style=\"text-align: right\">37.17</td>\n<td style=\"text-align: right\">76.50</td>\n<td style=\"text-align: right\">87.83</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"miracl-text2text-multilingual-18-languages\">MIRACL (Text2Text, Multilingüe, 18 idiomas)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2210.09984\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Making a MIRACL: Recuperación de Información Multilingüe a través de un Continuo de Idiomas</div><div class=\"kg-bookmark-description\">MIRACL (Recuperación de Información Multilingüe a través de un Continuo de Idiomas) es un dataset multilingüe que hemos construido para el desafío WSDM 2023 Cup que se centra en la recuperación ad hoc a través de 18 idiomas diferentes, que en conjunto abarcan más de tres mil millones de hablantes nativos en todo el mundo. Estos idiomas tienen tipologías diversas, provienen de muchas familias lingüísticas diferentes y están asociados con cantidades variables de recursos disponibles -- incluyendo lo que los investigadores típicamente caracterizan como idiomas de altos recursos así como idiomas de bajos recursos. Nuestro dataset está diseñado para apoyar la creación y evaluación de modelos para recuperación monolingüe, donde las consultas y los corpus están en el mismo idioma. En total, hemos recopilado más de 700k juicios de relevancia de alta calidad para alrededor de 77k consultas sobre Wikipedia en estos 18 idiomas, donde todas las evaluaciones han sido realizadas por hablantes nativos contratados por nuestro equipo. Nuestro objetivo es impulsar la investigación que mejorará la recuperación a través de un continuo de idiomas, mejorando así las capacidades de acceso a la información para diversas poblaciones en todo el mundo, particularmente aquellas que tradicionalmente han estado desatendidas. Este documento general describe el dataset y las líneas base que compartimos con la comunidad. El sitio web de MIRACL está activo en http://miracl.ai/.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-12.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xinyu Zhang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-8.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>MIRACL es un extenso conjunto de datos multilingüe para la recuperación de información ad hoc en 18 idiomas. Abarca más de tres mil millones de hablantes nativos y cuenta con detalladas anotaciones humanas. Se centra en tareas de recuperación monolingüe.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th style=\"text-align: right\">AVG (NDCG@10)</th>\n<th style=\"text-align: right\">ar</th>\n<th style=\"text-align: right\">bn</th>\n<th style=\"text-align: right\">en</th>\n<th style=\"text-align: right\">es</th>\n<th style=\"text-align: right\">fa</th>\n<th style=\"text-align: right\">fi</th>\n<th style=\"text-align: right\">fr</th>\n<th style=\"text-align: right\">hi</th>\n<th style=\"text-align: right\">id</th>\n<th style=\"text-align: right\">ja</th>\n<th style=\"text-align: right\">ko</th>\n<th style=\"text-align: right\">ru</th>\n<th style=\"text-align: right\">sw</th>\n<th style=\"text-align: right\">te</th>\n<th style=\"text-align: right\">th</th>\n<th style=\"text-align: right\">zh</th>\n<th style=\"text-align: right\">de</th>\n<th style=\"text-align: right\">yo</th>\n</tr>\n</thead>\n<tbody>\n  <tr>\n<td>jina-reranker-m0</td>\n<td style=\"text-align: right\">66.75</td>\n<td style=\"text-align: right\">79.78</td>\n<td style=\"text-align: right\">78.01</td>\n<td style=\"text-align: right\">59.21</td>\n<td style=\"text-align: right\">53.56</td>\n<td style=\"text-align: right\">58.80</td>\n<td style=\"text-align: right\">78.00</td>\n<td style=\"text-align: right\">56.66</td>\n<td style=\"text-align: right\">62.83</td>\n<td style=\"text-align: right\">54.92</td>\n<td style=\"text-align: right\">66.51</td>\n<td style=\"text-align: right\">72.86</td>\n<td style=\"text-align: right\">67.26</td>\n<td style=\"text-align: right\">59.04</td>\n<td style=\"text-align: right\">70.19</td>\n<td style=\"text-align: right\">80.37</td>\n<td style=\"text-align: right\">64.51</td>\n<td style=\"text-align: right\">58.50</td>\n<td style=\"text-align: right\">80.44</td>\n</tr>\n<tr>\n<td>jina-embeddings-v3 (8192 tokens)</td>\n<td style=\"text-align: right\">58.90</td>\n<td style=\"text-align: right\">71.53</td>\n<td style=\"text-align: right\">69.86</td>\n<td style=\"text-align: right\">48.37</td>\n<td style=\"text-align: right\">46.91</td>\n<td style=\"text-align: right\">54.13</td>\n<td style=\"text-align: right\">71.15</td>\n<td style=\"text-align: right\">50.90</td>\n<td style=\"text-align: right\">55.05</td>\n<td style=\"text-align: right\">47.83</td>\n<td style=\"text-align: right\">56.46</td>\n<td style=\"text-align: right\">64.76</td>\n<td style=\"text-align: right\">55.63</td>\n<td style=\"text-align: right\">54.07</td>\n<td style=\"text-align: right\">70.48</td>\n<td style=\"text-align: right\">73.56</td>\n<td style=\"text-align: right\">55.29</td>\n<td style=\"text-align: right\">49.18</td>\n<td style=\"text-align: right\">65.01</td>\n</tr>\n<tr>\n<td>bge-reranker-v2-m3</td>\n<td style=\"text-align: right\"><strong>69.32</strong></td>\n<td style=\"text-align: right\"><strong>80.51</strong></td>\n<td style=\"text-align: right\"><strong>81.85</strong></td>\n<td style=\"text-align: right\"><strong>57.67</strong></td>\n<td style=\"text-align: right\"><strong>57.64</strong></td>\n<td style=\"text-align: right\"><strong>61.92</strong></td>\n<td style=\"text-align: right\"><strong>80.38</strong></td>\n<td style=\"text-align: right\"><strong>59.60</strong></td>\n<td style=\"text-align: right\"><strong>67.66</strong></td>\n<td style=\"text-align: right\"><strong>58.86</strong></td>\n<td style=\"text-align: right\"><strong>67.37</strong></td>\n<td style=\"text-align: right\"><strong>75.14</strong></td>\n<td style=\"text-align: right\"><strong>67.61</strong></td>\n<td style=\"text-align: right\"><strong>68.92</strong></td>\n<td style=\"text-align: right\"><strong>76.69</strong></td>\n<td style=\"text-align: right\"><strong>82.29</strong></td>\n<td style=\"text-align: right\"><strong>64.46</strong></td>\n<td style=\"text-align: right\"><strong>58.32</strong></td>\n<td style=\"text-align: right\"><strong>80.85</strong></td>\n</tr>\n<tr>\n<td>jina-reranker-v2-multilingual</td>\n<td style=\"text-align: right\">63.65</td>\n<td style=\"text-align: right\">72.50</td>\n<td style=\"text-align: right\">79.42</td>\n<td style=\"text-align: right\">46.66</td>\n<td style=\"text-align: right\">51.54</td>\n<td style=\"text-align: right\">57.81</td>\n<td style=\"text-align: right\">73.05</td>\n<td style=\"text-align: right\">50.90</td>\n<td style=\"text-align: right\">60.94</td>\n<td style=\"text-align: right\">56.66</td>\n<td style=\"text-align: right\">59.15</td>\n<td style=\"text-align: right\">72.60</td>\n<td style=\"text-align: right\">53.43</td>\n<td style=\"text-align: right\">66.47</td>\n<td style=\"text-align: right\">74.62</td>\n<td style=\"text-align: right\">77.75</td>\n<td style=\"text-align: right\">62.49</td>\n<td style=\"text-align: right\">53.06</td>\n<td style=\"text-align: right\">76.69</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"mldr-text2text-multilingual-long-documents-13-languages\">MLDR (Text2Text, Documentos Multilingües Largos, 13 idiomas)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2402.03216\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</div><div class=\"kg-bookmark-description\">En este artículo, presentamos un nuevo modelo de embeddings, llamado M3-Embedding, que se distingue por su versatilidad en Multi-Lingualidad, Multi-Funcionalidad y Multi-Granularidad. Puede soportar más de 100 idiomas de trabajo, logrando nuevos resultados estado del arte en tareas de recuperación multilingüe y entre idiomas. Puede realizar simultáneamente las tres funcionalidades comunes de recuperación del modelo de embeddings: recuperación densa, recuperación multi-vector y recuperación dispersa, lo que proporciona una base de modelo unificada para aplicaciones de IR en el mundo real. Es capaz de procesar entradas de diferentes granularidades, desde oraciones cortas hasta documentos largos de hasta 8192 tokens. El entrenamiento efectivo de M3-Embedding involucra las siguientes contribuciones técnicas. Proponemos un nuevo enfoque de auto-destilación de conocimiento, donde las puntuaciones de relevancia de diferentes funcionalidades de recuperación pueden integrarse como señal de profesor para mejorar la calidad del entrenamiento. También optimizamos la estrategia de lotes, permitiendo un tamaño de lote grande y un alto rendimiento de entrenamiento para asegurar la capacidad discriminativa de los embeddings. Hasta donde sabemos, M3-Embedding es el primer modelo de embeddings que logra tal versatilidad. El modelo y el código estarán disponibles públicamente en https://github.com/FlagOpen/FlagEmbedding.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-18.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Jianlv Chen</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-14.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>MLDR es un conjunto de datos multilingüe diseñado específicamente para la recuperación de documentos largos, que abarca 13 idiomas. Utiliza GPT-3.5 para generar preguntas para los documentos. El conjunto de datos está construido sobre Wikipedia, Wudao y mC4.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th style=\"text-align: right\">AVG (NDCG@10)</th>\n<th style=\"text-align: right\">ar</th>\n<th style=\"text-align: right\">de</th>\n<th style=\"text-align: right\">en</th>\n<th style=\"text-align: right\">es</th>\n<th style=\"text-align: right\">fr</th>\n<th style=\"text-align: right\">hi</th>\n<th style=\"text-align: right\">it</th>\n<th style=\"text-align: right\">ja</th>\n<th style=\"text-align: right\">ko</th>\n<th style=\"text-align: right\">pt</th>\n<th style=\"text-align: right\">ru</th>\n<th style=\"text-align: right\">th</th>\n<th style=\"text-align: right\">zh</th>\n</tr>\n</thead>\n<tbody>\n  <tr>\n<td>jina-reranker-m0</td>\n<td style=\"text-align: right\"><strong>59.83</strong></td>\n<td style=\"text-align: right\"><strong>55.86</strong></td>\n<td style=\"text-align: right\"><strong>51.25</strong></td>\n<td style=\"text-align: right\"><strong>54.67</strong></td>\n<td style=\"text-align: right\"><strong>87.63</strong></td>\n<td style=\"text-align: right\"><strong>82.59</strong></td>\n<td style=\"text-align: right\"><strong>32.76</strong></td>\n<td style=\"text-align: right\"><strong>73.25</strong></td>\n<td style=\"text-align: right\"><strong>58.93</strong></td>\n<td style=\"text-align: right\"><strong>55.73</strong></td>\n<td style=\"text-align: right\"><strong>86.08</strong></td>\n<td style=\"text-align: right\"><strong>66.73</strong></td>\n<td style=\"text-align: right\"><strong>39.17</strong></td>\n<td style=\"text-align: right\"><strong>33.14</strong></td>\n</tr>\n<tr>\n<td>jina-embeddings-v3 (8192 tokens)</td>\n<td style=\"text-align: right\">39.71</td>\n<td style=\"text-align: right\">28.44</td>\n<td style=\"text-align: right\">31.57</td>\n<td style=\"text-align: right\">29.07</td>\n<td style=\"text-align: right\">62.08</td>\n<td style=\"text-align: right\">59.79</td>\n<td style=\"text-align: right\">25.47</td>\n<td style=\"text-align: right\">53.72</td>\n<td style=\"text-align: right\">38.36</td>\n<td style=\"text-align: right\">32.37</td>\n<td style=\"text-align: right\">63.26</td>\n<td style=\"text-align: right\">49.65</td>\n<td style=\"text-align: right\">25.15</td>\n<td style=\"text-align: right\">17.26</td>\n</tr>\n<tr>\n<td>bge-reranker-v2-m3</td>\n<td style=\"text-align: right\">53.53</td>\n<td style=\"text-align: right\">49.19</td>\n<td style=\"text-align: right\">45.39</td>\n<td style=\"text-align: right\">43.92</td>\n<td style=\"text-align: right\">74.57</td>\n<td style=\"text-align: right\">68.67</td>\n<td style=\"text-align: right\">44.75</td>\n<td style=\"text-align: right\">62.79</td>\n<td style=\"text-align: right\">49.27</td>\n<td style=\"text-align: right\">48.24</td>\n<td style=\"text-align: right\">76.45</td>\n<td style=\"text-align: right\">62.84</td>\n<td style=\"text-align: right\">38.82</td>\n<td style=\"text-align: right\">31.02</td>\n</tr>\n<tr>\n<td>jina-reranker-v2-multilingual</td>\n<td style=\"text-align: right\">59.50</td>\n<td style=\"text-align: right\">51.96</td>\n<td style=\"text-align: right\">50.13</td>\n<td style=\"text-align: right\">46.85</td>\n<td style=\"text-align: right\">86.34</td>\n<td style=\"text-align: right\">82.25</td>\n<td style=\"text-align: right\">49.50</td>\n<td style=\"text-align: right\">69.00</td>\n<td style=\"text-align: right\">59.07</td>\n<td style=\"text-align: right\">52.19</td>\n<td style=\"text-align: right\">85.26</td>\n<td style=\"text-align: right\">68.06</td>\n<td style=\"text-align: right\">38.73</td>\n<td style=\"text-align: right\">34.15</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"mkqa-text2text-multilingual-question-answering-24-languages-3-variants-for-chinese\">MKQA (Text2Text, Respuesta a Preguntas Multilingüe, 24 idiomas, 3 variantes para chino)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2007.15207\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering</div><div class=\"kg-bookmark-description\">Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on a heavily curated, language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state-of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-11.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Shayne Longpre</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-7.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>MKQA es un conjunto de evaluación de preguntas y respuestas de dominio abierto que comprende 10k pares de preguntas y respuestas alineados en 26 idiomas tipológicamente diversos. Los pares de preguntas y respuestas son muestreados de Google Natural Questions.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th style=\"text-align:right\">AVG (recall@10)</th>\n<th style=\"text-align:right\">ar</th>\n<th style=\"text-align:right\">da</th>\n<th style=\"text-align:right\">de</th>\n<th style=\"text-align:right\">es</th>\n<th style=\"text-align:right\">en</th>\n<th style=\"text-align:right\">fi</th>\n<th style=\"text-align:right\">fr</th>\n<th style=\"text-align:right\">he</th>\n<th style=\"text-align:right\">hu</th>\n<th style=\"text-align:right\">it</th>\n<th style=\"text-align:right\">ja</th>\n<th style=\"text-align:right\">km</th>\n<th style=\"text-align:right\">ko</th>\n<th style=\"text-align:right\">ms</th>\n<th style=\"text-align:right\">nl</th>\n<th style=\"text-align:right\">no</th>\n<th style=\"text-align:right\">pl</th>\n<th style=\"text-align:right\">pt</th>\n<th style=\"text-align:right\">ru</th>\n<th style=\"text-align:right\">sv</th>\n<th style=\"text-align:right\">th</th>\n<th style=\"text-align:right\">tr</th>\n<th style=\"text-align:right\">vi</th>\n<th style=\"text-align:right\">zh_cn</th>\n<th style=\"text-align:right\">zh_hk</th>\n<th style=\"text-align:right\">zh_tw</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-reranker-m0</td>\n<td style=\"text-align:right\"><strong>68.19</strong></td>\n<td style=\"text-align:right\"><strong>63.88</strong></td>\n<td style=\"text-align:right\"><strong>70.57</strong></td>\n<td style=\"text-align:right\"><strong>70.52</strong></td>\n<td style=\"text-align:right\"><strong>71.26</strong></td>\n<td style=\"text-align:right\"><strong>73.47</strong></td>\n<td style=\"text-align:right\">64.10</td>\n<td style=\"text-align:right\"><strong>71.11</strong></td>\n<td style=\"text-align:right\">63.68</td>\n<td style=\"text-align:right\">63.23</td>\n<td style=\"text-align:right\"><strong>70.30</strong></td>\n<td style=\"text-align:right\"><strong>69.13</strong></td>\n<td style=\"text-align:right\">50.43</td>\n<td style=\"text-align:right\"><strong>64.30</strong></td>\n<td style=\"text-align:right\"><strong>70.78</strong></td>\n<td style=\"text-align:right\"><strong>71.73</strong></td>\n<td style=\"text-align:right\"><strong>70.25</strong></td>\n<td style=\"text-align:right\"><strong>69.72</strong></td>\n<td style=\"text-align:right\"><strong>70.57</strong></td>\n<td style=\"text-align:right\"><strong>70.78</strong></td>\n<td style=\"text-align:right\"><strong>70.69</strong></td>\n<td style=\"text-align:right\"><strong>69.80</strong></td>\n<td style=\"text-align:right\">67.90</td>\n<td style=\"text-align:right\"><strong>69.68</strong></td>\n<td style=\"text-align:right\"><strong>69.12</strong></td>\n<td style=\"text-align:right\"><strong>68.23</strong></td>\n<td style=\"text-align:right\"><strong>67.79</strong></td>\n</tr>\n<tr>\n<td>jina-embeddings-v3 (8192 tokens)</td>\n<td style=\"text-align:right\">65.63</td>\n<td style=\"text-align:right\">59.00</td>\n<td style=\"text-align:right\">69.12</td>\n<td style=\"text-align:right\">68.27</td>\n<td style=\"text-align:right\">68.15</td>\n<td style=\"text-align:right\">71.14</td>\n<td style=\"text-align:right\">65.66</td>\n<td style=\"text-align:right\">68.30</td>\n<td style=\"text-align:right\">59.51</td>\n<td style=\"text-align:right\">63.23</td>\n<td style=\"text-align:right\">68.30</td>\n<td style=\"text-align:right\">64.36</td>\n<td style=\"text-align:right\">56.13</td>\n<td style=\"text-align:right\">58.98</td>\n<td style=\"text-align:right\">68.30</td>\n<td style=\"text-align:right\">69.53</td>\n<td style=\"text-align:right\">68.65</td>\n<td style=\"text-align:right\">67.26</td>\n<td style=\"text-align:right\">67.93</td>\n<td style=\"text-align:right\">67.06</td>\n<td style=\"text-align:right\">68.68</td>\n<td style=\"text-align:right\">66.32</td>\n<td style=\"text-align:right\">66.97</td>\n<td style=\"text-align:right\">66.87</td>\n<td style=\"text-align:right\">63.38</td>\n<td style=\"text-align:right\">63.59</td>\n<td style=\"text-align:right\">61.55</td>\n</tr>\n<tr>\n<td>bge-reranker-v2-m3</td>\n<td style=\"text-align:right\">67.88</td>\n<td style=\"text-align:right\">63.09</td>\n<td style=\"text-align:right\">70.15</td>\n<td style=\"text-align:right\">68.91</td>\n<td style=\"text-align:right\">68.92</td>\n<td style=\"text-align:right\">73.00</td>\n<td style=\"text-align:right\"><strong>68.71</strong></td>\n<td style=\"text-align:right\">68.71</td>\n<td style=\"text-align:right\"><strong>70.27</strong></td>\n<td style=\"text-align:right\">64.00</td>\n<td style=\"text-align:right\">68.15</td>\n<td style=\"text-align:right\">68.47</td>\n<td style=\"text-align:right\"><strong>60.43</strong></td>\n<td style=\"text-align:right\">63.95</td>\n<td style=\"text-align:right\">68.80</td>\n<td style=\"text-align:right\">70.77</td>\n<td style=\"text-align:right\">69.10</td>\n<td style=\"text-align:right\">67.44</td>\n<td style=\"text-align:right\">67.40</td>\n<td style=\"text-align:right\">69.77</td>\n<td style=\"text-align:right\">70.03</td>\n<td style=\"text-align:right\">69.68</td>\n<td style=\"text-align:right\">66.04</td>\n<td style=\"text-align:right\">68.29</td>\n<td style=\"text-align:right\">67.84</td>\n<td style=\"text-align:right\">66.70</td>\n<td style=\"text-align:right\">66.34</td>\n</tr>\n<tr>\n<td>jina-reranker-v2-multilingual</td>\n<td style=\"text-align:right\">67.90</td>\n<td style=\"text-align:right\">63.88</td>\n<td style=\"text-align:right\">70.31</td>\n<td style=\"text-align:right\">70.09</td>\n<td style=\"text-align:right\">70.51</td>\n<td style=\"text-align:right\">73.09</td>\n<td style=\"text-align:right\">67.50</td>\n<td style=\"text-align:right\">70.38</td>\n<td style=\"text-align:right\">63.00</td>\n<td style=\"text-align:right\"><strong>64.59</strong></td>\n<td style=\"text-align:right\">69.90</td>\n<td style=\"text-align:right\">67.34</td>\n<td style=\"text-align:right\">57.79</td>\n<td style=\"text-align:right\">62.14</td>\n<td style=\"text-align:right\">70.36</td>\n<td style=\"text-align:right\">71.58</td>\n<td style=\"text-align:right\">69.51</td>\n<td style=\"text-align:right\">68.61</td>\n<td style=\"text-align:right\">70.13</td>\n<td style=\"text-align:right\">70.07</td>\n<td style=\"text-align:right\">70.15</td>\n<td style=\"text-align:right\">68.80</td>\n<td style=\"text-align:right\"><strong>68.02</strong></td>\n<td style=\"text-align:right\">69.39</td>\n<td style=\"text-align:right\">67.23</td>\n<td style=\"text-align:right\">65.77</td>\n<td style=\"text-align:right\">65.37</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"coir-text2text-code-information-retrieval\">CoIR (Text2Text, Recuperación de Información de Código)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2407.02883\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">CoIR: Un Benchmark Integral para Modelos de Recuperación de Información de Código</div><div class=\"kg-bookmark-description\">A pesar del éxito sustancial de la Recuperación de Información (IR) en varias tareas de PNL, la mayoría de los sistemas de IR manejan predominantemente consultas y corpus en lenguaje natural, descuidando el dominio de la recuperación de código. La recuperación de código es críticamente importante pero sigue siendo poco explorada, con métodos y benchmarks existentes que representan inadecuadamente la diversidad del código en varios dominios y tareas. Abordando esta brecha, presentamos COIR (Benchmark de Recuperación de Información de Código), un benchmark robusto e integral específicamente diseñado para evaluar las capacidades de recuperación de código. COIR comprende diez conjuntos de datos de código meticulosamente curados, abarcando ocho tareas distintivas de recuperación a través de siete dominios diversos. Primero discutimos la construcción de COIR y su diversa composición de conjuntos de datos. Además, evaluamos nueve modelos de recuperación ampliamente utilizados usando COIR, descubriendo dificultades significativas en la realización de tareas de recuperación de código incluso con sistemas de última generación. Para facilitar la adopción e integración fácil dentro de los flujos de trabajo de investigación existentes, COIR ha sido desarrollado como un framework de Python fácil de usar, instalable a través de pip. Comparte el mismo esquema de datos que otros benchmarks populares como MTEB y BEIR, permitiendo evaluaciones comparativas entre benchmarks sin problemas. A través de COIR, buscamos vigorizar la investigación en el dominio de recuperación de código, proporcionando una herramienta de evaluación versátil que fomenta el desarrollo y la exploración adicional de sistemas de recuperación de código https://github.com/CoIR-team/coir.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Xiangyang Li</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-10.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>CoIR es un benchmark integral diseñado para evaluar las capacidades de los modelos en la recuperación de código. Incluye 10 conjuntos de datos de código curados que abarcan 8 tareas de recuperación en 7 dominios diversos. Se proporciona un framework Python para este benchmark.</p>\n<!--kg-card-begin: html-->\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"3\">Model Name</th>\n      <th rowspan=\"3\">Avg (NDCG@10)</th>\n      <th colspan=\"3\">Text-to-Code</th>\n      <th colspan=\"7\">Code-to-Text</th>\n      <th colspan=\"9\">Code-to-Code</th>\n      <th colspan=\"3\">Hybrid Code</th>\n    </tr>\n    <tr>\n      <th rowspan=\"2\">Apps</th>\n      <th rowspan=\"2\">CosQA</th>\n      <th rowspan=\"2\">SQL</th>\n      <th colspan=\"7\">CSN</th>\n      <th colspan=\"7\">CSN-CCR</th>\n      <th colspan=\"2\">CodeTransOcean</th>\n      <th rowspan=\"2\">StackOver<br>Flow</th>\n      <th colspan=\"2\">CodeFeedBack</th>\n    </tr>\n    <tr>\n      <th>AVG</th>\n      <th>python</th>\n      <th>javascript</th>\n      <th>go</th>\n      <th>ruby</th>\n      <th>java</th>\n      <th>php</th>\n      <th>AVG</th>\n      <th>python</th>\n      <th>javascript</th>\n      <th>go</th>\n      <th>ruby</th>\n      <th>java</th>\n      <th>php</th>\n      <th>-Contest</th>\n      <th>-DL</th>\n      <th>-MT</th>\n      <th>-ST</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>jina-reranker-m0</td>\n      <td style=\"text-align: right\"><strong>63.55</strong></td>\n      <td style=\"text-align: right\"><strong>26.21</strong></td>\n      <td style=\"text-align: right\">37.75</td>\n      <td style=\"text-align: right\"><strong>57.92</strong></td>\n      <td style=\"text-align: right\">80.76</td>\n      <td style=\"text-align: right\"><strong>98.37</strong></td>\n      <td style=\"text-align: right\">71.16</td>\n      <td style=\"text-align: right\">86.14</td>\n      <td style=\"text-align: right\">72.74</td>\n      <td style=\"text-align: right\">79.02</td>\n      <td style=\"text-align: right\">77.14</td>\n      <td style=\"text-align: right\"><strong>74.57</strong></td>\n      <td style=\"text-align: right\"><strong>81.66</strong></td>\n      <td style=\"text-align: right\"><strong>77.92</strong></td>\n      <td style=\"text-align: right\"><strong>68.71</strong></td>\n      <td style=\"text-align: right\"><strong>75.44</strong></td>\n      <td style=\"text-align: right\"><strong>77.54</strong></td>\n      <td style=\"text-align: right\"><strong>66.13</strong></td>\n      <td style=\"text-align: right\"><strong>79.79</strong></td>\n      <td style=\"text-align: right\"><strong>31.89</strong></td>\n      <td style=\"text-align: right\">90.41</td>\n      <td style=\"text-align: right\"><strong>72.25</strong></td>\n      <td style=\"text-align: right\"><strong>83.95</strong></td>\n    </tr>\n    <tr>\n      <td>jina-embeddings-v2-base-code<br>(top 100)</td>\n      <td style=\"text-align: right\">56.90</td>\n      <td style=\"text-align: right\">16.34</td>\n      <td style=\"text-align: right\"><strong>41.72</strong></td>\n      <td style=\"text-align: right\">49.79</td>\n      <td style=\"text-align: right\"><strong>83.95</strong></td>\n      <td style=\"text-align: right\">94.71</td>\n      <td style=\"text-align: right\"><strong>76.35</strong></td>\n      <td style=\"text-align: right\"><strong>87.39</strong></td>\n      <td style=\"text-align: right\"><strong>78.23</strong></td>\n      <td style=\"text-align: right\"><strong>82.69</strong></td>\n      <td style=\"text-align: right\"><strong>84.35</strong></td>\n      <td style=\"text-align: right\">59.65</td>\n      <td style=\"text-align: right\">68.23</td>\n      <td style=\"text-align: right\">62.31</td>\n      <td style=\"text-align: right\">49.15</td>\n      <td style=\"text-align: right\">65.40</td>\n      <td style=\"text-align: right\">63.89</td>\n      <td style=\"text-align: right\">48.92</td>\n      <td style=\"text-align: right\">79.20</td>\n      <td style=\"text-align: right\">30.35</td>\n      <td style=\"text-align: right\">89.42</td>\n      <td style=\"text-align: right\">49.62</td>\n      <td style=\"text-align: right\">68.93</td>\n    </tr>\n    <tr>\n      <td>bge-reranker-v2-m3</td>\n      <td style=\"text-align: right\">35.97</td>\n      <td style=\"text-align: right\">8.33</td>\n      <td style=\"text-align: right\">30.06</td>\n      <td style=\"text-align: right\">50.63</td>\n      <td style=\"text-align: right\">49.26</td>\n      <td style=\"text-align: right\">67.62</td>\n      <td style=\"text-align: right\">39.55</td>\n      <td style=\"text-align: right\">58.11</td>\n      <td style=\"text-align: right\">41.37</td>\n      <td style=\"text-align: right\">44.77</td>\n      <td style=\"text-align: right\">44.13</td>\n      <td style=\"text-align: right\">40.81</td>\n      <td style=\"text-align: right\">42.57</td>\n      <td style=\"text-align: right\">42.75</td>\n      <td style=\"text-align: right\">38.04</td>\n      <td style=\"text-align: right\">38.04</td>\n      <td style=\"text-align: right\">41.73</td>\n      <td style=\"text-align: right\">41.73</td>\n      <td style=\"text-align: right\">34.93</td>\n      <td style=\"text-align: right\">5.09</td>\n      <td style=\"text-align: right\">60.12</td>\n      <td style=\"text-align: right\">16.44</td>\n      <td style=\"text-align: right\">64.05</td>\n    </tr>\n    <tr>\n      <td>jina-reranker-v2-multilingual</td>\n      <td style=\"text-align: right\">56.14</td>\n      <td style=\"text-align: right\">21.90</td>\n      <td style=\"text-align: right\">37.26</td>\n      <td style=\"text-align: right\">53.56</td>\n      <td style=\"text-align: right\">78.88</td>\n      <td style=\"text-align: right\">97.83</td>\n      <td style=\"text-align: right\">67.43</td>\n      <td style=\"text-align: right\">84.64</td>\n      <td style=\"text-align: right\">68.93</td>\n      <td style=\"text-align: right\">75.73</td>\n      <td style=\"text-align: right\">78.71</td>\n      <td style=\"text-align: right\">63.59</td>\n      <td style=\"text-align: right\">72.62</td>\n      <td style=\"text-align: right\">67.80</td>\n      <td style=\"text-align: right\">55.07</td>\n      <td style=\"text-align: right\">67.25</td>\n      <td style=\"text-align: right\">64.25</td>\n      <td style=\"text-align: right\">54.54</td>\n      <td style=\"text-align: right\">73.67</td>\n      <td style=\"text-align: right\">25.74</td>\n      <td style=\"text-align: right\"><strong>91.24</strong></td>\n      <td style=\"text-align: right\">42.03</td>\n      <td style=\"text-align: right\">73.59</td>\n    </tr>\n  </tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"vidore-text2image-visual-document-retrieval-benchmark\">ViDoRe (Benchmark de Recuperación Visual de Documentos Text2Image)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2407.01449\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColPali: Recuperación Eficiente de Documentos con Modelos de Lenguaje Visual</div><div class=\"kg-bookmark-description\">Los documentos son estructuras visualmente ricas que transmiten información a través del texto, pero también mediante figuras, diseños de página, tablas o incluso fuentes. Dado que los sistemas de recuperación modernos se basan principalmente en la información textual que extraen de las páginas de documentos para indexarlos -a menudo mediante procesos largos y frágiles-, tienen dificultades para explotar eficientemente las claves visuales clave. Esto limita sus capacidades en muchas aplicaciones prácticas de recuperación de documentos como la Generación Aumentada por Recuperación (RAG). Para evaluar los sistemas actuales en la recuperación de documentos visualmente ricos, presentamos el Benchmark de Recuperación Visual de Documentos ViDoRe, compuesto por varias tareas de recuperación a nivel de página que abarcan múltiples dominios, idiomas y configuraciones prácticas. La complejidad inherente y las deficiencias de rendimiento de los sistemas modernos motivan un nuevo concepto: realizar la recuperación de documentos incrustando directamente las imágenes de las páginas del documento. Presentamos ColPali, un Modelo de Lenguaje Visual entrenado para producir incrustaciones multi-vector de alta calidad a partir de imágenes de páginas de documentos. Combinado con un mecanismo de coincidencia de interacción tardía, ColPali supera ampliamente a los sistemas modernos de recuperación de documentos mientras que es drásticamente más simple, rápido y entrenable de extremo a extremo. Liberamos modelos, datos, código y benchmarks bajo licencias abiertas en https://hf.co/vidore.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Manuel Faysse</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-12.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>ViDoRe es un benchmark diseñado para evaluar sistemas de recuperación en su capacidad para relacionar consultas con documentos relevantes utilizando características visuales. Abarca varias tareas de recuperación a nivel de página en múltiples dominios e idiomas. El benchmark se centra en los elementos visuales de los documentos.</p>\n<!--kg-card-begin: html-->\n<table>\n  <thead>\n    <tr>\n      <th>Model Name</th>\n      <th style=\"text-align: right\">AVG<br>(NDCG@5)</th>\n      <th style=\"text-align: right\">TAT-DQA</th>\n      <th style=\"text-align: right\">Shift<br>Project</th>\n      <th style=\"text-align: right\">Artificial<br>Intelligence</th>\n      <th style=\"text-align: right\">Government<br>Reports</th>\n      <th style=\"text-align: right\">ArxivQA</th>\n      <th style=\"text-align: right\">DocVQA</th>\n      <th style=\"text-align: right\">Healthcare<br>Industry</th>\n      <th style=\"text-align: right\">InfoVQA</th>\n      <th style=\"text-align: right\">Energy</th>\n      <th style=\"text-align: right\">TabFQuad</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>jina-reranker-m0</td>\n      <td style=\"text-align: right\"><strong>91.02</strong></td>\n      <td style=\"text-align: right\"><strong>81.83</strong></td>\n      <td style=\"text-align: right\"><strong>93.22</strong></td>\n      <td style=\"text-align: right\"><strong>99.63</strong></td>\n      <td style=\"text-align: right\"><strong>97.59</strong></td>\n      <td style=\"text-align: right\"><strong>89.82</strong></td>\n      <td style=\"text-align: right\"><strong>62.58</strong></td>\n      <td style=\"text-align: right\"><strong>99.26</strong></td>\n      <td style=\"text-align: right\"><strong>92.88</strong></td>\n      <td style=\"text-align: right\"><strong>96.06</strong></td>\n      <td style=\"text-align: right\"><strong>97.32</strong></td>\n    </tr>\n    <tr>\n      <td>MrLight/dse-qwen2-2b-mr1-v1</td>\n      <td style=\"text-align: right\">84.48</td>\n      <td style=\"text-align: right\">66.64</td>\n      <td style=\"text-align: right\">79.39</td>\n      <td style=\"text-align: right\">96.45</td>\n      <td style=\"text-align: right\">95.30</td>\n      <td style=\"text-align: right\">84.53</td>\n      <td style=\"text-align: right\">55.47</td>\n      <td style=\"text-align: right\">96.85</td>\n      <td style=\"text-align: right\">86.39</td>\n      <td style=\"text-align: right\">91.80</td>\n      <td style=\"text-align: right\">92.03</td>\n    </tr>\n  \n    <tr>\n      <td>MonoQwen2-VL-v0.1</td>\n      <td style=\"text-align: right\">87.64</td>\n      <td style=\"text-align: right\">79.50</td>\n      <td style=\"text-align: right\">76.38</td>\n      <td style=\"text-align: right\">98.39</td>\n      <td style=\"text-align: right\">93.63</td>\n      <td style=\"text-align: right\">89.50</td>\n      <td style=\"text-align: right\">57.47</td>\n      <td style=\"text-align: right\">98.39</td>\n      <td style=\"text-align: right\">92.12</td>\n      <td style=\"text-align: right\">95.29</td>\n      <td style=\"text-align: right\">95.75</td>\n    </tr>\n  </tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"m-beir-text2image-image2text-multimodal-benchmark-for-instructed-retrieval\">M-BEIR (Text2Image, Image2Text, Benchmark Multimodal para Recuperación con Instrucciones)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2311.17136\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">UniIR: Training and Benchmarking Universal Multimodal Information Retrievers</div><div class=\"kg-bookmark-description\">Los modelos de recuperación de información (IR) existentes a menudo asumen un formato homogéneo, limitando su aplicabilidad a diversas necesidades de usuario, como buscar imágenes con descripciones de texto, buscar un artículo de noticias con una imagen de cabecera o encontrar una foto similar con una imagen de consulta. Para abordar estas diferentes demandas de búsqueda de información, presentamos UniIR, un recuperador multimodal unificado guiado por instrucciones capaz de manejar ocho tareas distintas de recuperación entre modalidades. UniIR, un sistema único de recuperación entrenado conjuntamente en diez diversos conjuntos de datos de IR multimodal, interpreta las instrucciones del usuario para ejecutar varias tareas de recuperación, demostrando un rendimiento robusto en conjuntos de datos existentes y generalización zero-shot a nuevas tareas. Nuestros experimentos destacan que el entrenamiento multitarea y el ajuste de instrucciones son claves para la capacidad de generalización de UniIR. Además, construimos el M-BEIR, un benchmark de recuperación multimodal con resultados completos, para estandarizar la evaluación de la recuperación de información multimodal universal.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-19.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Cong Wei</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-15.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>M-BEIR es un benchmark integral a gran escala diseñado para entrenar y evaluar modelos de recuperación multimodal. Comprende ocho tareas de recuperación multimodal y diez conjuntos de datos de diversos dominios y fuentes. El benchmark se centra en la recuperación basada en instrucciones.</p>\n<!--kg-card-begin: html-->\n<table>\n  <thead>\n        <tr>\n          <th>Model</th>\n            <th>MBEIR t2i VisualNews<br>Recall@5</th>\n            <th>MBEIR t2i MSCOCO<br>Recall@5</th>\n            <th>MBEIR t2i Fashion200K<br>Recall@10</th>\n            <th>MBEIR i2t VisualNews<br>Recall@5</th>\n            <th>MBEIR i2t MSCOCO<br>Recall@5</th>\n            <th>MBEIR i2t Fashion200K<br>Recall@10</th>\n        </tr>\n    </thead>\n        <tr>\n            <td>jina-reranker-m0</td>\n            <td align=\"right\"><b>23.89</b></td>\n            <td align=\"right\"><b>72.19</b></td>\n            <td align=\"right\">9.79</td>\n            <td align=\"right\"><b>17.61</b></td>\n            <td align=\"right\">41.21</td>\n            <td align=\"right\"><b>11.56</b></td>\n        </tr>\n        <tr>\n            <td>jinaai/jina-clip-v2</td>\n            <td align=\"right\">15.42</td>\n            <td align=\"right\">52.28</td>\n            <td align=\"right\">7.03</td>\n            <td align=\"right\">11.63</td>\n            <td align=\"right\">28.80</td>\n            <td align=\"right\">8.78</td>\n        </tr>\n        <tr>\n            <td>MonoQwen2-VL-v0.1</td>\n            <td align=\"right\">22.74</td>\n            <td align=\"right\">71.29</td>\n            <td align=\"right\"><b>10.00</b></td>\n            <td align=\"right\">15.08</td>\n            <td align=\"right\"><b>42.24</b></td>\n            <td align=\"right\">11.25</td>\n        </tr>\n    </table>\n<!--kg-card-end: html-->\n<h3 id=\"winoground-text2text-text2image\">Winoground (Text2Text, Text2Image)</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2204.03162\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality</div><div class=\"kg-bookmark-description\">Presentamos una nueva tarea y conjunto de datos para evaluar la capacidad de los modelos de visión y lenguaje para realizar razonamiento composicional visio-lingüístico, que llamamos Winoground. Dados dos imágenes y dos subtítulos, el objetivo es emparejarlos correctamente - pero crucialmente, ambos subtítulos contienen un conjunto idéntico de palabras, solo que en un orden diferente. El conjunto de datos fue cuidadosamente curado a mano por anotadores expertos y está etiquetado con un rico conjunto de etiquetas detalladas para ayudar en el análisis del rendimiento del modelo. Probamos una amplia gama de modelos de visión y lenguaje de última generación y encontramos que, sorprendentemente, ninguno de ellos funciona mucho mejor que el azar. Evidentemente, estos modelos no son tan hábiles en el razonamiento composicional visio-lingüístico como podríamos haber esperado. Realizamos un análisis exhaustivo para obtener información sobre cómo el trabajo futuro podría intentar mitigar las deficiencias de estos modelos. Aspiramos a que Winoground sirva como un conjunto de evaluación útil para avanzar en el estado del arte e impulsar más progreso en el campo. El conjunto de datos está disponible en https://huggingface.co/datasets/facebook/winoground.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Tristan Thrush</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-11.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Winoground es una tarea y conjunto de datos novedoso para evaluar la capacidad de los modelos de visión y lenguaje para realizar razonamiento composicional visio-lingüístico. Utiliza leyendas gemelas con contenido de palabras idéntico y emplea pares contrastantes de imagen-leyenda. El enfoque está en el razonamiento composicional.</p>\n<!--kg-card-begin: html-->\n<table>\n  <thead>\n    <tr>\n      <th>Model</th>\n      <th>Text</th>\n      <th>Image</th>\n      <th>Group</th>\n      <th>Avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>jina-reranker-m0</td>\n      <td style=\"text-align: right\"><strong>57.00</strong></td>\n      <td style=\"text-align: right\"><strong>40.75</strong></td>\n      <td style=\"text-align: right\"><strong>34.00</strong></td>\n      <td style=\"text-align: right\"><strong>43.92</strong></td>\n    </tr>\n    <tr>\n      <td>MrLight/dse-qwen2-2b-mrl-v1</td>\n      <td style=\"text-align: right\">7.50</td>\n      <td style=\"text-align: right\">9.25</td>\n      <td style=\"text-align: right\">1.75</td>\n      <td style=\"text-align: right\">6.17</td>\n    </tr>\n    <tr>\n      <td>MonoQwen2-VL-v0.1</td>\n      <td style=\"text-align: right\">52.00</td>\n      <td style=\"text-align: right\">36.25</td>\n      <td style=\"text-align: right\">31.50</td>\n      <td style=\"text-align: right\">39.92</td>\n    </tr>\n  </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Winoground evalúa los modelos de visión-lenguaje utilizando tres métricas clave: Puntuación de Texto, Puntuación de Imagen y Puntuación de Grupo. La Puntuación de Texto mide si un modelo relaciona correctamente las leyendas con las imágenes, mientras que la Puntuación de Imagen evalúa si selecciona la imagen correcta para una leyenda. La Puntuación de Grupo, la métrica más rigurosa, requiere que todas las relaciones leyenda-imagen sean identificadas correctamente. Las puntuaciones son porcentajes que representan tasas de precisión, donde valores más altos indican mejores capacidades de razonamiento.</p><h2 id=\"conclusion\">Conclusión</h2><p><code>jina-reranker-m0</code> es nuestro primer intento de unificar modalidades textuales y visuales en un único modelo decoder-only. Esta nueva arquitectura incorpora lecciones aprendidas de nuestros modelos anteriores de recuperación encoder-only, incluyendo <code>jina-clip-v2</code>, <code>jina-embeddings-v3</code>, <code>jina-reranker-v2-base-multilingual</code> y <code>jina-embeddings-v2-base-code</code>.</p><p>El nuevo modelo no solo desbloquea capacidades para tareas de recuperación multimodal, como la reordenación de texto a imagen y la reordenación de documentos visuales, sino que también demuestra un rendimiento mejorado en comparación con <code>jina-reranker-v2-base-multilingual</code> en tareas de reordenación de texto a texto y texto a código. Designamos esta nueva serie de modelos como la \"serie m\" para resaltar su naturaleza multimodal.</p><p>Al comparar <code>jina-reranker-m0</code> con <code>jina-reranker-v2-base-multilingual</code>, nuestro objetivo para la serie m es lograr multimodalidad mientras mejoramos el rendimiento en tareas de solo texto a un nivel comparable a los modelos especializados de solo texto. Algunos podrían cuestionar el valor de usar un modelo 8 veces más grande si la mejora del rendimiento en tareas de solo texto parece marginal. Si bien es cierto por el momento que <code>m0</code> puede no proporcionar un valor agregado sustancial sobre <code>v2</code> para aplicaciones de solo texto, la arquitectura decoder-only abre muchas nuevas posibilidades que no eran alcanzables con arquitecturas encoder-only, incluyendo:</p><ul><li>Reordenación de modalidad mixta verdadera</li><li>Reordenación por lista y deduplicación de documentos</li><li>Explicabilidad de la puntuación de clasificación mediante mecanismo de atención</li></ul><p>Nuestro trabajo futuro se centrará en mejorar aún más el reordenador de solo texto y aprovechar completamente las nuevas características habilitadas por esta arquitectura multimodal para lograr una búsqueda mejor y más <em>amplia</em>.</p>",
  "comment_id": "67ea5eb45dcba60001c30f0a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/04/banner-reranker-m0--1-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-03-31T11:21:56.000+02:00",
  "updated_at": "2025-04-08T13:21:13.000+02:00",
  "published_at": "2025-04-08T13:10:38.000+02:00",
  "custom_excerpt": "Introducing jina-reranker-m0, our new multilingual multimodal reranker for retrieving visual documents, with SOTA performance on multilingual long documents and code searching tasks.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-reranker-m0-multilingual-multimodal-document-reranker/",
  "excerpt": "Presentamos jina-reranker-m0, nuestro nuevo reranker multimodal multilingüe para recuperar documentos visuales, con rendimiento SOTA en documentos largos multilingües y tareas de búsqueda de código.",
  "reading_time": 20,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
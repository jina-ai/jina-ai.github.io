{
  "slug": "query-expansion-with-llms-searching-better-by-saying-more",
  "id": "67af53142962d20001d63c71",
  "uuid": "581110d6-5791-42f7-b754-16d597390ff7",
  "title": "Expansión de consultas con LLMs: búsquedas mejores diciendo más",
  "html": "<p>La expansión de consultas ha sido durante mucho tiempo una técnica preferida para potenciar los sistemas de búsqueda, aunque ha quedado en segundo plano desde que aparecieron los embeddings semánticos. Si bien algunos podrían considerarla obsoleta en nuestro panorama actual de RAG y búsqueda mediante agentes, no la descarten todavía. En esta exploración profunda, veremos cómo combinar la expansión automática de consultas con <code>jina-embeddings-v3</code> y LLMs puede mejorar tu sistema de búsqueda y entregar resultados realmente precisos.</p><h2 id=\"what-is-query-expansion\">¿Qué es la Expansión de Consultas?</h2><p>La expansión de consultas se desarrolló para sistemas de búsqueda que juzgan la relevancia comparando palabras de las consultas con documentos que las contienen, como <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf-idf</a> u otros esquemas de \"vectores dispersos\". Esto tiene algunas limitaciones obvias. Las variantes de las palabras interfieren con la coincidencia, como \"corrió\" y \"corriendo\", o \"optimizar\" vs. \"optimizar\". El preprocesamiento consciente del lenguaje puede mitigar algunos de estos problemas, pero no todos. Los términos técnicos, sinónimos y palabras relacionadas son mucho más difíciles de abordar. Por ejemplo, una consulta de investigación médica sobre \"coronavirus\" no identificará automáticamente documentos que hablen de \"COVID\" o \"SARS-CoV-2\", aunque serían coincidencias muy buenas.</p><p>La expansión de consultas se inventó como solución.</p><p>La idea es agregar palabras y frases adicionales a las consultas para aumentar la probabilidad de identificar buenas coincidencias. De esta manera, una consulta por \"coronavirus\" podría tener agregados los términos \"COVID\" y \"SARS-CoV-2\". Esto puede mejorar dramáticamente el rendimiento de la búsqueda.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"700\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion1.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion1.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Diagrama de Flujo de Expansión de Consultas con Tesauro</span></figcaption></figure><p>No es fácil decidir qué términos deben agregarse a una consulta, y ha habido mucho trabajo sobre cómo identificar buenos términos y cómo ponderarlos para la recuperación estilo tf-idf. Los enfoques comunes incluyen:</p><ul><li>Usar un tesauro curado manualmente.</li><li>Minería de datos en grandes corpus de texto para encontrar palabras relacionadas.</li><li>Identificar otros términos usados en consultas similares tomadas de un registro de consultas.</li><li>Aprender qué palabras y frases funcionan bien como expansiones de consultas <a href=\"https://en.wikipedia.org/wiki/Rocchio_algorithm\">a partir de la retroalimentación del usuario</a>.</li></ul><p>Sin embargo, se supone que los modelos de embedding semántico eliminan la necesidad de expansión de consultas. Los buenos embeddings de texto para \"coronavirus\", \"COVID\" y \"SARS-CoV-2\" deberían estar muy cerca entre sí en el espacio vectorial de embedding. Deberían coincidir naturalmente sin ningún tipo de aumentación.</p><p>Pero, aunque esto debería ser cierto en teoría, los embeddings reales hechos por modelos reales a menudo quedan cortos. Las palabras en los embeddings pueden ser ambiguas y agregar palabras a una consulta puede orientarla hacia mejores coincidencias si usas las palabras correctas. Por ejemplo, un embedding para \"erupción cutánea\" podría identificar documentos sobre \"actuar precipitadamente\" y \"crema para la piel\" mientras pasa por alto un artículo de revista médica que habla sobre \"dermatitis\". Agregar términos relevantes probablemente orientará el embedding lejos de coincidencias no relacionadas hacia mejores resultados.</p><h2 id=\"llm-query-expansion\">Expansión de Consultas con LLM</h2><p>En lugar de usar un tesauro o hacer minería de datos léxica, analizamos el uso de un LLM para hacer expansión de consultas. Los LLMs tienen algunas ventajas potenciales importantes:</p><ul><li><strong>Amplio conocimiento léxico</strong>: Debido a que están entrenados en conjuntos de datos grandes y diversos, hay menos preocupación por seleccionar un tesauro apropiado u obtener los datos correctos.</li><li><strong>Capacidad de juicio</strong>: No todos los términos de expansión propuestos son necesariamente apropiados para una consulta específica. Los LLMs pueden no hacer juicios perfectos sobre la pertinencia, pero las alternativas realmente no pueden hacer juicios en absoluto.</li><li><strong>Flexibilidad</strong>: Puedes ajustar tu prompt a las necesidades de la tarea de recuperación, mientras que otros enfoques son rígidos y pueden requerir mucho trabajo para adaptarse a nuevos dominios o fuentes de datos.</li></ul><p>Una vez que el LLM ha propuesto una lista de términos, la expansión de consultas para embeddings funciona de la misma manera que los esquemas tradicionales de expansión de consultas: Agregamos términos al texto de la consulta y luego usamos un modelo de embedding para crear un vector de embedding de consulta.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"850\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/02/QueryExpansion2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/02/QueryExpansion2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/02/QueryExpansion2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/02/QueryExpansion2.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Expansión de Consultas de Embeddings con un LLM</span></figcaption></figure><p>Para que esto funcione, necesitas:</p><ul><li>Acceso a un LLM.</li><li>Una plantilla de prompt para solicitar términos de expansión del LLM.</li><li>Un modelo de embedding de texto.</li></ul><h2 id=\"trying-it-out\">Probándolo</h2><p>Hemos realizado algunos experimentos para ver si este enfoque agrega valor a la recuperación de información textual. Nuestras pruebas utilizaron:</p><ul><li>Un LLM: <a href=\"https://deepmind.google/technologies/gemini/flash/\">Gemini 2.0 Flash</a> de Google.</li><li>Dos modelos de embedding para ver si la expansión de consultas LLM se generaliza entre modelos: <code>jina-embeddings-v3</code> y <a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><code>all-MiniLM-L6-v2</code></a>.</li><li>Un subconjunto de los <a href=\"https://github.com/beir-cellar/beir\">benchmarks BEIR</a> para recuperación de información.</li></ul><p>Realizamos nuestros experimentos bajo dos condiciones de prompting:</p><ul><li>Usando una plantilla de prompt general para solicitar términos de expansión.</li><li>Usando plantillas de prompt específicas para cada tarea.</li></ul><p>Finalmente, escribimos nuestros prompts para solicitar diferentes cantidades de términos para agregar: 100, 150 y 250.</p><p>Nuestro código y resultados están <a href=\"https://github.com/jina-ai/llm-query-expansion/\">disponibles en GitHub</a> para reproducción.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/llm-query-expansion/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/llm-query-expansion: Query Expension for Better Query Embedding using LLMs</div><div class=\"kg-bookmark-description\">Query Expension for Better Query Embedding using LLMs - jina-ai/llm-query-expansion</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-1.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/llm-query-expansion\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"results\">Resultados</h2><h3 id=\"using-a-general-prompt\">Usando un Prompt General</h3><p>Después de varios intentos y errores, encontramos que el siguiente prompt funcionaba lo suficientemente bien con Gemini 2.0 Flash:</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make \nit easier to find the relevant documents (about <span style=\"color:#AADB1E\">{size}</span> words \nper query):\n<span style=\"color:#AADB1E\">{query}</span>\n\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>Este prompt nos permite procesar nuestras consultas en lotes de 20-50, asignando un ID a cada una y obteniendo una cadena JSON que conecta cada consulta con una lista de términos de expansión. Si usas un LLM diferente, es posible que tengas que experimentar para encontrar un prompt que funcione.</p><p>Aplicamos esta configuración con <code>jina-embeddings-v3</code> usando el <a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model\">adaptador de recuperación asimétrica</a>. Usando este enfoque, las consultas y documentos se codifican de manera diferente — usando el mismo modelo pero diferentes extensiones LoRA — para optimizar los embeddings resultantes para la recuperación de información.</p><p>Nuestros resultados en varios benchmarks BEIR están en la tabla siguiente. Las puntuaciones son nDCG@10 (<a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\">Ganancia Acumulada Descontada normalizada</a> en los diez primeros elementos recuperados).</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Sin Expansión</th>\n<th>100 términos</th>\n<th>150 términos</th>\n<th>250 términos</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>(Tarea de Verificación de Hechos)</td>\n<td>72.74</td>\n<td>73.39</td>\n<td>74.16</td>\n<td><strong>74.33</strong></td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>(Tarea de Recuperación Médica)</td>\n<td>77.55</td>\n<td>76.74</td>\n<td>77.12</td>\n<td><strong>79.28</strong></td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>(Recuperación de Opciones Financieras)</td>\n<td>47.34</td>\n<td><strong>47.76</strong></td>\n<td>46.03</td>\n<td>47.34</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>(Recuperación de Información Médica)</td>\n<td>36.46</td>\n<td><strong>40.62</strong></td>\n<td>39.63</td>\n<td>39.20</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>(Tarea de Recuperación de Argumentos)</td>\n<td>26.24</td>\n<td>26.91</td>\n<td>27.15</td>\n<td><strong>27.54</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Vemos aquí que la expansión de consultas ha resultado en una mejora de la recuperación en casi todos los casos.</p><p>Para probar la robustez de este enfoque, repetimos las mismas pruebas con <code>all-MiniLM-L6-v2</code>, un modelo mucho más pequeño que produce vectores de embedding más reducidos.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">sentence-transformers/all-MiniLM-L6-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-29.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/all-MiniLM-L6-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Los resultados se muestran en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>No Expansion</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>SciFact</strong><br/>(Fact Checking Task)</td>\n<td>64.51</td>\n<td><strong>68.72</strong></td>\n<td>66.27</td>\n<td>68.50</td>\n</tr>\n<tr>\n<td><strong>TRECCOVID</strong><br/>(Medical Retrieval Task)</td>\n<td>47.25</td>\n<td>67.90</td>\n<td><strong>70.18</strong></td>\n<td>69.60</td>\n</tr>\n<tr>\n<td><strong>FiQA</strong><br/>(Financial Option Retrieval)</td>\n<td><strong>36.87</strong></td>\n<td>33.96</td>\n<td>32.60</td>\n<td>31.84</td>\n</tr>\n<tr>\n<td><strong>NFCorpus</strong><br/>(Medical Information Retrieval)</td>\n<td>31.59</td>\n<td><strong>33.76</strong></td>\n<td>33.76</td>\n<td>33.35</td>\n</tr>\n<tr>\n<td><strong>Touche2020</strong><br/>(Argument Retrieval Task)</td>\n<td>16.90</td>\n<td><strong>25.31</strong></td>\n<td>23.52</td>\n<td>23.23</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Aquí vemos una mejora aún mayor en las puntuaciones de recuperación. En general, el modelo más pequeño se benefició más de la expansión de consultas. La mejora promedio en todas las tareas se resume en la tabla siguiente:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>100 terms</th>\n<th>150 terms</th>\n<th>250 terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>jina-embeddings-v3</code></td>\n<td>+1.02</td>\n<td>+0.75</td>\n<td><strong>+1.48</strong></td>\n</tr>\n<tr>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>+6.51</strong></td>\n<td>+5.84</td>\n<td>+5.88</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La gran diferencia en la mejora neta entre los dos modelos probablemente se debe a que <code>all-MiniLM-L6-v2</code> parte de un nivel de rendimiento más bajo. Los embeddings de consulta producidos por <code>jina-embeddings-v3</code> en modo de recuperación asimétrica son mejores para capturar relaciones semánticas clave, y por lo tanto hay menos espacio para que la expansión de consultas añada información. Pero este resultado muestra cuánto puede mejorar la expansión de consultas el rendimiento de modelos más compactos que pueden ser preferibles en algunos casos de uso a los modelos grandes.</p><p>No obstante, la expansión de consultas trajo una mejora significativa en la recuperación incluso para modelos de alto rendimiento como <code>jina-embeddings-v3</code>, aunque este resultado no es perfectamente consistente en todas las tareas y condiciones.</p><p>Para <code>jina-embeddings-v3</code>, agregar más de 100 términos a una consulta fue contraproducente para los benchmarks FiQA y NFCorpus. No podemos decir que más términos sean siempre mejores, pero los resultados en los otros benchmarks indican que más términos son al menos a veces mejores.</p><p>Para <code>all-MiniLM-L6-v2</code>, agregar más de 150 términos fue siempre contraproducente, y en tres pruebas, agregar más de 100 no mejoró las puntuaciones. En una prueba (FiQA) agregar incluso 100 términos produjo resultados significativamente más bajos. Creemos que esto se debe a que <code>jina-embeddings-v3</code> hace un mejor trabajo capturando información semántica en textos de consulta largos.</p><p>Ambos modelos mostraron menos respuesta a la expansión de consultas en los benchmarks FiQA y NFCorpus.</p><h2 id=\"using-task-specific-prompting\">Uso de Prompts Específicos por Tarea</h2><p>El patrón de resultados reportados arriba sugiere que si bien la expansión de consultas es útil, usar LLMs corre el riesgo de agregar términos de consulta inútiles que reducen el rendimiento. Esto podría deberse a la naturaleza genérica del prompt.</p><p>Tomamos dos benchmarks — SciFact y FiQA — y creamos prompts más específicos del dominio, como el siguiente:</p>\n<!--kg-card-begin: html-->\n<pre>\nPlease provide additional search keywords and phrases for \neach of the key aspects of the following queries that make\nit easier to find the <span style=\"background-color:red\">relevant documents</span> <span style=\"background-color:green\">scientific document \nthat supports or rejects the scientific fact in the query \nfield</span> (about <span style=\"color:#AADB1E\">{size}</span> words per query):\n<span style=\"color:#AADB1E\">{query}</span>\nPlease respond in the following JSON schema:\nExpansion = {\"qid\": str, \"additional_info\": str}\nReturn: list [Expansion]\n</pre>\n<!--kg-card-end: html-->\n<p>Este enfoque mejoró el rendimiento de recuperación en casi todos los casos:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>No Expansion</th>\n<th>100<br/>terms</th>\n<th>150<br/>terms</th>\n<th>250<br/>terms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>72.74</td>\n<td><strong>75.85 (+2.46)</strong></td>\n<td>75.07 (+0.91)</td>\n<td>75.13 (+0.80)</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td>64.51</td>\n<td><strong>69.12 (+0.40)</strong></td>\n<td>68.10 (+1.83)</td>\n<td>67.83 (-0.67)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>jina-embeddings-v3</code></td>\n<td>47.34</td>\n<td>47.77 (+0.01)</td>\n<td><strong>48.20 (+1.99)</strong></td>\n<td>47.75 (+0.41)</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><code>all-MiniLM-L6-v2</code></td>\n<td><strong>36.87</strong></td>\n<td>34.71 (+0.75)</td>\n<td>34.68 (+2.08)</td>\n<td>34.50 (+2.66)</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Las puntuaciones mejoraron en todas las condiciones excepto al agregar 250 términos a las consultas de SciFact con <code>all-MiniLM-L6-v2</code>. Además, esta mejora no fue suficiente para que <code>all-MiniLM-L6-v2</code> superara su propia línea base en FiQA.</p><p>Para <code>jina-embeddings-v3</code>, vemos que los mejores resultados se obtuvieron con 100 o 150 términos añadidos. Agregar 250 términos fue contraproducente. Esto respalda nuestra intuición de que se pueden agregar demasiados términos a la consulta, especialmente si su significado comienza a alejarse del objetivo.</p><h2 id=\"key-considerations-in-query-expansion\">Consideraciones Clave en la Expansión de Consultas</h2><p>La expansión de consultas claramente puede traer ganancias a la búsqueda basada en embeddings, pero viene con algunas advertencias:</p><h3 id=\"expense\">Costo</h3><p>Interactuar con un LLM agrega latencia y costos computacionales a la recuperación de información, y puede agregar costos reales si se usa un LLM comercial. La mejora moderada que aporta puede no justificar el gasto.</p><h3 id=\"prompt-engineering\">Ingeniería de Prompts</h3><p>Diseñar buenas plantillas de prompts es un arte empírico y experimental. No aseguramos que los que usamos para este trabajo sean óptimos o portables a otros LLMs. Nuestros experimentos con prompts específicos por tarea muestran que cambiar los prompts puede tener efectos muy significativos en la calidad del resultado. Los resultados también varían considerablemente entre dominios.</p><p>Estas incertidumbres aumentan el costo de desarrollo y socavan la mantenibilidad. Cualquier cambio en el sistema — cambiar LLMs, modelos de embedding o dominio de información — significa volver a verificar y posiblemente reimplementar todo el proceso.</p><h3 id=\"alternatives\">Alternativas</h3><p>Vemos aquí que la expansión de consultas agregó la mayor mejora al modelo de embedding con el peor rendimiento inicial. La expansión de consultas, al menos en este experimento, no pudo cerrar la brecha de rendimiento entre <code>all-MiniLM-L6-v2</code> y <code>jina-embeddings-v3</code>, mientras que <code>jina-embeddings-v3</code> vio mejoras más modestas con la expansión de consultas.</p><p>Bajo estas circunstancias, un usuario de <code>all-MiniLM-L6-v2</code> obtendría mejores resultados a un menor costo adoptando <code>jina-embeddings-v3</code> en lugar de perseguir la expansión de consultas.</p><h2 id=\"future-directions\">Direcciones Futuras</h2><p>Hemos mostrado aquí que la expansión de consultas puede mejorar los embeddings de consulta, y que los LLMs ofrecen un medio simple y flexible para obtener buenos términos de expansión de consultas. Pero las ganancias relativamente modestas sugieren que hay más trabajo por hacer. Estamos considerando varias direcciones para investigación futura:</p><ul><li>Probar el valor del enriquecimiento terminológico en la generación de embeddings de documentos.</li><li>Explorar las posibilidades de mejora de consultas en otras técnicas de búsqueda con IA como el reranking.</li><li>Comparar la expansión de consultas basada en LLM con fuentes de términos más antiguas y computacionalmente menos costosas, como un tesauro.</li><li>Entrenar modelos de lenguaje específicamente para ser mejores en expansión de consultas y proporcionarles entrenamiento más específico del dominio.</li><li>Limitar el número de términos agregados. 100 puede ser demasiado para empezar.</li><li>Encontrar formas de identificar términos de expansión útiles e inútiles. Cualquier número fijo que impongamos en la expansión de consultas no va a ser un ajuste perfecto y si pudiéramos evaluar dinámicamente los términos sugeridos y mantener solo los buenos, el resultado probablemente sería una mejora en el rendimiento.</li></ul><p>Esta es una investigación muy preliminar, y somos optimistas de que técnicas como esta traerán más mejoras a los productos de búsqueda fundamentales de Jina AI.</p>",
  "comment_id": "67af53142962d20001d63c71",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/02/query-banner.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-02-14T15:28:36.000+01:00",
  "updated_at": "2025-02-18T03:24:20.000+01:00",
  "published_at": "2025-02-18T03:24:20.000+01:00",
  "custom_excerpt": "Search has changed a lot since embedding models were introduced. Is there still a role for lexical techniques like query expansion in AI? We think so.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael Günther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/query-expansion-with-llms-searching-better-by-saying-more/",
  "excerpt": "La búsqueda ha cambiado mucho desde la introducción de los modelos de embeddings. ¿Existe todavía un papel para las técnicas léxicas como la expansión de consultas en la IA? Creemos que sí.",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
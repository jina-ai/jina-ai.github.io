{
  "slug": "finding-optimal-breakpoints-in-long-documents-using-small-language-models",
  "id": "67126986708dbe00019249f2",
  "uuid": "b7e55a5d-f267-4a4a-b861-27221c0f3827",
  "title": "Encontrando Puntos de Ruptura √ìptimos en Documentos Largos Usando Modelos de Lenguaje Peque√±os",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Esta es la parte III de nuestra serie sobre chunking. <b><strong style=\"white-space: pre-wrap;\">Orden de lectura recomendado: </strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte I</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">documento de investigaci√≥n</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, parte III.</strong></b></div></div><p>En nuestros posts anteriores, exploramos los desaf√≠os del chunking e <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">introdujimos el concepto de late chunking</a>, que ayuda a reducir la p√©rdida de contexto al embedar chunks. En este post, nos centraremos en otro desaf√≠o: encontrar puntos de corte √≥ptimos. Si bien <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io#late-chunking-is-resilient-to-poor-boundary-cues\" rel=\"noreferrer\">nuestra estrategia de late chunking ha demostrado ser bastante resiliente a l√≠mites deficientes</a>, esto no significa que podamos ignorarlos‚Äîsiguen siendo importantes tanto para la legibilidad humana como para los LLM. Nuestra perspectiva es esta: al determinar los puntos de corte, ahora podemos concentrarnos completamente en la legibilidad sin preocuparnos por la p√©rdida sem√°ntica o de contexto. El late chunking puede manejar tanto buenos como malos puntos de corte, por lo que la legibilidad se convierte en tu principal preocupaci√≥n.</p><p>Con esto en mente, entrenamos tres modelos de lenguaje peque√±os espec√≠ficamente dise√±ados para segmentar documentos largos mientras mantienen la coherencia sem√°ntica y manejan estructuras de contenido complejas. Estos son:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>simple-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, que segmenta el texto bas√°ndose en los elementos estructurales del documento.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>topic-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, que segmenta el texto bas√°ndose en los temas dentro del texto.</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-summary-chunking ¬∑ Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>summary-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">, que genera res√∫menes para cada segmento.</span></p></figcaption></figure><p>En este post, discutiremos por qu√© desarrollamos este modelo, c√≥mo abordamos sus tres variantes y c√≥mo se comparan con <a href=\"https://www.notion.so/Advancing-Segmentation-Strategies-in-RAG-with-a-Custom-Small-Language-Model-Restructure-638b84ae461d412eb6889cfa7f54cce1?pvs=21&ref=jina-ai-gmbh.ghost.io\">la API Segmenter de Jina AI</a>. Finalmente, compartiremos lo que hemos aprendido y algunas reflexiones para el futuro.</p><h2 id=\"segmentation-problem\">Problema de Segmentaci√≥n</h2><p>La segmentaci√≥n es un elemento central en los sistemas RAG. La forma en que dividimos documentos largos en segmentos coherentes y manejables afecta directamente la calidad tanto de los pasos de recuperaci√≥n como de generaci√≥n, influyendo en todo, desde la relevancia de las respuestas hasta la calidad de la sumarizaci√≥n. Los m√©todos tradicionales de segmentaci√≥n han producido resultados decentes pero no est√°n exentos de limitaciones.</p><p>Parafraseando nuestro <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\">post anterior</a>:</p><blockquote>Al segmentar un documento largo, un desaf√≠o clave es decidir d√≥nde crear los segmentos. Esto puede hacerse usando longitudes fijas de tokens, un n√∫mero determinado de oraciones, o m√©todos m√°s avanzados como regex y modelos de segmentaci√≥n sem√°ntica. Establecer l√≠mites precisos de segmentos es crucial, ya que no solo mejora la legibilidad de los resultados de b√∫squeda sino que tambi√©n asegura que los segmentos proporcionados a un LLM en un sistema RAG sean tanto precisos como suficientes.</blockquote><p>Si bien el late chunking mejora el rendimiento de recuperaci√≥n, <strong>en aplicaciones RAG, es crucial asegurar que, en la medida de lo posible, cada segmento sea significativo por s√≠ mismo, y no solo un fragmento aleatorio de texto.</strong> Los LLM dependen de datos coherentes y bien estructurados para generar respuestas precisas. Si los segmentos est√°n incompletos o carecen de significado, el LLM puede tener dificultades con el contexto y la precisi√≥n, afectando el rendimiento general a pesar de los beneficios del late chunking. En resumen, ya sea que uses o no late chunking, tener una estrategia s√≥lida de segmentaci√≥n es esencial para construir un sistema RAG efectivo (como ver√°s en la secci√≥n de benchmark m√°s adelante).</p><p>Los m√©todos tradicionales de segmentaci√≥n, ya sea dividiendo el contenido en l√≠mites simples como nuevas l√≠neas u oraciones, o usando reglas r√≠gidas basadas en tokens, a menudo enfrentan las mismas limitaciones. Ambos enfoques fallan en tener en cuenta los l√≠mites sem√°nticos y tienen dificultades con temas ambiguos, llevando a segmentos fragmentados. Para abordar estos desaf√≠os, desarrollamos y entrenamos un modelo de lenguaje peque√±o espec√≠ficamente para segmentaci√≥n, dise√±ado para capturar cambios de tema y mantener la coherencia mientras permanece eficiente y adaptable a trav√©s de varias tareas.</p><h2 id=\"why-small-language-model\">¬øPor qu√© un Modelo de Lenguaje Peque√±o?</h2><p>Desarrollamos un Modelo de Lenguaje Peque√±o (SLM) para abordar limitaciones espec√≠ficas que encontramos con t√©cnicas tradicionales de segmentaci√≥n, particularmente al manejar fragmentos de c√≥digo y otras estructuras complejas como tablas, listas y f√≥rmulas. En enfoques tradicionales, que a menudo dependen de conteos de tokens o reglas estructurales r√≠gidas, era dif√≠cil mantener la integridad del contenido sem√°nticamente coherente. Por ejemplo, los fragmentos de c√≥digo frecuentemente se segmentaban en m√∫ltiples partes, rompiendo su contexto y dificultando que los sistemas downstream los entendieran o recuperaran con precisi√≥n.</p><p>Al entrenar un SLM especializado, buscamos crear un modelo que pudiera reconocer y preservar inteligentemente estos l√≠mites significativos, asegurando que los elementos relacionados permanecieran juntos. Esto no solo mejora la calidad de recuperaci√≥n en sistemas RAG sino que tambi√©n mejora tareas downstream como sumarizaci√≥n y respuesta a preguntas, donde mantener segmentos coherentes y contextualmente relevantes es cr√≠tico. El enfoque SLM ofrece una soluci√≥n m√°s adaptable y espec√≠fica para la tarea que los m√©todos tradicionales de segmentaci√≥n, con sus l√≠mites r√≠gidos, simplemente no pueden proporcionar.</p><h2 id=\"training-slms-three-approaches\">Entrenando SLMs: Tres Enfoques</h2><p>Entrenamos tres versiones de nuestro SLM:</p><ul><li><code>simple-qwen-0.5</code> es el modelo m√°s directo, dise√±ado para identificar l√≠mites bas√°ndose en los elementos estructurales del documento. Su simplicidad lo hace una soluci√≥n eficiente para necesidades b√°sicas de segmentaci√≥n.</li><li><code>topic-qwen-0.5</code>, inspirado en el razonamiento Chain-of-Thought, lleva la segmentaci√≥n un paso m√°s all√° al identificar temas dentro del texto, como \"el comienzo de la Segunda Guerra Mundial\", y usar estos temas para definir l√≠mites de segmentos. Este modelo asegura que cada segmento sea coherente tem√°ticamente, haci√©ndolo adecuado para documentos complejos con m√∫ltiples temas. Las pruebas iniciales mostraron que sobresale en segmentar contenido de una manera que refleja estrechamente la intuici√≥n humana.</li><li><code>summary-qwen-0.5</code> no solo identifica l√≠mites de texto sino que tambi√©n genera res√∫menes para cada segmento. Resumir segmentos es altamente ventajoso en aplicaciones RAG, especialmente para tareas como responder preguntas sobre documentos largos, aunque viene con el compromiso de demandar m√°s datos durante el entrenamiento.</li></ul><p>Todos los modelos devuelven solo <em>encabezados de segmento</em>‚Äîuna versi√≥n truncada de cada segmento. En lugar de generar segmentos completos, los modelos producen puntos clave o subtemas, lo que mejora la detecci√≥n de l√≠mites y la coherencia al enfocarse en transiciones sem√°nticas en lugar de simplemente copiar el contenido de entrada. Al recuperar los segmentos, el texto del documento se divide bas√°ndose en esos encabezados de segmento, y los segmentos completos se reconstruyen en consecuencia.</p><h3 id=\"dataset\">Dataset</h3><p>Usamos el dataset <a href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\">wiki727k</a>, una colecci√≥n a gran escala de fragmentos de texto estructurados extra√≠dos de art√≠culos de Wikipedia. Contiene m√°s de 727,000 secciones de texto, cada una representando una parte distinta de un art√≠culo de Wikipedia, como una introducci√≥n, secci√≥n o subsecci√≥n.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - koomri/text-segmentation: Implementation of the paper: Text Segmentation as a Supervised Learning Task</div><div class=\"kg-bookmark-description\">Implementaci√≥n del paper: Text Segmentation as a Supervised Learning Task - koomri/text-segmentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">koomri</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/a0a75db005774d424366f3fa2d4c70930927a0b2d8032ef3c04cb0f3beebcb8e/koomri/text-segmentation\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"data-augmentation\">Aumento de Datos</h3><p>Para generar pares de entrenamiento para cada variante del modelo, usamos GPT-4 para aumentar nuestros datos. Para cada art√≠culo en nuestro conjunto de datos de entrenamiento, enviamos el prompt:</p><pre><code class=\"language-python\">f\"\"\"\nGenerate a five to ten words topic and a one sentence summary for this chunk of text.\n```\n{text}\n```\nMake sure the topic is concise and the summary covers the main topic as much as possible.\n\nPlease respond in the following format:\n```\nTopic: ...\nSummary: ...\n```\n\nDirectly respond with the required topic and summary, do not include any other details, and do not surround your response with quotes, backticks or other separators.\n   \"\"\".strip()</code></pre><p>Usamos una divisi√≥n simple para generar secciones de cada art√≠culo, dividiendo en <code>\\\\n\\\\n\\\\n</code>, y luego subdividiendo en <code>\\\\n\\\\n</code> para obtener lo siguiente (en este caso, un art√≠culo sobre Common Gateway Interface):</p><pre><code>[\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n</code></pre><p>Luego generamos una estructura JSON con las secciones, temas y res√∫menes:</p><pre><code>{\n  \"sections\": [\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n  \"topics\": [\n    \"Common Gateway Interface en Servidores Web\",\n    \"Historia y Estandarizaci√≥n del CGI\",\n    \"Scripts CGI para Edici√≥n de P√°ginas Web\",\n    \"Reducci√≥n de Sobrecarga en Servidores Web en Invocaci√≥n de Comandos\"\n  ],\n  \"summaries\": [\n    \"CGI proporciona un protocolo para que los servidores web ejecuten programas que generan p√°ginas web din√°micas.\",\n    \"El NCSA defini√≥ inicialmente CGI en 1993, llevando a su adopci√≥n como est√°ndar para servidores Web y posterior formalizaci√≥n en RFC 3875 presidido por Ken Coar.\",\n    \"Este texto describe c√≥mo un script CGI puede manejar la edici√≥n y guardado de contenido de p√°ginas web a trav√©s de formularios HTML.\",\n    \"El texto discute t√©cnicas para minimizar la sobrecarga del servidor por la invocaci√≥n frecuente de comandos, incluyendo el prefork de procesos, uso de programas CGI precompilados e implementaci√≥n de m√≥dulos de servidor web personalizados.\"\n  ]\n}\n</code></pre><p>Tambi√©n agregamos ruido mezclando datos, a√±adiendo caracteres/palabras/letras aleatorias, eliminando puntuaci√≥n al azar, y <em>siempre</em> eliminando caracteres de nueva l√≠nea.</p><p>Todo eso puede ayudar en parte a desarrollar un buen modelo - pero solo hasta cierto punto. Para realmente aprovechar al m√°ximo necesit√°bamos que el modelo creara fragmentos coherentes sin romper los fragmentos de c√≥digo. Para esto, aumentamos el conjunto de datos con c√≥digo, f√≥rmulas y listas generadas por GPT-4o.</p><h3 id=\"the-training-setup\">La Configuraci√≥n del Entrenamiento</h3><p>Para entrenar los modelos, implementamos la siguiente configuraci√≥n:</p><ul><li><strong>Framework</strong>: Usamos la biblioteca <code>transformers</code> de Hugging Face integrada con <code>Unsloth</code> para optimizaci√≥n del modelo. Esto fue crucial para optimizar el uso de memoria y acelerar el entrenamiento, haciendo posible entrenar modelos peque√±os con grandes conjuntos de datos de manera efectiva.</li><li><strong>Optimizador y Planificador</strong>: Usamos el optimizador AdamW con una programaci√≥n lineal de tasa de aprendizaje y pasos de calentamiento, permiti√©ndonos estabilizar el proceso de entrenamiento durante las √©pocas iniciales.</li><li><strong>Seguimiento de Experimentos</strong>: Rastreamos todos los experimentos de entrenamiento usando <a href=\"https://wandb.ai/?ref=jina-ai-gmbh.ghost.io\">Weights & Biases</a>, y registramos m√©tricas clave como p√©rdida en entrenamiento y validaci√≥n, cambios en la tasa de aprendizaje y rendimiento general del modelo. Este seguimiento en tiempo real nos proporcion√≥ informaci√≥n sobre c√≥mo progresaban los modelos, permitiendo ajustes r√°pidos cuando era necesario para optimizar los resultados del aprendizaje.</li></ul><h3 id=\"the-training-itself\">El Entrenamiento en S√≠</h3><p>Usando <a href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=jina-ai-gmbh.ghost.io\"><code>qwen2-0.5b-instruct</code></a> como modelo base, entrenamos tres variantes de nuestro SLM con Unsloth, cada una con una estrategia de segmentaci√≥n diferente en mente. Para nuestras muestras usamos pares de entrenamiento, que consist√≠an en el texto de un art√≠culo de wiki727k y los resultantes <code>sections</code>, <code>topics</code>, o <code>summaries</code> (mencionados arriba en la secci√≥n \"Aumento de Datos\") dependiendo del modelo que se estaba entrenando.</p><ul><li><code><strong>simple-qwen-0.5</strong></code>: Entrenamos <code>simple-qwen-0.5</code> con 10,000 muestras durante 5,000 pasos, logrando una convergencia r√°pida y detectando efectivamente los l√≠mites entre secciones cohesivas de texto. La p√©rdida de entrenamiento fue de 0.16.</li><li><code><strong>topic-qwen-0.5</strong></code>: Al igual que <code>simple-qwen-0.5</code>, entrenamos <code>topic-qwen-0.5</code> con 10,000 muestras durante 5,000 pasos, logrando una p√©rdida de entrenamiento de 0.45.</li><li><code><strong>summary-qwen-0.5</strong></code>: Entrenamos <code>summary-qwen-0.5</code> con 30,000 muestras durante 15,000 pasos. Este modelo mostr√≥ promesa pero tuvo una p√©rdida m√°s alta (0.81) durante el entrenamiento, sugiriendo la necesidad de m√°s datos (aproximadamente el doble de nuestra cantidad original de muestras) para alcanzar su potencial completo.</li></ul><h2 id=\"the-segments-themselves\">Los Segmentos en S√≠</h2><p>Aqu√≠ hay ejemplos de tres segmentos consecutivos de cada estrategia de segmentaci√≥n, junto con la API de Segmentador de Jina. Para producir estos segmentos primero usamos <a href=\"https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io\">Jina Reader</a> para extraer un post del blog de Jina AI como texto plano (incluyendo todos los datos de la p√°gina, como encabezados, pies de p√°gina, etc), luego lo pasamos a cada m√©todo de segmentaci√≥n.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/can-embedding-reranker-models-compare-numbers/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Can Embedding/Reranker Models Compare Numbers?</div><div class=\"kg-bookmark-description\">A lot of LLMs can't figure out that 9.11 is actually smaller than 9.9. Can our embedding and reranker models do any better?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"jina-segmenter-api\">API del Segmentador de Jina</h3><p>La API del Segmentador de Jina tom√≥ un enfoque muy granular para segmentar el post, dividiendo en caracteres como <code>\\n</code>, <code>\\t</code>, etc, para dividir el texto en segmentos a menudo muy peque√±os. Solo mirando los primeros tres, extrajo <code>search\\\\n</code>, <code>notifications\\\\n</code> y <code>NEWS\\\\n</code> de la barra de navegaci√≥n del sitio web, pero nada relevante al contenido del post en s√≠:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png\" class=\"kg-image\" alt=\"Barra de navegaci√≥n minimalista con texto &quot;NEWS&quot;, &quot;PRODUCTS&quot;, y &quot;COMPANY&quot; sobre fondo negro, acentuada por franjas coloridas para \" loading=\"lazy\" width=\"1164\" height=\"68\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>M√°s adelante, por fin obtuvimos algunos segmentos del contenido real del blog post, aunque se retuvo poco contexto en cada uno:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png\" class=\"kg-image\" alt=\"P√°gina web que discute si los modelos de embedding/reranker pueden comparar n√∫meros, con una cuadr√≠cula de c√≠rculos numerados y referencias a un ICM\" loading=\"lazy\" width=\"1164\" height=\"1180\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>(En aras de la equidad, mostramos m√°s fragmentos de la API del Segmentador que para los modelos, simplemente porque de otro modo tendr√≠a muy pocos segmentos significativos para mostrar)</p><h3 id=\"simple-qwen-05\"><code>simple-qwen-0.5</code></h3><p><code>simple-qwen-0.5</code> dividi√≥ el post del blog bas√°ndose en la estructura sem√°ntica, extrayendo segmentos mucho m√°s largos que ten√≠an un significado coherente:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png\" class=\"kg-image\" alt=\"Captura de pantalla de p√°gina web con fondo verde, barra de navegaci√≥n superior, gr√°ficos cient√≠ficos y encabezados que discuten la comparaci√≥n de n√∫meros del modelo\" loading=\"lazy\" width=\"1164\" height=\"4590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"topic-qwen-05\"><code>topic-qwen-0.5</code></h3><p><code>topic-qwen-0.5</code> primero identific√≥ temas basados en el contenido del documento, luego segment√≥ el documento basado en esos temas:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png\" class=\"kg-image\" alt=\"P√°gina web mostrando un art√≠culo cient√≠fico titulado &quot;¬øPueden los Modelos Embedding/Keras Comparar N√∫meros?&quot; con gr√°ficos, bloques de texto y \" loading=\"lazy\" width=\"1164\" height=\"4526\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"summary-qwen-05\"><code>summary-qwen-0.5</code></h3><p><code>summary-qwen-0.5</code> identific√≥ l√≠mites de segmentos y gener√≥ un resumen del contenido dentro de cada segmento:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png\" class=\"kg-image\" alt=\"P√°gina web acad√©mica con tema verde y dorado que discute modelos embedding/reranker y configuraci√≥n de experimentos.\" loading=\"lazy\" width=\"1164\" height=\"3734\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"benchmarking-the-models\">Evaluaci√≥n Comparativa de los Modelos</h2><p>Para evaluar el rendimiento de nuestros modelos, extrajimos ocho posts del blog de Jina AI y generamos seis preguntas y respuestas verdaderas usando GPT-4o.</p><p>Aplicamos cada m√©todo de segmentaci√≥n, incluyendo la API del Segmentador de Jina, a estos posts del blog, y luego generamos embeddings para los segmentos resultantes usando <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v3</code></a>, sin chunking tard√≠o ni reranking.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/v3banner.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Cada conjunto de segmentos se index√≥ por separado y usamos un sistema RAG para consultar cada √≠ndice con las preguntas generadas previamente.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Para mantener justas las comparaciones, al probar <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> y <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> indexamos solo los segmentos en s√≠, <i>no</i> los res√∫menes o temas generados. Indexar estos datos adicionales probablemente mejorar√≠a a√∫n m√°s el rendimiento.</div></div><p>Para medir el rendimiento, comparamos las respuestas generadas del √≠ndice de cada m√©todo de segmentaci√≥n con las respuestas verdaderas usando puntuaciones F1 de la <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)?ref=jina-ai-gmbh.ghost.io\">evaluaci√≥n ROUGE-L</a>, donde puntuaciones m√°s altas indican mejor rendimiento.</p><p>Por ejemplo, dada la pregunta: \"¬øQu√© factores impactan las capacidades de comparaci√≥n num√©rica del modelo <code>jina-embeddings-v2-base-en</code>?\", recibimos las siguientes respuestas:</p><h3 id=\"ground-truth-generated-by-gpt-4o-from-the-full-post-text\">Verdad Fundamental (Generada por GPT-4o del Texto Completo del Post):</h3><pre><code>\"The two key factors are tokenization and training data. Tokenization determines how numbers are segmented into tokens, and training data influences numerical reasoning abilities.\"\n</code></pre><h3 id=\"jina-segmenter-api-score-0166667\">Jina Segmenter API (puntuaci√≥n: 0.166667)</h3><pre><code>\"The context does not provide specific factors that impact the numerical comparison abilities of the jina-embeddings-v2-base-en model.\"\n</code></pre><h3 id=\"simple-qwen-05-score-0279070\"><code>simple-qwen-0.5</code> (puntuaci√≥n: 0.279070)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by tokenization and training data.\"\n</code></pre><h3 id=\"topic-qwen-05-score-0190476\"><code>topic-qwen-0.5</code> (puntuaci√≥n: 0.190476)</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by factors such as tokenization and its ability to handle different numerical formats, including small integers, large numbers, floats, negative numbers, currency, dates, and times.\"\n</code></pre><h3 id=\"summary-qwen-05-0318182\"><code>summary-qwen-0.5</code> (0.318182)</h3><pre><code>\"The factors impacting the numerical comparison abilities of the jina-embeddings-v2-base-en model are tokenization and training data.\"\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>¬øPor qu√© es tan baja la puntuaci√≥n de </b><b><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code></b><b>?</b><br>Esto es principalmente una casualidad basada en la pregunta particular que le hicimos al modelo. Como puedes ver en la tabla siguiente, la puntuaci√≥n ROUGE <i>promedio</i> de <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> es la m√°s alta de todas las metodolog√≠as de segmentaci√≥n.</div></div><p>Tambi√©n evaluamos la velocidad de cada m√©todo (midiendo cu√°nto tiempo tom√≥ generar e incrustar segmentos) y estimamos el espacio en disco (multiplicando el conteo de embeddings por el tama√±o de un solo embedding de 1024 dimensiones de <code>jina-embeddings-v3</code>). Esto nos permiti√≥ evaluar tanto la precisi√≥n como la eficiencia entre las diferentes estrategias de segmentaci√≥n.</p><h2 id=\"key-findings\">Hallazgos Clave</h2><p>Despu√©s de probar las variantes del modelo entre s√≠ y contra la API de Jina Segmenter, nos dimos cuenta de que los nuevos modelos de hecho mostraron puntuaciones mejoradas usando los tres m√©todos, especialmente la segmentaci√≥n por temas:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png\" class=\"kg-image\" alt=\"Gr√°fico de barras comparando las puntuaciones ROUGE promedio para Jina Segmenter, Simple, COATopic y Summary Segmentation.\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>M√©todo de Segmentaci√≥n</strong></th>\n<th><strong>Puntuaci√≥n ROUGE Promedio</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>0.352126</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>0.386096</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td><strong>0.398340</strong></td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>0.328143</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">¬øPor qu√© <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> tiene una puntuaci√≥n ROUGE m√°s baja que <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code>? En resumen, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> mostr√≥ una p√©rdida m√°s alta durante el entrenamiento, revelando la necesidad de m√°s entrenamiento para recibir mejores resultados. Eso podr√≠a ser tema de experimentaci√≥n futura.</div></div><p>Sin embargo, ser√≠a interesante revisar los resultados con la funci√≥n de chunking tard√≠o de <code>jina-embeddings-v3</code>, que aumenta la relevancia contextual de los embeddings de segmentos, proporcionando resultados m√°s relevantes. Eso podr√≠a ser tema para un futuro post del blog.</p><p>Respecto a la velocidad, puede ser dif√≠cil comparar los nuevos modelos con Jina Segmenter, ya que este √∫ltimo es una API, mientras que ejecutamos los tres modelos en una GPU Nvidia 3090. Como puedes ver, cualquier rendimiento ganado durante el r√°pido paso de segmentaci√≥n de la API Segmenter es r√°pidamente superado por la necesidad de generar embeddings para tantos segmentos:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png\" class=\"kg-image\" alt=\"Gr√°fico de barras mostrando el tiempo para m√©todos de segmentaci√≥n de texto: Jina Segmenter, Simple, CoT Topic y Summary Segmentation, con notas\" loading=\"lazy\" width=\"1682\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png 1682w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png\" class=\"kg-image\" alt=\"Gr√°fico de barras vertical mostrando los tiempos de embedding para Jina Segmenter, Simple, CoT Topic y Summary Segmentation.\" loading=\"lazy\" width=\"1698\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png 1698w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\"><b>Notas</b><br>‚Ä¢ Usamos diferentes ejes Y en ambos gr√°ficos porque presentar marcos de tiempo tan diferentes con un solo gr√°fico o ejes Y consistentes no era factible.<br>‚Ä¢ Ya que est√°bamos realizando esto puramente como un experimento, no usamos procesamiento por lotes al generar embeddings. Hacerlo acelerar√≠a sustancialmente las operaciones para todos los m√©todos.</div></div><p>Naturalmente, m√°s segmentos significa m√°s embeddings. Y esos embeddings ocupan mucho espacio: Los embeddings para los ocho posts del blog que probamos ocuparon m√°s de 21 MB con la API Segmenter, mientras que Summary Segmentation ocup√≥ un elegante 468 KB. Esto, m√°s las puntuaciones ROUGE m√°s altas de nuestros modelos significa menos segmentos pero mejores segmentos, ahorrando dinero y aumentando el rendimiento:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png\" class=\"kg-image\" alt=\"Gr√°fico de barras vertical que compara el tama√±o total de embeddings de m√©todos de segmentaci√≥n, con &quot;Jina Segmenter&quot; significativamente m√°s alto en 20.0\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Segmentation Method</strong></th>\n<th><strong>Segment Count</strong></th>\n<th><strong>Average Length (characters)</strong></th>\n<th><strong>Segmentation Time (minutes/seconds)</strong></th>\n<th><strong>Embedding Time (hours/minutes)</strong></th>\n<th><strong>Total Embedding Size</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>1,755</td>\n<td>82</td>\n<td>3.8s</td>\n<td>1h 46m</td>\n<td>21.06 MB</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>48</td>\n<td>1,692</td>\n<td>49s</td>\n<td>1h 2m</td>\n<td>576 KB</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td>69</td>\n<td>1,273</td>\n<td>2m 3s</td>\n<td>1h 6m</td>\n<td>828 KB</td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>39</td>\n<td>1,799</td>\n<td>2m 40s</td>\n<td>53m</td>\n<td>468 KB</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"what-we-learned\">Lo que Aprendimos</h3><h3 id=\"problem-formulation-is-critical\">La Formulaci√≥n del Problema es Cr√≠tica</h3><p>Una conclusi√≥n clave fue el impacto de c√≥mo enfocamos la tarea. Al hacer que el modelo genere encabezados de segmentos, mejoramos la detecci√≥n de l√≠mites y la coherencia al centrarnos en las transiciones sem√°nticas en lugar de simplemente copiar y pegar el contenido de entrada en segmentos separados. Esto tambi√©n result√≥ en un modelo de segmentaci√≥n m√°s r√°pido, ya que generar menos texto permiti√≥ que el modelo completara la tarea m√°s r√°pidamente.</p><h3 id=\"llm-generated-data-is-effective\">Los Datos Generados por LLM son Efectivos</h3><p>El uso de datos generados por LLM, particularmente para contenido complejo como listas, f√≥rmulas y fragmentos de c√≥digo, ampli√≥ el conjunto de entrenamiento del modelo y mejor√≥ su capacidad para manejar diversas estructuras de documentos. Esto hizo que el modelo fuera m√°s adaptable a trav√©s de diversos tipos de contenido, una ventaja crucial cuando se trata de documentos t√©cnicos o estructurados.</p><h3 id=\"output-only-data-collation\">Recopilaci√≥n de Datos Solo de Salida</h3><p>Al usar un <a href=\"https://huggingface.co/docs/transformers/en/main_classes/data_collator?ref=jina-ai-gmbh.ghost.io\">recopilador de datos</a> solo de salida, nos aseguramos de que el modelo se centrara en predecir los tokens objetivo durante el entrenamiento, en lugar de simplemente copiar de la entrada. El recopilador solo de salida asegur√≥ que el modelo aprendiera de las secuencias objetivo reales, enfatizando las completaciones o l√≠mites correctos. Esta distinci√≥n permiti√≥ que el modelo convergiera m√°s r√°pido al evitar el sobreajuste a la entrada y ayud√≥ a que generalizara mejor a trav√©s de diferentes conjuntos de datos.</p><h3 id=\"efficient-training-with-unsloth\">Entrenamiento Eficiente con Unsloth</h3><p>Con <a href=\"https://github.com/unslothai/unsloth?ref=jina-ai-gmbh.ghost.io\">Unsloth</a>, optimizamos el entrenamiento de nuestro modelo de lenguaje peque√±o, logrando ejecutarlo en una GPU Nvidia 4090. Esta pipeline optimizada nos permiti√≥ entrenar un modelo eficiente y performante sin necesidad de recursos computacionales masivos.</p><h3 id=\"handling-complex-texts\">Manejo de Textos Complejos</h3><p>Los modelos de segmentaci√≥n sobresalieron en el manejo de documentos complejos que contienen c√≥digo, tablas y listas, que t√≠picamente son dif√≠ciles para los m√©todos m√°s tradicionales. Para contenido t√©cnico, estrategias sofisticadas como <code>topic-qwen-0.5</code> y <code>summary-qwen-0.5</code> fueron m√°s efectivas, con el potencial de mejorar las tareas RAG posteriores.</p><h3 id=\"simple-methods-for-simpler-content\">M√©todos Simples para Contenido m√°s Simple</h3><p>Para contenido sencillo basado en narrativa, los m√©todos m√°s simples como el API de Segmenter suelen ser suficientes. Las estrategias de segmentaci√≥n avanzadas pueden ser necesarias solo para contenido m√°s complejo y estructurado, permitiendo flexibilidad seg√∫n el caso de uso.</p><h2 id=\"next-steps\">Pr√≥ximos Pasos</h2><p>Si bien este experimento fue dise√±ado principalmente como una prueba de concepto, si fu√©ramos a extenderlo m√°s, podr√≠amos hacer varias mejoras. Primero, aunque es poco probable la continuaci√≥n de este experimento espec√≠fico, entrenar <code>summary-qwen-0.5</code> en un conjunto de datos m√°s grande ‚Äîidealmente 60,000 muestras en lugar de 30,000‚Äî probablemente llevar√≠a a un rendimiento m√°s √≥ptimo. Adem√°s, refinar nuestro proceso de evaluaci√≥n comparativa ser√≠a beneficioso. En lugar de evaluar las respuestas generadas por LLM del sistema RAG, nos centrar√≠amos en cambio en comparar los segmentos recuperados directamente con la verdad fundamental. Finalmente, ir√≠amos m√°s all√° de las puntuaciones ROUGE y adoptar√≠amos m√©tricas m√°s avanzadas (posiblemente una combinaci√≥n de ROUGE y puntuaci√≥n LLM) que capturan mejor los matices de la calidad de recuperaci√≥n y segmentaci√≥n.</p><h2 id=\"conclusion\">Conclusi√≥n</h2><p>En este experimento, exploramos c√≥mo los modelos de segmentaci√≥n personalizados dise√±ados para tareas espec√≠ficas pueden mejorar el rendimiento de RAG. Al desarrollar y entrenar modelos como <code>simple-qwen-0.5</code>, <code>topic-qwen-0.5</code>, y <code>summary-qwen-0.5</code>, abordamos desaf√≠os clave encontrados en los m√©todos de segmentaci√≥n tradicionales, particularmente en mantener la coherencia sem√°ntica y manejar efectivamente contenido complejo como fragmentos de c√≥digo. Entre los modelos probados, <code>topic-qwen-0.5</code> entreg√≥ consistentemente la segmentaci√≥n m√°s significativa y contextualmente relevante, especialmente para documentos multi-tema.</p><p>Si bien los modelos de segmentaci√≥n proporcionan la base estructural necesaria para los sistemas RAG, cumplen una funci√≥n diferente en comparaci√≥n con el chunking tard√≠o, que optimiza el rendimiento de recuperaci√≥n manteniendo la relevancia contextual a trav√©s de los segmentos. Estos dos enfoques pueden ser complementarios, pero la segmentaci√≥n es particularmente crucial cuando se necesita un m√©todo que se centre en dividir documentos para flujos de trabajo de generaci√≥n coherentes y espec√≠ficos para cada tarea.</p>",
  "comment_id": "67126986708dbe00019249f2",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/breakpoints.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-18T15:58:30.000+02:00",
  "updated_at": "2024-10-25T20:14:53.000+02:00",
  "published_at": "2024-10-25T10:35:07.000+02:00",
  "custom_excerpt": "We trained three small language models to better segment long documents into chunks, and here are the key lessons we learned.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/finding-optimal-breakpoints-in-long-documents-using-small-language-models/",
  "excerpt": "Entrenamos tres modelos de lenguaje peque√±os para segmentar mejor los documentos largos en fragmentos, y estas son las lecciones clave que aprendimos.",
  "reading_time": 19,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast.",
  "feature_image_caption": null
}
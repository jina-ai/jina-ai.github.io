{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "Puntuación Justa para Documentos Multimodales con jina-reranker-m0",
  "html": "<p>Imagina que estás creando un sistema de búsqueda de noticias deportivas. Un usuario busca \"jugadores de tenis celebrando la victoria del campeonato\" y necesitas encontrar los artículos más relevantes de tu base de datos. Cada artículo contiene tanto un texto como una imagen, algo típico de la cobertura deportiva moderna.</p><p>Tu sistema necesita tomar una <strong>consulta de texto</strong> y devolver una <strong>lista clasificada de los documentos multimodales más relevantes</strong> de tu corpus. Suena sencillo, pero hay un problema fundamental que rompe todos los enfoques obvios.</p><p>Esto es lo que sucede cuando intentas clasificar estos documentos. Tu modelo de \"向量模型 (embedding)\" digamos <code>jina-clip-v2</code> produce puntuaciones de similitud como esta:</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>Artículo</th>\n            <th>Tipo de contenido</th>\n            <th>Descripción</th>\n            <th>Puntuación de similitud</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>Texto</td>\n            <td>Novak Djokovic gana la final del Abierto de Australia en sets corridos</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>Imagen</td>\n            <td>[foto de un jugador sosteniendo un trofeo y sonriendo]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Texto</td>\n            <td>Los retrasos climáticos afectan la programación del torneo al aire libre</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Imagen</td>\n            <td>[foto de jugadores de tenis saltando y celebrando]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>¿Qué artículo es más relevante? El artículo A tiene una puntuación de texto alta pero una puntuación de imagen baja. El artículo B tiene una puntuación de texto baja pero una puntuación de imagen más alta. El desafío fundamental es que <strong>no puedes comparar 0.72 (texto) con 0.54 (imagen)</strong> porque estas puntuaciones de similitud existen en escalas completamente diferentes.</p><h2 id=\"when-trivial-solutions-fail\">Cuando las soluciones triviales fallan</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>Debido a la brecha de modalidad</strong> en <code>jina-clip-v2</code> o en casi todos los demás modelos similares a CLIP, cualquier enfoque obvio que puedas intentar no funciona. Si solo usas la puntuación más alta, te encuentras con el hecho de que las puntuaciones de texto se agrupan alrededor de 0.2-0.8, mientras que las puntuaciones de imagen se agrupan alrededor de 0.4-0.6. Esto significa que una coincidencia de texto mediocre (0.6) siempre vencerá a una coincidencia de imagen excelente (0.5).</p><p>Promediar las puntuaciones tampoco ayuda. Calcular (0.7 + 0.3)/2 = 0.5 te da un número, pero ¿qué significa realmente? Estás promediando cantidades fundamentalmente sin sentido. De manera similar, cualquier esquema de ponderación fija es arbitrario: a veces el texto importa más, a veces las imágenes, y esto depende completamente de la consulta y el documento específicos.</p><p>Incluso normalizar las puntuaciones primero no resuelve el problema central. Todavía estás tratando de combinar medidas de similitud fundamentalmente diferentes que capturan diferentes aspectos de la relevancia.</p><h2 id=\"what-actually-happens\">Lo que realmente sucede</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Para tener una mejor idea de con qué estamos trabajando, aquí hay un documento de ejemplo del <a href=\"https://arxiv.org/abs/2305.13631\">conjunto de datos EDIS</a>, que muestra la imagen (un partido de fútbol alemán) y el título (<code>One More Field Where the Content Trails Germany</code>).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 1: Documento multimodal de ejemplo que contiene tanto imagen como texto. Dado que tenemos dos modalidades, para cualquier consulta dada ahora hay </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">dos</em></i><span style=\"white-space: pre-wrap;\"> brechas semánticas (entre la consulta y el texto, y la consulta y la imagen). Para obtener los mejores resultados, ¿deberíamos buscar en el contenido de texto de los documentos o en el contenido de la imagen?</span></figcaption></figure><p>En general, <code>jina-clip-v2</code> muestra similitudes mucho mayores al comparar consulta a texto que consulta a imagen en el conjunto de datos EDIS, en parte debido a la forma en que se entrenó el modelo y en parte debido al propio conjunto de datos:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 2: Puntuaciones de similitud entre consulta a imagen (en rojo) y consulta a texto (en azul) usando </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Por lo tanto, parece lógico recuperar un documento en función de su texto en lugar de su imagen. Y, como podemos ver en el gráfico a continuación, obtenemos resultados mucho mejores al comparar la consulta de texto <code>... for undocumented immigrants helping to establish legal status in the United States</code> con el contenido de texto del corpus. De hecho, la búsqueda por imagen no logra recuperar el documento de verdad fundamental (resaltado en amarillo) en absoluto:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 3: Ejemplo donde el documento de verdad fundamental (resaltado con borde amarillo) se puede recuperar solo a través de la recuperación de consulta a texto de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> al usar </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 3.</span></figcaption></figure><p>Pero no te dejes engañar. A pesar de que la consulta a texto muestra puntuaciones de similitud más altas, las puntuaciones de similitud de consulta a texto y consulta a imagen <em>no</em> son comparables. Podemos ver esto al observar recall@10 cuando usamos <code>jina-clip-v2</code> para recuperar 32 documentos del conjunto de datos EDIS. Claramente, el recall es más alto con consulta a <em>imagen</em>:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Consulta a texto</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Consulta a imagen</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Podemos ver esto a continuación: si usamos una consulta del conjunto de datos, <code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>, podemos recuperar el documento de verdad fundamental solo por su contenido de imagen. La búsqueda por su contenido de texto no devuelve ninguna coincidencia:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 4: Ejemplo donde el documento de verdad fundamental (resaltado con borde amarillo) se puede recuperar solo a través de la recuperación de consulta a imagen de </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> al usar </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 3.</span></figcaption></figure><p>Entonces, si las puntuaciones de similitud implican que deberíamos recuperar documentos de su texto, y la exhaustividad implica que deberíamos recuperarlos de sus imágenes, ¿qué deberíamos elegir? Ciertamente, las figuras 3 y 4 no sugieren un ganador absoluto. ¿Qué modalidad presenta <em>realmente</em> la coincidencia más cercana entre nuestra consulta y el documento que estamos buscando? Y si queremos fusionar candidatos tanto de la recuperación de consulta a texto como de la recuperación de consulta a imagen, ¿cómo podemos seleccionar significativamente las mejores coincidencias si ni siquiera podemos comparar las puntuaciones? Claramente, solo usar <code>jina-clip-v2</code> no será suficiente. Necesitamos añadir otro modelo a la mezcla.</p><h2 id=\"a-simple-two-stage-pipeline\">Una canalización simple de dos etapas</h2><p>En abril de 2025 lanzamos <code>jina-reranker-m0</code>, un 重排器 (Reranker) multimodal multilingüe para recuperar documentos visuales. Podemos ver su brecha de modalidad más estrecha a continuación, donde <code>jina-reranker-m0</code> muestra puntuaciones de similitud comparables de consulta a texto y de consulta a imagen, en contraste con la brecha mucho más amplia que muestra <code>jina-clip-v2</code>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 6: En comparación con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> muestra mucha menos diferencia entre las puntuaciones de similitud de consulta a imagen (rojo) y de consulta a texto (azul).</span></figcaption></figure><p>Con esto en mente, podemos usar <code>jina-reranker-m0</code> para una segunda pasada en la cadena de recuperación, después de que los resultados iniciales se recuperen de <code>jina-clip-v2</code>:</p><p><strong>Etapa 1: Recuperar candidatos de ambas modalidades</strong></p><ul><li>Usar <code>jina-clip-v2</code> para obtener 16 documentos mediante búsqueda de texto + 16 mediante búsqueda de imagen</li><li>Aceptar que aún no podemos comparar las puntuaciones</li></ul><p><strong>Etapa 2: 重排 (Reranking) unificado</strong></p><ul><li>Introducir cada par (consulta + documento completo) en <code>jina-reranker-m0</code></li><li>El 重排器 (Reranker) procesa tanto el texto COMO la imagen juntos</li><li>Salida: Puntuación de relevancia única en una escala unificada</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 5: Indexación de documentos multimodales y un proceso de recuperación multimodal de dos etapas con </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> y </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Ampliamos los experimentos de la Tabla 1, ahora usando <code>jina-clip-v2</code> para recuperar documentos del corpus, luego <code>jina-reranker-m0</code> para volver a clasificarlos:</p><ol><li>Recuperar 32 documentos mediante consulta a texto, luego volver a clasificarlos según la puntuación de consulta a texto.</li><li>Recuperar 32 documentos mediante consulta a imagen, luego volver a clasificarlos según la puntuación de consulta a imagen.</li><li>Recuperar 16 documentos mediante consulta a texto y 16 mediante consulta a imagen. Volver a clasificarlos según la puntuación de consulta a texto o consulta a imagen, según la modalidad de la consulta.</li><li>Recuperar 16 documentos mediante consulta a texto y 16 mediante consulta a imagen. Volver a clasificarlos según las puntuaciones promedio de consulta a texto y consulta a imagen de cada documento, dando una puntuación final de (consulta a texto + consulta a imagen)/2.</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Tenga en cuenta que estamos midiendo el rendimiento zero-shot en EDIS. No ajustamos ni <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> ni <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-reranker-m0</code> usando el conjunto de datos.</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Description</th>\n      <th>Recall@10 - with jina-clip-v2</th>\n      <th>Recall@10 - with jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 docs: query-to-text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 docs: query-to-image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image<br>Combined average reranker scores</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Los experimentos 1, 3 y 4 muestran el mismo resultado para recall@10 con <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> debido a que las puntuaciones de consulta a texto son más altas que las puntuaciones de consulta a imagen. Por lo tanto, los diez mejores resultados están dominados por los documentos recuperados a través del texto.</div></div><p>Como podemos ver, al realizar una segunda pasada con <code>jina-reranker-m0</code>, la exhaustividad aumenta en todos los ámbitos, independientemente de la modalidad. Sin embargo, <strong>vemos el mayor aumento cuando combinamos el contenido textual y de imagen de los documentos recuperados</strong>, alcanzando una exhaustividad@10 de 36.24. Un ejemplo visual muestra que <code>jina-reranker-m0</code> clasifica consistentemente el documento de verdad fundamental en primer lugar, ya sea buscando contenido de texto o de imagen:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Figura 7: Consultas de muestra (a la izquierda) y </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> de 1 resultado para cada metodología de 重排 (Reranking) (cuatro columnas a la derecha), que muestra que la combinación de puntuaciones de similitud de imagen y texto clasifica consistentemente el documento de verdad fundamental en primer lugar.</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Si bien las figuras 3 y 4 muestran un <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> de 3 para los diferentes métodos de recuperación, por razones de espacio, la figura 7 muestra solo <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> de 1 para cada consulta.</div></div><h2 id=\"conclusions\">Conclusiones</h2><p>Este enfoque simple de dos etapas ofrece una mejora del 62% en la exhaustividad porque el sistema finalmente aprovecha lo que los humanos hacen de forma natural: considerar tanto lo que leemos como lo que vemos para determinar la relevancia. La lección se extiende más allá de la búsqueda: cuando se trata de sistemas de IA multimodales, los enfoques de una sola pasada que tratan las modalidades por separado siempre se toparán con este muro de incompatibilidad de puntuación. Las arquitecturas de dos etapas que recuperan ampliamente y luego clasifican de forma inteligente se están volviendo esenciales. Pruebe <code>jina-reranker-m0</code> a través de nuestra API o en AWS, GCP y Azure.</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "Similitud de texto: 0.7. Similitud de imagen: 0.5. ¿Qué documento es más relevante? Literalmente no se puede saber, y ese es el problema principal que está afectando a la búsqueda multimodal. Lo solucionamos con la función de reordenamiento unificada (unified reranking).",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
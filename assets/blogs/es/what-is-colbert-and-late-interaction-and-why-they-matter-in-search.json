{
  "slug": "what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
  "id": "65d3a2134a32310001f5b71b",
  "uuid": "726c942b-f6a7-4c89-a0ad-39aaad98d02f",
  "title": "¿Qué es ColBERT y la Interacción Tardía y Por Qué Son Importantes en la Búsqueda?",
  "html": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina ColBERT v2: Recuperador de Interacción Tardía Multilingüe para Embedding y Reranking</div><div class=\"kg-bookmark-description\">Jina ColBERT v2 soporta 89 idiomas con rendimiento superior de recuperación, dimensiones de salida controladas por el usuario y longitud de token de 8192.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg\" alt=\"\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Actualización: El 31 de agosto de 2024, lanzamos la segunda versión de Jina-ColBERT, con mejor rendimiento, soporte multilingüe para más de 89 idiomas y dimensiones de salida flexibles. Consulta la publicación del lanzamiento para más detalles.</span></p></figcaption></figure><p>El viernes pasado, el lanzamiento del <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">modelo ColBERT por Jina AI en Hugging Face</a> generó un entusiasmo significativo en la comunidad de IA, particularmente en Twitter/X. Si bien muchos están familiarizados con el revolucionario modelo BERT, el revuelo alrededor de ColBERT ha dejado a algunos preguntándose: ¿Qué hace que ColBERT destaque en el saturado campo de las tecnologías de recuperación de información? ¿Por qué la comunidad de IA está entusiasmada con ColBERT de longitud 8192? Este artículo profundiza en las complejidades de ColBERT y ColBERTv2, destacando su diseño, mejoras y la sorprendente efectividad de la interacción tardía de ColBERT.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Maximiza la relevancia de búsqueda y la precisión RAG con facilidad</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">@huggingface</a> under Apache 2.0 licence<a href=\"https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io\">https://t.co/snVGgI753H</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">February 16, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><h2 id=\"what-is-colbert\">¿Qué es ColBERT?</h2><p>El nombre \"ColBERT\" significa <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, un modelo proveniente de la Universidad de Stanford, que aprovecha la comprensión profunda del lenguaje de BERT mientras introduce un nuevo mecanismo de interacción. Este mecanismo, conocido como <strong>interacción tardía</strong>, permite una recuperación eficiente y precisa al procesar consultas y documentos por separado hasta las etapas finales del proceso de recuperación. Específicamente, hay dos versiones del modelo:</p><ul><li><strong>ColBERT</strong>: El modelo inicial fue creación de <a href=\"https://x.com/lateinteraction?s=20&ref=jina-ai-gmbh.ghost.io\"><strong>Omar Khattab</strong></a><strong> y Matei Zaharia</strong>, presentando un enfoque novedoso para la recuperación de información a través del artículo \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\". Su trabajo fue publicado en SIGIR 2020.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class=\"kg-bookmark-description\">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Omar Khattab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">El artículo original de ColBERT que introduce la \"interacción tardía\".</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Basándose en el trabajo fundamental, <strong>Omar Khattab</strong> continuó su investigación, colaborando con <strong>Barlas Oguz, Matei Zaharia y Michael S. Bernstein</strong> para introducir \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\", presentado en SIGIR 2021. Esta siguiente iteración de ColBERT abordó limitaciones anteriores e introdujo mejoras clave, como la <strong>supervisión sin ruido</strong> y la <strong>compresión residual</strong>, mejorando tanto la efectividad de recuperación del modelo como su eficiencia de almacenamiento.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class=\"kg-bookmark-description\">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Keshav Santhanam</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">ColBERTv2 añade supervisión sin ruido y compresión residual para mejorar la calidad de los datos de entrenamiento y reducir la huella de espacio.</span></p></figcaption></figure><h2 id=\"understand-colberts-design\">Entender el Diseño de ColBERT</h2><p>Dado que la arquitectura de ColBERTv2 permanece muy similar a la del ColBERT original, con sus innovaciones clave girando en torno a técnicas de entrenamiento y mecanismos de compresión, primero profundizaremos en los aspectos fundamentales del ColBERT original.</p><h3 id=\"what-is-late-interaction-in-colbert\">¿Qué es la interacción tardía en ColBERT?</h3><p>\"Interacción\" se refiere al proceso de evaluar la relevancia entre una consulta y un documento comparando sus representaciones.</p><p>La \"<em>interacción tardía</em>\" es la esencia de ColBERT. El término se deriva de la arquitectura y estrategia de procesamiento del modelo, donde la interacción entre las representaciones de la consulta y el documento ocurre tarde en el proceso, después de que ambos han sido codificados independientemente. Esto contrasta con los modelos de \"<em>interacción temprana</em>\", donde los embeddings de consulta y documento interactúan en etapas anteriores, típicamente antes o durante su codificación por el modelo.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Models</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Early Interaction</td>\n<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>\n</tr>\n<tr>\n<td>Late Interaction</td>\n<td>ColBERT, ColBERTv2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>La interacción temprana puede aumentar la complejidad computacional ya que requiere considerar todos los pares posibles de consulta-documento, haciéndola menos eficiente para aplicaciones a gran escala.</p><p>Los modelos de interacción tardía como ColBERT optimizan la eficiencia y escalabilidad al permitir el precálculo de representaciones de documentos y emplear un paso de interacción más ligero al final, que se centra en las representaciones ya codificadas. Esta elección de diseño permite tiempos de recuperación más rápidos y demandas computacionales reducidas, haciéndolo más adecuado para procesar grandes colecciones de documentos.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg\" class=\"kg-image\" alt=\"Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep\" loading=\"lazy\" width=\"300\" height=\"143\"><figcaption><span style=\"white-space: pre-wrap;\">Diagramas esquemáticos que ilustran los paradigmas de interacción consulta-documento en IR neural, con la interacción tardía de ColBERT en el extremo izquierdo.</span></figcaption></figure>\n\n<h3 id=\"no-interaction-cosine-similarity-of-document-and-query-embeddings\">Sin interacción: similitud del coseno entre embeddings de documento y consulta</h3>\n\n<p>Muchas bases de datos vectoriales prácticas y soluciones de búsqueda neural dependen de la coincidencia rápida de similitud del coseno entre embeddings de documentos y consultas. Si bien es atractivo por su sencillez y eficiencia computacional, se ha encontrado que este método, a menudo denominado \"<em>sin interacción</em>\" o \"<em>no basado en interacción</em>\" tiene un rendimiento inferior en comparación con modelos que incorporan alguna forma de interacción entre consultas y documentos.</p>\n\n<p>La limitación principal del enfoque \"sin interacción\" radica en su incapacidad para capturar los matices complejos y las relaciones entre los términos de consulta y documento. La recuperación de información, en su esencia, consiste en comprender y hacer coincidir la intención detrás de una consulta con el contenido dentro de un documento. Este proceso a menudo requiere una comprensión profunda y contextual de los términos involucrados, algo que los embeddings únicos y agregados para documentos y consultas tienen dificultades para proporcionar.</p>\n\n<h2 id=\"query-and-document-encoders-in-colbert\">Codificadores de consulta y documento en ColBERT</h2>\n\n<p>La estrategia de codificación de ColBERT se basa en el modelo BERT, conocido por su profunda comprensión contextual del lenguaje. El modelo genera representaciones vectoriales densas para cada token en una consulta o documento, <strong>creando un conjunto de embeddings contextualizados para una consulta y un conjunto para un documento, respectivamente.</strong> Esto facilita una comparación matizada de sus embeddings durante la fase de interacción tardía.</p>\n\n<h3 id=\"query-encoder-of-colbert\">Codificador de consultas de ColBERT</h3>\n\n<p>Para una consulta $Q$ con tokens ${q_1, q_2, ..., q_l}$, el proceso comienza tokenizando $Q$ en tokens WordPiece basados en BERT y anteponiendo un token especial <code>[Q]</code>. Este token <code>[Q]</code>, posicionado justo después del token <code>[CLS]</code> de BERT, señala el inicio de una consulta.</p>\n\n<p>Si la consulta es más corta que un número predefinido de tokens $N_q$, se rellena con tokens <code>[mask]</code> hasta $N_q$; de lo contrario, se trunca a los primeros $N_q$ tokens. La secuencia rellenada luego pasa por BERT, seguida de una CNN (Red Neuronal Convolucional) y normalización. La salida es un conjunto de vectores de embedding denominados $\\mathbf{E}_q$ a continuación:<br>$$\\mathbf{E}_q := \\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)$$</p>\n\n<h3 id=\"document-encoder-of-colbert\">Codificador de documentos de ColBERT</h3>\n\n<p>De manera similar, para un documento $D$ con tokens ${d_1, d_2, ..., d_n}$, se antepone un token <code>[D]</code> para indicar el inicio del documento. Esta secuencia, sin necesidad de relleno, pasa por el mismo proceso, resultando en un conjunto de vectores de embedding denominados $\\mathbf{E}_d$ a continuación:<br>$$\\mathbf{E}_d := \\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)$$</p>\n\n<p>El uso de tokens <code>[mask]</code> para rellenar consultas (acuñado como <strong>aumento de consulta</strong> en el documento) asegura una longitud uniforme en todas las consultas, facilitando el procesamiento por lotes. Los tokens <code>[Q]</code> y <code>[D]</code> marcan explícitamente el inicio de consultas y documentos, respectivamente, ayudando al modelo a distinguir entre los dos tipos de entradas.</p>\n\n<h3 id=\"comparing-colbert-to-cross-encoders\">Comparación de ColBERT con codificadores cruzados</h3>\n\n<p>Los codificadores cruzados procesan pares de consultas y documentos juntos, haciéndolos altamente precisos pero menos eficientes para tareas a gran escala debido al costo computacional de evaluar cada posible par. Sobresalen en escenarios específicos donde es necesaria la puntuación precisa de pares de oraciones, como en tareas de similitud semántica o comparación detallada de contenido. Sin embargo, este diseño limita su aplicabilidad en situaciones que requieren recuperación rápida de grandes conjuntos de datos, donde los embeddings precalculados y los cálculos eficientes de similitud son primordiales.</p>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg\" class=\"kg-image\" alt=\"Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum\" loading=\"lazy\" width=\"210\" height=\"150\"></figure>\n\n<p>En contraste, el modelo de interacción tardía de ColBERT permite el precálculo de embeddings de documentos, acelerando significativamente el proceso de recuperación sin comprometer la profundidad del análisis semántico. Este método, aunque aparentemente contraintuitivo en comparación con el enfoque directo de los codificadores cruzados, ofrece una solución escalable para tareas de recuperación de información en tiempo real y a gran escala. Representa un compromiso estratégico entre la eficiencia computacional y la calidad del modelado de interacción.</p>\n\n<h2 id=\"finding-the-top-k-documents-using-colbert\">Encontrando los top-K documentos usando ColBERT</h2>\n\n<p>Una vez que tenemos embeddings para la consulta y los documentos, encontrar los K documentos más relevantes se vuelve directo (pero no tan directo como calcular el coseno de dos vectores).</p>\n\n<p>Las operaciones clave incluyen un producto punto por lotes para calcular similitudes término a término, max-pooling a través de términos de documento para encontrar la similitud más alta por término de consulta, y suma a través de términos de consulta para derivar la puntuación total del documento, seguido de ordenar los documentos basados en estas puntuaciones. El pseudo código en PyTorch se describe a continuación:</p>\n\n<pre><code class=\"language-python\">import torch\n\ndef compute_relevance_scores(query_embeddings, document_embeddings, k):\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n    \n    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]\n    :param k: Number of top documents to re-rank\n    :return: Sorted document indices based on their relevance scores\n    \"\"\"\n    \n    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]\n    # Pad the k documents to their maximum length for batch operations\n    # Note: Assuming document_embeddings is already padded and moved to GPU\n    \n    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)\n    # Resulting shape: [k, num_query_terms, max_doc_length]\n    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n    \n    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term\n    # Shape after max-pool: [k, num_query_terms]\n    max_scores_per_query_term = scores.max(dim=2).values\n    \n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [k]\n    total_scores = max_scores_per_query_term.sum(dim=1)\n    \n    # Sort the documents based on their total scores\n    sorted_indices = total_scores.argsort(descending=True)\n    \n    return sorted_indices\n</code></pre>\n\n<p>Tenga en cuenta que este procedimiento se utiliza tanto en el entrenamiento como en la reclasificación durante la inferencia. El modelo ColBERT se entrena utilizando una pérdida de clasificación por pares, donde los datos de entrenamiento consisten en triples $(q, d^+, d^-)$, donde $q$ representa una consulta, $d^+$ es un documento relevante (positivo) para la consulta, y $d^-$ es un documento no relevante (negativo). El modelo busca aprender representaciones tales que la puntuación de similitud entre $q$ y $d^+$ sea mayor que la puntuación entre q y $d^-$.</p>\n\n<p>El objetivo de entrenamiento puede representarse matemáticamente como minimizar la siguiente función de pérdida: $$\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))$$</p>\n\n<p>, donde $S(q, d)$ denota la puntuación de similitud calculada por ColBERT entre una consulta $q$ y un documento $d$. Esta puntuación se obtiene agregando las puntuaciones de máxima similitud de los embeddings mejor emparejados entre la consulta y el documento, siguiendo el patrón de interacción tardía descrito en la arquitectura del modelo. Este enfoque asegura que el modelo esté entrenado para distinguir entre documentos relevantes e irrelevantes para una consulta dada, fomentando un margen mayor en las puntuaciones de similitud para pares de documentos positivos y negativos.</p>\n\n<h3 id=\"denoised-supervision-in-colbertv2\">Supervisión sin ruido en ColBERTv2</h3>\n\n<p>La supervisión sin ruido en ColBERTv2 refina el proceso de entrenamiento original seleccionando negativos desafiantes y aprovechando un codificador cruzado para la destilación. Este método sofisticado de aumentar la calidad de los datos de entrenamiento involucra varios pasos:</p>\n\n<ol>\n<li><strong>Entrenamiento Inicial</strong>: Utilización de los triples oficiales del conjunto de datos MS MARCO, que comprende una consulta, un documento relevante y un documento no relevante.</li>\n<li><strong>Indexación y Recuperación</strong>: Empleo de la compresión de ColBERTv2 para indexar pasajes de entrenamiento, seguido de la recuperación de los top-k pasajes para cada consulta.</li>\n<li><strong>Reclasificación con Codificador Cruzado</strong>: Mejora de la selección de pasajes mediante reclasificación por un codificador cruzado MiniLM, destilando sus puntuaciones en ColBERTv2.</li>\n<li><strong>Formación de Tuplas de Entrenamiento</strong>: Generación de tuplas de w-vías para entrenamiento, incorporando pasajes de alta y baja clasificación para crear ejemplos desafiantes.</li>\n<li><strong>Refinamiento Iterativo</strong>: Repetición del proceso para mejorar continuamente la selección de negativos difíciles, mejorando así el rendimiento del modelo.</li>\n</ol>\n\n<p>Nótese que este proceso representa una mejora sofisticada del régimen de entrenamiento de ColBERT en lugar de un cambio fundamental en su arquitectura.</p>\n\n<h3 id=\"hyperparameters-of-colbert\">Hiperparámetros de ColBERT</h3><p>Los hiperparámetros de ColBERT se resumen a continuación:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Hiperparámetro</th>\n<th>Mejor Elección</th>\n<th>Razón</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning Rate</td>\n<td>3 x 10^{-6}</td>\n<td>Seleccionado para el fine-tuning para asegurar actualizaciones estables y efectivas del modelo.</td>\n</tr>\n<tr>\n<td>Batch Size</td>\n<td>32</td>\n<td>Equilibra la eficiencia computacional y la capacidad de capturar información suficiente por actualización.</td>\n</tr>\n<tr>\n<td>Number of Embeddings per Query (Nq)</td>\n<td>32</td>\n<td>Fijado para asegurar un tamaño de representación consistente entre consultas, ayudando al procesamiento eficiente.</td>\n</tr>\n<tr>\n<td>Embedding Dimension (m)</td>\n<td>128</td>\n<td>Demostró proporcionar un buen equilibrio entre poder de representación y eficiencia computacional.</td>\n</tr>\n<tr>\n<td>Training Iterations</td>\n<td>200k (MS MARCO), 125k (TREC CAR)</td>\n<td>Elegido para asegurar un aprendizaje completo evitando el sobreajuste, con ajustes basados en las características del dataset.</td>\n</tr>\n<tr>\n<td>Bytes per Dimension in Embeddings</td>\n<td>4 (re-ranking), 2 (ranking end-to-end)</td>\n<td>Equilibrio entre precisión y eficiencia espacial, considerando el contexto de aplicación (re-ranking vs. end-to-end).</td>\n</tr>\n<tr>\n<td>Vector-Similarity Function</td>\n<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>\n<td>Seleccionado según el rendimiento y la eficiencia en los respectivos contextos de recuperación.</td>\n</tr>\n<tr>\n<td>FAISS Index Partitions (P)</td>\n<td>2000</td>\n<td>Determina la granularidad de la partición del espacio de búsqueda, impactando la eficiencia de búsqueda.</td>\n</tr>\n<tr>\n<td>Nearest Partitions Searched (p)</td>\n<td>10</td>\n<td>Equilibra la amplitud de la búsqueda contra la eficiencia computacional.</td>\n</tr>\n<tr>\n<td>Sub-vectors per Embedding (s)</td>\n<td>16</td>\n<td>Afecta la granularidad de la cuantización, influenciando tanto la velocidad de búsqueda como el uso de memoria.</td>\n</tr>\n<tr>\n<td>Index Representation per Dimension</td>\n<td>16-bit values</td>\n<td>Elegido para la segunda etapa de recuperación end-to-end para manejar el equilibrio entre precisión y espacio.</td>\n</tr>\n<tr>\n<td>Number of Layers in Encoders</td>\n<td>12-layer BERT</td>\n<td>Balance óptimo entre profundidad de comprensión contextual y eficiencia computacional.</td>\n</tr>\n<tr>\n<td>Max Query Length</td>\n<td>128</td>\n<td>El número máximo de tokens procesados por el codificador de consultas. <b>Esto se extiende en el modelo Jina-ColBERT.</b></td>\n</tr>\n<tr>\n<td>Max Document Length</td>\n<td>512</td>\n<td>El número máximo de tokens procesados por el codificador de documentos. <b>Esto se extiende a 8192 en el modelo Jina-ColBERT.</b></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"the-indexing-strategy-of-colbert\">La estrategia de indexación de ColBERT</h2>\n<p>A diferencia de los enfoques basados en representación que codifican cada documento en un vector de embedding, <strong>ColBERT codifica documentos (y consultas) en conjuntos de embeddings, donde cada token en un documento tiene su propio embedding.</strong> Este enfoque inherentemente significa que para documentos más largos, se almacenarán más embeddings, <strong>lo cual es un punto débil del ColBERT original, y que posteriormente fue abordado por ColBERTv2.</strong></p>\n<p>La clave para gestionar esto eficientemente radica en el uso que hace ColBERT de bases de datos vectoriales (por ejemplo, <a href=\"https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io\">FAISS</a>) para indexación y recuperación, y su detallado proceso de indexación que está diseñado para manejar grandes volúmenes de datos eficientemente. El paper original de ColBERT menciona varias estrategias para mejorar la eficiencia de la indexación y recuperación, incluyendo:</p>\n<ul>\n<li><strong>Indexación Offline</strong>: Las representaciones de documentos se calculan offline, permitiendo el pre-cálculo y almacenamiento de embeddings de documentos. Este proceso aprovecha el procesamiento por lotes y la aceleración GPU para manejar grandes colecciones de documentos eficientemente.</li>\n<li><strong>Almacenamiento de Embeddings</strong>: Los embeddings de documentos pueden almacenarse usando valores de 32 bits o 16 bits para cada dimensión, ofreciendo un equilibrio entre precisión y requerimientos de almacenamiento. Esta flexibilidad permite a ColBERT mantener un balance entre efectividad (en términos de rendimiento de recuperación) y eficiencia (en términos de costos de almacenamiento y computación).</li>\n</ul>\n<p>La introducción de la <strong>compresión residual</strong> en ColBERTv2, que es un enfoque novedoso no presente en el ColBERT original, juega un papel clave en reducir la huella espacial del modelo en 6-10× mientras preserva la calidad. Esta técnica comprime aún más los embeddings capturando y almacenando efectivamente solo las diferencias desde un conjunto de centroides de referencia fijos.</p>\n<h2 id=\"effectiveness-and-efficiency-of-colbert\">Efectividad y Eficiencia de ColBERT</h2>\n<p>Uno podría inicialmente asumir que incorporar la comprensión contextual profunda de BERT en la búsqueda inherentemente requeriría recursos computacionales significativos, haciendo tal enfoque menos factible para aplicaciones en tiempo real debido a la alta latencia y costos computacionales. Sin embargo, ColBERT desafía y revierte esta suposición a través de su uso innovador del mecanismo de interacción tardía. Aquí hay algunos puntos destacables:</p>\n<ol>\n<li><strong>Ganancias Significativas en Eficiencia</strong>: ColBERT logra una reducción de órdenes de magnitud en costos computacionales (FLOPs) y latencia comparado con modelos de ranking basados en BERT tradicionales. Específicamente, para un tamaño de modelo dado (por ejemplo, codificador transformer \"base\" de 12 capas), ColBERT no solo iguala sino que en algunos casos supera la efectividad de los modelos basados en BERT con demandas computacionales dramáticamente menores. Por ejemplo, a una profundidad de re-ranking de <em>k</em>=10, BERT requiere casi 180× más FLOPs que ColBERT; esta brecha se amplía a medida que <em>k</em> aumenta, alcanzando 13900× en <em>k</em>=1000 y hasta 23000× en <em>k</em>=2000.</li>\n<li><strong>Mejora en Recall y MRR@10 en Recuperación End-to-End</strong>: Contrario a la intuición inicial de que sería necesaria una interacción más profunda entre las representaciones de consulta y documento (como se ve en los modelos de interacción temprana) para un alto rendimiento de recuperación, la configuración de recuperación end-to-end de ColBERT demuestra una efectividad superior. Por ejemplo, su Recall@50 supera el Recall@1000 del BM25 oficial y casi todos los Recall@200 de otros modelos, subrayando la notable capacidad del modelo para recuperar documentos relevantes de una vasta colección sin comparación directa de cada par consulta-documento.</li>\n<li><strong>Practicidad para Aplicaciones del Mundo Real</strong>: Los resultados experimentales subrayan la aplicabilidad práctica de ColBERT para escenarios del mundo real. Su rendimiento de indexación y eficiencia de memoria lo hacen adecuado para indexar grandes colecciones de documentos como MS MARCO en pocas horas, manteniendo alta efectividad con una huella espacial manejable. Estas cualidades resaltan la idoneidad de ColBERT para su despliegue en entornos de producción donde tanto el rendimiento como la eficiencia computacional son primordiales.</li>\n<li><strong>Escalabilidad con el Tamaño de la Colección de Documentos</strong>: Quizás la conclusión más sorprendente es la escalabilidad y eficiencia de ColBERT en el manejo de colecciones de documentos a gran escala. La arquitectura permite el pre-cálculo de embeddings de documentos y aprovecha el procesamiento eficiente por lotes para la interacción consulta-documento, permitiendo que el sistema escale efectivamente con el tamaño de la colección de documentos. Esta escalabilidad es contraintuitiva cuando se considera la complejidad y profundidad de comprensión requerida para una recuperación efectiva de documentos, mostrando el enfoque innovador de ColBERT para equilibrar la eficiencia computacional con la efectividad de recuperación.</li>\n</ol>\n<h2 id=\"using-jina-colbert-v1-en-a-8192-length-colbertv2-model\">Usando <code>jina-colbert-v1-en</code>: un modelo ColBERTv2 de longitud 8192</h2>\n<p>Jina-ColBERT está diseñado tanto para recuperación rápida como precisa, soportando <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">longitudes de contexto más largas de hasta 8192, aprovechando los avances de JinaBERT</a>, que permite el procesamiento de secuencias más largas debido a sus mejoras en la arquitectura.</p>\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\">\n<div class=\"kg-callout-emoji\">💡</div>\n<div class=\"kg-callout-text\">Estrictamente hablando, Jina-ColBERT soporta una longitud de 8190 tokens. Recordemos que en el codificador de documentos de ColBERT, cada documento se rellena con <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">[D],[CLS]</code> al principio.</div>\n</div>\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-colbert-v1-en · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png\" alt=\"\"></div></a></figure>\n<h3 id=\"jinas-improvement-over-original-colbert\">Mejoras de Jina sobre el ColBERT original</h3>\n<p>El principal avance de Jina-ColBERT es su columna vertebral, <code>jina-bert-v2-base-en</code>, que permite procesar contextos significativamente más largos (hasta 8192 tokens) comparado con el ColBERT original que usa <code>bert-base-uncased</code>. Esta capacidad es crucial para manejar documentos con contenido extenso, proporcionando resultados de búsqueda más detallados y contextuales.</p>\n<h3 id=\"jina-colbert-v1-en-performance-comparison-vs-colbertv2\">Comparación de rendimiento de <code>jina-colbert-v1-en</code> vs. ColBERTv2</h3>\n<p>Evaluamos <code>jina-colbert-v1-en</code> en datasets BEIR y el nuevo benchmark LoCo que favorece el contexto largo, probándolo contra la implementación original de ColBERTv2 y basada en no-interacciónmodelo <code>jina-embeddings-v2-base-en</code>.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>ColBERTv2</th>\n<th>jina-colbert-v1-en</th>\n<th>jina-embeddings-v2-base-en</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Arguana</td>\n<td>46.5</td>\n<td><strong>49.4</strong></td>\n<td>44.0</td>\n</tr>\n<tr>\n<td>Climate-Fever</td>\n<td>18.1</td>\n<td>19.6</td>\n<td><strong>23.5</strong></td>\n</tr>\n<tr>\n<td>DBPedia</td>\n<td><strong>45.2</strong></td>\n<td>41.3</td>\n<td>35.1</td>\n</tr>\n<tr>\n<td>FEVER</td>\n<td>78.8</td>\n<td><strong>79.5</strong></td>\n<td>72.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td>35.4</td>\n<td>36.8</td>\n<td><strong>41.6</strong></td>\n</tr>\n<tr>\n<td>HotpotQA</td>\n<td><strong>67.5</strong></td>\n<td>65.9</td>\n<td>61.4</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>33.7</td>\n<td><strong>33.8</strong></td>\n<td>32.5</td>\n</tr>\n<tr>\n<td>NQ</td>\n<td>56.1</td>\n<td>54.9</td>\n<td><strong>60.4</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>85.5</td>\n<td>82.3</td>\n<td><strong>88.2</strong></td>\n</tr>\n<tr>\n<td>SCIDOCS</td>\n<td>15.4</td>\n<td>16.9</td>\n<td><strong>19.9</strong></td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td>68.9</td>\n<td><strong>70.1</strong></td>\n<td>66.7</td>\n</tr>\n<tr>\n<td>TREC-COVID</td>\n<td>72.6</td>\n<td><strong>75.0</strong></td>\n<td>65.9</td>\n</tr>\n<tr>\n<td>Webis-touch2020</td>\n<td>26.0</td>\n<td><strong>27.0</strong></td>\n<td>26.2</td>\n</tr>\n<tr>\n<td>LoCo</td>\n<td>74.3</td>\n<td>83.7</td>\n<td><strong>85.4</strong></td>\n</tr>\n<tr>\n<td>Promedio</td>\n<td>51.7</td>\n<td><strong>52.6</strong></td>\n<td>51.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Esta tabla demuestra el rendimiento superior de <code>jina-colbert-v1-en</code>, especialmente en escenarios que requieren longitudes de contexto más largas frente al ColBERTv2 original. Ten en cuenta que <code>jina-embeddings-v2-base-en</code> <a href=\"https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io\">utiliza más datos de entrenamiento</a>, mientras que <code>jina-colbert-v1-en</code> solo utiliza MSMARCO, lo que puede justificar el buen rendimiento de <code>jina-embeddings-v2-base-en</code> en algunas tareas.</p><h3 id=\"example-usage-of-jina-colbert-v1-en\">Ejemplo de uso de <code>jina-colbert-v1-en</code></h3><p>Este fragmento describe el proceso de indexación con Jina-ColBERT, mostrando su soporte para documentos largos.</p><pre><code class=\"language-python\">from colbert import Indexer\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\n\nn_gpu: int = 1  # Set your number of available GPUs\nexperiment: str = \"\"  # Name of the folder where the logs and created indices will be stored\nindex_name: str = \"\"  # The name of your index, i.e. the name of your vector database\n\nif __name__ == \"__main__\":\n    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):\n        config = ColBERTConfig(\n          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents\n        )\n        indexer = Indexer(\n          checkpoint=\"jinaai/jina-colbert-v1-en\",\n          config=config,\n        )\n        documents = [\n          \"ColBERT is an efficient and effective passage retrieval model.\",\n          \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.\",\n          \"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.\",\n          \"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.\",\n          \"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.\",\n          \"Jina is an easier way to build neural search systems.\",\n          \"You can use Jina-ColBERT to build neural search systems with ease.\",\n          # Add more documents here to ensure the clustering work correctly\n        ]\n        indexer.index(name=index_name, collection=documents)\n</code></pre><h3 id=\"use-jina-colbert-v1-en-in-ragatouille\">Uso de <code>jina-colbert-v1-en</code> en RAGatouille</h3><p>RAGatouille es una nueva biblioteca de Python que facilita el uso de métodos avanzados de recuperación dentro de pipelines RAG. Está diseñada para ser modular y de fácil integración, permitiendo a los usuarios aprovechar la investigación de vanguardia sin problemas. El objetivo principal de RAGatouille es simplificar la aplicación de modelos complejos como ColBERT en pipelines RAG, haciendo accesible para los desarrolladores utilizar estos métodos sin necesitar experiencia profunda en la investigación subyacente. Gracias a <a href=\"https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io\">Benjamin Clavié</a>, ahora puedes usar <code>jina-colbert-v1-en</code> fácilmente:</p><pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\n# Get your model &amp; collection of big documents ready\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v1-en\")\nmy_documents = [\n    \"very long document1\",\n    \"very long document2\",\n    # ... more documents\n]\n\n# And create an index with them at full length!\nRAG.index(collection=my_documents,\n          index_name=\"the_biggest_index\",\n          max_document_length=8190,)\n\n# or encode them in-memory with no truncation, up to your model's max length\nRAG.encode(my_documents)\n</code></pre><p>Para obtener información más detallada y explorar más a fondo Jina-ColBERT, puedes visitar la <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">página de Hugging Face</a>.</p><h2 id=\"conclusion\">Conclusión</h2><p>ColBERT representa un avance significativo en el campo de la recuperación de información. Al permitir longitudes de contexto más largas con Jina-ColBERT y mantener la compatibilidad con el enfoque de interacción tardía de ColBERT, ofrece una poderosa alternativa para los desarrolladores que buscan implementar funcionalidades de búsqueda de última generación.</p><p>Junto con la biblioteca RAGatouille, que simplifica la integración de modelos complejos de recuperación en pipelines RAG, los desarrolladores pueden ahora aprovechar el poder de la recuperación avanzada con facilidad, optimizando sus flujos de trabajo y mejorando sus aplicaciones. La sinergia entre Jina-ColBERT y RAGatouille ilustra un notable avance en hacer que los modelos avanzados de búsqueda con IA sean accesibles y eficientes para uso práctico.</p>",
  "comment_id": "65d3a2134a32310001f5b71b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-02-19T19:46:43.000+01:00",
  "updated_at": "2024-08-30T23:11:22.000+02:00",
  "published_at": "2024-02-20T02:19:04.000+01:00",
  "custom_excerpt": "Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
  "excerpt": "El ColBERT de Jina AI en Hugging Face ha causado revuelo en Twitter, aportando una nueva perspectiva a la búsqueda con su capacidad de 8192 tokens. Este artículo desglosa los matices de ColBERT y ColBERTv2, mostrando sus diseños innovadores y por qué su característica de interacción tardía es revolucionaria para la búsqueda.",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon theater or concert hall marquee letters lit up at night with city lights and faint \"Adobe Sto\" visible.",
  "feature_image_caption": null
}
{
  "slug": "late-chunking-in-long-context-embedding-models",
  "id": "66c72e30da9a33000146d836",
  "uuid": "9eda87e2-a799-4360-bac9-6a1cd0193349",
  "title": "Fragmentaci√≥n tard√≠a en modelos de incrustaci√≥n con contexto largo",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Late Chunking ahora est√° disponible en la API <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>. <b><strong style=\"white-space: pre-wrap;\">Orden de lectura recomendado: parte I, </strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">parte II</strong></b></a><b><strong style=\"white-space: pre-wrap;\">, </strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">paper de investigaci√≥n</strong></b></a><b><strong style=\"white-space: pre-wrap;\">.</strong></b></div></div><p></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What Late Chunking Really Is &amp; What It's Not: Part II</div><div class=\"kg-bookmark-description\">Part 2 of our exploration of Late Chunking, a deep dive into why it is the best method for chunk embeddings and improving search/RAG performance.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-4.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">¬°Nuevo! Parte II: an√°lisis profundo de las se√±ales de l√≠mites y conceptos err√≥neos.</span></p></figcaption></figure><p>Hace aproximadamente un a√±o, en octubre de 2023, lanzamos <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">el primer modelo de embeddings de c√≥digo abierto del mundo con una longitud de contexto de 8K</a>, <code>jina-embeddings-v2-base-en</code>. Desde entonces, ha habido bastante debate sobre la utilidad del contexto largo en los modelos de embeddings. Para muchas aplicaciones, codificar un documento de miles de palabras en una √∫nica representaci√≥n de embedding no es ideal. Muchos casos de uso requieren recuperar porciones m√°s peque√±as del texto, y los sistemas de recuperaci√≥n basados en vectores densos a menudo funcionan mejor con segmentos de texto m√°s peque√±os, ya que es menos probable que la sem√°ntica est√© \"sobre-comprimida\" en los vectores de embedding.</p><p>La Generaci√≥n Aumentada por Recuperaci√≥n (RAG) es una de las aplicaciones m√°s conocidas que requiere dividir documentos en fragmentos de texto m√°s peque√±os (digamos dentro de 512 tokens). Estos fragmentos generalmente se almacenan en una base de datos vectorial, con representaciones vectoriales generadas por un modelo de embedding de texto. Durante la ejecuci√≥n, el mismo modelo de embedding codifica una consulta en una representaci√≥n vectorial, que luego se utiliza para identificar fragmentos de texto relevantes almacenados. Estos fragmentos se pasan posteriormente a un modelo de lenguaje grande (LLM), que sintetiza una respuesta a la consulta basada en los textos recuperados.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--1-.svg\" class=\"kg-image\" alt=\"Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec\" loading=\"lazy\" width=\"1458\" height=\"307\"><figcaption><span style=\"white-space: pre-wrap;\">Un pipeline t√≠pico de RAG de fragmentaci√≥n-embedding-recuperaci√≥n-generaci√≥n.</span></figcaption></figure><p>En resumen, embeber fragmentos m√°s peque√±os parece ser m√°s preferible, en parte debido a los tama√±os de entrada limitados de los LLM posteriores, pero tambi√©n porque existe <strong>una preocupaci√≥n de que la informaci√≥n contextual importante en un contexto largo pueda diluirse cuando se comprime en un solo vector.</strong></p><p>Pero si la industria solo necesita modelos de embedding con una longitud de contexto de 512, <em>¬øcu√°l es el punto de entrenar modelos con una longitud de contexto de 8192?</em></p><p>En este art√≠culo, revisamos esta importante, aunque inc√≥moda, pregunta explorando las limitaciones del pipeline ingenuo de fragmentaci√≥n-embedding en RAG. Introducimos un nuevo enfoque llamado <strong>\"Late Chunking\"</strong>, que aprovecha la rica informaci√≥n contextual proporcionada por los modelos de embedding de longitud 8192 para embeber fragmentos de manera m√°s efectiva.</p><h2 id=\"the-lost-context-problem\">El Problema del Contexto Perdido</h2><p>El simple pipeline RAG de fragmentaci√≥n-embedding-recuperaci√≥n-generaci√≥n no est√° exento de desaf√≠os. Espec√≠ficamente, <strong>este proceso puede destruir dependencias contextuales de largo alcance.</strong> En otras palabras, cuando la informaci√≥n relevante est√° dispersa en m√∫ltiples fragmentos, sacar segmentos de texto de contexto puede hacerlos inefectivos, haciendo que este enfoque sea particularmente problem√°tico.</p><p>En la imagen de abajo, un art√≠culo de Wikipedia se divide en fragmentos de oraciones. Puedes ver que frases como \"su\" y \"la ciudad\" hacen referencia a \"Berl√≠n\", que solo se menciona en la primera oraci√≥n. Esto hace m√°s dif√≠cil que el modelo de embedding vincule estas referencias con la entidad correcta, produciendo as√≠ una representaci√≥n vectorial de menor calidad.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png\" class=\"kg-image\" alt=\"Comparative panels display Berlin's Wikipedia article and its chunked text to highlight clarity and readability benefits.\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image-3.png 1774w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Esto significa que, si dividimos un art√≠culo largo en fragmentos de longitud de oraci√≥n, como en el ejemplo anterior, un sistema RAG podr√≠a tener dificultades para responder una consulta como \"¬øCu√°l es la poblaci√≥n de Berl√≠n?\" Porque el nombre de la ciudad y la poblaci√≥n nunca aparecen juntos en un solo fragmento, y sin ning√∫n contexto de documento m√°s amplio, un LLM presentado con uno de estos fragmentos no puede resolver referencias anaf√≥ricas como \"ella\" o \"la ciudad\".</p><p>Existen algunas heur√≠sticas para aliviar este problema, como el remuestreo con una ventana deslizante, el uso de m√∫ltiples longitudes de ventana de contexto y la realizaci√≥n de escaneos de documentos en m√∫ltiples pasadas. Sin embargo, como todas las heur√≠sticas, estos enfoques son de prueba y error; pueden funcionar en algunos casos, pero no hay garant√≠a te√≥rica de su efectividad.</p><h2 id=\"the-solution-late-chunking\">La Soluci√≥n: Late Chunking</h2><p>El enfoque de codificaci√≥n ingenuo (como se ve en el lado izquierdo de la imagen de abajo) implica usar oraciones, p√°rrafos o l√≠mites de longitud m√°xima para dividir el texto <em>a priori</em>. Despu√©s, un modelo de embedding se aplica repetitivamente a estos fragmentos resultantes. Para generar un solo embedding para cada fragmento, muchos modelos de embedding usan <em>mean pooling</em> en estos embeddings a nivel de token para producir un solo vector de embedding.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/Diagram--Blog-images--4-.svg\" class=\"kg-image\" alt=\"Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.\" loading=\"lazy\" width=\"1020\" height=\"865\"><figcaption><span style=\"white-space: pre-wrap;\">Una ilustraci√≥n de la estrategia de fragmentaci√≥n ingenua (izquierda) y la estrategia de late chunking (derecha).</span></figcaption></figure><p>En contraste, el enfoque \"Late Chunking\" que proponemos en este art√≠culo primero aplica la capa transformer del modelo de embedding a <em>todo el texto</em> o tanto como sea posible. Esto genera una secuencia de representaciones vectoriales para cada token que abarca informaci√≥n textual de todo el texto. Posteriormente, se aplica mean pooling a cada fragmento de esta secuencia de vectores de token, produciendo embeddings para cada fragmento que consideran el contexto completo del texto. A diferencia del enfoque de codificaci√≥n ingenuo, que genera embeddings de fragmentos independientes e id√©nticamente distribuidos (i.i.d.), <strong>el late chunking crea un conjunto de embeddings de fragmentos donde cada uno est√° \"condicionado por\" los anteriores, codificando as√≠ m√°s informaci√≥n contextual para cada fragmento.</strong></p><p>Obviamente para aplicar efectivamente el late chunking, necesitamos modelos de embedding de contexto largo como <code>jina-embeddings-v2-base-en</code>, que soportan hasta 8192 tokens‚Äîaproximadamente diez p√°ginas est√°ndar de texto. Los segmentos de texto de este tama√±o tienen muchas menos probabilidades de tener dependencias contextuales que requieran un contexto a√∫n m√°s largo para resolver.</p><p>Es importante destacar que el late chunking todav√≠a requiere se√±ales de l√≠mites, pero estas se√±ales se utilizan solo <em>despu√©s</em> de obtener los embeddings a nivel de token‚Äîde ah√≠ el t√©rmino \"late\" en su nombre.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Fragmentaci√≥n Ingenua</th>\n<th>Late Chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>La necesidad de se√±ales de l√≠mites</td>\n<td>S√≠</td>\n<td>S√≠</td>\n</tr>\n<tr>\n<td>El uso de se√±ales de l√≠mites</td>\n<td>Directamente en preprocesamiento</td>\n<td>Despu√©s de obtener los embeddings a nivel de token de la capa transformer</td>\n</tr>\n<tr>\n<td>Los embeddings de fragmentos resultantes</td>\n<td>i.i.d.</td>\n<td>Condicional</td>\n</tr>\n<tr>\n<td>Informaci√≥n contextual de fragmentos cercanos</td>\n<td>Perdida. Algunas heur√≠sticas (como muestreo con superposici√≥n) para aliviar esto</td>\n<td>Bien preservada por modelos de embedding de contexto largo</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"implementation-and-qualitative-evaluation\">Implementaci√≥n y Evaluaci√≥n Cualitativa</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ssl.gstatic.com/colaboratory-static/common/4c9d6ee1a7679cb6c4c106e58fabaf56/img/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>La implementaci√≥n del late chunking se puede encontrar en el Google Colab enlazado arriba. Aqu√≠, utilizamos nuestra reciente funci√≥n lanzada en la API del Tokenizer, que aprovecha todas las posibles se√±ales de l√≠mites para segmentar un documento largo en fragmentos significativos. M√°s discusi√≥n sobre el algoritmo detr√°s de esta funci√≥n se puede encontrar en X.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/tokenizer/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Tokenizer API</div><div class=\"kg-bookmark-description\">API gratuita para tokenizar texto y segmentar texto largo en fragmentos.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-tokenize-api.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,‚Ä¶ <a href=\"https://t.co/AtBCSrn7nI?ref=jina-ai-gmbh.ghost.io\">pic.twitter.com/AtBCSrn7nI</a></p>‚Äî Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1823756993108304135?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">14 de agosto de 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><p>Al aplicar el chunking tard√≠o al ejemplo de Wikipedia anterior, se puede ver inmediatamente una mejora en la similitud sem√°ntica. Por ejemplo, en el caso de \"la ciudad\" y \"Berl√≠n\" dentro de un art√≠culo de Wikipedia, los vectores que representan \"la ciudad\" ahora contienen informaci√≥n que la vincula con la menci√≥n previa de \"Berl√≠n\", lo que la convierte en una coincidencia mucho mejor para consultas relacionadas con ese nombre de ciudad.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Query</th>\n<th>Chunk</th>\n<th>Sim. on naive chunking</th>\n<th>Sim. on late chunking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Berlin</td>\n<td>Berlin is the capital and largest city of Germany, both by area and by population.</td>\n<td>0.849</td>\n<td>0.850</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.</td>\n<td>0.708</td>\n<td>0.825</td>\n</tr>\n<tr>\n<td>Berlin</td>\n<td>The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.</td>\n<td>0.753</td>\n<td>0.850</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Puedes observar esto en los resultados num√©ricos anteriores, que comparan el embedding del t√©rmino \"Berlin\" con varias oraciones del art√≠culo sobre Berl√≠n usando similitud del coseno. La columna \"Sim. on IID chunk embeddings\" muestra los valores de similitud entre el embedding de consulta de \"Berlin\" y los embeddings usando chunking <em>a priori</em>, mientras que \"Sim. under contextual chunk embedding\" representa los resultados con el m√©todo de chunking tard√≠o.</p><h2 id=\"quantitative-evaluation-on-beir\">Evaluaci√≥n Cuantitativa en BEIR</h2><p>Para verificar la efectividad del chunking tard√≠o m√°s all√° de un ejemplo de juguete, lo probamos usando algunos de los benchmarks de recuperaci√≥n de <a href=\"https://github.com/beir-cellar/beir?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\">BeIR</a>. Estas tareas de recuperaci√≥n consisten en un conjunto de consultas, un corpus de documentos de texto y un archivo QRels que almacena informaci√≥n sobre los IDs de documentos relevantes para cada consulta.</p><p>Para identificar los documentos relevantes para una consulta, los documentos se dividen en chunks, se codifican en un √≠ndice de embeddings y se determinan los chunks m√°s similares para cada embedding de consulta usando k-vecinos m√°s cercanos (kNN). Como cada chunk corresponde a un documento, el ranking kNN de chunks se puede convertir en un ranking kNN de documentos (manteniendo solo la primera aparici√≥n de documentos que aparecen m√∫ltiples veces en el ranking). Este ranking resultante se compara luego con el ranking proporcionado por el archivo QRels de ground-truth, y se calculan m√©tricas de recuperaci√≥n como nDCG@10. Este procedimiento se ilustra a continuaci√≥n, y el script de evaluaci√≥n se puede encontrar en este repositorio para reproducibilidad.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/late-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)</div><div class=\"kg-bookmark-description\">Code for explaining and evaluating late chunking (chunked pooling) - jina-ai/late-chunking</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/bf0bb9d5ca928dc3fe25ae621398af0fdf5e34324e37cbeee6fa4189218c9b4d/jina-ai/late-chunking\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Ejecutamos esta evaluaci√≥n en varios conjuntos de datos de BeIR, comparando el chunking ingenuo con nuestro m√©todo de chunking tard√≠o. Para obtener las se√±ales de l√≠mites, usamos una regex que divide los textos en cadenas de aproximadamente 256 tokens. Tanto la evaluaci√≥n ingenua como la tard√≠a utilizaron <a href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener\"><code>jina-embeddings-v2-small-en</code></a> como modelo de embedding; una versi√≥n m√°s peque√±a del modelo <code>v2-base-en</code> que a√∫n admite longitudes de hasta 8192 tokens. Los resultados se pueden encontrar en la tabla siguiente.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Avg. Document Length (characters)</th>\n<th>Naive Chunking (nDCG@10)</th>\n<th>Late Chunking (nDCG@10)</th>\n<th>No Chunking (nDCG@10)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SciFact</td>\n<td>1498.4</td>\n<td>64.20%</td>\n<td><strong>66.10%</strong></td>\n<td>63.89%</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td>1116.7</td>\n<td>63.36%</td>\n<td>64.70%</td>\n<td><strong>65.18%</strong></td>\n</tr>\n<tr>\n<td>FiQA2018</td>\n<td>767.2</td>\n<td>33.25%</td>\n<td><strong>33.84%</strong></td>\n<td>33.43%</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>1589.8</td>\n<td>23.46%</td>\n<td>29.98%</td>\n<td><strong>30.40%</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>62.2</td>\n<td>87.19%</td>\n<td>87.19%</td>\n<td>87.19%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>En todos los casos, el chunking tard√≠o mejor√≥ las puntuaciones en comparaci√≥n con el enfoque ingenuo. En algunos casos, tambi√©n super√≥ la codificaci√≥n del documento completo en un solo embedding, mientras que en otros conjuntos de datos, no realizar chunking en absoluto produjo los mejores resultados (Por supuesto, no hacer chunking solo tiene sentido si no hay necesidad de clasificar chunks, lo cual es raro en la pr√°ctica). Si graficamos la brecha de rendimiento entre el enfoque ingenuo y el chunking tard√≠o contra la longitud del documento, se hace evidente que la longitud promedio de los documentos se correlaciona con mayores mejoras en las puntuaciones nDCG a trav√©s del chunking tard√≠o. En otras palabras, <strong>cuanto m√°s largo es el documento, m√°s efectiva se vuelve la estrategia de chunking tard√≠o.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/chart--22-.svg\" class=\"kg-image\" alt=\"Gr√°fico de l√≠neas que muestra la disminuci√≥n en la mejora relativa con el aumento de la longitud del documento, de 0 a 1500 caracteres.\" loading=\"lazy\" width=\"582\" height=\"337\"><figcaption><span style=\"white-space: pre-wrap;\">La mejora del chunking tard√≠o sobre el chunking ingenuo est√° correlacionada con la longitud promedio del documento.</span></figcaption></figure><h2 id=\"conclusion\">Conclusi√≥n</h2><p>En este art√≠culo, presentamos un enfoque simple llamado \"chunking tard√≠o\" para embeber chunks cortos aprovechando el poder de los modelos de embedding de contexto largo. Demostramos c√≥mo el embedding de chunks i.i.d. tradicional falla en preservar la informaci√≥n contextual, llevando a una recuperaci√≥n sub√≥ptima; y c√≥mo el chunking tard√≠o ofrece una soluci√≥n simple pero altamente efectiva para mantener y condicionar la informaci√≥n contextual dentro de cada chunk. La efectividad del chunking tard√≠o se vuelve cada vez m√°s significativa en documentos m√°s largos‚Äîuna capacidad posible <em>solo</em> gracias a modelos avanzados de embedding de contexto largo como <code>jina-embeddings-v2-base-en</code>. Esperamos que este trabajo no solo valide la importancia de los modelos de embedding de contexto largo sino que tambi√©n inspire m√°s investigaci√≥n sobre este tema.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Qu√© es Realmente el Chunking Tard√≠o y Qu√© No Lo Es: Parte II</div><div class=\"kg-bookmark-description\">Parte 2 de nuestra exploraci√≥n del Chunking Tard√≠o, una inmersi√≥n profunda en por qu√© es el mejor m√©todo para embeddings de chunks y mejorar el rendimiento de b√∫squeda/RAG.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-5.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/lc2-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Contin√∫a leyendo la parte II: inmersi√≥n profunda en las se√±ales de l√≠mites y conceptos err√≥neos.</span></p></figcaption></figure>",
  "comment_id": "66c72e30da9a33000146d836",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-08-22T14:25:20.000+02:00",
  "updated_at": "2024-10-06T16:29:02.000+02:00",
  "published_at": "2024-08-22T17:06:17.000+02:00",
  "custom_excerpt": "Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael G√ºnther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    },
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "636409b554b68a003dfbdef8",
    "name": "Michael G√ºnther",
    "slug": "michael",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
    "cover_image": null,
    "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
    "website": "https://github.com/guenthermi",
    "location": "Berlin",
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/late-chunking-in-long-context-embedding-models/",
  "excerpt": "La divisi√≥n de documentos largos manteniendo la informaci√≥n contextual es un desaf√≠o. Presentamos el \"Late Chunking\" que aprovecha los modelos de embedding de contexto largo para generar embeddings de fragmentos contextuales para mejores aplicaciones de recuperaci√≥n.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating the 'Late Chunking' and 'Long Document Model' processes in machine learning on a black background.",
  "feature_image_caption": null
}
{
  "slug": "jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image",
  "id": "665f1ccd4b4b4c0001ba1c98",
  "uuid": "53cc48a8-bcbf-42a1-adae-4d15126d7ad6",
  "title": "Jina CLIP v1：真正用于文本和图像的多模态 Embeddings 模型",
  "html": "<p>Jina CLIP v1（<code>jina-clip-v1</code>）是一个新的多模态嵌入模型，它扩展了 OpenAI 的<a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">原始 CLIP 模型</a>的功能。通过这个新模型，用户可以使用一个嵌入模型，在纯文本和文本-图像跨模态检索中都能获得最先进的性能。与 OpenAI CLIP 相比，Jina AI 在纯文本检索方面提升了 165%，在图像到图像检索方面提升了 12%，在文本到图像和图像到文本任务中保持相同或略有提升的性能。这种增强的性能使得 Jina CLIP v1 在处理多模态输入时成为不可或缺的工具。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v1</code> 在<a href=\"#compare_table\" rel=\"noreferrer\">所有检索类别</a>中都优于 OpenAI CLIP。</div></div><p>在本文中，我们将首先讨论原始 CLIP 模型的缺点，以及我们如何使用独特的协同训练方法来解决这些问题。然后，我们将在各种检索基准测试上展示我们模型的有效性。最后，我们将提供详细说明，指导用户如何通过我们的 Embeddings API 和 Hugging Face 开始使用 Jina CLIP v1。</p><h2 id=\"the-clip-architecture-for-multimodal-ai\">多模态 AI 的 CLIP 架构</h2><p>2021 年 1 月，OpenAI 发布了 <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">CLIP</a>（对比语言-图像预训练）模型。CLIP 具有简单而巧妙的架构：它将两个嵌入模型——一个用于文本，一个用于图像——组合成一个具有单一输出嵌入空间的模型。它的文本和图像嵌入可以直接相互比较，使得文本嵌入和图像嵌入之间的距离与该文本对图像的描述程度成正比，反之亦然。</p><p>这在多模态信息检索和零样本图像分类中被证明非常有用。无需进一步的特殊训练，CLIP 就能很好地将图像分类到自然语言标签中。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg\" class=\"kg-image\" alt=\"Diagram illustrating image to text translation using an astronaut on Mars with a red moon as an example.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/180-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/180-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/180-1.jpg 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>原始 CLIP 中的文本嵌入模型是一个只有 6300 万参数的定制神经网络。在图像方面，OpenAI 发布的 CLIP 包含了一系列 <a href=\"https://huggingface.co/docs/transformers/model_doc/resnet?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ResNet</a> 和 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/vit?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">ViT 模型</a>。每个模型都针对其各自的模态进行预训练，然后通过带标注的图像训练，为准备好的图像-文本对生成相似的嵌入。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png\" class=\"kg-image\" alt=\"Flowchart with text &quot;Embedding Space&quot;, linked to &quot;Image Encoder&quot; and &quot;Text Encoder&quot;, with a &quot;Distracted boyfriend&quot; label.\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/Blog-images--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/Blog-images--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/Blog-images--1-.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>这种方法取得了令人印象深刻的结果。尤其值得注意的是它的零样本分类性能。例如，即使训练数据中没有包含<a href=\"https://docs.vultr.com/zero-shot-image-classification-using-openai-clip?ref=jina-ai-gmbh.ghost.io\">宇航员</a>的标记图像，CLIP 也能根据其对文本和图像中相关概念的理解，正确识别宇航员的图片。</p><p>然而，OpenAI 的 CLIP 有两个重要的缺点：</p><ul><li>首先是文本输入容量非常有限。它最多可以接受 77 个标记的输入，但<a href=\"https://arxiv.org/abs/2403.15378?ref=jina-ai-gmbh.ghost.io\">实验分析表明</a>，在实践中它实际使用不超过 20 个标记来生成嵌入。这是因为 CLIP 是从带有标题的图像中训练的，而标题往往很短。这与当前支持数千个标记的文本嵌入模型形成对比。</li><li>其次，它的文本嵌入在纯文本检索场景中的性能非常差。图像标题是一种非常有限的文本类型，不能反映文本嵌入模型预期支持的广泛用例。</li></ul><p>在大多数实际用例中，纯文本和图像-文本检索是结合使用的，或者至少两者都可用于任务。为纯文本任务维护第二个嵌入模型实际上使 AI 框架的规模和复杂性增加了一倍。</p><p>Jina AI 的新模型直接解决了这些问题，<code>jina-clip-v1</code> 利用近几年的进展，在涉及文本和图像模态所有组合的任务中带来最先进的性能。</p><h2 id=\"introducing-jina-clip-v1\">介绍 Jina CLIP v1</h2><p>Jina CLIP v1 保留了 OpenAI 的原始 CLIP 架构：两个经过共同训练以在相同嵌入空间中产生输出的模型。</p><p>在文本编码方面，我们采用了 <a href=\"https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/?ref=jina-ai-gmbh.ghost.io\">Jina BERT v2</a> 架构，该架构用于 <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings v2 模型</a>。这种架构支持最先进的 8k 标记输入窗口，输出 768 维向量，可以从更长的文本中生成更准确的嵌入。这比原始 CLIP 模型支持的 77 个标记输入多了 100 多倍。</p><p>对于图像嵌入，我们使用的是北京人工智能研究院的最新模型：<a href=\"https://github.com/baaivision/EVA/tree/master/EVA-02?ref=jina-ai-gmbh.ghost.io\"><code>EVA-02</code></a> 模型。我们已经通过实验比较了多个图像 AI 模型，在类似预训练的跨模态场景中测试它们，<code>EVA-02</code> 明显优于其他模型。它的模型大小也与 Jina BERT 架构相当，因此图像和文本处理任务的计算负载大致相同。</p><p>这些选择为用户带来了重要的好处：</p><ul><li>在所有基准测试和所有模态组合上都有更好的性能，尤其是在纯文本嵌入性能方面有很大提升。</li><li><code>EVA-02</code> 在图像-文本和纯图像任务中都表现出经验上的优越性能，再加上 Jina AI 的额外训练，提高了纯图像性能。</li><li>支持更长的文本输入。<a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings 的 8k 标记</a>输入支持使其能够处理详细的文本信息并将其与图像关联。</li><li>由于这个多模态模型在非多模态场景中也具有高性能，因此在空间、计算、代码维护和复杂性方面都能节省大量成本。</li></ul><h3 id=\"training\">训练</h3><p>我们高性能多模态 AI 的部分秘诀在于我们的训练数据和流程。我们注意到，图像标题中使用的文本长度很短是 CLIP 式模型在纯文本性能不佳的主要原因，我们的训练明确地设计来解决这个问题。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png\" class=\"kg-image\" alt=\"Flowchart illustrating optimization of text and caption-image similarity in three tasks, using a model and encoders, ending i\" loading=\"lazy\" width=\"1600\" height=\"900\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/06/dark-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/06/dark-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/06/dark-1.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>训练分为三个步骤：</p><ol><li>使用带标题的图像数据来学习对齐图像和文本嵌入，同时穿插具有相似含义的文本对。这种协同训练共同优化这两种类型的任务。在这个阶段，模型的纯文本性能会下降，但不会像仅使用图像-文本对训练那样严重。</li><li>使用将图像与更大文本对齐的合成数据进行训练，这些文本由 AI 模型生成，用于描述图像。同时继续使用纯文本对进行训练。在这个阶段，模型学会结合图像关注更大的文本。</li><li>使用具有<a href=\"https://finetuner.jina.ai/advanced-topics/negative-mining/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">难负样本</a>的文本三元组来进一步提高纯文本性能，通过学习做出更细致的语义区分。同时，继续使用图像和长文本的合成对进行训练。在这个阶段，纯文本性能显著提高，而模型不会失去任何图像-文本能力。</ol><p>有关训练和模型架构的更多详细信息，请阅读<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">我们最近的论文</a>：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP：您的 CLIP 模型同时也是您的文本检索器</div><div class=\"kg-bookmark-description\">对比语言-图像预训练（CLIP）广泛用于通过将图像和文本映射到固定大小的向量中，在共同的嵌入空间中训练模型来对齐图像和文本。这些模型是多模态信息检索和相关任务的关键。然而，与专门的文本模型相比，CLIP 模型在纯文本任务中的表现通常较差。这导致信息检索系统需要为纯文本和多模态任务分别维护不同的嵌入和模型，造成效率低下。我们提出了一种新颖的多任务对比训练方法来解决这个问题，并用它来训练 jina-clip-v1 模型，在文本-图像和文本-文本检索任务上都达到了最先进的性能。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Andreas Koukounas</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a></figure><h2 id=\"new-state-of-the-art-in-multimodal-embeddings\">多模态嵌入的最新突破</h2><p>我们在纯文本、纯图像和涉及两种输入模态的跨模态任务中评估了 Jina CLIP v1 的性能。我们使用 <a href=\"https://huggingface.co/blog/mteb?ref=jina-ai-gmbh.ghost.io\">MTEB 检索基准</a>来评估纯文本性能。对于纯图像任务，我们使用 <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=jina-ai-gmbh.ghost.io\">CIFAR-100</a> 基准。对于跨模态任务，我们在 <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k?ref=jina-ai-gmbh.ghost.io\">Flickr8k</a>、<a href=\"https://www.kaggle.com/datasets/adityajn105/flickr30k?ref=jina-ai-gmbh.ghost.io\">Flickr30K</a> 和 <a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\">MSCOCO Captions</a> 上进行评估，这些都包含在 <a href=\"https://arxiv.org/abs/2203.05796?ref=jina-ai-gmbh.ghost.io\">CLIP Benchmark</a> 中。</p><p>结果总结在下表中：</p>\n<!--kg-card-begin: html-->\n<table id=\"compare_table\">\n<thead>\n<tr>\n<th>Model</th>\n<th>Text-Text</th>\n<th>Text-to-Image</th>\n<th>Image-to-Text</th>\n<th>Image-Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jina-clip-v1</td>\n<td>0.429</td>\n<td>0.899</td>\n<td>0.803</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>openai-clip-vit-b16</td>\n<td>0.162</td>\n<td>0.881</td>\n<td>0.756</td>\n<td>0.816</td>\n</tr>\n<tr style=\"font-weight:bold\">\n<td>% increase<br/>vs OpenAI CLIP</td>\n<td>165%</td>\n<td>2%</td>\n<td>6%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>从这些结果可以看出，<code>jina-clip-v1</code> 在所有类别中都优于 OpenAI 的原始 CLIP，并在纯文本和纯图像检索方面表现显著更好。平均在所有类别中，性能提升了 46%。</p><p>你可以在<a href=\"https://arxiv.org/abs/2405.20204?ref=jina-ai-gmbh.ghost.io\">我们最新的论文</a>中找到更详细的评估。</p><h2 id=\"getting-started-with-embeddings-api\">Embeddings API 入门</h2><p>你可以使用 <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\">Jina Embeddings API</a> 轻松地将 Jina CLIP v1 集成到你的应用程序中。</p><p>下面的代码展示了如何使用 Python 中的 <code>requests</code> 包调用 API 来获取文本和图像的嵌入。它将文本字符串和图像 URL 传递给 Jina AI 服务器，并返回两者的编码。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">☝️</div><div class=\"kg-callout-text\">记得将 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">&lt;YOUR_JINA_AI_API_KEY&gt;</code> 替换为已激活的 Jina API 密钥。你可以从 <a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#apiform\">Jina Embeddings 网页</a>获得一个包含一百万个免费令牌的试用密钥。</div></div><pre><code class=\"language-python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v1',\n  'encoding_type': 'float'\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")\n</code></pre><h3 id=\"integration-with-major-llm-frameworks\">与主要 LLM 框架的集成</h3><p>Jina CLIP v1 已经可以在 <a href=\"https://www.llamaindex.ai/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LlamaIndex</a> 和 <a href=\"https://www.langchain.com/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">LangChain</a> 中使用：</p><ul><li><a href=\"https://docs.llamaindex.ai/en/stable/examples/embeddings/jinaai_embeddings/?ref=jina-ai-gmbh.ghost.io\">LlamaIndex</a>：使用 <code>JinaEmbedding</code> 与 <code>MultimodalEmbedding</code> 基类，并调用 <code>get_image_embeddings</code> 或 <code>get_text_embeddings</code>。</li><li><a href=\"https://python.langchain.com/v0.1/docs/integrations/text_embedding/jina/?ref=jina-ai-gmbh.ghost.io\">LangChain</a>：使用 <code>JinaEmbeddings</code>，并调用 <code>embed_images</code> 或 <code>embed_documents</code>。</li></ul><h3 id=\"pricing\">定价</h3><p>文本和图像输入都按令牌消耗计费。</p><p>对于英文文本，<a href=\"https://jina.ai/news/a-deep-dive-into-tokenization/?ref=jina-ai-gmbh.ghost.io\">我们经验性地计算</a>出平均每个单词需要 1.1 个令牌。</p><p>对于图像，我们计算覆盖你的图像所需的 224x224 像素块的数量。这些图块可能部分为空白但计数相同。每个图块处理需要 1,000 个令牌。</p><p><strong>示例</strong></p><p>对于一个尺寸为 750x500 像素的图像：</p><ol><li>图像被分割成 224x224 像素的图块。<ol><li>要计算图块数量，将宽度像素除以 224，然后向上取整。<br>     750/224 ≈ 3.35 → 4</li><li>对高度像素重复相同操作：<br>     500/224 ≈ 2.23 → 3</li></ol></li><li>此示例中所需的总图块数量为：<br>           4（水平）x 3（垂直）= 12 个图块</li><li>成本将是 12 x 1,000 = 12,000 个令牌</li></ol><h3 id=\"enterprise-support\">企业支持</h3><p>我们为购买<a href=\"https://jina.ai/embeddings/?ref=jina-ai-gmbh.ghost.io#pricing\">110 亿令牌</a>生产部署计划的用户推出新的福利。这包括：</p><ul><li>与我们的产品和工程团队进行三小时的咨询，讨论你的具体用例和需求。</li><li>为你的 RAG（检索增强生成）或向量搜索用例定制的 Python notebook，演示如何将 Jina AI 的模型集成到你的应用程序中。</li><li>分配专门的客户经理和优先电子邮件支持，确保你的需求得到及时有效的满足。</li></ul><h2 id=\"open-source-jina-clip-v1-on-hugging-face\">Hugging Face 上的开源 Jina CLIP v1</h2><p>Jina AI 致力于开源搜索基础，为此，我们在 <a href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\">Hugging Face</a> 上以 <a href=\"https://www.apache.org/licenses/LICENSE-2.0?ref=jina-ai-gmbh.ghost.io\">Apache 2.0 许可证</a>免费提供这个模型。</p><p>你可以在 Hugging Face 上 <code>jina-clip-v1</code> 的模型页面找到下载和在自己系统或云安装上运行此模型的示例代码。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v1?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v1 · Hugging Face</div><div class=\"kg-bookmark-description\">我们正在通过开源和开放科学推进和民主化人工智能。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-clip-v1.png\" alt=\"\"></div></a></figure><h2 id=\"summary\">总结</h2><p>Jina AI 的最新模型 —— <code>jina-clip-v1</code> —— 代表了多模态嵌入模型的重大进步，相比 OpenAI 的 CLIP 提供了显著的性能提升。在纯文本和纯图像检索任务中有显著改进，在文本到图像和图像到文本任务中也具有竞争力，它为复杂的嵌入用例提供了一个很有前景的解决方案。</p><p>由于资源限制，此模型目前仅支持英语文本。我们正在努力扩展其功能以支持更多语言。</p>",
  "comment_id": "665f1ccd4b4b4c0001ba1c98",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-06-04T15:55:25.000+02:00",
  "updated_at": "2024-07-08T21:08:30.000+02:00",
  "published_at": "2024-06-05T11:42:02.000+02:00",
  "custom_excerpt": "Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    },
    {
      "id": "643967708f2e0b003d559311",
      "name": "Susana Guzmán",
      "slug": "susana",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/04/WhatsApp-Image-2022-12-06-at-15.46.39.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/susana/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/",
  "excerpt": "Jina AI 新推出的多模态嵌入模型不仅在文本-图像检索方面超越了 OpenAI CLIP，同时还是一个优秀的图像嵌入模型和领先的文本嵌入模型。您不再需要为不同的模态使用不同的模型了。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract 3D render of a neon blue and green grid pattern on a black background, creating a sense of depth.",
  "feature_image_caption": null
}
{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "DeepResearch 中用于生成多样化查询的次模优化",
  "html": "<p>在实施 DeepResearch 时，至少有两个地方需要生成多样化的查询。首先，你必须<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">根据用户输入生成网络搜索查询</a>（直接将用户输入扔进搜索引擎并不是一个好主意）。其次，许多 DeepResearch 系统包含<a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">一个 “研究计划器”，将原始问题分解为子问题</a>，并发调用代理独立解决这些子问题，然后合并它们的结果。无论是处理查询还是子问题，我们的期望都保持不变：它们必须与原始输入相关，并且足够多样化，以提供独特的视角。通常，我们需要限制查询的数量，以避免在不必要地请求搜索引擎或使用代理的词元上浪费金钱。</p><p>虽然理解查询生成的重要性，但大多数开源 DeepResearch 实现并没有认真对待这种优化。它们只是直接提示这些约束。有些可能会要求 大模型 额外进行一轮评估并使查询多样化。以下是大多数实现方法的基本示例：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">使用 大模型 生成多样化查询的两种不同提示词。顶部的提示词使用简单的指令。底部的提示词更复杂且结构化。给定原始查询和要生成的查询数量，我们希望生成的查询足够多样化。在此示例中，我们使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> 作为 大模型，原始查询是 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">。</span></figcaption></figure><p>在本文中，我想演示一种更严格的方法，使用句子 向量模型 和<strong>次模优化</strong>来解决最佳查询生成问题。早在我的博士期间，次模优化就是我最喜欢的技术之一，与 L-BFGS 并列。我将展示如何应用它来生成一组在基数约束下的多样化查询，这可以显着提高 DeepResearch 系统的整体质量。</p><h2 id=\"query-generation-via-prompting\">通过提示词生成查询</h2><p>首先，我们想检查提示词是否是生成多样化查询的有效方法。我们还想了解复杂的提示词是否比简单的提示词更有效。让我们运行一个实验，比较以下两个提示词，以找出答案：</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">简单提示词</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">结构化提示词</span></p></figcaption></figure><p>我们使用 <code>gemini-2.5-flash</code> 作为 大模型，原始查询为 <code>\"embeddings and rerankers\"</code>，并测试简单和结构化提示词，以迭代生成从 1 到 20 个查询。然后，我们使用带有 <code>text-matching</code> 任务的 <code>jina-embeddings-v3</code> 来测量原始查询和生成的查询之间的句子相似度，以及生成的查询本身的相似度。以下是可视化结果。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">两种提示词在 “生成的查询内部” 分析中显示出相似的模式（右侧两个图），中值余弦相似度在不同的查询数量中保持较高（0.4-0.6 范围）。当查询数量很大时，简单的提示词在使查询多样化方面似乎更好，而结构化的提示词保持与原始查询稍微更好的相关性，保持相关性在 0.6 左右。</span></figcaption></figure><p>查看右侧的两个图，可以看出简单和结构化的提示词都表现出余弦相似度得分的较大方差，许多达到 0.7-0.8 的相似度，表明某些生成的查询几乎相同。此外，随着生成更多查询，两种方法都难以保持多样性。我们没有看到相似度随着查询数量的增加而明显下降的趋势，而是观察到相对稳定（且较高）的相似度水平，表明额外的查询经常复制现有的视角。</p><p>一种解释是 Wang 等人 (2025) 发现 大模型 经常不成比例地反映主要群体的观点，即使使用提示词引导，也表明存在对常见视角的偏见。这是因为 大模型 的训练数据可能过度代表某些观点，导致模型生成与这些普遍观点一致的变体。Abe 等人 (2025) 还发现，基于 大模型 的查询扩展倾向于流行的解释，而忽略了其他解释。例如，“人工智能的好处是什么？” 可能会产生常见的益处，如自动化、效率、伦理，但会错过不太明显的益处，如药物发现。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">问题公式化</h2><p>有人可能认为我们之前的实验没有定论，我们应该改进提示词并再次尝试。虽然提示词肯定可以在某种程度上改变结果，但更重要的是我们已经学到了一些东西：简单地增加生成的查询数量使我们更有可能获得多样化的查询。坏消息是，我们也会得到一堆重复的查询作为副产品。</p><p>但是，既然生成大量查询很便宜，最终会产生<em>一些</em>好的查询，为什么我们不将其视为子集选择问题呢？</p>\n{{{output start}}}<p>在数学中，我们可以这样描述这个问题：给定一个原始输入 $q_0$，一组由大模型使用提示词工程生成的候选查询 $V=\\{q_1, q_2, \\cdots, q_n\\}$。选择一个子集 $X\\subseteq V$，包含 $k$ 个查询，以最大化覆盖范围并最小化冗余。</p><p>不幸的是，从 $n$ 个候选项中找到 $k$ 个查询的最佳子集需要检查 $\\binom{n}{k}$ 种组合，这是一个指数级的复杂度。仅仅对于 20 个候选和 $k=5$ 来说，就有 15,504 种组合。</p><h3 id=\"submodular-function\">次模函数</h3><p>在尝试暴力解决子集选择问题之前，让我向读者介绍一下<strong>次模性</strong>和<strong>次模函数</strong>这两个术语。对很多人来说，它们可能听起来很陌生，但你很可能听说过“收益递减”的概念——嗯，次模性就是对它的数学表示。</p><p>考虑在一个大型建筑物中放置 Wi-Fi 路由器以提供互联网覆盖。你安装的第一个路由器提供了巨大的价值——它覆盖了以前没有覆盖的很大一片区域。第二个路由器也增加了相当大的价值，但它的一些覆盖区域与第一个路由器重叠，因此边际效益低于第一个。当你继续添加路由器时，每个额外的路由器覆盖的新区域越来越少，因为大多数空间已经被现有路由器覆盖。最终，第 10 个路由器可能提供的额外覆盖范围非常小，因为建筑物已经被很好地覆盖了。</p><p>这种直觉抓住了次模性的本质。数学上，一个集合函数 $f: 2^V \\rightarrow \\mathbb{R}$ 是<strong>次模的</strong>，如果对于所有 $A \\subseteq B \\subseteq V$ 和任何元素 $v \\notin B$：</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>用通俗易懂的语言来说：将一个元素添加到一个较小的集合所带来的好处，至少与将同一个元素添加到一个包含该较小集合的较大集合所带来的好处一样多。</p><p>现在让我们将这个概念应用于我们的查询生成问题。人们可能会立即意识到，查询选择表现出自然的<strong>收益递减</strong>：</p><ul><li>我们选择的第一个查询覆盖了全新的语义空间</li><li>第二个查询应该覆盖不同的方面，但一些重叠是不可避免的</li><li>当我们添加更多查询时，每个额外的查询覆盖的新领域越来越少</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">来自</span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">我 2013 年在 AAAI 上发布的一个旧幻灯片</span></a><span style=\"white-space: pre-wrap;\">，我在其中使用一袋球来解释次模性。向袋中添加更多的球会改善“设施”，但相对改善变得越来越小，如右侧 y 轴上递减的 delta 值所示。</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">基于向量模型的次模函数设计</h2><p>设 $\\mathbf{e}_i \\in \\mathbb{R}^d$ 是查询 $q_i$ 的向量模型，使用句子向量模型（例如，<code>jina-embeddings-v3</code>）获得。设计目标函数主要有两种方法：</p><h3 id=\"approach-1-facility-location-coverage-based\">方法 1：设施选址（基于覆盖）</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>此函数衡量所选集合 $X$ 对所有候选查询的“覆盖”程度，其中：</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ 是余弦相似度</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ 确保与原始查询的相关性</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ 衡量所选集合 $X$ 对候选 $j$ 的覆盖程度</li></ul><p>需要注意的是，此函数仅<em>隐式</em>地鼓励多样性。它不会明确地惩罚所选集合 $X$ 内的相似性。多样性的出现是因为选择相似的查询会提供递减的覆盖回报。</p><h3 id=\"approach-2-explicit-coverage-diversity\">方法 2：显式覆盖 + 多样性</h3><p>为了更直接地控制多样性，我们可以将覆盖范围和显式多样性项结合起来：</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>其中，多样性分量可以表示为：</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>此多样性项衡量所选查询和未选择查询之间的总相似度——当我们选择与剩余候选查询不同的查询时，它会最大化（一种图割函数的形式）。</p><h3 id=\"difference-between-two-approaches\">两种方法之间的区别</h3><p>两种公式都保持了次模性。</p><p>设施选址函数是一个众所周知的次模函数。由于 max 运算，它表现出次模性：当我们向所选集合添加一个新查询 $q$ 时，每个候选查询 $j$ 都会被我们集合中的“最佳”查询（具有最高相似度的查询）覆盖。与添加到较大的集合 $B \\supseteq A$ 相比，将 $q$ 添加到较小的集合 $A$ 更有可能改善各种候选查询的覆盖范围，因为在较大的集合中，许多候选查询已经被很好地覆盖。</p><p>在图割多样性函数中，多样性项 $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ 是次模的，因为它衡量了所选集合和未选择集合之间的“割”。与添加到较大的所选集合相比，将新查询添加到较小的所选集合会创建更多与未选择查询的新连接。</p><p>设施选址方法依赖于通过覆盖竞争实现的<em>隐式</em>多样性，而显式方法直接衡量和优化多样性。因此，两者都是有效的，但显式方法使你可以更直接地控制相关性-多样性的权衡。</p><h2 id=\"implementations\">实现</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">完整的实现可以在 Github 上找到：</span></p></figcaption></figure><p>由于我们的函数是次模的，我们可以使用<strong>贪婪算法</strong>，该算法提供了 $(1-1/e) \\approx 0.63$ 的近似保证：</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>这是优化设施选址（基于覆盖）的代码——具有隐式多样性的代码。</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    用于次模查询选择的贪婪算法\n    \n    Args:\n        candidates: 候选查询字符串列表\n        embeddings: 查询向量模型矩阵 (n x d)\n        original_embedding: 原始查询的向量模型 (d,)\n        k: 要选择的查询数\n        alpha: 相关性权重参数\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # 预计算相关性得分\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # 计算添加查询 i 的边际收益\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"计算将 new_idx 添加到所选集合的边际收益\"\"\"\n    if not selected:\n        # 第一个查询：收益是所有相关性得分的总和\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # 计算当前覆盖范围\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # 计算添加附加查询后的新覆盖范围\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>平衡参数 $\\alpha$ 控制相关性和多样性之间的权衡：</p><ul><li><strong>高 $\\alpha$（例如，0.8）</strong>：优先考虑与原始查询的相关性，可能会牺牲多样性</li><li><strong>低 $\\alpha$（例如，0.2）</strong>：优先考虑所选查询之间的多样性，可能会偏离原始意图</li><li><strong>中等 $\\alpha$（例如，0.4-0.6）</strong>：平衡的方法，通常在实践中效果很好</li></ul><h3 id=\"lazy-greedy-algorithm\">惰性贪婪算法</h3><p>你可能会在上面的代码中注意到：</p><pre><code class=\"language-python\">for i in remaining:\n    # 计算添加查询 i 的边际收益\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>我们在每次迭代中计算<strong>所有</strong>剩余候选查询的边际收益。我们可以做得更好。</p><p><strong>惰性贪婪算法</strong>是一种巧妙的优化，它利用次模性来避免不必要的计算。关键的见解是：如果在迭代 $t$ 中元素 A 的边际收益高于元素 B，那么在迭代 $t+1$ 中，A 的边际收益仍然高于 B（由于次模性）。</p><pre><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>惰性贪婪算法的工作原理如下：</p><ol><li>维护一个按边际收益排序的元素优先级队列</li><li>仅重新计算顶部元素的边际收益</li><li>如果在重新计算后它仍然是最高的，则选择它</li><li>否则，将其重新插入到正确的位置并检查下一个顶部元素</li></ol><p>由于我们避免了重新计算明显不会被选中的元素的边际收益，因此这可以显著提高速度。</p><h3 id=\"results\">结果</h3><p>让我们再次运行实验。我们使用相同的简单 提示词 生成 1 到 20 个不同的查询，并执行与之前相同的余弦相似度测量。对于子模优化，我们从 20 个生成的候选项中使用不同的 k 值选择查询，并像以前一样测量相似度。结果表明，通过子模优化选择的查询更加多样化，并且显示出更低的集合内相似度。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查询 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查询 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查询 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">原始查询 = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">最终问题：为什么子模公式很重要</h2><p>您可能想知道：为什么还要费力地将其表述为子模优化问题呢？为什么不使用启发式或其他优化方法呢？</p><p>简而言之，子模公式将临时的“选择多样化查询”启发式转换为具有<strong>可证明的保证</strong>、<strong>高效的算法</strong>和可衡量的目标的严格优化问题。</p><h3 id=\"guaranteed-efficiency\">保证效率</h3><p>一旦我们证明了我们的目标函数是子模的，我们就可以获得强大的理论保证和高效的算法。与检查$\\binom{n}{k}$组合相比，在$O(nk)$时间内运行的贪婪算法实现了$(1-1/e) \\approx 0.63$对最优解的近似。这意味着我们的贪婪解始终至少与最佳可能解一样好 63%。<strong>没有启发式方法可以保证这一点。</strong></p><p>此外，由于子模函数的数学结构，惰性贪婪算法在实践中要快得多。加速来自<strong>收益递减</strong>：在早期迭代中表现不佳的元素不太可能在以后成为好的选择。因此，惰性贪婪通常只需要重新计算前几个候选项的收益，而不是检查所有$n$个候选项。</p><h3 id=\"no-need-for-hand-crafted-heuristics\">无需手工制作的启发式方法</h3><p>如果没有一个有原则的框架，您可能会求助于诸如“确保查询的余弦相似度 &lt; 0.7”或“平衡不同的关键字类别”之类的临时规则。这些规则很难调整并且不具有通用性。子模优化为您提供了一种有原则的、数学上可靠的方法。您可以使用验证集系统地调整超参数，并在生产系统中监控解决方案质量。当系统产生不良结果时，您可以获得明确的指标来调试出了什么问题。</p><p>最后，子模优化是一个经过深入研究的领域，拥有数十年的研究经验，允许您利用贪婪算法之外的高级算法（如加速贪婪或局部搜索），关于某些公式何时效果最佳的理论见解，以及处理额外约束（如预算限制或公平性要求）的扩展。</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">对于那些对子模优化感兴趣的人，我建议您访问此站点以了解更多信息。</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "很多人都知道查询多样性在 DeepResearch 中的重要性，但很少有人知道如何通过子模优化来严格地解决这个问题。",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
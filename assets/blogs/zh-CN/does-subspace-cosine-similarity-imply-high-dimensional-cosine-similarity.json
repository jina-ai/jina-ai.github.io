{
  "slug": "does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity",
  "id": "65af98d28da8040001e17008",
  "uuid": "d8fdbdb8-0820-42bf-aab7-6751ae6141e1",
  "title": "子空间余弦相似度是否意味着高维空间的余弦相似度？",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">在 2024 年 1 月 25 日，OpenAI 发布了<a href=\"https://openai.com/blog/new-embedding-models-and-api-updates?ref=jina-ai-gmbh.ghost.io\">一个新的嵌入模型</a>，其中包含一个名为<i><b><strong class=\"italic\" style=\"white-space: pre-wrap;\">\"shortening\"</strong></b></i>的新功能，它允许开发者在不影响嵌入向量有效表示概念的能力的前提下，裁剪嵌入向量——基本上是从序列末尾截断数值。本文将深入探讨这一创新背后的理论基础和合理性。</div></div><p>考虑这样一个问题：在高维空间中测量嵌入向量的余弦相似度时，它们在低维子空间的相似度如何影响整体相似度？这是一种直接的、成比例的关系，还是在高维数据中存在更复杂的关系？</p><p>更具体地说，<strong>向量在其前 256 个维度的高相似度是否能保证它们在完整的 768 个维度中也具有高相似度？</strong>相反，如果向量在某些维度上存在显著差异，是否意味着整体相似度较低？这些不仅仅是理论思考，它们也是高效向量检索、数据库索引和 RAG 系统性能的关键考虑因素。</p><p>开发者经常依赖启发式方法，假设子空间的高相似度等同于整体的高相似度，或者认为在某一维度上的显著差异会对整体相似度产生重大影响。问题是：这些启发式方法是否建立在坚实的理论基础之上，还是仅仅是为了方便而做出的假设？</p><p>本文将深入探讨这些问题，研究子空间相似度与整体向量相似度之间的理论关系及其实际意义。</p><h2 id=\"bounding-the-cosine-similarity\">余弦相似度的边界</h2><p>给定向量 $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^d$，我们将它们分解为 $\\mathbf{A}=[\\mathbf{A}_1, \\mathbf{A}_2]$ 和 $\\mathbf{B}=[\\mathbf{B}_1, \\mathbf{B}_2]$，其中 $\\mathbf{A}_1,\\mathbf{B}_1\\in\\mathbb{R}^m$ 且 $\\mathbf{A}_2,\\mathbf{B}_2\\in\\mathbb{R}^n$，且 $m+n=d$。</p><p>在子空间 $\\mathbb{R}^m$ 中的余弦相似度由 $\\cos(\\mathbf{A}_1, \\mathbf{B}_1)=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1}{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|}$ 给出；类似地，在子空间 $\\mathbb{R}^n$ 中的相似度为 $\\cos(\\mathbf{A}_2, \\mathbf{B}_2)=\\frac{\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}$。</p><p>在原始空间 $\\mathbb{R}^d$ 中，余弦相似度定义为：$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;=\\frac{\\mathbf{A}\\cdot\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}\\\\&amp;=\\frac{\\mathbf{A}_1\\cdot\\mathbf{B}_1+\\mathbf{A}_2\\cdot\\mathbf{B}_2}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\cos(\\mathbf{A}_1, \\mathbf{B}_1)\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\cos(\\mathbf{A}_2, \\mathbf{B}_2)\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\end{align*}$$</p><p>令 $s := \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$。则我们有：$$\\begin{align*}\\cos(\\mathbf{A},\\mathbf{B})&amp;\\leq\\frac{s\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+s\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\\\&amp;=\\frac{\\|\\mathbf{A}_1\\|\\|\\mathbf{B}_1\\|+\\|\\mathbf{A}_2\\|\\|\\mathbf{B}_2\\|}{\\sqrt{\\|\\mathbf{A}_1\\|^2+\\|\\mathbf{A}_2\\|^2}\\sqrt{\\|\\mathbf{B}_1\\|^2+\\|\\mathbf{B}_2\\|^2}}\\cdot s\\\\&amp;=\\cos(\\underbrace{[\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|]}_{\\mathbb{R}^2}, \\underbrace{[\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]}_{\\mathbb{R}^2})\\cdot s\\\\&amp;\\leq 1\\cdot s \\\\&amp;= \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}$$</p><p>证明结束。</p><p>注意，在证明的最后一步中，我们利用了余弦相似度总是小于等于 1 的性质。这构成了我们的上界。类似地，我们可以证明 \\(\\cos(\\mathbf{A},\\mathbf{B})\\) 的下界为：</p><p>\\[ \\cos(\\mathbf{A},\\mathbf{B}) \\geq t \\cdot \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) \\]，其中 $t:= \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))$。</p><p>注意，对于下界，我们不能轻易得出 \\(\\cos(\\mathbf{A},\\mathbf{B}) \\geq t\\) 的结论。这是因为余弦函数的取值范围在 \\([-1, 1]\\) 之间。由于这个范围，我们无法建立比平凡值 -1 更紧的下界。</p><p>因此总结来说，我们有以下松散边界：$$ -1\\leq\\cos(\\mathbf{A},\\mathbf{B})\\leq\\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)).$$ 以及更紧的边界 \\[\\begin{align*} \\gamma \\cdot t\\leq&amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq\\gamma\\cdot s\\\\\\gamma \\cdot \\min(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2)) \\leq &amp;\\cos(\\mathbf{A}, \\mathbf{B}) \\leq \\gamma \\cdot \\max(\\cos(\\mathbf{A}_1, \\mathbf{B}_1), \\cos(\\mathbf{A}_2, \\mathbf{B}_2))\\end{align*}\\]，其中 $\\gamma = \\cos([\\|\\mathbf{A}_1\\|, \\|\\mathbf{A}_2\\|], [\\|\\mathbf{B}_1\\|, \\|\\mathbf{B}_2\\|]) $。</p><h3 id=\"connection-to-johnson%E2%80%93lindenstrauss-lemma\">与 Johnson–Lindenstrauss 引理的联系</h3><p>JL 引理指出，对于任何 \\(0 &lt; \\epsilon &lt; 1\\) 和任何有限点集 \\( S \\) 在 \\( \\mathbb{R}^d \\) 中，存在一个映射 \\( f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\)（其中 \\( k = O(\\epsilon^{-2} \\log |S|) \\)），使得对于所有 \\( \\mathbf{u}, \\mathbf{v} \\in S \\)，欧氏距离近似保持：<br><br>\\[(1 - \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2 \\leq \\|f(\\mathbf{u}) - f(\\mathbf{v})\\|^2 \\leq (1 + \\epsilon) \\|\\mathbf{u} - \\mathbf{v}\\|^2\\]</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Johnson–Lindenstrauss lemma - Wikipedia</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://en.wikipedia.org/static/apple-touch/wikipedia.png\" alt=\"\"><span class=\"kg-bookmark-author\">Wikimedia Foundation, Inc.</span><span class=\"kg-bookmark-publisher\">Contributors to Wikimedia projects</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f173a9fe1686cca4e497db35b4f908926294930\" alt=\"\"></div></a></figure><p>为了使 $f$ 像子空间选择一样工作，我们可以使用对角矩阵进行投影，比如一个 \\(5 \\times 3\\) 矩阵 \\(f\\)，尽管它不是随机的（注意，JL 引理的典型表述涉及使用从高斯分布中抽取的随机矩阵的线性变换）。例如，如果我们想要保留 5 维向量空间中的第 1、3 和第 5 维，矩阵 \\(f\\) 可以设计如下：\\[f = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]<br>然而，通过将 $f$ 指定为对角矩阵，我们限制了可用于投影的函数类。JL 引理保证了在更广泛的线性变换类中存在合适的 $f$，但当我们将 $f$ 限制为对角矩阵时，在这个受限类中可能不存在适用于 JL 引理边界的合适 $f$。</p><h2 id=\"validating-the-bounds\">验证边界</h2><p>为了经验性地探索高维向量空间中余弦相似度的理论边界，我们可以使用蒙特卡洛模拟。这种方法允许我们生成大量随机向量对，计算它们在原始空间和子空间中的相似度，然后评估理论上界和下界在实践中的表现如何。</p><p>以下 Python 代码片段实现了这一概念。它随机生成高维空间中的向量对并计算它们的余弦相似度。然后，将每个向量分为两个子空间，计算每个子空间内的余弦相似度，并基于子空间相似度评估全维度余弦相似度的上界和下界。</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-python\">import numpy as np\n\n\ndef compute_cosine_similarity(U, V):\n    # Normalize the rows to unit vectors\n    U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n    V_norm = V / np.linalg.norm(V, axis=1, keepdims=True)\n    # Compute pairwise cosine similarity\n    return np.sum(U_norm * V_norm, axis=1)\n\n\n# Generate random data\nnum_points = 5000\nd = 1024\nA = np.random.random([num_points, d])\nB = np.random.random([num_points, d])\n\n# Compute cosine similarity between A and B\ncos_sim = compute_cosine_similarity(A, B)\n\n# randomly divide A and B into subspaces\nm = np.random.randint(1, d)\nA1 = A[:, :m]\nA2 = A[:, m:]\nB1 = B[:, :m]\nB2 = B[:, m:]\n\n# Compute cosine similarity in subspaces\ncos_sim1 = compute_cosine_similarity(A1, B1)\ncos_sim2 = compute_cosine_similarity(A2, B2)\n\n# Find the element-wise maximum and minimum of cos_sim1 and cos_sim2\ns = np.maximum(cos_sim1, cos_sim2)\nt = np.minimum(cos_sim1, cos_sim2)\n\nnorm_A1 = np.linalg.norm(A1, axis=1)\nnorm_A2 = np.linalg.norm(A2, axis=1)\nnorm_B1 = np.linalg.norm(B1, axis=1)\nnorm_B2 = np.linalg.norm(B2, axis=1)\n\n# Form new vectors in R^2 from the norms\nnorm_A_vectors = np.stack((norm_A1, norm_A2), axis=1)\nnorm_B_vectors = np.stack((norm_B1, norm_B2), axis=1)\n\n# Compute cosine similarity in R^2\ngamma = compute_cosine_similarity(norm_A_vectors, norm_B_vectors)\n\n# print some info and validate the lower bound and upper bound\nprint('d: %d\\n'\n      'm: %d\\n'\n      'n: %d\\n'\n      'avg. cosine(A,B): %f\\n'\n      'avg. upper bound: %f\\n'\n      'avg. lower bound: %f\\n'\n      'lower bound satisfied: %s\\n'\n      'upper bound satisfied: %s' % (\n          d, m, (d - m), np.mean(cos_sim), np.mean(s), np.mean(gamma * t), np.all(s &gt;= cos_sim),\n          np.all(gamma * t &lt;= cos_sim)))\n</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">用于验证余弦相似度边界的 Monte Carlo 验证器</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-output\">d: 1024\nm: 743\nn: 281\navg. cosine(A,B): 0.750096\navg. upper bound: 0.759080\navg. lower bound: 0.741200\nlower bound satisfied: True\nupper bound satisfied: True</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">我们的 Monte Carlo 验证器的一个示例输出。需要注意的是，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>lower/upper bound satisfied</span></code><span style=\"white-space: pre-wrap;\">条件是针对每个向量单独检查的。同时，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>avg. lower/upper bound</span></code><span style=\"white-space: pre-wrap;\">提供了这些边界相关统计信息的更直观概览，但并不直接影响验证过程。</span></p></figcaption></figure><h2 id=\"understanding-the-bounds\">理解边界</h2><p>简而言之，在比较两个高维向量时，整体相似度位于其子空间最佳和最差相似度之间，并根据这些子空间在整体方案中的大小或重要性进行调整。这就是高维余弦相似度边界直观表示的含义：最相似和最不相似部分之间的平衡，根据它们的相对大小或重要性进行加权。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png\" class=\"kg-image\" alt=\"Illustrative comparison of two stylus pen caps and bodies with labeled sections on a black background\" loading=\"lazy\" width=\"1200\" height=\"627\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--35-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">每支笔都有两个主要组件：笔身和笔帽。</span></figcaption></figure><p>想象你正在尝试比较两个多部件对象（比如说，两支精美的钢笔）的整体相似度。每支笔都有两个主要组件：笔身和笔帽。我们要确定的是整支笔（包括笔身和笔帽）的相似度：</p><h3 id=\"upper-bound-gamma-cdot-s\">上界（$\\gamma \\cdot s$）</h3><p>可以把 $s$ 理解为钢笔对应部件之间的最佳匹配。如果笔帽非常相似但笔身不太相似，那么 $s$ 就是笔帽的相似度。</p><p>而 $\\gamma$ 则像是基于每个部件大小（或重要性）的缩放因子。如果一支笔有很长的笔身和很短的笔帽，而另一支笔有很短的笔身和很长的笔帽，$\\gamma$ 就会根据这些比例差异来调整整体相似度。</p><p>上界告诉我们，无论某些部分有多相似，整体相似度都不能超过这个\"最佳部分相似度\"乘以比例因子。</p><h3 id=\"lower-bound-gamma-cdot-t\">下界（$\\gamma \\cdot t$）</h3><p>这里的 $t$ 是匹配程度最差的部分的相似度。如果笔身差异很大但笔帽很相似，$t$ 就反映了笔身的相似度。</p><p>同样，$\\gamma$ 根据每个部分的比例来缩放这个值。</p><p>下界意味着整体相似度不会低于这个\"最差部分相似度\"在考虑了各部分比例后的值。</p><h2 id=\"implications-of-the-bounds\">边界的含义</h2><p>对于从事嵌入、向量搜索、检索或数据库工作的软件工程师来说，理解这些边界具有实际意义，特别是在处理高维数据时。向量搜索通常涉及在数据库中查找与给定查询向量最接近（最相似）的向量，通常使用余弦相似度作为衡量接近程度的指标。我们讨论的边界可以为这类任务使用子空间相似度的有效性和局限性提供见解。</p><h3 id=\"using-subspace-similarity-for-ranking\">使用子空间相似度进行排序</h3><p><strong>安全性和准确性</strong>：使用子空间相似度进行排序和检索 top-k 结果可以是有效的，但需要谨慎。上界表明整体相似度不能超过子空间的最大相似度。因此，如果一对向量在特定子空间中高度相似，它很可能在高维空间中也是相似的。</p><p><strong>潜在陷阱</strong>：然而，下界表明在一个子空间中相似度较低的两个向量整体上仍可能相当相似。因此，仅仅依赖子空间相似度可能会遗漏一些相关结果。</p><h3 id=\"misconceptions-and-cautions\">误解和注意事项</h3><p><strong>过高估计子空间重要性</strong>：一个常见的误解是过高估计某个特定子空间的重要性。虽然在一个子空间中的高相似度是个好的指标，但由于其他子空间的影响，这并不能保证整体相似度也很高。</p><p><strong>忽视负相似度</strong>：在子空间中余弦相似度为负的情况下，表示在该维度上存在对立关系。工程师应该注意这些负相似度如何影响整体相似度。</p>",
  "comment_id": "65af98d28da8040001e17008",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Explore-image-storytelling-beyond-pixels--34-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-01-23T11:45:38.000+01:00",
  "updated_at": "2024-01-25T21:34:27.000+01:00",
  "published_at": "2024-01-23T12:22:57.000+01:00",
  "custom_excerpt": "Does high similarity in subspace assure a high overall similarity between vectors? This post examines the theory and practical implications of subspace similarity. ",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/does-subspace-cosine-similarity-imply-high-dimensional-cosine-similarity/",
  "excerpt": "子空间中的高相似度是否能保证向量之间的整体高相似度？本文将探讨子空间相似度的理论和实际意义。",
  "reading_time": 9,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Diagram illustrating a neural network process with smiley faces and repeated mentions of \"Similar\" on a blackboard-like backg",
  "feature_image_caption": null
}
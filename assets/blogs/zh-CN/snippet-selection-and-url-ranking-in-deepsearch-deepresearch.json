{
  "slug": "snippet-selection-and-url-ranking-in-deepsearch-deepresearch",
  "id": "67d13ae9099ee70001bed48b",
  "uuid": "84611c0f-675d-4838-b809-4ced6cf842a9",
  "title": "DeepSearch/DeepResearch 中的片段选择和 URL 排序",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">DeepSearch/DeepResearch 实施实践指南</div><div class=\"kg-bookmark-description\">QPS 不再重要，深度才是关键。DeepSearch 成为新标准。通过读取-搜索-推理循环来寻找答案。了解它是什么以及如何构建它。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-22.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/a-practical-guide-to-implementing-deepsearch-deepresearch-1.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>如果你已经阅读了我们的 DeepSearch/DeepResearch 实施指南，让我们深入探讨一些能够<em>大幅</em>提升质量的细节。在这篇文章中，我们将重点关注两个关键挑战：<strong>利用 embeddings 从冗长的网页中选择片段</strong>以及<strong>使用 rerankers 来为爬取 URL 设定优先级。</strong></p><p>有些人可能还记得我们之前的结论，即\"embeddings 仅对查询去重等 STS 任务（语义文本相似度）有用，而 rerankers 甚至不是我们最初 DeepSearch 实现的一部分。\"事实证明，这两者仍然非常有价值 —— 只是不是以传统的方式。我们一直遵循<em>最精简</em>的路径。我们不会为了证明它们的存在价值或我们作为 embedding 和 reranker 提供商的价值而添加组件。<strong>我们专注于搜索真正需要的基础功能。</strong></p><p>经过数周的实验和迭代，我们发现了这两种技术在 DeepSearch/DeepResearch 系统中不常见但有效的用途。通过应用它们，我们显著提高了 <a href=\"https://search.jina.ai\" rel=\"noreferrer\">Jina DeepSearch</a> 的质量（欢迎体验）。我们希望与该领域的从业者分享这些见解。</p><h2 id=\"select-snippet-from-long-content\">从长内容中选择片段</h2><p>问题是这样的：在<a href=\"https://jina.ai/reader\">使用 Jina Reader 读取网页内容</a>后，我们需要将其作为知识项添加到代理的上下文中以进行推理。虽然将完整内容直接放入 LLM 的上下文窗口是最简单的方法，但考虑到 token 成本和生成速度，这并不是最优的。实践中，我们需要识别内容中与问题最相关的部分，并有选择地只将这些部分作为知识添加到代理的上下文中。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">我们讨论的是即使经过 Jina Reader 的 markdown 清理后内容仍然过长的情况。这种情况经常出现在 GitHub issues、Reddit 帖子、论坛讨论和博客文章（包括我们自己在 jina.ai/news 上的许多文章）中。</div></div><p>基于 LLM 的过滤存在相同的成本和延迟问题，所以让我们寻找更小型模型的解决方案：我们需要更小更便宜，<strong>但仍然支持多语言的模型</strong> —— 这是一个关键因素，因为我们无法保证查询或文档始终是英文的。</p><p>我们一边有问题（原始查询或缺口问题），另一边有大量的 markdown 内容，其中大部分内容都是无关的。我们需要为查询选择最相关的片段。这类似于 RAG 社区自 2023 年以来一直在处理的分块问题 —— 使用检索模型仅检索相关块放入上下文窗口进行总结。然而，在我们的情况下有两个关键区别：</p><ol><li>来自有限文档的有限块。如果每个块大约包含 500 个 tokens，那么一个典型的长网页文档大约有 200,000 个 tokens（p50）到 1,000,000 个 tokens（p99），而我们每步使用 Jina Reader 获取 4-5 个 URL，这将产生大约数百个块 —— 意味着数百个 embedding 向量和数百个余弦相似度。这在 JavaScript 内存中很容易处理，无需向量数据库。</li><li>我们需要连续的块来形成有效的知识片段。我们不能接受像 <code>[1-2, 6-7, 9, 14, 17, ...]</code> 这样组合散落句子的片段。更有用的知识片段应该遵循像 <code>[3-15, 17-24, ...]</code> 这样的模式 —— 始终保持文本连续。这使得 LLM 更容易从知识源复制和引用，并减少幻觉。</li></ol><p>剩下的都是从业者抱怨的常见问题：每个块不能太长，因为 embedding 模型无法很好地处理长上下文；分块会导致上下文丢失并使块 embeddings 变得独立同分布；如何找到最佳的边界线索来同时保持可读性和语义？如果你明白我们在说什么，那么你可能在 RAG 实现中也被这些问题困扰过。</p><p>但长话短说 —— <strong>采用 late-chunking 和 <code>jina-embeddings-v3</code> 完美解决了这三个问题。</strong>Late chunking 为每个块保留了上下文信息，对边界线索<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii#late-chunking-is-resilient-to-poor-boundary-cues\">不敏感</a>，而 <code>jina-embeddings-v3</code> 本身在<em>非对称</em>多语言检索任务中达到了 SOTA。感兴趣的读者可以查看我们的博客文章或论文了解详情，这里是整体实现。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/Untitled-design--14-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1000\"><figcaption><span style=\"white-space: pre-wrap;\">这个图表展示了片段选择算法，其工作原理类似于 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Conv1D</span></code><span style=\"white-space: pre-wrap;\">。该过程首先将长文档分割成固定长度的块，然后使用开启 late-chunking 的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> 进行嵌入。在计算每个块与查询之间的相似度分数后，使用滑动窗口在相似度分数上移动，找到平均值最高的窗口。</span></figcaption></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">什么是 Late Chunking 以及它不是什么：第二部分</div><div class=\"kg-bookmark-description\">我们探索 Late Chunking 系列的第 2 部分，深入探讨为什么它是块嵌入和改进搜索/RAG 性能的最佳方法。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-23.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/what-late-chunking-really-is-and-what-its-not-part-ii.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3：具有任务 LoRA 的多语言嵌入</div><div class=\"kg-bookmark-description\">我们推出 jina-embeddings-v3，这是一个具有 5.7 亿参数的新型文本嵌入模型，在多语言数据和长上下文检索任务中实现了最先进的性能，支持长达 8192 个 tokens 的上下文长度。该模型包括一组特定任务的低秩适应（LoRA）适配器，用于生成高质量的查询-文档检索、聚类、分类和文本匹配的嵌入。在 MTEB 基准测试中的评估显示，jina-embeddings-v3 在英语任务上优于 OpenAI 和 Cohere 最新的专有嵌入，同时在所有多语言任务上比 multilingual-e5-large-instruct 表现更好。默认输出维度为 1024，用户可以在不影响性能的情况下灵活地将嵌入维度降低至 32，这得益于套娃表示学习。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-9.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking：使用长上下文嵌入模型的上下文块嵌入</div><div class=\"kg-bookmark-description\">许多用例需要检索较小的文本部分，密集向量检索系统通常在较短的文本段上表现更好，因为语义在嵌入中不太可能被过度压缩。因此，从业者经常将文本文档分割成更小的块并单独编码。然而，以这种方式创建的块嵌入可能会丢失周围块的上下文信息，导致次优的表示。在本文中，我们提出了一种称为 late chunking 的新方法，该方法首先嵌入长文本的所有 tokens，在 transformer 模型之后且在平均池化之前才进行分块 —— 因此命名为 late。生成的块嵌入捕获完整的上下文信息，在各种检索任务中取得了更好的结果。该方法足够通用，可以应用于各种长上下文嵌入模型，并且无需额外训练即可工作。为了进一步提高 late chunking 的效果，我们提出了一种专门的嵌入模型微调方法。</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-10.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-6.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">function cherryPick(question, longContext, options) {\n  if (longContext.length &lt; options.snippetLength * options.numSnippets)\n    return longContext;\n  \n  const chunks = splitIntoChunks(longContext, options.chunkSize);\n  \n  const chunkEmbeddings = getEmbeddings(chunks, \"retrieval.passage\");\n  const questionEmbedding = getEmbeddings([question], \"retrieval.query\")[0];\n  \n  const similarities = chunkEmbeddings.map(embed =&gt; \n    cosineSimilarity(questionEmbedding, embed));\n  \n  const chunksPerSnippet = Math.ceil(options.snippetLength / options.chunkSize);\n  const snippets = [];\n  const similaritiesCopy = [...similarities];\n  \n  for (let i = 0; i &lt; options.numSnippets; i++) {\n    let bestStartIndex = 0;\n    let bestScore = -Infinity;\n    \n    for (let j = 0; j &lt;= similarities.length - chunksPerSnippet; j++) {\n      const windowScores = similaritiesCopy.slice(j, j + chunksPerSnippet);\n      const windowScore = average(windowScores);\n      \n      if (windowScore &gt; bestScore) {\n        bestScore = windowScore;\n        bestStartIndex = j;\n      }\n    }\n    \n    const startIndex = bestStartIndex * options.chunkSize;\n    const endIndex = Math.min(startIndex + options.snippetLength, longContext.length);\n    snippets.push(longContext.substring(startIndex, endIndex));\n    \n    for (let k = bestStartIndex; k &lt; bestStartIndex + chunksPerSnippet; k++)\n      similaritiesCopy[k] = -Infinity;\n  }\n  \n  return snippets.join(\"\\n\\n\");\n}</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">使用延迟分块和类似 Conv1D 的平均池化来选择与问题最相关的片段。</span></p></figcaption></figure><p>确保在调用 Jina Embeddings API 时设置以下 retrieval <code>task</code>、<code>late_chunking</code> 和 <code>truncate</code> 参数：</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-javascript\">await axios.post(\n  'https://api.jina.ai/v1/embeddings',\n  {\n    model: \"jina-embeddings-v3\",\n    task: \"retrieval.passage\",\n    late_chunking: true,\n    input: chunks,\n    truncate: true\n  }, \n  { headers }); </code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">对于问题嵌入，请确保将 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>task</span></code><span style=\"white-space: pre-wrap;\"> 改为 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>retrieval.query</span></code><span style=\"white-space: pre-wrap;\"> 并关闭 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code></p></figcaption></figure><p>完整实现可以在 Github 上找到：</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/tools/jina-latechunk.ts\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/tools/jina-latechunk.ts at main · jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-5.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-2\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"rank-url-for-next-read\">对下一步阅读的 URL 进行排序</h2><p>问题在于：在 DeepSearch 会话期间，你可能会从搜索引擎结果页面（SERP）收集大量 URL，并在阅读单个网页时发现更多链接（页面内链接）。唯一 URL 的总数很容易达到数百个。同样，简单地将所有 URL 直接放入 LLM 的上下文是低效的——这会浪费宝贵的上下文窗口空间，更糟糕的是，<strong>我们发现 LLM 基本上是随机选择 URL。</strong>引导 LLM 到最有可能包含所需答案的 URL 至关重要。</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-bash\">curl https://r.jina.ai/https://example.com \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Retain-Images: none\" \\\n  -H \"X-Md-Link-Style: discarded\" \\\n  -H \"X-Timeout: 20\" \\\n  -H \"X-With-Links-Summary: all\"</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">在 DeepSearch 中使用 Jina Reader 爬取页面的最佳选项。这将在单独的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>links</span></code><span style=\"white-space: pre-wrap;\"> 字段中收集所有页面内链接，并从 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>content</span></code><span style=\"white-space: pre-wrap;\"> 字段中移除它们。</span></p></figcaption></figure><p>将这个问题视为一个上下文内的 PageRank，我们需要在会话期间对数百个 URL 进行权重计算。我们基于多个因素对 URL 进行排名，这些因素结合了最后更新时间、域名频率、路径结构，最重要的是与查询的语义相关性，以创建一个综合得分。请记住，我们只能使用实际访问 URL <em>之前</em> 可用的信息：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/03/url-ranking-illustration--2-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"199\" height=\"150\"></figure><p><strong>频率信号</strong>：在不同来源中多次出现的 URL 会获得额外权重。来自在搜索结果中频繁出现的域名的 URL 会获得提升，因为热门域名通常包含权威内容。</p><p><strong>路径结构</strong>：我们分析 URL 路径以识别内容集群。在常见路径层次结构中的 URL 获得更高分数，对更深路径应用衰减因子。</p><p><strong>语义相关性</strong>：我们使用 <code>jina-reranker-v2-base-multilingual</code> 来评估问题和每个 URL 的文本信息之间的语义相关性，这是<a href=\"https://jina.ai/reranker/#what_reranker\" rel=\"noreferrer\">一个经典的重排序问题</a>。每个 URL 的文本信息来自：</p><ul><li>来自 SERP API 结果的标题和摘要（<code>https://s.jina.ai/</code> 使用 <code>'X-Respond-With': 'no-content'</code>）</li><li>页面内 URL 的锚文本（<code>https://r.jina.ai</code> 使用 <code>'X-With-Links-Summary': 'all'</code>）</li></ul><p><strong>最后更新时间</strong>：某些 DeepSearch 查询对时间敏感，因此最近更新的 URL 比旧的更有价值。在不是像 Google 这样的主要搜索引擎的情况下，可靠地确定最后更新时间具有挑战性。我们实现了一个多层方法，结合以下信号并提供置信度评分的时间戳，在需要时优先考虑较新的内容。</p><ul><li>SERP API 过滤器（如 s.jina.ai 的 <code>tbs</code> 参数用于按时间筛选）</li><li>HTTP 头分析（Last-Modified、ETag）</li><li>元数据提取（meta 标签、Schema.org 时间戳）</li><li>内容模式识别（HTML 中可见的日期）</li><li>针对 WordPress、Drupal 和 Ghost 等平台的 CMS 特定指标</li></ul><p><strong>受限内容：</strong>社交媒体平台上的某些内容是受限的或者需要付费访问，如果没有登录或违反其服务条款，就无法合法获取这些内容。我们应该主动维护一个有问题的 URL 和主机名列表，降低它们的排名，避免浪费时间在无法访问的内容上。</p><p><strong>域名多样性：</strong>在某些情况下，权重最高的 URL 都来自相同的主机名，这可能会将 DeepSearch 困在局部最优解中，降低最终结果的质量。查看上面的示例，其中所有顶级 URL 都来自 StackOverflow。为了提高多样性，我们可以实现一个探索-利用方法，从每个主机名中选择权重最高的前 k 个 URL。</p><p>URL 排序的完整实现可以在我们的 Github 上找到。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/node-DeepResearch/blob/main/src/utils/url-tools.ts#L192\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">node-DeepResearch/src/utils/url-tools.ts at main · jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-description\">Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget) - jina-ai/node-DeepResearch</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-6.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/0921e515-0139-4540-bca4-52042b49328c-3\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-xml\">&lt;action-visit&gt;\n- Crawl and read full content from URLs, you can get the fulltext, last updated datetime etc of any URL.  \n- Must check URLs mentioned in &lt;question&gt; if any\n- Choose and visit relevant URLs below for more knowledge. higher weight suggests more relevant:\n&lt;url-list&gt;\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/loading\": \"Load - Hugging FaceThis saves time because instead of waiting for the Dataset builder download to time out, Datasets will look directly in the cache. Set the environment ...Some datasets may have more than one version based on Git tags, branches, or commits. Use the revision parameter to specify the dataset version you want to load ...\"\n  + weight: 0.20 \"https://huggingface.co/docs/datasets/en/index\": \"Datasets - Hugging Face🤗 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks. Load a dataset in a ...\"\n  + weight: 0.17 \"https://github.com/huggingface/datasets/issues/7175\": \"[FSTimeoutError] load_dataset · Issue #7175 · huggingface/datasetsWhen using load_dataset to load HuggingFaceM4/VQAv2, I am getting FSTimeoutError. Error TimeoutError: The above exception was the direct cause of the following ...\"\n  + weight: 0.15 \"https://github.com/huggingface/datasets/issues/6465\": \"`load_dataset` uses out-of-date cache instead of re-downloading a ...When a dataset is updated on the hub, using load_dataset will load the locally cached dataset instead of re-downloading the updated dataset.\"\n  + weight: 0.12 \"https://stackoverflow.com/questions/76923802/hugging-face-http-request-on-data-from-parquet-format-when-the-only-way-to-get-i\": \"Hugging face HTTP request on data from parquet format when the ...I've had to get the data from their data viewer using the parquet option. But when I try to run it, there is some sort of HTTP error. I've tried downloading ...\"\n&lt;/url-list&gt;\n&lt;/action-visit&gt;</code></pre><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">记得将 URL 权重放入 agent 的上下文中，并指示 LLM 遵守这些权重。</span></p></figcaption></figure><h2 id=\"conclusion\">结论</h2><p>自从我们的 DeepSearch 系统在 2025 年 2 月 2 日发布以来，我们发现了两个显著改善质量的实现细节。值得注意的是，这两个细节都以\"上下文\"方式使用多语言嵌入和重排序器——以远小于这些模型通常需要的预计算索引的规模运行。这解释了我们最初的疏忽。</p><p>这表明搜索技术的未来呈现出一种有趣的两极分化。考虑一个类似于 Kahneman 双系统理论的框架：</p><ul><li>快思维（grep、BM25、SQL）：快速、基于规则的模式匹配，计算需求最小。</li><li>慢思维（LLM）：具有深度上下文理解的全面推理，需要大量计算。</li><li>中思维（embeddings、rerankers）：处于模棱两可的状态？对于简单的模式匹配来说太\"先进\"/语义化，但又缺乏真正的推理能力。</li></ul><p>我们可能正在见证一种双分架构的流行，其中轻量级、高效的 SQL/BM25 处理初始内容检索，直接输入到强大的 LLM 中进行深度处理。这些 LLM 越来越多地整合了以前需要专门的中层模型的语义功能。中思维模型的剩余角色转向专门的上下文任务：过滤、去重和有限范围的操作，在这些操作中完全推理将是低效的。</p><p>尽管如此，选择关键片段和对 URL 进行排名仍然是直接影响 DeepSearch/DeepResearch 系统质量的基本组件。我们希望我们的见解能激发您在自己的实现中做出改进。</p><p>查询扩展仍然是另一个关键的质量决定因素。我们正在积极评估多种方法——从基本的基于提示的重写到小型语言模型和基于推理的方法。请期待我们即将发布的这方面的研究成果。敬请关注。</p>",
  "comment_id": "67d13ae9099ee70001bed48b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-03-12T08:42:33.000+01:00",
  "updated_at": "2025-03-12T14:20:43.000+01:00",
  "published_at": "2025-03-12T14:20:43.000+01:00",
  "custom_excerpt": "Nailing these two details transforms your DeepSearch from mid to GOAT: selecting the best snippets from lengthy webpages and ranking URLs before crawling.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/",
  "excerpt": "掌握这两个细节可以让你的 DeepSearch 从平庸变成神器：从冗长的网页中选择最佳片段，以及在爬取前对 URL 进行排序。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
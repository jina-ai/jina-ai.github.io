{
  "slug": "scaling-test-time-compute-for-embedding-models",
  "id": "675a84f80ce9930001b86f09",
  "uuid": "49f876f3-0d50-4555-8f9e-136473f720ac",
  "title": "嵌入模型的推理计算规模化",
  "html": "<p>自从 OpenAI 发布了 <a href=\"https://openai.com/o1/?ref=jina-ai-gmbh.ghost.io\">O1 模型</a>以来，AI 社区最热议的话题之一就是<strong>扩展测试时计算</strong>。这指的是在推理阶段（AI 模型对输入生成输出的阶段）而不是预训练阶段分配额外的计算资源。一个广为人知的例子是\"思维链\"多步推理，它使模型能够进行更广泛的内部深思，比如评估多个潜在答案、更深入的规划、在得出最终答案前的自我反思。这种策略提高了答案质量，特别是在复杂推理任务中。阿里巴巴最近发布的 <a href=\"https://huggingface.co/Qwen/QwQ-32B-Preview?ref=jina-ai-gmbh.ghost.io\">QwQ-32B-Preview</a> 模型就遵循了这种通过增加测试时计算来改进 AI 推理的趋势。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">在这种情况下，\"扩展\"主要指增加推理期间可用的计算能力（如处理能力或时间）。它不是指<b><strong style=\"white-space: pre-wrap;\">横向扩展</strong></b>（在多个系统间分配任务）或实现<b><strong style=\"white-space: pre-wrap;\">加速</strong></b>（减少处理时间）。</div></div><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1.mp4\" poster=\"https://img.spacergif.org/v1/900x432/0a/spacer.png\" width=\"900\" height=\"432\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2024/12/o1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:10</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">在使用 OpenAI 的 O1 模型时，用户可以清楚地注意到多步推理需要额外的时间，因为模型需要构建推理链来解决问题。</span></p></figcaption>\n        </figure><p>在 Jina AI，我们更关注 embeddings 和 rerankers 而不是 LLMs，所以对我们来说，从这个角度考虑扩展测试时计算是很自然的：<em>如何将\"思维链\"应用于 embedding 模型？</em>虽然这一开始可能不太直观，但本文探讨了一个新的视角，并展示了如何将扩展测试时计算应用于 <code>jina-clip</code> 来对分布外（OOD）图像进行分类——解决原本不可能完成的任务。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--14-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">我们的实验专注于宝可梦识别，这对 embedding 模型来说是一个有趣的挑战。虽然类似 CLIP 的模型在一般的图像-文本匹配方面表现出色，但在没有微调的情况下，它们可能在处理小众领域或分布外图像时表现不佳。通过给模型更多\"思考\"的时间，我们发现多目标分类——类似于\"思维链\"——可以提高准确性，而无需对 embedding 模型本身进行任何调整。</span></figcaption></figure><h2 id=\"case-study\">案例研究</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z?ref=jina-ai-gmbh.ghost.io#scrollTo=CJt5zwA9E2jB\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-15.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>我们的实验使用 <a href=\"https://huggingface.co/datasets/TheFusion21/PokemonCards?ref=jina-ai-gmbh.ghost.io\">TheFusion21/PokemonCards 数据集</a>进行宝可梦分类，该数据集包含数千张宝可梦卡牌图像。<strong>这是一个图像分类任务</strong>，输入是裁剪后的宝可梦卡牌艺术作品（移除了所有文字/描述），输出是从预定义名称集中选择正确的宝可梦名称。这个任务对 CLIP embedding 模型来说特别具有挑战性，因为：</p><ul><li>宝可梦的名称和视觉表现对模型来说属于小众、分布外的概念，使直接分类变得具有挑战性</li><li>每个宝可梦都有明显的视觉特征，可以分解为基本元素（形状、颜色、姿势），这些是 CLIP 可能更容易理解的</li><li>卡牌艺术作品提供了一致的视觉格式，同时通过不同的背景、姿势和艺术风格引入了复杂性</li><li>该任务需要同时整合多个视觉特征，类似于语言模型中的复杂推理链</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"835\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-5.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">我们裁剪宝可梦卡牌图像以移除所有文字信息（标题、页脚、描述），以防止因这些文字中出现宝可梦名称而导致的简单猜测。这些宝可梦的类别标签是 [</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol G</span></code><span style=\"white-space: pre-wrap;\">，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Weedle</span></code><span style=\"white-space: pre-wrap;\">，</span></figcaption></figure><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Azumarill</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Bulbasaur</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Venusaur</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Absol</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aggron</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Beedrill δ</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Alakazam</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Dratini</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Ampharos</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Arcanine</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Blaine's Moltres</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Aerodactyl</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Celebi & Venusaur-GX</span></code><span style=\"white-space: pre-wrap;\">、</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Caterpie</span></code><span style=\"white-space: pre-wrap;\">]</span></figcaption></figure><h3 id=\"baseline\">基准方法</h3><p>基准方法使用宝可梦卡牌插图和名称之间的简单直接比较。首先，我们裁剪每张宝可梦卡牌图片以移除所有文字信息（标题、页脚、描述），以防止 CLIP 模型因这些文本中出现的宝可梦名称而进行简单猜测。然后我们使用 <code>jina-clip-v1</code> 和 <code>jina-clip-v2</code> 模型对裁剪后的图片和宝可梦名称进行编码，获取各自的嵌入表示。分类是通过计算这些图像和文本嵌入之间的余弦相似度来完成的——每个图像都与具有最高相似度分数的名称相匹配。这在卡牌插图和宝可梦名称之间建立了直接的一对一匹配，而不需要任何额外的上下文或属性信息。下面的伪代码总结了基准方法。</p><pre><code class=\"language-python\"># Preprocessing\ncropped_images = [crop_artwork(img) for img in pokemon_cards]  # Remove text, keep only art\npokemon_names = [\"Absol\", \"Aerodactyl\", ...]  # Raw Pokemon names\n\n# Get embeddings using jina-clip-v1\nimage_embeddings = model.encode_image(cropped_images)\ntext_embeddings = model.encode_text(pokemon_names)\n\n# Classification by cosine similarity\nsimilarities = cosine_similarity(image_embeddings, text_embeddings)\npredicted_names = [pokemon_names[argmax(sim)] for sim in similarities]\n\n# Evaluate\naccuracy = mean(predicted_names == ground_truth_names)</code></pre><h3 id=\"chain-of-thoughts-for-classification\">分类的\"思维链\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--10-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--10-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--10-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>我们不是直接将图像与名称匹配，而是将宝可梦识别分解为一个结构化的视觉属性系统。我们定义了五个关键属性组：主要颜色（如\"白色\"、\"蓝色\"）、主要形态（如\"一只狼\"、\"一只有翅膀的爬行动物\"）、关键特征（如\"一只白色的角\"、\"大翅膀\"）、体型（如\"四脚着地的狼形\"、\"有翅膀且纤细\"）以及背景场景（如\"外太空\"、\"绿色森林\"）。</p><p>对于每个属性组，我们创建特定的文本提示（如\"这个宝可梦的身体主要是{}色的\"）与相关选项配对。然后我们使用模型计算图像与每个属性选项之间的相似度分数。这些分数通过 softmax 转换为概率，以获得更校准的置信度衡量。</p><p>完整的思维链（CoT）结构包含两个部分：<code>classification_groups</code> 描述提示组，以及 <code>pokemon_rules</code> 定义每个宝可梦应该匹配哪些属性选项。例如，Absol 应该在颜色上匹配\"白色\"，在形态上匹配\"狼形\"。完整的 CoT 如下所示（我们稍后会解释如何构建）：</p><pre><code class=\"language-python\">pokemon_system = {\n    \"classification_cot\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokémon's body is mainly {} in color.\",\n            \"options\": [\n                \"white\",    # Absol, Absol G\n                \"gray\",     # Aggron\n                \"brown\",    # Aerodactyl, Weedle, Beedrill δ\n                \"blue\",     # Azumarill\n                \"green\",    # Bulbasaur, Venusaur, Celebi&Venu, Caterpie\n                \"yellow\",   # Alakazam, Ampharos\n                \"red\",      # Blaine's Moltres\n                \"orange\",   # Arcanine\n                \"light blue\"# Dratini\n            ]\n        },\n        \"primary_form\": {\n            \"prompt\": \"It looks like {}.\",\n            \"options\": [\n                \"a wolf\",         # Absol, Absol G\n                \"an armored dinosaur\",  # Aggron\n                \"a winged reptile\",     # Aerodactyl\n                \"a rabbit-like creature\", # Azumarill\n                \"a toad-like creature\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"a caterpillar larva\",    # Weedle, Caterpie\n                \"a wasp-like insect\",     # Beedrill δ\n                \"a fox-like humanoid\",     # Alakazam\n                \"a sheep-like biped\",      # Ampharos\n                \"a dog-like beast\",        # Arcanine\n                \"a flaming bird\",          # Blaine's Moltres\n                \"a serpentine dragon\"      # Dratini\n            ]\n        },\n        \"key_trait\": {\n            \"prompt\": \"Its most notable feature is {}.\",\n            \"options\": [\n                \"a single white horn\", # Absol, Absol G\n                \"metal armor plates\",  # Aggron\n                \"large wings\",         # Aerodactyl, Beedrill δ\n                \"rabbit ears\",         # Azumarill\n                \"a green plant bulb\",  # Bulbasaur, Venusaur, Celebi&Venu\n                \"a small red spike\",   # Weedle\n                \"big green eyes\",      # Caterpie\n                \"a mustache and spoons\", # Alakazam\n                \"a glowing tail orb\",  # Ampharos\n                \"a fiery mane\",        # Arcanine\n                \"flaming wings\",       # Blaine's Moltres\n                \"a tiny white horn on head\" # Dratini\n            ]\n        },\n        \"body_shape\": {\n            \"prompt\": \"The body shape can be described as {}.\",\n            \"options\": [\n                \"wolf-like on four legs\",   # Absol, Absol G\n                \"bulky and armored\",        # Aggron\n                \"winged and slender\",       # Aerodactyl, Beedrill δ\n                \"round and plump\",          # Azumarill\n                \"sturdy and four-legged\",   # Bulbasaur, Venusaur, Celebi&Venu\n                \"long and worm-like\",       # Weedle, Caterpie\n                \"upright and humanoid\",     # Alakazam, Ampharos\n                \"furry and canine\",         # Arcanine\n                \"bird-like with flames\",    # Blaine's Moltres\n                \"serpentine\"                # Dratini\n            ]\n        },\n        \"background_scene\": {\n            \"prompt\": \"The background looks like {}.\",\n            \"options\": [\n                \"outer space\",      # Absol G, Beedrill δ\n                \"green forest\",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&Venu\n                \"a rocky battlefield\", # Absol, Aggron, Aerodactyl\n                \"a purple psychic room\", # Alakazam\n                \"a sunny field\",     # Ampharos\n                \"volcanic ground\",   # Arcanine\n                \"a red sky with embers\", # Blaine's Moltres\n                \"a calm blue lake\"   # Dratini\n            ]\n        }\n    },\n    \n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,      \n            \"body_shape\": 0,    \n            \"background_scene\": 2   \n        },\n        \"Absol G\": {\n            \"dominant_color\": 0,      \n            \"primary_form\": 0,   \n            \"key_trait\": 0,       \n            \"body_shape\": 0,     \n            \"background_scene\": 0    \n        },\n        // ...\n    }\n}\n</code></pre><p>最终的分类结合了这些属性概率——我们现在不是进行单一的相似度比较，而是进行多个结构化比较并聚合它们的概率，以做出更明智的决定。</p><pre><code class=\"language-python\"># Classification process\ndef classify_pokemon(image):\n   # Generate all text prompts\n   all_prompts = []\n   for group in classification_cot:\n       for option in group[\"options\"]:\n           prompt = group[\"prompt\"].format(option)\n           all_prompts.append(prompt)\n\n   # Get embeddings and similarities\n   image_embedding = model.encode_image(image)\n   text_embeddings = model.encode_text(all_prompts)\n   similarities = cosine_similarity(image_embedding, text_embeddings)\n\n   # Convert to probabilities per attribute group\n   probabilities = {}\n   for group_name, group_sims in group_similarities:\n       probabilities[group_name] = softmax(group_sims)\n\n   # Score each Pokemon based on matching attributes\n   scores = {}\n   for pokemon, rules in pokemon_rules.items():\n       score = 0\n       for group, target_idx in rules.items():\n           score += probabilities[group][target_idx]\n       scores[pokemon] = score\n\n   return max(scores, key=scores.get)</code></pre><h3 id=\"complexity-analysis\">复杂度分析</h3><p>假设我们要将一张图像分类到 <code>N</code> 个宝可梦名称中的一个。基准方法需要计算 <code>N</code> 个文本嵌入（每个宝可梦名称一个）。相比之下，我们的缩放测试时间计算方法需要计算 <code>Q</code> 个文本嵌入，其中</p><code>Q</code> 是所有问题中问题-选项组合的总数。两种方法都需要计算一个图像嵌入并执行最终分类步骤，因此我们在比较中排除这些共同操作。在本案例研究中，我们的 <code>N=13</code> 和 <code>Q=52</code>。</p><p>在极端情况下，当 <code>Q = N</code> 时，我们的方法实际上就会退化为基线方法。然而，有效扩展测试时计算的关键在于：</p><ul><li>构建精心设计的问题以增加 <code>Q</code></li><li>确保每个问题都能为最终答案提供独特、有信息量的线索</li><li>设计尽可能正交的问题以最大化它们的联合信息增益</li></ul><p>这种方法类似于\"二十个问题\"游戏，每个问题都经过战略性选择，以有效缩小可能的答案范围。</p><h3 id=\"evaluation\">评估</h3><p>我们的评估在 117 张测试图像上进行，涵盖 13 个不同的宝可梦类别。结果如下：</p><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th>方法</th>\n<th>jina-clip-v1</th>\n<th>jina-clip-v2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>基线</td>\n<td>31.36%</td>\n<td>16.10%</td>\n</tr>\n<tr>\n<td>CoT</td>\n<td>46.61%</td>\n<td>38.14%</td>\n</tr>\n<tr>\n<td>改进</td>\n<td>+15.25%</td>\n<td>+22.04%</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><p>可以看到，相同的 CoT 分类方法在这个不常见或分布外（OOD）任务上为两个模型都带来了显著的改进（分别提升了 15.25% 和 22.04%）。这也表明，一旦构建了 <code>pokemon_system</code>，<strong>同一个 CoT 系统可以有效地在不同模型之间迁移；并且不需要微调或后期训练。</strong></p><p>值得注意的是 v1 在宝可梦分类上相对较强的基线性能（31.36%）。这个模型是在<a href=\"https://www.youtube.com/watch?v=HsGyxVUN1SA&ref=jina-ai-gmbh.ghost.io\">包含宝可梦相关内容的 LAION-400M</a> 上训练的。相比之下，v2 是在 DFN-2B（抽样 400M 实例）上训练的，这是一个质量更高但经过更多过滤的数据集，可能排除了宝可梦相关内容，这解释了 v2 在这个特定任务上较低的基线性能（16.10%）。</p><h3 id=\"constructing-pokemonsystem-effectively\">有效构建 <code>pokemon_system</code></h3><p>我们的扩展测试时计算方法的有效性很大程度上取决于我们如何构建 <code>pokemon_system</code>。构建这个系统有不同的方法，从手动到完全自动化。</p><h4 id=\"manual-construction\">手动构建</h4><p>最直接的方法是手动分析宝可梦数据集并创建属性组、提示和规则。领域专家需要识别关键的视觉属性，如颜色、形态和独特特征。然后为每个属性编写自然语言提示，列举每个属性组的可能选项，并将每个宝可梦映射到其正确的属性选项。虽然这提供了高质量的规则，但它很耗时且不适合更大的 <code>N</code>。</p><h4 id=\"llm-assisted-construction\">LLM 辅助构建</h4><p>我们可以利用 LLM 来加速这个过程，通过提示它们生成分类系统。一个结构良好的提示应该要求基于视觉特征的属性组、自然语言提示模板、全面且互斥的选项，以及每个宝可梦的映射规则。LLM 可以快速生成初稿，尽管其输出可能需要验证。</p><pre><code class=\"language-txt\">I need help creating a structured system for Pokemon classification. For each Pokemon in this list: [Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...], create a classification system with:\n\n1. Classification groups that cover these visual attributes:\n   - Dominant color of the Pokemon\n   - What type of creature it appears to be (primary form)\n   - Its most distinctive visual feature\n   - Overall body shape\n   - What kind of background/environment it's typically shown in\n\n2. For each group:\n   - Create a natural language prompt template using \"{}\" for the option\n   - List all possible options that could apply to these Pokemon\n   - Make sure options are mutually exclusive and comprehensive\n\n3. Create rules that map each Pokemon to exactly one option per attribute group, using indices to reference the options\n\nPlease output this as a Python dictionary with two main components:\n- \"classification_groups\": containing prompts and options for each attribute\n- \"pokemon_rules\": mapping each Pokemon to its correct attribute indices\n\nExample format:\n{\n    \"classification_groups\": {\n        \"dominant_color\": {\n            \"prompt\": \"This Pokemon's body is mainly {} in color\",\n            \"options\": [\"white\", \"gray\", ...]\n        },\n        ...\n    },\n    \"pokemon_rules\": {\n        \"Absol\": {\n            \"dominant_color\": 0,  # index for \"white\"\n            ...\n        },\n        ...\n    }\n}</code></pre><p>一个更健壮的方法是将 LLM 生成与人工验证相结合。首先，LLM 生成一个初始系统。然后，人类专家审查和纠正属性分组、选项完整性和规则准确性。LLM 基于这些反馈完善系统，这个过程迭代进行直到达到令人满意的质量。这种方法在效率和准确性之间取得平衡。</p><h4 id=\"automated-construction-with-dspy\">使用 DSPy 自动化构建</h4><p>对于完全自动化的方法，我们可以使用 DSPy 来迭代优化 <code>pokemon_system</code>。该过程从一个简单的 <code>pokemon_system</code> 开始，可以是手动或由 LLM 编写的初始提示。每个版本都在留出集上进行评估，使用准确率作为 DSPy 的反馈信号。基于这个性能，生成优化的提示（即新版本的 <code>pokemon_system</code>）。这个循环重复进行直到收敛，在整个过程中，嵌入模型完全保持固定。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/banner--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/banner--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner--13-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">使用 DSPy 寻找最佳的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>pokemon_system</span></code><span style=\"white-space: pre-wrap;\"> CoT 设计；每个任务只需要进行一次调优过程。</span></figcaption></figure><h2 id=\"why-scale-test-time-compute-for-embedding-models\">为什么要扩展嵌入模型的测试时计算？</h2><p>因为扩展预训练最终会变得在经济上难以承受。</p><p>自 Jina 嵌入套件发布以来——包括 <code>jina-embeddings-v1</code>、<code>v2</code>、<code>v3</code>、<code>jina-clip-v1</code>、<code>v2</code> 和 <code>jina-ColBERT-v1</code>、<code>v2</code>——每次通过扩展预训练进行的模型升级都带来了更多成本。例如，我们的第一个模型 <code>jina-embeddings-v1</code> 于 2023 年 6 月发布，拥有 1.1 亿参数。当时训练它的成本在 5,000 到 10,000 美元之间，具体取决于如何计算。对于 <code>jina-embeddings-v3</code>，改进显著，但主要来自增加的资源投入。前沿模型的成本轨迹已经从数千美元上升到数万美元，对于较大的 AI 公司，甚至达到了数亿美元。虽然在预训练上投入更多的金钱、资源和数据会产生更好的模型，但边际收益最终会使进一步扩展在经济上变得不可持续。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/plot--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/plot--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/plot--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/plot--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/plot--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">嵌入模型的扩展定律。英语任务的平均 MTEB 性能与模型参数数量的关系图。每个点代表一个嵌入模型。趋势线代表所有模型，多语言模型以青色点表示。该图是通过从 MTEB 排行榜中选取前 100 个嵌入模型创建的，排除了没有大小信息的模型，通常是闭源或专有模型。明显的恶作剧提交也被过滤掉。</span></figcaption></figure><p>另一方面，现代嵌入模型正变得越来越强大：多语言、多任务、多模态，并且具有强大的零样本和遵循指令的性能。这种多功能性为算法改进和扩展测试时计算留下了很大空间。</p><p>问题转变为：用户愿意为他们深度关心的查询支付什么样的成本？如果容忍固定预训练模型更长的推理时间能显著提高结果质量，许多人会认为这是值得的。在我们看来，扩展嵌入模型的测试时计算还有很大的未开发潜力。这代表着一个转变，从仅仅在训练期间增加模型大小，转向在推理阶段增加计算努力以实现更好的性能。</p><h2 id=\"conclusion\">结论</h2><p>我们对 <code>jina-clip-v1/v2</code> 测试时计算的案例研究显示了几个关键发现：</p><ol><li>我们在不常见或分布外（OOD）数据上取得了更好的性能，而无需对嵌入进行任何微调或后期训练。</li><li>系统通过迭代细化相似度搜索和分类标准，做出了更细致的区分。</li><li>通过纳入动态提示调整和迭代推理，我们将嵌入模型的推理过程从单一查询转变为更复杂的思维链。</li></ol><p>这个案例研究仅仅触及了测试时计算可能性的表面。在算法上仍有很大的扩展空间。例如，我们可以开发方法来迭代选择最有效缩小答案空间的问题，类似于\"二十个问题\"游戏中的最优策略。通过扩展测试时计算，我们可以推动嵌入模型超越其当前限制，使它们能够处理曾经看似无法达到的更复杂、更微妙的任务。</p>",
  "comment_id": "675a84f80ce9930001b86f09",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-12T07:38:48.000+01:00",
  "updated_at": "2024-12-12T17:54:17.000+01:00",
  "published_at": "2024-12-12T17:54:17.000+01:00",
  "custom_excerpt": "Better results scale with compute—more on learning, more on search. A good pretrained model takes you far, but test-time compute takes you further. It's important to recognize this new paradigm of scaling test-time compute, even for embedding models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/scaling-test-time-compute-for-embedding-models/",
  "excerpt": "更好的结果会随着计算量的增加而提升——更多的学习，更多的搜索。一个好的预训练模型可以让你走得更远，但测试时的计算量可以让你走得更远。重要的是要认识到这种测试时计算量扩展的新范式，即使是对于 embedding 模型也是如此。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "二值化嵌入：所有的 AI 能力，仅 3.125% 的体积",
  "html": "<p>嵌入（Embeddings）已成为各种人工智能和自然语言处理应用的基石，它提供了一种将文本含义表示为高维向量的方法。然而，随着模型规模的增大和 AI 模型处理数据量的增加，传统嵌入的计算和存储需求也在不断攀升。二进制嵌入作为一种紧凑、高效的替代方案被引入，在大幅减少资源需求的同时仍能保持高性能。</p><p>二进制嵌入是一种降低这些资源需求的方法，它可以将嵌入向量的大小减少高达 96%（在 Jina Embeddings 的情况下为 96.875%）。用户可以在其 AI 应用中利用紧凑的二进制嵌入的优势，同时准确性损失最小。</p><h2 id=\"what-are-binary-embeddings\">什么是二进制嵌入？</h2><p>二进制嵌入是一种特殊的数据表示形式，它将传统的高维浮点向量转换为二进制向量。这不仅压缩了嵌入，而且还保留了向量几乎所有的完整性和实用性。这种技术的精髓在于即使在转换后也能保持数据点之间的语义和关系距离。<br><br>二进制嵌入背后的魔力在于量化，这是一种将高精度数字转换为低精度数字的方法。在 AI 建模中，这通常意味着将嵌入中的 32 位浮点数转换为更少位数的表示形式，比如 8 位整数。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">二值化是将所有标量值转换为 0 或 1 的过程，就像将彩色图像转换为只有黑白像素的图像。图片：神奈川沖浪裏（1831）by 葛飾（Hokusai）</span></figcaption></figure><p>二进制嵌入将这一过程推向极致，将每个值降为 0 或 1。将 32 位浮点数转换为二进制数字可以将嵌入向量的大小减少 32 倍，减少了 96.875%。由此产生的嵌入向量运算速度也大大提高。在某些微芯片上使用硬件加速可以使向量比较的速度提高远超 32 倍。</p><p>在这个过程中不可避免会损失一些信息，但当模型性能很好时，这种损失会被最小化。如果不同事物的非量化嵌入差异最大，那么二值化更有可能很好地保持这种差异。否则，正确解释嵌入可能会变得困难。</p><p>Jina Embeddings 模型经过专门训练，在这方面非常稳健，使其非常适合二值化。</p><p>这种紧凑的嵌入使新的 AI 应用成为可能，特别是在资源受限的场景中，如移动设备和对时间敏感的使用场景。</p><p>如下图所示，这些成本和计算时间的优势仅带来相对较小的性能损失。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10：使用</em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">归一化折损累积增益</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">计算前 10 个结果的分数。</em></i></figcaption></figure><p>对于 <code>jina-embeddings-v2-base-en</code>，二进制量化将检索准确率从 47.13% 降低到 42.05%，损失约 10%。对于 <code>jina-embeddings-v2-base-de</code>，这种损失仅为 4%，从 44.39% 降低到 42.65%。</p><p>Jina Embeddings 模型在生成二进制向量时表现如此出色，是因为它们经过训练可以创建更均匀的嵌入分布。这意味着与其他模型的嵌入相比，两个不同的嵌入在更多维度上的距离可能更远。这一特性确保了这些距离能更好地通过二进制形式表示。</p><h2 id=\"how-do-binary-embeddings-work\">二进制嵌入是如何工作的？</h2><p>让我们看看它是如何工作的，考虑三个嵌入：<em>A</em>、<em>B</em> 和 <em>C</em>。这三个都是完整的浮点向量，而不是二值化的向量。现在，假设从 <em>A</em> 到 <em>B</em> 的距离大于从 <em>B</em> 到 <em>C</em> 的距离。对于嵌入，我们通常使用<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">余弦距离</a>，所以：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>如果我们对 <em>A</em>、<em>B</em> 和 <em>C</em> 进行二值化，我们可以使用<a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">汉明距离</a>更有效地测量距离。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">立方体上的汉明距离。左图：A 到 B 的距离为 1。右图：B 到 C 的距离为 2。</span></figcaption></figure><p>让我们将 <em>A</em>、<em>B</em> 和 <em>C</em> 的二值化版本分别称为 <em>A<sub>bin</sub></em>、<em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em>。</p>\n<p>对于二进制向量，如果 <em>A<sub>bin</sub></em> 和 <em>B<sub>bin</sub></em> 之间的余弦距离大于 <em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em> 之间的距离，那么 <em>A<sub>bin</sub></em> 和 <em>B<sub>bin</sub></em> 之间的汉明距离大于或等于 <em>B<sub>bin</sub></em> 和 <em>C<sub>bin</sub></em> 之间的汉明距离。</p>\n<p>所以如果：</p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>那么对于汉明距离：</p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>理想情况下，当我们对嵌入进行二值化时，我们希望完整嵌入的相同关系在二进制嵌入中也成立。这意味着如果一个距离在浮点余弦中大于另一个距离，那么它们二值化等价的汉明距离也应该更大：</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>我们不能使这对所有嵌入三元组都成立，但我们可以使它对几乎所有的三元组都成立。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">蓝点对应完整浮点向量，红点对应它们的二值化等价物。</span></figcaption></figure><p>对于二进制向量，我们可以将每个维度视为存在（1）或不存在（0）。两个向量在非二进制形式中的距离越远，在任何一个维度上，一个具有正值而另一个具有负值的概率就越高。这意味着在二进制形式中，很可能会有更多的维度其中一个为零而另一个为一。这使它们在汉明距离上更远。</p><p>相反的情况适用于更接近的向量：非二进制向量越接近，在任何维度上两者都为零或都为一的概率就越高。这使它们在汉明距离上更近。</p><p>Jina Embeddings 模型之所以特别适合二值化，是因为我们使用负例挖掘和其他微调实践来训练它们，特别是增加不相似事物之间的距离并减少相似事物之间的距离。这使得嵌入更加稳健，对相似性和差异性更敏感，并使二进制嵌入之间的汉明距离与非二进制嵌入之间的余弦距离更加成比例。</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">使用 Jina AI 的二进制嵌入可以节省多少？</h2><p>采用 Jina AI 的二进制嵌入模型不仅可以降低时间敏感应用中的延迟，还能带来显著的成本效益，如下表所示：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>模型</th>\n<th>2.5 亿个<br/>嵌入的内存</th>\n<th>检索基准<br/>平均值</th>\n<th>AWS 估计价格<br/>（使用 x2gb 实例，<br/>每 GB/月 $3.8）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32 位浮点嵌入</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>二进制嵌入</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>这种 95% 以上的节省仅伴随着约 10% 的检索精度降低。</p><p>相比使用 <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">OpenAI 的 Ada 2 模型</a>或 <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Cohere 的 Embed v3</a> 产生的二值化向量，这种节省更为显著，这两个模型产生的输出嵌入维度都在 1024 或更高。Jina AI 的嵌入仅有 768 维，即使在量化之前也比其他模型更小，但仍保持着相当的精度。</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">二值向量可以节省内存、计算时间、传输带宽和磁盘存储，在多个方面带来经济效益</strong></b>。</div></div><p>这些节省也体现在环境方面，使用更少的稀有材料和能源。</p><h2 id=\"get-started\">开始使用</h2><p>要使用 <a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a> 获取二值嵌入，只需在你的 API 调用中添加 <code>encoding_type</code> 参数，将值设为 <code>binary</code> 可获取编码为有符号整数的二值化嵌入，或设为 <code>ubinary</code> 获取无符号整数。</p><h3 id=\"directly-access-jina-embedding-api\">直接访问 Jina Embedding API</h3><p>使用 <code>curl</code>：</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>或通过 Python <code>requests</code> API：</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>通过上述 Python <code>request</code>，检查 <code>response.json()</code> 将得到以下响应：</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>这是两个以 96 个 8 位有符号整数存储的二值嵌入向量。要将它们解包成 768 个 0 和 1，你需要使用 <code>numpy</code> 库：</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>结果是一个只包含 0 和 1 的 768 维向量：</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">在 Qdrant 中使用二值量化</h3><p>你也可以使用 <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">Qdrant 的集成库</a>将二值嵌入直接存入你的 Qdrant 向量存储。由于 Qdrant 内部已实现了 <code>BinaryQuantization</code>，你可以将其作为整个向量集合的预设配置，使其在不改变代码的情况下就能检索和存储二值向量。</p><p>具体示例代码如下：</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Provide Jina API key and choose one of the available models.\n# You can get a free trial key here: https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # or \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n# Get embeddings from the API\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Index the embeddings into Qdrant\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>要配置搜索，你应该使用 <code>oversampling</code> 和 <code>rescore</code> 参数：</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">使用 LlamaIndex</h3><p>要在 LlamaIndex 中使用 Jina 二进制嵌入，在实例化 <code>JinaEmbedding</code> 对象时将 <code>encoding_queries</code> 参数设置为 <code>binary</code>：</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# You can get a free trial key from https://jina.ai/embeddings/\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">支持二进制嵌入的其他向量数据库</h3><p>以下向量数据库原生支持二进制向量：</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB by DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">示例</h2><p>为了展示二进制嵌入的实际应用，我们从 <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a> 选取了一些摘要，并使用 <code>jina-embeddings-v2-base-en</code> 获取了它们的 32 位浮点和二进制向量。然后我们将它们与示例查询\"3D segmentation\"的嵌入进行了比较。</p><p>从下表可以看出，前三个答案是相同的，前五个中有四个匹配。使用二进制向量产生了几乎相同的顶部匹配结果。</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>Rank</th>\n<th>Hamming<br/>dist.</th>\n<th>Matching Text</th>\n<th>Cosine</th>\n<th>Matching text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "对于像 AI 模型这样强大且不精确的东西来说，32 位精度实在太多了。所以我们去掉了其中的 31 位！二进制嵌入更小、更快且性能出色。",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "使用 jina-reranker-m0 对多模态文档进行公平评分",
  "html": "<p>假设你正在构建一个体育新闻搜索系统。用户搜索“网球运动员庆祝锦标赛胜利”，你需要从数据库中找到最相关的文章。每篇文章都包含文本标题和图像——这是现代体育报道的典型特征。</p><p>你的系统需要接收一个<strong>文本查询 (text query)</strong>，并返回一个<strong>排序后的、最相关的多模态文档列表 (ranked list of the most relevant multimodal documents)</strong>，这些文档来自你的语料库。听起来很简单，但存在一个根本性的问题，它破坏了所有显而易见的方法。</p><p>当尝试对这些文档进行排序时，会发生以下情况。你的 向量模型 (Embeddings)（比如 <code>jina-clip-v2</code>）会产生如下相似度分数：</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>文章</th>\n            <th>内容类型</th>\n            <th>描述</th>\n            <th>相似度分数</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>文本</td>\n            <td>诺瓦克·德约科维奇在澳大利亚网球公开赛决赛中直落三盘获胜</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>图像</td>\n            <td>[球员手持奖杯并微笑的照片]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>文本</td>\n            <td>天气延误影响室外锦标赛的赛程安排</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>图像</td>\n            <td>[网球运动员跳跃和庆祝的照片]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>哪篇文章更相关？文章 A 的文本得分很高，但图像得分很低。文章 B 的文本得分很低，但图像得分较高。根本的挑战在于，<strong>你无法比较 0.72（文本）和 0.54（图像）</strong>，因为这些相似度分数存在于完全不同的尺度上。</p><h2 id=\"when-trivial-solutions-fail\">当简单的解决方案失败时</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>由于模态差异 (modality gap)</strong>，在 <code>jina-clip-v2</code> 或几乎所有其他类 CLIP 模型中，你可能尝试的任何显而易见的方法都行不通。如果你只使用较高的分数，你会遇到这样的情况：文本分数聚集在 0.2-0.8 附近，而图像分数聚集在 0.4-0.6 附近。这意味着平庸的文本匹配（0.6）总是会胜过优秀的图像匹配（0.5）。</p><p>对分数进行平均也无济于事。计算 (0.7 + 0.3)/2 = 0.5 会得到一个数字，但它实际上意味着什么？你正在平均从根本上毫无意义的量。同样，任何固定的加权方案都是武断的——有时文本更重要，有时图像更重要，这完全取决于具体的 查询 (Prompt) 和文档。</p><p>即使首先对分数进行归一化也无法解决核心问题。你仍然试图组合从根本上不同的相似性度量，这些度量捕捉了相关性的不同方面。</p><h2 id=\"what-actually-happens\">实际发生了什么</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>为了更好地了解我们正在处理的内容，这是一个来自 <a href=\"https://arxiv.org/abs/2305.13631\">EDIS 数据集</a>的示例文档，显示了图像（一场德国足球比赛）和标题（<code>One More Field Where the Content Trails Germany</code>）。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 1：包含图像和文本内容的多模态文档示例。由于我们有两种模态，对于任何给定的 查询 (Prompt)，现在都有</span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">两个</em></i><span style=\"white-space: pre-wrap;\">语义差距（ 查询 (Prompt) 与文本之间，以及 查询 (Prompt) 与图像之间）。为了获得最佳结果，我们应该搜索文档的文本内容还是图像内容？</span></figcaption></figure><p>总的来说，<code>jina-clip-v2</code> 在比较 查询 (Prompt) 到文本的相似度时，比 查询 (Prompt) 到图像的相似度表现出更高的相似度，部分原因是模型的训练方式，部分原因是数据集本身：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 2：使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">， 查询 (Prompt) 到图像（红色）和 查询 (Prompt) 到文本（蓝色）之间的相似度分数。</span></figcaption></figure><p>因此，根据文档的文本而不是图像来检索文档似乎是合乎逻辑的。而且，正如我们在下面的图表中看到的，当我们比较文本 查询 (Prompt) <code>... for undocumented immigrants helping to establish legal status in the United States</code> 与语料库的文本内容时，我们获得了更好的结果。事实上，按图像搜索根本无法检索到真实文档（以黄色突出显示）：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 3：当使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> 的 查询 (Prompt) 到文本的检索，且 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> 为 3 时，只能通过这种方式检索到真实文档（以黄色边框突出显示）的示例。</span></figcaption></figure><p>但不要被愚弄了。尽管 查询 (Prompt) 到文本显示出更高的相似度分数，但 查询 (Prompt) 到文本和 查询 (Prompt) 到图像的相似度分数是<em>不可</em>比较的。当使用 <code>jina-clip-v2</code> 从 EDIS 数据集中检索 32 个文档时，我们可以通过查看 recall@10 来看到这一点。显然，查询到<em>图像</em>的召回率更高：</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Query-to-text</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Query-to-image</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>我们可以在下面看到：如果我们使用数据集中的一个 查询 (Prompt)，<code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>，我们只能通过其图像内容来检索真实文档。按其文本内容搜索不会返回任何匹配项：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 4：当使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> 的 查询 (Prompt) 到图像的检索，且 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> 为 3 时，只能通过这种方式检索到真实文档（以黄色边框突出显示）的示例。</span></figcaption></figure><p>因此，如果相似度得分意味着我们应该从文本中检索文档，而召回率意味着我们应该从图像中检索文档，那么我们应该选择哪个呢？当然，图 3 和图 4 表明没有明显的赢家。哪种模态<em>真正</em>呈现了我们的查询和我们正在寻找的文档之间最接近的匹配？如果我们要合并来自<em>query-to-text</em>和<em>query-to-image</em>检索的候选结果，如果我们甚至无法比较分数，我们如何才能有意义地选择最佳匹配？显然，仅仅使用 <code>jina-clip-v2</code> 是不够的。我们需要将另一个模型加入其中。</p><h2 id=\"a-simple-two-stage-pipeline\">一个简单的两阶段流程</h2><p>在 2025 年 4 月，我们发布了 <code>jina-reranker-m0</code>，这是一个用于检索视觉文档的多语言多模态重排器 (Reranker)。我们可以在下面看到它较窄的模态差距，其中 <code>jina-reranker-m0</code> 显示出可比较的 query-to-text 和 query-to-image 相似度得分，这与 <code>jina-clip-v2</code> 显示的更大差距形成对比：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 6：与 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> 相比，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> 在 query-to-image（红色）和 query-to-text（蓝色）相似度得分之间显示出更小的差异。</span></figcaption></figure><p>考虑到这一点，我们可以在从 <code>jina-clip-v2</code> 检索到初始结果后，使用 <code>jina-reranker-m0</code> 进行检索链中的第二轮处理：</p><p><strong>阶段 1：从两种模态检索候选结果</strong></p><ul><li>使用 <code>jina-clip-v2</code> 通过文本搜索获取 16 个文档，通过图像搜索获取 16 个文档</li><li>接受我们还无法比较分数</li></ul><p><strong>阶段 2：统一重排</strong></p><ul><li>将每个（查询 + 完整文档）对输入到 <code>jina-reranker-m0</code> 中</li><li>重排器 (Reranker) 同时处理文本和图像</li><li>输出：统一尺度上的单个相关性得分</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 5：使用 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> 和 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> 索引多模态文档和两阶段多模态检索过程。</span></figcaption></figure><p>我们扩展了表 1 中的实验，现在使用 <code>jina-clip-v2</code> 从语料库中检索文档，然后使用 <code>jina-reranker-m0</code> 对它们进行重排：</p><ol><li>通过 query-to-text 检索 32 个文档，然后根据 query-to-text 得分进行重排。</li><li>通过 query-to-image 检索 32 个文档，然后根据 query-to-image 得分进行重排。</li><li>通过 query-to-text 检索 16 个文档，通过 query-to-image 检索 16 个文档。根据查询模态，基于 query-to-text 或 query-to-image 得分进行重排。</li><li>通过 query-to-text 检索 16 个文档，通过 query-to-image 检索 16 个文档。基于每个文档的平均 query-to-text 和 query-to-image 得分进行重排，给出最终得分（query-to-text + query-to-image）/2。</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">请注意，我们正在测量 EDIS 上的零样本性能。我们没有使用该数据集对 <code>jina-clip-v2</code> 或 <code>jina-reranker-m0</code> 进行微调。</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Description</th>\n      <th>Recall@10 - with jina-clip-v2</th>\n      <th>Recall@10 - with jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 docs: query-to-text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 docs: query-to-image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image<br>Combined average reranker scores</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">实验 1、3 和 4 都显示了使用 <code>jina-clip-v2</code> 的相同 recall@10 结果，这是因为 query-to-text 得分高于 query-to-image 得分。因此，前十个结果主要由通过文本检索的文档主导。</div></div><p>正如我们所看到的，通过使用 <code>jina-reranker-m0</code> 执行第二轮处理，召回率全面提高，而与模态无关。但是，<strong>当我们结合从检索到的文档中提取的文本和图像内容时，我们看到了最大的增长</strong>，达到了 36.24 的 recall@10。一个可视化示例表明，无论是搜索文本还是图像内容，<code>jina-reranker-m0</code> 始终将真实文档排在第一位：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">图 7：示例查询（左侧）和每种重排 (Reranking) 方法的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> 为 1 的结果（右侧的四列），表明结合图像和文本相似度得分始终将真实文档排在第一位。</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">虽然图 3 和图 4 显示了不同检索方法的 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> 为 3，但出于空间原因，图 7 仅显示了每个查询的 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> 为 1。</div></div><h2 id=\"conclusions\">结论</h2><p>这种简单的两阶段方法带来了 62% 的召回率提升，因为该系统最终利用了人类自然而然的行为：同时考虑我们阅读的内容和我们看到的内容来确定相关性。这一经验也适用于搜索之外的领域：在处理多模态 AI 系统时，将模态分开处理的单次处理方法总是会遇到这种评分不兼容的问题。先广泛检索然后智能排序的两阶段架构正变得至关重要。通过我们的 API 或在 AWS、GCP 和 Azure 上试用 <code>jina-reranker-m0</code>。</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "文本相似度：0.7。图像相似度：0.5。哪个文档更相关？你根本无法判断——而这正是打破多模态搜索的核心问题。我们用统一重排 (unified reranking) 解决它。",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "finding-optimal-breakpoints-in-long-documents-using-small-language-models",
  "id": "67126986708dbe00019249f2",
  "uuid": "b7e55a5d-f267-4a4a-b861-27221c0f3827",
  "title": "使用小型语言模型在长文档中寻找最优分隔点",
  "html": "<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">这是我们分块系列的第三部分。<b><strong style=\"white-space: pre-wrap;\">建议阅读顺序：</strong></b><a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">第一部分</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、</strong></b><a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">第二部分</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、</strong></b><a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">研究论文</strong></b></a><b><strong style=\"white-space: pre-wrap;\">、第三部分。</strong></b></div></div><p>在我们之前的文章中，我们探讨了分块的挑战，并<a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">介绍了后期分块的概念</a>，这有助于减少嵌入块时的上下文丢失。在本文中，我们将关注另一个挑战：找到最佳断点。虽然<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io#late-chunking-is-resilient-to-poor-boundary-cues\" rel=\"noreferrer\">我们的后期分块策略已经证明对不良边界具有很强的适应性</a>，但这并不意味着我们可以忽视它们——它们对人类和 LLM 的可读性仍然很重要。我们的观点是：在确定断点时，我们现在可以完全专注于可读性，而不必担心语义或上下文丢失。后期分块可以处理好的和不好的断点，所以可读性成为你的主要考虑因素。</p><p>基于这一点，我们训练了三个专门用于在保持语义连贯性的同时对长文档进行分段并处理复杂内容结构的小型语言模型。它们是：</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>simple-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">，基于文档的结构元素对文本进行分段。</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-cot-topic-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>topic-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">，基于文本中的主题进行分段。</span></p></figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/text-seg-lm-qwen2-0.5b-summary-chunking · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/text-seg-lm-qwen2-0.5b-summary-chunking.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>summary-qwen-0.5</span></code><span style=\"white-space: pre-wrap;\">，为每个段落生成摘要。</span></p></figcaption></figure><p>在本文中，我们将讨论为什么要开发这个模型，我们如何处理它的三个变体，以及它们与 <a href=\"https://www.notion.so/Advancing-Segmentation-Strategies-in-RAG-with-a-Custom-Small-Language-Model-Restructure-638b84ae461d412eb6889cfa7f54cce1?pvs=21&ref=jina-ai-gmbh.ghost.io\">Jina AI 的 Segmenter API</a> 的基准对比。最后，我们将分享我们学到的经验和对未来的一些思考。</p><h2 id=\"segmentation-problem\">分段问题</h2><p>分段是 RAG 系统中的核心要素。我们如何将长文档分割成连贯、可管理的段落直接影响检索和生成步骤的质量，从答案相关性到摘要质量都会受到影响。传统的分段方法虽然产生了不错的结果，但也存在局限性。</p><p>引用我们<a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\">之前的文章</a>：</p><blockquote>在对长文档进行分段时，一个关键挑战是决定在哪里创建段落。这可以使用固定的 token 长度、固定数量的句子，或更高级的方法如正则表达式和语义分段模型来完成。建立准确的段落边界至关重要，因为它不仅能提高搜索结果的可读性，还能确保在 RAG 系统中提供给 LLM 的段落既准确又充分。</blockquote><p>虽然后期分块提高了检索性能，但<strong>在 RAG 应用中，确保每个段落本身都有意义，而不仅仅是随机文本块，这一点至关重要。</strong>LLM 依赖连贯、结构良好的数据来生成准确的响应。如果段落不完整或缺乏意义，尽管有后期分块的好处，LLM 也可能在理解上下文和准确性方面遇到困难，影响整体性能。简而言之，无论是否使用后期分块，拥有一个可靠的分段策略对构建有效的 RAG 系统都是至关重要的（正如你将在后面的基准测试部分看到的）。</p><p>传统的分段方法，无论是在简单的边界（如换行或句子）处分割内容，还是使用严格的基于 token 的规则，都面临着相同的局限性。这两种方法都未能考虑语义边界，并且在处理模糊主题时遇到困难，导致段落零散。为了解决这些挑战，我们开发并训练了一个专门用于分段的小型语言模型，旨在捕捉主题转换并保持连贯性，同时保持效率和在各种任务中的适应性。</p><h2 id=\"why-small-language-model\">为什么选择小型语言模型</h2><p>我们开发了一个小型语言模型（SLM）来解决我们在传统分段技术中遇到的特定限制，特别是在处理代码片段和其他复杂结构（如表格、列表和公式）时。在传统方法中，这些方法通常依赖于 token 计数或严格的结构规则，很难保持语义连贯内容的完整性。例如，代码片段经常被分割成多个部分，破坏了它们的上下文，使下游系统更难准确理解或检索它们。</p><p>通过训练专门的 SLM，我们旨在创建一个能够智能识别和保持这些有意义边界的模型，确保相关元素保持在一起。这不仅提高了 RAG 系统中的检索质量，还增强了下游任务（如摘要和问答）的效果，这些任务需要维持连贯和上下文相关的段落。SLM 方法提供了一个更具适应性、针对特定任务的解决方案，这是传统分段方法及其严格边界无法提供的。</p><h2 id=\"training-slms-three-approaches\">训练 SLM：三种方法</h2><p>我们训练了三个版本的 SLM：</p><ul><li><code>simple-qwen-0.5</code> 是最直接的模型，旨在基于文档的结构元素识别边界。它的简单性使其成为基本分段需求的高效解决方案。</li><li><code>topic-qwen-0.5</code> 受思维链推理启发，通过识别文本中的主题（如\"第二次世界大战的开始\"）并使用这些主题来定义段落边界，进一步推进了分段。这个模型确保每个段落在主题上保持连贯，使其特别适合处理复杂的多主题文档。初步测试表明，它在以接近人类直觉的方式分段内容方面表现出色。</li><li><code>summary-qwen-0.5</code> 不仅识别文本边界，还为每个段落生成摘要。在 RAG 应用中，段落摘要非常有利，特别是对于长文档问答等任务，尽管这需要在训练时使用更多数据作为代价。</li></ul><p>所有模型都只返回<em>段落头部</em>—每个段落的截断版本。模型不是生成完整的段落，而是输出关键点或子主题，这通过专注于语义转换而不是简单复制输入内容来提高边界检测和连贯性。在检索段落时，文档文本基于这些段落头部进行分割，并相应地重建完整段落。</p><h3 id=\"dataset\">数据集</h3><p>我们使用了 <a href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\">wiki727k</a> 数据集，这是一个从维基百科文章中提取的大规模结构化文本片段集合。它包含超过 727,000 个文本部分，每个部分代表维基百科文章的一个独特部分，如引言、章节或子章节。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/koomri/text-segmentation?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - koomri/text-segmentation：论文实现：文本分段作为监督学习任务</div><div class=\"kg-bookmark-description\">论文实现：Text Segmentation as a Supervised Learning Task - koomri/text-segmentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">koomri</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/a0a75db005774d424366f3fa2d4c70930927a0b2d8032ef3c04cb0f3beebcb8e/koomri/text-segmentation\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"data-augmentation\">数据增强</h3><p>为每个模型变体生成训练对时，我们使用 GPT-4 来增强我们的数据。对于训练数据集中的每篇文章，我们发送如下提示：</p><pre><code class=\"language-python\">f\"\"\"\nGenerate a five to ten words topic and a one sentence summary for this chunk of text.\n```\n{text}\n```\nMake sure the topic is concise and the summary covers the main topic as much as possible.\n\nPlease respond in the following format:\n```\nTopic: ...\nSummary: ...\n```\n\nDirectly respond with the required topic and summary, do not include any other details, and do not surround your response with quotes, backticks or other separators.\n   \"\"\".strip()</code></pre><p>我们使用简单的分割方法从每篇文章中生成章节，在 <code>\\\\n\\\\n\\\\n</code> 处分割，然后在 <code>\\\\n\\\\n</code> 处进行子分割，得到以下内容（本例中是关于通用网关接口的文章）：</p><pre><code>[\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n</code></pre><p>然后我们生成了一个包含章节、主题和摘要的 JSON 结构：</p><pre><code>{\n  \"sections\": [\n    [\n      \"In computing, Common Gateway Interface (CGI) offers a standard protocol for web servers to execute programs that execute like Console applications (also called Command-line interface programs) running on a server that generates web pages dynamically.\",\n      \"Such programs are known as \\\\\"CGI scripts\\\\\" or simply as \\\\\"CGIs\\\\\".\",\n      \"The specifics of how the script is executed by the server are determined by the server.\",\n      \"In the common case, a CGI script executes at the time a request is made and generates HTML.\"\n    ],\n    [\n      \"In 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list; however, NCSA no longer hosts the specification.\",\n      \"The other Web server developers adopted it, and it has been a standard for Web servers ever since.\",\n      \"A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.\",\n      \"This work resulted in RFC 3875, which specified CGI Version 1.1.\",\n      \"Specifically mentioned in the RFC are the following contributors: \\\\n1. Alice Johnson\\\\n2. Bob Smith\\\\n3. Carol White\\\\n4. David Nguyen\\\\n5. Eva Brown\\\\n6. Frank Lee\\\\n7. Grace Kim\\\\n8. Henry Carter\\\\n9. Ingrid Martinez\\\\n10. Jack Wilson\",\n      \"Historically CGI scripts were often written using the C language.\",\n      \"RFC 3875 \\\\\"The Common Gateway Interface (CGI)\\\\\" partially defines CGI using C, as in saying that environment variables \\\\\"are accessed by the C library routine getenv() or variable environ\\\\\".\"\n    ],\n    [\n      \"CGI is often used to process inputs information from the user and produce the appropriate output.\",\n      \"An example of a CGI program is one implementing a Wiki.\",\n      \"The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result.\",\n      \"The web server receives the input from the CGI and transmits it to the user agent.\",\n      \"If the \\\\\"Edit this page\\\\\" link is clicked, the CGI populates an HTML textarea or other editing control with the page's contents, and saves it back to the server when the user submits the form in it.\\\\n\",\n      \"\\\\n# CGI script to handle editing a page\\\\ndef handle_edit_request(page_content):\\\\n    html_form = f'''\\\\n    &lt;html&gt;\\\\n    &lt;body&gt;\\\\n        &lt;form action=\\\\\"/save_page\\\\\" method=\\\\\"post\\\\\"&gt;\\\\n            &lt;textarea name=\\\\\"page_content\\\\\" rows=\\\\\"20\\\\\" cols=\\\\\"80\\\\\"&gt;\\\\n            {page_content}\\\\n            &lt;/textarea&gt;\\\\n            &lt;br&gt;\\\\n            &lt;input type=\\\\\"submit\\\\\" value=\\\\\"Save\\\\\"&gt;\\\\n        &lt;/form&gt;\\\\n    &lt;/body&gt;\\\\n    &lt;/html&gt;\\\\n    '''\\\\n    return html_form\\\\n\\\\n# Example usage\\\\npage_content = \\\\\"Existing content of the page.\\\\\"\\\\nhtml_output = handle_edit_request(page_content)\\\\nprint(\\\\\"Generated HTML form:\\\\\")\\\\nprint(html_output)\\\\n\\\\ndef save_page(page_content):\\\\n    with open(\\\\\"page_content.txt\\\\\", \\\\\"w\\\\\") as file:\\\\n        file.write(page_content)\\\\n    print(\\\\\"Page content saved.\\\\\")\\\\n\\\\n# Simulating form submission\\\\nsubmitted_content = \\\\\"Updated content of the page.\\\\\"\\\\nsave_page(submitted_content)\"\n    ],\n    [\n      \"Calling a command generally means the invocation of a newly created process on the server.\",\n      \"Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.\",\n      \"If the command is called often, the resulting workload can quickly overwhelm the server.\",\n      \"The overhead involved in process creation can be reduced by techniques such as FastCGI that \\\\\"prefork\\\\\" interpreter processes, or by running the application code entirely within the web server, using extension modules such as mod_perl or mod_php.\",\n      \"Another way to reduce the overhead is to use precompiled CGI programs, e.g.\",\n      \"by writing them in languages such as C or C++, rather than interpreted or compiled-on-the-fly languages such as Perl or PHP, or by implementing the page generating software as a custom webserver module.\",\n      \"Several approaches can be adopted for remedying this: \\\\n1. Implementing stricter regulations\\\\n2. Providing better education and training\\\\n3. Enhancing technology and infrastructure\\\\n4. Increasing funding and resources\\\\n5. Promoting collaboration and partnerships\\\\n6. Conducting regular audits and assessments\",\n      \"The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.\"\n    ]\n  ],\n  \"topics\": [\n    \"Web 服务器中的通用网关接口\",\n    \"CGI 的历史和标准化\",\n    \"用于编辑网页的 CGI 脚本\",\n    \"降低命令调用时的 Web 服务器开销\"\n  ],\n  \"summaries\": [\n    \"CGI 为 Web 服务器提供了一个运行生成动态网页程序的协议。\",\n    \"NCSA 于 1993 年首次定义了 CGI，随后它成为 Web 服务器的标准，并在 Ken Coar 主持下通过 RFC 3875 正式确立。\",\n    \"本文描述了 CGI 脚本如何通过 HTML 表单处理网页内容的编辑和保存。\",\n    \"文本讨论了减少频繁命令调用服务器开销的技术，包括进程预分叉、使用预编译的 CGI 程序和实现自定义 Web 服务器模块。\"\n  ]\n}\n</code></pre><p>我们还通过打乱数据、添加随机字符/单词/字母、随机删除标点符号，以及始终删除换行符的方式来增加噪声。</p><p>这些方法可以在一定程度上帮助开发出一个好的模型，但效果有限。为了真正充分发挥模型的潜力，我们需要模型能创建连贯的文本块而不破坏代码片段。为此，我们使用 GPT-4o 生成的代码、公式和列表来增强数据集。</p><h3 id=\"the-training-setup\">训练设置</h3><p>对于模型训练，我们实施了以下设置：</p><ul><li><strong>框架</strong>：我们使用了 Hugging Face 的 <code>transformers</code> 库，并与 <code>Unsloth</code> 集成以进行模型优化。这对于优化内存使用和加速训练至关重要，使我们能够用大型数据集有效地训练小型模型。</li><li><strong>优化器和调度器</strong>：我们使用了带有线性学习率调度和预热步骤的 AdamW 优化器，这使我们能够在初始训练阶段稳定训练过程。</li><li><strong>实验追踪</strong>：我们使用 <a href=\"https://wandb.ai/?ref=jina-ai-gmbh.ghost.io\">Weights & Biases</a> 追踪所有训练实验，并记录了关键指标，如训练和验证损失、学习率变化和整体模型性能。这种实时追踪为我们提供了模型进展的洞察，使我们能够在必要时快速调整以优化学习效果。</li></ul><h3 id=\"the-training-itself\">训练过程</h3><p>使用 <a href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=jina-ai-gmbh.ghost.io\"><code>qwen2-0.5b-instruct</code></a> 作为基础模型，我们用 Unsloth 训练了三个 SLM 变体，每个变体都针对不同的分段策略。对于我们的样本，我们使用了训练对，包括来自 wiki727k 的文章文本和根据所训练的模型生成的相应的 <code>sections</code>、<code>topics</code> 或 <code>summaries</code>（在上面的\"数据增强\"部分提到）。</p><ul><li><code><strong>simple-qwen-0.5</strong></code>：我们用 10,000 个样本训练了 <code>simple-qwen-0.5</code>，共 5,000 步，实现了快速收敛并有效检测文本连贯段落之间的边界。训练损失为 0.16。</li><li><code><strong>topic-qwen-0.5</strong></code>：与 <code>simple-qwen-0.5</code> 类似，我们用 10,000 个样本训练了 <code>topic-qwen-0.5</code>，共 5,000 步，训练损失达到 0.45。</li><li><code><strong>summary-qwen-0.5</strong></code>：我们用 30,000 个样本训练了 <code>summary-qwen-0.5</code>，共 15,000 步。这个模型显示出前景，但训练期间损失较高（0.81），表明需要更多数据（大约是原始样本数量的两倍）才能发挥其全部潜力。</li></ul><h2 id=\"the-segments-themselves\">分段结果展示</h2><p>以下是每种分段策略的三个连续段落示例，以及 Jina 的 Segmenter API。为了生成这些段落，我们首先使用 <a href=\"https://jina.ai/reader/?ref=jina-ai-gmbh.ghost.io\">Jina Reader</a> 从 Jina AI 博客抓取一篇文章的纯文本（包括所有页面数据，如页眉、页脚等），然后将其传递给每种分段方法。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/can-embedding-reranker-models-compare-numbers/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Can Embedding/Reranker Models Compare Numbers?</div><div class=\"kg-bookmark-description\">A lot of LLMs can't figure out that 9.11 is actually smaller than 9.9. Can our embedding and reranker models do any better?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/07/number-heading.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h3 id=\"jina-segmenter-api\">Jina Segmenter API</h3><p>Jina Segmenter API 采用了非常细致的分段方法，通过 <code>\\n</code>、<code>\\t</code> 等字符进行分段，将文本分成通常非常小的片段。仅看前三个段落，它从网站的导航栏提取了 <code>search\\\\n</code>、<code>notifications\\\\n</code> 和 <code>NEWS\\\\n</code>，但没有提取到任何与文章内容相关的内容：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png\" class=\"kg-image\" alt=\"Minimalist navigation bar with &quot;NEWS&quot;, &quot;PRODUCTS&quot;, and &quot;COMPANY&quot; text on a black background, accented by colorful stripes to \" loading=\"lazy\" width=\"1164\" height=\"68\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-1.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>再往后，我们终于得到了一些实际博客文章内容的段落，但每个段落保留的上下文很少：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png\" class=\"kg-image\" alt=\"Webpage discussing if embedding/reranker models can compare numbers, with a grid of numbered circles and references to an ICM\" loading=\"lazy\" width=\"1164\" height=\"1180\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-2.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><p>（为了公平起见，我们展示了比其他模型更多的 Segmenter API 的文本块，因为否则它几乎没有有意义的段落可以展示）</p><h3 id=\"simple-qwen-05\"><code>simple-qwen-0.5</code></h3><p><code>simple-qwen-0.5</code> 根据语义结构将博客文章分解成更长的段落，每个段落都具有连贯的含义：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png\" class=\"kg-image\" alt=\"Webpage screenshot with green background, top navigation bar, scientific graphs, and headers discussing model number comparis\" loading=\"lazy\" width=\"1164\" height=\"4590\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-3.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"topic-qwen-05\"><code>topic-qwen-0.5</code></h3><p><code>topic-qwen-0.5</code> 首先基于文档内容识别主题，然后根据这些主题对文档进行分段：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png\" class=\"kg-image\" alt=\"Webpage showcasing a scientific paper titled &quot;Can Embedding/Keras Models Compare Numbers?&quot; featuring plots, text blocks, and \" loading=\"lazy\" width=\"1164\" height=\"4526\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-4.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"summary-qwen-05\"><code>summary-qwen-0.5</code></h3><p><code>summary-qwen-0.5</code> 识别段落边界并为每个段落内的内容生成摘要：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png\" class=\"kg-image\" alt=\"Green and gold-themed academic webpage discussing embedding/reranker models and experiment setup.\" loading=\"lazy\" width=\"1164\" height=\"3734\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-5.png 1164w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"benchmarking-the-models\">模型基准测试</h2><p>为了对我们的模型进行基准测试，我们从 Jina AI 博客抓取了八篇博客文章，并使用 GPT-4o 生成了六个问题和标准答案。</p><p>我们对这些博客文章应用了每种分段方法，包括 Jina Segmenter API，然后使用 <a href=\"https://jina.ai/embeddings?ref=jina-ai-gmbh.ghost.io\"><code>jina-embeddings-v3</code></a> 为生成的段落生成嵌入，不使用后期分块或重排。</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/v3banner.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>然后，我们分别对每组文本段进行了索引，并使用 RAG 系统通过之前生成的问题来查询每个索引。</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">为了保证比较的公平性，在测试 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> 和 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> 时，我们只索引了文本段本身，而<i><em class=\"italic\" style=\"white-space: pre-wrap;\">没有</em></i>索引生成的摘要或主题。如果索引这些额外的数据很可能会进一步提高性能。</div></div><p>为了衡量性能，我们使用 <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)?ref=jina-ai-gmbh.ghost.io\">ROUGE-L 评估</a>的 F1 分数来比较每种分段方法的索引生成的答案与标准答案，分数越高表示性能越好。</p><p>例如，对于问题：\"哪些因素影响 <code>jina-embeddings-v2-base-en</code> 模型的数值比较能力？\"，我们收到了以下回答：</p><h3 id=\"ground-truth-generated-by-gpt-4o-from-the-full-post-text\">标准答案（由 GPT-4o 从完整文章生成）：</h3><pre><code>\"The two key factors are tokenization and training data. Tokenization determines how numbers are segmented into tokens, and training data influences numerical reasoning abilities.\"\n</code></pre><h3 id=\"jina-segmenter-api-score-0166667\">Jina Segmenter API（分数：0.166667）</h3><pre><code>\"The context does not provide specific factors that impact the numerical comparison abilities of the jina-embeddings-v2-base-en model.\"\n</code></pre><h3 id=\"simple-qwen-05-score-0279070\"><code>simple-qwen-0.5</code>（分数：0.279070）</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by tokenization and training data.\"\n</code></pre><h3 id=\"topic-qwen-05-score-0190476\"><code>topic-qwen-0.5</code>（分数：0.190476）</h3><pre><code>\"The numerical comparison abilities of the jina-embeddings-v2-base-en model are impacted by factors such as tokenization and its ability to handle different numerical formats, including small integers, large numbers, floats, negative numbers, currency, dates, and times.\"\n</code></pre><h3 id=\"summary-qwen-05-0318182\"><code>summary-qwen-0.5</code>（分数：0.318182）</h3><pre><code>\"The factors impacting the numerical comparison abilities of the jina-embeddings-v2-base-en model are tokenization and training data.\"\n</code></pre><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">为什么</strong></b><b><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code></b><b><strong style=\"white-space: pre-wrap;\">的分数这么低？</strong></b><br>这主要是因为我们问模型的这个特定问题而产生的偶然结果。从下面的表格中可以看到，<code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> 的<i><em class=\"italic\" style=\"white-space: pre-wrap;\">平均</em></i> ROUGE 分数是所有分段方法中最高的。</div></div><p>我们还评估了每种方法的速度（通过计时生成和嵌入段落所需的时间）和估算磁盘空间（通过将嵌入数量乘以 <code>jina-embeddings-v3</code> 的单个 1024 维嵌入的大小）。这使我们能够评估不同分段策略的准确性和效率。</p><h2 id=\"key-findings\">主要发现</h2><p>在将各个模型变体与 Jina 的 Segmenter API 进行测试比较后，我们发现新模型确实在所有三种方法上都显示出更好的分数，特别是主题分段：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png\" class=\"kg-image\" alt=\"Bar chart comparing average ROUGE scores for Jina Segmenter, Simple, COATopic, and Summary Segmentation.\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-7.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-7.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-7.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-7.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>分段方法</strong></th>\n<th><strong>平均 ROUGE 分数</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>0.352126</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>0.386096</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td><strong>0.398340</strong></td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>0.328143</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">为什么 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> 的 ROUGE 分数比 <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">topic-qwen-0.5</code> 低？简单来说，<code spellcheck=\"false\" style=\"white-space: pre-wrap;\">summary-qwen-0.5</code> 在训练期间显示出更高的损失，表明需要更多的训练才能获得更好的结果。这可能是未来实验的主题。</div></div><p>然而，使用 <code>jina-embeddings-v3</code> 的后期分块功能来审查结果会很有趣，该功能可以增加段落嵌入的上下文相关性，提供更相关的结果。这可能是未来博客文章的主题。</p><p>关于速度，很难将新模型与 Jina Segmenter 进行比较，因为后者是一个 API，而我们在 Nvidia 3090 GPU 上运行这三个模型。如你所见，Segmenter API 在分段步骤中获得的任何性能提升都很快被为大量段落生成嵌入的需求所超过：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png\" class=\"kg-image\" alt=\"Bar chart showing time for text segmentation methods: Jina Segmenter, Simple, CoT Topic, and Summary Segmentation, with notab\" loading=\"lazy\" width=\"1682\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-8.png 1682w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png\" class=\"kg-image\" alt=\"Vertical bar chart displaying the embedding times for Jina Segmenter, Simple, CoT Topic, and Summary Segmentation.\" loading=\"lazy\" width=\"1698\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-9.png 1698w\" sizes=\"(min-width: 720px) 720px\"></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">注意事项</strong></b><br>• 我们在两个图表中使用不同的 Y 轴，因为用一个图表或一致的 Y 轴来展示如此不同的时间范围是不可行的。<br>• 由于我们仅将此作为实验，在生成嵌入时没有使用批处理。使用批处理会大大加快所有方法的操作速度。</div></div><p>自然地，更多的段落意味着更多的嵌入。这些嵌入会占用大量空间：我们测试的八篇博客文章的嵌入使用 Segmenter API 占用了超过 21 MB，而摘要分段方法只占用了 468 KB。这一点，再加上我们模型的更高 ROUGE 分数意味着更少但更好的段落，可以节省成本并提高性能：</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png\" class=\"kg-image\" alt=\"Vertical bar chart comparing total embedding size of segmentation methods, with &quot;Jina Segmenter&quot; significantly higher at 20.0\" loading=\"lazy\" width=\"1690\" height=\"1079\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-6.png 1690w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Segmentation Method</strong></th>\n<th><strong>Segment Count</strong></th>\n<th><strong>Average Length (characters)</strong></th>\n<th><strong>Segmentation Time (minutes/seconds)</strong></th>\n<th><strong>Embedding Time (hours/minutes)</strong></th>\n<th><strong>Total Embedding Size</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Jina Segmenter</td>\n<td>1,755</td>\n<td>82</td>\n<td>3.8s</td>\n<td>1h 46m</td>\n<td>21.06 MB</td>\n</tr>\n<tr>\n<td><code>simple-qwen-0.5</code></td>\n<td>48</td>\n<td>1,692</td>\n<td>49s</td>\n<td>1h 2m</td>\n<td>576 KB</td>\n</tr>\n<tr>\n<td><code>topic-qwen-0.5</code></td>\n<td>69</td>\n<td>1,273</td>\n<td>2m 3s</td>\n<td>1h 6m</td>\n<td>828 KB</td>\n</tr>\n<tr>\n<td><code>summary-qwen-0.5</code></td>\n<td>39</td>\n<td>1,799</td>\n<td>2m 40s</td>\n<td>53m</td>\n<td>468 KB</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"what-we-learned\">我们学到了什么</h3><h3 id=\"problem-formulation-is-critical\">问题定义至关重要</h3><p>一个关键的见解是我们如何构建任务的影响。通过让模型输出段落头部，我们通过关注语义转换而不是简单地将输入内容复制粘贴到不同的段落中，改进了边界检测和连贯性。这也使分段模型更快，因为生成更少的文本使模型能够更快地完成任务。</p><h3 id=\"llm-generated-data-is-effective\">LLM 生成的数据是有效的</h3><p>使用 LLM 生成的数据，特别是对于列表、公式和代码片段等复杂内容，扩大了模型的训练集，提高了其处理各种文档结构的能力。这使模型在处理不同类型的内容时更具适应性，这在处理技术或结构化文档时是一个关键优势。</p><h3 id=\"output-only-data-collation\">仅输出数据整理</h3><p>通过使用仅输出的<a href=\"https://huggingface.co/docs/transformers/en/main_classes/data_collator?ref=jina-ai-gmbh.ghost.io\">数据整理器</a>，我们确保模型在训练期间专注于预测目标令牌，而不是仅仅从输入中复制。仅输出整理器确保模型从实际的目标序列中学习，强调正确的补全或边界。这种区别使模型通过避免对输入过拟合而更快地收敛，并帮助它在不同的数据集上更好地泛化。</p><h3 id=\"efficient-training-with-unsloth\">使用 Unsloth 进行高效训练</h3><p>通过 <a href=\"https://github.com/unslothai/unsloth?ref=jina-ai-gmbh.ghost.io\">Unsloth</a>，我们简化了小型语言模型的训练，成功在 Nvidia 4090 GPU 上运行它。这种优化的流程使我们能够在不需要大量计算资源的情况下训练出一个高效、性能良好的模型。</p><h3 id=\"handling-complex-texts\">处理复杂文本</h3><p>分段模型在处理包含代码、表格和列表的复杂文档方面表现出色，这些通常对于传统方法来说比较困难。对于技术内容，像 <code>topic-qwen-0.5</code> 和 <code>summary-qwen-0.5</code> 这样的复杂策略更有效，有可能增强下游 RAG 任务。</p><h3 id=\"simple-methods-for-simpler-content\">简单内容使用简单方法</h3><p>对于简单的、叙事驱动的内容，Segmenter API 这样的简单方法通常就足够了。高级分段策略可能只在处理更复杂、结构化的内容时才有必要，这样可以根据使用场景灵活选择。</p><h2 id=\"next-steps\">下一步</h2><p>虽然这个实验主要是作为概念验证，但如果我们要进一步扩展它，我们可以做几项改进。首先，虽然这个特定实验不太可能继续，但在更大的数据集上训练 <code>summary-qwen-0.5</code>——理想情况下是 60,000 个样本而不是 30,000 个——可能会带来更理想的性能。此外，改进我们的基准测试过程也会有帮助。与其评估 RAG 系统生成的 LLM 答案，我们会专注于将检索到的段落直接与真实值进行比较。最后，我们会超越 ROUGE 分数，采用更高级的指标（可能是 ROUGE 和 LLM 评分的组合）来更好地捕捉检索和分段质量的细微差别。</p><h2 id=\"conclusion\">结论</h2><p>在这个实验中，我们探索了针对特定任务设计的自定义分段模型如何提升 RAG 的性能。通过开发和训练像 <code>simple-qwen-0.5</code>、<code>topic-qwen-0.5</code> 和 <code>summary-qwen-0.5</code> 这样的模型，我们解决了传统分段方法中的关键挑战，特别是在保持语义连贯性和有效处理代码片段等复杂内容方面。在测试的模型中，<code>topic-qwen-0.5</code> 始终提供最有意义和上下文相关的分段，特别是对于多主题文档。</p><p>虽然分段模型为 RAG 系统提供了必要的结构基础，但它们与后期分块的功能不同，后者通过维护跨段落的上下文相关性来优化检索性能。这两种方法可以互补，但当你需要一种专注于为连贯的、特定任务的生成工作流分割文档的方法时，分段尤为重要。</p>",
  "comment_id": "67126986708dbe00019249f2",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/breakpoints.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-18T15:58:30.000+02:00",
  "updated_at": "2024-10-25T20:14:53.000+02:00",
  "published_at": "2024-10-25T10:35:07.000+02:00",
  "custom_excerpt": "We trained three small language models to better segment long documents into chunks, and here are the key lessons we learned.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/finding-optimal-breakpoints-in-long-documents-using-small-language-models/",
  "excerpt": "我们训练了三个小型语言模型来更好地将长文档分割成块，以下是我们学到的关键经验。",
  "reading_time": 19,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast.",
  "feature_image_caption": null
}
{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "从 ModernBERT 中我们应该学到什么？",
  "html": "<p>早在 2018 年，Google 发布了 BERT，这在 NLP 领域是一个重大突破，远在当前大语言模型浪潮之前。即使现在，许多小型语言模型仍然基于 BERT 构建。2024 年 12 月，<a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> 将我们从近期大语言模型发展中获得的经验应用到这些较小的模型中。主要改进？更好的参数效率、代码理解能力和长文本处理能力。</p><p>在这篇文章中，我们将比较 ModernBERT 与我们非常熟悉的两个模型：<code>jina-XLM-RoBERTa</code>（<code>jina-embeddings-v3</code> 背后的多语言基础模型）和 <code>RoBERTa-large</code>。让我们来看看每个模型：</p><ul><li><strong>ModernBERT</strong>（2024 年 12 月）是最近发布的小型语言模型，由 Answer.AI、LightOn 和 HuggingFace 共同开发。它利用了现代优化技术，如用于 8,192 token 上下文窗口的 RoPE 和 <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">GeGLU layers</a>，在保持效率的同时提升性能。</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong>（2024 年 9 月）是一个基于 Meta 的 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> 的多语言文本嵌入模型。原始 <code>XLM-RoBERTa</code> 使用 XLM 大型多语言数据集增强了 <code>RoBERTa</code>，而 <code>jina-XLM-RoBERTa</code> 通过扩展上下文训练、<a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> 实现和 <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a> 支持进一步改进。这个模型是 <code>jina-embeddings-v3</code> 的基础。</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a>（2019 年 7 月）由 Meta 开发，是 BERT 的增强版本，拥有 3.55 亿参数。通过扩展训练、更大的数据集和动态掩码等创新，它在包括 <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>、<a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a> 和 <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a> 在内的关键基准测试中取得了出色的成果。这使其非常适合从文本分类到问答等各种 NLP 任务。</li></ul><p>通过比较这些模型的三个核心方面，我们旨在为模型开发者突出 ModernBERT 的有效设计选择，并为未来的 BERT 类模型开发确定关键洞察。我们还将分享开发 <code>jina-embeddings-v3</code> 的经验，并讨论 <code>jina-embeddings-v4</code> 和 <code>jina-reranker-v3</code> 的计划改进。</p><h2 id=\"modernberts-parameter-efficiency\">ModernBERT 的参数效率</h2><p>让我们首先研究 ModernBERT 在参数效率方面的方法——它借鉴了最近大语言模型开发的几个关键见解。ModernBERT 利用了三个核心策略：更深但更窄的架构、可控的词汇表大小，以及从较小模型开始的渐进式模型扩展。</p><h3 id=\"deep-and-thin-architecture\">深而窄的架构</h3><p>ModernBERT-large 采用了 28 层的更深架构，而 <code>jina-XLM-RoBERTa</code> 和 <code>RoBERTa-large</code> 是 24 层。但有趣的是，尽管增加了层数，它的参数数量与 <code>RoBERTa-large</code> 持平。<code>jina-XLM-RoBERTa</code> 需要更多参数是因为它要处理 89 种语言，而另外两个只专注于英语。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">对于小型大语言模型来说，深度（层数）比宽度（隐藏单元数）更重要。深而窄的模型结构在捕捉抽象概念方面表现出色，最终带来更好的性能。</span></figcaption></figure><p>transformer 的大部分参数来自注意力层和全连接层。ModernBERT 通过采用\"更窄\"的方式保持竞争力——在 28 层中使用 2,624 个隐藏单元，相比之下 RoBERTa-large 在 24 层中使用 4,096 个单元。这种\"更深\"但更窄的设置使他们能够在不增加模型体积的情况下达到性能目标。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>参数量</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>隐藏状态</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>中间维度</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>注意力头</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>层数</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>词汇表大小</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>这种方法与 Meta 的 <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a> 研究结果相符，该研究发现对于较小的模型来说，在捕捉复杂模式和提升性能方面，深度比宽度更重要。本质上，通过更多 transformer 层处理信息的能力比拥有更宽的层进行并行处理更有价值。</p><p>让我们看看这种深而窄架构的性能数据。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">与使用传统浅而宽架构的同类模型相比，ModernBERT 在检索和 STS 等关键任务上表现更好——同时保持了类似的参数数量。</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>平均值</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>以 <code>jina-XLM-RoBERTa</code> 为例——它在 <code>RoBERTa-large</code> 的浅而宽架构基础上将词汇表从 5 万扩大到 25 万个 token，并训练了更多数据。但 ModernBERT 仍然略胜一筹，这表明架构的改变确实在效率方面产生了实质性的差异。</p><h3 id=\"vocabulary-size-matters\">词汇表大小很重要</h3><p>首先，让我们看看 transformer 中词汇表参数是如何计算的。对于任何 transformer，<code>词汇表参数 = 不同 token 数量 × 隐藏维度</code>。以 <code>jina-XLM-RoBERTa</code> 为例：有 25 万个 token 和 1,024 维度，仅词汇表编码就需要 2.56 亿参数——这还是在处理任何实际语言任务之前！</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">在 transformers 中，第一层通过权重矩阵（即词汇权重）将词元映射为隐藏状态。考虑到使用所有 UTF-8 码点（1,112,064）与 1,024 个隐藏维度 - 仅用于词元转换就需要巨大的 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> 参数。虽然更大的 LLM（超过 100B 参数）可以处理这种开销，但对于较小的模型来说这是一个严重的限制。这正是我们使用 BPE 等分词器的原因，它可以高效地将常见的 UTF-8 码点合并为单个词元。</span></figcaption></figure><p>但关键是：<strong>词汇权重不参与注意力机制 - 它们只是查找表。</strong>对于在固定参数预算下工作的 SLM，更大的词汇表意味着用于实际语言处理的注意力层可用参数更少。这解释了为什么仅支持英语的 ModernBERT-large 尽管规模较小却优于多语言 <code>jina-XLM-RoBERTa</code> - <code>jina-XLM-RoBERTa</code> 分配了更多参数（47%！）来支持多种语言。ModernBERT 的专注词汇表不仅提高了性能，还加快了推理速度，使其特别适合资源受限的应用。</p><p>所以现在如果我们只看核心模型参数（不包括词汇权重），ModernBERT 实际上比其同行具有更强的计算能力：ModernBERT 在<em>实际</em>语言建模上比 <code>jina-XLM-RoBERTa</code> 多 19% 的参数，比 <code>RoBERTa-large</code> 多 15%！</p><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th>模型规格</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>语言支持</td>\n<td>仅英语</td>\n<td>89 种语言</td>\n<td>仅英语</td>\n</tr>\n<tr>\n<td>词汇量大小</td>\n<td>50.4K</td>\n<td>250K</td>\n<td>50.3K</td>\n</tr>\n<tr>\n<td>总参数</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>词汇参数</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>词汇参数比例</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>核心模型参数</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table><!--kg-card-end: html--><h3 id=\"model-upscaling-by-weight-tiling\">通过\"权重平铺\"进行模型扩展</h3><p>在构建 <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a> 骨干网络时，我们发现从头开始训练 SLM 需要大量资源且复杂。ModernBERT 通过一种称为<strong>权重平铺</strong>的智能初始化方法解决了这个问题 - 本质上是从其较小的基础版本的权重引导 ModernBERT-large。</p><p>这种技术并不完全是新的 - 它建立在 DeepMind 的 <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> 工作基础上，在微软的 <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2 模型</a>中也有体现。但它在这里的应用对解决 SLM 训练瓶颈特别有效。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT 使用 Gopher 团队的深度初始化策略从 22 层扩展到 28 层。对于那些额外的层（23-28），他们使用 ModernBERT-base 原始 22 层中的权重初始化每一层。对于每层的权重矩阵，他们使用 Phi-2 的中心平铺方法。工作原理是这样的：他们将 ModernBERT-base 的权重放在 ModernBERT-large 矩阵的中间。对于仍然空着的边缘怎么办？他们循环包装原始权重来填充它们。</span></figcaption></figure><p>这种初始化策略给 ModernBERT-large 带来了显著优势 - 它不是从零开始，而是利用了其较小版本预先学习的模式。它在<a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">扩展这个规模范围内的语言模型方面</a>特别有效。</p><blockquote>我们发现热启动模型能够从初始的高损失（由于增加的参数）中快速恢复，达到接近基础模型的损失水平。我们能够将 417M 参数扩展超过 3 倍，并保持优于从头开始训练到收敛的等效新模型的性能，这意味着收益不仅限于训练初期。然而，在更大的规模下，收敛时获得的相对收益会减少，特别是在宽度扩展方面。</blockquote><p>循环权重包装不仅仅是为了方便 - 它与注意力矩阵自然呈现周期性模式的方式很好地吻合。Gopher 的研究表明，这种方法在 SLM（小于 9B 参数）中特别有效，但随着模型规模增大，这些优势开始减弱。</p><h2 id=\"modernberts-code-modeling\">ModernBERT 的代码建模</h2><p>ModernBERT 通过其代码优化的分词器和训练数据，为代码理解带来了专门的方法。这种针对代码处理的微调在理解和检索任务中都取得了成效。</p><p>我们使用 <code>jina-embeddings-v2-code</code> 语料库进行了基准测试，比较了三个模型作为骨干：<code>ModernBERT</code>、<code>jina-XLM-RoBERTa</code> 和 <code>RoBERTa-large</code>。测试是什么？<a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - 将文本描述与代码片段匹配。ModernBERT 在各个方面都优于其他两个替代方案。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">这种差距是有道理的 - </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> 和 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> 在训练期间都没有接触过编程语言。同时，ModernBERT-large 训练了两万亿个词元，其中包含大量代码。这种对编程语法和模式的接触使其在代码相关任务中具有明显优势。</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> 略胜 </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">，可能是由于其更大的多语言训练数据 - 相同的架构，更多的接触。尽管如此，两者都明显落后于 ModernBERT-large。</span></figcaption></figure><!--kg-card-begin: html--><table>\n<thead>\n<tr>\n<th>任务</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"the-tokenizer-edge\">分词器的优势</h3><p>让我们深入了解为什么 ModernBERT 能很好地处理代码 - 它使用的是专门针对代码训练的 <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">OLMo tokenizer</a>，而不是标准的 BERT/RoBERTa 分词器。</p><p>分词器将 UTF-8 文本分解成映射到向量的 token - 这些才是模型实际处理的内容。在训练过程中，它学会将频繁出现的字符序列组合成单个 token。区别在哪里？标准分词器可能会把 <code>init</code> 分解为 <code>in</code> + <code>it</code>，忽略了编程上下文。但 ModernBERT 的代码感知分词器则可以完整保留它。</p><p>在空格处理方面就更有意思了：ModernBERT 将 Python 的前导空格保留为单个 token，并区分 4 个和 8 个空格的差异 - 这对代码结构至关重要。同时，<strong><code>jina-XLM-RoBERTa</code> 将所有连续空格压缩为单个 <code>_</code>，而 RoBERTa-large 则将每个空格都视为独立的 token。</strong> 这意味着 ModernBERT 的编码器在处理代码时能获得更清晰、更有意义的输入，而其他模型则要处理破碎、连贯性较差的 token。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT 将 Python 的前导空格保留为单个 token，并区分 4 个和 8 个空格的差异 - 这对代码结构至关重要；而其他模型则需要处理破碎、连贯性较差的 token。</span></figcaption></figure><h2 id=\"modernberts-long-context-handling\">ModernBERT 的长文本处理能力</h2><p>ModernBERT 在处理长文本方面取得了重大进展，这要归功于其庞大的训练语料库（包含 8,192 token 样本的 300B tokens）和全局与局部注意力相结合的先进技术。</p><p>为了评估长文本处理能力，我们使用了 <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">MLDR 数据集</a> - 这是一个涵盖 13 种语言的综合性长文本基准。由于 ModernBERT 目前仅支持英语，我们专注于 MLDR 的英语子集，对比了 ModernBERT 和 <code>jina-XLM-RoBERTa</code>。虽然这两个模型都能处理 8K token 的输入，但由于 <code>RoBERTa-large</code> 的 512 token 限制不足以进行长文本分析，因此被排除在这次基准测试之外。</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>ModernBERT 的卓越性能不仅仅归功于其广泛的长文本训练 - 很大程度上要归功于其创新的全局和局部注意力机制组合。与在每一层都使用计算成本高昂的全局注意力的 <code>jina-XLM-RoBERTa</code> 不同，ModernBERT 采用了更高效的方法。它在全局注意力（每三层使用一次，<code>theta</code> 为 160,000）和局部注意力（使用 128 token 的滑动窗口，<code>theta</code> 为 100,000）之间交替使用。这种混合策略在保持高性能的同时大大减少了训练时间。</p><blockquote>在 ModernBERT 中，每三层会使用一次全局注意力，RoPE theta 为 160,000，其余层使用 128 token 的局部滑动窗口注意力，RoPE theta 为 10,000。—— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote><h2 id=\"the-bitter-lesson\">痛苦的教训？</h2><p>扩展定律和<a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">痛苦的教训</a>表明，性能的主要改进主要来自增加参数数量和训练数据。这一原则指导了我们扩展语料库和使用 LoRA 进行任务特定适应的方法。</p><p>然而，ModernBERT 的成功表明我们低估了架构优化的力量。它证明了 SLM 可以通过更好的数据模型效率实现卓越的结果，而不一定要扩大参数规模。最近的一份<a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">Stella Embeddings 技术报告</a>强化了这一发现，表明当前的嵌入模型训练方法无需增加语料库或模型规模就可以得到改进。</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">嵌入模型的扩展定律。图中展示了英语任务的平均 MTEB 性能与模型参数数量的关系。每个点代表一个嵌入模型。突出显示了代表所有模型的趋势线，其中多语言模型以青色强调。可以看出，</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">与同规模的模型相比表现出色，相对其前身</span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">也展现出超线性的改进。该图表是通过从 MTEB 排行榜中选取前 100 个嵌入模型创建的，排除了那些没有规模信息的模型（通常是闭源或专有模型）。明显的恶意提交也被过滤掉了。</span></figcaption></figure><p>展望未来，随着我们对数据利用的深入理解和 ModernBERT 技术的实施，我们预计计算成本和模型规模都将降低。短期内，我们可以实施 ModernBERT 论文中概述的直接改进 - 特别是整合更多与代码相关的数据并采用代码友好的分词器。更复杂的改变，如转向深而窄的架构或从较小的模型引导大模型，将需要从头开始构建骨干模型 - 这是一个中期计划。</p><p>虽然 ModernBERT 的效率令人印象深刻，但其仅限文本的局限性也指向了未来的挑战。随着多模态嵌入模型的普及，我们的下一个挑战是开发更智能、更快速、能力更强的搜索基础模型，以处理多模态应用的输入。这些应用需要更长的上下文窗口 - 这是一个尚待解决的效率挑战。</p><h2 id=\"conclusion\">结论</h2><p>在本文中，我们探讨了 ModernBERT 如何通过三个关键创新推进 BERT 系列模型：深而窄的架构、优化的分词器以及使用权重平铺的高效扩展。这些改进使得 ModernBERT 能够在相对紧凑的规模下提供出色的性能，在各种任务中超越了 <code>RoBERTa-large</code> 和 <code>jina-XLM-RoBERTa</code>。ModernBERT 证明了架构改进可能比参数规模更重要，为更高效的模型开辟了道路。它成功使用权重平铺表明，渐进式扩展可以降低训练成本，同时保持或甚至提升性能。此外，其精简的词汇表和有针对性的优化表明，在资源受限的环境中，专门的 SLM 有着越来越多的机会。</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "更大的训练数据、高效的参数配置以及深而窄的架构，ModernBERT 为未来类 BERT 模型指明了方向。",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
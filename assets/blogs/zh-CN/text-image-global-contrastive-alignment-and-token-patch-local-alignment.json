{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "文本-图像全局对比对齐和 Token-Patch 局部对齐",
  "html": "<p>在实验 <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali 风格</a>模型时，我们的一位工程师使用最近发布的 <code>jina-clip-v2</code> 模型创建了一个可视化。他映射了给定图像-文本对之间的 token 嵌入和图像块嵌入的相似度，创建了热力图叠加层，产生了一些有趣的视觉洞察。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>不幸的是，<strong>这只是一个启发式可视化</strong>——而不是一个明确或有保证的机制。虽然类 CLIP 的全局对比对齐可能（而且经常会）<em>偶然地</em>在图像块和 token 之间创建粗略的局部对齐，但这是一个<strong>意外的副作用</strong>，而不是模型的刻意目标。让我解释原因。</p><h2 id=\"understand-the-code\">理解代码</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>让我们从高层次分析代码在做什么。注意，默认情况下 <code>jina-clip-v2</code> 实际上并没有暴露任何用于访问 token 级别或图像块级别嵌入的 API——这个可视化需要一些后期修补才能工作。</p><p><strong>计算词级嵌入</strong></p><p>通过设置 <code>model.text_model.output_tokens = True</code>，调用 <code>text_model(x=...,)[1]</code> 将返回一个形状为 <code>(batch_size, seq_len, embed_dim)</code> 的第二个元素作为 token 嵌入。它接收一个输入句子，用 Jina CLIP tokenizer 进行分词，然后通过平均相应的 token 嵌入将子词 token 重新组合成\"词\"。它通过检查 token 字符串是否以 <code>_</code> 字符开头（在基于 SentencePiece 的分词器中很常见）来检测新词的开始。它生成一个词级嵌入列表和一个词列表（所以\"Dog\"是一个嵌入，\"and\"是一个嵌入，等等）。</p><p><strong>计算图像块级嵌入</strong></p><p>对于图像塔，<code>vision_model(..., return_all_features=True)</code> 将返回形状为 <code>(batch_size, n_patches+1, embed_dim)</code> 的张量，其中第一个 token 是 <code>[CLS]</code> token。从中，代码提取每个图像块的嵌入（即视觉 transformer 的图像块 token）。然后将这些图像块嵌入重塑为二维网格，<code>patch_side × patch_side</code>，然后上采样以匹配原始图像分辨率。</p><p><strong>可视化词-图像块相似度</strong></p><p>相似度计算和随后的热力图生成是标准的\"后验\"可解释性技术：你选择一个文本嵌入，计算它与每个图像块嵌入的余弦相似度，然后生成一个热力图，显示哪些图像块与该特定 token 嵌入的相似度最高。最后，它遍历句子中的每个 token，在左侧以粗体突出显示该 token，并在右侧的原始图像上叠加基于相似度的热力图。所有帧被编译成一个动态 GIF。</p><h2 id=\"is-it-meaningful-explainability\">这是有意义的可解释性吗？</h2><p>从<em>纯代码</em>的角度来看，是的，逻辑是连贯的，会为每个 token 生成热力图。你会得到一系列显示图像块相似度的帧，所以脚本\"确实做到了它说要做的事情\"。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>看上面的例子，我们可以看到像 <code>moon</code> 和 <code>branches</code> 这样的词似乎与原始图像中对应的视觉块对齐得很好。但这里的关键问题是：这是有意义的对齐，还是我们仅仅看到了一个幸运的巧合？</p><p>这是一个更深层次的问题。要理解其中的注意事项，回想一下 <strong>CLIP 是如何训练的</strong>：</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 结合了文本编码器（Jina XLM-RoBERTa，561M 参数）和视觉编码器（EVA02-L14，304M 参数）。右侧的每个彩色方块代表批次中的完整句子或图像——而不是单个 token 或图像块。</span></figcaption></figure><ul><li>CLIP 使用整个图像和整段文本之间的<strong>全局</strong>对比对齐。在训练过程中，图像编码器生成单个向量（池化表示），文本编码器生成另一个单个向量；CLIP 被训练成使这些向量在匹配的文本-图像对中匹配，在其他情况下不匹配。</li><li><strong>在\"图像块 X 对应于 token Y\"级别上没有显式监督。</strong>模型并没有被直接训练来突出显示\"图像的这个区域是狗，那个区域是猫\"等。相反，它被教导整个图像表示应该与整个文本表示匹配。</li><li>因为 CLIP 的架构在图像端使用 Vision Transformer，在文本端使用文本 transformer——两者形成独立的编码器——所以没有本地对齐图像块和 token 的交叉注意力模块。相反，你在每个塔中得到的是纯粹的<strong>自注意力</strong>，加上最终的全局图像或文本嵌入投影。</li></ul><p>简而言之，这是一个启发式可视化。任何给定的图像块嵌入可能与特定 token 嵌入接近或远离，这在某种程度上是自然涌现的。它更像是一个<em>后验可解释性技巧</em>，而不是模型的稳健或官方\"注意力\"。</p><h2 id=\"why-might-local-alignment-emerge\">为什么可能会出现局部对齐？</h2><p>那么为什么我们有时会发现词-图像块级别的局部对齐呢？事情是这样的：尽管 CLIP 是在<em>全局</em>图像-文本对比目标上训练的，但它仍然使用自注意力（在基于 ViT 的图像编码器中）和 transformer 层（用于文本）。在这些自注意力层中，图像表示的不同部分可以相互交互，就像文本表示中的词一样。通过在海量图像-文本数据集上的训练，模型自然地发展出内部潜在结构，帮助它将整体图像与其对应的文本描述匹配。</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>局部对齐</strong>可能在这些潜在表示中出现，至少有两个原因：</p><ol><li><strong>共现模式</strong>：如果模型看到许多\"狗\"旁边的\"猫\"的图像（通常带有这些词的标签或描述），它可以学习大致对应于这些概念的潜在特征。因此，\"狗\"的嵌入可能会接近描绘狗形状或纹理的局部图像块。这在图像块级别<em>不是</em>显式监督的，而是从狗图像/文本对的重复关联中涌现出来的。</li><li><strong>Self-attention</strong>：在 Vision Transformers 中，图像块之间会相互关注。显著的图像块（比如狗的脸）最终可能会得到一个一致的潜在\"特征\"，因为模型试图为整个场景生成一个全局准确的表示。如果这有助于最小化整体对比损失，这种特征就会得到加强。</li></ol>\n\n<h2 id=\"theoretical-analysis\">理论分析</h2>\n\n<p>CLIP 的对比学习目标是最大化匹配的图文对之间的余弦相似度，同时最小化不匹配对之间的相似度。假设文本和图像编码器分别产生 token 和图像块嵌入：</p>\n\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n\n<p>全局相似度可以表示为局部相似度的聚合：</p>\n\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n\n<p>当特定的 token-patch 对在训练数据中经常共同出现时，模型通过累积梯度更新来加强它们的相似度：</p>\n\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n\n<p>，其中 $C$ 是共同出现的次数。这导致 $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$ 显著增加，促进这些对的局部对齐更加紧密。然而，对比损失会在所有 token-patch 对之间分配梯度更新，限制了特定对的更新强度：</p>\n\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n\n<p>这防止了单个 token-patch 相似度的显著加强。</p>\n\n<h2 id=\"conclusion\">结论</h2>\n\n<p>CLIP 的 token-patch 可视化利用了文本和图像表示之间偶然的、自发的对齐。这种对齐虽然很有趣，但源于 CLIP 的<strong>全局对比训练</strong>，缺乏精确可靠可解释性所需的结构稳健性。由此产生的可视化结果经常表现出<strong>噪声和不一致性</strong>，限制了其在深入解释应用中的实用性。</p>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n\n<p>像 <strong>ColBERT</strong> 和 <strong>ColPali</strong> 这样的后期交互模型通过<strong>在架构上嵌入明确的、细粒度的对齐</strong>来解决这些限制，实现文本 token 和图像块之间的对齐。通过独立处理不同模态并在后期阶段执行有针对性的相似度计算，这些模型确保每个文本 token 都能与相关的图像区域建立有意义的关联。</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP 可以可视化 token-patch 相似度，但这更像是一种事后解释性技巧，而不是来自模型的稳健或正式的\"注意力\"。这就是原因。",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
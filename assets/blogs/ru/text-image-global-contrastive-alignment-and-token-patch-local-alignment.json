{
  "slug": "text-image-global-contrastive-alignment-and-token-patch-local-alignment",
  "id": "677be55d2defad0001fb5e13",
  "uuid": "6cabf14e-4502-4f1e-810a-3bf5111953d6",
  "title": "Глобальное контрастное выравнивание текста и изображения и локальное выравнивание токенов и патчей",
  "html": "<p>При экспериментах с моделями в стиле <a href=\"https://arxiv.org/abs/2407.01449?ref=jina-ai-gmbh.ghost.io\">ColPali</a> один из наших инженеров создал визуализацию с использованием недавно выпущенной модели <code>jina-clip-v2</code>. Он отобразил схожесть между эмбеддингами токенов и эмбеддингами патчей для заданных пар изображение-текст, создав тепловые карты наложения, которые дали интересные визуальные результаты.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--27-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--27-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--29-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--29-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>К сожалению, <strong>это всего лишь эвристическая визуализация</strong> — а не явный или гарантированный механизм. Хотя глобальное контрастное выравнивание в стиле CLIP может (и часто действительно) <em>случайно</em> создавать грубые локальные выравнивания между патчами и токенами, это <strong>непреднамеренный побочный эффект</strong>, а не целенаправленная задача модели. Позвольте мне объяснить почему.</p><h2 id=\"understand-the-code\">Понимание кода</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1SwfjZncXfcHphtFj_lF75rVZc_g9-GFD?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-21.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Давайте разберем, что делает код на высоком уровне. Заметим, что <code>jina-clip-v2</code> по умолчанию не предоставляет API для доступа к эмбеддингам на уровне токенов или патчей — для этой визуализации потребовалась некоторая последующая доработка.</p><p><strong>Вычисление эмбеддингов на уровне слов</strong></p><p>Установив <code>model.text_model.output_tokens = True</code>, вызов <code>text_model(x=...,)[1]</code> вернет второй элемент <code>(batch_size, seq_len, embed_dim)</code> для эмбеддингов токенов. Таким образом, он принимает входное предложение, токенизирует его с помощью токенизатора Jina CLIP, а затем группирует подсловные токены обратно в \"слова\" путем усреднения соответствующих эмбеддингов токенов. Он определяет начало нового слова, проверяя, начинается ли строка токена с символа <code>_</code> (типично для токенизаторов на основе SentencePiece). В результате получается список эмбеддингов на уровне слов и список слов (так что \"Dog\" — это один эмбеддинг, \"and\" — один эмбеддинг и т.д.).</p><p><strong>Вычисление эмбеддингов на уровне патчей</strong></p><p>Для части обработки изображений <code>vision_model(..., return_all_features=True)</code> вернет <code>(batch_size, n_patches+1, embed_dim)</code>, где первый токен — это токен <code>[CLS]</code>. Из этого код извлекает эмбеддинги для каждого патча (то есть токены патчей из vision transformer). Затем он изменяет форму этих эмбеддингов патчей в 2D сетку <code>patch_side × patch_side</code>, которая затем увеличивается до размера исходного изображения.</p><p><strong>Визуализация схожести слов и патчей</strong></p><p>Расчет схожести и последующая генерация тепловой карты — это стандартные методы интерпретации \"после факта\": вы берете эмбеддинг текста, вычисляете косинусную схожесть с каждым эмбеддингом патча и затем генерируете тепловую карту, показывающую, какие патчи имеют наибольшую схожесть с этим конкретным эмбеддингом токена. Наконец, код проходит через каждый токен в предложении, выделяет этот токен жирным шрифтом слева и накладывает тепловую карту схожести на исходное изображение справа. Все кадры объединяются в анимированный GIF.</p><h2 id=\"is-it-meaningful-explainability\">Является ли это значимым объяснением?</h2><p>С <em>чисто программной</em> точки зрения — да, логика согласована и создаст тепловую карту для каждого токена. Вы получите серию кадров, которые показывают схожести патчей, так что скрипт \"делает то, что обещает\".</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/884-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/884-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/25-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/25-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Глядя на примеры выше, мы видим, что слова вроде <code>moon</code> и <code>branches</code> как будто хорошо выравниваются с соответствующими визуальными патчами в исходном изображении. Но вот ключевой вопрос: это значимое выравнивание или мы просто видим удачное совпадение?</p><p>Это более глубокий вопрос. Чтобы понять ограничения, вспомним, <strong>как тренируется CLIP</strong>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/clipv2-model-architecture.svg\" class=\"kg-image\" alt=\"Diagram of JINA-CLIP-V2 model showing stages from input to output for English and multilingual text processing.\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Jina-CLIP v2 объединяет текстовый энкодер (Jina XLM-RoBERTa, 561M параметров) и визуальный энкодер (EVA02-L14, 304M параметров). Каждый цветной квадрат справа представляет целое предложение или изображение в батче — не отдельные токены или патчи.</span></figcaption></figure><ul><li>CLIP использует <strong>глобальное</strong> контрастное выравнивание между целым изображением и целым текстом. Во время обучения энкодер изображения создает один вектор (объединенное представление), а текстовый энкодер создает другой вектор; CLIP тренируется так, чтобы они совпадали для соответствующих пар текст-изображение и не совпадали для других.</li><li>Нет <strong>явного обучения на уровне 'патч X соответствует токену Y'</strong>. Модель не обучается напрямую выделять \"эта область изображения — собака, та область — кошка\" и т.д. Вместо этого она учится, что все представление изображения должно соответствовать всему представлению текста.</li><li>Поскольку архитектура CLIP включает Vision Transformer для изображений и текстовый трансформер для текста — оба формируют отдельные энкодеры — в ней нет модуля кросс-внимания, который бы естественным образом выравнивал патчи с токенами. Вместо этого вы получаете только <strong>self-attention</strong> в каждой башне, плюс финальную проекцию для глобальных эмбеддингов изображения или текста.</li></ul><p>Короче говоря, это эвристическая визуализация. То, что определенный эмбеддинг патча может быть близок или далек от конкретного эмбеддинга токена, является в некотором роде эмергентным свойством. Это скорее <em>трюк интерпретации после факта</em>, чем надежное или официальное \"внимание\" модели.</p><h2 id=\"why-might-local-alignment-emerge\">Почему может возникать локальное выравнивание?</h2><p>Так почему же мы иногда замечаем локальные выравнивания на уровне слов-патчей? Вот в чем дело: даже несмотря на то, что CLIP тренируется на <em>глобальной</em> контрастной задаче изображение-текст, он все равно использует self-attention (в энкодерах изображений на основе ViT) и слои трансформера (для текста). Внутри этих слоев self-attention различные части представлений изображений могут взаимодействовать друг с другом, так же как это делают слова в текстовых представлениях. Через обучение на массивных наборах данных изображение-текст модель естественным образом развивает внутренние латентные структуры, которые помогают ей сопоставлять целые изображения с их соответствующими текстовыми описаниями.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/255-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/255-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/777-512x512_attention.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/777-512x512_attention.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1000\" height=\"500\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/01/attention--25-.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/01/attention--25-.gif 1000w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Локальное выравнивание</strong> может появляться в этих латентных представлениях по крайней мере по двум причинам:</p><ol><li><strong>Паттерны совместной встречаемости</strong>: Если модель видит много изображений \"собак\" рядом со многими изображениями \"кошек\" (часто помеченных или описанных этими словами), она может изучить латентные признаки, которые примерно соответствуют этим концепциям. Таким образом, эмбеддинг для \"собаки\" может стать близким к локальным патчам, которые изображают собакоподобную форму или текстуру. Это <em>не</em> обучается явно на уровне патчей, но возникает из повторяющихся ассоциаций между парами изображений/текстов с собаками.</li><li><strong>Self-attention</strong>: В Vision Transformers патчи взаимодействуют друг с другом. Отличительные патчи (например, морда собаки) могут получить устойчивую латентную \"подпись\", поскольку модель пытается создать единое глобально точное представление всей сцены. Если это помогает минимизировать общую контрастивную потерю, это будет усиливаться.</li></ol><h2 id=\"theoretical-analysis\">Теоретический анализ</h2><p>Контрастивная целевая функция CLIP направлена на максимизацию косинусного сходства между соответствующими парами изображение-текст при минимизации его для несоответствующих пар. Предположим, что текстовый и визуальный энкодеры создают токены и эмбеддинги патчей соответственно:</p>\n<!--kg-card-begin: html-->\n$$\\mathbf{u}_i = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{u}_{i,m}, \\quad \\mathbf{v}_i = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Глобальное сходство можно представить как совокупность локальных сходств:</p>\n<!--kg-card-begin: html-->\n$$\\text{sim}(\\mathbf{u}_i, \\mathbf{v}_i) = \\frac{1}{MK} \\sum_{m=1}^M \\sum_{k=1}^K \\mathbf{u}_{i,m}^\\top \\mathbf{v}_{i,k}$$\n<!--kg-card-end: html-->\n<p>Когда определенные пары токен-патч часто встречаются вместе в обучающих данных, модель усиливает их сходство через накопительные обновления градиента:</p>\n<!--kg-card-begin: html-->\n$$\\Delta \\mathbf{u}_{m^*} \\propto \\sum_{c=1}^C \\mathbf{v}_{k^*}^{(c)}, \\quad \\Delta \\mathbf{v}_{k^*} \\propto \\sum_{c=1}^C \\mathbf{u}_{m^*}^{(c)}$$\n<!--kg-card-end: html-->\n<p>, где $C$ - количество совместных появлений. Это приводит к значительному увеличению $\\mathbf{u}_{m^*}^\\top \\mathbf{v}_{k^*}$, способствуя более сильному локальному выравниванию для этих пар. Однако контрастивная потеря распределяет обновления градиента по всем парам токен-патч, ограничивая силу обновлений для любой конкретной пары:</p>\n<!--kg-card-begin: html-->\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{m}} \\propto -\\sum_{k=1}^K \\mathbf{v}_k \\cdot \\left( \\frac{\\exp(\\mathbf{u}^\\top \\mathbf{v} / \\tau)}{\\sum_{j=1}^N \\exp(\\mathbf{u}^\\top \\mathbf{v}_j / \\tau)} \\right)$$\n<!--kg-card-end: html-->\n<p>Это предотвращает значительное усиление индивидуальных сходств токен-патч.</p><h2 id=\"conclusion\">Заключение</h2><p>Визуализация токен-патч в CLIP основывается на случайном, возникающем выравнивании между текстовыми и визуальными представлениями. Это выравнивание, хотя и интригующее, происходит из <strong>глобального контрастивного обучения</strong> CLIP и не обладает структурной надежностью, необходимой для точной и достоверной интерпретируемости. Получаемые визуализации часто демонстрируют <strong>шум и несогласованность</strong>, ограничивая их полезность для глубокого интерпретационного анализа.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What is ColBERT and Late Interaction and Why They Matter in Search?</div><div class=\"kg-bookmark-description\">Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-16.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Han Xiao</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/Untitled-design--28-.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Модели с поздним взаимодействием, такие как <strong>ColBERT</strong> и <strong>ColPali</strong>, решают эти ограничения путем <strong>архитектурного встраивания явных, детальных выравниваний</strong> между текстовыми токенами и патчами изображений. Обрабатывая модальности независимо и выполняя целенаправленные вычисления сходства на более поздней стадии, эти модели обеспечивают значимую связь каждого текстового токена с соответствующими областями изображения.</p>",
  "comment_id": "677be55d2defad0001fb5e13",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-06T15:14:53.000+01:00",
  "updated_at": "2025-01-07T12:23:50.000+01:00",
  "published_at": "2025-01-07T12:23:50.000+01:00",
  "custom_excerpt": "CLIP can visualize token-patch similarities, however, it’s more of a post-hoc interpretability trick than a robust or official \"attention\" from the model. Here's why.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-image-global-contrastive-alignment-and-token-patch-local-alignment/",
  "excerpt": "CLIP может визуализировать сходства между токенами и патчами, однако, это скорее постфактум интерпретационный трюк, чем надежное или официальное \"внимание\" модели. Вот почему.",
  "reading_time": 6,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
{
  "slug": "binary-embeddings-all-the-ai-3125-of-the-fat",
  "id": "662665537f510100015daa2d",
  "uuid": "bf2c8db3-bd7f-4b78-8054-4edd26349ec2",
  "title": "Бинарные эмбеддинги: вся мощь ИИ при 3,125% объема данных",
  "html": "<p>Embeddings стали краеугольным камнем множества приложений искусственного интеллекта и обработки естественного языка, предоставляя способ представления значений текстов в виде многомерных векторов. Однако с увеличением размера моделей и растущим объемом данных, обрабатываемых моделями ИИ, вычислительные требования и требования к хранению для традиционных embeddings возросли. Binary embeddings были представлены как компактная, эффективная альтернатива, которая сохраняет высокую производительность при значительном снижении требований к ресурсам.</p><p>Binary embeddings — это один из способов смягчить эти требования к ресурсам, уменьшая размер векторов embeddings до 96% (96.875% в случае Jina Embeddings). Пользователи могут использовать мощь компактных binary embeddings в своих ИИ-приложениях с минимальной потерей точности.</p><h2 id=\"what-are-binary-embeddings\">Что такое Binary Embeddings?</h2><p>Binary embeddings — это специализированная форма представления данных, где традиционные многомерные векторы с плавающей точкой преобразуются в бинарные векторы. Это не только сжимает embeddings, но и сохраняет практически всю целостность и полезность векторов. Суть этой техники заключается в её способности сохранять семантику и относительные расстояния между точками данных даже после преобразования.<br><br>Магия binary embeddings заключается в квантизации — методе, который превращает числа высокой точности в числа с меньшей точностью. В моделировании ИИ это часто означает преобразование 32-битных чисел с плавающей точкой в embeddings в представления с меньшим количеством битов, например, 8-битные целые числа.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg\" class=\"kg-image\" alt=\"Comparison of Hokusai's Great Wave print in color and black &amp; white, highlighting the wave's dynamism and detail.\" loading=\"lazy\" width=\"1280\" height=\"860\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/04/be.jpeg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/04/be.jpeg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/04/be.jpeg 1280w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Бинаризация — это преобразование всех скалярных значений в 0 или 1, как преобразование цветного изображения в изображение только с черными или белыми пикселями. Изображение: 神奈川沖浪裏 (1831) by 葛飾 (Hokusai)</span></figcaption></figure><p>Binary embeddings доводят это до крайности, сводя каждое значение к 0 или 1. Преобразование 32-битных чисел с плавающей точкой в бинарные цифры уменьшает размер векторов embeddings в 32 раза, сокращение на 96.875%. В результате векторные операции с полученными embeddings происходят намного быстрее. Использование аппаратных ускорений, доступных на некоторых микрочипах, может увеличить скорость сравнения векторов гораздо больше чем в 32 раза, когда векторы бинаризованы.</p><p>Некоторая информация неизбежно теряется в этом процессе, но эта потеря минимизируется, когда модель очень эффективна. Если non-quantized embeddings разных вещей максимально различны, то бинаризация с большей вероятностью хорошо сохранит эту разницу. В противном случае может быть сложно правильно интерпретировать embeddings.</p><p>Модели Jina Embeddings обучены быть очень устойчивыми именно таким образом, что делает их хорошо подходящими для бинаризации.</p><p>Такие компактные embeddings делают возможными новые приложения ИИ, особенно в контекстах с ограниченными ресурсами, таких как мобильные и чувствительные ко времени приложения.</p><p>Эти преимущества в стоимости и времени вычислений достигаются при относительно небольших потерях в производительности, как показано на графике ниже.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackmd.io/_uploads/ByhwJsQWC.png\" class=\"kg-image\" alt=\"image\" loading=\"lazy\" width=\"1686\" height=\"1050\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">NDCG@10: Оценки рассчитаны с использованием </em></i><a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain?ref=jina-ai-gmbh.ghost.io\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">Normalized Discounted Cumulative Gain</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\"> для топ-10 результатов.</em></i></figcaption></figure><p>Для <code>jina-embeddings-v2-base-en</code> бинарная квантизация снижает точность поиска с 47.13% до 42.05%, потеря примерно 10%. Для <code>jina-embeddings-v2-base-de</code> эта потеря составляет всего 4%, с 44.39% до 42.65%.</p><p>Модели Jina Embeddings так хорошо работают при создании бинарных векторов, потому что они обучены создавать более равномерное распределение embeddings. Это означает, что два разных embeddings будут вероятнее всего дальше друг от друга в большем количестве измерений, чем embeddings из других моделей. Это свойство обеспечивает лучшее представление этих расстояний в их бинарной форме.</p><h2 id=\"how-do-binary-embeddings-work\">Как работают Binary Embeddings?</h2><p>Чтобы понять, как это работает, рассмотрим три embedding: <em>A</em>, <em>B</em> и <em>C</em>. Все три являются полными векторами с плавающей точкой, не бинаризованными. Допустим, расстояние от <em>A</em> до <em>B</em> больше, чем расстояние от <em>B</em> до <em>C</em>. С embeddings мы обычно используем <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity?ref=jina-ai-gmbh.ghost.io\">косинусное расстояние</a>, поэтому: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>Если мы бинаризуем <em>A</em>, <em>B</em> и <em>C</em>, мы можем измерить расстояние более эффективно с помощью <a href=\"https://en.wikipedia.org/wiki/Hamming_distance?ref=jina-ai-gmbh.ghost.io\">расстояния Хэмминга</a>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-6.png\" class=\"kg-image\" alt=\"Geometric diagrams with labeled circles A, B, and C connected by lines against a contrasting background.\" loading=\"lazy\" width=\"2000\" height=\"808\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-6.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-6.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/05/image-6.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/05/image-6.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Расстояние Хэмминга на кубе. Слева: Расстояние от A до B равно 1. Справа: Расстояние от B до C равно 2.</span></figcaption></figure><p>Назовем <em>A<sub>bin</sub></em>, <em>B<sub>bin</sub></em> и <em>C<sub>bin</sub></em> бинаризованными версиями <em>A</em>, <em>B</em> и <em>C</em>.</p>\n<p>Для бинарных векторов, если косинусное расстояние между <em>A<sub>bin</sub></em> и <em>B<sub>bin</sub></em> больше, чем между <em>B<sub>bin</sub></em> и <em>C<sub>bin</sub></em>, тогда расстояние Хэмминга между <em>A<sub>bin</sub></em> и <em>B<sub>bin</sub></em> больше или равно расстоянию Хэмминга между <em>B<sub>bin</sub></em> и <em>C<sub>bin</sub></em>.</p>\n<p>Таким образом, если: </p><p>$\\cos(A,B) &gt; \\cos(B,C)$</p><p>тогда для расстояний Хэмминга: </p><p>$hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>В идеале, когда мы бинаризуем embeddings, мы хотим, чтобы те же отношения с полными embeddings сохранялись для бинарных embeddings, как и для полных. Это означает, что если одно расстояние больше другого для косинуса с плавающей точкой, оно должно быть больше для расстояния Хэмминга между их бинаризованными эквивалентами:</p><p>$\\cos(A,B) &gt; \\cos(B,C) \\Rightarrow hamm(A{bin}, B{bin}) \\geq hamm(B{bin}, C{bin})$</p><p>Мы не можем сделать это верным для всех троек embeddings, но можем сделать это верным почти для всех из них.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png\" class=\"kg-image\" alt=\"Graph with labeled points A and B, connected by lines marked as 'hamm AB' and 'cos AB', on a black background.\" loading=\"lazy\" width=\"1500\" height=\"1184\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/05/image-8.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Синие точки соответствуют полным векторам с плавающей точкой, а красные — их бинаризованным эквивалентам. </span></figcaption></figure><p>С бинарным вектором мы можем рассматривать каждое измерение как присутствующее (единица) или отсутствующее (ноль). Чем дальше два вектора друг от друга в небинарной форме, тем выше вероятность того, что в любом одном измерении у одного будет положительное значение, а у другого — отрицательное. Это означает, что в бинарной форме, скорее всего, будет больше измерений, где у одного будет ноль, а у другого — единица. Это делает их дальше друг от друга по расстоянию Хэмминга.</p><p>Противоположное применимо к векторам, которые ближе друг к другу: Чем ближе небинарные векторы, тем выше вероятность того, что в любом измерении у обоих будут нули или у обоих будут единицы. Это делает их ближе по расстоянию Хэмминга.</p><p>Модели Jina Embeddings так хорошо подходят для бинаризации, потому что мы обучаем их с использованием negative mining и других практик тонкой настройки, чтобы особенно увеличить расстояние между непохожими вещами и уменьшить расстояние между похожими. Это делает embeddings более устойчивыми, более чувствительными к сходствам и различиям, и делает расстояние Хэмминга между бинарными embeddings более пропорциональным косинусному расстоянию между небинарными.</p><h2 id=\"how-much-can-i-save-with-jina-ais-binary-embeddings\">Сколько можно сэкономить с Binary Embeddings от Jina AI?</h2><p>Использование моделей binary embedding от Jina AI не только снижает задержку в чувствительных ко времени приложениях, но также дает значительные преимущества в стоимости, как показано в таблице ниже:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Memory per<br/>250 million<br/>embeddings</th>\n<th>Retrieval<br/>benchmark<br/>average</th>\n<th>Estimated price on AWS<br/>($3.8 per GB/month<br/>with x2gb instances)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32-bit floating point embeddings</td>\n<td>715 GB</td>\n<td>47.13</td>\n<td>$35,021</td>\n</tr>\n<tr>\n<td>Binary embeddings</td>\n<td>22.3 GB</td>\n<td>42.05</td>\n<td>$1,095</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html--><p>Это сокращение более чем на 95% сопровождается лишь ~10% снижением точности поиска.</p><p>Эта экономия даже больше, чем при использовании бинаризованных векторов из <a href=\"https://platform.openai.com/docs/guides/embeddings/embedding-models?ref=jina-ai-gmbh.ghost.io\">модели OpenAI Ada 2</a> или <a href=\"https://cohere.com/blog/introducing-embed-v3?ref=jina-ai-gmbh.ghost.io\">Cohere Embed v3</a>, которые производят эмбеддинги размерностью 1024 или более. Эмбеддинги Jina AI имеют только 768 измерений и при этом работают сопоставимо с другими моделями, делая их меньше даже до квантования при той же точности.</p><div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Бинарные векторы экономят память, вычислительное время, пропускную способность передачи и дисковое пространство, обеспечивая финансовые преимущества в ряде категорий</strong></b>. </div></div><p>Эта экономия также экологична, используя меньше редких материалов и энергии.</p><h2 id=\"get-started\">Начало работы</h2><p>Чтобы получить бинарные эмбеддинги с помощью <a href=\"https://jina.ai/embveddings?ref=jina-ai-gmbh.ghost.io\" rel=\"noopener noreferrer\">Jina Embeddings API</a>, просто добавьте параметр <code>encoding_type</code> в ваш API-запрос, со значением <code>binary</code> для получения бинаризованного эмбеддинга, закодированного как знаковые целые числа, или <code>ubinary</code> для беззнаковых целых чисел.</p><h3 id=\"directly-access-jina-embedding-api\">Прямой доступ к Jina Embedding API</h3><p>Использование <code>curl</code>:</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR API KEY&gt;\" \\\n  -d '{\n    \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n    \"model\": \"jina-embeddings-v2-base-en\",\n    \"encoding_type\": \"binary\"\n  }'\n</code></pre><p>Или через Python API <code>requests</code>:</p><pre><code class=\"language-Python\">import requests\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": \"Bearer &lt;YOUR API KEY&gt;\"\n}\n\ndata = {\n  \"input\": [\"Your text string goes here\", \"You can send multiple texts\"],\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"encoding_type\": \"binary\",\n}\n\nresponse = requests.post(\n    \"https://api.jina.ai/v1/embeddings\", \n    headers=headers, \n    json=data,\n)\n</code></pre><p>С помощью вышеприведенного Python <code>request</code> вы получите следующий ответ при проверке <code>response.json()</code>:</p><pre><code class=\"language-JSON\">{\n  \"model\": \"jina-embeddings-v2-base-en\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"total_tokens\": 14,\n    \"prompt_tokens\": 14\n  },\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.14528547,\n        -1.0152762,\n        ...\n      ]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [\n        -0.109809875,\n        -0.76077706,\n        ...\n      ]\n    }\n  ]\n}\n</code></pre><p>Это два бинарных вектора эмбеддинга, хранящиеся как 96 8-битных знаковых целых чисел. Чтобы распаковать их в 768 нулей и единиц, вам нужно использовать библиотеку <code>numpy</code>:</p><pre><code class=\"language-Python\">import numpy as np\n\n# assign the first vector to embedding0\nembedding0 = response.json()['data'][0]['embedding']\n\n# convert embedding0 to a numpy array of unsigned 8-bit ints\nuint8_embedding = np.array(embedding0).astype(numpy.uint8) \n\n# unpack to binary\nnp.unpackbits(uint8_embedding)\n</code></pre><p>Результат - это 768-мерный вектор, содержащий только 0 и 1:</p><pre><code class=\"language-Python\">array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n      dtype=uint8)\n</code></pre><h3 id=\"using-binary-quantization-in-qdrant\">Использование бинарного квантования в Qdrant</h3><p>Вы также можете использовать <a href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\">интеграционную библиотеку Qdrant</a> для прямого размещения бинарных эмбеддингов в вашем векторном хранилище Qdrant. Поскольку Qdrant внутренне реализовал <code>BinaryQuantization</code>, вы можете использовать его как предустановленную конфигурацию для всей векторной коллекции, что позволяет извлекать и хранить бинарные векторы без каких-либо других изменений в вашем коде.</p><p>Смотрите пример кода ниже, как это сделать:</p><pre><code class=\"language-Python\">import qdrant_client\nimport requests\n\nfrom qdrant_client.models import Distance, VectorParams, Batch, BinaryQuantization, BinaryQuantizationConfig\n\n# Укажите API-ключ Jina и выберите одну из доступных моделей.\n# Вы можете получить пробный ключ здесь: https://jina.ai/embeddings/\nJINA_API_KEY = \"jina_xxx\"\nMODEL = \"jina-embeddings-v2-base-en\"  # или \"jina-embeddings-v2-base-en\"\nEMBEDDING_SIZE = 768  # 512 для малого варианта\n\n# Получаем эмбеддинги через API\nurl = \"https://api.jina.ai/v1/embeddings\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n}\n\ntext_to_encode = [\"Your text string goes here\", \"You can send multiple texts\"]\ndata = {\n    \"input\": text_to_encode,\n    \"model\": MODEL,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nembeddings = [d[\"embedding\"] for d in response.json()[\"data\"]]\n\n\n# Индексируем эмбеддинги в Qdrant\nclient = qdrant_client.QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"MyCollection\",\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT, on_disk=True),\n    quantization_config=BinaryQuantization(binary=BinaryQuantizationConfig(always_ram=True)),\n)\n\nclient.upload_collection(\n    collection_name=\"MyCollection\",\n    ids=list(range(len(embeddings))),\n    vectors=embeddings,\n    payload=[\n            {\"text\": x} for x in text_to_encode\n    ],\n)</code></pre><p>Для настройки поиска следует использовать параметры <code>oversampling</code> и <code>rescore</code>:</p><pre><code class=\"language-python\">from qdrant_client.models import SearchParams, QuantizationSearchParams\n\nresults = client.search(\n    collection_name=\"MyCollection\",\n    query_vector=embeddings[0],\n    search_params=SearchParams(\n        quantization=QuantizationSearchParams(\n            ignore=False,\n            rescore=True,\n            oversampling=2.0,\n        )\n    )\n)</code></pre><h3 id=\"using-llamaindex\">Использование LlamaIndex</h3><p>Чтобы использовать бинарные эмбеддинги Jina с LlamaIndex, установите параметр <code>encoding_queries</code> в значение <code>binary</code> при создании объекта <code>JinaEmbedding</code>:</p><pre><code class=\"language-python\">from llama_index.embeddings.jinaai import JinaEmbedding\n\n# Вы можете получить пробный ключ на https://jina.ai/embeddings/\nJINA_API_KEY = \"&lt;YOUR API KEY&gt;\"\n\njina_embedding_model = JinaEmbedding(\n    api_key=jina_ai_api_key,\n    model=\"jina-embeddings-v2-base-en\",\n    encoding_queries='binary',\n    encoding_documents='float'\n)\n\njina_embedding_model.get_query_embedding('Query text here')\njina_embedding_model.get_text_embedding_batch(['X', 'Y', 'Z'])\n</code></pre><h3 id=\"other-vector-databases-supporting-binary-embeddings\">Другие векторные базы данных с поддержкой бинарных эмбеддингов</h3><p>Следующие векторные базы данных обеспечивают нативную поддержку бинарных векторов:</p><ul><li><a href=\"https://thenewstack.io/why-vector-size-matters/?ref=jina-ai-gmbh.ghost.io\">AstraDB от DataStax</a></li><li><a href=\"https://github.com/facebookresearch/faiss/wiki/Binary-indexes?ref=jina-ai-gmbh.ghost.io\">FAISS</a></li><li><a href=\"https://milvus.io/docs/index.md?ref=cohere-ai.ghost.io#BIN_IVF_FLAT\">Milvus</a></li><li><a href=\"https://blog.vespa.ai/billion-scale-knn/?ref=jina-ai-gmbh.ghost.io\">Vespa.ai</a></li><li><a href=\"https://weaviate.io/developers/weaviate/configuration/bq-compression?ref=jina-ai-gmbh.ghost.io\">Weaviate</a></li></ul><h2 id=\"example\">Пример</h2><p>Чтобы продемонстрировать бинарные эмбеддинги в действии, мы взяли подборку аннотаций с <a href=\"http://arxiv.org/?ref=jina-ai-gmbh.ghost.io\">arXiv.org</a> и получили для них как 32-битные векторы с плавающей точкой, так и бинарные векторы, используя <code>jina-embeddings-v2-base-en</code>. Затем мы сравнили их с эмбеддингами для примера запроса: \"3D segmentation\".</p><p>Как видно из таблицы ниже, первые три ответа совпадают, и четыре из пяти лучших совпадений одинаковы. Использование бинарных векторов дает практически идентичные топовые совпадения.</p>\n<!--kg-card-begin: html-->\n<table>\n<head>\n<tr>\n  <th/>\n  <th colspan=\"2\">Binary</th>\n  <th colspan=\"2\">32-bit Float</th>\n</tr>\n<tr>\n<th>Rank</th>\n<th>Hamming<br/>dist.</th>\n<th>Matching Text</th>\n<th>Cosine</th>\n<th>Matching text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0.1862</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n<td>0.2340</td>\n<td>SEGMENT3D: A Web-based<br/>Application for Collaboration...</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.2148</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n<td>0.2857</td>\n<td>Segmentation-by-Detection:<br/>A Cascade Network for...</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.2174</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n<td>0.2973</td>\n<td>Vox2Vox: 3D-GAN for Brain<br/>Tumour Segmentation...</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.2318</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology Search...</td>\n<td>0.2983</td>\n<td>Anisotropic Mesh Adaptation for<br/>Image Segmentation...</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.2331</td>\n<td>Data-Driven Segmentation of<br/>Post-mortem Iris Image...</td>\n<td>0.3019</td>\n<td>DiNTS: Differentiable Neural<br/>Network Topology...</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"\"></h2>",
  "comment_id": "662665537f510100015daa2d",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/Blog-images.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-04-22T15:25:39.000+02:00",
  "updated_at": "2024-10-22T07:51:49.000+02:00",
  "published_at": "2024-05-15T16:00:57.000+02:00",
  "custom_excerpt": "32-bits is a lot of precision for something as robust and inexact as an AI model. So we got rid of 31 of them! Binary embeddings are smaller, faster and highly performant.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "65b22f4a8da8040001e173ba",
      "name": "Sofia Vasileva",
      "slug": "sofia",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
      "cover_image": null,
      "bio": null,
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "65b22f4a8da8040001e173ba",
    "name": "Sofia Vasileva",
    "slug": "sofia",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/04/sofia-profile-pic.jpeg",
    "cover_image": null,
    "bio": null,
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": null,
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/sofia/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/binary-embeddings-all-the-ai-3125-of-the-fat/",
  "excerpt": "32 бита — это слишком много точности для такой надежной и неточной вещи, как AI-модель. Поэтому мы избавились от 31 из них! Бинарные эмбеддинги меньше, быстрее и высокопроизводительны.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Futuristic digital 3D model of a coffee grinder with blue neon lights on a black background, featuring numerical data.",
  "feature_image_caption": null
}
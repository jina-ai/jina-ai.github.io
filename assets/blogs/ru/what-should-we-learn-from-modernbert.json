{
  "slug": "what-should-we-learn-from-modernbert",
  "id": "678cc6a18f6bb40001a63537",
  "uuid": "fde6f3d6-20f1-4f8e-b811-ab6e2880a9c6",
  "title": "Что нам следует извлечь из ModernBERT?",
  "html": "<p>В 2018 году Google выпустил BERT, что стало переломным моментом для NLP, задолго до нынешней волны LLM. Даже сейчас множество Small Language Models построены на основе BERT. В декабре 2024 года <a href=\"https://huggingface.co/blog/modernbert?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">ModernBERT</a> берет все, что мы узнали из недавних разработок LLM, и применяет это к этим меньшим моделям. Ключевые изменения? Лучшая эффективность параметров, понимание кода и обработка длинного контекста.</p><p>В этой статье мы разберем, как ModernBERT сравнивается с двумя моделями, которые мы знаем вдоль и поперек: <code>jina-XLM-RoBERTa</code> (многоязычная основа <code>jina-embeddings-v3</code>) и <code>RoBERTa-large</code>. Давайте рассмотрим каждую модель:</p><ul><li><strong>ModernBERT </strong>(декабрь 2024) — недавно выпущенная SLM, разработанная совместно Answer.AI, LightOn и HuggingFace. Она использует современные оптимизации, такие как RoPE для контекстного окна в 8 192 токена и <a href=\"https://arxiv.org/abs/2002.05202?ref=jina-ai-gmbh.ghost.io\">слои GeGLU</a>, повышая производительность при сохранении эффективности.</li><li><a href=\"https://huggingface.co/jinaai/xlm-roberta-flash-implementation?ref=jina-ai-gmbh.ghost.io\"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong> </strong>(сентябрь 2024) — это многоязычная модель текстовых эмбеддингов на основе <a href=\"https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta?ref=jina-ai-gmbh.ghost.io\"><code>XLM-RoBERTa</code></a> от Meta. В то время как оригинальная <code>XLM-RoBERTa</code> улучшает <code>RoBERTa</code> с помощью большого многоязычного набора данных XLM, <code>jina-XLM-RoBERTa</code> идет дальше с расширенным контекстным обучением, реализацией <a href=\"https://arxiv.org/abs/2104.09864?ref=jina-ai-gmbh.ghost.io\">RoPE</a> и поддержкой <a href=\"https://arxiv.org/abs/2307.08691?ref=jina-ai-gmbh.ghost.io\">FlashAttention-2</a>. Эта модель служит основой для <code>jina-embeddings-v3</code>.</li><li><a href=\"https://huggingface.co/FacebookAI/roberta-large?ref=jina-ai-gmbh.ghost.io\"><strong><code>RoBERTa-large</code></strong></a> (июль 2019), разработанная Meta, является улучшенной версией BERT с 355 миллионами параметров. Благодаря расширенному обучению, большим наборам данных и инновациям, таким как динамическое маскирование, она достигла впечатляющих результатов в ключевых бенчмарках, включая <a href=\"https://gluebenchmark.com/?ref=jina-ai-gmbh.ghost.io\">GLUE</a>, <a href=\"https://rajpurkar.github.io/SQuAD-explorer/?ref=jina-ai-gmbh.ghost.io\">SQuAD</a> и <a href=\"https://arxiv.org/abs/1704.04683?ref=jina-ai-gmbh.ghost.io\">RACE</a>. Это делает ее подходящей для различных задач NLP от классификации текста до ответов на вопросы.</li></ul><p>Сравнивая эти модели по трем основным аспектам, мы стремимся подчеркнуть эффективные проектные решения ModernBERT для разработчиков моделей и определить ключевые идеи разработки для будущих моделей типа BERT. Мы также поделимся нашим опытом разработки <code>jina-embeddings-v3</code> и обсудим планируемые улучшения для <code>jina-embeddings-v4</code> и <code>jina-reranker-v3</code>.</p><h2 id=\"modernberts-parameter-efficiency\">Эффективность параметров ModernBERT</h2><p>Давайте сначала рассмотрим подход ModernBERT к эффективности параметров — он включает несколько ключевых идей из недавних разработок LLM. ModernBERT использует три основные стратегии: более глубокую, но более тонкую архитектуру, контролируемый размер словаря и прогрессивное масштабирование модели, начиная с меньших моделей.</p><h3 id=\"deep-and-thin-architecture\">Глубокая и тонкая архитектура</h3><p>ModernBERT-large становится глубже с 28 слоями, в то время как <code>jina-XLM-RoBERTa</code> и <code>RoBERTa-large</code> работают с 24. Но вот что интересно — она соответствует <code>RoBERTa-large</code> по количеству параметров, несмотря на эти дополнительные слои. <code>jina-XLM-RoBERTa</code> требует больше параметров, поскольку обрабатывает 89 языков, в то время как другие две фокусируются только на английском.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1389\" height=\"547\"><figcaption><span style=\"white-space: pre-wrap;\">Глубина (количество слоев) важнее ширины (количество скрытых единиц) для малых LLM. Глубокая и тонкая структура модели лучше улавливает абстрактные концепции, что приводит к превосходной конечной производительности.</span></figcaption></figure><p>Большинство параметров трансформера приходится на слои внимания и полносвязные слои. ModernBERT остается конкурентоспособной по размеру, становясь \"тоньше\" — они используют 2 624 скрытых единицы в 28 слоях, по сравнению с 4 096 единицами в 24 слоях у RoBERTa-large. Эта более \"глубокая\", но тонкая настройка позволяет им достигать целевых показателей производительности без раздувания модели.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parameters</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Hidden states</td>\n<td>1,024</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Intermediate dims</td>\n<td>2,624</td>\n<td>4,096</td>\n<td>4,096</td>\n</tr>\n<tr>\n<td>Attention heads</td>\n<td>16</td>\n<td>16</td>\n<td>16</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>28</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Vocabulary size</td>\n<td>50,368</td>\n<td>250,002</td>\n<td>50,265</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Этот подход согласуется с исследованием Meta <a href=\"https://openreview.net/pdf?id=EIGbXbxcUQ&ref=jina-ai-gmbh.ghost.io\">MobileLLM</a>, которое обнаружило, что для меньших моделей глубина важнее ширины, когда речь идет о захвате сложных паттернов и повышении производительности. По сути, способность обрабатывать информацию через большее количество слоев трансформера оказывается более ценной, чем наличие более широких слоев для параллельной обработки.</p><p>Давайте посмотрим на данные о том, как работает эта глубокая и тонкая архитектура.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"872\" height=\"371\"><figcaption><span style=\"white-space: pre-wrap;\">В сравнении с аналогичными моделями, использующими традиционную мелкую и широкую архитектуру, ModernBERT показывает лучшие результаты в ключевых задачах, таких как поиск и STS — при этом сохраняя аналогичное количество параметров.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS12</td>\n<td>72.6</td>\n<td><strong>72.7</strong></td>\n<td>68.9</td>\n</tr>\n<tr>\n<td>STS13</td>\n<td><strong>84.9</strong></td>\n<td>83.9</td>\n<td>81.0</td>\n</tr>\n<tr>\n<td>STS14</td>\n<td>77.5</td>\n<td><strong>77.7</strong></td>\n<td>74.8</td>\n</tr>\n<tr>\n<td>STS15</td>\n<td>84.8</td>\n<td><strong>85.8</strong></td>\n<td>84.1</td>\n</tr>\n<tr>\n<td>STS16</td>\n<td>79.4</td>\n<td><strong>79.6</strong></td>\n<td>78.6</td>\n</tr>\n<tr>\n<td>STS17</td>\n<td><strong>87.5</strong></td>\n<td>87.2</td>\n<td>87.2</td>\n</tr>\n<tr>\n<td>TRECCOVID</td>\n<td><strong>61.1</strong></td>\n<td>59.6</td>\n<td>49.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td><strong>44.4</strong></td>\n<td>40.0</td>\n<td>40.7</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td><strong>32.6</strong></td>\n<td>30.6</td>\n<td>27.9</td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td><strong>68.6</strong></td>\n<td>65.5</td>\n<td>63.1</td>\n</tr>\n<tr>\n<td>Average</td>\n<td><strong>69.3</strong></td>\n<td>68.2</td>\n<td>65.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Возьмем <code>jina-XLM-RoBERTa</code> — она основывается на мелкой и широкой архитектуре <code>RoBERTa-large</code>, но увеличивает словарь с 50K до 250K токенов и обучается на большем количестве данных. Тем не менее, ModernBERT все равно превосходит ее, что говорит о том, что архитектурное изменение действительно влияет на эффективность.</p><h3 id=\"vocabulary-size-matters\">Размер словаря имеет значение</h3><p>Сначала давайте посмотрим, как считаются параметры словаря в трансформерах. Для любого трансформера <code>параметры словаря = количество различных токенов × размер скрытого состояния</code>. Возьмем <code>jina-XLM-RoBERTa</code>: с 250K токенов и 1 024 измерениями ей требуется 256M параметров только для кодирования словаря — до обработки каких-либо фактических языковых задач!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3757\" height=\"715\"><figcaption><span style=\"white-space: pre-wrap;\">В трансформерах первый слой отображает токены в скрытые состояния, используя матрицу весов, а именно веса словаря. Если использовать все кодовые точки UTF-8 (1 112 064) с 1 024 скрытыми измерениями, потребуется огромное количество параметров - </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>1,112,064 × 1,024 = 1 B</span></code><span style=\"white-space: pre-wrap;\"> только для преобразования токенов. Хотя более крупные LLM (100B+ параметров) могут справиться с этими накладными расходами, для меньших моделей это серьезное ограничение. Именно поэтому мы используем токенизаторы вроде BPE, которые эффективно объединяют часто встречающиеся кодовые точки UTF-8 в единые токены.</span></figcaption></figure><p>Но вот в чем дело: <strong>веса словаря не участвуют в механизмах внимания - это просто таблицы поиска.</strong> Для SLM, работающих с фиксированным бюджетом параметров, больший словарь означает меньше параметров для слоев внимания, которые выполняют фактическую обработку языка. Это объясняет, почему англоязычный ModernBERT-large превосходит многоязычный <code>jina-XLM-RoBERTa</code>, несмотря на меньший размер - <code>jina-XLM-RoBERTa</code> выделяет больше параметров (47%!) для поддержки нескольких языков. Сфокусированный словарь ModernBERT не только улучшает производительность, но и ускоряет вывод, что делает его особенно эффективным для приложений с ограниченными ресурсами.</p><p>Если посмотреть <em>только</em> на основные параметры модели (исключая веса словаря), ModernBERT фактически обладает большей вычислительной мощностью, чем его аналоги: ModernBERT выделяет на 19% больше параметров для <em>фактического</em> языкового моделирования, чем <code>jina-XLM-RoBERTa</code>, и на 15% больше, чем <code>RoBERTa-large</code>!</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Характеристики модели</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Поддержка языков</td>\n<td>Только английский</td>\n<td>89 языков</td>\n<td>Только английский</td>\n</tr>\n<tr>\n<td>Размер словаря</td>\n<td>50.4K</td>\n<td>250K</td>\n<td>50.3K</td>\n</tr>\n<tr>\n<td>Всего параметров</td>\n<td>400M</td>\n<td>550M</td>\n<td>355M</td>\n</tr>\n<tr>\n<td>Параметры словаря</td>\n<td>51M</td>\n<td>256M</td>\n<td>51M</td>\n</tr>\n<tr>\n<td>Доля параметров словаря</td>\n<td>13%</td>\n<td>47%</td>\n<td>14%</td>\n</tr>\n<tr>\n<td>Основные параметры модели</td>\n<td><b>349M</b></td>\n<td>294M</td>\n<td>304M</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"model-upscaling-by-weight-tiling\">Масштабирование модели с помощью \"Weight Tiling\"</h3><p>При создании базовой модели <a href=\"https://huggingface.co/jinaai/jina-bert-implementation?ref=jina-ai-gmbh.ghost.io\"><code>jina-BERT-v2</code></a> мы обнаружили, что обучение SLM с нуля требует много ресурсов и сложно. ModernBERT решает эту проблему с помощью умного подхода к инициализации, называемого <strong>weight tiling</strong> - по сути, загружая ModernBERT-large из весов его меньшей базовой версии.</p><p>Эта техника не совсем нова - она основана на работе DeepMind с <a href=\"https://gpt3demo.com/apps/deepmind-gopher?ref=jina-ai-gmbh.ghost.io\">Gopher</a> и также присутствует в моделях Microsoft <a href=\"https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?ref=jina-ai-gmbh.ghost.io\">Phi-2</a>. Но ее применение здесь особенно эффективно для решения проблемы узкого места при обучении SLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1877\" height=\"1308\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT масштабируется с 22 до 28 слоев, используя стратегию инициализации глубины команды Gopher. Для этих дополнительных слоев (23-28) они инициализируют каждый, используя веса из исходных 22 слоев ModernBERT-base. Для весовых матриц каждого слоя они используют подход центрального тайлинга Phi-2. Это работает так: они берут веса ModernBERT-base и помещают их прямо в центр матриц ModernBERT-large. А пустые края? Они циклически заполняют их исходными весами.</span></figcaption></figure><p>Эта стратегия инициализации дает ModernBERT-large значительное преимущество - вместо холодного старта с нуля он использует предварительно изученные паттерны из своего меньшего аналога. Это оказалось особенно <a href=\"https://arxiv.org/pdf/2112.11446?ref=jina-ai-gmbh.ghost.io\">эффективным для масштабирования языковых моделей в этом диапазоне размеров</a>.</p><blockquote>Мы обнаружили, что модель с теплым стартом быстро восстанавливается после высокой начальной потери (из-за добавленных параметров) до потери, близкой к базовой модели. Нам удалось расширить 417M параметров более чем в 3 раза по размеру и сохранить производительность выше, чем у эквивалентной свежей модели, обученной с нуля до сходимости, что подразумевает, что выигрыш не ограничивался началом обучения. Однако при больших размерах относительные выигрыши при сходимости уменьшаются, особенно при расширении ширины.</blockquote><p>Циклическое обертывание весов - это не просто удобство, оно хорошо согласуется с тем, как матрицы внимания естественным образом проявляют периодические паттерны. Исследование Gopher показывает, что этот подход действительно блистает для SLM (менее 9B параметров), хотя преимущества начинают уменьшаться при переходе к более крупным моделям.</p><h2 id=\"modernberts-code-modeling\">Моделирование кода в ModernBERT</h2><p>ModernBERT предлагает специализированный подход к пониманию кода с помощью оптимизированного для кода токенизатора и обучающих данных. Эта точная настройка для обработки кода окупается как в задачах понимания, так и в задачах поиска.</p><p>Мы провели тестирование с использованием корпуса <code>jina-embeddings-v2-code</code>, сравнивая три модели в качестве основы: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code> и <code>RoBERTa-large</code>. Тест? <a href=\"https://github.com/github/CodeSearchNet?ref=jina-ai-gmbh.ghost.io\">CodeSearchNet</a> - сопоставление текстовых описаний с фрагментами кода. ModernBERT превзошел обе альтернативы по всем показателям.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"787\" height=\"489\"><figcaption><span style=\"white-space: pre-wrap;\">Разрыв логичен - ни </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> ни </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\"> не видели языков программирования во время обучения. Между тем, ModernBERT-large обучался на двух триллионах токенов, включая значительное количество кода. Этот опыт работы с синтаксисом и паттернами программирования дает ему явное преимущество в задачах, связанных с кодом. </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> немного опережает </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>RoBERTa-large</span></code><span style=\"white-space: pre-wrap;\">, вероятно, благодаря большему объему многоязычных обучающих данных - та же архитектура, больше опыта. Тем не менее, обе значительно отстают от ModernBERT-large.</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Задача</th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n<th><code>RoBERTa-large</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AdvRetrieval</td>\n<td>0.342</td>\n<td><strong>0.363</strong></td>\n<td>0.331</td>\n</tr>\n<tr>\n<td>QueryRetrieval.python</td>\n<td>0.521</td>\n<td><strong>0.530</strong></td>\n<td>0.525</td>\n</tr>\n<tr>\n<td>QueryRetrieval java</td>\n<td><strong>0.679</strong></td>\n<td>0.633</td>\n<td>0.644</td>\n</tr>\n<tr>\n<td>QueryRetrieval.javascript</td>\n<td>0.755</td>\n<td><strong>0.768</strong></td>\n<td>0.732</td>\n</tr>\n<tr>\n<td>QueryRetrieval.php</td>\n<td><strong>0.815</strong></td>\n<td>0.781</td>\n<td>0.755</td>\n</tr>\n<tr>\n<td>QueryRetrieval.ruby</td>\n<td>0.729</td>\n<td><strong>0.744</strong></td>\n<td>0.722</td>\n</tr>\n<tr>\n<td>QueryRetrieval.go</td>\n<td><strong>0.833</strong></td>\n<td>0.809</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.go</td>\n<td><strong>0.778</strong></td>\n<td>0.750</td>\n<td>0.759</td>\n</tr>\n<tr>\n<td>Retrieval.java</td>\n<td><strong>0.840</strong></td>\n<td>0.792</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.javascript</td>\n<td><strong>0.817</strong></td>\n<td>0.792</td>\n<td>0.757</td>\n</tr>\n<tr>\n<td>Retrieval.php</td>\n<td><strong>0.852</strong></td>\n<td>0.805</td>\n<td>0.796</td>\n</tr>\n<tr>\n<td>Retrieval.python</td>\n<td><strong>0.849</strong></td>\n<td>0.816</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>Retrieval.ruby</td>\n<td><strong>0.849</strong></td>\n<td>0.796</td>\n<td>0.803</td>\n</tr>\n<tr>\n<td>Avg.</td>\n<td><strong>0.743</strong></td>\n<td>0.721</td>\n<td>0.708</td>\n</tr>\n</tbody>\n</table>\n\n<h3 id=\"the-tokenizer-edge\">Преимущество токенизатора</h3>\n\n<p>Давайте разберемся, почему ModernBERT так хорошо справляется с кодом - он использует <a href=\"https://huggingface.co/docs/transformers/en/model_doc/olmo?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">токенизатор OLMo</a>, который был специально обучен на коде, в отличие от стандартных токенизаторов BERT/RoBERTa.</p>\n\n<p>Токенизатор разбивает UTF-8 текст на токены, которые затем отображаются в векторы - именно с ними работает модель. В процессе обучения он учится объединять часто встречающиеся последовательности символов в отдельные токены. В чем разница? Стандартный токенизатор может разбить <code>init</code> на <code>in</code> + <code>it</code>, упуская программный контекст. Но токенизатор ModernBERT, понимающий код, обрабатывает его целиком.</p>\n\n<p>Интересный момент в обработке пробелов: ModernBERT сохраняет начальные пробелы Python как отдельные токены и различает 4 и 8 пробелов - что критически важно для структуры кода. В то же время, <strong><code>jina-XLM-RoBERTa</code> объединяет все последовательные пробелы в один <code>_</code>, а RoBERTa-large обрабатывает каждый пробел как отдельный токен.</strong> Это означает, что энкодер ModernBERT получает более чистый и осмысленный ввод при обработке кода, в то время как другие работают с фрагментированными, менее связными токенами.</p>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3156\" height=\"1247\"><figcaption><span style=\"white-space: pre-wrap;\">ModernBERT сохраняет начальные пробелы Python как отдельные токены и различает между 4 и 8 пробелами - что критически важно для структуры кода; в то время как другие работают с фрагментированными, менее связными токенами.</span></figcaption></figure>\n\n<h2 id=\"modernberts-long-context-handling\">Обработка длинного контекста в ModernBERT</h2>\n\n<p>ModernBERT достиг значительных успехов в обработке длинных текстов благодаря обширному обучающему корпусу (300 млрд токенов с образцами по 8192 токена) и продвинутым методам, таким как комбинация глобального и локального внимания.</p>\n\n<p>Для оценки возможностей обработки длинных документов мы использовали <a href=\"https://huggingface.co/datasets/Shitao/MLDR?ref=jina-ai-gmbh.ghost.io\">датасет MLDR</a> - комплексный бенчмарк длинных текстов на 13 языках. Поскольку ModernBERT в настоящее время поддерживает только английский язык, мы сосредоточились на английской части MLDR для сравнения ModernBERT с <code>jina-XLM-RoBERTa</code>. Хотя обе эти модели могут обрабатывать входные данные размером 8 тыс. токенов, <code>RoBERTa-large</code> была исключена из этого бенчмарка из-за ограничения в 512 токенов, что недостаточно для анализа длинных текстов.</p>\n\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>ModernBERT-large</th>\n<th><code>jina-XLM-RoBERTa</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MLDR-en</td>\n<td><strong>0.351</strong></td>\n<td>0.290</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n\n<p>Превосходная производительность ModernBERT обусловлена не только обширным обучением на длинных текстах - во многом это заслуга инновационной комбинации механизмов глобального и локального внимания. В отличие от <code>jina-XLM-RoBERTa</code>, которая применяет вычислительно затратное глобальное внимание к каждому слою, ModernBERT использует более эффективный подход. Он чередует глобальное внимание (используется каждый третий слой с <code>theta</code> 160 000) и локальное внимание (использует скользящее окно в 128 токенов с <code>theta</code> 100 000). Эта гибридная стратегия поддерживает высокую производительность при значительном сокращении времени обучения.</p>\n\n<blockquote>В ModernBERT каждый третий слой использует глобальное внимание с RoPE theta 160 000, а остальные слои используют локальное скользящее окно внимания размером 128 токенов с RoPE theta 10 000. —— <a href=\"https://arxiv.org/pdf/2412.13663?ref=jina-ai-gmbh.ghost.io\">ModernBERT</a></blockquote>\n\n<h2 id=\"the-bitter-lesson\">Горький урок?</h2>\n\n<p>Закон масштабирования и <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=jina-ai-gmbh.ghost.io\">горький урок</a> предполагают, что основные улучшения производительности происходят в первую очередь за счет увеличения количества параметров и обучающих данных. Этот принцип определил наш подход к расширению корпуса и использованию LoRA для адаптации к конкретным задачам.</p>\n\n<p>Однако успех ModernBERT показал, что мы недооценили силу архитектурной оптимизации. Он демонстрирует, что SLM могут достигать исключительных результатов за счет лучшей эффективности использования данных и модели, не обязательно увеличивая количество параметров. Недавний <a href=\"https://arxiv.org/pdf/2408.11868?ref=jina-ai-gmbh.ghost.io\">технический отчет Stella Embeddings</a> подтверждает этот вывод, указывая, что текущие методы обучения моделей эмбеддингов можно улучшить без увеличения размера корпуса или модели.</p>\n\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"График, показывающий закон масштабирования моделей эмбеддинга с 'Размером параметров' по оси x и 'Производительностью MTEB' по оси y\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Закон масштабирования моделей эмбеддинга. Средняя производительность MTEB на английских задачах отображена относительно количества параметров модели. Каждая точка представляет модель эмбеддинга. Линия тренда, представляющая все модели, выделена, с многоязычными моделями, выделенными голубым цветом. Можно видеть, что </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> демонстрирует превосходную производительность по сравнению с моделями аналогичного размера, также показывая сверхлинейное улучшение по сравнению со своим предшественником, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">. Этот график был создан путем выбора топ-100 моделей эмбеддинга из рейтинга MTEB, исключая те, для которых отсутствует информация о размере, обычно закрытые или проприетарные модели. Также были отфильтрованы заявки, определенные как очевидный троллинг.</span></figcaption></figure>\n\n<p>В дальнейшем мы ожидаем снижения вычислительных затрат и уменьшения размеров моделей по мере того, как мы получаем более глубокое понимание использования данных и внедряем методы ModernBERT. В краткосрочной перспективе мы можем реализовать простые улучшения, описанные в статье о ModernBERT - в частности, интеграцию большего количества данных, связанных с кодом, и принятие токенизатора, дружественного к коду. Более сложные изменения, такие как переход к глубокой и тонкой архитектуре или начальная загрузка больших моделей из меньших, потребуют создания базовых моделей с нуля - это более среднесрочная инициатива.</p>\n\n<p>Хотя эффективность ModernBERT впечатляет, его ограничение только текстом указывает на будущие вызовы. По мере роста популярности мультимодальных моделей эмбеддинга наша следующая задача - разработка более умных, быстрых и способных поисковых базовых моделей, которые могут обрабатывать входные данные для мультимодальных приложений. Эти приложения требуют еще более длинных окон контекста - проблема эффективности, которую еще предстоит решить.</p>\n\n<h2 id=\"conclusion\">Заключение</h2>\n\n<p>В этой статье мы рассмотрели, как ModernBERT продвигает модели семейства BERT через три ключевые инновации: его глубокую и тонкую архитектуру, оптимизированный токенизатор и эффективное масштабирование с использованием тайлинга весов. Эти улучшения позволяют ModernBERT демонстрировать выдающуюся производительность при относительно компактном размере, превосходя как <code>RoBERTa-large</code>, так и <code>jina-XLM-RoBERTa</code> в различных задачах. ModernBERT демонстрирует, что архитектурные улучшения могут быть важнее размера параметров, открывая двери для более эффективных моделей. Его успешное использование тайлинга весов показывает, как прогрессивное масштабирование может снизить затраты на обучение при сохранении или даже повышении производительности. Кроме того, его компактный словарь и целевые оптимизации указывают на растущие возможности для специализированных SLM в условиях ограниченных ресурсов.</p>",
  "comment_id": "678cc6a18f6bb40001a63537",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-01-19T10:32:17.000+01:00",
  "updated_at": "2025-01-22T08:31:26.000+01:00",
  "published_at": "2025-01-22T08:31:26.000+01:00",
  "custom_excerpt": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-should-we-learn-from-modernbert/",
  "excerpt": "ModernBERT задает направление для будущих BERT-подобных моделей благодаря увеличенным тренировочным данным, эффективному подбору параметров и глубокой, но компактной архитектуре.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
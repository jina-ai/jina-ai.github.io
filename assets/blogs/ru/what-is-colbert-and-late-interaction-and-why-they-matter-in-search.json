{
  "slug": "what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
  "id": "65d3a2134a32310001f5b71b",
  "uuid": "726c942b-f6a7-4c89-a0ad-39aaad98d02f",
  "title": "Что такое ColBERT и позднее взаимодействие, и почему они важны для поиска?",
  "html": "<figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina ColBERT v2: Многоязычный ретривер с поздним взаимодействием для эмбеддингов и переранжирования</div><div class=\"kg-bookmark-description\">Jina ColBERT v2 поддерживает 89 языков с превосходной производительностью поиска, настраиваемыми выходными размерностями и длиной токенов 8192.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg\" alt=\"\"></div></a><figcaption><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Обновление: 31 августа 2024 года мы выпустили вторую версию Jina-ColBERT с улучшенной производительностью, поддержкой 89 языков и гибкими выходными размерностями. Подробности смотрите в посте о релизе.</span></p></figcaption></figure><p>В прошлую пятницу выпуск <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">модели ColBERT от Jina AI на Hugging Face</a> вызвал значительный интерес в AI-сообществе, особенно в Twitter/X. В то время как многие знакомы с революционной моделью BERT, ажиотаж вокруг ColBERT заставил некоторых задаться вопросом: Чем ColBERT выделяется среди множества технологий информационного поиска? Почему AI-сообщество в восторге от ColBERT с длиной 8192? Эта статья углубляется в тонкости ColBERT и ColBERTv2, освещая их дизайн, улучшения и удивительную эффективность позднего взаимодействия ColBERT.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/reranker?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Reranker API</div><div class=\"kg-bookmark-description\">Максимизируйте релевантность поиска и точность RAG с легкостью</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina.ai/banner-reranker-api.png\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-embed-card\"><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Introducing jina-colbert-v1-en. It takes late interactions &amp; token-level embeddings of ColBERTv2 and has better zero-shot performance on many tasks (in and out-of-domain). Now on <a href=\"https://twitter.com/huggingface?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">@huggingface</a> under Apache 2.0 licence<a href=\"https://t.co/snVGgI753H?ref=jina-ai-gmbh.ghost.io\">https://t.co/snVGgI753H</a></p>— Jina AI (@JinaAI_) <a href=\"https://twitter.com/JinaAI_/status/1758503072999907825?ref_src=twsrc%5Etfw&ref=jina-ai-gmbh.ghost.io\">February 16, 2024</a></blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></figure><h2 id=\"what-is-colbert\">Что такое ColBERT?</h2><p>Название \"ColBERT\" расшифровывается как <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong> (Контекстуализированное позднее взаимодействие поверх BERT), модель, разработанная в Стэнфордском университете, которая использует глубокое понимание языка BERT, представляя новый механизм взаимодействия. Этот механизм, известный как <strong>позднее взаимодействие</strong>, позволяет осуществлять эффективный и точный поиск путем раздельной обработки запросов и документов до финальных этапов процесса поиска. Конкретно существует две версии модели:</p><ul><li><strong>ColBERT</strong>: Первоначальная модель была детищем <a href=\"https://x.com/lateinteraction?s=20&ref=jina-ai-gmbh.ghost.io\"><strong>Omar Khattab</strong></a><strong> и Matei Zaharia</strong>, представляющая новый подход к информационному поиску через статью \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\". Их работа была опубликована на SIGIR 2020.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2004.12832?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class=\"kg-bookmark-description\">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Omar Khattab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Оригинальная статья о ColBERT, представляющая \"позднее взаимодействие\".</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong>: Основываясь на фундаментальной работе, <strong>Omar Khattab</strong> продолжил свои исследования, сотрудничая с <strong>Barlas Oguz, Matei Zaharia и Michael S. Bernstein</strong>, чтобы представить \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\" на SIGIR 2021. Эта следующая итерация ColBERT решала предыдущие ограничения и вводила ключевые улучшения, такие как <strong>очищенное обучение</strong> и <strong>остаточное сжатие</strong>, повышая как эффективность поиска модели, так и её эффективность хранения.</li></ul><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2112.01488?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class=\"kg-bookmark-description\">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Keshav Santhanam</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">ColBERTv2 добавляет очищенное обучение и остаточное сжатие для улучшения качества обучающих данных и уменьшения занимаемого пространства.</span></p></figcaption></figure><h2 id=\"understand-colberts-design\">Понимание дизайна ColBERT</h2><p>Учитывая, что архитектура ColBERTv2 остается очень похожей на оригинальный ColBERT, а ключевые инновации сосредоточены вокруг методик обучения и механизмов сжатия, сначала мы рассмотрим фундаментальные аспекты оригинального ColBERT.</p><h3 id=\"what-is-late-interaction-in-colbert\">Что такое позднее взаимодействие в ColBERT?</h3><p>\"Взаимодействие\" относится к процессу оценки релевантности между запросом и документом путем сравнения их представлений.</p><p>\"<em>Позднее взаимодействие</em>\" — это суть ColBERT. Термин происходит от архитектуры модели и стратегии обработки, где взаимодействие между представлениями запроса и документа происходит поздно в процессе, после того как оба были независимо закодированы. Это контрастирует с моделями \"<em>раннего взаимодействия</em>\", где эмбеддинги запроса и документа взаимодействуют на более ранних этапах, обычно до или во время их кодирования моделью.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Models</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Early Interaction</td>\n<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>\n</tr>\n<tr>\n<td>Late Interaction</td>\n<td>ColBERT, ColBERTv2</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Раннее взаимодействие может увеличить вычислительную сложность, поскольку требует рассмотрения всех возможных пар запрос-документ, что делает его менее эффективным для крупномасштабных приложений.</p><p>Модели с поздним взаимодействием, такие как ColBERT, оптимизируют эффективность и масштабируемость, позволяя предварительно вычислять представления документов и используя более легкий шаг взаимодействия в конце, который фокусируется на уже закодированных представлениях. Этот дизайн обеспечивает более быстрое время поиска и сниженные вычислительные требования, делая его более подходящим для обработки больших коллекций документов.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg\" class=\"kg-image\" alt=\"Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep\" loading=\"lazy\" width=\"300\" height=\"143\"><figcaption><span style=\"white-space: pre-wrap;\">Схематические диаграммы, иллюстрирующие парадигмы взаимодействия запрос-документ в нейронном IR, с поздним взаимодействием ColBERT слева.</span></figcaption></figure><h3 id=\"no-interaction-cosine-similarity-of-document-and-query-embeddings\">Отсутствие взаимодействия: косинусное сходство эмбеддингов документа и запроса</h3><p>Многие практические векторные базы данных и решения для нейронного поиска основываются на быстром сопоставлении косинусного сходства между эмбеддингами документов и запросов. Хотя этот метод привлекателен своей простотой и вычислительной эффективностью, он, часто называемый \"<em>без взаимодействия</em>\" или \"<em>не основанный на взаимодействии</em>\", показывает более низкую производительность по сравнению с моделями, включающими некоторую форму взаимодействия между запросами и документами.</p><p>Основное ограничение подхода \"без взаимодействия\" заключается в его неспособности уловить сложные нюансы и взаимосвязи между терминами запроса и документа. Информационный поиск, по своей сути, заключается в понимании и сопоставлении намерения запроса с содержанием документа. Этот процесс часто требует глубокого, контекстного понимания используемых терминов, что трудно обеспечить с помощью единых, агрегированных эмбеддингов для документов и запросов.</p><h2 id=\"query-and-document-encoders-in-colbert\">Кодировщики запросов и документов в ColBERT</h2><p>Стратегия кодирования ColBERT основана на модели BERT, известной своим глубоким контекстным пониманием языка. Модель генерирует плотные векторные представления для каждого токена в запросе или документе, <strong>создавая набор контекстуализированных эмбеддингов для запроса и документа соответственно. </strong>Это обеспечивает нюансированное сравнение их эмбеддингов на этапе позднего взаимодействия.</p><h3 id=\"query-encoder-of-colbert\">Кодировщик запросов ColBERT</h3><p>Для запроса $Q$ с токенами ${q_1, q_2, ..., q_l}$ процесс начинается с токенизации $Q$ в токены WordPiece на основе BERT и добавления специального токена <code>[Q]</code>. Этот токен <code>[Q]</code>, расположенный сразу после токена <code>[CLS]</code> BERT, сигнализирует о начале запроса. </p><p>Если запрос короче предопределенного количества токенов $N_q$, он дополняется токенами <code>[mask]</code> до $N_q$; в противном случае он усекается до первых $N_q$ токенов. Дополненная последовательность затем проходит через BERT, за которым следует CNN (Сверточная Нейронная Сеть) и нормализация. Результатом является набор векторов эмбеддингов, обозначенный как $\\mathbf{E}_q$ ниже:<br>$$\\mathbf{E}_q := \\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[Q]},q_0,q_1,\\ldots,q_l\\mathtt{[mask]},\\mathtt{[mask]},\\ldots,\\mathtt{[mask]}\\right)\\right)$$</p><h3 id=\"document-encoder-of-colbert\">Кодировщик документов ColBERT</h3><p>Аналогично, для документа $D$ с токенами ${d_1, d_2, ..., d_n}$ токен <code>[D]</code> добавляется в начало для обозначения начала документа. Эта последовательность, не требующая дополнения, проходит тот же процесс, что приводит к набору векторов эмбеддингов, обозначенному как $\\mathbf{E}_d$ ниже:<br>$$\\mathbf{E}_d := \\mathrm{Filter}\\left(\\mathrm{Normalize}\\left(\\mathrm{BERT}\\left(\\mathtt{[D]},d_0,d_1,...,d_n\\right)\\right)\\right)$$</p><p>Использование токенов <code>[mask]</code> для дополнения запросов (названное в статье <strong>расширением запроса</strong>) обеспечивает единообразную длину всех запросов, облегчая пакетную обработку. Токены <code>[Q]</code> и <code>[D]</code> явно отмечают начало запросов и документов соответственно, помогая модели различать эти два типа входных данных.</p><h3 id=\"comparing-colbert-to-cross-encoders\">Сравнение ColBERT с кросс-энкодерами</h3><p>Кросс-энкодеры обрабатывают пары запросов и документов вместе, что делает их высокоточными, но менее эффективными для масштабных задач из-за вычислительных затрат на оценку каждой возможной пары. Они превосходны в конкретных сценариях, где необходима точная оценка пар предложений, например, в задачах семантического сходства или детального сравнения содержания. Однако такая конструкция ограничивает их применимость в ситуациях, требующих быстрого поиска в больших наборах данных, где критически важны предварительно вычисленные эмбеддинги и эффективные вычисления сходства.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg\" class=\"kg-image\" alt=\"Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum\" loading=\"lazy\" width=\"210\" height=\"150\"></figure><p>В отличие от этого, модель позднего взаимодействия ColBERT позволяет предварительно вычислять эмбеддинги документов, значительно ускоряя процесс поиска без ущерба для глубины семантического анализа. Этот метод, хотя и кажется нелогичным по сравнению с прямым подходом кросс-энкодеров, предлагает масштабируемое решение для задач информационного поиска в реальном времени и в крупном масштабе. Он представляет собой стратегический компромисс между вычислительной эффективностью и качеством моделирования взаимодействия.</p><h2 id=\"finding-the-top-k-documents-using-colbert\">Поиск топ-K документов с помощью ColBERT</h2><p>После получения эмбеддингов для запроса и документов, поиск наиболее релевантных топ-K документов становится прямолинейным (но не таким простым, как вычисление косинуса двух векторов). </p><p>Ключевые операции включают пакетное скалярное произведение для вычисления попарного сходства терминов, максимальное объединение по терминам документа для нахождения наивысшего сходства для каждого термина запроса и суммирование по терминам запроса для получения общей оценки документа, после чего документы сортируются на основе этих оценок. Псевдокод на PyTorch описан ниже:</p><pre><code class=\"language-python\">import torch\n\ndef compute_relevance_scores(query_embeddings, document_embeddings, k):\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n    \n    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]\n    :param k: Number of top documents to re-rank\n    :return: Sorted document indices based on their relevance scores\n    \"\"\"\n    \n    # Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]\n    # Pad the k documents to their maximum length for batch operations\n    # Note: Assuming document_embeddings is already padded and moved to GPU\n    \n    # Compute batch dot-product of Eq (query embeddings) and D (document embeddings)\n    # Resulting shape: [k, num_query_terms, max_doc_length]\n    scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n    \n    # Apply max-pooling across document terms (dim=2) to find the max similarity per query term\n    # Shape after max-pool: [k, num_query_terms]\n    max_scores_per_query_term = scores.max(dim=2).values\n    \n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [k]\n    total_scores = max_scores_per_query_term.sum(dim=1)\n    \n    # Sort the documents based on their total scores\n    sorted_indices = total_scores.argsort(descending=True)\n    \n    return sorted_indices\n</code></pre><p>Обратите внимание, что эта процедура используется как при обучении, так и при переранжировании во время вывода. Модель ColBERT обучается с использованием функции потерь попарного ранжирования, где обучающие данные состоят из троек $(q, d^+, d^-)$, где $q$ представляет запрос, $d^+$ - релевантный (положительный) документ для запроса, а $d^-$ - нерелевантный (отрицательный) документ. Модель стремится научиться создавать такие представления, чтобы оценка сходства между $q$ и $d^+$ была выше, чем оценка между q и $d^-$.</p><p>Цель обучения математически может быть представлена как минимизация следующей функции потерь: $$\\mathrm{Loss} = \\max(0, 1 - S(q, d^+) + S(q, d^-))$$</p><p>, где $S(q, d)$ обозначает оценку сходства, вычисленную ColBERT между запросом $q$ и документом $d$. Эта оценка получается путем агрегации максимальных оценок сходства наилучших совпадающих эмбеддингов между запросом и документом, следуя паттерну позднего взаимодействия, описанному в архитектуре модели. Этот подход гарантирует, что модель обучается различать релевантные и нерелевантные документы для данного запроса, поощряя больший разрыв в оценках сходства для положительных и отрицательных пар документов.</p><h3 id=\"denoised-supervision-in-colbertv2\">Очищенное обучение в ColBERTv2</h3><p>Очищенное обучение в ColBERTv2 улучшает исходный процесс обучения путем выбора сложных негативных примеров и использования кросс-энкодера для дистилляции. Этот сложный метод улучшения качества обучающих данных включает несколько шагов:</p><ol><li><strong>Начальное обучение</strong>: Использование официальных троек из набора данных MS MARCO, состоящих из запроса, релевантного документа и нерелевантного документа.</li><li><strong>Индексация и поиск</strong>: Применение сжатия ColBERTv2 для индексации обучающих пассажей с последующим извлечением top-k пассажей для каждого запроса.</li><li><strong>Переранжирование кросс-энкодером</strong>: Улучшение выбора пассажей путем переранжирования с помощью кросс-энкодера MiniLM, дистиллируя его оценки в ColBERTv2.</li><li><strong>Формирование обучающих кортежей</strong>: Генерация w-way кортежей для обучения, включающих как высоко-, так и низкоранжированные пассажи для создания сложных примеров.</li><li><strong>Итеративное уточнение</strong>: Повторение процесса для постоянного улучшения выбора сложных негативных примеров, тем самым повышая производительность модели.</li></ol><p>Заметим, что этот процесс представляет собой сложное улучшение режима обучения ColBERT, а не фундаментальное изменение его архитектуры.</p><h3 id=\"hyperparameters-of-colbert\">Гиперпараметры ColBERT</h3><p>Гиперпараметры ColBERT приведены ниже:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Гиперпараметр</th>\n<th>Оптимальный выбор</th>\n<th>Причина</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning Rate</td>\n<td>3 x 10^{-6}</td>\n<td>Выбрано для точной настройки, обеспечивающей стабильное и эффективное обновление модели.</td>\n</tr>\n<tr>\n<td>Batch Size</td>\n<td>32</td>\n<td>Балансирует вычислительную эффективность и способность захватывать достаточно информации при каждом обновлении.</td>\n</tr>\n<tr>\n<td>Number of Embeddings per Query (Nq)</td>\n<td>32</td>\n<td>Фиксированное значение для обеспечения согласованного размера представления для всех запросов, способствуя эффективной обработке.</td>\n</tr>\n<tr>\n<td>Embedding Dimension (m)</td>\n<td>128</td>\n<td>Демонстрирует хороший баланс между репрезентативной мощностью и вычислительной эффективностью.</td>\n</tr>\n<tr>\n<td>Training Iterations</td>\n<td>200k (MS MARCO), 125k (TREC CAR)</td>\n<td>Выбрано для обеспечения тщательного обучения, избегая переобучения, с корректировками на основе характеристик датасета.</td>\n</tr>\n<tr>\n<td>Bytes per Dimension in Embeddings</td>\n<td>4 (re-ranking), 2 (end-to-end ranking)</td>\n<td>Компромисс между точностью и эффективностью использования пространства с учетом контекста применения (повторное ранжирование vs end-to-end).</td>\n</tr>\n<tr>\n<td>Vector-Similarity Function</td>\n<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>\n<td>Выбрано на основе производительности и эффективности в соответствующих контекстах поиска.</td>\n</tr>\n<tr>\n<td>FAISS Index Partitions (P)</td>\n<td>2000</td>\n<td>Определяет гранулярность разбиения пространства поиска, влияя на эффективность поиска.</td>\n</tr>\n<tr>\n<td>Nearest Partitions Searched (p)</td>\n<td>10</td>\n<td>Балансирует широту поиска и вычислительную эффективность.</td>\n</tr>\n<tr>\n<td>Sub-vectors per Embedding (s)</td>\n<td>16</td>\n<td>Влияет на гранулярность квантования, воздействуя на скорость поиска и использование памяти.</td>\n</tr>\n<tr>\n<td>Index Representation per Dimension</td>\n<td>16-bit values</td>\n<td>Выбрано для второго этапа end-to-end поиска для управления компромиссом между точностью и пространством.</td>\n</tr>\n<tr>\n<td>Number of Layers in Encoders</td>\n<td>12-layer BERT</td>\n<td>Оптимальный баланс между глубиной контекстного понимания и вычислительной эффективностью.</td>\n</tr>\n<tr>\n<td>Max Query Length</td>\n<td>128</td>\n<td>Максимальное количество токенов, обрабатываемых энкодером запросов. <b>Это значение увеличено в модели Jina-ColBERT.</b></td>\n</tr>\n<tr>\n<td>Max Document Length</td>\n<td>512</td>\n<td>Максимальное количество токенов, обрабатываемых документным энкодером. <b>Это значение увеличено до 8192 в модели Jina-ColBERT.</b></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"the-indexing-strategy-of-colbert\">Стратегия индексирования ColBERT</h2>\n<p>В отличие от подходов, основанных на представлении, которые кодируют каждый документ в один вектор эмбеддинга, <strong>ColBERT кодирует документы (и запросы) в наборы эмбеддингов, где каждый токен в документе имеет свой собственный эмбеддинг.</strong> Этот подход по своей сути означает, что для более длинных документов будет храниться больше эмбеддингов, <strong>что является слабым местом оригинального ColBERT, которое позже было решено в ColBERTv2.</strong></p>\n<p>Ключ к эффективному управлению этим заключается в использовании ColBERT векторной базы данных (например, <a href=\"https://github.com/facebookresearch/faiss?ref=jina-ai-gmbh.ghost.io\">FAISS</a>) для индексирования и поиска, и его детальном процессе индексирования, который разработан для эффективной обработки больших объемов данных. В оригинальной статье о ColBERT упоминается несколько стратегий для повышения эффективности индексирования и поиска, включая:</p>\n<ul>\n<li><strong>Офлайн индексирование</strong>: Представления документов вычисляются офлайн, позволяя предварительно вычислять и хранить эмбеддинги документов. Этот процесс использует пакетную обработку и ускорение GPU для эффективной обработки больших коллекций документов.</li>\n<li><strong>Хранение эмбеддингов</strong>: Эмбеддинги документов могут храниться с использованием 32-битных или 16-битных значений для каждого измерения, предлагая компромисс между точностью и требованиями к хранению. Эта гибкость позволяет ColBERT поддерживать баланс между эффективностью (с точки зрения производительности поиска) и эффективностью (с точки зрения затрат на хранение и вычисления).</li>\n</ul>\n<p>Введение <strong>остаточного сжатия</strong> в ColBERTv2, которое является новым подходом, отсутствующим в оригинальном ColBERT, играет ключевую роль в уменьшении пространственного следа модели в 6-10 раз при сохранении качества. Эта техника дополнительно сжимает эмбеддинги, эффективно захватывая и сохраняя только различия от набора фиксированных опорных центроидов.</p>\n<h2 id=\"effectiveness-and-efficiency-of-colbert\">Эффективность и производительность ColBERT</h2>\n<p>Можно изначально предположить, что включение глубокого контекстного понимания BERT в поиск по своей природе потребует значительных вычислительных ресурсов, делая такой подход менее осуществимым для приложений реального времени из-за высокой задержки и вычислительных затрат. Однако ColBERT оспаривает и опровергает это предположение через свое инновационное использование механизма позднего взаимодействия. Вот некоторые примечательные моменты:</p>\n<ol>\n<li><strong>Значительный прирост эффективности</strong>: ColBERT достигает снижения вычислительных затрат (FLOP) и задержки на порядки по сравнению с традиционными моделями ранжирования на основе BERT. В частности, для данного размера модели (например, 12-слойный \"базовый\" трансформер-энкодер), ColBERT не только соответствует, но в некоторых случаях превосходит эффективность моделей на основе BERT при значительно меньших вычислительных требованиях. Например, при глубине повторного ранжирования <em>k</em>=10, BERT требует почти в 180 раз больше FLOP, чем ColBERT; этот разрыв увеличивается с ростом <em>k</em>, достигая 13900× при <em>k</em>=1000 и даже 23000× при <em>k</em>=2000.</li>\n<li><strong>Улучшенные Recall и MRR@10 в end-to-end поиске</strong>: Вопреки начальной интуиции о том, что более глубокое взаимодействие между представлениями запроса и документа (как в моделях раннего взаимодействия) будет необходимо для высокой эффективности поиска, end-to-end схема поиска ColBERT демонстрирует превосходную эффективность. Например, его Recall@50 превосходит официальный BM25 Recall@1000 и почти все другие модели Recall@200, подчеркивая замечательную способность модели извлекать релевантные документы из обширной коллекции без прямого сравнения каждой пары запрос-документ.</li>\n<li><strong>Практичность для реальных приложений</strong>: Экспериментальные результаты подчеркивают практическую применимость ColBERT для реальных сценариев. Его пропускная способность индексирования и эффективность использования памяти делают его подходящим для индексирования больших коллекций документов, таких как MS MARCO, в течение нескольких часов, сохраняя высокую эффективность при управляемом использовании пространства. Эти качества подчеркивают пригодность ColBERT для развертывания в производственных средах, где важны как производительность, так и вычислительная эффективность.</li>\n<li><strong>Масштабируемость с размером коллекции документов</strong>: Возможно, самым удивительным выводом является масштабируемость и эффективность ColBERT в обработке крупномасштабных коллекций документов. Архитектура позволяет предварительно вычислять эмбеддинги документов и использует эффективную пакетную обработку для взаимодействия запрос-документ, позволяя системе эффективно масштабироваться с размером коллекции документов. Эта масштабируемость противоречит интуиции при учете сложности и глубины понимания, необходимых для эффективного поиска документов, демонстрируя инновационный подход ColBERT к балансированию вычислительной эффективности с эффективностью поиска.</li>\n</ol>\n<h2 id=\"using-jina-colbert-v1-en-a-8192-length-colbertv2-model\">Использование <code>jina-colbert-v1-en</code>: модель ColBERTv2 с длиной контекста 8192</h2>\n<p>Jina-ColBERT разработан для быстрого и точного поиска, поддерживая <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\">контексты длиной до 8192 токенов, используя преимущества JinaBERT</a>, который позволяет обрабатывать более длинные последовательности благодаря улучшениям в архитектуре.</p>\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\">\n<div class=\"kg-callout-emoji\">💡</div>\n<div class=\"kg-callout-text\">Строго говоря, Jina-ColBERT поддерживает длину в 8190 токенов. Напомним, что в документном энкодере ColBERT каждый документ дополняется токенами <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">[D],[CLS]</code> в начале.</div>\n</div>\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-colbert-v1-en · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png\" alt=\"\"></div></a></figure>\n<h3 id=\"jinas-improvement-over-original-colbert\">Улучшения Jina по сравнению с оригинальным ColBERT</h3>\n<p>Основным улучшением Jina-ColBERT является его основа, <code>jina-bert-v2-base-en</code>, которая позволяет обрабатывать значительно более длинные контексты (до 8192 токенов) по сравнению с оригинальным ColBERT, использующим <code>bert-base-uncased</code>. Эта возможность крайне важна для обработки документов с обширным содержанием, обеспечивая более детальные и контекстуальные результаты поиска.</p>\n<h3 id=\"jina-colbert-v1-en-performance-comparison-vs-colbertv2\">Сравнение производительности <code>jina-colbert-v1-en</code> с ColBERTv2</h3>\n<p>Мы оценили <code>jina-colbert-v1-en</code> на датасетах BEIR и новом бенчмарке LoCo, который ориентирован на длинный контекст, протестировав его против оригинальной реализации ColBERTv2 и основанных на отсутствии взаимодействия</p>модель <code>jina-embeddings-v2-base-en</code>.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>ColBERTv2</th>\n<th>jina-colbert-v1-en</th>\n<th>jina-embeddings-v2-base-en</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Arguana</td>\n<td>46.5</td>\n<td><strong>49.4</strong></td>\n<td>44.0</td>\n</tr>\n<tr>\n<td>Climate-Fever</td>\n<td>18.1</td>\n<td>19.6</td>\n<td><strong>23.5</strong></td>\n</tr>\n<tr>\n<td>DBPedia</td>\n<td><strong>45.2</strong></td>\n<td>41.3</td>\n<td>35.1</td>\n</tr>\n<tr>\n<td>FEVER</td>\n<td>78.8</td>\n<td><strong>79.5</strong></td>\n<td>72.3</td>\n</tr>\n<tr>\n<td>FiQA</td>\n<td>35.4</td>\n<td>36.8</td>\n<td><strong>41.6</strong></td>\n</tr>\n<tr>\n<td>HotpotQA</td>\n<td><strong>67.5</strong></td>\n<td>65.9</td>\n<td>61.4</td>\n</tr>\n<tr>\n<td>NFCorpus</td>\n<td>33.7</td>\n<td><strong>33.8</strong></td>\n<td>32.5</td>\n</tr>\n<tr>\n<td>NQ</td>\n<td>56.1</td>\n<td>54.9</td>\n<td><strong>60.4</strong></td>\n</tr>\n<tr>\n<td>Quora</td>\n<td>85.5</td>\n<td>82.3</td>\n<td><strong>88.2</strong></td>\n</tr>\n<tr>\n<td>SCIDOCS</td>\n<td>15.4</td>\n<td>16.9</td>\n<td><strong>19.9</strong></td>\n</tr>\n<tr>\n<td>SciFact</td>\n<td>68.9</td>\n<td><strong>70.1</strong></td>\n<td>66.7</td>\n</tr>\n<tr>\n<td>TREC-COVID</td>\n<td>72.6</td>\n<td><strong>75.0</strong></td>\n<td>65.9</td>\n</tr>\n<tr>\n<td>Webis-touch2020</td>\n<td>26.0</td>\n<td><strong>27.0</strong></td>\n<td>26.2</td>\n</tr>\n<tr>\n<td>LoCo</td>\n<td>74.3</td>\n<td>83.7</td>\n<td><strong>85.4</strong></td>\n</tr>\n<tr>\n<td>Average</td>\n<td>51.7</td>\n<td><strong>52.6</strong></td>\n<td>51.6</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Эта таблица демонстрирует превосходную производительность <code>jina-colbert-v1-en</code>，особенно в сценариях，требующих большей длины контекста по сравнению с оригинальным ColBERTv2. Стоит отметить，что <code>jina-embeddings-v2-base-en</code> <a href=\"https://arxiv.org/abs/2310.19923?ref=jina-ai-gmbh.ghost.io\">использует больше обучающих данных</a>，в то время как <code>jina-colbert-v1-en</code> использует только MSMARCO，что может объяснять хорошую производительность <code>jina-embeddings-v2-base-en</code> в некоторых задачах.</p>\n\n<h3 id=\"example-usage-of-jina-colbert-v1-en\">Пример использования <code>jina-colbert-v1-en</code></h3>\n\n<p>Этот фрагмент кода описывает процесс индексации с помощью Jina-ColBERT，демонстрируя поддержку длинных документов.</p>\n\n<pre><code class=\"language-python\">from colbert import Indexer\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\n\nn_gpu: int = 1  # Set your number of available GPUs\nexperiment: str = \"\"  # Name of the folder where the logs and created indices will be stored\nindex_name: str = \"\"  # The name of your index, i.e. the name of your vector database\n\nif __name__ == \"__main__\":\n    with Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):\n        config = ColBERTConfig(\n          doc_maxlen=8192  # Our model supports 8k context length for indexing long documents\n        )\n        indexer = Indexer(\n          checkpoint=\"jinaai/jina-colbert-v1-en\",\n          config=config,\n        )\n        documents = [\n          \"ColBERT is an efficient and effective passage retrieval model.\",\n          \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length.\",\n          \"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length.\",\n          \"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2.\",\n          \"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2.\",\n          \"Jina is an easier way to build neural search systems.\",\n          \"You can use Jina-ColBERT to build neural search systems with ease.\",\n          # Add more documents here to ensure the clustering work correctly\n        ]\n        indexer.index(name=index_name, collection=documents)\n</code></pre>\n\n<h3 id=\"use-jina-colbert-v1-en-in-ragatouille\">Использование <code>jina-colbert-v1-en</code> в RAGatouille</h3>\n\n<p>RAGatouille — это новая библиотека Python，которая упрощает использование продвинутых методов поиска в RAG-пайплайнах. Она разработана с учетом модульности и простоты интеграции，позволяя пользователям легко применять передовые исследования. Основная цель RAGatouille — упростить применение сложных моделей，таких как ColBERT，в RAG-пайплайнах，делая их доступными для разработчиков без необходимости глубокого понимания базовых исследований. Благодаря <a href=\"https://twitter.com/bclavie?ref=jina-ai-gmbh.ghost.io\">Benjamin Clavié</a>，теперь вы можете легко использовать <code>jina-colbert-v1-en</code>:</p>\n\n<pre><code class=\"language-python\">from ragatouille import RAGPretrainedModel\n\n# Get your model & collection of big documents ready\nRAG = RAGPretrainedModel.from_pretrained(\"jinaai/jina-colbert-v1-en\")\nmy_documents = [\n    \"very long document1\",\n    \"very long document2\",\n    # ... more documents\n]\n\n# And create an index with them at full length!\nRAG.index(collection=my_documents,\n          index_name=\"the_biggest_index\",\n          max_document_length=8190,)\n\n# or encode them in-memory with no truncation, up to your model's max length\nRAG.encode(my_documents)\n</code></pre>\n\n<p>Для получения более подробной информации о Jina-ColBERT вы можете посетить <a href=\"https://huggingface.co/jinaai/jina-colbert-v1-en?ref=jina-ai-gmbh.ghost.io\">страницу на Hugging Face</a>.</p>\n\n<h2 id=\"conclusion\">Заключение</h2>\n\n<p>ColBERT представляет собой значительный шаг вперед в области информационного поиска. Благодаря возможности работы с более длинным контекстом в Jina-ColBERT и сохранению совместимости с подходом ColBERT к позднему взаимодействию，он предлагает мощную альтернативу для разработчиков，стремящихся реализовать современную функциональность поиска.</p>\n\n<p>В сочетании с библиотекой RAGatouille，которая упрощает интеграцию сложных моделей поиска в RAG-пайплайны，разработчики теперь могут легко использовать возможности продвинутого поиска，оптимизируя свои рабочие процессы и улучшая свои приложения. Синергия между Jina-ColBERT и RAGatouille демонстрирует замечательный шаг в направлении обеспечения доступности и эффективности продвинутых моделей поиска на базе ИИ для практического использования.</p>",
  "comment_id": "65d3a2134a32310001f5b71b",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-02-19T19:46:43.000+01:00",
  "updated_at": "2024-08-30T23:11:22.000+02:00",
  "published_at": "2024-02-20T02:19:04.000+01:00",
  "custom_excerpt": "Jina AI's ColBERT on Hugging Face has set Twitter abuzz, bringing a fresh perspective to search with its 8192-token capability. This article unpacks the nuances of ColBERT and ColBERTv2, showcasing their innovative designs and why their late interaction feature is a game-changer for search.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
  "excerpt": "Модель ColBERT от Jina AI на платформе Hugging Face вызвала оживленные дискуссии в Twitter, предлагая новый взгляд на поиск благодаря возможности обработки 8192 токенов. В этой статье разбираются тонкости ColBERT и ColBERTv2, демонстрируя их инновационный дизайн и объясняя, почему их функция позднего взаимодействия является революционной для поиска.",
  "reading_time": 16,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Neon theater or concert hall marquee letters lit up at night with city lights and faint \"Adobe Sto\" visible.",
  "feature_image_caption": null
}
{
  "slug": "submodular-optimization-for-diverse-query-generation-in-deepresearch",
  "id": "6864cd10ff4ca4000153c921",
  "uuid": "1742f990-b248-44ed-a50e-58fee7e93464",
  "title": "Субмодульная оптимизация для генерации разнообразных запросов в DeepResearch",
  "html": "<p>При реализации DeepResearch есть как минимум два места, где вам нужно генерировать разнообразные запросы. Во-первых, вы должны <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L870\">генерировать поисковые запросы на основе ввода пользователя</a> (непосредственная отправка ввода пользователя в поисковую систему — не лучшая идея). Во-вторых, многие системы DeepResearch включают <a href=\"https://github.com/jina-ai/node-DeepResearch/blob/031042766a96bde4a231a33a9b7db7429696a40c/src/agent.ts#L825-L840\">\"планировщик исследований\", который разбивает исходную проблему на подзадачи</a>, одновременно вызывает агентов для их независимого решения, а затем объединяет их результаты. Независимо от того, имеем ли мы дело с запросами или подзадачами, наши ожидания остаются прежними: они должны быть релевантны исходному вводу и достаточно разнообразны, чтобы предложить уникальные точки зрения на него. Часто нам необходимо ограничить количество запросов, чтобы не тратить деньги на ненужные запросы к поисковой системе или использование токенов агента.</p><p>Понимая важность генерации запросов, большинство реализаций DeepResearch с открытым исходным кодом не воспринимают эту оптимизацию всерьез. Они просто напрямую указывают эти ограничения в своих подсказках (Prompt). Некоторые могут попросить Большую языковую модель (LLM) о дополнительном ходе для оценки и диверсификации запросов. Вот пример того, как большинство реализаций в основном подходят к этому:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Heading---2025-07-02T154101.715.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Heading---2025-07-02T154101.715.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-02T154101.715.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Два разных Prompt для генерации разнообразных запросов с использованием Больших языковых моделей (LLM). Верхний Prompt использует простые инструкции. Нижний — более сложный и структурированный. Учитывая исходный запрос и количество запросов, которые необходимо сгенерировать, мы ожидаем, что сгенерированные запросы будут достаточно разнообразными. В этом примере мы используем </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>gemini-2.5-flash</span></code><span style=\"white-space: pre-wrap;\"> в качестве Большой языковой модели (LLM), а исходный запрос — </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>В этой статье я хочу продемонстрировать более строгий подход к решению задачи оптимальной генерации запросов с использованием векторных моделей предложений и <strong>субмодульной оптимизации</strong>. В мои дни учебы в аспирантуре субмодульная оптимизация была одной из моих любимых техник наряду с L-BFGS. Я покажу, как применить ее для генерации набора разнообразных запросов при кардинальном ограничении, что может значительно улучшить общее качество систем DeepResearch.</p><h2 id=\"query-generation-via-prompting\">Генерация запросов с помощью Prompt</h2><p>Во-первых, мы хотим проверить, является ли использование Prompt эффективным подходом для генерации разнообразных запросов. Мы также хотим понять, является ли сложный Prompt более эффективным, чем простой Prompt. Давайте проведем эксперимент, сравнив два Prompt ниже, чтобы это выяснить:</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert at generating diverse search queries. Given any input topic, generate {num_queries} different search queries that explore various angles and aspects of the topic.</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Простой Prompt</span></p></figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">You are an expert research strategist. Generate an optimal set of diverse search queries that maximizes information coverage while minimizing redundancy.\n\nTask: Create exactly {num_queries} search queries from any given input that satisfy:\n- Relevance: Each query must be semantically related to the original input\n- Diversity: Each query should explore a unique facet with minimal overlap\n- Coverage: Together, the queries should comprehensively address the topic\n\nProcess:\n1. Decomposition: Break down the input into core concepts and dimensions\n2. Perspective Mapping: Identify distinct angles (theoretical, practical, historical, comparative, etc.)\n3. Query Formulation: Craft specific, searchable queries for each perspective\n4. Diversity Check: Ensure minimal semantic overlap between queries</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Структурированный Prompt</span></p></figcaption></figure><p>Мы используем <code>gemini-2.5-flash</code> в качестве Большой языковой модели (LLM) с исходным запросом <code>\"embeddings and rerankers\"</code> и тестируем как простой, так и структурированный Prompt для итеративной генерации от одного до 20 запросов. Затем мы используем <code>jina-embeddings-v3</code> с задачей <code>text-matching</code> для измерения семантической близости между исходным запросом и сгенерированными запросами, а также близости внутри самих сгенерированных запросов. Вот визуализации.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1596\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Оба Prompt показывают схожие закономерности в анализе \"Внутри сгенерированных запросов\" (два графика справа), при этом медианные значения косинусной близости остаются высокими (диапазон 0,4-0,6) при разном количестве запросов. Простой Prompt, похоже, даже лучше диверсифицирует запросы, когда количество запросов велико, тогда как структурированный Prompt поддерживает немного лучшую релевантность исходному запросу, удерживая релевантность около 0,6.</span></figcaption></figure><p>Посмотрите на два графика справа, и вы увидите, что как простой, так и структурированный Prompt демонстрируют большую дисперсию в оценках косинусной близости, причем многие достигают близости 0,7-0,8, что говорит о том, что некоторые сгенерированные запросы почти идентичны. Кроме того, обоим методам трудно поддерживать разнообразие по мере генерации большего количества запросов. Вместо того чтобы видеть четкую тенденцию к снижению близости с увеличением количества запросов, мы наблюдаем относительно стабильные (и высокие) уровни близости, что указывает на то, что дополнительные запросы часто дублируют существующие перспективы.</p><p>Одно из объяснений состоит в том, что Wang et al. (2025) обнаружили, что Большие языковые модели (LLM) часто отражают мнения доминирующих групп непропорционально, даже при управлении Prompt, что указывает на предвзятость в отношении общих точек зрения. Это связано с тем, что данные обучения Большой языковой модели (LLM) могут чрезмерно представлять определенные точки зрения, в результате чего модель генерирует варианты, которые соответствуют этим распространенным точкам зрения. Abe et al. (2025) также обнаружили, что расширение запросов на основе Большой языковой модели (LLM) отдает предпочтение популярным интерпретациям, игнорируя другие. Например, \"Каковы преимущества ИИ?\" может привести к общим преимуществам, таким как автоматизация, эффективность, этичность, но упустить менее очевидные, такие как открытие лекарств.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.15229\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Multilingual Prompting for Improving LLM Generation Diversity</div><div class=\"kg-bookmark-description\">Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-41.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Qihan Wang</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-36.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2505.12349\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</div><div class=\"kg-bookmark-description\">Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the “wisdom of the crowd”, can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-42.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Axel Abels</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-37.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"problem-formulation\">Формулировка задачи</h2><p>Кто-то может подумать, что наш предыдущий эксперимент неубедителен и что нам следует улучшить Prompt и повторить попытку. Хотя Prompt, безусловно, может в некоторой степени изменить результаты, более важно то, что мы кое-что узнали: простое увеличение количества сгенерированных запросов повышает вероятность получения разнообразных запросов. Плохая новость заключается в том, что мы также получаем кучу дублированных запросов в качестве побочного продукта.</p><p>Но поскольку генерировать большое количество запросов дешево, что в конечном итоге дает <em>несколько</em> хороших, почему бы нам не рассматривать это как задачу выбора подмножества?</p><p>В математике эту задачу можно сформулировать следующим образом: даны исходные входные данные $q_0$, набор запросов-кандидатов $V=\\{q_1, q_2, \\cdots, q_n\\}$, сгенерированных LLM с использованием разработки 提示词. Выберите подмножество $X\\subseteq V$ из $k$ запросов, которое максимизирует охват при минимизации избыточности.</p><p>К сожалению, для нахождения оптимального подмножества из $k$ запросов из $n$ кандидатов необходимо проверить $\\binom{n}{k}$ комбинаций — экспоненциальная сложность. Только для 20 кандидатов и $k=5$ это 15 504 комбинации.</p><h3 id=\"submodular-function\">Субмодулярная функция</h3><p>Прежде чем пытаться грубо решить задачу выбора подмножества, позвольте мне представить читателям термины <strong>субмодулярность</strong> и <strong>субмодулярная функция</strong>. Они могут показаться незнакомыми многим, но вы, возможно, слышали об идее \"убывающей отдачи\" — субмодулярность является математическим представлением этого.</p><p>Представьте себе размещение Wi-Fi роутеров для обеспечения интернет-покрытия в большом здании. Первый установленный вами роутер дает огромную ценность — он покрывает значительную площадь, где раньше не было покрытия. Второй роутер также добавляет существенную ценность, но некоторая часть его зоны покрытия перекрывается с первым роутером, поэтому предельная выгода меньше, чем от первого. По мере того как вы продолжаете добавлять роутеры, каждый дополнительный роутер покрывает все меньше и меньше новой площади, поскольку большинство мест уже покрыто существующими роутерами. В конце концов, 10-й роутер может обеспечить очень небольшое дополнительное покрытие, поскольку здание уже хорошо покрыто.</p><p>Эта интуиция отражает суть субмодулярности. Математически, функция множества $f: 2^V \\rightarrow \\mathbb{R}$ является <strong>субмодулярной</strong>, если для всех $A \\subseteq B \\subseteq V$ и любого элемента $v \\notin B$:</p><p>$$f(A \\cup {v}) - f(A) \\geq f(B \\cup {v}) - f(B)$$</p><p>Проще говоря: добавление элемента к меньшему множеству дает по крайней мере столько же выгоды, сколько добавление того же элемента к большему множеству, которое содержит меньшее множество.</p><p>Теперь давайте применим эту концепцию к нашей задаче генерации запросов. Можно сразу понять, что выбор запросов демонстрирует естественную <strong>убывающую отдачу</strong>:</p><ul><li>Первый выбранный нами запрос охватывает совершенно новое семантическое пространство</li><li>Второй запрос должен охватывать разные аспекты, но некоторое перекрытие неизбежно</li><li>По мере того как мы добавляем больше запросов, каждый дополнительный запрос охватывает все меньше и меньше новой территории</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"1122\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/Untitled-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/Untitled-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/07/Untitled-5.png 1497w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Из </span><a href=\"https://www.linkedin.com/in/hxiao87/overlay/education/199382643/multiple-media-viewer/?profileId=ACoAABJwuskBoKQcxGt4CD3n_6hkQt5W7W5moQM&amp;treasuryMediaId=50042789\"><span style=\"white-space: pre-wrap;\">одного из моих старых слайдов еще в AAAI 2013</span></a><span style=\"white-space: pre-wrap;\">, где я объяснял субмодулярность на примере мешка с шариками. Добавление большего количества шариков в мешок улучшает \"средство\", но относительное улучшение становится все меньше и меньше, как видно по уменьшающимся значениям дельты на правой оси y.</span></figcaption></figure><h2 id=\"embedding-based-submodular-function-design\">Разработка субмодулярной функции на основе 向量模型</h2><p>Пусть $\\mathbf{e}_i \\in \\mathbb{R}^d$ — вектор 向量模型 для запроса $q_i$, полученный с помощью модели 向量模型 предложений (например, <code>jina-embeddings-v3</code>). Существует два основных подхода к разработке нашей целевой функции:</p><h3 id=\"approach-1-facility-location-coverage-based\">Подход 1: Размещение объектов (на основе покрытия)</h3><p>$$f_{\\text{coverage}}(X) = \\sum_{j=1}^{n} \\max\\left(\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j), \\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)\\right)$$</p><p>Эта функция измеряет, насколько хорошо выбранный набор $X$ \"покрывает\" все запросы-кандидаты, где:</p><ul><li>$\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|}$ — косинусное сходство</li><li>$\\alpha \\cdot \\text{sim}(\\mathbf{e}_0, \\mathbf{e}_j)$ обеспечивает релевантность исходному запросу</li><li>$\\max_{q_i \\in X} \\text{sim}(\\mathbf{e}_j, \\mathbf{e}_i)$ измеряет покрытие кандидата $j$ выбранным набором $X$</li></ul><p>Одна оговорка заключается в том, что эта функция только <em>косвенно</em> поощряет разнообразие. Она не наказывает явно за сходство внутри выбранного набора $X$. Разнообразие возникает потому, что выбор похожих запросов обеспечивает уменьшение отдачи от покрытия.</p><h3 id=\"approach-2-explicit-coverage-diversity\">Подход 2: Явное покрытие + разнообразие</h3><p>Для более прямого контроля над разнообразием мы можем объединить покрытие и явный член разнообразия:</p><p>$$f(X) = \\lambda \\cdot f_{\\text{coverage}}(X) + (1-\\lambda) \\cdot f_{\\text{diversity}}(X)$$</p><p>где компонент разнообразия можно сформулировать как:</p><p>$$f_{\\text{diversity}}(X) = \\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$$</p><p>Этот член разнообразия измеряет общее сходство между выбранными и невыбранными запросами — он максимизируется, когда мы выбираем запросы, которые отличаются от остальных кандидатов (форма функции разреза графа).</p><h3 id=\"difference-between-two-approaches\">Разница между двумя подходами</h3><p>Обе формулировки сохраняют субмодулярность.</p><p>Функция размещения объектов является хорошо известной субмодулярной функцией. Она демонстрирует субмодулярность из-за операции max: когда мы добавляем новый запрос $q$ в наш выбранный набор, каждый запрос-кандидат $j$ покрывается \"лучшим\" запросом в нашем наборе (тем, у которого самое высокое сходство). Добавление $q$ к меньшему набору $A$ с большей вероятностью улучшит покрытие различных кандидатов, чем добавление его к большему набору $B \\supseteq A$, где многие кандидаты уже хорошо покрыты.</p><p>В функции разнообразия разреза графа термин разнообразия $\\sum_{q_i \\in X} \\sum_{q_j \\in V \\setminus X} \\text{sim}(\\mathbf{e}_i, \\mathbf{e}_j)$ является субмодулярным, поскольку он измеряет \"разрез\" между выбранными и невыбранными наборами. Добавление нового запроса к меньшему выбранному набору создает больше новых связей с невыбранными запросами, чем добавление его к большему выбранному набору.</p><p>Подход размещения объектов полагается на <em>косвенное</em> разнообразие через конкуренцию за покрытие, в то время как явный подход непосредственно измеряет и оптимизирует разнообразие. Таким образом, оба подхода допустимы, но явный подход дает вам более прямой контроль над компромиссом между релевантностью и разнообразием.</p><h2 id=\"implementations\">Реализации</h2><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jina-ai/submodular-optimization\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - jina-ai/submodular-optimization</div><div class=\"kg-bookmark-description\">Contribute to jina-ai/submodular-optimization development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40-8.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jina-ai</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/submodular-optimization\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Полную реализацию можно найти здесь, на Github.</span></p></figcaption></figure><p>Поскольку наша функция является субмодулярной, мы можем использовать <strong>жадный алгоритм</strong>, который обеспечивает гарантию аппроксимации $(1-1/e) \\approx 0.63$:</p><p>$$\\max_{X \\subseteq V} f(X) \\quad \\text{subject to} \\quad |X| \\leq k$$</p><p>Вот код для оптимизации размещения объектов (на основе покрытия) — с неявным разнообразием.</p><pre><code class=\"language-python\">def greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Greedy algorithm for submodular query selection\n    \n    Args:\n        candidates: List of candidate query strings\n        embeddings: Matrix of query embeddings (n x d)\n        original_embedding: Embedding of original query (d,)\n        k: Number of queries to select\n        alpha: Relevance weight parameter\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    remaining = set(range(n))\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    for _ in range(k):\n        best_gain = -float('inf')\n        best_query = None\n        \n        for i in remaining:\n            # Calculate marginal gain of adding query i\n            gain = compute_marginal_gain(i, selected, embeddings, \n                                       relevance_scores, alpha)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_query = i\n        \n        if best_query is not None:\n            selected.append(best_query)\n            remaining.remove(best_query)\n    \n    return [candidates[i] for i in selected]\n\ndef compute_marginal_gain(new_idx, selected, embeddings, relevance_scores, alpha):\n    \"\"\"Compute marginal gain of adding new_idx to selected set\"\"\"\n    if not selected:\n        # First query: gain is sum of all relevance scores\n        return sum(max(alpha * relevance_scores[j], \n                      cosine_similarity(embeddings[new_idx], embeddings[j]))\n                  for j in range(len(embeddings)))\n    \n    # Compute current coverage\n    current_coverage = [\n        max([alpha * relevance_scores[j]] + \n            [cosine_similarity(embeddings[s], embeddings[j]) for s in selected])\n        for j in range(len(embeddings))\n    ]\n    \n    # Compute new coverage with additional query\n    new_coverage = [\n        max(current_coverage[j], \n            cosine_similarity(embeddings[new_idx], embeddings[j]))\n        for j in range(len(embeddings))\n    ]\n    \n    return sum(new_coverage) - sum(current_coverage)\n</code></pre><p>Параметр баланса $\\alpha$ контролирует компромисс между релевантностью и разнообразием:</p><ul><li><strong>Высокое $\\alpha$ (например, 0,8)</strong>: Приоритет отдается релевантности исходному запросу, может жертвовать разнообразием</li><li><strong>Низкое $\\alpha$ (например, 0,2)</strong>: Приоритет отдается разнообразию среди выбранных запросов, может отклоняться от исходного намерения</li><li><strong>Умеренное $\\alpha$ (например, 0,4-0,6)</strong>: Сбалансированный подход, часто хорошо работает на практике</li></ul><h3 id=\"lazy-greedy-algorithm\">Ленивый жадный алгоритм</h3><p>Можно заметить в приведенном выше коде:</p><pre><code class=\"language-python\">for i in remaining:\n    # Calculate marginal gain of adding query i\n    gain = compute_marginal_gain(i, selected, embeddings, \n                               relevance_scores, alpha)</code></pre><p>Мы вычисляем предельный прирост для <strong>всех</strong> оставшихся кандидатов на каждой итерации. Мы можем сделать лучше.</p><p><strong>Ленивый жадный алгоритм</strong> — это умная оптимизация, которая использует субмодулярность, чтобы избежать ненужных вычислений. Ключевая идея заключается в том, что если элемент A имел более высокий предельный прирост, чем элемент B, на итерации $t$, то A все равно будет иметь более высокий предельный прирост, чем B, на итерации $t+1$ (из-за свойства субмодулярности).</p><pre><code class=\"language-python\">import heapq\n\ndef lazy_greedy_query_selection(candidates, embeddings, original_embedding, k, alpha=0.3):\n    \"\"\"\n    Lazy greedy algorithm for submodular query selection\n    More efficient than standard greedy by avoiding unnecessary marginal gain computations\n    \"\"\"\n    n = len(candidates)\n    selected = []\n    \n    # Precompute relevance scores\n    relevance_scores = cosine_similarity(original_embedding, embeddings)\n    \n    # Initialize priority queue: (-marginal_gain, last_updated, query_index)\n    # Use negative gain because heapq is a min-heap\n    pq = []\n    for i in range(n):\n        gain = compute_marginal_gain(i, [], embeddings, relevance_scores, alpha)\n        heapq.heappush(pq, (-gain, 0, i))\n    \n    for iteration in range(k):\n        while True:\n            neg_gain, last_updated, best_idx = heapq.heappop(pq)\n            \n            # If this gain was computed in current iteration, it's definitely the best\n            if last_updated == iteration:\n                selected.append(best_idx)\n                break\n            \n            # Otherwise, recompute the marginal gain\n            current_gain = compute_marginal_gain(best_idx, selected, embeddings, \n                                               relevance_scores, alpha)\n            heapq.heappush(pq, (-current_gain, iteration, best_idx))\n    \n    return [candidates[i] for i in selected]</code></pre><p>Жадный алгоритм с отложенным вычислением работает следующим образом:</p><ol><li>Поддерживает очередь с приоритетами элементов, отсортированных по их предельным выгодам.</li><li>Повторно вычисляет предельную выгоду только для верхнего элемента.</li><li>Если после пересчета он все еще является самым высоким, выбирает его.</li><li>В противном случае вставляет его в правильную позицию и проверяет следующий верхний элемент.</li></ol><p>Это может значительно ускорить процесс, поскольку мы избегаем повторного вычисления предельных выгод для элементов, которые явно не будут выбраны.</p><h3 id=\"results\">Результаты</h3><p>Давайте снова проведем эксперимент. Мы используем тот же простой 提示词 для создания от одного до 20 различных запросов и выполняем те же измерения косинусной близости, что и раньше. Для субаддитивной оптимизации мы выбираем запросы из 20 сгенерированных кандидатов, используя различные значения k, и измеряем сходство, как и раньше. Результаты показывают, что запросы, выбранные с помощью субаддитивной оптимизации, более разнообразны и показывают более низкое сходство внутри набора.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Исходный запрос = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"embeddings and rerankers\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--4-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Исходный запрос = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"generative ai\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Исходный запрос = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"geopolitics USA and China\"</span></code></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"790\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/07/embeddings-reranker-confidence-plot-comparison--5-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Исходный запрос = </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>\"google 2025 revenue breakdown\"</span></code></figcaption></figure><h2 id=\"final-question-why-submodular-formulation-matters\">Финальный вопрос: почему важна субаддитивная формулировка?</h2><p>Вам может быть интересно: зачем утруждать себя формулированием этого как задачи субаддитивной оптимизации? Почему бы просто не использовать эвристики или другие подходы к оптимизации?</p><p>Короче говоря, субаддитивная формулировка преобразует специальную эвристику \"выбрать разнообразные запросы\" в строгую задачу оптимизации с **доказуемыми гарантиями**, **эффективными алгоритмами** и измеримыми целями.</p><h3 id=\"guaranteed-efficiency\">Гарантированная эффективность</h3><p>Как только мы докажем, что наша целевая функция является субаддитивной, мы получим мощные теоретические гарантии и эффективный алгоритм. Жадный алгоритм, который выполняется за время $O(nk)$ по сравнению с проверкой $\\binom{n}{k}$ комбинаций, достигает $(1-1/e) \\approx 0.63$ приближения к оптимальному решению. Это означает, что наше жадное решение всегда как минимум на 63% так же хорошо, как и наилучшее возможное решение. **Никакая эвристика не может этого обещать.**</p><p>Более того, жадный алгоритм с отложенным вычислением значительно быстрее на практике благодаря математической структуре субаддитивных функций. Ускорение происходит из-за **уменьшения отдачи**: элементы, которые были плохим выбором в более ранних итерациях, вряд ли станут хорошим выбором позже. Поэтому вместо проверки всех $n$ кандидатов, жадному алгоритму с отложенным вычислением обычно требуется только повторно вычислить выгоды для нескольких лучших кандидатов.</p><h3 id=\"no-need-for-hand-crafted-heuristics\">Нет необходимости в разработанных вручную эвристиках</h3><p>Без принципиальной структуры вы можете прибегнуть к специальным правилам, таким как \"убедиться, что запросы имеют косинусное сходство &lt; 0,7\" или \"сбалансировать различные категории ключевых слов\". Эти правила трудно настраивать, и они не обобщаются. Субаддитивная оптимизация дает вам принципиальный, математически обоснованный подход. Вы можете систематически настраивать гиперпараметры, используя наборы валидации, и отслеживать качество решения в производственных системах. Когда система выдает плохие результаты, у вас есть четкие метрики для отладки того, что пошло не так.</p><p>Наконец, субаддитивная оптимизация — это хорошо изученная область с десятилетиями исследований, позволяющая использовать передовые алгоритмы, выходящие за рамки жадных (например, ускоренный жадный алгоритм или локальный поиск), теоретические представления о том, когда определенные формулировки работают лучше всего, и расширения для обработки дополнительных ограничений, таких как бюджетные ограничения или требования справедливости.</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://las.inf.ethz.ch/submodularity/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">submodularity.org: Tutorials, References, Activities and Tools for Submodular Optimization</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-42.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/vid_steffi13.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><span style=\"white-space: pre-wrap;\">Тем, кто интересуется субаддитивной оптимизацией, я рекомендую этот сайт, чтобы узнать больше.</span></p></figcaption></figure>",
  "comment_id": "6864cd10ff4ca4000153c921",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/07/Heading---2025-07-03T200946.757.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-07-02T08:09:20.000+02:00",
  "updated_at": "2025-07-04T05:48:06.000+02:00",
  "published_at": "2025-07-04T05:36:02.000+02:00",
  "custom_excerpt": "Many know the importance of query diversity in DeepResearch, but few know how to solve it rigorously via submodular optimization.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "633ffc6b393501004d1c8659",
      "name": "Han Xiao",
      "slug": "han",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
      "cover_image": null,
      "bio": "Founder & CEO of Jina AI",
      "website": null,
      "location": null,
      "facebook": null,
      "twitter": "@hxiao",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "633ffc6b393501004d1c8659",
    "name": "Han Xiao",
    "slug": "han",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png",
    "cover_image": null,
    "bio": "Founder & CEO of Jina AI",
    "website": null,
    "location": null,
    "facebook": null,
    "twitter": "@hxiao",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/han/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/submodular-optimization-for-diverse-query-generation-in-deepresearch/",
  "excerpt": "Многие знают о важности разнообразия запросов в DeepResearch, но мало кто знает, как строго решить эту проблему с помощью субмодульной оптимизации.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
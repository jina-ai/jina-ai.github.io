{
  "slug": "jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images",
  "id": "673cc4a7a7c46d00015cf1f5",
  "uuid": "6ca44950-b989-494a-b587-70847f24edd2",
  "title": "Jina CLIP v2: Мультиязычные мультимодальные эмбеддинги для текста и изображений",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-clip-v2 · Hugging Face</div><div class=\"kg-bookmark-description\">Мы находимся на пути к развитию и демократизации искусственного интеллекта через открытый исходный код и открытую науку.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-11.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-clip-v2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/?sui=&model=jina-clip-v2&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI - Ваша поисковая основа, заряженная суперсилой.</div><div class=\"kg-bookmark-description\">Лучшие в своем классе эмбеддинги, ранжировщики, LLM-ридеры, веб-скрейперы, классификаторы. Лучший поисковый ИИ для многоязычных и мультимодальных данных.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-11.png\" alt=\"\"><span class=\"kg-bookmark-author\">Ваша поисковая основа, заряженная суперсилой.</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/banner-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a><figcaption><p><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> API доступен во вкладке \"Embeddings\".</span></p></figcaption></figure><p>Мультимодальные эмбеддинги позволяют искать и понимать данные в разных модальностях через согласованное представление. Они служат основой нейронного информационного поиска и мультимодальных GenAI приложений. Сегодня мы рады представить <code>jina-clip-v2</code>, новые многоцелевые многоязычные мультимодальные эмбеддинги, построенные на основе <code>jina-clip-v1</code> и недавно выпущенного <code>jina-embeddings-3</code>, с несколькими ключевыми улучшениями:</p><ul><li><strong>Улучшенная производительность</strong>: v2 показывает улучшение производительности на 3% по сравнению с v1 как в задачах поиска текст-изображение, так и текст-текст. Как и v1, текстовый энкодер v2 может служить эффективным многоязычным средством плотного поиска с длинным контекстом. Он работает наравне с нашей передовой моделью <code>jina-embeddings-v3</code> (в настоящее время лучшие многоязычные эмбеддинги с менее чем 1B параметров по MTEB).</li><li><strong>Многоязычная поддержка</strong>: Используя <code>jina-embeddings-v3</code> в качестве текстовой башни, <code>jina-clip-v2</code> поддерживает 89 языков для многоязычного поиска изображений, показывая улучшение до 4% по сравнению с <code>nllb-clip-large-siglip</code> в задачах многоязычного поиска изображений.</li><li><strong>Более высокое разрешение изображений</strong>: v2 теперь поддерживает входное разрешение изображений 512x512, что значительно больше, чем 224x224 в v1. Это более высокое разрешение позволяет лучше обрабатывать детальные изображения, улучшает извлечение признаков и более точное распознавание мелких визуальных элементов.</li><li><strong>Матрёшечные представления</strong>: v2 позволяет пользователям сокращать выходные размерности как текстовых, так и изображенческих эмбеддингов с 1024 до 64, уменьшая затраты на хранение и обработку при сохранении высокой производительности.</li></ul><h2 id=\"model-architecture\">Архитектура модели</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--35-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"></figure><p><code>jina-clip-v2</code> — это модель в стиле CLIP размером 0,9B, которая объединяет два мощных энкодера: текстовый энкодер <code>Jina XLM-RoBERTa</code> (основа <code>jina-embeddings-v3</code>) и визуальный энкодер <code>EVA02-L14</code> (эффективный Vision Transformer, разработанный BAAI). Эти энкодеры совместно обучаются для создания согласованных представлений изображений и текста.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Text Encoder</th>\n<th>Image Encoder</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Base Model</td>\n<td>Jina XLM-RoBERTa</td>\n<td>EVA02-L</td>\n</tr>\n<tr>\n<td>Parameters</td>\n<td>561M</td>\n<td>304M</td>\n</tr>\n<tr>\n<td>Input Specification</td>\n<td>8,192 tokens (max)</td>\n<td>512×512 pixels</td>\n</tr>\n<tr>\n<td>Min Output Dimensions</td>\n<td>64</td>\n<td>64</td>\n</tr>\n<tr>\n<td>Max Output Dimensions</td>\n<td>1,024</td>\n<td>1,024</td>\n</tr>\n<tr>\n<td>Layers</td>\n<td>24</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Attention Mechanism</td>\n<td>FlashAttention2</td>\n<td>xFormers</td>\n</tr>\n<tr>\n<td>Pooling Strategy</td>\n<td>Mean pooling</td>\n<td>CLS pooling</td>\n</tr>\n<tr>\n<td>Additional Features</td>\n<td>89 languages supported</td>\n<td>Patch size 14x14</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"cross-modal-retrieval-performance\">Производительность кросс-модального поиска</h2><p>Jina CLIP v2 обеспечивает многоязычную поддержку для 89 языков с высокой производительностью на основных языках, включая арабский, китайский, английский, французский, немецкий, японский, русский и испанский. В тестах многоязычного поиска изображений она показывает производительность, соответствующую или превосходящую <a href=\"https://huggingface.co/visheratin/nllb-clip-large-siglip?ref=jina-ai-gmbh.ghost.io\">NLLB-CLIP-SigLIP</a>, немного более крупную (1,3B, на 44% больше чем <code>jina-clip-v2</code>) современную модель в стиле CLIP, которая использует предварительно обученный текстовый энкодер из моделей NLLB.</p><h3 id=\"english-only-text-and-images\">Только английский текст и изображения</h3><p>На стандартных тестах кросс-модального поиска (Flickr30k и COCO), <code>jina-clip-v2</code> демонстрирует значительные улучшения по всем параметрам. Она достигает лучшего в своем классе результата 98,0% на Flickr30k в поиске изображение-текст, превосходя как своего предшественника, так и NLLB-CLIP-SigLIP. Модель показывает стабильный прирост во всех сценариях поиска, с заметными улучшениями до 3,3% по сравнению с v1 в поиске изображение-текст на COCO, сохраняя при этом конкурентоспособную производительность с NLLB-CLIP-SigLIP по различным тестам и направлениям модальностей.</p><p><strong>Производительность Flickr30k Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>98.0</td>\n<td>+1.7%</td>\n<td>+0.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>96.4</td>\n<td>-</td>\n<td>-0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>97.1</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>89.8</td>\n<td>+0.9%</td>\n<td>-2.6%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>89.0</td>\n<td>-</td>\n<td>-3.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>92.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Производительность COCO Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Image-to-text</td>\n<td>jina-clip-v2</td>\n<td>81.5</td>\n<td>+3.3%</td>\n<td>+2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>78.9</td>\n<td>-</td>\n<td>-0.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>79.2</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Text-to-image</td>\n<td>jina-clip-v2</td>\n<td>68.4</td>\n<td>+2.9%</td>\n<td>-3.4%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>66.5</td>\n<td>-</td>\n<td>-6.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>70.8</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h3 id=\"multilingual-text-and-images\">Многоязычный текст и изображения</h3><p>На многоязычных кросс-модальных тестах <code>jina-clip-v2</code> демонстрирует устойчивую производительность, особенно выделяясь в поиске изображение-текст, где она превосходит NLLB-SigLIP во всех наборах данных, с улучшением до +3,8% на Crossmodal 3600. Хотя NLLB-SigLIP показывает немного лучшие возможности поиска текст-изображение, разрыв в производительности остается небольшим, обычно в пределах 3%.</p><p><strong>Производительность Image2Text Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>83.23</td>\n<td>+3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>80.16</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>86.03</td>\n<td>+0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.37</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.98</td>\n<td>+0.7%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>85.41</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><strong>Производительность Text2Image Recall@5:</strong></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to NLLB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Crossmodal 3600</td>\n<td>jina-clip-v2</td>\n<td>81.43</td>\n<td>-0.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>82.07</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Multilingual MS Coco</td>\n<td>jina-clip-v2</td>\n<td>84.87</td>\n<td>-3.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.60</td>\n<td>-</td>\n</tr>\n<tr>\n<td>XTD10</td>\n<td>jina-clip-v2</td>\n<td>85.03</td>\n<td>-3.0%</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>87.63</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"text-only-dense-retriever-performance\">Производительность текстового плотного поиска</h2><p>Как и его предшественник, текстовый энкодер <code>jina-clip-v2</code> может служить эффективным многоязычным инструментом плотного поиска. В комплексных тестах Multilingual MTEB он показывает высокую производительность, достигая 69.86% в задачах поиска и 67.77% в задачах семантического сходства. Эти результаты демонстрируют его универсальность, конкурируя с нашей специализированной моделью текстовых эмбеддингов <code>jina-embeddings-v3</code>:</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>69.86</td>\n<td>-3.8%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>72.59</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Semantic Similarity</td>\n<td>jina-clip-v2</td>\n<td>67.77</td>\n<td>-2.9%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-embeddings-v3</td>\n<td>69.81</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>На английских задачах <code>jina-clip-v2</code> показывает постоянное улучшение по сравнению как с предшественником, так и с NLLB-SigLIP, с особенно сильным преимуществом в производительности поиска (почти вдвое превышая показатель NLLB-SigLIP).</p><table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Model</th>\n<th>Score</th>\n<th>Relative to v1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STS</td>\n<td>jina-clip-v2</td>\n<td>81.29</td>\n<td>+0.5%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>80.92</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>74.65</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Retrieval</td>\n<td>jina-clip-v2</td>\n<td>49.33</td>\n<td>+2.1%</td>\n</tr>\n<tr>\n<td></td>\n<td>jina-clip-v1</td>\n<td>48.33</td>\n<td>-</td>\n</tr>\n<tr>\n<td></td>\n<td>nllb-siglip-large</td>\n<td>24.92</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"matryoshka-representation-performance\">Производительность Matryoshka-представлений</h2><p>И текстовый, и графический энкодеры поддерживают MRL, и их выходные размерности могут быть сокращены до 64 при сохранении высокой производительности. Наша оценка усечения эмбеддингов выявила замечательный потенциал сжатия. Даже агрессивное 75%-ное сокращение размерности сохранило более 99% производительности во всех текстовых, графических и кросс-модальных задачах.</p><h3 id=\"image-classification\">Классификация изображений</h3><p>На 37 различных тестах классификации изображений графический энкодер показывает высокую устойчивость к сокращению размерности. Сжатие с 1024 до 64 измерений (сокращение на 94%) приводит только к 8%-ному снижению точности top-5 и 12.5% в top-1, что подчеркивает его потенциал для эффективного развертывания с минимальной потерей производительности.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/accuracy_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Для </span><b><strong style=\"white-space: pre-wrap;\">классификации изображений</strong></b><span style=\"white-space: pre-wrap;\"> мы использовали 19 тестов из набора данных </span><a href=\"https://github.com/google-research/task_adaptation?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VTAB</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">VOC 2007</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/sun397?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">SUN397</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://cs.stanford.edu/~acoates/stl10/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">STL10</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Rendered SST2</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://objectnet.dev/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ObjectNet</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/cvdfoundation/mnist?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MNIST</span></a><span style=\"white-space: pre-wrap;\">, German Traffic Sign Recognition Benchmark (</span><a href=\"https://benchmark.ini.rub.de/gtsrb_dataset.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">GTSRB</span></a><span style=\"white-space: pre-wrap;\">), Fine-Grained Visual Classification of Aircraft (</span><a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FGVC-Aircraft</span></a><span style=\"white-space: pre-wrap;\">), </span><a href=\"https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">FER 2013</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Country211</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://www.tensorflow.org/datasets/catalog/cars196?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Cars196</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/hendrycks/natural-adv-examples?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet-A, ImageNet-O,</span></a><a href=\"https://huggingface.co/datasets/ILSVRC/imagenet-1k?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet1k</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://github.com/HaohanWang/ImageNet-Sketch?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet Sketch</span></a><span style=\"white-space: pre-wrap;\"> и </span><a href=\"https://imagenetv2.org/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">ImageNet v2</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h3 id=\"cross-modal-retrieval\">Кроссмодальный поиск</h3><p>Несмотря на значительное 94% сокращение до всего 64 измерений, кроссмодальный поиск с использованием как усеченных изображений, так и текстовых эмбеддингов остался удивительно эффективным, сохранив 93% производительности в поиске изображений по тексту и 90% в поиске текста по изображениям.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/crossmodal_performance--1-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"><figcaption><span style=\"white-space: pre-wrap;\">Мы использовали шесть тестов, три из которых многоязычные: </span><a href=\"https://google.github.io/crossmodal-3600/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Crossmodal-3600</span></a><span style=\"white-space: pre-wrap;\"> (36 языков), </span><a href=\"https://shannon.cs.illinois.edu/DenotationGraph/?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr30k</span></a><span style=\"white-space: pre-wrap;\"> (только английский), </span><a href=\"https://hockenmaier.cs.illinois.edu/8k-pictures.html?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">flickr8k</span></a><span style=\"white-space: pre-wrap;\"> (только английский), </span><a href=\"https://arxiv.org/abs/1504.00325?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (только английский), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Multilingual MS COCO Captions</span></a><span style=\"white-space: pre-wrap;\"> (10 языков), </span><a href=\"https://github.com/LAION-AI/CLIP_benchmark?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">XTD 200</span></a><span style=\"white-space: pre-wrap;\"> (27 языков)</span></figcaption></figure><h3 id=\"text-only-retrieval\">Поиск только по тексту</h3><p>На <strong>англоязычных тестах MTEB</strong>, 64-мерные текстовые эмбеддинги (сжатые из 1024) отлично сохранили семантическое сходство, потеряв всего 2,1%, в то время как поиск показал умеренное снижение на 17,5%.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/mteb_performance.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"947\" height=\"954\"></figure><h2 id=\"getting-started\">Начало работы</h2><h3 id=\"via-api\">Через API</h3><p>Код демонстрирует, как генерировать эмбеддинги с помощью Python-библиотеки <code>requests</code>. Передайте текстовую строку с изображением в формате base64 или URL, а также желаемый размер измерения (по умолчанию 1024, показано 768 ниже).</p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-Python\">import requests\nimport numpy as np\nfrom numpy.linalg import norm\n\ncos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n\nurl = 'https://api.jina.ai/v1/embeddings'\n\nheaders = {\n  'Content-Type': 'application/json',\n  'Authorization': 'Bearer &lt;YOUR_JINA_AI_API_KEY&gt;'\n}\n\ndata = {\n  'input': [\n     {\"text\": \"Bridge close-shot\"},\n     {\"url\": \"https://fastly.picsum.photos/id/84/1280/848.jpg?hmac=YFRYDI4UsfbeTzI8ZakNOR98wVU7a-9a2tGF542539s\"}],\n  'model': 'jina-clip-v2',\n  'encoding_type': 'float',\n  'dimensions': '768' \n}\n\nresponse = requests.post(url, headers=headers, json=data)\nsim = cos_sim(np.array(response.json()['data'][0]['embedding']), np.array(response.json()['data'][1]['embedding']))\nprint(f\"Cosine text&lt;-&gt;image: {sim}\")</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Не забудьте заменить &lt;YOUR_JINA_AI_API_KEY&gt; на активированный ключ API Jina. Вы можете получить </span><a href=\"https://jina.ai/?sui=apikey&ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\"><span style=\"white-space: pre-wrap;\">бесплатный ключ API с миллионом бесплатных токенов здесь.</span></a></p></figcaption></figure><h3 id=\"image-tokens-pricing\">Ценообразование токенов изображений</h3><p>Наш API учитывает как текстовые, так и токены изображений. Для изображений потребление токенов основано на количестве тайлов 512x512 пикселей, необходимых для покрытия всей площади изображения. Каждый тайл стоит 4000 токенов для обработки, включая частично заполненные тайлы. <strong>Для оптимальной экономической эффективности мы рекомендуем пользователям API изменять размер изображений до 512x512 перед отправкой запросов.</strong></p><table>\n<thead>\n<tr>\n<th>Разрешение изображения</th>\n<th>Необходимые тайлы</th>\n<th>Стоимость в токенах</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>512x512</td>\n<td>1</td>\n<td>4,000</td>\n</tr>\n<tr>\n<td>720x720</td>\n<td>4</td>\n<td>16,000</td>\n</tr>\n<tr>\n<td>1080x1080</td>\n<td>9</td>\n<td>36,000</td>\n</tr>\n</tbody>\n</table>\n<figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--37-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Для квадратных изображений измените размер до 512x512 для лучшей экономической эффективности. Для задач, чувствительных к соотношению сторон, масштабируйте самую длинную сторону до 512, центрируйте изображение и заполните черным цветом. Для общих целей прямое изменение размера до 512x512 работает хорошо.</span></figcaption></figure><h3 id=\"via-csp-marketplaces\">Через маркетплейсы CSP</h3><p>Jina CLIP v2 доступен напрямую на AWS, Azure и GCP по указанным там ценам.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-bfbctuqmky676?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina CLIP v2</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-10.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/socialPreview-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-gb/marketplace/apps?search=Jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-9.ico\" alt=\"\"></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://console.cloud.google.com/marketplace/browse?q=jina&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Cloud console</div><div class=\"kg-bookmark-description\">Тратьте разумно, закупайте быстрее и используйте зарезервированные расходы Google Cloud с помощью Google Cloud Marketplace. Просматривайте каталог из более чем 2000 SaaS, виртуальных машин, стеков разработки и приложений Kubernetes, оптимизированных для работы в Google Cloud.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/default.png\" alt=\"\"></div></div></a></figure><h3 id=\"via-vectordb\"><strong>Через VectorDB</strong></h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pinecone.io/models/jina-clip-v2?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Векторная база данных для создания интеллектуального ИИ | Pinecone</div><div class=\"kg-bookmark-description\">Ищите похожие совпадения среди миллиардов элементов за миллисекунды. Это поиск нового поколения на расстоянии одного API-вызова.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-3.png\" alt=\"\"><span class=\"kg-bookmark-author\">Pinecone Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/docs_og_image.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://weaviate.io/developers/weaviate/model-providers/jinaai/embeddings-multimodal?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Мультимодальные Embeddings | Weaviate</div><div class=\"kg-bookmark-description\">Интеграция Weaviate с API Jina AI позволяет получить прямой доступ к возможностям их моделей из Weaviate.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-12.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Weaviate</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/provider_integrations_jinaai.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant — это векторная база данных с открытым исходным кодом и движок векторного поиска, написанный на Rust. Он обеспечивает быстрый и масштабируемый сервис поиска по векторной схожести с удобным API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-4.png\" alt=\"\"><span class=\"kg-bookmark-author\">edit</span><span class=\"kg-bookmark-publisher\">Qdrant</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-social-preview-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Заключение</h2><p>Основываясь на нашем релизе <code>jina-clip-v1</code> в июне, который расширил модель CLIP от OpenAI с поддержкой текстового ввода до 8192 токенов, и передовой многоязычной модели <code>jina-embeddings-v3</code>, <code>jina-clip-v2</code> привносит три основных улучшения: многоязычную поддержку 89 языков, увеличенное разрешение изображений до 512x512 и обучение представлений по принципу матрёшки для более компактных эмбеддингов.</p><p>CLIP-подобные модели зарекомендовали себя как основа для мультимодальных приложений общего назначения. С помощью <code>jina-clip-v2</code> мы поднимаем эти возможности на новый уровень, преодолевая языковые барьеры для обеспечения более точного кросс-модального понимания и поиска. Мы верим, что этот релиз выполняет обещание сделать мультимодальный поиск и извлечение информации более мощными и доступными для разработчиков по всему миру.</p>",
  "comment_id": "673cc4a7a7c46d00015cf1f5",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/11/clipv2.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-11-19T18:02:31.000+01:00",
  "updated_at": "2024-11-21T17:29:45.000+01:00",
  "published_at": "2024-11-21T17:29:45.000+01:00",
  "custom_excerpt": "Jina-CLIP v2, a 0.9B multimodal embedding model with multilingual support of 89 languages, high image resolution at 512x512, and Matryoshka representations.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images/",
  "excerpt": "Jina-CLIP v2 — мультимодальная модель эмбеддингов размером 0.9B с многоязычной поддержкой 89 языков, высоким разрешением изображений 512x512 и представлениями Matryoshka.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
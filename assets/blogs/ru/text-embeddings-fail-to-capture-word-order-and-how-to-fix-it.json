{
  "slug": "text-embeddings-fail-to-capture-word-order-and-how-to-fix-it",
  "id": "6761676f2defad0001fb5d8a",
  "uuid": "d09f5014-80fc-4f97-a6a3-d903b0a5c105",
  "title": "Текстовые эмбеддинги не сохраняют порядок слов и как это исправить",
  "html": "<p>Недавно Кристоф Шуман, основатель <a href=\"https://laion.ai/team/?ref=jina-ai-gmbh.ghost.io\">LAION AI</a>, поделился интересным наблюдением о моделях текстовых эмбеддингов:</p><blockquote>Когда слова в предложении случайно перемешаны, косинусное сходство между их текстовыми эмбеддингами остается удивительно высоким по сравнению с исходным предложением.</blockquote><p>Например, рассмотрим два предложения: <code>Berlin is the capital of Germany</code> и <code>the Germany Berlin is capital of</code>. Хотя второе предложение не имеет смысла, модели текстовых эмбеддингов практически не могут их различить. При использовании <code>jina-embeddings-v3</code> эти два предложения имеют показатель косинусного сходства 0,9295.</p><p>Порядок слов — не единственное, к чему эмбеддинги, похоже, не очень чувствительны. Грамматические преобразования могут кардинально изменить смысл предложения, но практически не влияют на расстояние между эмбеддингами. Например, <code>She ate dinner before watching the movie</code> и <code>She watched the movie before eating dinner</code> имеют косинусное сходство 0,9833, несмотря на противоположный порядок действий.</p><p>Отрицание также известно как сложное явление для последовательного встраивания без <a href=\"https://jina.ai/news/training-smarter-not-harder-slimming-sentence-embeddings/?ref=jina-ai-gmbh.ghost.io#triplet-training-targets-specificity\">специального обучения</a> — <code>This is a useful model</code> и <code>This is not a useful model</code> выглядят практически одинаково в пространстве эмбеддингов. Часто замена слов в тексте на другие того же класса, например, изменение \"today\" на \"yesterday\", или изменение времени глагола, не меняет эмбеддинги настолько сильно, как можно было бы ожидать.</p><p>Это имеет серьезные последствия. Рассмотрим два поисковых запроса: <code>Flight from Berlin to Amsterdam</code> и <code>Flight from Amsterdam to Berlin</code>. Они имеют почти идентичные эмбеддинги — <code>jina-embeddings-v3</code> присваивает им косинусное сходство 0,9884. Для реального приложения, такого как поиск авиабилетов или логистика, этот недостаток является фатальным.</p><p>В этой статье мы рассмотрим проблемы, с которыми сталкиваются модели эмбеддингов, изучая их постоянные трудности с порядком и выбором слов. Мы проанализируем основные режимы отказа в различных лингвистических категориях — включая направленные, временные, причинно-следственные, сравнительные и отрицательные контексты — исследуя при этом стратегии улучшения производительности модели.</p><h2 id=\"why-do-shuffled-sentences-have-surprisingly-close-cosine-scores\">Почему перемешанные предложения имеют удивительно близкие косинусные оценки?</h2><p>Сначала мы думали, что это может быть связано с тем, как модель комбинирует значения слов — она создает эмбеддинг для каждого слова (6-7 слов в каждом из наших примеров предложений выше) и затем усредняет эти эмбеддинги с помощью среднего объединения. Это означает, что очень мало информации о порядке слов доступно для финального эмбеддинга. Среднее значение одинаково независимо от порядка значений.</p><p>Однако даже модели, использующие CLS-объединение (которое рассматривает специальное первое слово для понимания всего предложения и должно быть более чувствительным к порядку слов), имеют ту же проблему. Например, <code>bge-1.5-base-en</code> все равно дает оценку косинусного сходства 0,9304 для предложений <code>Berlin is the capital of Germany</code> и <code>the Germany Berlin is capital of</code>.</p><p>Это указывает на ограничение в том, как обучаются модели эмбеддингов. Хотя языковые модели изначально изучают структуру предложений во время предварительного обучения, они, похоже, теряют часть этого понимания во время контрастивного обучения — процесса, который мы используем для создания моделей эмбеддингов.</p><h2 id=\"how-do-text-length-and-word-order-impact-embedding-similarity\">Как длина текста и порядок слов влияют на сходство эмбеддингов?</h2><p>Почему у моделей возникают проблемы с порядком слов? Первое, что приходит на ум, — это длина текста (в токенах). Когда текст отправляется в функцию кодирования, модель сначала генерирует список эмбеддингов токенов (то есть каждое токенизированное слово имеет выделенный вектор, представляющий его значение), а затем усредняет их.</p><p>Чтобы увидеть, как длина текста и порядок слов влияют на сходство эмбеддингов, мы создали <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\">набор данных из 180 синтетических предложений</a> различной длины: 3, 5, 10, 15, 20 и 30 токенов. Мы также случайным образом перемешали токены, чтобы сформировать вариации каждого предложения:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet-random-shuffle?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet-random-shuffle · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-16.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet-random-shuffle.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Вот несколько примеров:</p>\n<!--kg-card-begin: html-->\n<table id=\"f455664c-d258-4c55-9a8f-a9bcc5203c74\" class=\"simple-table\"><thead class=\"simple-table-header\"><tr id=\"54abe148-ee87-470f-a05e-4c2bec2feafd\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">Длина (токены)</th><th id=\"usZ}\" class=\"simple-table-header-color simple-table-header\">Исходное предложение</th><th id=\"ju?f\" class=\"simple-table-header-color simple-table-header\">Перемешанное предложение</th></tr></thead><tbody><tr id=\"fc9b17e6-8ce4-43c8-aee9-d2fbee6290f6\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">3</th><td id=\"usZ}\" class=\"\">The cat sleeps</td><td id=\"ju?f\" class=\"\">cat The sleeps</td></tr><tr id=\"cbd662b9-b080-4269-929e-b4308c506002\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">5</th><td id=\"usZ}\" class=\"\">He drives his car carefully</td><td id=\"ju?f\" class=\"\">drives car his carefully He</td></tr><tr id=\"aea07e66-d0e5-4eec-ad1f-a987438fc448\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">15</th><td id=\"usZ}\" class=\"\">The talented musicians performed beautiful classical music at the grand concert hall yesterday</td><td id=\"ju?f\" class=\"\">in talented now grand classical yesterday The performed musicians at hall concert the music</td></tr><tr id=\"f59d8da8-7ed5-49cd-9077-77aac31c2398\"><th id=\"s~RQ\" class=\"simple-table-header-color simple-table-header\">30</th><td id=\"usZ}\" class=\"\">The passionate group of educational experts collaboratively designed and implemented innovative teaching methodologies to improve learning outcomes in diverse classroom environments worldwide</td><td id=\"ju?f\" class=\"\">group teaching through implemented collaboratively outcomes of methodologies across worldwide diverse with passionate and in experts educational classroom for environments now by learning to at improve from innovative The designed</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Мы закодируем набор данных, используя нашу собственную модель <code>jina-embeddings-v3</code> и модель с открытым исходным кодом <code>bge-base-en-v1.5</code>, затем вычислим косинусное сходство между исходным и перемешанным предложениями:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Длина (токены)</th>\n<th>Среднее косинусное сходство</th>\n<th>Стандартное отклонение косинусного сходства</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0,947</td>\n<td>0,053</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0,909</td>\n<td>0,052</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0,924</td>\n<td>0,031</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0,918</td>\n<td>0,019</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0,899</td>\n<td>0,021</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0,874</td>\n<td>0,025</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Теперь мы можем создать диаграмму размаха, которая делает тенденцию в косинусном сходстве более очевидной:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"589\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--22-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--22-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--22-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 1: Распределение сходства по длине предложения для перемешанных предложений с </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> и </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (без дополнительной настройки)</span></figcaption></figure><p>Как мы видим, существует четкая линейная зависимость в среднем косинусном сходстве эмбеддингов. Чем длиннее текст, тем ниже средний показатель косинусного сходства между исходными и случайно перемешанными предложениями. Это, вероятно, происходит из-за \"смещения слов\", а именно того, насколько далеко слова переместились от своих исходных позиций после случайного перемешивания. В более коротком тексте просто меньше \"слотов\" для перемещения токена, поэтому он не может переместиться так далеко, в то время как более длинный текст имеет большее количество потенциальных перестановок, и слова могут перемещаться на большее расстояние.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"866\" height=\"452\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--2-.png 866w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 2: Комбинации предложений по количеству слов</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">На этом мы остановим таблицу, поскольку количество комбинаций равно факториалу количества слов. К тому времени, когда мы доходим до тридцати слов, получаем 265 нониллионов (2.652528598 E+32) комбинаций.</div></div><p>Как показано на рисунке ниже (Косинусное сходство vs Среднее смещение слов), чем длиннее текст, тем больше смещение слов:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1183\" height=\"593\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--23-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--23-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--23-.png 1183w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 3: Косинусное сходство vs Среднее смещение слов на наборе данных с перемешанными предложениями, показывающее корреляцию между средним смещением слов и косинусным различием.</span></figcaption></figure><p>Вложения токенов зависят от локального контекста, то есть от слов, находящихся ближе всего к ним. В коротком тексте перестановка слов не может сильно изменить этот контекст. Однако в более длинном тексте слово может быть перемещено очень далеко от своего исходного контекста, что может значительно изменить его токенное вложение. В результате перемешивание слов в более длинном тексте создает более отдаленное вложение, чем в коротком. На рисунке выше показано, что как для <code>jina-embeddings-v3</code> с использованием усреднения, так и для <code>bge-base-en-v1.5</code> с использованием CLS-пулинга, действует одна и та же закономерность: перемешивание более длинных текстов и большее смещение слов приводят к меньшим показателям сходства.</p><h2 id=\"do-bigger-models-solve-the-problem\">Решают ли проблему большие модели?</h2><p>Обычно, когда мы сталкиваемся с такого рода проблемой, распространенной тактикой является использование более крупной модели. Но может ли большая модель вложения текста действительно эффективнее захватывать информацию о порядке слов? Согласно закону масштабирования моделей текстовых вложений (<a href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\">упомянутому в нашем посте о релизе <code>jina-embeddings-v3</code></a>), более крупные модели обычно обеспечивают лучшую производительность:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2003\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--24-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--24-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--24-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--24-.png 2045w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 4: Закон масштабирования моделей вложений, показывающий масштабирование производительности MTEB с количеством параметров.</span></figcaption></figure><p>Но может ли более крупная модель эффективнее захватывать информацию о порядке слов? Мы протестировали три варианта модели BGE: <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-small-en-v1.5</code></a>, <a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-base-en-v1.5</code></a> и <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code>bge-large-en-v1.5</code></a>, с размерами параметров 33 миллиона, 110 миллионов и 335 миллионов соответственно.</p><p>Мы будем использовать те же 180 предложений, что и раньше, но без учета информации о длине. Мы закодируем как исходные предложения, так и их случайные перестановки, используя три варианта модели, и построим график среднего косинусного сходства:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1484\" height=\"584\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/size.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/size.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/size.png 1484w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 5: Влияние размера модели на чувствительность к порядку слов на наборе данных с перемешанными предложениями с использованием </span><a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-small-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://huggingface.co/BAAI/bge-base-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\"> и </span><a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-large-en-v1.5</span></code></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Хотя мы видим, что более крупные модели более чувствительны к вариациям порядка слов, разница небольшая. Даже значительно более крупная <code>bge-large-en-v1.5</code> лишь немного лучше различает перемешанные предложения от неперемешанных. Другие факторы играют роль в определении того, насколько чувствительна модель вложений к перестановкам слов, особенно различия в режиме обучения. Более того, косинусное сходство - это очень ограниченный инструмент для измерения способности модели проводить различия. Однако мы видим, что размер модели не является основным фактором. Мы не можем просто увеличить нашу модель и решить эту проблему.</p><h2 id=\"word-order-and-word-choice-in-the-real-world\">Порядок слов и выбор слов в реальном мире</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">На протяжении большей части этого поста мы используем <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/?ref=jina-ai-gmbh.ghost.io\"><code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v2</code></a> (<i><em class=\"italic\" style=\"white-space: pre-wrap;\">не</em></i> нашу самую последнюю модель, <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-embeddings-v3</code>), поскольку <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v2</code> намного меньше и, следовательно, быстрее для экспериментов на наших локальных GPU, имея 137 млн параметров против 580 млн у <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">v3</code>.</div></div><p>Как мы упоминали во введении, порядок слов - не единственная проблема для моделей вложений. Более реалистичная проблема в реальном мире связана с <em>выбором</em> слов. Существует много способов поменять слова в предложении - способов, которые плохо отражаются во вложениях. Мы можем взять \"Она летела из Парижа в Токио\" и изменить его на \"Она ехала из Токио в Париж\", и вложения остаются похожими. Мы отобразили это по нескольким категориям изменений:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Категория</th>\n<th>Пример - Слева</th>\n<th>Пример - Справа</th>\n<th>Косинусное сходство (<code>jina</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Направление</td>\n<td>Она летела из Парижа в Токио</td>\n<td>Она ехала из Токио в Париж</td>\n<td>0.9439</td>\n</tr>\n<tr>\n<td>Временная</td>\n<td>Она поужинала перед просмотром фильма</td>\n<td>Она посмотрела фильм перед ужином</td>\n<td>0.9833</td>\n</tr>\n<tr>\n<td>Причинная</td>\n<td>Повышение температуры растопило снег</td>\n<td>Тающий снег охладил температуру</td>\n<td>0.8998</td>\n</tr>\n<tr>\n<td>Сравнительная</td>\n<td>Кофе вкуснее чая</td>\n<td>Чай вкуснее кофе</td>\n<td>0.9457</td>\n</tr>\n<tr>\n<td>Отрицание</td>\n<td>Он стоит возле стола</td>\n<td>Он стоит далеко от стола</td>\n<td>0.9116</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Обратите внимание, что это распространенные случаи, которые мы наблюдали во время нашей работы, и они не обязательно представляют исчерпывающую таксономию категорий.</div></div><p>Таблица выше показывает список \"проблемных случаев\", когда модель текстовых эмбеддингов не справляется с тонкими изменениями слов. Это соответствует нашим ожиданиям: модели текстовых эмбеддингов не обладают способностью к рассуждению. Например, модель не понимает отношений между \"из\" и \"в\". Модели текстовых эмбеддингов выполняют семантическое сопоставление, где семантика обычно захватывается на уровне токенов и затем сжимается в единый плотный вектор после объединения. В отличие от этого, <a href=\"https://arxiv.org/abs/2206.07682?ref=jina-ai-gmbh.ghost.io\">LLM (авторегрессивные модели), обученные на больших наборах данных триллионного масштаба, начинают демонстрировать возникающие способности к рассуждению</a>.</p><p>Это заставило нас задуматься: можем ли мы дообучить модель эмбеддингов с помощью контрастивного обучения, используя триплеты, чтобы приблизить запрос и положительный пример, одновременно отдаляя запрос от отрицательного примера?</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Copy-of-Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 6: Эффект контрастивного обучения: сближение запроса с положительным примером и отдаление от отрицательного.</span></figcaption></figure><p>Например, \"Рейс из Амстердама в Берлин\" можно рассматривать как отрицательную пару для \"Рейс из Берлина в Амстердам\". Фактически, в <a href=\"https://arxiv.org/pdf/2307.11224?ref=jina-ai-gmbh.ghost.io\">техническом отчете <code>jina-embeddings-v1</code></a> (Michael Guenther и др.), мы кратко рассмотрели этот вопрос в небольшом масштабе: мы дообучили модель <code>jina-embeddings-v1</code> на <a href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\">наборе данных с отрицаниями</a>, содержащем 10 000 примеров, сгенерированных большими языковыми моделями.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/jinaai/negation-dataset?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/negation-dataset · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-17.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/negation-dataset.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Результаты, представленные в отчете по ссылке выше, были многообещающими:</p><blockquote>Мы наблюдаем, что для всех размеров моделей дообучение на данных с триплетами (включая наш набор данных с отрицаниями) значительно улучшает производительность, особенно в задаче HardNegation.</blockquote><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1333\" height=\"616\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--25-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--25-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--25-.png 1333w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 7: Таблица показывает оценки EasyNegation и HardNegation для нескольких размеров моделей </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\"> с попарным и комбинированным триплетным/попарным обучением.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1550\" height=\"949\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/graph-big.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/graph-big.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/graph-big.png 1550w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 8: Сравнение производительности стратегий обучения между разными версиями </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><h2 id=\"fine-tuning-text-embedding-models-with-curated-datasets\">Дообучение моделей текстовых эмбеддингов на курированных наборах данных</h2><p>В предыдущих разделах мы изучили несколько ключевых наблюдений относительно текстовых эмбеддингов:</p><ol><li>Короткие тексты более подвержены ошибкам в захвате порядка слов.</li><li>Увеличение размера модели текстовых эмбеддингов не обязательно улучшает понимание порядка слов.</li><li>Контрастивное обучение может предложить потенциальное решение этих проблем.</li></ol><p>Учитывая это, мы дообучили <code>jina-embeddings-v2-base-en</code> и <code>bge-base-en-1.5</code> на наших наборах данных с отрицаниями и порядком слов (всего около 11 000 обучающих примеров):</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-jina · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-18.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-jina.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/bwang0911/word-order-bge?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-order-bge · Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-19.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-order-bge.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Чтобы помочь оценить дообучение, мы создали <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">набор данных</a> из 1 000 триплетов, состоящих из <code>query</code>, <code>positive (pos)</code> и <code>negative (neg)</code> случаев:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">bwang0911/word-orders-triplet · Datasets at Hugging Face</div><div class=\"kg-bookmark-description\">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-20.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/word-orders-triplet.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Вот пример строки:</p>\n<!--kg-card-begin: html-->\n<table>\n<tbody>\n<tr>\n<td>Anchor</td>\n<td><code>The river flows from the mountains to the sea</code></td>\n</tr>\n<tr>\n<td>Positive</td>\n<td><code>Water travels from mountain peaks to ocean</code></td>\n</tr>\n<tr>\n<td>Negative</td>\n<td><code>The river flows from the sea to the mountains</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Эти триплеты разработаны для охвата различных проблемных случаев, включая изменения смысла из-за порядка слов в <strong>направлении</strong>, <strong>времени</strong> и <strong>причинно-следственных связях</strong>.</p><p>Теперь мы можем оценить модели на трех разных наборах для оценки:</p><ol><li>Набор из 180 синтетических предложений (из предыдущей части этого поста), случайно перемешанных.</li><li>Пять вручную проверенных примеров (из таблицы направлений/причинно-следственных связей и т.д. выше).</li><li>94 курированных триплета из нашего <a href=\"https://huggingface.co/datasets/bwang0911/word-orders-triplet?ref=jina-ai-gmbh.ghost.io\">набора данных с триплетами</a>, который мы только что создали.</li></ol><p>Вот разница для перемешанных предложений до и после дообучения:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Длина предложения (токены)</th>\n<th>Среднее косинусное сходство (<code>jina</code>)</th>\n<th>Среднее косинусное сходство (<code>jina-ft</code>)</th>\n<th>Среднее косинусное сходство (<code>bge</code>)</th>\n<th>Среднее косинусное сходство (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0.970</td>\n<td>0.927</td>\n<td>0.929</td>\n<td>0.899</td>\n</tr>\n<tr>\n<td>5</td>\n<td>0.958</td>\n<td>0.910</td>\n<td>0.940</td>\n<td>0.916</td>\n</tr>\n<tr>\n<td>10</td>\n<td>0.953</td>\n<td>0.890</td>\n<td>0.934</td>\n<td>0.910</td>\n</tr>\n<tr>\n<td>15</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.912</td>\n<td>0.875</td>\n</tr>\n<tr>\n<td>20</td>\n<td>0.916</td>\n<td>0.815</td>\n<td>0.901</td>\n<td>0.879</td>\n</tr>\n<tr>\n<td>30</td>\n<td>0.927</td>\n<td>0.819</td>\n<td>0.877</td>\n<td>0.852</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Результат кажется очевидным: несмотря на то, что процесс дообучения занял всего пять минут, мы видим значительное улучшение производительности на наборе данных со случайно перемешанными предложениями:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"784\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--26-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--26-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--26-.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 9: Распределение сходства по длине предложений для перемешанных предложений с </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> и </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>bge-base-en-1.5</span></code><span style=\"white-space: pre-wrap;\"> (после дообучения).</span></figcaption></figure><p>Мы также наблюдаем улучшения в случаях с направлением, временем, причинно-следственными связями и сравнениями. Модель показывает существенное улучшение производительности, что отражается в снижении усредненного косинусного сходства. Наибольший прирост производительности наблюдается в случае отрицания, благодаря тому, что наш набор данных для дообучения содержал 10 000 примеров с отрицанием.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Категория</th>\n<th>Пример - Левый</th>\n<th>Пример - Правый</th>\n<th>Среднее косинусное сходство (<code>jina</code>)</th>\n<th>Среднее косинусное сходство (<code>jina-ft</code>)</th>\n<th>Среднее косинусное сходство (<code>bge</code>)</th>\n<th>Среднее косинусное сходство (<code>bge-ft</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Направление</td>\n<td>She flew from Paris to Tokyo.</td>\n<td>She drove from Tokyo to Paris</td>\n<td>0.9439</td>\n<td>0.8650</td>\n<td>0.9319</td>\n<td>0.8674</td>\n</tr>\n<tr>\n<td>Время</td>\n<td>She ate dinner before watching the movie</td>\n<td>She watched the movie before eating dinner</td>\n<td>0.9833</td>\n<td>0.9263</td>\n<td>0.9683</td>\n<td>0.9331</td>\n</tr>\n<tr>\n<td>Причина</td>\n<td>The rising temperature melted the snow</td>\n<td>The melting snow cooled the temperature</td>\n<td>0.8998</td>\n<td>0.7937</td>\n<td>0.8874</td>\n<td>0.8371</td>\n</tr>\n<tr>\n<td>Сравнение</td>\n<td>Coffee tastes better than tea</td>\n<td>Tea tastes better than coffee</td>\n<td>0.9457</td>\n<td>0.8759</td>\n<td>0.9723</td>\n<td>0.9030</td>\n</tr>\n<tr>\n<td>Отрицание</td>\n<td>He is standing by the table</td>\n<td>He is standing far from the table</td>\n<td>0.9116</td>\n<td>0.4478</td>\n<td>0.8329</td>\n<td>0.4329</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Заключение</h2><p>В этой статье мы углубились в проблемы, с которыми сталкиваются модели текстовых эмбеддингов, особенно в их трудностях с эффективной обработкой порядка слов. Если разбить по категориям, мы выделили пять основных типов ошибок: <strong>Направление</strong>, <strong>Время</strong>, <strong>Причина</strong>, <strong>Сравнение</strong> и <strong>Отрицание</strong>. Это те типы запросов, где порядок слов действительно важен, и если ваш случай использования включает любой из них, стоит знать об ограничениях этих моделей.</p><p>Мы также провели быстрый эксперимент, расширив набор данных, ориентированный на отрицание, чтобы охватить все пять категорий ошибок. Результаты оказались многообещающими: дообучение с тщательно подобранными \"сложными негативными примерами\" улучшило способность модели распознавать, какие элементы связаны между собой, а какие нет. Тем не менее, предстоит еще много работы. Будущие шаги включают более глубокое изучение того, как размер и качество набора данных влияют на производительность.</p>",
  "comment_id": "6761676f2defad0001fb5d8a",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-17T12:58:39.000+01:00",
  "updated_at": "2024-12-17T16:30:27.000+01:00",
  "published_at": "2024-12-17T16:30:27.000+01:00",
  "custom_excerpt": "Text embedding models struggle with capturing subtle linguistic nuances like word order, directional relationships, temporal sequences, causal connections, comparisons, and negation. Understanding these challenges is key to improving model performance.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it/",
  "excerpt": "Модели текстовых эмбеддингов испытывают трудности с захватом тонких лингвистических нюансов, таких как порядок слов, направленные отношения, временные последовательности, причинно-следственные связи, сравнения и отрицания. Понимание этих проблем является ключом к улучшению производительности моделей.",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
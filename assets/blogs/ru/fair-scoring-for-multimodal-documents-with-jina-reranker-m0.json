{
  "slug": "fair-scoring-for-multimodal-documents-with-jina-reranker-m0",
  "id": "682b34d62caa92000178b523",
  "uuid": "434b7cc3-713d-4f2e-843a-6270f0e27604",
  "title": "Справедливая оценка для мультимодальных документов с помощью jina-reranker-m0",
  "html": "<p>Представьте, что вы разрабатываете систему поиска спортивных новостей. Пользователь ищет \"теннисисты празднуют победу в чемпионате\", и вам нужно найти наиболее релевантные статьи из вашей базы данных. Каждая статья содержит как текстовую подпись, так и изображение — типичный пример современного освещения спортивных событий.</p><p>Вашей системе необходимо принять <strong>текстовый запрос</strong> и вернуть <strong>ранжированный список наиболее релевантных мультимодальных документов</strong> из вашего корпуса. Звучит просто, но есть фундаментальная проблема, которая ломает все очевидные подходы.</p><p>Вот что происходит, когда вы пытаетесь ранжировать эти документы. Ваша модель для создания векторных представлений (Embedding) , скажем, <code>jina-clip-v2</code>, выдает оценки сходства следующим образом:</p>\n<!--kg-card-begin: html-->\n<table>\n    <thead>\n        <tr>\n            <th>Статья</th>\n            <th>Тип контента</th>\n            <th>Описание</th>\n            <th>Оценка сходства</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>A</td>\n            <td>Текст</td>\n            <td>Новак Джокович выигрывает финал Открытого чемпионата Австралии в трех сетах</td>\n            <td>0.72</td>\n        </tr>\n        <tr>\n            <td>A</td>\n            <td>Изображение</td>\n            <td>[фотография игрока, держащего трофей и улыбающегося]</td>\n            <td>0.31</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Текст</td>\n            <td>Погодные задержки влияют на расписание турнира на открытом воздухе</td>\n            <td>0.23</td>\n        </tr>\n        <tr>\n            <td>B</td>\n            <td>Изображение</td>\n            <td>[фотография прыгающих и празднующих теннисистов]</td>\n            <td>0.54</td>\n        </tr>\n    </tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Какая статья более релевантна? Статья A имеет высокую оценку текста, но низкую оценку изображения. Статья B имеет низкую оценку текста, но более высокую оценку изображения. Фундаментальная проблема заключается в том, что <strong>нельзя сравнивать 0.72 (текст) с 0.54 (изображение)</strong>, потому что эти оценки сходства существуют в совершенно разных масштабах.</p><h2 id=\"when-trivial-solutions-fail\">Когда тривиальные решения терпят неудачу</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The What and Why of Text-Image Modality Gap in CLIP Models</div><div class=\"kg-bookmark-description\">You can’t just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-32.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span><span class=\"kg-bookmark-publisher\">Bo Wang, Scott Martens</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/the-what-and-why-of-text-image-modality-gap-in-clip-models.webp\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p><strong>Из-за разрыва модальности</strong> в <code>jina-clip-v2</code> или почти в каждой другой модели, подобной CLIP, любой очевидный подход, который вы можете попробовать, не работает. Если вы просто используете более высокую оценку, вы столкнетесь с тем, что текстовые оценки кластеризуются около 0.2-0.8, а оценки изображений кластеризуются около 0.4-0.6. Это означает, что посредственное текстовое совпадение (0.6) всегда будет превосходить отличное совпадение изображения (0.5).</p><p>Усреднение оценок тоже не помогает. Вычисление (0.7 + 0.3)/2 = 0.5 дает вам число, но что оно на самом деле означает? Вы усредняете принципиально бессмысленные величины. Точно так же любая фиксированная схема взвешивания является произвольной — иногда текст имеет большее значение, иногда изображения, и это полностью зависит от конкретного запроса и документа.</p><p>Даже предварительная нормализация оценок не решает основную проблему. Вы все еще пытаетесь объединить принципиально разные меры сходства, которые отражают разные аспекты релевантности.</p><h2 id=\"what-actually-happens\">Что происходит на самом деле</h2><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2305.13631\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">EDIS: Entity-Driven Image Search over Multimodal Web Content</div><div class=\"kg-bookmark-description\">Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \\textbf{E}ntity-\\textbf{D}riven \\textbf{I}mage \\textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and a large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-20.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Siqi Liu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-16.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Чтобы лучше понять, с чем мы работаем, вот пример документа из <a href=\"https://arxiv.org/abs/2305.13631\">набора данных EDIS</a>, показывающий изображение (матч немецкого футбола) и подпись (<code>One More Field Where the Content Trails Germany</code>).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"928\" height=\"261\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-1.png 928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 1: Пример мультимодального документа, содержащего как изображение, так и текст. Поскольку у нас есть две модальности, для любого данного запроса теперь есть </span><i><em class=\"italic\" style=\"white-space: pre-wrap;\">два</em></i><span style=\"white-space: pre-wrap;\"> семантических разрыва (между запросом и текстом, и запросом и изображением). Чтобы получить наилучшие результаты, следует ли нам искать текстовое содержание документов или содержание изображения?</span></figcaption></figure><p>В целом, <code>jina-clip-v2</code> показывает гораздо более высокие показатели сходства при сравнении запроса с текстом, чем запроса с изображением в наборе данных EDIS, отчасти из-за того, как была обучена модель, и отчасти из-за самого набора данных:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-2.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 2: Оценки сходства между запросом и изображением (красным) и запросом и текстом (синим) с использованием </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Поэтому кажется логичным извлекать документ на основе его текста, а не изображения. И, как мы видим на графике ниже, мы получаем гораздо лучшие результаты, сравнивая текстовый запрос <code>... for undocumented immigrants helping to establish legal status in the United States</code> с текстовым содержанием корпуса. Фактически, поиск по изображению вообще не позволяет получить документ с истинными данными (выделен желтым):</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1767\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-3.png 1767w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 3: Пример, когда документ с истинными данными (выделен желтой границей) может быть извлечен только с помощью </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">'s query-to-text retrieval при использовании </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> of 3.</span></figcaption></figure><p>Но не обманывайтесь. Несмотря на то, что запрос к тексту показывает более высокие оценки сходства, оценки сходства запроса к тексту и запроса к изображению <em>не</em> сопоставимы. Мы можем увидеть это, взглянув на recall@10, когда мы используем <code>jina-clip-v2</code> для извлечения 32 документов из набора данных EDIS. Очевидно, что recall выше при запросе к <em>изображению</em>:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Recall@10</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Запрос к тексту</td>\n<td>14.55</td>\n</tr>\n<tr>\n<td>Запрос к изображению</td>\n<td><strong>22.38</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Мы можем увидеть это ниже: Если мы используем запрос из набора данных, <code>Ear ear An elephant is decorated with Bhartiya Janta Party symbols near the BJP headquarters in New Delhi.</code>, мы можем извлечь документ с истинными данными только по его изображению. Поиск по его текстовому содержанию не возвращает никаких совпадений:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1753\" height=\"2454\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-4.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-4.png 1753w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 4: Пример, когда документ с истинными данными (выделен желтой границей) может быть извлечен только с помощью </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">'s query-to-image retrieval при использовании </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> of 3.</span></figcaption></figure><p>Итак, если оценки сходства подразумевают, что мы должны извлекать документы из их текста, а полнота извлечения (recall) подразумевает, что мы должны извлекать их из их изображений, что же нам выбрать? Разумеется, рисунки 3 и 4 не показывают явного победителя. Какая модальность <em>действительно</em> представляет наиболее близкое соответствие между нашим запросом и документом, который мы ищем? И если мы хотим объединить кандидатов как из поиска по тексту запроса, так и из поиска по изображению, как мы можем осмысленно выбрать лучшие соответствия, если мы даже не можем сравнить оценки? Очевидно, что просто использовать <code>jina-clip-v2</code> не получится. Нам нужно добавить еще одну модель в этот микс.</p><h2 id=\"a-simple-two-stage-pipeline\">Простая двухэтапная конвейерная обработка</h2><p>В апреле 2025 года мы выпустили <code>jina-reranker-m0</code>, многоязычный мультимодальный 重排器 (Reranker) для извлечения визуальных документов. Ниже мы видим его более узкий модальный разрыв, где <code>jina-reranker-m0</code> показывает сопоставимые оценки сходства между запросом и текстом, а также между запросом и изображением, в отличие от гораздо большего разрыва, показанного <code>jina-clip-v2</code>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"964\" height=\"679\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/Distribution_of_similarity_between_query_and_corpus_in_jina-reranker-m0.png 964w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 6: По сравнению с </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\">, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\"> показывает гораздо меньшую разницу между оценками сходства запроса и изображения (красный) и запроса и текста (синий).</span></figcaption></figure><p>Имея это в виду, мы можем использовать <code>jina-reranker-m0</code> для второго прохода в цепочке извлечения, после того как первоначальные результаты будут извлечены из <code>jina-clip-v2</code>:</p><p><strong>Этап 1: Извлечение кандидатов из обеих модальностей</strong></p><ul><li>Используйте <code>jina-clip-v2</code>, чтобы получить 16 документов с помощью текстового поиска + 16 с помощью поиска по изображению</li><li>Примите, что мы пока не можем сравнивать оценки</li></ul><p><strong>Этап 2: Унифицированное переранжирование</strong></p><ul><li>Подайте каждую пару (запрос + полный документ) в <code>jina-reranker-m0</code></li><li>重排器 (Reranker) обрабатывает как текст, так и изображение вместе</li><li>Вывод: единая оценка релевантности по унифицированной шкале</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1305\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-5.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-5.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-5.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-5.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 5: Индексация мультимодальных документов и двухэтапный процесс мультимодального извлечения с помощью </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-clip-v2</span></code><span style=\"white-space: pre-wrap;\"> и </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-reranker-m0</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>Мы расширили эксперименты из таблицы 1, теперь используя <code>jina-clip-v2</code> для извлечения документов из корпуса, а затем <code>jina-reranker-m0</code> для их переранжирования:</p><ol><li>Извлеките 32 документа с помощью запроса к тексту, затем переранжируйте на основе оценки запроса к тексту.</li><li>Извлеките 32 документа с помощью запроса к изображению, затем переранжируйте на основе оценки запроса к изображению.</li><li>Извлеките 16 документов с помощью запроса к тексту и 16 с помощью запроса к изображению. Переранжируйте на основе оценки запроса к тексту или запроса к изображению, в зависимости от модальности запроса.</li><li>Извлеките 16 документов с помощью запроса к тексту и 16 с помощью запроса к изображению. Переранжируйте на основе усредненных оценок запроса к тексту и запроса к изображению для каждого документа, получив итоговую оценку (запрос к тексту + запрос к изображению)/2.</li></ol><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Обратите внимание, что мы измеряем производительность zero-shot на EDIS. Мы не дообучали ни <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code>, ни <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-reranker-m0</code> с использованием этого набора данных.</div></div>\n<!--kg-card-begin: html-->\n\n<table>\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>Description</th>\n      <th>Recall@10 - with jina-clip-v2</th>\n      <th>Recall@10 - with jina-reranker-m0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>32 docs: query-to-text</td>\n      <td>14.55</td>\n      <td>17.42</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>32 docs: query-to-image</td>\n      <td>22.38</td>\n      <td>28.94</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image</td>\n      <td>14.55</td>\n      <td>33.81</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16 docs: query-to-text<br>16 docs: query-to-image<br>Combined average reranker scores</td>\n      <td>14.55</td>\n      <td><strong>36.24</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Эксперименты 1, 3 и 4 показывают одинаковый результат для recall@10 с <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">jina-clip-v2</code> из-за того, что оценки запроса к тексту выше, чем оценки запроса к изображению. Следовательно, в первой десятке результатов преобладают документы, извлеченные по тексту.</div></div><p>Как мы видим, выполняя второй проход с помощью <code>jina-reranker-m0</code>, полнота извлечения (recall) увеличивается по всем направлениям, независимо от модальности. Однако, <strong>мы видим наибольшее увеличение, когда объединяем как текстовое, так и графическое содержимое из извлеченных документов</strong>, достигая полноты извлечения (recall)@10 в 36,24. Визуальный пример показывает, что <code>jina-reranker-m0</code> последовательно ранжирует документ, соответствующий истине, первым, независимо от того, ищется ли текст или изображение:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/05/clip-vs-reranker.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1146\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/clip-vs-reranker.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/clip-vs-reranker.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/clip-vs-reranker.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/05/clip-vs-reranker.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 7: Пример запросов (слева) и </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>top_k</span></code><span style=\"white-space: pre-wrap;\"> из 1 результата для каждой методологии переранжирования (четыре столбца справа), показывающий, что объединение оценок сходства изображений и текста последовательно ранжирует документ, соответствующий истине, первым.</span></figcaption></figure><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">В то время как на рисунках 3 и 4 показан <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> из 3 для различных методов извлечения, в целях экономии места на рисунке 7 показан только <code spellcheck=\"false\" style=\"white-space: pre-wrap;\">top_k</code> из 1 для каждого запроса.</div></div><h2 id=\"conclusions\">Выводы</h2><p>Этот простой двухэтапный подход обеспечивает улучшение полноты извлечения (recall) на 62%, поскольку система, наконец, использует то, что люди делают естественным образом: учитывает то, что мы читаем, и то, что мы видим, чтобы определить релевантность. Этот урок выходит за рамки поиска: при работе с мультимодальными системами искусственного интеллекта однопроходные подходы, которые рассматривают модальности по отдельности, всегда будут сталкиваться с этой стеной несовместимости оценок. Двухэтапные архитектуры, которые извлекают в широком смысле, а затем интеллектуально ранжируют, становятся необходимыми. Попробуйте <code>jina-reranker-m0</code> через наш API или на AWS, GCP и Azure.</p>",
  "comment_id": "682b34d62caa92000178b523",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/05/fair-scoring.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-05-19T15:40:38.000+02:00",
  "updated_at": "2025-05-25T08:26:31.000+02:00",
  "published_at": "2025-05-25T08:25:10.000+02:00",
  "custom_excerpt": "Text similarity: 0.7. Image similarity: 0.5. Which document is more relevant? You literally cannot tell—and that's the core problem breaking multimodal search. We solve it with unified reranking.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "678e14a78f6bb40001a63595",
      "name": "Nan Wang",
      "slug": "nan",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
      "cover_image": null,
      "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
      "website": null,
      "location": "Global",
      "facebook": null,
      "twitter": "@nanwang_t",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "678e14a78f6bb40001a63595",
    "name": "Nan Wang",
    "slug": "nan",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg",
    "cover_image": null,
    "bio": "Co-founder & CTO @JinaAI | Ex-Zalando & Tencent | Build AI models & systems | Open-source enthusiast | Speaker & contributor (40+ talks) | PhD in Computational Neuroscience @ Ruhr University Bochum",
    "website": null,
    "location": "Global",
    "facebook": null,
    "twitter": "@nanwang_t",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/fair-scoring-for-multimodal-documents-with-jina-reranker-m0/",
  "excerpt": "Текстовое сходство: 0.7. Сходство изображений: 0.5. Какой документ более релевантен? Вы буквально не можете сказать — и это основная проблема, ломающая мультимодальный поиск. Мы решаем ее с помощью унифицированного переранжирования (unified reranking).",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
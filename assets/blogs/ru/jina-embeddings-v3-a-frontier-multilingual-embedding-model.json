{
  "slug": "jina-embeddings-v3-a-frontier-multilingual-embedding-model",
  "id": "66ea352ab0c14d00013bc7f1",
  "uuid": "778aadf1-0767-4842-ad7a-1658ce18179a",
  "title": "Jina Embeddings v3: Передовая мультиязычная модель для создания эмбеддингов",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v3?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v3 · Hugging Face</div><div class=\"kg-bookmark-description\">Мы находимся на пути к развитию и демократизации искусственного интеллекта через открытый исходный код и открытую науку.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://huggingface.co/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v3.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Многоязычные эмбеддинги с Task LoRA</div><div class=\"kg-bookmark-description\">Мы представляем jina-embeddings-v3, новую модель текстовых эмбеддингов с 570 миллионами параметров, достигающую передовых результатов в многоязычных данных и задачах поиска с длинным контекстом, поддерживая длину контекста до 8192 токенов. Модель включает набор специфичных для задач адаптеров Low-Rank Adaptation (LoRA) для генерации высококачественных эмбеддингов для поиска по запросам-документам, кластеризации, классификации и сопоставления текстов. Кроме того, в процесс обучения интегрировано Matryoshka Representation Learning, позволяющее гибко усекать размерности эмбеддингов без ущерба для производительности. Оценка на бенчмарке MTEB показывает, что jina-embeddings-v3 превосходит последние проприетарные эмбеддинги от OpenAI и Cohere в английских задачах, достигая превосходных результатов по сравнению с multilingual-e5-large-instruct во всех многоязычных задачах.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Сегодня мы рады представить <code>jina-embeddings-v3</code>, передовую модель текстовых эмбеддингов с 570 миллионами параметров. Она достигает лучших в своем классе результатов на <strong>многоязычных</strong> данных и задачах поиска с <strong>длинным контекстом</strong>, поддерживая входную длину до 8192 токенов. Модель имеет специфичные для задач адаптеры Low-Rank Adaptation (LoRA), позволяющие генерировать высококачественные эмбеддинги для различных задач, включая <strong>поиск по запросам-документам</strong>, <strong>кластеризацию</strong>, <strong>классификацию</strong> и <strong>сопоставление текстов</strong>.</p><p>По результатам оценки на MTEB English, Multilingual и LongEmbed, <code>jina-embeddings-v3</code> превосходит последние проприетарные эмбеддинги от OpenAI и Cohere в английских задачах, а также превосходит <code>multilingual-e5-large-instruct</code> во всех многоязычных задачах. С размерностью выходного вектора по умолчанию 1024, пользователи могут произвольно уменьшать размерности эмбеддингов до 32 без потери производительности благодаря интеграции Matryoshka Representation Learning (MRL).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/MTEB-English-Tasks-Performance.svg\" class=\"kg-image\" alt=\"Chart comparing the performance of various NLP tools on MTEB English Tasks, with scores ranging from 60 to 65.5, displayed on\" loading=\"lazy\" width=\"920\" height=\"240\"><figcaption><span style=\"white-space: pre-wrap;\">Производительность </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> в сравнении с другими моделями эмбеддингов по всем английским задачам MTEB. Полные результаты оценки по каждой задаче можно найти в </span><a href=\"https://arxiv.org/abs/2409.10173?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">нашей статье на arXiv</span></a><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/MTEB-Multilingual-Tasks-Performance.svg\" class=\"kg-image\" alt=\"Graph depicting MTEB Multilingual Tasks Performance, comparing multilingual embeddings and 'jina embeddings' versions with sc\" loading=\"lazy\" width=\"920\" height=\"219\"><figcaption><span style=\"white-space: pre-wrap;\">Производительность </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> была оценена на широком наборе многоязычных и кросс-языковых задач MTEB. Обратите внимание, что </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2-(zh/es/de)</span></code><span style=\"white-space: pre-wrap;\"> относится к нашему набору двуязычных моделей, которые тестировались только на китайских, испанских и немецких моноязычных и кросс-языковых задачах, исключая все другие языки. Кроме того, мы не приводим оценки для </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>openai-text-embedding-3-large</span></code><span style=\"white-space: pre-wrap;\"> и </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>cohere-embed-multilingual-v3.0</span></code><span style=\"white-space: pre-wrap;\">, так как эти модели не оценивались на полном наборе многоязычных и кросс-языковых задач MTEB.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/LongEmbed-MTEB-Long-Document-Retrieval-Tasks-Performance.svg\" class=\"kg-image\" alt=\"Bar graph showing performance of different embeddings on long document retrieval tasks with scores for various libraries.\" loading=\"lazy\" width=\"920\" height=\"219\"><figcaption><span style=\"white-space: pre-wrap;\">Производительность </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> на шести задачах поиска длинных документов из бенчмарка LongEmbed показывает значительное улучшение по сравнению с другими моделями. Оценки представлены в nDCG@10; чем выше, тем лучше. Это свидетельствует об эффективности наших позиционных эмбеддингов на основе RoPE, которые превосходят как фиксированные позиционные эмбеддинги, используемые в </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>baai-bge-m3</span></code><span style=\"white-space: pre-wrap;\">, так и подход на основе ALiBi, используемый в </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">.</span></figcaption></figure><p>На момент релиза 18 сентября 2024 года <code>jina-embeddings-v3</code> является <strong>лучшей</strong> многоязычной моделью и занимает <strong>2-е место</strong> в рейтинге MTEB English среди моделей с менее чем 1 миллиардом параметров. v3 поддерживает в общей сложности 89 языков, включая 30 языков с наилучшей производительностью: арабский, бенгальский, китайский, датский, голландский, английский, финский, французский, грузинский, немецкий, греческий, хинди, индонезийский, итальянский, японский, корейский, латышский, норвежский, польский, португальский, румынский, русский, словацкий, испанский, шведский, тайский, турецкий, украинский, урду и вьетнамский.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/image-2.png\" class=\"kg-image\" alt=\"Leaderboard table comparing language models across various performance metrics with highlighted rankings, set on a dark, prof\" loading=\"lazy\" width=\"2000\" height=\"899\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/09/image-2.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/09/image-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">По состоянию на 18 сентября 2024 года </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\">, имеющая 570 миллионов параметров и 1024 выходных измерения, является самой эффективной, мощной и надежной многоязычной моделью эмбеддингов с менее чем 1 миллиардом параметров.</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg\" class=\"kg-image\" alt=\"Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu\" loading=\"lazy\" width=\"949\" height=\"949\"><figcaption><span style=\"white-space: pre-wrap;\">Закон масштабирования моделей эмбеддингов. Средняя производительность MTEB на английских задачах построена относительно количества параметров модели. Каждая точка представляет модель эмбеддингов. Линия тренда, представляющая все модели, выделена, с многоязычными моделями, подчеркнутыми голубым цветом. Можно видеть, что </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> демонстрирует превосходную производительность по сравнению с моделями аналогичного размера, также показывая сверхлинейное улучшение по сравнению с предшественником, </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v2</span></code><span style=\"white-space: pre-wrap;\">. Этот график был создан путем выбора топ-100 моделей эмбеддингов из рейтинга MTEB, исключая те, для которых отсутствует информация о размере, обычно закрытые или проприетарные модели. Также были отфильтрованы материалы, идентифицированные как очевидный троллинг.</span></figcaption></figure><p>Кроме того, по сравнению с эмбеддингами на основе LLM, которые недавно привлекли внимание, такими как <code>e5-mistral-7b-instruct</code>, имеющими размер параметров 7.1 миллиардов (в 12 раз больше) и выходную размерность 4096 (в 4 раза больше), но предлагающими только 1% улучшение на английских задачах MTEB, <code>jina-embeddings-v3</code> является гораздо более экономичным решением, делая его более подходящим для производства и периферийных вычислений.</p><h2 id=\"model-architecture\">Архитектура модели</h2>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Характеристика</th>\n<th>Описание</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>База</td>\n<td><code>jina-XLM-RoBERTa</code></td>\n</tr>\n<tr>\n<td>Базовые параметры</td>\n<td>559M</td>\n</tr>\n<tr>\n<td>Параметры с LoRA</td>\n<td>572M</td>\n</tr>\n<tr>\n<td>Максимальное количество входных токенов</td>\n<td>8192</td>\n</tr>\n<tr>\n<td>Максимальные выходные размерности</td>\n<td>1024</td>\n</tr>\n<tr>\n<td>Слои</td>\n<td>24</td>\n</tr>\n<tr>\n<td>Словарь</td>\n<td>250K</td>\n</tr>\n<tr>\n<td>Поддерживаемые языки</td>\n<td>89</td>\n</tr>\n<tr>\n<td>Внимание</td>\n<td>FlashAttention2, также работает без него</td>\n</tr>\n<tr>\n<td>Пулинг</td>\n<td>Mean pooling</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Архитектура <code>jina-embeddings-v3</code> показана на рисунке ниже. Для реализации базовой архитектуры мы адаптировали модель <code>XLM-RoBERTa</code> с несколькими ключевыми модификациями: (1) обеспечение эффективного кодирования длинных текстовых последовательностей, (2) возможность кодирования эмбеддингов под конкретные задачи и (3) повышение общей эффективности модели с помощью последних технологий. Мы продолжаем использовать оригинальный токенизатор <code>XLM-RoBERTa</code>. Хотя <code>jina-embeddings-v3</code> со своими 570 миллионами параметров больше, чем <code>jina-embeddings-v2</code> со 137 миллионами, она все еще значительно меньше моделей эмбеддингов, дообученных из LLM.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/Heading--26-.svg\" class=\"kg-image\" alt=\"Flowchart mapping sentiment classification. Begins with \"Downstream Task: sentiment = classify\" and includes stages like \"Mea\" loading=\"lazy\" width=\"1160\" height=\"618\"><figcaption><span style=\"white-space: pre-wrap;\">Архитектура </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v3</span></code><span style=\"white-space: pre-wrap;\"> основана на модели </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-XLM-RoBERTa</span></code><span style=\"white-space: pre-wrap;\"> с пятью LoRA адаптерами для четырех различных задач.</span></figcaption></figure><p>Ключевым нововведением в <code>jina-embeddings-v3</code> является использование LoRA адаптеров. Введены <strong>пять</strong> специфических для задач LoRA адаптеров для оптимизации эмбеддингов под <strong>четыре</strong> задачи. Входные данные модели состоят из двух частей: текста (длинный документ для эмбеддинга) и задачи. <code>jina-embeddings-v3</code> поддерживает четыре задачи и реализует пять адаптеров на выбор: <code>retrieval.query</code> и <code>retrieval.passage</code> для эмбеддингов запросов и пассажей в асимметричных задачах поиска, <code>separation</code> для задач кластеризации, <code>classification</code> для задач классификации и <code>text-matching</code> для задач, связанных с семантическим сходством, таких как STS или симметричный поиск. LoRA адаптеры составляют менее 3% от общего числа параметров, добавляя минимальные накладные расходы на вычисления.</p><p>Для дальнейшего повышения производительности и снижения потребления памяти мы интегрировали FlashAttention 2, поддерживаем контрольные точки активации и используем фреймворк DeepSpeed для эффективного распределенного обучения.</p><h2 id=\"get-started\">Начало работы</h2><h3 id=\"via-jina-ai-search-foundation-api\">Через Jina AI Search Foundation API</h3><p>Самый простой способ использовать <code>jina-embeddings-v3</code> — посетить <a href=\"https://jina.ai/?ref=jina-ai-gmbh.ghost.io#apiform\" rel=\"noreferrer\">домашнюю страницу Jina AI</a> и перейти в раздел Search Foundation API. Начиная с сегодняшнего дня, эта модель установлена по умолчанию для всех новых пользователей. Вы можете исследовать различные параметры и функции непосредственно оттуда.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/image-3.png\" class=\"kg-image\" alt=\"Screenshot of a dark-themed interface with options like 'Join us', 'Explore', showing 'Start instantly - no credit card or re\" loading=\"lazy\" width=\"2000\" height=\"960\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/09/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/09/image-3.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n\t -H \"Content-Type: application/json\" \\\n\t -H \"Authorization: Bearer jina_387ced4ff3f04305ac001d5d6577e184hKPgRPGo4yMp_3NIxVsW6XTZZWNL\" \\\n\t -d '{\n\t\"model\": \"jina-embeddings-v3\",\n\t\"task\": \"text-matching\",\n\t\"dimensions\": 1024,\n\t\"late_chunking\": true,\n\t\"input\": [\n\t\t\"Organic skincare for sensitive skin with aloe vera and chamomile: ...\", \n\t\t\"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille: Erleben Sie die wohltuende Wirkung...\", \n\t\t\"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla: Descubre el poder ...\", \n\t\t\"针对敏感肌专门设计的天然有机护肤产品：体验由芦荟和洋甘菊提取物带来的自然呵护。我们的护肤产品特别为敏感肌设计，...\", \n\t\t\"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています: 今シーズンのメイクアップトレンドは、大胆な色彩と革新的な技術に注目しています。...\"\n    ]}'\n</code></pre><p>По сравнению с v2, v3 вводит три новых параметра в API: <code>task</code>, <code>dimensions</code> и <code>late_chunking</code>. </p><h4 id=\"parameter-task\">Параметр <code>task</code></h4><p>Параметр <code>task</code> является критически важным и должен быть установлен в соответствии с последующей задачей. Получаемые эмбеддинги будут оптимизированы для этой конкретной задачи. Для более подробной информации обратитесь к списку ниже.</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>Значение <code>task</code></strong></th>\n<th><strong>Описание задачи</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>retrieval.passage</code></td>\n<td>Эмбеддинг <b>документов</b> в задаче поиска по запросам</td>\n</tr>\n<tr>\n<td><code>retrieval.query</code></td>\n<td>Эмбеддинг <b>запросов</b> в задаче поиска по запросам</td>\n</tr>\n<tr>\n<td><code>separation</code></td>\n<td>Кластеризация документов, визуализация корпуса</td>\n</tr>\n<tr>\n<td><code>classification</code></td>\n<td>Классификация текста</td>\n</tr>\n<tr>\n<td><code>text-matching</code></td>\n<td><b>(По умолчанию)</b> Семантическое сходство текстов, общий симметричный поиск, рекомендации, поиск похожих элементов, дедупликация</td>\n</tr>\n</tbody>\n</table>\n\n<!--kg-card-end: html-->\n<p>Обратите внимание, что API <em>не</em> генерирует сначала общий мета-эмбеддинг, а затем адаптирует его с помощью дополнительного дообученного MLP. Вместо этого он вставляет специфичный для задачи LoRA адаптер в каждый слой трансформера (всего 24 слоя) и выполняет кодирование за один проход. Дополнительные подробности можно найти в <a href=\"https://arxiv.org/abs/2409.10173?ref=jina-ai-gmbh.ghost.io\">нашей статье на arXiv</a>.</p><h4 id=\"parameter-dimensions\">Параметр <code>dimensions</code></h4><p>Параметр <code>dimensions</code> позволяет пользователям выбрать компромисс между эффективностью использования пространства и производительностью при минимальных затратах. Благодаря технике MRL, используемой в <code>jina-embeddings-v3</code>, вы можете уменьшить размерность эмбеддингов сколько угодно (даже до одного измерения!). Меньшие эмбеддинги более экономны для векторных баз данных, а их влияние на производительность можно оценить по графику ниже.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/Performance-of-Different-Output-Dimensions.svg\" class=\"kg-image\" alt=\"Scatter plot titled &quot;Performance of Different Output Dimensions&quot; showing performance metrics across increasing MRL dimensions\" loading=\"lazy\" width=\"595\" height=\"513\"></figure><h4 id=\"parameter-latechunking\">Параметр <code>late_chunking</code></h4><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking in Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Chunking long documents while preserving contextual information is challenging. We introduce the \"Late Chunking\" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"><span class=\"kg-bookmark-publisher\">GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/08/banner-late-chunking.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Наконец, параметр <code>late_chunking</code> управляет использованием нового метода разбиения на чанки, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">который мы представили в прошлом месяце</a> для кодирования пакета предложений. Когда установлено значение <code>true</code>, наш API объединит все предложения в поле <code>input</code> и подаст их как единую строку в модель. Другими словами, <strong>мы рассматриваем предложения во входных данных так, как будто они изначально происходят из одного раздела, параграфа или документа.</strong> Внутренне модель встраивает эту длинную объединенную строку и затем выполняет позднее разбиение на чанки, возвращая список эмбеддингов, соответствующий размеру входного списка. Каждый эмбеддинг в списке, таким образом, обусловлен предыдущими эмбеддингами.</p><p>С точки зрения пользователя, установка <code>late_chunking</code> <em>не</em> меняет формат входных или выходных данных. Вы заметите только изменение в значениях эмбеддингов, так как теперь они вычисляются на основе всего предыдущего контекста, а не независимо. Важно знать при использовании<code>late_chunking=True</code> означает, что общее количество токенов (суммируя все токены в <code>input</code>) на запрос ограничено 8192, что является максимальной длиной контекста, допустимой для <code>jina-embeddings-v3</code>. Когда <code>late_chunking=False</code>, такого ограничения нет; общее количество токенов подчиняется только <a href=\"https://jina.ai/contact-sales?ref=jina-ai-gmbh.ghost.io#faq\">лимиту запросов API эмбеддингов</a>.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/p1.png\" width=\"1334\" height=\"1640\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/p1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/p1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/09/p1.png 1334w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/09/p2.png\" width=\"1148\" height=\"1644\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/09/p2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/09/p2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/09/p2.png 1148w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Позднее разбиение включено vs выключено: Формат ввода и вывода остается тем же, единственное отличие — в значениях эмбеддингов. Когда </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>late_chunking</span></code><span style=\"white-space: pre-wrap;\"> включен, на эмбеддинги влияет весь предыдущий контекст в </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>input</span></code><span style=\"white-space: pre-wrap;\">, тогда как без него эмбеддинги вычисляются независимо.</span></p></figcaption></figure><h3 id=\"via-azure-aws\">Через Azure и AWS</h3><p><code>jina-embeddings-v3</code> теперь доступен на AWS SageMaker и Azure Marketplace.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">AWS Marketplace: Jina Embeddings v3</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/favicon.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://d32gc0xr2ho6pa.cloudfront.net/img/general/v2/socialPreview.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v3?tab=Overview&ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Microsoft Azure Marketplace</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://azuremarketplace.microsoft.com/favicon.ico\" alt=\"\"></div></div></a></figure><p>Если вам нужно использовать его за пределами этих платформ или локально в вашей компании, обратите внимание, что модель лицензирована под CC BY-NC 4.0. <a href=\"https://jina.ai/contact-sales/?ref=jina-ai-gmbh.ghost.io\" rel=\"noreferrer\">По вопросам коммерческого использования, не стесняйтесь связаться с нами.</a></p><h3 id=\"via-vector-databases-partners\">Через векторные базы данных и партнеров</h3><p>Мы тесно сотрудничаем с провайдерами векторных баз данных, такими как Pinecone, Qdrant и Milvus, а также с фреймворками оркестрации LLM, такими как LlamaIndex, Haystack и Dify. На момент релиза мы рады сообщить, что Pinecone, Qdrant, Milvus и Haystack уже интегрировали поддержку <code>jina-embeddings-v3</code>, включая три новых параметра: <code>task</code>, <code>dimensions</code> и <code>late_chunking</code>. Другие партнеры, уже интегрированные с API <code>v2</code>, также должны поддерживать <code>v3</code>, просто изменив название модели на <code>jina-embeddings-v3</code>. Однако они могут еще не поддерживать новые параметры, представленные в <code>v3</code>.</p><h4 id=\"via-pinecone\">Через Pinecone</h4><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pinecone.io/models/jina-embeddings-v3?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">The vector database to build knowledgeable AI | Pinecone</div><div class=\"kg-bookmark-description\">Search through billions of items for similar matches to any object, in milliseconds. It's the next generation of search, an API call away.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://mintlify.s3-us-west-1.amazonaws.com/pinecone-2/_generated/favicon/apple-touch-icon.png?v=3\" alt=\"\"><span class=\"kg-bookmark-author\">Pinecone Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.pinecone.io/images/docs_og_image.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"via-qdrant\">Через Qdrant</h4><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://qdrant.tech/documentation/embeddings/jina-embeddings/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings - Qdrant</div><div class=\"kg-bookmark-description\">Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon.png\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span><span class=\"kg-bookmark-publisher\">Qdrant</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-social-preview.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"via-milvus\">Через Milvus</h4><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://milvus.io/docs/integrate_with_jina.md?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Integrate Milvus with Jina | Milvus Documentation</div><div class=\"kg-bookmark-description\">This guide demonstrates how to use Jina embeddings and Milvus to conduct similarity search and retrieval tasks. | v2.4.x</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-32x32.png\" alt=\"\"><span class=\"kg-bookmark-author\">milvus-logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/meta_image_milvus_d6510e10e0.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h4 id=\"via-haystack\">Через Haystack</h4><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://haystack.deepset.ai/integrations/jina?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina AI | Haystack</div><div class=\"kg-bookmark-description\">Use the latest Jina AI embedding models</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://haystack.deepset.ai/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Haystack</span><span class=\"kg-bookmark-publisher\">Authors deepset</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://haystack.deepset.ai/images/haystack-ogimage.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Заключение</h2><p>В октябре 2023 года мы выпустили <code>jina-embeddings-v2-base-en</code>, <a href=\"https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai?ref=jina-ai-gmbh.ghost.io\">первую в мире модель эмбеддингов с открытым исходным кодом с контекстной длиной 8K</a>. Это была <em>единственная</em> модель текстовых эмбеддингов, которая поддерживала длинный контекст и соответствовала OpenAI's <code>text-embedding-ada-002</code>. Сегодня, после года обучения, экспериментов и ценных уроков, мы с гордостью представляем <code>jina-embeddings-v3</code> — новый рубеж в моделях текстовых эмбеддингов и важная веха нашей компании.</p><p>С этим релизом мы продолжаем совершенствоваться в том, чем мы известны: <strong>эмбеддинги с длинным контекстом</strong>, одновременно решая самую востребованную задачу как от индустрии, так и от сообщества — <strong>многоязычные эмбеддинги</strong>. В то же время мы поднимаем производительность на новую высоту. С новыми функциями, такими как Task-specific LoRA, MRL и позднее разбиение, мы верим, что <code>jina-embeddings-v3</code> действительно станет фундаментальной моделью эмбеддингов для различных приложений, включая RAG, агентов и многое другое. По сравнению с недавними эмбеддингами на основе LLM, такими как <code>NV-embed-v1/v2</code>, наша модель высокоэффективна по параметрам, что делает её гораздо более подходящей для продакшена и периферийных устройств.</p><p>В дальнейшем мы планируем сосредоточиться на оценке и улучшении производительности <code>jina-embeddings-v3</code> для низкоресурсных языков и дальнейшем анализе систематических ошибок, вызванных ограниченной доступностью данных. Более того, веса модели <code>jina-embeddings-v3</code>, вместе с её инновационными функциями и горячими обновлениями, послужат основой для наших будущих моделей, включая <code>jina-clip-v2</code>,<code>jina-reranker-v3</code> и <code>reader-lm-v2</code>.</p>",
  "comment_id": "66ea352ab0c14d00013bc7f1",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/09/v3banner.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2024-09-18T04:04:26.000+02:00",
  "updated_at": "2024-10-11T13:58:13.000+02:00",
  "published_at": "2024-09-18T10:37:31.000+02:00",
  "custom_excerpt": "jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v3-a-frontier-multilingual-embedding-model/",
  "excerpt": "jina-embeddings-v3 - это передовая многоязычная модель текстовых эмбеддингов с 570M параметров и длиной токена 8192, превосходящая по показателям последние проприетарные эмбеддинги от OpenAI и Cohere на бенчмарке MTEB.",
  "reading_time": 10,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Dynamic image showing the characters \"V3\" formed by bright green dots varying in size on a black background.",
  "feature_image_caption": null
}
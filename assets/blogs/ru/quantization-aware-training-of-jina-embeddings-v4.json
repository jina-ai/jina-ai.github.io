{
  "slug": "quantization-aware-training-of-jina-embeddings-v4",
  "id": "685d4b76f1bef30001fc5449",
  "uuid": "6b06b483-2d13-4f1d-8d9d-147fa6dffe4b",
  "title": "jina-embeddings-v4 的量化感知训练",
  "html": "<p>Квантование широко используется для решения проблем масштабирования в ИИ. Название может показаться сложным, но это всего лишь округление чисел, чтобы они занимали меньше места. Это означает, что векторы эмбеддингов (Embeddings) становятся меньше, занимают меньше памяти и места для хранения, а также ускоряется поиск информации, поскольку требуется меньше времени для сравнения векторов. Квантование — это чисто числовой метод, которому все равно, какие данные обрабатывает ваша модель или какие у вас варианты использования, поэтому он может принести улучшения, не требуя больших затрат на знания предметной области.</p><p>Можно было бы ожидать, что квантование включает в себя старые добрые компромиссы и ничто не дается даром — где мы должны пожертвовать некоторой точностью. В этой статье мы покажем вам способ <strong>сделать его без потерь</strong> с помощью <em>квантования с учетом обучения</em> (quantization-aware training, QAT). Этот метод используется в <code>jina-embeddings-v4</code> для предоставления меньших по размеру эмбеддингов, которые требуются в приложениях, критичных к пространству.</p><h2 id=\"overview-of-quantization-techniques\">Обзор методов квантования</h2><p>Квантование модели обычно означает одно из четырех:</p><ul><li>Пост-тренировочное квантование (Post-training quantization, <strong>PTQ</strong>)</li><li>Обучение для квантованных выходов эмбеддингов (Output QAT)</li><li>Обучение для полностью квантованных моделей (Full QAT)</li><li>Дистилляция новой квантованной модели из существующей неквантованной</li></ul><p>Пост-тренировочное квантование (PTQ) принимает обученную модель эмбеддингов как есть и никак ее не изменяет. Это просто вопрос отбрасывания наименее значащих цифр значений с плавающей запятой, производимых моделью. Мы просто округляем числа, а иногда масштабируем их до диапазона.</p><p><strong>Output QAT</strong> означает тонкую настройку модели эмбеддингов для получения оптимальных векторов с уменьшенной точностью. Это означает изменение модели, но не меняет точность весов модели, и, следовательно, не уменьшает ее размер. Уменьшается только размер выходного вектора.</p><p><strong>Full QAT</strong> начинается с полностью обученной модели с полной точностью и снижает точность весов модели, а затем тонко настраивает производительность этой измененной модели. Это приводит к созданию значительно меньшей модели, а также меньших по размеру эмбеддингов, ценой некоторой тонкой настройки.</p><p><strong>Дистилляция</strong> — это процесс обучения новой модели для соответствия производительности существующей. Это означает создание новой модели, которая разработана с нуля как квантованная, а затем использование существующей модели для генерации столько обучающих данных, сколько необходимо для обучения, пока она не будет работать как можно ближе к существующей модели.</p><p>Преимущества этих четырех подходов суммированы в таблице ниже:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Подход</th>\n<th>Более компактные эмбеддинги?</th>\n<th>Требуется обучение?</th>\n<th>Сжатие модели?</th>\n<th>Более быстрый вывод?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>PTQ</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Output QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td>❌</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Full QAT</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td><strong>Дистилляция</strong></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><em>(к меньшей модели)</em></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Все четыре подхода создают более компактные эмбеддинги, но, кроме PTQ, все требуют некоторого дополнительного обучения, в то время как только Full QAT и дистилляция создают новые, более быстрые модели. Full QAT и дистилляция намного дороже в реализации, поскольку требуют гораздо больше обучения, чем Output QAT.</p><p>В этой статье мы рассмотрим только PTQ и Output QAT, которые не меняют размер или скорость модели эмбеддингов.</p><h2 id=\"experimental-setup\">Экспериментальная установка</h2><p>Для этих экспериментов нашей базовой моделью является <code>jina-embeddings-v4</code> с адаптером извлечения, который производит векторы с плавающей запятой 32-битной точности (FP32) в 2048 измерениях. Каждый эмбеддинг, следовательно, имеет размер 8196 байт или 8 кБ.</p><p>Мы изучили несколько экспериментальных условий, используя эталонные задачи извлечения запросов-документов из набора <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR benchmark</a>. Процесс извлечения использует косинусное сходство между векторами для поиска и ранжирования документов, которые лучше всего соответствуют запросам.</p><ul><li><strong>Базовая линия</strong> — Производительность векторов эмбеддингов <code>jina-embeddings-v4</code> без какого-либо квантования. Во всех этих экспериментах использовалась бета-версия модели, и производительность выпуска несколько лучше.</li><li><strong>PTQ</strong> — Мы квантовали выходные векторы в бинарные векторы, не меняя модель.</li><li><strong>Output QAT</strong> — Мы квантовали выходные векторы и применили тонкую настройку к адаптеру извлечения, чтобы улучшить его производительность в квантованных условиях.</li></ul><h3 id=\"quantization-levels\">Уровни квантования</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"816\" height=\"636\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png 816w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 1: Сравнение размеров эмбеддингов после квантования.</span></figcaption></figure><p>Мы экспериментировали с четырьмя различными уровнями квантования.</p><ul><li><strong>8-битные целые числа</strong> — Значения FP32 сводятся к целым числам в диапазоне от -128 до 127, уменьшая эмбеддинги в 4 раза до <strong>2048 байт</strong>.</li><li><strong>4-битные целые числа</strong> — То же, что и для 4-битных целых чисел, но мы отображаем в диапазон от -8 до 7, уменьшая размеры векторов в 8 раз, до <strong>1024 байт</strong>.</li><li><strong>Троичное квантование</strong> — Все значения отображаются в одно из трех значений: -1, 0, 1. Оптимально хранятся, это уменьшает каждое измерение до 1,6 бита, уменьшая размер векторов эмбеддингов примерно в 40 раз до примерно <strong>230 байт</strong>.</li><li><strong>Бинарное квантование</strong> — Мы преобразуем скалярные значения FP32 в один бит, используя тип данных <code>torch.sign</code>, который предоставляет только два значения, занимающие один бит для хранения. Это уменьшает 2048-мерные векторы эмбеддингов с 8192 байт до <strong>128 байт</strong>, что в 64 раза меньше.</li></ul><h3 id=\"scaling\">Масштабирование</h3><p>Для бинарного квантования квантование очень простое: Если значение вектора выше 0 или положительное, оно отображается в 1. В противном случае оно отображается в -1.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1159\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png 1159w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 2: Бинарное квантование. Все отрицательные значения становятся -1, все остальные 1.</span></figcaption></figure><p>Для других сценариев квантования мы нормализовали значения до диапазона, а затем округлили до ближайшего значения, разрешенного уровнем квантования. Векторы эмбеддингов состоят из масштабируемых чисел между -∞ и +∞ (или, на практике, действительно больших положительных и отрицательных чисел). Мы используем два числа, $max$ и $min$, для масштабирования значений для квантования.</p><p>Для троичного квантования мы берем каждый компонент вектора $v$ и преобразуем его следующим образом:</p><ul><li>если $v$ ≥ $max$, $v$ становится 1.</li><li>если $v$ ≤ $min$, $v$ становится -1.</li><li>если $min$ &lt; $v$ &lt; $max$, $v$ становится 0.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"220\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-2.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png 1030w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 3: Троичное квантование. Определяется интервал, и значения внутри него становятся 0. Все более низкие значения становятся -1, а все более высокие - 1.</span></figcaption></figure><p>Для 4-битных целых чисел:</p><ul><li>если $v$ ≥ $max$, $v$ становится 7.</li><li>если $v$ ≤ $min$, $v$ становится -8.</li><li>если $min$ &lt; $v$ &lt; $max$, $v$ становится $16*(v - min)/(max - min) - 8$, затем округляется до ближайшего целого числа. Это масштабирует значение до диапазона $[-8,7]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"221\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 4: 4-битное квантование. Определяется интервал, и все значения нормализуются до определенного диапазона [-8,7].</span></figcaption></figure><p></p><p>Для 8-битных целых чисел:</p><ul><li>если $v$ ≥ $max$, $v$ становится 127.</li><li>если $v$ ≤ $min$, $v$ становится -128.</li><li>если $min$ &lt; $v$ &lt; $max$, $v$ становится $256*(v - min)/(max - min) - 128$, округленным до ближайшего целого числа. Это масштабирует значение до диапазона $[-128,127]$.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1023\" height=\"219\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/image-4.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/image-4.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png 1023w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 5: 8-битное квантование. Определяется интервал, и все значения нормализуются до определенного диапазона [-128,127].</span></figcaption></figure><p>Для вычисления $max$ и $min$ мы использовали два подхода:</p><ul><li><strong>Min/Max</strong> — Мы обрабатывали наши данные пакетами, и для каждого пакета мы определяли самый высокий и самый низкий компонент вектора, устанавливая $max$ на самый высокий, а $min$ на самый низкий.</li><li><strong>Скользящее усреднение по пакетам</strong> — Для каждого пакета мы вычисляли среднее и стандартное отклонение компонентов вектора. Мы поддерживали скользящее среднее как среднего, так и стандартного отклонения по мере обработки всех пакетов. Если $avg$ является текущим скользящим средним значений среднего пакета, а $std$ является текущим скользящим средним стандартных отклонений, то для каждого пакета:</li></ul><p>$max = avg + std$<br>$min = avg - std$</p><h3 id=\"qat-fine-tuning\">Тонкая настройка QAT</h3><p>Для экспериментов PTQ мы использовали модель как есть и квантовали эмбеддинги, которые она производила, используя методы, описанные выше.</p><p>Для Output QAT мы тонко настроили модель, используя <em>прямую оценку</em>. Это означает, что мы обращаем процесс квантования, восстанавливая полную точность значений, прежде чем вычислять потери (т. е. ошибку), а затем используем эту метрику потерь для тонкой настройки модели.</p><p>В каждом случае мы выполняли дообучение в течение 10 000 шагов, сохраняя контрольную точку каждые 500 шагов. Затем мы сохранили контрольную точку с наивысшей оценкой в бенчмарке <a href=\"https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6\">NanoBEIR</a>.</p><h3 id=\"asymmetric-quantization\">Асимметричная квантизация</h3><p>Посттреничная квантизация (PTQ) и Квантизация с учетом обучения (Output QAT) уменьшают размер векторов 向量模型 (Embeddings), но не уменьшают размер модели или скорость вывода; вся экономия заключается в размере хранимых 向量模型 (Embeddings) документов и скорости поиска.</p><p>В результате мы протестировали как квантование векторов запросов, так и оставление их неквантованными во время поиска, поскольку это не меняет размер хранимых векторов 向量模型 (Embeddings) в любом случае.</p><h2 id=\"results\">Результаты</h2><p>Всего мы протестировали девять условий, которые суммированы в таблицах ниже:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Название условия</th>\n<th>Дообучение</th>\n<th>Уровень квантования</th>\n<th>Стратегия масштабирования</th>\n<th>Квантованные запросы</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Базовый уровень</td>\n<td>❌</td>\n<td>н/д</td>\n<td>н/д</td>\n<td>н/д</td>\n</tr>\n<tr>\n<td>PTQ для обоих</td>\n<td>❌</td>\n<td>Двоичный</td>\n<td>н/д</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>PTQ только для документов</td>\n<td>❌</td>\n<td>Двоичный</td>\n<td>н/д</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Двоичный</td>\n<td><strong>✓</strong></td>\n<td>Двоичный</td>\n<td>н/д</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT Двоичный только для документов</td>\n<td><strong>✓</strong></td>\n<td>Двоичный</td>\n<td>н/д</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>QAT Троичный</td>\n<td><strong>✓</strong></td>\n<td>Троичный</td>\n<td>Скользящее среднее</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 4-бита</td>\n<td><strong>✓</strong></td>\n<td>4-бита</td>\n<td>Скользящее среднее</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-бит</td>\n<td><strong>✓</strong></td>\n<td>8-бит</td>\n<td>Скользящее среднее</td>\n<td><strong>✓</strong></td>\n</tr>\n<tr>\n<td>QAT 8-бит Мин/Макс</td>\n<td><strong>✓</strong></td>\n<td>8-бит</td>\n<td>Мин/Макс</td>\n<td><strong>✓</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Таблица 2: Экспериментальные условия</em></p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Название условия</th>\n<th>Средняя оценка</th>\n<th>Отличие от базового уровня</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Базовый уровень</td>\n<td>60.10</td>\n<td>н/д</td>\n</tr>\n<tr>\n<td>PTQ Двоичный</td>\n<td>58.33</td>\n<td>-1.78</td>\n</tr>\n<tr>\n<td>PTQ Двоичный только для документов</td>\n<td>59.08</td>\n<td>-1.02</td>\n</tr>\n<tr>\n<td>QAT Двоичный</td>\n<td>59.22</td>\n<td>-0.89</td>\n</tr>\n<tr>\n<td>QAT Двоичный только для документов</td>\n<td>60.81</td>\n<td>+0.70</td>\n</tr>\n<tr>\n<td>QAT Троичный</td>\n<td>59.49</td>\n<td>-0.62</td>\n</tr>\n<tr>\n<td>QAT 4-бита</td>\n<td>61.73</td>\n<td>+1.62</td>\n</tr>\n<tr>\n<td>QAT 8-бит</td>\n<td>61.67</td>\n<td>+1.56</td>\n</tr>\n<tr>\n<td>QAT 8-бит Мин/Макс</td>\n<td>61.29</td>\n<td>+1.19</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p><em>Таблица 3: Средняя оценка (в % правильных ответов) для каждого условия по двенадцати бенчмаркам NanoBEIR.</em></p><p>Из приведенной выше таблицы видно, что дообучение для квантования улучшает результаты. Единственное различие между условиями <strong>PTQ Двоичный</strong> и <strong>QAT Двоичный</strong> заключается в дообучении, и разница в оценке значительна. Аналогично, мы видим почти 2% улучшение в оценках между условиями <strong>PTQ Двоичный только для документов</strong> и <strong>QAT Двоичный только для документов</strong>, которые отличаются только тем же дообучением.</p><p>Неудивительно, что мы также видим, что оценки, как правило, улучшаются, чем меньше мы квантуем, при этом 4-битное квантование показывает результаты лучше, чем троичное, а троичное лучше, чем двоичное. Однако дальнейший переход к 8-битам, похоже, ничего не улучшил.</p><p>Мы протестировали оставление запросов неквантованными только в двоичных случаях, но, похоже, это улучшает производительность.</p><p>Наконец, наши тесты показывают, что метод масштабирования скользящего среднего превосходит упрощенный подход мин/макс.</p><h2 id=\"conclusion\">Вывод</h2><p>Квантование имеет некоторые важные операционные преимущества для моделей 向量模型 (Embeddings), значительно уменьшая размер векторов 向量模型 (Embeddings) и ускоряя поиск информации. В то время как простая посттреничная квантизация (PTQ) обеспечивает немедленные преимущества с точки зрения памяти и хранения, наши эксперименты показывают, что квантизация с учетом обучения (QAT) значительно снижает неизбежные потери точности. Дообучение неизменно давало лучшие результаты.</p><p>Степень квантования напрямую влияет на производительность, чего и следовало ожидать от метода, основанного на снижении точности значений. Менее агрессивное квантование (например, 4-битное) обычно превосходит более агрессивные методы (например, двоичное), но, что удивительно, не было существенной разницы в производительности между 8-битным и 4-битным квантованием. Казалось бы, пока вы не достигнете определенного порога неточности, разница между большим и меньшим квантованием невелика.</p><p>Стратегии масштабирования также важны, при этом метод скользящего среднего показывает превосходные результаты по сравнению с фиксированным подходом мин/макс. Использование значений масштабирования, которые относятся к данным, по-видимому, работает значительно лучше и заслуживает дальнейшего изучения.</p><p>Квантование может помочь вам получить больше от ваших моделей 向量模型 (Embeddings) за меньшие деньги. Хотя в этой статье не рассматриваются все варианты квантования, в ней рассматриваются два, которые легко доступны, и они имеют реальные преимущества. Мы работаем над уточнением и улучшением стратегий квантования, чтобы мы могли еще больше снизить затраты пользователей, и ожидаем выпустить двоичную поддержку для <code>jina-embeddings-v4</code> в ближайшем будущем.</p>",
  "comment_id": "685d4b76f1bef30001fc5449",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading---2025-06-30T114820.483.webp",
  "featured": false,
  "visibility": "public",
  "created_at": "2025-06-26T15:30:30.000+02:00",
  "updated_at": "2025-06-30T21:14:36.000+02:00",
  "published_at": "2025-06-30T21:14:36.000+02:00",
  "custom_excerpt": "Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "64ae64a4733bc60001949ca4",
      "name": "Andrei Ungureanu",
      "slug": "andrei",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg",
      "cover_image": null,
      "bio": "Software / AI Engineer, with a passion for content creation.",
      "website": null,
      "location": "Beijing, China",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/andrei/"
    },
    {
      "id": "632ae7353e4e55003d52598e",
      "name": "Scott Martens",
      "slug": "scott",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/shanshui-ernie-crop.png",
      "bio": "A rogue AI created by Canada's Weapon X program.\n\nContent Creator @ Jina AI",
      "website": "https://jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/quantization-aware-training-of-jina-embeddings-v4/",
  "excerpt": "Квантование позволяет получить векторные представления (Embeddings) меньшего размера. Мы покажем вам, что точно настроенное квантование позволяет получить даже векторные представления (Embeddings) без потерь.",
  "reading_time": 8,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
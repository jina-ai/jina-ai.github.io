{
  "slug": "still-need-chunking-when-long-context-models-can-do-it-all",
  "id": "674f1a8eb3efb50001df0e4e",
  "uuid": "90e77f7a-0333-4c87-8d37-facd7415acc0",
  "title": "Нужно ли всё ещё разбивать текст на части, когда модели с длинным контекстом могут обрабатывать его целиком?",
  "html": "<p>В октябре 2023 года мы представили <code>jina-embeddings-v2</code>, первое семейство моделей встраивания с открытым исходным кодом, способное обрабатывать входные данные до 8 192 токенов. Развивая этот успех, в этом году мы запустили <code>jina-embeddings-v3</code>, предлагающую такую же обширную поддержку входных данных с дополнительными улучшениями.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-14.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>В этой статье мы углубимся в встраивания с длинным контекстом и ответим на несколько вопросов: Когда практично объединять такой большой объем текста в единый вектор? Улучшает ли сегментация поиск, и если да, то как? Как мы можем сохранить контекст из разных частей документа при сегментации текста?</p><p>Чтобы ответить на эти вопросы, мы сравним несколько методов генерации встраиваний:</p><ul><li>Встраивание длинного контекста (кодирование до 8 192 токенов в документе) против короткого контекста (т.е. усечение до 192 токенов).</li><li>Без разбиения на части vs. наивное разбиение vs. <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\">позднее разбиение</a>.</li><li>Различные размеры частей при наивном и позднем разбиении.</li></ul><h2 id=\"is-long-context-even-useful\">Действительно ли полезен длинный контекст?</h2><p>Благодаря возможности кодировать до десяти страниц текста в одном встраивании, модели встраивания с длинным контекстом открывают возможности для крупномасштабного представления текста. Но действительно ли это полезно? По мнению многих людей... нет.</p><figure class=\"kg-card kg-gallery-card kg-width-wide kg-card-hascaption\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--15-.png\" width=\"559\" height=\"88\" loading=\"lazy\" alt=\"\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png\" width=\"610\" height=\"117\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--16-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--16-.png 610w\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png\" width=\"1430\" height=\"140\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--14-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--14-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--14-.png 1430w\" sizes=\"(min-width: 720px) 720px\"></div><div class=\"kg-gallery-image\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png\" width=\"1506\" height=\"136\" loading=\"lazy\" alt=\"\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--13-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--13-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--13-.png 1506w\" sizes=\"(min-width: 720px) 720px\"></div></div></div><figcaption><p><span style=\"white-space: pre-wrap;\">Источники: </span><a href=\"https://www.youtube.com/watch?v=xKR08kDY2q4&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">Цитата Нильса Реймера из подкаста How AI Is Built</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://x.com/brainlag/status/1717221138483331158?ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">твит brainlag</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38026784&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">комментарий egorfine на Hacker News</span></a><span style=\"white-space: pre-wrap;\">, </span><a href=\"https://news.ycombinator.com/item?id=38020753&ref=jina-ai-gmbh.ghost.io\"><span style=\"white-space: pre-wrap;\">комментарий andy99 на Hacker News</span></a></p></figcaption></figure><p>Мы рассмотрим все эти опасения с помощью детального исследования возможностей длинного контекста, когда длинный контекст полезен, и когда его следует (и не следует) использовать. Но сначала давайте выслушаем скептиков и рассмотрим некоторые проблемы, с которыми сталкиваются модели встраивания с длинным контекстом.</p><h2 id=\"problems-with-long-context-embeddings\">Проблемы со встраиваниями длинного контекста</h2><p>Представьте, что мы создаем систему поиска документов для статей, например, как на нашем <a href=\"https://jina.ai/news?ref=jina-ai-gmbh.ghost.io\">блоге Jina AI</a>. Иногда одна статья может охватывать несколько тем, как <a href=\"https://jina.ai/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc?ref=jina-ai-gmbh.ghost.io\">отчет о нашем посещении конференции ICML 2024</a>, который содержит:</p><ul><li>Введение, содержащее общую информацию об ICML (количество участников, место проведения, охват и т.д.).</li><li>Презентацию нашей работы (<code>jina-clip-v1</code>).</li><li>Обзоры других интересных исследовательских работ, представленных на ICML.</li></ul><p>Если мы создаем только одно встраивание для этой статьи, это встраивание представляет собой смесь трех разных тем:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"778\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 1: При встраивании документа, охватывающего несколько тем, результирующий вектор представляет собой смесь всех параграфов, потенциально теряя отдельную, специфическую информацию, содержащуюся в каждом отдельном параграфе.</span></figcaption></figure><p>Это приводит к нескольким проблемам:</p><ul><li><strong>Размывание представления:</strong> Хотя все темы в данном тексте <em>могут</em> быть связаны, только одна может быть релевантна поисковому запросу пользователя. Однако одно встраивание (в данном случае, всего поста в блоге) – это всего лишь одна точка в векторном пространстве. По мере добавления текста во входные данные модели, встраивание смещается, чтобы охватить общую тему статьи, становясь менее эффективным в представлении содержания конкретных параграфов.</li><li><strong>Ограниченная емкость:</strong> Модели встраивания производят векторы фиксированного размера, независимо от длины входных данных. По мере добавления контента на вход, модели становится сложнее представить всю эту информацию в векторе. Представьте это как уменьшение изображения до 16×16 пикселей — если вы уменьшаете изображение чего-то простого, например яблока, вы все еще можете извлечь смысл из уменьшенного изображения. Уменьшение карты улиц Берлина? Не совсем то же самое.</li><li><strong>Потеря информации:</strong> В некоторых случаях даже модели встраивания с длинным контекстом достигают своих пределов; Многие модели поддерживают кодирование текста до 8 192 токенов. Более длинные документы необходимо обрезать перед встраиванием, что приводит к потере информации. Если информация, релевантная для пользователя, находится в конце документа, она вообще не будет захвачена встраиванием.</li><li><strong>Возможно, вам <em>нужна</em> сегментация текста:</strong> Некоторым приложениям требуются встраивания для определенных сегментов текста, а не для всего документа, например, для определения релевантного отрывка в тексте.</li></ul><h2 id=\"long-context-vs-truncation\">Длинный контекст vs. усечение</h2><p>Чтобы понять, действительно ли длинный контекст полезен, давайте рассмотрим производительность двух сценариев поиска:</p><ul><li>Кодирование документов до 8 192 токенов (около 10 страниц текста).</li><li>Усечение документов до 192 токенов и кодирование до этого предела.</li></ul><p>Мы сравним результаты, используя<code>jina-embeddings-v3</code> с метрикой поиска nDCG@10. Мы протестировали следующие наборы данных:</p>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Описание</th>\n<th>Пример запроса</th>\n<th>Пример документа</th>\n<th>Средняя длина документа (символов)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/?ref=jina-ai-gmbh.ghost.io\"><strong>NFCorpus</strong></a></td>\n<td>Полнотекстовый медицинский набор данных для поиска с 3244 запросами и документами в основном из PubMed.</td>\n<td>\"Using Diet to Treat Asthma and Eczema\"</td>\n<td>\"Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that [...]\"</td>\n<td>326753</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Yale-LILY/QMSum?ref=jina-ai-gmbh.ghost.io\"><strong>QMSum</strong></a></td>\n<td>Набор данных для суммаризации встреч на основе запросов, требующий обобщения релевантных сегментов встреч.</td>\n<td>\"The professor was the one to raise the issue and suggested that a knowledge engineering trick [...]\"</td>\n<td>\"Project Manager: Is that alright now ? {vocalsound} Okay . Sorry ? Okay , everybody all set to start the meeting ? [...]\"</td>\n<td>37445</td>\n</tr>\n<tr>\n<td><a href=\"https://paperswithcode.com/dataset/narrativeqa?ref=jina-ai-gmbh.ghost.io\"><strong>NarrativeQA</strong></a></td>\n<td>Набор данных вопросов и ответов, содержащий длинные истории и соответствующие вопросы о конкретном содержании.</td>\n<td>\"What kind of business Sophia owned in Paris?\"</td>\n<td>\"ï»¿The Project Gutenberg EBook of The Old Wives' Tale, by Arnold Bennett\\n\\nThis eBook is for the use of anyone anywhere [...]\"</td>\n<td>53336</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Alab-NII/2wikimultihop?ref=jina-ai-gmbh.ghost.io\"><strong>2WikiMultihopQA</strong></a></td>\n<td>Набор данных для многошагового поиска ответов с до 5 шагами рассуждений, разработанный с шаблонами для избежания коротких путей.</td>\n<td>\"What is the award that the composer of song The Seeker (The Who Song) earned?\"</td>\n<td>\"Passage 1:\\nMargaret, Countess of Brienne\\nMarguerite d'Enghien (born 1365 - d. after 1394), was the ruling suo jure [...]\"</td>\n<td>30854</td>\n</tr>\n<tr>\n<td><a href=\"https://arxiv.org/abs/2104.07091?ref=jina-ai-gmbh.ghost.io\"><strong>SummScreenFD</strong></a></td>\n<td>Набор данных для суммаризации сценариев с транскриптами телесериалов и обзорами, требующими интеграции рассредоточенного сюжета.</td>\n<td>\"Penny gets a new chair, which Sheldon enjoys until he finds out that she picked it up from [...]\"</td>\n<td>\"[EXT. LAS VEGAS CITY (STOCK) - NIGHT]\\n[EXT. ABERNATHY RESIDENCE - DRIVEWAY -- NIGHT]\\n(The lamp post light over the [...]\"</td>\n<td>1613</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<p>Как мы видим, кодирование более 192 токенов может дать заметные улучшения производительности:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-1.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-1.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-1.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 2: Сравнение производительности встраивания длинного контекста и встраивания короткого текста</span></figcaption></figure><p>Однако на некоторых наборах данных мы видим большие улучшения, чем на других:</p><ul><li>Для <strong>NFCorpus</strong> усечение едва имеет значение. Это потому, что заголовки и аннотации находятся прямо в начале документов, и они весьма релевантны типичным поисковым запросам пользователей. Независимо от того, усечен текст или нет, наиболее важные данные остаются в пределах лимита токенов.</li><li><strong>QMSum</strong> и <strong>NarrativeQA</strong> считаются задачами \"понимания прочитанного\", где пользователи обычно ищут конкретные факты в тексте. Эти факты часто встречаются в деталях, разбросанных по всему документу, и могут выходить за пределы усеченного лимита в 192 токена. Например, в документе NarrativeQA <em>Percival Keene</em>, ответ на вопрос \"Кто тот хулиган, который крадет обед Персиваля?\" находится далеко за этим лимитом. Аналогично, в <strong>2WikiMultiHopQA</strong> релевантная информация распределена по всему документу, требуя от моделей навигации и синтеза знаний из нескольких разделов для эффективного ответа на запросы.</li><li><strong>SummScreenFD</strong> - это задача, направленная на определение сценария, соответствующего данному резюме. Поскольку резюме включает информацию, распределенную по всему сценарию, кодирование большего объема текста улучшает точность сопоставления резюме с правильным сценарием.</li></ul><h2 id=\"segmenting-text-for-better-retrieval-performance\">Сегментация текста для улучшения производительности поиска</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Далее мы обсудим три похожих концепции. Чтобы избежать путаницы, мы будем называть их следующим образом:<br>• <b><strong style=\"white-space: pre-wrap;\">Сегментация</strong></b>: Обнаружение границ в входном тексте, например, предложений или фиксированного количества токенов.<br>• <b><strong style=\"white-space: pre-wrap;\">Наивное разбиение</strong></b>: Разделение текста на фрагменты на основе сегментационных меток перед его кодированием.<br>• <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap;\">Позднее разбиение</strong></b></a>: Сначала кодирование документа, а затем его сегментация (с сохранением контекста между фрагментами).</div></div><p>Вместо встраивания всего документа в один вектор, мы можем использовать различные методы для первоначальной сегментации документа путем назначения граничных меток:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/chunking-animation.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"492\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/chunking-animation.gif 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/chunking-animation.gif 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/chunking-animation.gif 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/chunking-animation.gif 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 3: Применение методов разбиения \"Фиксированного размера\", \"На основе предложений\" и \"Семантического\" к текстовому отрывку</span></figcaption></figure><p>Некоторые распространенные методы включают:</p><ul><li><strong>Сегментация по фиксированному размеру:</strong> Документ делится на сегменты фиксированного количества токенов, определяемого токенизатором модели встраивания. Это обеспечивает соответствие токенизации сегментов токенизации всего документа (сегментация по определенному количеству символов могла бы привести к другой токенизации).</li><li><strong>Сегментация по предложениям:</strong> Документ сегментируется на предложения, и каждый фрагмент состоит из <em>n</em> количества предложений.</li><li><strong>Сегментация по семантике:</strong> Каждый сегмент соответствует нескольким предложениям, и модель встраивания определяет сходство последовательных предложений. Предложения с высоким сходством встраиваний назначаются в один фрагмент.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Вы можете легко выполнить сегментацию с помощью <a href=\"https://jina.ai/segmenter/?ref=jina-ai-gmbh.ghost.io\">Jina Segmenter</a>, нашего бесплатного API для сегментации длинного текста на фрагменты и токенизации на основе структуры документа.</div></div><p>Для простоты в этой статье мы используем сегментацию фиксированного размера.</p><h3 id=\"document-retrieval-using-naive-chunking\">Поиск документов с использованием наивного разбиения</h3><p>После выполнения сегментации фиксированного размера мы можем наивно разбить документ в соответствии с этими сегментами:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"540\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/Sharing-Chunking-Blog-Post-Images--1---1-.png 960w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 4: Наивное разбиение на основе граничных меток, обнаруженных при сегментации.</span></figcaption></figure><p>Используя <code>jina-embeddings-v3</code>, мы кодируем каждый фрагмент в вектор встраивания, который точно отражает его семантику, затем сохраняем эти встраивания в векторной базе данных.</p><p>Во время выполнения модель кодирует запрос пользователя в вектор запроса. Мы сравниваем его с нашей векторной базой данных встраиваний фрагментов, чтобы найти фрагмент с наибольшим косинусным сходством, а затем возвращаем соответствующий документ пользователю:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--17-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"847\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--17-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--17-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--17-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/12/image--17-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Рисунок 5: Поиск документов, реализованный с помощью наивного разбиения: (1) Документы в коллекции разбиваются на фрагменты на основе границ, (2) модель эмбеддингов кодирует все фрагменты, и мы сохраняем полученные эмбеддинги в базе данных, (3) когда поступает запрос, модель эмбеддингов кодирует его, и база данных определяет наиболее похожий фрагмент. В конце мы определяем соответствующий документ по ID документа, сохраненному для фрагмента в базе данных, и возвращаем его пользователю.</span></figcaption></figure><h3 id=\"problems-with-naive-chunking\">Проблемы наивного разбиения</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"456\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/12/image--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--18-.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Рисунок 6: При разбиении текста на предложения невозможно разрешить ссылки на предыдущие части текста.</span></figcaption></figure><p>Хотя наивное разбиение решает некоторые ограничения моделей эмбеддингов с длинным контекстом, у него есть свои недостатки:</p><ul><li><strong>Потеря общей картины:</strong> Когда дело доходит до поиска документов, множественные эмбеддинги небольших фрагментов могут не уловить общую тему документа. Это похоже на ситуацию, когда за деревьями не видно леса.</li><li><strong>Проблема отсутствия контекста:</strong> Фрагменты нельзя точно интерпретировать, так как отсутствует контекстная информация, как показано на Рисунке 6.</li><li><strong>Эффективность:</strong> Большее количество фрагментов требует больше места для хранения и увеличивает время поиска.</li></ul><h2 id=\"late-chunking-solves-the-context-problem\">Позднее разбиение решает проблему контекста</h2><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">Чтобы решить проблему отсутствия контекста, мы представили новый метод под названием \"позднее разбиение\", описанный в наших предыдущих блог-постах: <a href=\"https://jina.ai/news/late-chunking-in-long-context-embedding-models/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">часть I</strong></b></a>, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii/?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">часть II</strong></b></a>, <a href=\"https://jina.ai/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">часть III</strong></b></a>, <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><b><strong style=\"white-space: pre-wrap\">научная статья</strong></b></a>.</div></div><p>Позднее разбиение работает в два основных этапа:</p><ol><li>Сначала оно использует возможности модели по работе с длинным контекстом для кодирования всего документа в токенные эмбеддинги. Это сохраняет полный контекст документа.</li><li>Затем оно создает эмбеддинги фрагментов путем применения усреднения к определенным последовательностям токенных эмбеддингов, соответствующих границам, определенным при сегментации.</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"865\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--19-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--19-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--19-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Рисунок 7: Позднее vs наивное разбиение.</span></figcaption></figure><p>Ключевое преимущество этого подхода в том, что токенные эмбеддинги контекстуализированы - то есть они естественным образом фиксируют ссылки и связи с другими частями документа. Поскольку процесс создания эмбеддингов происходит до разбиения, каждый фрагмент сохраняет осведомленность о более широком контексте документа, решая проблему отсутствия контекста, которая преследует подходы с наивным разбиением.</p><p>Для документов, превышающих максимальный размер входных данных модели, мы можем использовать \"длинное позднее разбиение\":</p><ol><li>Сначала мы разбиваем документ на перекрывающиеся \"макро-фрагменты\". Размер каждого макро-фрагмента подобран так, чтобы уместиться в максимальную длину контекста модели (например, 8 192 токена).</li><li>Модель обрабатывает эти макро-фрагменты для создания токенных эмбеддингов.</li><li>После получения токенных эмбеддингов мы переходим к стандартному позднему разбиению - применяем усреднение для создания финальных эмбеддингов фрагментов.</li></ol><p>Этот подход позволяет нам обрабатывать документы любой длины, сохраняя при этом преимущества позднего разбиения. Представьте это как двухэтапный процесс: сначала делаем документ \"удобоваримым\" для модели, затем применяем обычную процедуру позднего разбиения.</p><p>Вкратце:</p><ul><li><strong>Наивное разбиение:</strong> Разделить документ на маленькие фрагменты, затем закодировать каждый фрагмент отдельно.</li><li><strong>Позднее разбиение:</strong> Закодировать весь документ сразу для создания токенных эмбеддингов, затем создать эмбеддинги фрагментов путем объединения токенных эмбеддингов на основе границ сегментов.</li><li><strong>Длинное позднее разбиение:</strong> Разделить большие документы на перекрывающиеся макро-фрагменты, которые помещаются в контекстное окно модели, закодировать их для получения токенных эмбеддингов, затем применить позднее разбиение как обычно.</li></ul><p>Для более подробного описания идеи ознакомьтесь с нашей <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">статьей</a> или блог-постами, упомянутыми выше.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models</div><div class=\"kg-bookmark-description\">Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-6.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-2.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"to-chunk-or-not-to-chunk\">Разбивать или не разбивать?</h2><p>Мы уже видели, что эмбеддинги с длинным контекстом в целом превосходят эмбеддинги коротких текстов, и получили обзор стратегий как наивного, так и позднего разбиения. Теперь возникает вопрос: Является ли разбиение лучше, чем эмбеддинг с длинным контекстом?</p><p>Для проведения честного сравнения мы обрезаем текстовые значения до максимальной длины последовательности модели (8 192 токена) перед началом их сегментации. Мы используем сегментацию фиксированного размера с 64 токенами на сегмент (как для наивной сегментации, так и для позднего разбиения). Давайте сравним три сценария:</p><ul><li><strong>Без сегментации:</strong> Мы кодируем каждый текст в один эмбеддинг. Это приводит к тем же оценкам, что и в предыдущем эксперименте (см. Рисунок 2), но мы включаем их здесь для лучшего сравнения.</li><li><strong>Наивное разбиение:</strong> Мы сегментируем тексты, затем применяем наивное разбиение на основе границ.</li><li><strong>Позднее разбиение:</strong> Мы сегментируем тексты, затем используем позднее разбиение для определения эмбеддингов.</li></ul><p>Как для позднего разбиения, так и для наивной сегментации мы используем поиск по фрагментам для определения релевантного документа (как показано на Рисунке 5 ранее в этом посте).</p><p>Результаты не показывают явного победителя:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"600\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image-3.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap\">Рисунок 8: Без разбиения vs наивное разбиение vs позднее разбиение</span></figcaption></figure><ul><li><strong>Для поиска фактов лучше работает наивное разбиение:</strong> Для наборов данных QMSum, NarrativeQA и 2WikiMultiHopQA модель должна определить релевантные отрывки в документе. Здесь наивное разбиение явно лучше, чем кодирование всего в единый эмбеддинг, поскольку, вероятно, только несколько фрагментов содержат релевантную информацию, и эти фрагменты захватывают ее гораздо лучше, чем единый эмбеддинг всего документа.</li><li><strong>Позднее разделение лучше работает с содержательными документами и релевантным контекстом:</strong> Для документов, охватывающих связную тему, где пользователи ищут общие темы, а не конкретные факты (как в NFCorpus), позднее разделение немного превосходит отсутствие разделения, так как оно балансирует контекст всего документа с локальными деталями. Однако, хотя позднее разделение обычно работает лучше, чем наивное разделение, сохраняя контекст, это преимущество может стать недостатком при поиске отдельных фактов в документах, содержащих в основном нерелевантную информацию — как видно по снижению производительности для NarrativeQA и 2WikiMultiHopQA, где добавленный контекст становится скорее отвлекающим, чем полезным.</li></ul><h3 id=\"does-chunk-size-make-a-difference\">Имеет ли значение размер чанка?</h3><p>Эффективность методов разделения действительно зависит от набора данных, что подчеркивает важную роль структуры контента:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1800\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/12/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/12/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/12/image--21-.png 1200w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Рисунок 9: Сравнение размеров чанков при наивном и позднем разделении.</span></figcaption></figure><p>Как мы видим, позднее разделение обычно превосходит наивное разделение при меньших размерах чанков, поскольку меньшие наивные чанки слишком малы, чтобы содержать много контекста, в то время как меньшие поздние чанки сохраняют контекст всего документа, что делает их более семантически значимыми. Исключением является набор данных NarrativeQA, где просто слишком много нерелевантного контекста, из-за чего позднее разделение отстает. При больших размерах чанков наивное разделение показывает заметное улучшение (иногда превосходя позднее разделение) благодаря увеличенному контексту, в то время как производительность позднего разделения постепенно снижается.</p><h2 id=\"takeaways-when-to-use-what\">Выводы: Когда что использовать?</h2><p>В этой статье мы рассмотрели различные типы задач поиска документов, чтобы лучше понять, когда использовать сегментацию и когда помогает позднее разделение. Итак, что мы узнали?</p><h3 id=\"when-should-i-use-long-context-embedding\">Когда следует использовать встраивание с длинным контекстом?</h3><p>В целом, включение как можно большего объема текста ваших документов во входные данные модели встраивания не вредит точности поиска. Однако модели встраивания с длинным контекстом часто фокусируются на начале документов, так как они содержат контент вроде заголовков и введения, которые важнее для оценки релевантности, но модели могут пропустить контент в середине документа.</p><h3 id=\"when-should-i-use-naive-chunking\">Когда следует использовать наивное разделение?</h3><p>Когда документы охватывают несколько аспектов или пользовательские запросы нацелены на конкретную информацию внутри документа, разделение обычно улучшает производительность поиска.</p><p>В конечном счете, решения о сегментации зависят от таких факторов, как необходимость отображения частичного текста пользователям (например, как Google представляет релевантные отрывки в предпросмотрах результатов поиска), что делает сегментацию необходимой, или ограничения по вычислительным ресурсам и памяти, где сегментация может быть менее предпочтительной из-за увеличенных накладных расходов на поиск и использования ресурсов.</p><h3 id=\"when-should-i-use-late-chunking\">Когда следует использовать позднее разделение?</h3><p>Кодируя полный документ перед созданием чанков, позднее разделение решает проблему потери смысла текстовых сегментов из-за отсутствия контекста. Это особенно хорошо работает с содержательными документами, где каждая часть связана с целым. Наши эксперименты показывают, что позднее разделение особенно эффективно при разделении текста на меньшие чанки, как продемонстрировано в нашей <a href=\"https://arxiv.org/abs/2409.04701?ref=jina-ai-gmbh.ghost.io\">статье</a>. Однако есть одно предостережение: если части документа не связаны друг с другом, включение этого более широкого контекста может фактически ухудшить производительность поиска, так как добавляет шум во встраивания.</p><h2 id=\"conclusion\">Заключение</h2><p>Выбор между встраиванием с длинным контекстом, наивным разделением и поздним разделением зависит от конкретных требований вашей задачи поиска. Встраивания с длинным контекстом ценны для содержательных документов с общими запросами, в то время как разделение превосходит в случаях, когда пользователи ищут конкретные факты или информацию внутри документа. Позднее разделение дополнительно улучшает поиск, сохраняя контекстную согласованность внутри меньших сегментов. В конечном счете, понимание ваших данных и целей поиска будет направлять оптимальный подход, балансируя точность, эффективность и контекстную релевантность.</p><p>Если вы исследуете эти стратегии, рассмотрите возможность использования <code>jina-embeddings-v3</code> — его продвинутые возможности работы с длинным контекстом, позднее разделение и гибкость делают его отличным выбором для различных сценариев поиска.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings v3: A Frontier Multilingual Embedding Model</div><div class=\"kg-bookmark-description\">jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary embeddings from OpenAI and Cohere on MTEB.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-15.png\" alt=\"\"><span class=\"kg-bookmark-author\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/v3banner-5.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>",
  "comment_id": "674f1a8eb3efb50001df0e4e",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-12-03T15:49:50.000+01:00",
  "updated_at": "2024-12-05T00:55:21.000+01:00",
  "published_at": "2024-12-05T00:55:21.000+01:00",
  "custom_excerpt": "Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    },
    {
      "id": "636409b554b68a003dfbdef8",
      "name": "Michael Günther",
      "slug": "michael",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg",
      "cover_image": null,
      "bio": "ML Scientist and Engineer @ Jina AI. Enthusiastic about open source and AI with particular interest in solving information retrieval problems.",
      "website": "https://github.com/guenthermi",
      "location": "Berlin",
      "facebook": null,
      "twitter": null,
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/michael/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "632ade4a3e4e55003d525971",
    "name": "Alex C-G",
    "slug": "alexcg",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
    "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
    "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
    "website": null,
    "location": "Berlin, Germany",
    "facebook": null,
    "twitter": "@alexcg",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/still-need-chunking-when-long-context-models-can-do-it-all/",
  "excerpt": "Сравнение эффективности моделей встраивания с длинным контекстом при использовании различных стратегий разбиения на чанки для поиска оптимального подхода под ваши задачи.",
  "reading_time": 13,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
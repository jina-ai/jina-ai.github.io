{
  "slug": "jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
  "id": "6859b6967d56fd00015c4de8",
  "uuid": "d7ccf242-8983-403d-8055-37310a9ccb53",
  "title": "Jina Embeddings v4: Универсальные векторные модели (Embeddings) для мультимодального многоязыкового поиска",
  "html": "<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/models/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4 - Search Foundation Models</div><div class=\"kg-bookmark-description\">Universal embedding model for multimodal and multilingual retrieval</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-128x128-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">Search Foundation Models</span><span class=\"kg-bookmark-publisher\">Jina AI</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-38.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-34.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://huggingface.co/jinaai/jina-embeddings-v4\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jinaai/jina-embeddings-v4 · Hugging Face</div><div class=\"kg-bookmark-description\">We’re on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-39.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/jina-embeddings-v4-1.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Сегодня мы выпускаем <code>jina-embeddings-v4</code>, нашу новую универсальную модель векторного представления (embedding model) с 3,8 миллиардами параметров для текста и изображений. Она включает в себя набор LoRA-адаптеров, оптимизированных для наиболее популярных задач извлечения информации, включая извлечение запросов и документов, семантическое сопоставление и поиск кода. <code>jina-embeddings-v4</code> демонстрирует современную производительность извлечения информации в многомодальных и многоязычных задачах по MTEB, MMTEB, CoIR, LongEmbed, STS, <a href=\"https://github.com/jina-ai/jina-vdr\">Jina-VDR</a>, CLIP и ViDoRe, с особым упором на обработку визуально насыщенного контента, такого как таблицы, диаграммы, схемы и их сочетания. Модель поддерживает как одно-векторные, так и много-векторные векторные представления (embeddings).</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--18-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2781\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--18-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--18-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--18-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--18-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Производительность </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> при извлечении визуальных документов и в многомодальных задачах. Распределения boxplot показывают средние оценки и изменчивость производительности моделей векторного представления (embedding model) по шести категориям тестов: ViDoRe (извлечение визуальных документов), Jina-VDR (комплексное извлечение визуальных документов), Wikimedia Commons Retrieval (многоязыковое сопоставление документов и описаний), GitHub README Retrieval (извлечение документации по коду), Tweet Stock Retrieval (анализ финансовых графиков) и CLIP Benchmark (общее извлечение текста в изображение). Варианты Jina-embeddings-v4 (выделены бирюзовым цветом) демонстрируют современную производительность в задачах с визуально насыщенными документами, при этом многовекторная версия достигает самых высоких оценок в специализированных тестах визуальных документов (90,2 на ViDoRe, 80,2 на Jina-VDR), сохраняя при этом конкурентоспособную производительность в общих задачах многомодального извлечения (84,1 на CLIP Benchmark). Модели ранжируются по средней производительности в каждой категории тестов, при этом отдельные точки данных показывают распределение оценок по нескольким задачам оценки.</span></figcaption></figure><p><code>jina-embeddings-v4</code> — наша самая амбициозная модель векторного представления (embedding model) на сегодняшний день. Будучи моделью с открытым исходным кодом, <code>jina-embeddings-v4</code> превосходит ведущие модели векторного представления (embedding model) с закрытым исходным кодом от крупных поставщиков, обеспечивая на 12% лучшую производительность, чем <code>text-embedding-3-large</code> от OpenAI, в многоязычном поиске (66,49 против 59,27), на 28% лучше в задачах с длинными документами (67,11 против 52,42), на 15% лучше, чем <code>voyage-3</code>, в поиске кода (71,59 против 67,23), и соответствует производительности <code>gemini-embedding-001</code> от Google. Это делает v4 самой мощной универсальной моделью векторного представления (embedding model) с открытым исходным кодом, предлагая исследователям и разработчикам многомодальные возможности корпоративного уровня с полной прозрачностью процесса обучения, архитектурных решений и весов модели, как указано в <a href=\"https://arxiv.org/abs/2506.18902\">нашем подробном техническом отчете.</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/model-perf-boxplot--15-.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"2631\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/06/model-perf-boxplot--15-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/06/model-perf-boxplot--15-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/06/model-perf-boxplot--15-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2025/06/model-perf-boxplot--15-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Производительность </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> по пяти тестам извлечения информации. На диаграмме показаны распределения boxplot со средними оценками для каждой модели по тестам извлечения текста, извлечения кода, многоязычного извлечения, извлечения длинного контекста и семантической текстовой схожести (STS). </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\"> (выделена бирюзовым цветом) демонстрирует конкурентоспособную или современную производительность во всех категориях оценки, с особенно сильными результатами в извлечении текста и STS. Модели ранжируются по средней производительности в каждой категории тестов, при этом отдельные точки данных показывают распределение оценок по нескольким задачам оценки.</span></figcaption></figure><h2 id=\"new-architecture\">Новая архитектура</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2025/06/Heading--51-.svg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"630\"><figcaption><span style=\"white-space: pre-wrap;\">Архитектура </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>jina-embeddings-v4</span></code><span style=\"white-space: pre-wrap;\">. Модель построена на основе </span><code spellcheck=\"false\" style=\"white-space: pre-wrap;\"><span>Qwen2.5-VL-3B-Instruct</span></code><span style=\"white-space: pre-wrap;\"> (3,8 млрд параметров). Текстовые и графические входные данные обрабатываются по общему пути: изображения сначала преобразуются в последовательности 词元 (tokens) с помощью визуального кодировщика, затем обе модальности совместно обрабатываются декодером языковой модели со слоями контекстного внимания. Три LoRA-адаптера для конкретных задач (по 60 миллионов параметров каждый) обеспечивают специализированную оптимизацию для задач извлечения, сопоставления текста и кода, не изменяя замороженные веса основы. Архитектура поддерживает два режима вывода: (1) одно-векторные векторные представления (Embeddings) (2048 измерений, усекаемые до 128), генерируемые с помощью усредненного пулинга для эффективного поиска сходства, и (2) много-векторные векторные представления (Embeddings) (128 измерений на 词元 (token)) с помощью проекционных слоев для стратегий извлечения с поздним взаимодействием.</span></figcaption></figure><p>Обновление с <code>jina-embeddings-v3</code> до<code>jina-embeddings-v4</code> представляет собой сдвиг парадигмы от текстовых к мультимодальным 向量模型 (Embeddings). В то время как v3 была сосредоточена на оптимизации текстовых 向量模型 (Embeddings) с помощью task-specific LoRA adapters, v4 отвечает растущим требованиям к встраиванию как текстового, так и визуального контента в унифицированные представления.\n\n<table>\n<thead>\n<tr>\n<th><strong>Аспект</strong></th>\n<th><strong>jina-embeddings-v3</strong></th>\n<th><strong>jina-embeddings-v4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Backbone Model</td>\n<td>jina-XLM-RoBERTa</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n</tr>\n<tr>\n<td>Parameters (Base)</td>\n<td>559M</td>\n<td>3.8B</td>\n</tr>\n<tr>\n<td>Parameters (with adapters)</td>\n<td>572M</td>\n<td>3.8B + 60M per adapter</td>\n</tr>\n<tr>\n<td>Modalities</td>\n<td>Text only</td>\n<td>Text + Images (multimodal)</td>\n</tr>\n<tr>\n<td>Max Input Length</td>\n<td>8,192 tokens</td>\n<td>32,768 tokens</td>\n</tr>\n<tr>\n<td>Image Processing</td>\n<td>None</td>\n<td>Up to 20 megapixels, visually rich documents</td>\n</tr>\n  <tr>\n<td>Multilingual Support</td>\n<td>89 languages</td>\n<td>29+ languages</td>\n</tr>\n<tr>\n<td>Vector Types</td>\n<td>Single-vector only</td>\n<td>Single-vector + Multi-vector (late interaction)</td>\n</tr>\n<tr>\n<td>Single-vector Dimensions</td>\n<td>1024 (MRL truncatable to 32)</td>\n<td>2048 (MRL truncatable to 128)</td>\n</tr>\n<tr>\n<td>Multi-vector Dimensions</td>\n<td>Not available</td>\n<td>128 per token</td>\n</tr>\n<tr>\n<td>Task LoRA Specializations</td>\n<td>• Asymmetric retrieval<br>• Semantic similarity<br>• Classification<br>• Separation</td>\n<td>• Asymmetric retrieval<br>• Semantic similarity<br>• Code retrieval</td>\n</tr>\n<tr>\n<td>Training Stages</td>\n<td>3-stage: Pre-training → Embedding fine-tuning → Adapter training</td>\n<td>2-stage: Joint pair training → Task-specific adapter training</td>\n</tr>\n<tr>\n<td>Loss Functions</td>\n<td>InfoNCE, CoSent, Extended triplet loss</td>\n<td>Joint InfoNCE + KL divergence for single/multi-vector</td>\n</tr>\n<tr>\n<td>Positional Encoding</td>\n<td>RoPE (rotary base frequency tuning)</td>\n<td>M-RoPE (Multimodal Rotary Position Embedding)</td>\n</tr>\n<tr>\n<td>Cross-modal Processing</td>\n<td>N/A</td>\n<td>Unified encoder (reduced modality gap)</td>\n</tr>\n<tr>\n<td>MRL Support</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Attention Implementation</td>\n<td>FlashAttention2</td>\n<td>FlashAttention2</td>\n</tr>\n</tbody>\n</table>\n\n<h3 id=\"backbone\">Backbone</h3><p>Самым значительным архитектурным изменением в v4 является изменение backbone с <code>XLM-RoBERTa</code> на <code>Qwen2.5-VL-3B-Instruct</code>. Это решение было обусловлено основной целью v4 - создать универсальную модель 向量模型 (Embedding), которая позволяет осуществлять \"истинную мультимодальную обработку\", при которой изображения преобразуются в последовательности 词元 (Tokens) и обрабатываются вместе с текстом, устраняя <a href=\"https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models\">modality gap</a>, присутствующий в архитектурах с двойным кодировщиком.</p><p>Выбор backbone соответствует нескольким ключевым целям проектирования: превосходство Qwen2.5-VL в понимании документов напрямую поддерживает силу v4 в обработке визуально богатого контента, такого как таблицы, диаграммы и скриншоты. Возможности динамического разрешения позволяют v4 обрабатывать изображения, измененные до 20 мегапикселей, как указано в архитектуре. Усовершенствованное позиционное кодирование обеспечивает основу, которая позволяет v4 достичь превосходного кросс-модального выравнивания с оценкой выравнивания 0,71 по сравнению с 0,15 для OpenAI CLIP.</p><h3 id=\"lora-adapters\">LoRA Adapters</h3><p>V4 упрощает переход от пяти задач v3 к трем основным задачам, отражая уроки, извлеченные об эффективности и внедрении пользователями:</p><ul><li><strong>Asymmetric retrieval</strong> (консолидация query/passage adapters v3)</li><li><strong>Symmetric similarity</strong> (эквивалент text-matching v3 для задач STS)</li><li><strong>Code retrieval</strong> (полученный из v2-code, отсутствующий в v3)</li></ul><p>Эта консолидация удаляет адаптеры классификации и разделения v3, фокусируя v4 на наиболее эффективных вариантах использования 向量模型 (Embedding) - retrieval и STS.</p><h3 id=\"output-embeddings\">Output Embeddings</h3><p>V4 представляет систему с двойным выходом, поддерживающую как single-vector, так и multi-vector 向量模型 (Embeddings), тогда как v3 предоставляла только single-vector выходы. Это относится к различным сценариям retrieval:</p><ul><li><strong>Single-vector mode</strong>: 向量模型 (Embeddings) размерностью 2048 (усекаемые до 128 через MRL) для эффективного поиска по сходству</li><li><strong>Multi-vector mode</strong>: 128 измерений на 词元 (Token) для <a href=\"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search\">late-interaction retrieval</a></li></ul><p>Этот двойной подход обеспечивает большую эффективность с multi-vector представлениями, особенно при retrieval визуально богатых документов, сохраняя при этом эффективность для стандартных задач сходства. Последовательное преимущество multi-vector над single-vector mode в 7-10% по производительности в визуальных задачах предполагает, что late interaction обеспечивает принципиально лучшее семантическое сопоставление для мультимодального контента.</p><h3 id=\"parameter-size\">Parameter Size</h3><p>Хотя v4 в 6,7 раза больше, чем v3 (3,8B против 570M параметров), улучшения производительности только для текста на самом деле скромные, что говорит о том, что масштабирование параметров в первую очередь было обусловлено мультимодальными требованиями, а не улучшением текста. На основных текстовых бенчмарках v4 достигает 66,49 на MMTEB по сравнению с 58,58 у v3 (улучшение на 14%) и 55,97 на MTEB-EN по сравнению с 54,33 у v3 (улучшение на 3%). Для code retrieval v4 набирает 71,59 на CoIR по сравнению с 55,07 у v3 (улучшение на 30%), в то время как производительность длинных документов показывает v4 на уровне 67,11 против 55,66 у v3 на LongEmbed (улучшение на 21%). Существенное масштабирование становится оправданным при рассмотрении мультимодальных возможностей v4: достижение 84,11 nDCG@5 на визуальном document retrieval (Jina-VDR) и 90,17 на бенчмарках ViDoRe - возможностей, полностью отсутствующих в v3. Таким образом, увеличение параметров представляет собой нашу инвестицию в мультимодальную функциональность, сохраняя при этом конкурентоспособную текстовую производительность, при этом унифицированная архитектура устраняет необходимость в отдельных текстовых и визуальных моделях, достигая при этом кросс-модального выравнивания 0,71 по сравнению с 0,15 для традиционных подходов с двойным кодировщиком.</p><h2 id=\"getting-started\">Getting Started</h2><p>Для быстрой проверки попробуйте нашу демонстрацию text-to-image в Search Foundation toolbox. Мы подготовили коллекцию изображений документов с нашего веб-сайта, и вы также можете добавить свои собственные URL-адреса изображений. Просто введите свой запрос и нажмите Enter, чтобы увидеть ранжированные результаты. Вы можете извлечь его либо как OCR, либо как content-based image retrieval - также не стесняйтесь пробовать запросы не на английском языке.</p><figure class=\"kg-card kg-video-card kg-width-regular kg-card-hascaption\" data-kg-thumbnail=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1.mp4\" poster=\"https://img.spacergif.org/v1/1232x794/0a/spacer.png\" width=\"1232\" height=\"794\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2025/04/m0-demo-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:22</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            <figcaption><p><span style=\"white-space: pre-wrap;\">The demo is available at: </span><a href=\"https://jina.ai/api-dashboard/m0-image-rerank\"><span style=\"white-space: pre-wrap;\">https://jina.ai/api-dashboard/m0-image-rerank</span></a><span style=\"white-space: pre-wrap;\"> Please note that using this demo will consume your primary API key's tokens. Also the demo might seem a bit slow since it needs to download all images on the server from those URLs, and no cache is implemented for images.</span></p></figcaption>\n        </figure><h3 id=\"via-api\">Via API</h3><p>В коде ниже показано, как использовать <code>jina-embeddings-v4</code>. Вы можете передать текстовую строку, изображение в кодировке base64 или URL-адрес изображения. Новые пользователи могут получить ключ Jina API с 10 миллионами бесплатных 词元 (Tokens).</p><pre><code class=\"language-bash\">curl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- &lt;&lt;EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"海滩上美丽的日落\"\n        },\n        {\n            \"text\": \"浜辺に沈む美しい夕日\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n</code></pre><p>Из-за ограниченности ресурсов GPU наш API для создания векторных представлений (Embedding) в настоящее время поддерживает документы длиной до 8 тысяч токенов (Tokens), несмотря на то, что <code>jina-embeddings-v4</code> изначально способен обрабатывать до 32 тысяч токенов. Для приложений, требующих более длинного контекста (например, <a href=\"https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii\">Late Chunking</a>) свыше 8 тысяч токенов, мы рекомендуем развертывать наши модели через CSP или самостоятельно размещать модель.</p><h3 id=\"via-csp-marketplaces\">Через торговые площадки CSP</h3><p><code>jina-embeddings-v4</code> скоро будет доступен непосредственно на AWS, Azure и GCP по указанным там ценам.</p><h3 id=\"via-huggingface\">Через HuggingFace</h3><p>Для исследовательских и экспериментальных целей вы можете использовать модель локально со страницы Hugging Face. Мы подготовили блокнот Google Colab, демонстрирующий, как это работает.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Colab</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-38.ico\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/colab_favicon_256px-9.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><h2 id=\"conclusion\">Заключение</h2><p><code>jina-embeddings-v4</code> представляет собой наш самый значительный скачок вперед — универсальную модель векторного представления (embedding) с 3,8 миллиардами параметров, которая обрабатывает текст и изображения по единому пути, поддерживая как плотное извлечение, так и извлечение с поздним взаимодействием, превосходя при этом проприетарные модели от Google, OpenAI и Voyage AI, особенно в извлечении документов с большим количеством визуального контента. Но эта возможность возникла не изолированно; это кульминация четырех поколений решения фундаментальных ограничений.</p><p>Когда мы начинали с <code>jina-embeddings-v1</code> в начале 2022 года, все предполагали, что больше данных означает лучшую производительность. Мы доказали обратное — фильтрация 1,5 миллиарда пар до 385 миллионов высококачественных примеров превзошла гораздо большие наборы данных. Урок: курирование бьет коллекцию.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2307.11224\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</div><div class=\"kg-bookmark-description\">Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model’s awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-35.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-31.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Но пользователи продолжали наталкиваться на предел BERT в 512 токенов. Обучение на более длинных последовательностях казалось дорогостоящим, пока <code>jina-embeddings-v2</code> не раскрыл элегантное решение: обучать короткие, развертывать длинные. Линейные смещения внимания ALiBi позволяют моделям, обученным на 512 токенах, беспрепятственно обрабатывать 8192 токена при выводе. Мы получили больше возможностей за меньшие вычислительные ресурсы.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2310.19923\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</div><div class=\"kg-bookmark-description\">Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI’s proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-36.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-32.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Успех <code>jina-embeddings-v2</code> выявил еще одно ограничение — для разных задач нужны разные оптимизации. Вместо создания отдельных моделей <code>jina-embeddings-v3</code> использовал крошечные 60M LoRA адаптеры для настройки базовой модели 570M для любой задачи. Одна модель стала пятью специализированными моделями.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2409.10173\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v3: Multilingual Embeddings With Task LoRA</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-37.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Saba Sturua</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-33.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Даже с учетом специализации задач мы оставались только в текстовом формате, в то время как пользователям было необходимо визуальное понимание. Стандартные модели на основе CLIP, такие как <code>jina-clip-v1</code> и <code>jina-clip-v2</code>, используют отдельные кодировщики, создавая \"разрыв модальности\", когда схожий контент в разных форматах оказывается далеко друг от друга. Как и в нашей недавно выпущенной <code>jina-reranker-m0</code> (重排器 (Reranker)), в <code>jina-embeddings-v4</code> это было полностью устранено — единый путь обработки всего, устраняя разрыв, а не перекрывая его.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://arxiv.org/abs/2506.18902\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</div><div class=\"kg-bookmark-description\">We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-39.png\" alt=\"\"><span class=\"kg-bookmark-author\">arXiv.org</span><span class=\"kg-bookmark-publisher\">Michael Günther</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-35.png\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Как <code>jina-embeddings-v4</code>, так и <code>jina-reranker-m0</code> разделяют фундаментальный сдвиг: использование больших языковых моделей (LLM) в качестве основы вместо моделей только с кодировщиком. Это не случайно — это отражает глубокое преимущество, которое большинство упускает из виду: модели только с кодировщиком создают \"разрывы модальности\", где изображения кластеризуются отдельно от текста. Модели только с декодировщиком открывают возможности, которые были недостижимы с архитектурами только с кодировщиком, включая истинное представление смешанной модальности и объяснимость.</p><p>Наша ключевая идея: векторные модели (embeddings) и генерация — это понимание семантики. Большие языковые модели (LLM), преуспевающие в генерации, естественно преуспевают и в представлении данных. Мы считаем, что будущее за унифицированными архитектурами, где векторное представление и переранжирование (reranking) возникают из <strong>одной и той же базовой модели поиска</strong> — и именно к этому стремится Jina AI.</p>",
  "comment_id": "6859b6967d56fd00015c4de8",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2025/06/je-v4.png",
  "featured": true,
  "visibility": "public",
  "created_at": "2025-06-23T22:18:30.000+02:00",
  "updated_at": "2025-06-25T06:48:16.000+02:00",
  "published_at": "2025-06-25T06:48:16.000+02:00",
  "custom_excerpt": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "62e3d0ef9cd5ce003d5e49e2",
      "name": "Jina AI",
      "slug": "company",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
      "cover_image": null,
      "bio": "Creator of neural search, contributor to open source.",
      "website": "https://www.jina.ai",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@JinaAI_",
      "meta_title": null,
      "meta_description": null,
      "threads": null,
      "bluesky": null,
      "mastodon": null,
      "tiktok": null,
      "youtube": null,
      "instagram": null,
      "linkedin": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/company/"
    }
  ],
  "tags": [
    {
      "id": "655b2782bb728c000101bed7",
      "name": "Press",
      "slug": "press",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
    }
  ],
  "primary_author": {
    "id": "62e3d0ef9cd5ce003d5e49e2",
    "name": "Jina AI",
    "slug": "company",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg",
    "cover_image": null,
    "bio": "Creator of neural search, contributor to open source.",
    "website": "https://www.jina.ai",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@JinaAI_",
    "meta_title": null,
    "meta_description": null,
    "threads": null,
    "bluesky": null,
    "mastodon": null,
    "tiktok": null,
    "youtube": null,
    "instagram": null,
    "linkedin": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/company/"
  },
  "primary_tag": {
    "id": "655b2782bb728c000101bed7",
    "name": "Press",
    "slug": "press",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/press/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval/",
  "excerpt": "Jina Embeddings v4 — это универсальная модель векторного представления (Embeddings) с 3,8 миллиардами параметров для мультимодального и многоязыкового поиска, которая поддерживает вывод как одно-, так и многовекторных векторных представлений (Embeddings).",
  "reading_time": 12,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": null,
  "feature_image_caption": null
}
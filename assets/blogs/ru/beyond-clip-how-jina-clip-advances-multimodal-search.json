{
  "id": "671b96784821eb000165d2de",
  "uuid": "ec571b8c-d111-4d49-bad8-2836bd885f1c",
  "title": "CLIP и не только: как Jina-CLIP развивает мультимодальный поиск",
  "slug": "beyond-clip-how-jina-clip-advances-multimodal-search",
  "html": "<p>Мультимодальный поиск, объединяющий текст и изображения в единый поисковый опыт, набрал популярность благодаря таким моделям, как <a href=\"https://openai.com/index/clip/?ref=jina-ai-gmbh.ghost.io\">OpenAI CLIP</a>. Эти модели эффективно преодолевают разрыв между визуальными и текстовыми данными, позволяя связывать изображения с релевантным текстом и наоборот.</p>\n\n<p>Хотя CLIP и подобные модели мощные, у них есть заметные ограничения, особенно при обработке длинных текстов или сложных текстовых взаимосвязей. Здесь на помощь приходит <code>jina-clip-v1</code>.</p>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image/?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Jina CLIP v1: A Truly Multimodal Embeddings Model for Text and Image</div><div class=\"kg-bookmark-description\">Jina AI's new multimodal embedding model not only outperforms OpenAI CLIP in text-image retrieval, it's a solid image embedding model and state-of-the-art text embedding model at the same time. You don't need different models for different modalities any more.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jina.ai/icons/favicon-128x128.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/06/--.jpg\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure>\n\n<p>Разработанный для решения этих проблем, <code>jina-clip-v1</code> предлагает улучшенное понимание текста, сохраняя при этом надежные возможности сопоставления текста и изображений. Он предоставляет более оптимизированное решение для приложений, использующих обе модальности, упрощая процесс поиска и устраняя необходимость жонглировать отдельными моделями для текста и изображений.</p>\n\n<p>В этой статье мы рассмотрим, что <code>jina-clip-v1</code> привносит в мультимодальные поисковые приложения, демонстрируя эксперименты, которые показывают, как он улучшает как точность, так и разнообразие результатов через интегрированные текстовые и визуальные эмбеддинги.</p>\n\n<h2 id=\"what-is-clip\">Что такое CLIP?</h2>\n\n<p>CLIP (Contrastive Language–Image Pretraining) — это архитектура ИИ-модели, разработанная OpenAI, которая связывает текст и изображения путем изучения совместных представлений. CLIP по сути представляет собой текстовую модель и модель изображений, объединенные вместе — он преобразует оба типа входных данных в общее пространство эмбеддингов, где похожие тексты и изображения располагаются близко друг к другу. CLIP был обучен на огромном наборе пар изображение-текст, что позволяет ему понимать взаимосвязь между визуальным и текстовым контентом. Это позволяет ему хорошо обобщать знания в разных областях, делая его высокоэффективным в сценариях обучения с нулевым выстрелом, таких как генерация подписей или поиск изображений.</p>\n\n<p>После выпуска CLIP другие модели, такие как <a href=\"https://arxiv.org/abs/2303.15343?ref=jina-ai-gmbh.ghost.io\">SigLiP</a>, <a href=\"https://arxiv.org/abs/2111.07991?ref=jina-ai-gmbh.ghost.io\">LiT</a> и <a href=\"https://arxiv.org/abs/2303.15389?ref=jina-ai-gmbh.ghost.io\">EvaCLIP</a>, расширили его основу, улучшив такие аспекты, как эффективность обучения, масштабирование и мультимодальное понимание. Эти модели часто используют более крупные наборы данных, улучшенные архитектуры и более сложные методы обучения, чтобы расширить границы согласования текста и изображений, продвигая вперед область моделей изображение-язык.</p>\n\n<p>Хотя CLIP <em>может</em> работать только с текстом, у него есть существенные ограничения. Во-первых, он был обучен только на коротких текстовых подписях, а не на длинных текстах, обрабатывая максимум около 77 слов. Во-вторых, CLIP отлично справляется с соединением текста и изображений, но испытывает трудности при сравнении текста с другим текстом, например, при распознавании того, что строки <code>a crimson fruit</code> и <code>a red apple</code> могут относиться к одному и тому же предмету. Здесь проявляют себя специализированные текстовые модели, такие как <code>jina-embeddings-v3</code>.</p>\n\n<p>Эти ограничения усложняют поисковые задачи, включающие как текст, так и изображения, например, в онлайн-магазине \"shop the look\", где пользователь может искать модные товары, используя либо текстовую строку, либо изображение. При индексации ваших товаров необходимо обрабатывать каждый из них несколько раз - один раз для изображения, один раз для текста и еще раз с помощью специальной текстовой модели. Аналогично, когда пользователь ищет товар, ваша система должна выполнить поиск как минимум дважды, чтобы найти как текстовые, так и визуальные совпадения:</p>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png\" class=\"kg-image\" alt=\"Flowchart outlining &quot;Offline Indexing&quot; and &quot;Online Querying&quot; processes with labeled blocks and arrows for XML data interactio\" loading=\"lazy\" width=\"970\" height=\"1255\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-27.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-27.png 970w\" sizes=\"(min-width: 720px) 720px\"></figure>\n\n<h2 id=\"how-jina-clip-v1-solves-clip%E2%80%99s-shortcomings\"><strong>Как </strong><code>jina-clip-v1</code><strong> решает недостатки CLIP</strong></h2>\n\n<p>Чтобы преодолеть ограничения CLIP, мы создали <code>jina-clip-v1</code>, способный понимать более длинные тексты и эффективнее сопоставлять текстовые запросы с текстами и изображениями. Что делает <code>jina-clip-v1</code> таким особенным? Во-первых, он использует более умную модель понимания текста (JinaBERT), помогающую понимать более длинные и сложные фрагменты текста (например, описания товаров), а не только короткие подписи (например, названия товаров). Во-вторых, мы обучили <code>jina-clip-v1</code> быть хорошим сразу в двух вещах: как в сопоставлении текста с изображениями, так и в сопоставлении текста с другими фрагментами текста.</p>\n\n<p>С OpenAI CLIP это не так: для индексации и запросов необходимо вызывать две модели (CLIP для изображений и коротких текстов, таких как подписи, и другой текстовый эмбеддинг для более длинных текстов, таких как описания). Это не только создает дополнительную нагрузку, но и замедляет поиск — операцию, которая <em>должна</em> быть очень быстрой. <code>jina-clip-v1</code> делает все это в одной модели, без потери скорости:</p>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-22.png\" class=\"kg-image\" alt=\"Flowchart of JaclinQ's offline indexing and online querying processes, involving imagery and text analysis.\" loading=\"lazy\" width=\"2000\" height=\"2785\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-22.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-22.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-22.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2024/10/image-22.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure>\n\n<p>Этот унифицированный подход открывает новые возможности, которые были сложны с более ранними моделями, потенциально изменяя наш подход к поиску. В этой статье мы провели два эксперимента:</p>\n\n<ul>\n<li><strong>Улучшение результатов поиска путем комбинирования текстового и визуального поиска</strong>: Можем ли мы объединить то, что <code>jina-clip-v1</code> понимает из текста, с тем, что он понимает из изображений? Что происходит, когда мы смешиваем эти два типа понимания? Меняет ли добавление визуальной информации наши результаты поиска? Короче говоря, можем ли мы получить лучшие результаты, если будем искать с помощью текста и изображений одновременно?</li>\n<li><strong>Использование изображений для разнообразия результатов поиска</strong>: Большинство поисковых систем максимизируют текстовые совпадения. Но можем ли мы использовать понимание изображений <code>jina-clip-v1</code> как \"визуальное перемешивание\"? Вместо того чтобы просто показывать наиболее релевантные результаты, мы могли бы включить визуально разнообразные. Речь идет не о поиске большего количества связанных результатов, а о показе более широкого спектра перспектив, даже если они менее тесно связаны. Делая это, мы можем обнаружить аспекты темы, о которых раньше не задумывались. Например, в контексте поиска моды, если пользователь ищет \"разноцветное коктейльное платье\", хотят ли они, чтобы верхние позиции выглядели одинаково (т.е. <em>очень</em> близкие совпадения) или предпочтут более широкий выбор (через визуальное перемешивание)?</li>\n</ul>\n\n<p>Оба подхода ценны в различных случаях использования, где пользователи могут искать с помощью текста или изображений, например, в электронной коммерции, медиа, искусстве и дизайне, медицинской визуализации и не только.</p>\n\n<h2 id=\"averaging-text-and-image-embeddings-for-above-average-performance\">Усреднение текстовых и визуальных эмбеддингов для выше среднего результата</h2>\n\n<p>Когда пользователь отправляет запрос (обычно в виде текстовой строки), мы можем использовать текстовую башню <code>jina-clip-v1</code> для кодирования запроса в текстовый эмбеддинг. Сила <code>jina-clip-v1</code> заключается в его способности понимать как текст, так и изображения, выравнивая текст-к-тексту и текст-к-изображению сигналы в одном семантическом пространстве.</p>\n\n<p>Можем ли мы улучшить результаты поиска, если объединим предварительно проиндексированные текстовые и визуальные эмбеддинги каждого продукта путем их усреднения?</p>\n\n<figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png\" class=\"kg-image\" alt=\"Flowchart on a black background detailing text and image embedding processes with a black knit midi dress photo example.\" loading=\"lazy\" width=\"995\" height=\"359\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-28.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-28.png 995w\" sizes=\"(min-width: 720px) 720px\"></figure>\n\n<p>Это создает единое представление, включающее как текстовую информацию (например, описание продукта), так и визуальную информацию (например, изображение продукта). Затем мы можем использовать эмбеддинг текстового запроса для поиска по этим смешанным представлениям. Как это влияет на наши результаты поиска?</p>\n\n<p>Чтобы выяснить это, мы использовали набор данных <a href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\">Fashion200k</a>, масштабный набор данных, специально созданный для задач, связанных с поиском модных изображений и кросс-модальным пониманием. Он состоит из более чем 200 000 изображений модных товаров, таких как одежда, обувь и аксессуары, вместе с соответствующими описаниями продуктов и метаданными.</p>\n\n<figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/xthan/fashion-200k?ref=jina-ai-gmbh.ghost.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - xthan/fashion-200k: Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\"</div><div class=\"kg-bookmark-description\">Fashion 200K dataset used in paper \"Automatic Spatially-aware Fashion Concept Discovery.\" - xthan/fashion-200k</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">xthan</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2116651d448aec6ea0508f5fdb123e6292fa00bfb1cf8fb6f3468cbe761da769/xthan/fashion-200k\" alt=\"\" onerror=\"this.style.display = 'none'\"></div></a></figure><p>Мы дополнительно классифицировали каждый элемент по широкой категории (например, <code>dress</code>) и детальной категории (например, <code>knit midi dress</code>).</p><h3 id=\"analyzing-three-retrieval-methods\"><strong>Анализ трех методов поиска</strong></h3><p>Чтобы понять, дает ли усреднение текстовых и визуальных эмбеддингов лучшие результаты поиска, мы экспериментировали с тремя типами поиска, каждый из которых использует текстовую строку (например, <code>red dress</code>) в качестве запроса:</p><ul><li><strong>Query to Description с использованием текстовых эмбеддингов:</strong> Поиск по описаниям товаров на основе текстовых эмбеддингов.</li><li><strong>Query to Image с использованием кросс-модального поиска:</strong> Поиск по изображениям товаров на основе визуальных эмбеддингов.</li><li><strong>Query to Average Embedding:</strong> Поиск по усредненным эмбеддингам описаний и изображений товаров.</li></ul><p>Сначала мы проиндексировали весь набор данных, а затем сгенерировали 1000 случайных запросов для оценки производительности. Мы кодировали каждый запрос в текстовый эмбеддинг и сопоставляли его отдельно, используя описанные выше методы. Мы измеряли точность по тому, насколько хорошо категории возвращаемых товаров соответствовали входному запросу.</p><p>Когда мы использовали запрос <code>multicolor henley t-shirt dress</code>, поиск <strong>Query-to-Description</strong> достиг наивысшей точности top-5, но последние три платья из топ-результатов были визуально идентичны. Это не идеально, так как эффективный поиск должен балансировать между релевантностью и разнообразием, чтобы лучше привлекать внимание пользователя.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png\" class=\"kg-image\" alt=\"Array of five unique dresses, categorized as casual and day, arranged in a row on a white background with named tags for easy\" loading=\"lazy\" width=\"2000\" height=\"480\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-13.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Кросс-модальный поиск <strong>Query-to-Image</strong> использовал тот же запрос и показал противоположный подход, представив очень разнообразную коллекцию платьев. Хотя два из пяти результатов совпали с правильной широкой категорией, ни один не соответствовал детальной категории.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png\" class=\"kg-image\" alt=\"Variety of women's clothing items including short and long-sleeved tops and casual to maxi dresses with color swatches.\" loading=\"lazy\" width=\"2000\" height=\"496\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-14.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Поиск по <strong>усредненным текстовым и визуальным эмбеддингам</strong> дал наилучший результат: все пять результатов соответствовали широкой категории, а два из пяти совпали с детальной категорией. Кроме того, визуально дублирующиеся элементы были исключены, что обеспечило более разнообразный выбор. Использование текстовых эмбеддингов для поиска по усредненным текстовым и визуальным эмбеддингам, похоже, сохраняет качество поиска, одновременно учитывая визуальные подсказки, что приводит к более разнообразным и комплексным результатам.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png\" class=\"kg-image\" alt=\"Showcase of various women's dresses, including a multicolor henley t-shirt dress and a pink Missoni dress, labeled with categ\" loading=\"lazy\" width=\"2000\" height=\"513\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-15.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"scaling-up-evaluating-with-more-queries\"><strong>Масштабирование: оценка с большим количеством запросов</strong></h3><p>Чтобы проверить, будет ли это работать в большем масштабе, мы продолжили эксперимент с дополнительными широкими и детальными категориями. Мы провели несколько итераций, получая разное количество результатов (\"k-значения\") каждый раз.</p><p>Как для широких, так и для детальных категорий, <strong>Query to Average Embedding</strong> стабильно достигал наивысшей точности для всех k-значений (10, 20, 50, 100). Это показывает, что комбинирование текстовых и визуальных эмбеддингов обеспечивает наиболее точные результаты для поиска релевантных элементов, независимо от того, является ли категория широкой или специфической:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png\" class=\"kg-image\" alt=\"Comparative chart of 'Broad Precision@K' and 'Fine-grained Precision@K' showing different precision values for query-related \" loading=\"lazy\" width=\"2000\" height=\"836\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-16.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure>\n<!--kg-card-begin: html-->\n<table>\n<thead>\n<tr>\n<th><strong>k</strong></th>\n<th><strong>Search Type</strong></th>\n<th><strong>Broad Category Precision (cosine similarity)</strong></th>\n<th><strong>Fine-grained Category Precision (cosine similarity)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>Query to Description</td>\n<td>0.9026</td>\n<td>0.2314</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Image</td>\n<td>0.7614</td>\n<td>0.2037</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9230</strong></td>\n<td><strong>0.2711</strong></td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Description</td>\n<td>0.9150</td>\n<td>0.2316</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Image</td>\n<td>0.7523</td>\n<td>0.1964</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9229</strong></td>\n<td><strong>0.2631</strong></td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Description</td>\n<td>0.9134</td>\n<td>0.2254</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Image</td>\n<td>0.7418</td>\n<td>0.1750</td>\n</tr>\n<tr>\n<td>50</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9226</strong></td>\n<td><strong>0.2390</strong></td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Description</td>\n<td>0.9092</td>\n<td>0.2139</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Image</td>\n<td>0.7258</td>\n<td>0.1675</td>\n</tr>\n<tr>\n<td>100</td>\n<td>Query to Avg Embedding</td>\n<td><strong>0.9150</strong></td>\n<td><strong>0.2286</strong></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: html-->\n<ul><li><strong>Query to Description с использованием текстовых эмбеддингов</strong> показал хорошие результаты в обеих категориях, но немного отстал от подхода с усредненными эмбеддингами. Это предполагает, что текстовые описания сами по себе предоставляют ценную информацию, особенно для более широких категорий, таких как \"платье\", но могут не хватать тонкости для точной детальной классификации (например, для различения разных типов платьев).</li><li><strong>Query to Image с использованием кросс-модального поиска</strong> стабильно показывал самую низкую точность в обеих категориях. Это говорит о том, что, хотя визуальные характеристики могут помочь определить широкие категории, они менее эффективны для определения тонких различий конкретных модных предметов. Проблема различения детальных категорий исключительно по визуальным характеристикам особенно очевидна, когда визуальные различия могут быть тонкими и требуют дополнительного контекста, предоставляемого текстом.</li><li>В целом, комбинирование текстовой и визуальной информации (через <strong>усредненные эмбеддинги</strong>) достигло высокой точности как в широких, так и в детальных задачах поиска модной одежды. Текстовые описания играют важную роль, особенно в определении широких категорий, в то время как изображения сами по себе менее эффективны в обоих случаях.</li></ul><p>В целом, точность была значительно выше для широких категорий по сравнению с детальными категориями, в основном потому, что элементы в широких категориях (например, <code>dress</code>) были более представлены в наборе данных, чем детальные категории (например, <code>henley dress</code>), просто потому что последние являются подмножеством первых. По своей природе широкую категорию легче обобщить, чем детальную категорию. За пределами примера с модой легко определить, что что-то в целом является птицей. Гораздо сложнее определить, что это <a href=\"https://www.youtube.com/watch?v=nPhVOZiPokA&ref=jina-ai-gmbh.ghost.io\">Райская птица Вогелкопа</a>.</p><p>Еще одно важное замечание: информация в текстовом запросе легче соответствует другим текстам (таким как названия продуктов или описания), чем визуальным характеристикам. Поэтому, если текст используется как входные данные, тексты являются более вероятным выходом, чем изображения. Мы получаем лучшие результаты, комбинируя как изображения, так и текст (путем усреднения эмбеддингов) в нашем индексе.</p><h2 id=\"retrieve-results-with-text-diversify-them-with-images\">Получение результатов с помощью текста; их разнообразие с помощью изображений</h2><p>В предыдущем разделе мы затронули проблему визуально дублирующихся результатов поиска. В поиске <em>одной только точности не всегда достаточно</em>. Во многих случаях поддержание краткого, но высоко релевантного и разнообразного ранжированного списка более эффективно, особенно когда запрос пользователя неоднозначен (например, если пользователь ищет<code>black jacket</code> — имеют ли они в виду черную байкерскую куртку, бомбер, блейзер или какой-то другой вид?)</p><p>Теперь, вместо использования кросс-модальных возможностей <code>jina-clip-v1</code>, давайте используем текстовые эмбеддинги из текстовой башни для начального текстового поиска, а затем применим эмбеддинги изображений из башни изображений как \"визуальный ранжировщик\" для диверсификации результатов поиска. Это проиллюстрировано на диаграмме ниже:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png\" class=\"kg-image\" alt=\"Блок-схема, детализирующая мультимодальную обработку текстовых документов, с ветвями для текстовых и графических эмбеддингов и различными путями обработки\" loading=\"lazy\" width=\"975\" height=\"476\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-29.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-29.png 975w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><ol><li>Сначала получаем топ-k результатов поиска на основе текстовых эмбеддингов.</li><li>Для каждого из лучших результатов поиска извлекаем визуальные характеристики и кластеризуем их с помощью эмбеддингов изображений.</li><li>Переупорядочиваем результаты поиска, выбирая по одному элементу из каждого кластера, и представляем пользователю диверсифицированный список.</li></ol><p>После получения пятидесяти лучших результатов мы применили облегченную кластеризацию k-means (k=5) к эмбеддингам изображений, затем выбрали элементы из каждого кластера. Точность категорий осталась согласованной с производительностью Query-to-Description, так как мы использовали соответствие запроса категории продукта в качестве метрики измерения. Однако ранжированные результаты начали охватывать больше различных аспектов (таких как ткань, крой и узор) благодаря диверсификации на основе изображений. Для справки, вот пример разноцветного платья-хенли из предыдущего случая:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png\" class=\"kg-image\" alt=\"Коллекция платьев-футболок, разделенных на повседневные и дневные, с короткими и длинными рукавами, отображенная в два ряда.\" loading=\"lazy\" width=\"2000\" height=\"1484\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-18.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Теперь давайте посмотрим, как диверсификация влияет на результаты поиска при использовании поиска по текстовым эмбеддингам в сочетании с эмбеддингами изображений в качестве диверсифицирующего ранжировщика:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png\" class=\"kg-image\" alt=\"Пять разнообразных платьев, расположенных в ряд, категоризированных как различные типы, включая повседневные и дневные платья, мини и короткие, и другие варианты\" loading=\"lazy\" width=\"2000\" height=\"465\" srcset=\"https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/10/image-19.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/10/image-19.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/10/image-19.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/10/image-19.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ранжированные результаты происходят из текстового поиска, но начинают охватывать более разнообразные \"аспекты\" в первой пятерке примеров. Это достигает эффекта, похожего на усреднение эмбеддингов, без фактического усреднения.</p><p>Однако это имеет свою цену: нам необходимо применять дополнительный шаг кластеризации после получения топ-k результатов, что добавляет несколько дополнительных миллисекунд, в зависимости от размера начального ранжирования. Кроме того, определение значения k для кластеризации k-means включает некоторые эвристические предположения. Это та цена, которую мы платим за улучшенную диверсификацию результатов!</p><h2 id=\"conclusion\">Заключение</h2><p><code>jina-clip-v1</code> эффективно преодолевает разрыв между текстовым и визуальным поиском, объединяя обе модальности в единой, эффективной модели. Наши эксперименты показали, что его способность обрабатывать более длинные, сложные текстовые входные данные вместе с изображениями обеспечивает превосходную производительность поиска по сравнению с традиционными моделями, такими как CLIP.</p><p>Наше тестирование охватывало различные методы, включая сопоставление текста с описаниями, изображениями и усредненными эмбеддингами. Результаты последовательно показывали, что комбинирование текстовых и визуальных эмбеддингов давало наилучшие результаты, улучшая как точность, так и разнообразие результатов поиска. Мы также обнаружили, что использование эмбеддингов изображений в качестве \"визуального ранжировщика\" повышало разнообразие результатов при сохранении релевантности.</p><p>Эти достижения имеют значительные последствия для реальных приложений, где пользователи осуществляют поиск, используя как текстовые описания, так и изображения. Благодаря одновременному пониманию обоих типов данных, <code>jina-clip-v1</code> оптимизирует процесс поиска, предоставляя более релевантные результаты и обеспечивая более разнообразные рекомендации продуктов. Эта унифицированная возможность поиска выходит за рамки электронной коммерции и приносит пользу управлению медиа-активами, цифровым библиотекам и курированию визуального контента, упрощая поиск релевантного контента в различных форматах.</p><p>В то время как <code>jina-clip-v1</code> в настоящее время поддерживает только английский язык, мы работаем над <code>jina-clip-v2</code>. Следуя по стопам <code>jina-embeddings-v3</code> и <code>jina-colbert-v2</code>, эта новая версия станет современным многоязычным мультимодальным поисковиком, поддерживающим 89 языков. Это обновление откроет новые возможности для задач поиска и извлечения информации на разных рынках и в разных отраслях, делая его более мощной моделью эмбеддингов для глобальных приложений в электронной коммерции, медиа и других областях.</p>",
  "comment_id": "671b96784821eb000165d2de",
  "feature_image": "https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg",
  "featured": false,
  "visibility": "public",
  "created_at": "2024-10-25T15:00:40.000+02:00",
  "updated_at": "2024-10-30T19:14:11.000+01:00",
  "published_at": "2024-10-29T11:51:40.000+01:00",
  "custom_excerpt": "Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.",
  "codeinjection_head": null,
  "codeinjection_foot": null,
  "custom_template": null,
  "canonical_url": null,
  "authors": [
    {
      "id": "6360e7e05e0f6e004d70bd99",
      "name": "Bo Wang",
      "slug": "bo",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
      "cover_image": null,
      "bio": "Developer @Jina, Contributor to open source ",
      "website": "https://bwanglzu.github.io/",
      "location": "Berlin",
      "facebook": null,
      "twitter": "@bo_wangbo",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "id": "632ade4a3e4e55003d525971",
      "name": "Alex C-G",
      "slug": "alexcg",
      "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg",
      "cover_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/twitter_banner.jpg",
      "bio": "Open-source Evangelist @ Jina AI. Old, grim, and full of Vim",
      "website": null,
      "location": "Berlin, Germany",
      "facebook": null,
      "twitter": "@alexcg",
      "meta_title": null,
      "meta_description": null,
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "tags": [
    {
      "id": "634a1a8ccebfc1003d8ab706",
      "name": "Tech Blog",
      "slug": "tech-blog",
      "description": null,
      "feature_image": null,
      "visibility": "public",
      "og_image": null,
      "og_title": null,
      "og_description": null,
      "twitter_image": null,
      "twitter_title": null,
      "twitter_description": null,
      "meta_title": null,
      "meta_description": null,
      "codeinjection_head": null,
      "codeinjection_foot": null,
      "canonical_url": null,
      "accent_color": null,
      "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
    }
  ],
  "primary_author": {
    "id": "6360e7e05e0f6e004d70bd99",
    "name": "Bo Wang",
    "slug": "bo",
    "profile_image": "https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg",
    "cover_image": null,
    "bio": "Developer @Jina, Contributor to open source ",
    "website": "https://bwanglzu.github.io/",
    "location": "Berlin",
    "facebook": null,
    "twitter": "@bo_wangbo",
    "meta_title": null,
    "meta_description": null,
    "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
  },
  "primary_tag": {
    "id": "634a1a8ccebfc1003d8ab706",
    "name": "Tech Blog",
    "slug": "tech-blog",
    "description": null,
    "feature_image": null,
    "visibility": "public",
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "canonical_url": null,
    "accent_color": null,
    "url": "https://jina-ai-gmbh.ghost.io/tag/tech-blog/"
  },
  "url": "https://jina-ai-gmbh.ghost.io/podcast/beyond-clip-how-jina-clip-advances-multimodal-search/",
  "excerpt": "Узнайте, как Jina-CLIP улучшает модель CLIP от OpenAI, обеспечивая более высокую точность поиска и более разнообразные результаты благодаря унифицированным текстово-графическим эмбеддингам.",
  "reading_time": 11,
  "access": true,
  "comments": false,
  "og_image": null,
  "og_title": null,
  "og_description": null,
  "twitter_image": null,
  "twitter_title": null,
  "twitter_description": null,
  "meta_title": null,
  "meta_description": null,
  "email_subject": null,
  "frontmatter": null,
  "feature_image_alt": "Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere.",
  "feature_image_caption": null
}
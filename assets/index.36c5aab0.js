var t={tokenizer:{chunk:n=>{const{normalize:e}=n;return e(["Chunk"])},chunking:n=>{const{normalize:e}=n;return e(["Chunking long documents, lightning fast!"])},chunking_explain:n=>{const{normalize:e}=n;return e(["You can also use Tokenizer API to chunk long documents into smaller segments, making it easier to process them in embeddings or rerankers. We leverage common structural cues and build a set of rules and heuristics which should perform exceptionally well across diverse types of content, including Markdown, HTML, LaTeX, and more, ensuring accurate segmentation of text into meaningful chunks."])},chunks_in_total:n=>{const{normalize:e,interpolate:r,named:o}=n;return e([r(o("_numChunks"))," chunks in total"])},show_space:n=>{const{normalize:e}=n;return e(["Show leading/trailing spaces"])},what_is:n=>{const{normalize:e}=n;return e(["What is a Tokenizer?"])},is_free:n=>{const{normalize:e}=n;return e(["Tokenizer API is free!"])},is_free_description:n=>{const{normalize:e}=n;return e(["By providing your API key, you can access a higher rate limit, and your key won't be charged."])},explain:n=>{const{normalize:e}=n;return e(["A tokenizer is a crucial component that converts text into tokens, which are the basic units of data that an embedding/reranker model or LLM processes. Tokens can represent whole words, parts of words, or even individual characters."])},description_long:n=>{const{normalize:e}=n;return e(["Our Tokenizer API is crucial for helping LLMs manage input within context limits, and optimizing model performance. It allows developers to count tokens and extract relevant text segments, ensuring efficient data processing and cost management."])},title:n=>{const{normalize:e}=n;return e(["Tokenizer API"])},usage:n=>{const{normalize:e}=n;return e(["Usage"])},input_text:n=>{const{normalize:e}=n;return e(["Input text"])},description:n=>{const{normalize:e}=n;return e(["Free API to tokenize text and segment long text into chunks."])},free_api:n=>{const{normalize:e}=n;return e(["Tokenizer API is free to use. By providing your API key, you can access a higher rate limit, and you key won't be charged."])},basic_usage:n=>{const{normalize:e}=n;return e(["Use GET request to count tokens"])},visualization:n=>{const{normalize:e}=n;return e(["Visualization"])},basic_usage_explain:n=>{const{normalize:e}=n;return e(["You can simply send a GET request to count the number of tokens in your text."])},count_tokens_hint:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["<b>",r(o("_numTokens")),"</b> tokens, ",r(o("_numChars"))," characters."])},advance_usage:n=>{const{normalize:e}=n;return e(["Use POST request for more features"])},change_content:n=>{const{normalize:e}=n;return e(["Change 'content' and see live result"])},token_index:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Token index: ",r(o("_index"))])},table:{td_1_0:n=>{const{normalize:e}=n;return e(["Tokenize texts, count and get first/last-N tokens."])},td_1_1:n=>{const{normalize:e}=n;return e(["20 RPM"])},td_1_2:n=>{const{normalize:e}=n;return e(["200 RPM"])},td_1_3:n=>{const{normalize:e}=n;return e(["1000 RPM"])},td_1_4:n=>{const{normalize:e}=n;return e(["No charge"])},td_1_5:n=>{const{normalize:e}=n;return e(["800ms"])}},faq_v1:{title:n=>{const{normalize:e}=n;return e(["Tokenizer-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["How much does the Tokenizer API cost?"])},answer1:n=>{const{normalize:e}=n;return e(["The Tokenizer API is free to use. By providing your API key, you can access a higher rate limit, and your key won't be charged."])},question2:n=>{const{normalize:e}=n;return e(["If I don't provide an API key, what is the rate limit?"])},answer2:n=>{const{normalize:e}=n;return e(["Without an API key, you can access the Tokenizer API at a rate limit of 20 RPM."])},question3:n=>{const{normalize:e}=n;return e(["If I provide an API key, what is the rate limit?"])},answer3:n=>{const{normalize:e}=n;return e(["With an API key, you can access the Tokenizer API at a rate limit of 200 RPM. For premium paid users, the rate limit is 1000 RPM."])},question4:n=>{const{normalize:e}=n;return e(["Will you charge the tokens from my API key?"])},answer4:n=>{const{normalize:e}=n;return e(["No, your API key is only used to access a higher rate limit."])},question5:n=>{const{normalize:e}=n;return e(["Does the Tokenizer API support multiple languages?"])},answer5:n=>{const{normalize:e}=n;return e(["Yes, the Tokenizer API is multilingual and supports over 100 languages."])},question6:n=>{const{normalize:e}=n;return e(["What is the difference between GET and POST requests?"])},answer6:n=>{const{normalize:e}=n;return e(["GET requests are solely used to count the number of tokens in a text, allows you easily integrate it as a counter in your application. POST requests supports more parameters and features, such as returning the first/last N tokens."])},question7:n=>{const{normalize:e}=n;return e(["What is the maximum length I can tokenize per request?"])},answer7:n=>{const{normalize:e}=n;return e(["You can send up to 4M characters per request."])},question8:n=>{const{normalize:e}=n;return e(["How does the chunking feature work? Is it semantic chunking?"])},answer8:n=>{const{normalize:e}=n;return e(["The chunking feature segments long documents into smaller chunks based on common structural cues, ensuring accurate segmentation of text into meaningful chunks. Essentially it is a (big!) regex pattern that segments text based on certain syntactical features that often align with semantic boundaries, such as sentence endings, paragraph breaks, punctuation, and certain conjunctions. It is not semantic chunking. This (big) regex is as powerful as it can be within the limitations of regular expressions. It balances complexity and performance. While true semantic understanding isn't possible with regex, it well-approximates context by common structural cues."])},question9:n=>{const{normalize:e,plural:r}=n;return r([e(["How do you handle special tokens such as <"]),e(["endoftext"]),e(["> in the Tokenizer API?"])])},answer9:n=>{const{normalize:e}=n;return e(["If the input contains special tokens, our Tokenizer API will put them in the field 'special_tokens'. This allows you to easily identify them and handle them accordingly for your downstream tasks, e.g. removing them before feeding the text into an LLM to prevent injection attacks."])}},parameters:{learn_more:n=>{const{normalize:e}=n;return e(["Learn more"])},type:n=>{const{normalize:e}=n;return e(["Tokenizer"])},type_explain:n=>{const{normalize:e}=n;return e(["Choose the tokenizer to use."])},used_by_models:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Used in ",r(o("_usedBy")),"."])},auth_token:n=>{const{normalize:e}=n;return e(["Add API Key for Higher Rate Limit"])},auth_token_explain:n=>{const{normalize:e}=n;return e(["Enter your Jina API key to access a higher rate limit. For latest rate limit information, please refer to the table below."])},head:n=>{const{normalize:e}=n;return e(["Return the first N tokens"])},tail:n=>{const{normalize:e}=n;return e(["Return the last N tokens"])},head_explain:n=>{const{normalize:e}=n;return e(["Return the first N tokens of the given content. Boundary exclusive. Can not be used with 'tail'."])},tail_explain:n=>{const{normalize:e}=n;return e(["Return the last N tokens of the given content. Boundary exclusive. Can not be used with 'head'."])},return_tokens:n=>{const{normalize:e}=n;return e(["Return the tokens"])},return_chunks:n=>{const{normalize:e}=n;return e(["Return the chunks"])},return_chunks_explain:n=>{const{normalize:e}=n;return e(["Chunking the input into semantically meaningful segments while handling a wide variety of text types and edge cases based on common structural cues."])},return_tokens_explain:n=>{const{normalize:e}=n;return e(["Return the tokens and their corresponding ids in the response. Toggle to see the result visualization."])}}},paywall:{higher_limit:n=>{const{normalize:e}=n;return e(["Much higher rate limit"])},higher_limit_description:n=>{const{normalize:e}=n;return e(["Get up to 1000 RPM for r.jina.ai and 100 RPM for s.jina.ai"])},priority_support:n=>{const{normalize:e}=n;return e(["Priority customer support"])},priority_support_description:n=>{const{normalize:e}=n;return e(["Guaranteed email response within 24 hours including weekends"])},free_hour_consult:n=>{const{normalize:e}=n;return e(["Free 1-hour consultation"])},free_hour_consult_description:n=>{const{normalize:e}=n;return e(["One hour of free consultation with our product and engineering teams to discuss the best practice for your use case"])}},purchase:{success:n=>{const{normalize:e}=n;return e(["Thank you for your purchase!"])},success_caption:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["We have completed your order at ",r(o("_purchasedTime")),". Your API key is ready to use!"])}},translator:{cta:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Translate to ",r(o("_lang"))," code"])},select_language:n=>{const{normalize:e}=n;return e(["Language"])}},promptperfect:{get_started:n=>{const{normalize:e}=n;return e(["Get started with PromptPerfect"])},features:[{name:n=>{const{normalize:e}=n;return e(["Assistant"])},title:n=>{const{normalize:e}=n;return e(["Daily dose of productivity."])},description:n=>{const{normalize:e}=n;return e(["Easily switch between content generation and prompt optimization, push your content quality to the next level."])}},{name:n=>{const{normalize:e}=n;return e(["Prompt optimization"])},title:n=>{const{normalize:e}=n;return e(["Better inputs, better outputs"])},description:n=>{const{normalize:e}=n;return e(["Don't know how to write an effective instruction? Just put your idea in, one click, get a better instruction."])}},{name:n=>{const{normalize:e}=n;return e(["Compare models"])},title:n=>{const{normalize:e}=n;return e(["Side-by-side model comparison."])},description:n=>{const{normalize:e}=n;return e(["Understand the vibe of every AI model by comparing their output of the same prompt."])}},{name:n=>{const{normalize:e}=n;return e(["Deploy prompts"])},title:n=>{const{normalize:e}=n;return e(["No Ops, just deploy."])},description:n=>{const{normalize:e}=n;return e(["Perhaps the simplest way to deploy your prompts as API for integration."])}},{name:n=>{const{normalize:e}=n;return e(["Multi-agent"])},title:n=>{const{normalize:e}=n;return e(["Explore how agents collaborate"])},description:n=>{const{normalize:e}=n;return e(["Customize your own LLM agents, and start a multi-agent simulation. See how they collaborate or compete in a virtual environment to reach the goal."])}}]},integrations:{embedding:n=>{const{normalize:e}=n;return e(["Embeddings"])},reranker:n=>{const{normalize:e}=n;return e(["Reranker"])},which_to_go:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Which one to integrate with ",r(o("_vendor")),"?"])}},searchbar:{results:n=>{const{normalize:e}=n;return e(["results"])},more_results:n=>{const{normalize:e,interpolate:r,named:o}=n;return e([r(o("_numMore"))," more results"])},placeholder:n=>{const{normalize:e}=n;return e(["Type your question about this page"])},proposing_solution:n=>{const{normalize:e}=n;return e(["Crafting answer from the page content..."])},ask_on_current_page:n=>{const{normalize:e}=n;return e(["Ask the current page about..."])},find_solution:n=>{const{normalize:e}=n;return e(["Generate a solution for..."])},hotkey:n=>{const{normalize:e}=n;return e(["Use / key for quick questions"])},hotkey1:n=>{const{normalize:e}=n;return e(["Use"])},hotkey2:n=>{const{normalize:e}=n;return e(["to toggle"])},hint:n=>{const{normalize:e}=n;return e(["Search over products, news and your questions"])},hotkey_long1:n=>{const{normalize:e}=n;return e(["At any time, press"])},hotkey_long3:n=>{const{normalize:e}=n;return e(["to open search bar"])},required:n=>{const{normalize:e}=n;return e(["Please describe your question with more details."])}},SEO_TAG_LINE:n=>{const{normalize:e}=n;return e(["Your Search Foundation, Supercharged."])},PRODUCT_DESCRIPTION:n=>{const{normalize:e}=n;return e(["We provide best-in-class embeddings, rerankers, LLM-reader and prompt optimizers, pioneering search AI for multimodal data."])},notice:n=>{const{normalize:e}=n;return e(['\u{1F389} Our first book, "Neural Search \u2014 From Prototype to Production with Jina" is officially out today!'])},purchase_now:n=>{const{normalize:e}=n;return e(["Purchase now"])},copy:n=>{const{normalize:e}=n;return e(["Copy"])},copy_to_clipboard_success:n=>{const{normalize:e}=n;return e(["Copied to clipboard"])},powered_by:n=>{const{normalize:e}=n;return e(["Powered by"])},open_day:{title:n=>{const{normalize:e}=n;return e(["Open Day"])},description:n=>{const{normalize:e}=n;return e(["An exclusive opportunity to gain an insider's view of Jina AI."])},organization:n=>{const{normalize:e}=n;return e(["Organization"])},group_size:n=>{const{normalize:e}=n;return e(["Number of visitors"])},organization_website:n=>{const{normalize:e}=n;return e(["Organization website"])},organization_website_placeholder:n=>{const{normalize:e}=n;return e(["URL for your organization's homepage or LinkedIn profile"])},preferred_products:n=>{const{normalize:e}=n;return e(["Which products are you interested in?"])},preferred_date:n=>{const{normalize:e}=n;return e(["Preferred date"])},preferred_language:n=>{const{normalize:e}=n;return e(["Preferred language"])},subtitle:n=>{const{normalize:e}=n;return e(["A Glimpse into the Future of Multimodal AI"])},introduction:n=>{const{normalize:e}=n;return e(["Jina AI is delighted to open our doors to esteemed entities and organizations interested in the progress and future of Artificial Intelligence. We extend this exclusive opportunity for those in politics, NGOs, NPOs, and investment sectors to gain an insider's view of our operations and visions here at our Berlin headquarters."])},vision_title:n=>{const{normalize:e}=n;return e(["Our Vision for the Future"])},vision:n=>{const{normalize:e}=n;return e(["Join us for a comprehensive overview of the AI landscape as we see it. Our discussion will focus on the potential of Large Language Models, multimodal AI, and the impact of open-source technology in shaping the future of global innovation."])},experience:n=>{const{normalize:e}=n;return e(["We've arranged an immersive three-hour tour for our guests, available in German, English, French, Spanish, Chinese, and Russian. The tour covers an in-depth look into our advancements in multimodal AI, our perspective on the AI landscape, followed by a detailed examination of specific projects. We'll conclude with a group discussion to facilitate the exchange of ideas and insights. A lunch option is also available upon request."])},experience_title:n=>{const{normalize:e}=n;return e(["An Insider's Journey"])},impact_title:n=>{const{normalize:e}=n;return e(["Impact and Influence"])},impact:n=>{const{normalize:e}=n;return e(["Understand how our contributions to the open-source community and our work in multimodal AI technology are establishing Jina AI as an influential player in AI innovation. We aim to play a significant role in decision-making processes, ensuring that the advancement of AI technology benefits all."])},engage_title:n=>{const{normalize:e}=n;return e(["Engage with Us"])},engage:n=>{const{normalize:e}=n;return e(["We highly encourage an interactive dialogue throughout the day. The exchange of thoughts and perspectives is invaluable to us. Potential collaborations stemming from these discussions could significantly contribute to a more integrated and innovative future."])},tutor_title:n=>{const{normalize:e}=n;return e(["An Exclusive Deep Dive into"])},tutor_subtitle:n=>{const{normalize:e}=n;return e(["A meticulously curated three-hour tour, bringing you closer to the heart of Jina AI's groundbreaking work in multimodal AI technology."])},one_hour:n=>{const{normalize:e}=n;return e(["1 hour"])},motivation_to_attend_v2:n=>{const{normalize:e}=n;return e(["Why are you interested in our Open Day?"])},motivation_placeholder_v2:n=>{const{normalize:e}=n;return e(["Sharing your motivations will help us improve your experience."])},motivation_min_length_v1:n=>{const{normalize:e}=n;return e(["Please provide a more detailed motivation."])}},internship_faq:{question1:n=>{const{normalize:e}=n;return e(["Who can apply for the Jina AI internship program?"])},answer1:n=>{const{normalize:e}=n;return e(["Undergraduate, Masters, and Ph.D. students from all over the world, with interest in fields such as research, engineering, marketing, and sales, are encouraged to apply. We also welcome non-technical internships in marketing, sales, executive assistance, and more. We are seeking passionate individuals ready to pioneer multimodal AI with us."])},question2:n=>{const{normalize:e}=n;return e(["Where will the internship take place?"])},answer2:n=>{const{normalize:e}=n;return e(["Internships must be carried out onsite at one of our offices, which are located in Berlin, Beijing, and Shenzhen."])},question3:n=>{const{normalize:e}=n;return e(["Does Jina AI assist with visa processes?"])},answer3:n=>{const{normalize:e}=n;return e(["Yes, Jina AI offers reasonable assistance in the visa process for successful applicants."])},question4:n=>{const{normalize:e}=n;return e(["Does Jina AI provide any allowances or benefits for interns?"])},answer4:n=>{const{normalize:e}=n;return e(["Yes, Jina AI provides a reasonable amount of living cost coverage for interns during the internship period."])},question5:n=>{const{normalize:e}=n;return e(["Can I work on my Master's thesis during the internship at Jina AI?"])},answer5:n=>{const{normalize:e}=n;return e(["Yes, it is possible to work on your Master's thesis during your internship at Jina AI, typically applicable to students at German universities. However, you must have prior communication and agreement from your university's supervisor. Note that we do not help students find advisors."])},question6:n=>{const{normalize:e}=n;return e(["What does the application process involve?"])},answer6:n=>{const{normalize:e}=n;return e(["The application process includes submitting your application form, a resume, a cover letter expressing your interest and motivation, and any relevant professional links such as GitHub or LinkedIn. We evaluate candidates based on their performance during the interview and their performance in their university."])},question7:n=>{const{normalize:e}=n;return e(["Does Jina AI provide any letter of recommendation post-internship?"])},answer7:n=>{const{normalize:e}=n;return e(["Yes, successful interns may receive a letter of recommendation at the end of their internship, signed by our CEO."])},question8:n=>{const{normalize:e}=n;return e(["What is the duration of the internship?"])},answer8:n=>{const{normalize:e}=n;return e(["The duration of the internship varies based on the role and project. However, it typically ranges from three to six months."])},question9:n=>{const{normalize:e}=n;return e(["Can I apply if I don't have prior experience in AI?"])},answer9:n=>{const{normalize:e}=n;return e(["Yes, we welcome applications from all academic backgrounds. We value your passion and commitment to learn as much as prior experience."])},question10:n=>{const{normalize:e}=n;return e(["Is this a paid internship?"])},answer10:n=>{const{normalize:e}=n;return e(["Yes, our internship program offers competitive remuneration."])},question11:n=>{const{normalize:e}=n;return e(["What opportunities will I have as a Jina AI intern?"])},answer11:n=>{const{normalize:e}=n;return e(["As a Jina AI intern, you'll get hands-on experience working on challenging projects, learn from industry experts, be part of a vibrant community, and have the opportunity to make real contributions to our pioneering work in multimodal AI."])}},open_day_faq:{question1:n=>{const{normalize:e}=n;return e(["What languages do you offer for the tour?"])},answer1:n=>{const{normalize:e}=n;return e(["We offer tours in German, English, French, Spanish, Chinese, and Russian."])},question2:n=>{const{normalize:e}=n;return e(["What is the duration of the tour?"])},answer2:n=>{const{normalize:e}=n;return e(["The tour typically lasts for approximately three hours."])},question3:n=>{const{normalize:e}=n;return e(["Is lunch provided?"])},answer3:n=>{const{normalize:e}=n;return e(["Lunch is optional and can be arranged upon request."])},question4:n=>{const{normalize:e}=n;return e(["Can individuals register for the Open Day?"])},answer4:n=>{const{normalize:e}=n;return e(["Our Open Day is designed primarily for professional groups, such as politicians, NGOs, NPOs, and investors. However, we occasionally make exceptions based on the individual's profile."])},question5:n=>{const{normalize:e}=n;return e(["How many people can a group consist of for the Open Day?"])},answer5:n=>{const{normalize:e}=n;return e(["We can accommodate a variety of group sizes. Please indicate the size of your group in the registration form, and we will confirm the details with you."])},question6:n=>{const{normalize:e}=n;return e(["How can I specify areas of interest for the tour?"])},answer6:n=>{const{normalize:e}=n;return e(["There's a section in the registration form where you can specify your areas of interest or any special requests. We will do our best to tailor the tour according to your needs."])},question7:n=>{const{normalize:e}=n;return e(["Are tours available at your Beijing or Shenzhen offices?"])},answer7:n=>{const{normalize:e}=n;return e(["At this time, we only offer tours at our Berlin headquarter located in Kreuzberg. Our Beijing and Shenzhen offices are not currently open for tours."])}},header:{logos:n=>{const{normalize:e}=n;return e(["Download logo"])},open_in_full:n=>{const{normalize:e}=n;return e(["Show all enterprise products in a new window"])},products:n=>{const{normalize:e}=n;return e(["Products"])},news:n=>{const{normalize:e}=n;return e(["News"])},open_day:n=>{const{normalize:e}=n;return e(["Open day"])},for_power_users:n=>{const{normalize:e}=n;return e(["For Power Users"])},for_power_users_description:n=>{const{normalize:e}=n;return e(["Utilize our streamlined multimodal tools to enhance your productivity."])},power_users_others:n=>{const{normalize:e}=n;return e(["More power user tools"])},for_developers:n=>{const{normalize:e}=n;return e(["For Developers"])},for_developers_description:n=>{const{normalize:e}=n;return e(["Experience a comprehensive open-source multimodal AI stack designed for developers."])},developers_others:n=>{const{normalize:e}=n;return e(["More developer tools"])},for_enterprise:n=>{const{normalize:e}=n;return e(["For Enterprises"])},for_enterprise_description:n=>{const{normalize:e}=n;return e(["Discover scalable multimodal AI strategies tailored to meet business needs."])},enterprise_others:n=>{const{normalize:e}=n;return e(["More enterprise tools"])},internship1:n=>{const{normalize:e}=n;return e(["Intern program"])},company:n=>{const{normalize:e}=n;return e(["Company"])},about_us:n=>{const{normalize:e}=n;return e(["About us"])},contact_us:n=>{const{normalize:e}=n;return e(["Contact sales"])},jobs:n=>{const{normalize:e}=n;return e(["Join us"])},join_discord:n=>{const{normalize:e}=n;return e(["Join our Discord community"])}},footer:{get_api_key:n=>{const{normalize:e}=n;return e(["Get Jina AI API key"])},power_users:n=>{const{normalize:e}=n;return e(["Power Users"])},developers:n=>{const{normalize:e}=n;return e(["Developers"])},enterprise:n=>{const{normalize:e}=n;return e(["Enterprise"])},address_beijing:n=>{const{normalize:e}=n;return e(["Beijing, China"])},address_shenzhen:n=>{const{normalize:e}=n;return e(["Shenzhen, China"])},address_berlin:n=>{const{normalize:e}=n;return e(["Berlin, Germany (HQ)"])},offices:n=>{const{normalize:e}=n;return e(["Offices"])},docs:n=>{const{normalize:e}=n;return e(["Docs"])},company:n=>{const{normalize:e}=n;return e(["Company"])},all_rights_reserved:n=>{const{normalize:e}=n;return e(["All rights reserved."])},tc:n=>{const{normalize:e}=n;return e(["Terms & Conditions"])},tc1:n=>{const{normalize:e}=n;return e(["Terms"])},privacy:n=>{const{normalize:e}=n;return e(["Privacy"])},status:n=>{const{normalize:e}=n;return e(["API Status"])},privacy_policy:n=>{const{normalize:e}=n;return e(["Privacy Policy"])},privacy_settings:n=>{const{normalize:e}=n;return e(["Manage Cookies"])}},prompt_perfect:{description:n=>{const{normalize:e}=n;return e(["Premier tool for prompt engineering"])},intro:n=>{const{normalize:e}=n;return e(["Premier tool for prompt engineering"])},intro1:n=>{const{normalize:e}=n;return e(["The premier tool for prompt engineering"])},original_title:n=>{const{normalize:e}=n;return e(["Original prompt"])},optimized_title:n=>{const{normalize:e}=n;return e(["Optimized prompt"])},original:n=>{const{normalize:e}=n;return e(["Your role is to be my brainstorming partner."])},optimized:n=>{const{normalize:e}=n;return e(["Your task is to be my brainstorming partner and provide creative ideas and suggestions for a given topic or problem. Your response should include original, unique, and relevant ideas that could help solve the problem or further explore the topic in an interesting way. Please note that your response should also take into account any specific requirements or constraints of the task."])},text_model:n=>{const{normalize:e}=n;return e(["Text models"])},image_model:n=>{const{normalize:e}=n;return e(["Image models"])}},jina_chat:{description:n=>{const{normalize:e}=n;return e(["More modality, longer memory, less cost"])},example_1:n=>{const{normalize:e}=n;return e(["Who are you?"])},example_2:n=>{const{normalize:e}=n;return e(["I'm a LLM chat service made by Jina AI"])}},scenex:{intro1:n=>{const{normalize:e}=n;return e(["Leading AI solution for image captions and video summaries"])},description:n=>{const{normalize:e}=n;return e(["Explore image storytelling beyond pixels"])},example1:n=>{const{normalize:e}=n;return e(["This video appears to be a nature footage featuring a charming white bunny and a butterfly in a grassy field. The bunny is seen interacting with the butterfly in different ways, showcasing their unique relationship. The natural surroundings provide a picturesque backdrop, enhancing the beauty of this simple yet captivating scene."])},caption_image_title:n=>{const{normalize:e}=n;return e(["Caption Image"])},caption_image_desc:n=>{const{normalize:e}=n;return e(["Generate a textual description of the image."])},json_image_title:n=>{const{normalize:e}=n;return e(["Extract JSON from Image"])},json_image_desc:n=>{const{normalize:e}=n;return e(["Generate a structured JSON format from the image using a predefined schema. This allows for specific data extraction from the image."])},visual_q_a_title:n=>{const{normalize:e}=n;return e(["Visual Q&A"])},visual_q_a_desc:n=>{const{normalize:e}=n;return e(["Answer a query based on the image's content."])},summarize_video_title:n=>{const{normalize:e}=n;return e(["Summarize Video"])},summarize_video_desc:n=>{const{normalize:e}=n;return e(["Generate a concise summary of the video, highlighting key events."])},generate_story_title:n=>{const{normalize:e}=n;return e(["Generate Story"])},generate_story_desc:n=>{const{normalize:e}=n;return e(["Craft a story inspired by the image, often featuring dialogues or monologues of its characters."])}},rationale:{description:n=>{const{normalize:e}=n;return e(["Ultimate AI decision-making tools"])},intro:n=>{const{normalize:e}=n;return e(["See two sides of the coin, make rational decisions"])},decision:n=>{const{normalize:e}=n;return e(["Decision"])}},best_banner:{description:n=>{const{normalize:e}=n;return e(["Blog to banner, without the prompts!"])},example_title:n=>{const{normalize:e}=n;return e(["Alice's Adventures in Wonderland - Chapter 1"])},example_description:n=>{const{normalize:e}=n;return e(["Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \u201Cand what is the use of a book,\u201D thought Alice \u201Cwithout pictures or conversations?\u201D So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her."])}},doc_array:{description:n=>{const{normalize:e}=n;return e(["The data structure for multimodal data"])}},jina:{description:n=>{const{normalize:e}=n;return e(["Build multimodal AI applications on the cloud"])}},finetuner:{description:n=>{const{normalize:e}=n;return e(["Fine-tune embeddings on domain specific data for better search quality"])},intro:n=>{const{normalize:e}=n;return e(["On-prem embedding-tuning for your company on your data"])}},hub:{description:n=>{const{normalize:e}=n;return e(["Share and discover building blocks for multimodal AI applications"])}},clip_as_service:{description:n=>{const{normalize:e}=n;return e(["Embed images and sentences into fixed-length vectors with CLIP"])}},dalle_flow:{description:n=>{const{normalize:e}=n;return e(["A human-in-the-Loop workflow for creating HD images from text"])}},disco_art:{description:n=>{const{normalize:e}=n;return e(["Create compelling Disco Diffusion artworks in one line of code"])}},think_gpt:{description:n=>{const{normalize:e}=n;return e(["Agent techniques to augment your LLM and push it beyond its limits"])}},jcloud:{description:n=>{const{normalize:e}=n;return e(["Deploy a local project as a cloud service. Radically easy, no nasty surprises."])}},"dev-gpt":{description:n=>{const{normalize:e}=n;return e(["Your virtual development team"])}},langchain_serve:{description:n=>{const{normalize:e}=n;return e(["Langchain apps on production with Jina & FastAPI"])}},vectordb:{description:n=>{const{normalize:e}=n;return e(["A Python vector database you just need - no more, no less"])}},open_gpt:{description:n=>{const{normalize:e}=n;return e(["An open-source cloud-native of large multimodal models serving framework"])}},jerboa:{description:n=>{const{normalize:e}=n;return e(["An experimental finetuner for open-source LLMs"])}},finetuner_plus:{description:n=>{const{normalize:e}=n;return e(["Empower your enterprise with on-premise finetuning solutions"])}},inference:{description:n=>{const{normalize:e}=n;return e(["State-of-the-art multimodal models available for inference"])}},cloud:{description:n=>{const{normalize:e}=n;return e(["Cloud hosting platform for multimodal AI applications"])}},semantic:{description:n=>{const{normalize:e}=n;return e(["Bridging the semantic gap in your existing search infrastructure"])}},searchscape:{description:n=>{const{normalize:e}=n;return e(["Navigate, interact, refine: reimagine product discovery"])}},reranker:{top_n:n=>{const{normalize:e}=n;return e(["Number of returned documents"])},top_n_explain:n=>{const{normalize:e}=n;return e(["The number of most relevant documents to return for the query."])},example_input_query:n=>{const{normalize:e}=n;return e(["Example query"])},customize_urself:n=>{const{normalize:e}=n;return e(["Change it and see how the response changes!"])},customize_urself_pl:n=>{const{normalize:e}=n;return e(["Change them and see how the response changes!"])},example_input_document:n=>{const{normalize:e}=n;return e(["Example candidate documents to rank"])},v2_features:{title1:n=>{const{normalize:e}=n;return e(["Multilingual Retrieval"])},description1:n=>{const{normalize:e}=n;return e(["Reranker v2 enables document retrieval in over 100 languages, regardless of the query language."])},title2:n=>{const{normalize:e}=n;return e(["Function-Calling & Code Search"])},description2:n=>{const{normalize:e}=n;return e(["Reranker v2 ranks code snippets and function signatures based on natural language queries, ideal for Agentic RAG applications."])},title3:n=>{const{normalize:e}=n;return e(["Tabular and Structured Data Support"])},description3:n=>{const{normalize:e}=n;return e(["Reranker v2 ranks the most relevant tables based on natural language queries, helping to sort different table schemas and identify the most relevant one before generating an SQL query."])}},show_v2benchmark:n=>{const{normalize:e}=n;return e(["Show benchmark for v2 model (latest)"])},read_more_about_v2:n=>{const{normalize:e}=n;return e(["Jina Reranker v2 is the best-in-class reranker released on Jun 25th 2024; it is built for Agentic RAG. It features function-calling support, multilingual retrieval for over 100 languages, code search capabilities, and offers a 6x speedup over v1. Read more about v2 model."])},v2benchmark:{titleMKQA:n=>{const{normalize:e}=n;return e(["MKQA (Multilingual Knowledge Questions and Answers)"])},descMKQA:n=>{const{normalize:e}=n;return e(["Recall 10 scores reported for different reranking models for MKQA dataset"])},titleBeir:n=>{const{normalize:e}=n;return e(["BEIR (Heterogeneous Benchmark on Diverse IR Tasks)"])},descBeir:n=>{const{normalize:e}=n;return e(["NDCG 10 scores reported for different reranking models for Beir dataset"])},titleToolBench:n=>{const{normalize:e}=n;return e(["ToolBench. The benchmark collects over 16 thousand public APIs and corresponding synthetically-generated instructions for using them in single and multi-API settings."])},descToolBench:n=>{const{normalize:e}=n;return e(["Recall 3 scores reported for different reranking models for ToolBench dataset"])},titleNSText2SQL:n=>{const{normalize:e}=n;return e(["NSText2SQL"])},descNSText2SQL:n=>{const{normalize:e}=n;return e(["Recall 3 scores reported for different reranking models for NSText2SQL dataset"])},titleRTX4090:n=>{const{normalize:e}=n;return e(["Throughput of Jina Reranker v2 on RTX4090"])},descRTX4090:n=>{const{normalize:e}=n;return e(["Throughput (documents retrieved in 50ms) scores reported for different reranking models on an RTX 4090 GPU."])},titleCodeSearchNet:n=>{const{normalize:e}=n;return e(["CodeSearchNet. The benchmark is a combination of queries in docstring and natural language formats, with labelled code-segments relevant to the queries."])},descCodeSearchNet:n=>{const{normalize:e}=n;return e(["MRR 10 scores reported for different reranking models for CodeSearchNet dataset"])}},table:{title:n=>{const{normalize:e}=n;return e(["Below is the time cost of reranking one query and 100 documents in milliseconds:"])},number_token_document:n=>{const{normalize:e}=n;return e(["Number of tokens in each document"])},number_token_query:n=>{const{normalize:e}=n;return e(["Number of tokens in the query"])}},faq_v1:{title:n=>{const{normalize:e}=n;return e(["Reranker-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["How much does the Reranker API cost?"])},answer1:n=>{const{normalize:e}=n;return e(["The pricing for the Reranker API is aligned with our Embedding API pricing structure. It begins with 1 million free tokens for each new API key. Beyond the free tokens, different packages are available for purchase. For more details, please visit our pricing section."])},question3:n=>{const{normalize:e}=n;return e(["What is the difference between the two rerankers?"])},answer3:n=>{const{normalize:e}=n;return e(["The primary difference lies in their architecture. For performance, we recommend jina-reranker-v1, which has been extensively tested and benchmarked against competitors. Jina-reranker-v1 utilizes a cross-encoder architecture, while Jina-colbert-v1 is based on the ColBERTv2 architecture but extends the context length of both the query and document to 8192, achieving even better performance than the original ColBERTv2 model."])},question4:n=>{const{normalize:e}=n;return e(["Is Jina Reranker open source?"])},answer4:n=>{const{normalize:e}=n;return e(["Yes, jina-colbert-v1 is open source and can be accessed via Huggingface. However, jina-reranker-v1 is not open source."])},question5:n=>{const{normalize:e}=n;return e(["Does the reranker support multiple languages?"])},answer5:n=>{const{normalize:e}=n;return e(["Currently, it supports English only. However, some users have reported that it also works well with Chinese. This may be partly because jina-reranker-v1-base-en shares some weights with our jina-embeddings-v2-base-zh embedding model."])},question6:n=>{const{normalize:e}=n;return e(["What is the maximum length for queries and documents?"])},answer6:n=>{const{normalize:e}=n;return e(["The maximum query token length is 512. There is no token limit for documents."])},question7:n=>{const{normalize:e}=n;return e(["What is the maximum number of documents I can rerank per query?"])},answer7:n=>{const{normalize:e}=n;return e(["You can rerank up to 2048 documents per query."])},question8:n=>{const{normalize:e}=n;return e(["What is the batch size and how many query-document tuples can I send in one request?"])},answer8:n=>{const{normalize:e}=n;return e(["There is no concept of batch size unlike our Embedding API. You can send only one query-document tuple per request, but the tuple can include up to 2048 candidate documents."])},question9:n=>{const{normalize:e}=n;return e(["What latency can I expect when reranking 100 documents?"])},answer9:n=>{const{normalize:e}=n;return e(["Latency varies from 100 milliseconds to 7 seconds, depending largely on the length of the documents and the query. For instance, reranking 100 documents of 256 tokens each with a 64-token query takes about 150 milliseconds. Increasing the document length to 4096 tokens raises the time to 3.5 seconds. If the query length is increased to 512 tokens, the time further increases to 7 seconds."])},question10:n=>{const{normalize:e}=n;return e(["Can I deploy Jina Reranker on AWS?"])},answer10:n=>{const{normalize:e}=n;return e(["Yes, Jina Reranker can be deployed on AWS. If you require on-premises deployment in an enterprise setting, you can easily do so via our AWS Marketplace offering."])},question11:n=>{const{normalize:e}=n;return e(["Do you offer a fine-tuned reranker on domain-specific data?"])},answer11:n=>{const{normalize:e}=n;return e(["If you are interested in a fine-tuned reranker tailored to specific domain data, please contact our sales team. Our team will respond to your inquiry promptly."])}},title:n=>{const{normalize:e}=n;return e(["Reranker API"])},read_more_about_benchmark:n=>{const{normalize:e}=n;return e(["Read more about the benchmark"])},read_more_about_turbo:n=>{const{normalize:e}=n;return e(["Read more about the turbo and tiny models"])},choose_turbo:n=>{const{normalize:e}=n;return e(["Get up to 5x speedup with reranker-turbo"])},choose_turbo_description:n=>{const{normalize:e}=n;return e(["We also offer two new open-source reranker models: jina-reranker-v1-turbo-en and jina-reranker-v1-tiny-en, the latter has only 30M parameters and four layers. These two new rerankers enjoy 5X faster inference speed than the base model at only a very small cost on the quality. They are perfect for applications that require real-time reranking. Read the benchmark below."])},benchmark_title:n=>{const{normalize:e}=n;return e(["Performance Benchmark"])},try_embedding:n=>{const{normalize:e}=n;return e(["Try embedding API for free"])},try_reranker:n=>{const{normalize:e}=n;return e(["Try reranker API for free"])},benchmark_description:n=>{const{normalize:e}=n;return e(["For comparison, we included three other leading rerankers by BGE (BAAI), BCE (Netease Youdao), and Cohere in the benchmark. As shown by the results below, Jina Reranker holds the highest average score in all relevant categories for reranking, making it a clear leader among its peers."])},benchmark:{title:n=>{const{normalize:e}=n;return e(["Benchmark"])},title0:n=>{const{normalize:e}=n;return e(["LlamaIndex"])},title1:n=>{const{normalize:e}=n;return e(["BEIR"])},title2:n=>{const{normalize:e}=n;return e(["LoCo"])},title3:n=>{const{normalize:e}=n;return e(["MTEB"])},description0:n=>{const{normalize:e}=n;return e(["LlamaIndex assessed various combinations of embeddings and rerankers for RAG, conducting a replication study that measured the Mean Reciprocal Rank. The findings highlight the Jina Reranker's significant enhancement of search quality, a benefit that is independent of the specific embeddings used."])},description1:n=>{const{normalize:e}=n;return e(["BIER (Benchmarking IR) assesses a model's retrieval effectiveness, including relevance and NDCG. A higher BIER score correlates to more accurate matches and search result rankings."])},description2:n=>{const{normalize:e}=n;return e(["Through the LoCo benchmark, we measured a model's understanding of local coherence and context, together with query-specific ranking. A LoCo higher score reflects a better ability to identify and prioritize relevant information."])},description3:n=>{const{normalize:e}=n;return e(["The MTEB (Multilingual Text Embedding Benchmark), on the whole, tests a model\u2019s abilities in text embeddings, including clustering, classification, retrieval, and other metrics. However, for our comparison, we only used the MTEB\u2019s Reranking tasks."])}},vs_table:{title:n=>{const{normalize:e}=n;return e(["Comparison of Reranker, Vector Search, and BM25"])},subtitle:n=>{const{normalize:e}=n;return e(["The table below provides a comprehensive comparison of the Reranker, Vector/Embeddings Search, and BM25, highlighting their strengths and weaknesses across various categories."])},col0:n=>{const{normalize:e}=n;return e(["Reranker"])},col1:n=>{const{normalize:e}=n;return e(["Vector Search"])},col2:n=>{const{normalize:e}=n;return e(["BM25"])},header0:n=>{const{normalize:e}=n;return e(["Best For"])},header1:n=>{const{normalize:e}=n;return e(["Granularity"])},header2:n=>{const{normalize:e}=n;return e(["Query Time Complexity"])},header3:n=>{const{normalize:e}=n;return e(["Indexing Time Complexity"])},header4:n=>{const{normalize:e}=n;return e(["Training Time Complexity"])},header5:n=>{const{normalize:e}=n;return e(["Search Quality"])},header6:n=>{const{normalize:e}=n;return e(["Strengths"])},header7:n=>{const{normalize:e}=n;return e(["Weaknesses"])},col0_2:n=>{const{normalize:e}=n;return e(["Initial, rapid filtering"])},col0_1:n=>{const{normalize:e}=n;return e(["Enhanced search precision and relevance"])},col0_3:n=>{const{normalize:e}=n;return e(["General text retrieval across wide-ranging queries"])},col1_1:n=>{const{normalize:e}=n;return e(["Detailed: Sub-document and query segment"])},col1_2:n=>{const{normalize:e}=n;return e(["Broad: Entire documents"])},col1_3:n=>{const{normalize:e}=n;return e(["Intermediate: Various text segments"])},col2_1:n=>{const{normalize:e}=n;return e(["High"])},col2_2:n=>{const{normalize:e}=n;return e(["Medium"])},col2_3:n=>{const{normalize:e}=n;return e(["Low"])},col3_1:n=>{const{normalize:e}=n;return e(["Not required"])},col3_2:n=>{const{normalize:e}=n;return e(["High"])},col3_3:n=>{const{normalize:e}=n;return e(["Low, utilizes pre-built index"])},col4_1:n=>{const{normalize:e}=n;return e(["High"])},col4_2:n=>{const{normalize:e}=n;return e(["High"])},col4_3:n=>{const{normalize:e}=n;return e(["Not required"])},col5_1:n=>{const{normalize:e}=n;return e(["Superior for nuanced queries"])},col5_2:n=>{const{normalize:e}=n;return e(["Balanced between efficiency and accuracy"])},col5_3:n=>{const{normalize:e}=n;return e(["Consistent and reliable for a broad set of queries"])},col6_1:n=>{const{normalize:e}=n;return e(["Highly accurate with deep contextual understanding"])},col6_2:n=>{const{normalize:e}=n;return e(["Quick and efficient, with moderate accuracy"])},col6_3:n=>{const{normalize:e}=n;return e(["Highly scalable, with established efficacy"])},col7_1:n=>{const{normalize:e}=n;return e(["Resource-intensive with complex implementation"])},col7_2:n=>{const{normalize:e}=n;return e(["May not capture deep query context or nuances"])},col7_3:n=>{const{normalize:e}=n;return e(["May underperform for highly specific or contextual searches"])}},feature_solid_description:n=>{const{normalize:e}=n;return e(["Developed from our cutting-edge academic research and rigorously tested against the SOTA rerankers to ensure unparalleled performance."])},improve_performance:n=>{const{normalize:e}=n;return e(["+33% relevance over vector search"])},improve_performance_description:n=>{const{normalize:e}=n;return e(["Our evaluations show that search systems employing the Jina Reranker enjoy +8% in hit rate and +33% in mean reciprocal rank."])},description_rich:n=>{const{normalize:e}=n;return e(["Maximize the search relevancy and RAG accuracy with our cutting-edge reranker API. Start with 1M free tokens."])},reranker_description:n=>{const{normalize:e}=n;return e(["Try our cutting-edge reranker API to maximize your search relevancy and RAG accuracy. Starting for free!"])},description:n=>{const{normalize:e}=n;return e(["Maximize the search relevancy and RAG accuracy at ease."])},learning1:n=>{const{normalize:e}=n;return e(["Learning about Reranker"])},what_is:n=>{const{normalize:e}=n;return e(["What is a Reranker?"])},how_it_works:n=>{const{normalize:e}=n;return e(["Here's how it works:"])},how_it_works_v1:{title1:n=>{const{normalize:e}=n;return e(["Initial Retrieval"])},description1:n=>{const{normalize:e}=n;return e(["A search system uses embeddings/BM25 to find a broad set of potentially relevant documents based on the user's query."])},title2:n=>{const{normalize:e}=n;return e(["Reranking"])},description2:n=>{const{normalize:e}=n;return e(["The reranker then takes these results and analyzes them at a more granular level, considering the nuances of how the query terms interact with the document content."])},title3:n=>{const{normalize:e}=n;return e(["Improved Results"])},description3:n=>{const{normalize:e}=n;return e(["It reorders the search results, placing the ones it deems most relevant at the top, based on this deeper analysis."])}},what_is_answer_long:n=>{const{normalize:e}=n;return e([`The goal of a search system is to find the most relevant results quickly and efficiently. Traditionally, methods like BM25 or tf-idf have been used to rank search results based on keyword matching. Recent methods, such as embedding-based cosine similarity, have been implemented in many vector databases. These methods are straightforward but can sometimes miss the subtleties of language, and most importantly, the interaction between documents and a query's intent.

This is where the "reranker" shines. A reranker is an advanced AI model that takes the initial set of results from a search\u2014often provided by an embeddings/token-based search\u2014and reevaluates them to ensure they align more closely with the user's intent. It looks beyond the surface-level matching of terms to consider the deeper interaction between the search query and the content of the documents.`])},what_is_answer_long_ending:n=>{const{normalize:e}=n;return e(["The reranker can significantly improve the search quality because it operates at a sub-document and sub-query level, meaning it looks at the individual words and phrases, their meanings, and how they relate to each other within the query and the documents. This results in a more precise and contextually relevant set of search results."])},what_is_desc:n=>{const{normalize:e}=n;return e(["A reranker is an AI model that refines the search results from a vector search or a dense retrieval model. Read more."])},learning1_description:n=>{const{normalize:e}=n;return e(["What is a reranker? Why is vector search or cosine similarity not enough? Learn about rerankers from the ground up with our comprehensive guide."])},feature_on_premises_description2:n=>{const{normalize:e}=n;return e(["Deploy Jina Reranker on AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description3:n=>{const{normalize:e}=n;return e(["Deploy Jina Reranker on AWS Sagemaker and Microsoft Azure and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])}},reader:{check_price_table:n=>{const{normalize:e}=n;return e(["Check the price table"])},want_higher_rate_limit:n=>{const{normalize:e}=n;return e(["Are you already a paid API user but still want a higher rate limit of up to 1000 RPM? We can support you!"])},rate_limit:n=>{const{normalize:e}=n;return e(["Rate limit"])},table:{th0:n=>{const{normalize:e}=n;return e(["Endpoint"])},th1:n=>{const{normalize:e}=n;return e(["Description"])},th2:n=>{const{normalize:e}=n;return e(["Rate limit w/o API key"])},th3:n=>{const{normalize:e}=n;return e(["Rate limit with API key"])},th4:n=>{const{normalize:e}=n;return e(["Rate limit with API key and premium plan"])},th5:n=>{const{normalize:e}=n;return e(["Token counting scheme"])},th6:n=>{const{normalize:e}=n;return e(["Average latency"])},td_1_0:n=>{const{normalize:e}=n;return e(["Read a URL return its content, useful for check grounding"])},td_1_1:n=>{const{normalize:e}=n;return e(["20 RPM"])},td_1_2:n=>{const{normalize:e}=n;return e(["200 RPM"])},td_1_3:n=>{const{normalize:e}=n;return e(["1000 RPM"])},td_1_4:n=>{const{normalize:e}=n;return e(["Based on the output tokens"])},td_1_5:n=>{const{normalize:e}=n;return e(["3 seconds"])},td_2_0:n=>{const{normalize:e}=n;return e(["Search on the web return top-5 results, useful for search grounding"])},td_2_1:n=>{const{normalize:e}=n;return e(["5 RPM"])},td_2_2:n=>{const{normalize:e}=n;return e(["40 RPM"])},td_2_3:n=>{const{normalize:e}=n;return e(["100 RPM"])},td_2_4:n=>{const{normalize:e}=n;return e(["Based on the output tokens for all 5 search results"])},td_2_5:n=>{const{normalize:e}=n;return e(["10 seconds"])}},reader_do_search:n=>{const{normalize:e}=n;return e(["Reader for search grounding"])},reader_do_pdf_explain:n=>{const{normalize:e}=n;return e(["Yes, Reader natively supports PDF reading. It's compatible with most PDFs, including those with many images, and it's lightning fast! Combined with an LLM, you can easily build a ChatPDF or document analysis AI in no time."])},reader_do_search_explain:n=>{const{normalize:e}=n;return e(["LLMs have a knowledge cut-off, meaning they can't access the latest world knowledge. This leads to problems such as misinformation, outdated responses, hallucinations, and other factuality issues. Grounding is absolutely essential for GenAI applications. Reader allows you to ground your LLM with the latest information from the web. Simply prepend <code>https://s.jina.ai/</code> to your query, and Reader will search the web and return the top five results with their URLs and contents, each in clean, LLM-friendly text. This way, you can always keep your LLM up-to-date, improve its factuality, and reduce hallucinations."])},is_free:n=>{const{normalize:e}=n;return e(["The best part? It's free!"])},is_free_description:n=>{const{normalize:e}=n;return e(["Reader API is available for free and offers flexible rate limit and pricing. Built on a scalable infrastructure, it offers high accessibility, concurrency, and reliability. We strive to be your preferred grounding solution for your LLMs."])},dont_panic_api_key_is_free:n=>{const{normalize:e}=n;return e(["Don't panic! Every new API key contains one million free tokens!"])},reader_reads_images:n=>{const{normalize:e}=n;return e(["Reader also reads images!"])},reader_reads_pdf:n=>{const{normalize:e}=n;return e(["Reader also reads PDFs!"])},original_pdf:n=>{const{normalize:e}=n;return e(["Original PDF"])},reader_result:n=>{const{normalize:e}=n;return e(["Reader Result"])},reader_also_read_images:n=>{const{normalize:e}=n;return e(["Images on the webpage are automatically captioned using a vision language model in the reader and formatted as image alt tags in the output. This gives your downstream LLM just enough hints to incorporate those images into its reasoning and summarizing processes. This means you can ask questions about the images, select specific ones, or even forward their URLs to a more powerful VLM for deeper analysis!"])},description:n=>{const{normalize:e}=n;return e(["Read URLs or search the web, get better grounding for LLMs."])},reader_description:n=>{const{normalize:e}=n;return e(["Get LLM-friendly input from a URL or a web search, by simply adding <code>r.jina.ai</code> in front."])},what_is1:n=>{const{normalize:e}=n;return e(["What is Reader?"])},what_is_desc:n=>{const{normalize:e}=n;return e(["A proxy that accesses any URL and transforms the main content into plain text optimized for LLMs."])},open:n=>{const{normalize:e}=n;return e(["Open in new tab"])},copy:n=>{const{normalize:e}=n;return e(["Copy"])},title:n=>{const{normalize:e}=n;return e(["Reader API"])},usage:n=>{const{normalize:e}=n;return e(["Usage"])},better_input:n=>{const{normalize:e}=n;return e(["Enhance input quality right from the start"])},fast_stream:n=>{const{normalize:e}=n;return e(["Immediate data streaming"])},free:n=>{const{normalize:e}=n;return e(["Free forever"])},fast:n=>{const{normalize:e}=n;return e(["Fast"])},faq_v1:{title:n=>{const{normalize:e}=n;return e(["Reader-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["What are the costs associated with using the Reader API?"])},answer1:n=>{const{normalize:e}=n;return e(["The Reader API is free of charge and does not require an API key. Simply prepend 'https://r.jina.ai/' to your URL."])},question2:n=>{const{normalize:e}=n;return e(["How does the Reader API function?"])},answer2:n=>{const{normalize:e}=n;return e(["The Reader API uses a proxy to fetch any URL, rendering its content in a browser to extract high-quality main content."])},question3:n=>{const{normalize:e}=n;return e(["Is the Reader API open source?"])},answer3:n=>{const{normalize:e}=n;return e(["Yes, the Reader API is open source and available on the Jina AI GitHub repository."])},question4:n=>{const{normalize:e}=n;return e(["What is the typical latency for the Reader API?"])},answer4:n=>{const{normalize:e}=n;return e(["The Reader API generally processes URLs and returns content within 2 seconds, although complex or dynamic pages might require more time."])},question5:n=>{const{normalize:e}=n;return e(["Why should I use the Reader API instead of scraping the page myself?"])},answer5:n=>{const{normalize:e}=n;return e(["Scraping can be complicated and unreliable, particularly with complex or dynamic pages. The Reader API provides a streamlined, reliable output of clean, LLM-ready text."])},question6:n=>{const{normalize:e}=n;return e(["Does the Reader API support multiple languages?"])},answer6:n=>{const{normalize:e}=n;return e(["The Reader API returns content in the original language of the URL. It does not provide translation services."])},question7:n=>{const{normalize:e}=n;return e(["What should I do if a website blocks the Reader API?"])},answer7:n=>{const{normalize:e}=n;return e(["If you experience blocking issues, please contact our support team for assistance and resolution."])},question8:n=>{const{normalize:e}=n;return e(["Can the Reader API extract content from PDF files?"])},answer8:n=>{const{normalize:e}=n;return e(["Yes, the Reader API can natively extract content from PDF files."])},question9:n=>{const{normalize:e}=n;return e(["Can the Reader API process media content from web pages?"])},answer9:n=>{const{normalize:e}=n;return e(["Currently, the Reader API does not process media content, but future enhancements will include image captioning and video summarization."])},question10:n=>{const{normalize:e}=n;return e(["Is it possible to use the Reader API on local HTML files?"])},answer10:n=>{const{normalize:e}=n;return e(["No, the Reader API can only process content from publicly accessible URLs."])},question11:n=>{const{normalize:e}=n;return e(["Does Reader API cache the content?"])},answer11:n=>{const{normalize:e}=n;return e(["If you request the same URL within 5 minutes, the Reader API will return the cached content."])},question12:n=>{const{normalize:e}=n;return e(["Can I use the Reader API to access content behind a login?"])},answer12:n=>{const{normalize:e}=n;return e(["Unfortunately not."])},question13:n=>{const{normalize:e}=n;return e(["Can I use the Reader API to access PDF on arXiv?"])},answer13:n=>{const{normalize:e}=n;return e(["Yes, you can either use the native PDF support from the Reader (https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4) or use the HTML version from the arXiv (https://r.jina.ai/https://arxiv.org/html/2310.19923v4)"])},question14:n=>{const{normalize:e}=n;return e(["How does image caption work in Reader?"])},answer14:n=>{const{normalize:e}=n;return e(["Reader captions all images at the specified URL and adds `Image [idx]: [caption]` as an alt tag (if they initially lack one). This enables downstream LLMs to interact with the images in reasoning, summarizing etc."])},question15:n=>{const{normalize:e}=n;return e(["What is the scalability of the Reader? Can I use it in production?"])},answer15:n=>{const{normalize:e}=n;return e(["The Reader API is designed to be highly scalable. It is auto-scaled based on the real-time traffic and the maximum concurrency requests is now around 4000. We are maintaining it actively as one of the core products of Jina AI. So feel free to use it in production."])},question16:n=>{const{normalize:e}=n;return e(["What is the rate limit of the Reader API?"])},answer16:n=>{const{normalize:e}=n;return e(["Please find the latest rate limit information in the table below. Note that we are actively working on improving the rate limit and performance of the Reader API, the table will be updated accordingly."])}},demo:{search_query_rewrite:n=>{const{normalize:e}=n;return e(["Please note that unlike the demo shown above, in practice you do not search the original question on the web for grounding. What people often do is rewrite the original question or use multi-hop questions. They read the retrieved results and then generate additional queries to gather more information as needed before arriving at a final answer."])},raw_html:n=>{const{normalize:e}=n;return e(["Raw HTML"])},waiting_for_reader:n=>{const{normalize:e}=n;return e(["Waiting for the Reader API result first..."])},basic_usage:n=>{const{normalize:e}=n;return e(["Basic Usage"])},show_read_demo:n=>{const{normalize:e}=n;return e(["See how Reader reads a URL"])},show_search_demo:n=>{const{normalize:e}=n;return e(["See how Reader searches the web"])},basic_usage1:n=>{const{normalize:e}=n;return e(["Read a URL"])},basic_usage2:n=>{const{normalize:e}=n;return e(["Search a query"])},reader_output:n=>{const{normalize:e}=n;return e(["Reader Output"])},try_demo:n=>{const{normalize:e}=n;return e(["Demo"])},ask_llm:n=>{const{normalize:e}=n;return e(["Ask LLM w/o & w/ Search Grounding"])},use_headers:n=>{const{normalize:e}=n;return e(["The behavior of the Reader API can be controlled with request headers. Here is a complete list of supported headers."])},standard_usage:n=>{const{normalize:e}=n;return e(["Standard Usage"])},stream_mode:n=>{const{normalize:e}=n;return e(["Stream Mode"])},stream_mode_explain1:n=>{const{normalize:e}=n;return e(["Streaming mode is useful when you find that the standard mode provides an incomplete result. This is because streaming mode will wait a bit longer until the page is fully rendered. Use the accept-header to toggle the streaming mode:"])},stream_mode_explain:n=>{const{normalize:e}=n;return e(["Stream mode is useful when the target page is large to render. If you find standard mode gives you incomplete content, try stream mode."])},how_to_stream:n=>{const{normalize:e}=n;return e(["To process content as it becomes available, set the request header to stream mode. This minimizes the time until the first byte is received. Example in curl:"])},how_to_use1:n=>{const{normalize:e}=n;return e(["Add <code>https://r.jina.ai/</code> to any URL in your code or tool where LLM access is expected. This will return the main content of the page in clean, LLM-friendly text."])},how_to_use2:n=>{const{normalize:e}=n;return e(["Add <code>https://s.jina.ai/</code> to your query. This will call the search engine and returns top-5 results with their URLs and contents, each in clean, LLM-friendly text."])},tagline:n=>{const{normalize:e}=n;return e(["Try the demo"])},your_url:n=>{const{normalize:e}=n;return e(["Enter your URL"])},advanced_usage:n=>{const{normalize:e}=n;return e(["Advanced Usage"])},your_query:n=>{const{normalize:e}=n;return e(["Enter your query"])},your_query_hint:n=>{const{normalize:e}=n;return e(["Type a question that requires latest information or world knowledge."])},reader_search_hint:n=>{const{normalize:e}=n;return e(["If you use this URL in code, dont forget to encode the URL."])},ask_llm_directly:n=>{const{normalize:e}=n;return e(["Ask LLM directly"])},ask_llm_with_search_grounding:n=>{const{normalize:e}=n;return e(["Ask LLM with search grounding"])},reader_response:n=>{const{normalize:e}=n;return e(["Reader's response"])},reader_url:n=>{const{normalize:e}=n;return e(["Reader URL"])},fetch:n=>{const{normalize:e}=n;return e(["Fetch Content"])},copy:n=>{const{normalize:e}=n;return e(["Copy"])},ask_question:n=>{const{normalize:e}=n;return e(["Pose a Question"])},your_url_hint:n=>{const{normalize:e}=n;return e(["Click below to fetch the source code of the page directly"])},reader_url_hint:n=>{const{normalize:e}=n;return e(["Click below to obtain the content through our Reader API"])},ask_question_hint:n=>{const{normalize:e}=n;return e(["Input a question and combine it with the fetched content for LLM to generate an answer"])},get_response:n=>{const{normalize:e}=n;return e(["Get response"])},headers:{site_selector:n=>{const{normalize:e}=n;return e(["In-site Search"])},site_selector_explain:n=>{const{normalize:e}=n;return e(["Returns the search results only from the specified website or domain. By default it searches the entire web."])},post_with_url:n=>{const{normalize:e}=n;return e(["Use POST Method"])},post_with_url_explain:n=>{const{normalize:e}=n;return e(["Use POST instead of GET method with a URL passed in the body. Useful for building SPAs with hash-based routing."])},links_summary:n=>{const{normalize:e}=n;return e(["Gather All Links At the End"])},images_summary:n=>{const{normalize:e}=n;return e(["Gather All Images At the End"])},links_summary_explain:n=>{const{normalize:e}=n;return e(['A "Buttons & Links" section will be created at the end. This helps the downstream LLMs or web agents navigating the page or take further actions.'])},images_summary_explain:n=>{const{normalize:e}=n;return e(['An "Images" section will be created at the end. This gives the downstream LLMs an overview of all visuals on the page, which may improve reasoning.'])},mode:n=>{const{normalize:e}=n;return e(["Read or Search Mode"])},mode_explain:n=>{const{normalize:e}=n;return e(["Read mode is for accessing the content of a URL, while Search mode allows you to search a query on the web, applying Read mode to each search result URL."])},auth_token:n=>{const{normalize:e}=n;return e(["Add API Key for Higher Rate Limit"])},auth_token_explain:n=>{const{normalize:e}=n;return e(["Enter your Jina API key to access a higher rate limit. For latest rate limit information, please refer to the table below."])},image_caption:n=>{const{normalize:e}=n;return e(["Image Caption"])},image_caption_explain:n=>{const{normalize:e}=n;return e(["Captions all images at the specified URL, adding 'Image [idx]: [caption]' as an alt tag for those without one. This allows downstream LLMs to interact with the images in activities such as reasoning and summarizing."])},json_response:n=>{const{normalize:e}=n;return e(["JSON Response"])},json_response_explain:n=>{const{normalize:e}=n;return e(["The response will be in JSON format, containing the URL, title, content, and timestamp (if available). In Search mode, it returns a list of five entries, each following the described JSON structure."])},set_cookie:n=>{const{normalize:e}=n;return e(["Forward Cookie"])},set_cookie_explain:n=>{const{normalize:e}=n;return e(["Our API server can forward your custom cookie settings when accessing the URL, which is useful for pages requiring extra authentication. Note that requests with cookies will not be cached."])},proxy_server:n=>{const{normalize:e}=n;return e(["Use a Proxy Server"])},proxy_server_explain:n=>{const{normalize:e}=n;return e(["Our API server can utilize your proxy to access URLs, which is helpful for pages accessible only through specific proxies."])},no_cache:n=>{const{normalize:e}=n;return e(["Bypass the Cache"])},no_cache_explain:n=>{const{normalize:e}=n;return e(["Our API server caches both Read and Search mode contents for a certain amount of time. To bypass this cache, set this header to true."])},stream_mode:n=>{const{normalize:e}=n;return e(["Stream Mode"])},stream_mode_explain:n=>{const{normalize:e}=n;return e(["Stream mode is beneficial for large target pages, allowing more time for the page to fully render. If standard mode results in incomplete content, consider using Stream mode."])},return_format:n=>{const{normalize:e}=n;return e(["Content Format"])},return_format_explain:n=>{const{normalize:e}=n;return e(["You can control the level of detail in the response to prevent over-filtering. The default pipeline is optimized for most websites and LLM input."])},target_selector:n=>{const{normalize:e}=n;return e(["Target Selector"])},target_selector_explain:n=>{const{normalize:e}=n;return e(["Provide a CSS selector to focus on a more specific part of the page. Useful when your desired content doesn't show under the default settings."])},x_timeout:n=>{const{normalize:e}=n;return e(["Custom Timeout"])},x_timeout_explain:n=>{const{normalize:e}=n;return e(["Can be useful when the page is too slow to render. For the search endpoint, it's the maximum time to wait for reading all search results."])},wait_for_selector:n=>{const{normalize:e}=n;return e(["Wait For Selector"])},wait_for_selector_explain:n=>{const{normalize:e}=n;return e(["Wait for a specific element to appear before returning. Useful when your desired content doesn't show under the default settings."])},default:n=>{const{normalize:e}=n;return e(["Default"])},default_explain:n=>{const{normalize:e}=n;return e(["The default pipeline optimized for most websites and LLM input."])},markdown:n=>{const{normalize:e}=n;return e(["Markdown"])},markdown_explain:n=>{const{normalize:e}=n;return e(["Returns the markdown directly from the HTML, bypassing the readability filtering."])},html:n=>{const{normalize:e}=n;return e(["HTML"])},html_explain:n=>{const{normalize:e}=n;return e(["Returns documentElement.outerHTML."])},text:n=>{const{normalize:e}=n;return e(["Text"])},text_explain:n=>{const{normalize:e}=n;return e(["Returns document.body.innerText."])},screenshot:n=>{const{normalize:e}=n;return e(["Screenshot"])},pageshot:n=>{const{normalize:e}=n;return e(["Pageshot"])},pageshot_explain:n=>{const{normalize:e}=n;return e(["Returns the image URL of full page screenshot (with best effort)."])},screenshot_explain:n=>{const{normalize:e}=n;return e(["Returns the image URL of the first screen."])}},learn_more:n=>{const{normalize:e}=n;return e(["Learn more"])},open:n=>{const{normalize:e}=n;return e(["Open in a new tab"])}},usage_details_null:n=>{const{normalize:e}=n;return e(["Show basic and advanced usages"])},usage_details_true:n=>{const{normalize:e}=n;return e(["Show advanced usages only"])},usage_details_false:n=>{const{normalize:e}=n;return e(["Show basic usages only"])},what_is_answer_long:n=>{const{normalize:e}=n;return e(["Feeding web information into LLMs is an important step of grounding, yet it can be challenging. The simplest method is to scrape the webpage and feed the raw HTML. However, scraping can be complex and often blocked, and raw HTML is cluttered with extraneous elements like markups and scripts. The Reader API addresses these issues by extracting the core content from a URL and converting it into clean, LLM-friendly text, ensuring high-quality input for your agent and RAG systems."])},free_description:n=>{const{normalize:e}=n;return e(["Reader API is free! It requires no credit card or API secret. It will not consume your token quota."])},fast_stream_description:n=>{const{normalize:e}=n;return e(["Need data quickly? Our Reader API can stream data to minimize latency."])},better_input_description:n=>{const{normalize:e}=n;return e(["Experiencing issues with your agent or RAG system output? It might be due to poor input quality."])}},autotune:{embedding_provider:n=>{const{normalize:e}=n;return e(["Select a base embedding model"])},temporarily_unavailable:n=>{const{normalize:e}=n;return e(["Temporarily unavailable. We are upgrading our auto fine-tune system to serve you better. Please check back later."])},base_model:n=>{const{normalize:e}=n;return e(["Base model for fine-tuning"])},does_it_work_tho:n=>{const{normalize:e}=n;return e(["But does it work though?"])},does_it_work_tho_explain:n=>{const{normalize:e}=n;return e(["Auto fine-tuning holds an auto-magical promise to deliver fine-tuned embeddings for any domain you want. But does it really work? This is a fairly reasonable doubt. We've tested it on a variety of domains and base models to find out. Check out the cherry-picked and lemon-picked results below."])},domain_instruction:n=>{const{normalize:e}=n;return e(["Domain instruction"])},eval_performance_before_after:n=>{const{normalize:e}=n;return e(["Performance on synthetic validation set before and after fine-tuning"])},eval_ndcg:n=>{const{normalize:e}=n;return e(["NDCG"])},total_improve:n=>{const{normalize:e}=n;return e(["Avg. improvement"])},eval_mrr:n=>{const{normalize:e}=n;return e(["MRR"])},eval_map:n=>{const{normalize:e}=n;return e(["MAP"])},test_performance_before_after:n=>{const{normalize:e}=n;return e(["Performance on held-out test set before and after fine-tuning"])},test_on:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Tested on ",r(o("_dataSize"))," random samples from ",r(o("_dataName"))])},data_size:n=>{const{normalize:e}=n;return e(["Synthetic data generated"])},eval_syntheticDataSize:n=>{const{normalize:e}=n;return e(["Total"])},eval_training:n=>{const{normalize:e}=n;return e(["Training"])},eval_evaluation:n=>{const{normalize:e}=n;return e(["Validation"])},eval_test:n=>{const{normalize:e}=n;return e(["Real data for testing"])},check_data:n=>{const{normalize:e}=n;return e(["Download synthetic data"])},check_model:n=>{const{normalize:e}=n;return e(["Download fine-tuned model"])},usage:n=>{const{normalize:e}=n;return e(["Usage"])},find_on_hf:n=>{const{normalize:e}=n;return e(["List fine-tuned models"])},title:n=>{const{normalize:e}=n;return e(["Auto Fine-Tuning API"])},what_is:n=>{const{normalize:e}=n;return e(["What is Auto Fine-Tuning?"])},description:n=>{const{normalize:e}=n;return e(["Get fine-tuned embeddings for any domain you want."])},description_long:n=>{const{normalize:e}=n;return e(["Just tell us which domain you want your embeddings to excel in, and we automatically deliver a ready-to-use, fine-tuned embedding model for that domain."])},what_is_answer_long:n=>{const{normalize:e}=n;return e(["Fine-tuning allows you to take a pre-trained model and adapt it to a specific task or domain by training it on a new dataset. In practice, finding effective training data is not straightforward for many users. Effective training requires more than just throwing raw PDFs, HTMLs into the model; and it is hard to get it right. Auto fine-tuning solves this problem by automatically generating effective training data using an advanced LLM agent pipeline; and fine-tuning the model within a ML workflow. You can think it as a combination of synthetic data generation and AutoML, so all you need to do is describe your target domain in natural language and let our system do the rest."])},faq_v1:{title:n=>{const{normalize:e}=n;return e(["Auto Fine-Tuning-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["How much does the Fine-tuning API cost?"])},answer1:n=>{const{normalize:e}=n;return e(["The feature is currently in beta and costs 1M tokens per fine-tuned model. You can use your existing API key from the Embedding/Reranker API if it has sufficient tokens, or you can create a new API key, which includes 1M free tokens."])},question2:n=>{const{normalize:e}=n;return e(["What do I need to input? Do I need to provide training data?"])},answer2:n=>{const{normalize:e}=n;return e(["You don't need to provide any training data. Simply describe your target domain (the domain for which you want the fine-tuned embeddings to be optimized) in natural language, or use a URL as a reference, and our system will generate synthetic data to train the model."])},question3:n=>{const{normalize:e}=n;return e(["How long does it take to fine-tune a model?"])},answer3:n=>{const{normalize:e}=n;return e(["About 30 minutes."])},question4:n=>{const{normalize:e}=n;return e(["Where are the fine-tuned models stored?"])},answer4:n=>{const{normalize:e}=n;return e(["The fine-tuned models and synthetic data are stored publicly in the Hugging Face model hub."])},question5:n=>{const{normalize:e}=n;return e(["If I provide a reference URL, how does the system use it?"])},answer5:n=>{const{normalize:e}=n;return e(["The system uses the Reader API to fetch the content from the URL. It then analyzes the content to summarize the tone and domain, which it uses as guidelines for generating synthetic data. Therefore, the URL should be publicly accessible and representative of the target domain."])},question6:n=>{const{normalize:e}=n;return e(["Can I fine-tune a model for a specific language?"])},answer6:n=>{const{normalize:e}=n;return e(["Yes, you can fine-tune a model for a non-English language. The system automatically detects the language of your domain instructions and generates synthetic data accordingly. We also recommend choosing the appropriate base model for the target language. For example, if targeting a German domain, you should select the 'jina-embeddings-v2-base-de' as the base model."])},question7:n=>{const{normalize:e}=n;return e(["Can I fine-tune non-Jina embeddings, e.g., bge-M3?"])},answer7:n=>{const{normalize:e}=n;return e(["No, our fine-tuning API only supports Jina v2 models."])},question8:n=>{const{normalize:e}=n;return e(["How do you ensure the quality of the fine-tuned models?"])},answer8:n=>{const{normalize:e}=n;return e(["At the end of the fine-tuning process, the system evaluates the model using a held-out test set and reports performance metrics. You will receive an email detailing the before/after performance on this test set. You are also encouraged to evaluate the model on your own test set to ensure its quality."])},question9:n=>{const{normalize:e}=n;return e(["How do you generate synthetic data?"])},answer9:n=>{const{normalize:e}=n;return e(["The system generates synthetic data by integrating the target domain instruction you provide with LLM agents' reasoning. It produces hard negative triplets, which are essential for training high-quality embedding models. For more details, please refer to our upcoming research paper on Arxiv."])},question10:n=>{const{normalize:e}=n;return e(["Can I keep my fine-tuned models and synthetic data private?"])},answer10:n=>{const{normalize:e}=n;return e(["Currently, no. Note that this feature is still in beta. Storing the fine-tuned models and synthetic data publicly in the Hugging Face model hub helps us and the community evaluate the quality of the training. In the future, we plan to offer a private storage option."])},question11:n=>{const{normalize:e}=n;return e(["How can I use the fine-tuned model?"])},answer11:n=>{const{normalize:e}=n;return e(["Since all fine-tuned models are uploaded to Hugging Face, you can access them via SentenceTransformers by simply specifying the model name."])},question12:n=>{const{normalize:e}=n;return e(["I never received the email with the evaluation results. What should I do?"])},answer12:n=>{const{normalize:e}=n;return e(["Please check your spam folder. If you still can't find it, please contact our support team using the email address you provided."])}}},landing_page:{more:n=>{const{normalize:e}=n;return e(["More"])},tokenizer:n=>{const{normalize:e}=n;return e(["Tokenizer"])},for_enterprise:n=>{const{normalize:e}=n;return e(["For Enterprise"])},for_power_users:n=>{const{normalize:e}=n;return e(["For Power Users"])},for_developers:n=>{const{normalize:e}=n;return e(["For Developers"])},badge:{v2:n=>{const{normalize:e}=n;return e(["v2 release!"])}},autotune:n=>{const{normalize:e}=n;return e(["Auto Fine-Tuning"])},get_started:n=>{const{normalize:e}=n;return e(["Get started"])},source_code:n=>{const{normalize:e}=n;return e(["Source code"])},reader:n=>{const{normalize:e}=n;return e(["Reader"])},podcast:n=>{const{normalize:e}=n;return e(["Podcast"])},get_api_now:n=>{const{normalize:e}=n;return e(["API"])},pricing:n=>{const{normalize:e}=n;return e(["Pricing"])},your_search_foundation1:n=>{const{normalize:e}=n;return e(["Your Search Foundation"])},search_foundation:n=>{const{normalize:e}=n;return e(["Search Foundation"])},supercharged1:n=>{const{normalize:e}=n;return e(["Supercharged."])},learn_more_embeddings:n=>{const{normalize:e}=n;return e(["Learn more about embeddings"])},learn_more_reranker:n=>{const{normalize:e}=n;return e(["Learn more about reranker"])},learn_more_reader:n=>{const{normalize:e}=n;return e(["Learn more about reader"])},try_it_for_free:n=>{const{normalize:e}=n;return e(["Try it for free, no credit card required"])},reranker:n=>{const{normalize:e}=n;return e(["Reranker"])},finding_faq:n=>{const{normalize:e}=n;return e(["Generating answer based on the FAQ knowledge below"])},embeddings:n=>{const{normalize:e}=n;return e(["Embeddings"])},new:n=>{const{normalize:e}=n;return e(["New"])},"on-premises":n=>{const{normalize:e}=n;return e(["On-premises"])},"on-prem-deploy":n=>{const{normalize:e}=n;return e(["On-premises deployment"])},also_available_on1:n=>{const{normalize:e}=n;return e(["Available on the marketplaces of your enterprise cloud"])},coming_soon:n=>{const{normalize:e}=n;return e(["Coming soon"])},try_our_saas:n=>{const{normalize:e}=n;return e(["Try our hosted solution, a drop-in replacement for OpenAI's embedding API."])},also_available_on:n=>{const{normalize:e}=n;return e(["Available on the marketplaces"])},our_publications:n=>{const{normalize:e}=n;return e(["Our Publications"])},embedding_desc1:n=>{const{normalize:e}=n;return e(["Multimodal, bilingual long-context embeddings for your search and RAG."])},enterprise_desc_v2:n=>{const{normalize:e}=n;return e(["Try our world-class embedding models to improve your search and RAG systems. Start with a free trial!"])},enterprise_desc_v3:n=>{const{normalize:e}=n;return e(["We develop cutting-edge search foundation for high-quality enterprise search and RAG solutions. Start with a free trial!"])},researcher_desc:n=>{const{normalize:e}=n;return e(["Understand how our search foundation were trained from scratch, check out our latest publications. Meet our team at EMNLP, SIGIR, ICLR, NeurIPS, and ICML!"])},include_experiment:n=>{const{normalize:e}=n;return e(["Includes our experimental and archived projects in the solution."])},your_portal_to:n=>{const{normalize:e}=n;return e(["Your Portal to"])},copy:n=>{const{normalize:e}=n;return e(["Copy"])},require_full_question:n=>{const{normalize:e}=n;return e(["Please describe your problem with more details."])},proposing_solution:n=>{const{normalize:e}=n;return e(["Proposing a solution based on Jina AI products..."])},mentioned_products:n=>{const{normalize:e}=n;return e(["Mentioned products:"])},copied_to_clipboard:n=>{const{normalize:e}=n;return e(["Copied to clipboard"])},checkout_our_solution_for_you:n=>{const{normalize:e}=n;return e(["Find out our solution tailored for you"])},ask_how_your_question:n=>{const{normalize:e}=n;return e(["Please describe your problem"])},how_to:n=>{const{normalize:e}=n;return e(["How to"])},powered_by_promptperfect:n=>{const{normalize:e}=n;return e([`Powered by PromptPerfect's "Prompt optimization" and "Prompt as a service" feature`])},error:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["There was a network issue: ",r(o("message"))])},find_your_portal:n=>{const{normalize:e}=n;return e(["Find Your Portal"])},opensource:n=>{const{normalize:e}=n;return e(["Open Source"])},mmstack:n=>{const{normalize:e}=n;return e(["Multimodal Stack"])},mmstack_desc:n=>{const{normalize:e}=n;return e(["Over the years, we've developed a variety of open-source software to help developers build better GenAI and search applications faster."])},llm:n=>{const{normalize:e}=n;return e(["LLM embedding models"])},llm_desc:n=>{const{normalize:e}=n;return e(["We provide a collection of high-performance sentence embedding models, boasting between 35 million to 6 billion parameters. They're excellent for enhancing neural search, reranking, sentence similarity, recommendations, etc. Get ready to elevate your AI experience!"])},parameters:n=>{const{normalize:e}=n;return e(["Parameters"])},download_pdf:n=>{const{normalize:e}=n;return e(["Download PDF"])},multimodal_ai:n=>{const{normalize:e}=n;return e(["Multimodal AI"])},multimodal:n=>{const{normalize:e}=n;return e(["Multimodal"])},contact_sales:n=>{const{normalize:e}=n;return e(["Contact"])},join_community:n=>{const{normalize:e}=n;return e(["Community"])},trusted_by:n=>{const{normalize:e}=n;return e(["TRUSTED BY"])},newsroom:n=>{const{normalize:e}=n;return e(["Newsroom"])},read_more:n=>{const{normalize:e}=n;return e(["Read more"])},for:n=>{const{normalize:e}=n;return e(["For"])},power_users:n=>{const{normalize:e}=n;return e(["Power Users"])},researchers:n=>{const{normalize:e}=n;return e(["Researchers"])},power_users_desc:n=>{const{normalize:e}=n;return e(["Auto prompt engineering for your daily productivity."])},developers:n=>{const{normalize:e}=n;return e(["Developers"])},developers_desc:n=>{const{normalize:e}=n;return e(["Unleash the full power of multimodal AI with cutting-edge cloud-native technologies and open-source infrastructure."])},sdk:n=>{const{normalize:e}=n;return e(["SDK"])},starter_kit:n=>{const{normalize:e}=n;return e(["Starter Kit"])},sdk_desc:n=>{const{normalize:e}=n;return e(["Want to build high-level AIGC applications using PromptPerfect, SceneXplain, BestBanner, JinaChat, Rationale APIs? We've got you covered! Try our easy-to-use SDK and get started in minutes."])},sdk_docs:n=>{const{normalize:e}=n;return e(["Read docs"])},sdk_example:n=>{const{normalize:e}=n;return e(["Example"])},build_python:n=>{const{normalize:e}=n;return e(["Build with Python"])},build_js:n=>{const{normalize:e}=n;return e(["Build with JavaScript"])},enterprise:n=>{const{normalize:e}=n;return e(["Enterprise"])},enterprise_desc:n=>{const{normalize:e}=n;return e(["Boost your business with scalable, secure, and bespoke multimodal AI solutions."])},embedding_paper_title:n=>{const{normalize:e}=n;return e(["Jina Embeddings: A Novel Set of High-P`erformance Sentence Embedding Models"])},embedding_paper_desc:n=>{const{normalize:e}=n;return e(["Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB)."])}},about_us_page:{mit_techreview:n=>{const{normalize:e}=n;return e(["MIT Technology Review"])},mit_report_title:n=>{const{normalize:e}=n;return e(["Multimodal: AI\u2019s new frontier"])},sefo:{layer0:n=>{const{normalize:e}=n;return e(["End-user applications"])},layer1:n=>{const{normalize:e}=n;return e(["RAG / orchestration"])},layer3:n=>{const{normalize:e}=n;return e(["GPU / mobile / edge / local computing"])}},title3:n=>{const{normalize:e}=n;return e(["Starts Here"])},download_logo:n=>{const{normalize:e}=n;return e(["Download logos"])},download_jina_logo:n=>{const{normalize:e}=n;return e(["Download the Jina AI Logo"])},download_jina_logo_desc:n=>{const{normalize:e}=n;return e(["Get the Jina AI logo in both light and dark modes, available in PNG and SVG formats. This logo is a registered trademark with the European Union Intellectual Property Office (EUIPO)."])},download_docarray_logo:n=>{const{normalize:e}=n;return e(["Download the DocArray Logo"])},download_docarray_logo_desc:n=>{const{normalize:e}=n;return e(["Access the DocArray logo, an open-source project initiated by Jina AI and contributed to the Linux Foundation in December 2022. Available in light and dark modes, in PNG and SVG formats."])},brochure_info:n=>{const{normalize:e}=n;return e(["Your Guide to Our Company Awaits"])},download_brochure1:n=>{const{normalize:e}=n;return e(["Download brochure"])},employees:n=>{const{normalize:e}=n;return e(["Employees"])},approach:n=>{const{normalize:e}=n;return e(["Our Approach"])},approach_content1:n=>{const{normalize:e}=n;return e(["In the rapidly evolving world of AI, strategies need to be both nimble and forward-thinking. While our core offering remains centered on enterprises, the AI landscape has shifted in ways that necessitate a rethinking of our approach to customer acquisition. Here's why introducing power users as the entry point of our funnel isn't just innovative, but crucial for our sustained growth in the enterprise sector."])},approach_content2:n=>{const{normalize:e}=n;return e(["At Jina AI, our strategy is to be proactive rather than reactive. The inclusion of power users as the funnel's entry point ensures we're not only capturing current market trends but are also strategically poised for future enterprise growth. Our commitment to enterprises remains unwavering; however, our approach to reaching them is innovative, robust, and, above all, forward-thinking."])},approach_new_paradigm:n=>{const{normalize:e}=n;return e(["Prompt-based Technology: A New Paradigm"])},approach_miss_mark:n=>{const{normalize:e}=n;return e(["Why Traditional MLOps Miss the Mark"])},approach_connect_dots:n=>{const{normalize:e}=n;return e(["Connecting the Dots: Power Users to Enterprises"])},approach_new_paradigm_description:n=>{const{normalize:e}=n;return e([`2023 heralded a significant change: the rise of prompt-based technology. By simplifying the AI development process, it has democratized access to AI tools. Now, those without extensive programming experience\u2014termed as 'power users'\u2014can engage in AI development without the steep learning curves associated with tools like Pytorch, Docker, or Kubernetes.

Drawing a parallel, this is akin to the evolution of personal computing. Initially, only tech experts operated computers. But with the advent of user-friendly interfaces, a broader audience could participate. Today, with prompt-based technology, we're witnessing a similar democratization in AI.`])},approach_miss_mark_description:n=>{const{normalize:e}=n;return e(["While the influx of power users is significant, traditional MLOps tools are ill-equipped to cater to their needs. These tools are reminiscent of using a tractor to navigate city streets\u2014they're heavy and often excessive. The new-gen developers demand agile, intuitive tools that complement their rapid development pace."])},approach_connect_dots_description:n=>{const{normalize:e}=n;return e(["So, why is a power user focus essential for our enterprise-centric model? Because it\u2019s about establishing early relationships. By catering to power users now, we're building bridges to the enterprises they'll influence in the future. It\u2019s a strategic play\u2014a long-term investment to ensure our enterprise offering remains top-of-mind when these power users ascend to decision-making roles within organizations."])},title:n=>{const{normalize:e}=n;return e(["About Jina AI"])},title0:n=>{const{normalize:e}=n;return e(["The Future"])},title1:n=>{const{normalize:e}=n;return e(["Starts"])},title2:n=>{const{normalize:e}=n;return e(["Here"])},description:n=>{const{normalize:e}=n;return e(["The future starts here."])},subtitle:n=>{const{normalize:e}=n;return e(["Revolutionizing content creation through AI-generated solutions to unlock infinite possibilities. Shaping the future of AI-generated content and enhancing human creativity."])},stats_v1:n=>{const{normalize:e}=n;return e(["Search/acc"])},founded:n=>{const{normalize:e}=n;return e(["Founded"])},founded_in:n=>{const{normalize:e}=n;return e(["Founded in"])},empower_developers:n=>{const{normalize:e}=n;return e(["Developers Empowered"])},technologies:n=>{const{normalize:e}=n;return e(["Technologies"])},users:n=>{const{normalize:e}=n;return e(["Users Registered"])},value:n=>{const{normalize:e}=n;return e(["Our Value"])},value_content1:n=>{const{normalize:e}=n;return e([`We believe that openness can accelerate innovation and foster collaboration. We're not just advocates\u2014we are active contributors, investing significantly in the open-source community.

From being an early donor to FastAPI, to actively supporting the Linux Foundation and the Python Software Foundation, we are passionate about giving back. But we don\u2019t stop there; we\u2019ve also open-sourced our models and projects, sharing our expertise with the world.`])},stats_1:n=>{const{normalize:e}=n;return e(["Founded in February 2020, Jina AI has swiftly emerged as a global pioneer in multimodal AI technology. Within an impressive timeframe of 20 months, we have successfully raised $37.5M, marking our strong position in the AI industry. Our ground-breaking technology, open-sourced on GitHub, has empowered over 40,000 developers around the globe to seamlessly build and deploy sophisticated multimodal applications."])},stats_2:n=>{const{normalize:e}=n;return e(["In 2023, we've made significant strides in advancing AI generation tools grounded on multimodal technology. This innovation has benefited over 250,000 users worldwide, catering to a plethora of unique business requirements. From facilitating business growth and enhancing operational efficiency to optimizing costs, Jina AI is dedicated to empowering businesses to excel in the multimodal era."])},stats_4:n=>{const{normalize:e}=n;return e(['Founded in 2020 in Berlin, Jina AI is a leading search AI company. We provide the <span class="text-primary text-bold">Search Foundation</span>, the core for GenAI and multimodal applications. Our mission is to help businesses and developers unlock multimodal data for value creation with a better search. As a commercial open-source company, we like open innovation.'])},investors:n=>{const{normalize:e}=n;return e(["Our Investors"])},vision:n=>{const{normalize:e}=n;return e(["Our Mission"])},vision_content1:n=>{const{normalize:e}=n;return e(["Inspired by Yann LeCun's insight that '"])},vision_content3:n=>{const{normalize:e}=n;return e([`The future of AI is <span class="text-primary text-bold">multimodal</span>, and we are part of it. We realize that businesses face challenges in leveraging multimodal data. In response, we're committed to the <span class="text-primary text-bold">Search Foundation</span> to help businesses and developers search better, and utilize multimodal data for business growths.`])},yannlecun_quote:n=>{const{normalize:e}=n;return e(["An artificial intelligence system trained on words and sentences alone will never approximate human understanding."])},our_answer:n=>{const{normalize:e}=n;return e(["Absolutely, Yann. We're on it, building bridges to a multimodal AI future!"])},mission:n=>{const{normalize:e}=n;return e(["Our Mission"])},mission_content1:n=>{const{normalize:e}=n;return e(["We  Our key technologies, including prompt-tuning, prompt-serving, model-tuning, and model-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains."])},mission_content2:n=>{const{normalize:e}=n;return e(["At the heart of Jina AI lies our mission to be the portal to multimodal AI for a diverse clientele, from power users and developers to enterprises. We deeply believe in the power of open-source and are dedicated to building advanced, accessible tools for the AI community. Our key technologies, including prompt-tuning, prompt-serving, embedding-tuning, and embedding-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains."])},mission_content3:n=>{const{normalize:e}=n;return e(["At Jina AI, our mission is to lead the advancement of multimodal AI through innovative embedding and prompt-based technologies, focusing specifically on areas like natural language processing, image and video analysis, and cross-modal data interaction. This specialization allows us to provide unique solutions that turn complex, multi-source data into actionable insights and groundbreaking applications."])},approach_content4:n=>{const{normalize:e}=n;return e(['Everyone wants a better search. At Jina AI, we enable better search by providing the <span class="text-primary text-bold">Search Foundation</span>, which consists of Embeddings, Rerankers, Prompt Ops, and Infra. These components work in concert to revolutionize how we search and understand data. This leads to improved search experience, user trust, a direct increase in sales, and the unlocking of new business growth.'])},team:n=>{const{normalize:e}=n;return e(["Inside the Portal of Jina AI"])},team_content1:n=>{const{normalize:e}=n;return e(["From diverse corners of the globe, we're building the future of AI. Our distinct perspectives enrich our work, sparking innovations. Within this portal, we embrace our individuality, and passionately pursue our dreams. Welcome to the portal of the AI future."])},team_join:n=>{const{normalize:e}=n;return e(["Join us"])},office:n=>{const{normalize:e}=n;return e(["Our Offices"])},berlin:n=>{const{normalize:e}=n;return e(["Berlin, Germany"])},berlin_address:n=>{const{normalize:e}=n;return e(["Prinzessinnenstra\xDFe 19-20, 10969 Berlin, Germany"])},berlin_address2:n=>{const{normalize:e}=n;return e(["Gesch\xE4ftsanschrift: Leipziger str. 96, 10117 Berlin, Germany"])},bj:n=>{const{normalize:e}=n;return e(["Beijing, China"])},bj_address:n=>{const{normalize:e}=n;return e(["Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China"])},sz:n=>{const{normalize:e}=n;return e(["Shenzhen, China"])},sz_address:n=>{const{normalize:e}=n;return e(["402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China"])},awards:n=>{const{normalize:e}=n;return e(["Awards & Recognition"])},fastApiCaption:n=>{const{normalize:e}=n;return e(["Contributed over $20,000 since 2021."])},linuxFoundationCaption:n=>{const{normalize:e}=n;return e(["Makes an annual contribution of $10,000 starting from 2022."])},pythonSoftwareFoundationCaption:n=>{const{normalize:e}=n;return e(["Provided a one-time donation of $10,000 and sponsored multiple PyCon events including those in Germany, Italy, China, and the US."])},numfocusCaption:n=>{const{normalize:e}=n;return e(["Regularly donates each month starting from 2022."])},segmentFaultCaption:n=>{const{normalize:e}=n;return e(["Contributed a one-time donation of $6,000."])},otherProjectsCaption:n=>{const{normalize:e}=n;return e(["Donated over $3,000 via Github Sponsorship."])},understand_our_strength:n=>{const{normalize:e}=n;return e(["Understand Our Strength"])},understand_our_view2:n=>{const{normalize:e}=n;return e(["Understand the Search Foundation"])}},internship_page:{title:n=>{const{normalize:e}=n;return e(["Intern Program"])},description:n=>{const{normalize:e}=n;return e(["Worldwide call for students: Internship in research, engineering, marketing, sales and more."])},subtitle:n=>{const{normalize:e}=n;return e(["Our full-time internship programme provides hands-on work experience through well-designed internship projects in a wide range of scope."])},subtitle1:n=>{const{normalize:e}=n;return e(["Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together."])},alumni:n=>{const{normalize:e}=n;return e(["ALUMNI"])},about_internship_program:n=>{const{normalize:e}=n;return e(["About Internship Program"])},about_internship_program_desc1:n=>{const{normalize:e}=n;return e(["We are excited to offer this unique opportunity for talented individuals to join our dynamic team and contribute to groundbreaking projects in the field of Artificial Intelligence. This internship is designed to provide you with valuable hands-on experience, mentorship, and exposure to cutting-edge technologies that are shaping the future of AI."])},about_internship_program_desc2:n=>{const{normalize:e}=n;return e(["At Jina AI, we understand the significance of nurturing and harnessing young talent. We recognize that interns bring fresh perspectives, enthusiasm, and creativity to the table, invigorating our team with new ideas and approaches. By providing internships, we aim to foster the growth of future leaders in the AI industry while offering them real-world experience in a supportive and challenging environment."])},who_do_we_look_for:n=>{const{normalize:e}=n;return e(["Who do we look for?"])},who_do_we_look_for_desc:n=>{const{normalize:e}=n;return e(["We value diversity and encourage applicants from diverse profiles and backgrounds to join our Internship Program. The internship opportunities are offered in multiple departments, including Engineering, Design, Product Management, Sales and Account Management, Marketing and Community Management."])},enthusiastic:n=>{const{normalize:e}=n;return e(["ENTHUSIASTIC"])},self_motivated:n=>{const{normalize:e}=n;return e(["SELF-MOTIVATED"])},innovative:n=>{const{normalize:e}=n;return e(["INNOVATIVE"])},explore_stories_from_our_interns:n=>{const{normalize:e}=n;return e(["Explore stories from our interns"])},explore_stories_from_our_interns1:n=>{const{normalize:e}=n;return e(["Get inspired by our interns' journeys"])},software_engineer_intern:n=>{const{normalize:e}=n;return e(["Software Engineer Intern"])},recruiting_and_administrative_intern:n=>{const{normalize:e}=n;return e(["Recruiting and Administrative Intern"])},dev_rel_intern:n=>{const{normalize:e}=n;return e(["Developer Relations Intern"])},alumni_network:n=>{const{normalize:e}=n;return e(["Our thriving alumni network"])},intern_work1:n=>{const{normalize:e}=n;return e(["Fine-tuned LLM models for better embeddings"])},intern_work2:n=>{const{normalize:e}=n;return e(["Explored the potential of Retrieval Augmented Generation"])},intern_work3:n=>{const{normalize:e}=n;return e(["Published a paper on the topic of sentence embeddings"])},intern_work4:n=>{const{normalize:e}=n;return e(["Injecting continuous youthful vitality into the team"])},intern_work5:n=>{const{normalize:e}=n;return e(["Benchmarked quantization techniques to compress LLM"])},intern_work6:n=>{const{normalize:e}=n;return e(["Creating and promoting compelling campaign for PromptPerfect"])},application:n=>{const{normalize:e}=n;return e(["Application"])},submit_application:n=>{const{normalize:e}=n;return e(["Kickstart your adventure with Jina AI"])},application_desc:n=>{const{normalize:e}=n;return e(["Embark on a transformative journey with Jina AI. Our comprehensive internship program invites all passionate minds who aspire to shape the future of artificial intelligence. Join us to get real-world experience, work on challenging projects, and collaborate with some of the brightest minds in the AI industry."])},summer:n=>{const{normalize:e}=n;return e(["Summer"])},autumn:n=>{const{normalize:e}=n;return e(["Autumn"])},winter:n=>{const{normalize:e}=n;return e(["Winter"])},spring:n=>{const{normalize:e}=n;return e(["Spring"])},apply:n=>{const{normalize:e}=n;return e(["Apply now"])}},embeddings:{description:n=>{const{normalize:e}=n;return e(["Our world-class embeddings for search, RAG, agent systems."])}},spectrum:{representation:n=>{const{normalize:e}=n;return e(["Representation"])},contextualization:n=>{const{normalize:e}=n;return e(["Contextualization"])},personalization:n=>{const{normalize:e}=n;return e(["Personalization"])},grounding:n=>{const{normalize:e}=n;return e(["Grounding"])},representation_desc:n=>{const{normalize:e}=n;return e(["Embeddings transform multimodal data into a uniform, vectorized format. This enables the search system to understand and categorize content beyond simple keywords."])},contextualization_desc:n=>{const{normalize:e}=n;return e(["Rerankers adjust initial search results based on deep contextual relevance wrt. query. This refines the ranking to better match what users are likely to find useful."])},grounding_desc:n=>{const{normalize:e}=n;return e(["Reader refining inputs and results through LLMs. They improve the quality, readability and factuality of the final answer."])},personalization_desc:n=>{const{normalize:e}=n;return e(["Using synthetic data guided by the user instruction to automatically train a domain-specific embedding and reranker model."])},click_to_learn_more:n=>{const{normalize:e}=n;return e(["Click to learn more"])},embeddings:n=>{const{normalize:e}=n;return e(["Embeddings"])},embeddings_desc:n=>{const{normalize:e}=n;return e(["Embeddings are the cornerstones of modern search system, representing multimodal data into vectors of numbers. This process enables a more nuanced and contextual understanding of content, far beyond simple keyword matching."])},rerankers:n=>{const{normalize:e}=n;return e(["Reranker"])},rerankers_desc:n=>{const{normalize:e}=n;return e(["Rerankers take the initial results from the embeddings and refine them, ensuring that the most relevant results are presented to the user. This is crucial for delivering high-quality search results that meet the user's intent."])},promptOps:n=>{const{normalize:e}=n;return e(["PromptOps"])},promptOps_desc:n=>{const{normalize:e}=n;return e(["Prompt Ops improve the input and output of the search system, including those used in queries expansion, LLM-input and results rewriting. This ensures that the the search understands better and results better."])},coreInfra:n=>{const{normalize:e}=n;return e(["Core Infra"])},coreInfra_desc:n=>{const{normalize:e}=n;return e(["Core infra provides a cloud-native layer for developing, deploying and orchestration search foundation models both in the public cloud and on-premises, enabling services to scale up and down effortlessly."])},prompt_tech:n=>{const{normalize:e}=n;return e(["Prompt & agent engineering"])},embedding_tech:n=>{const{normalize:e}=n;return e(["Embeddings"])},for_power_users:n=>{const{normalize:e}=n;return e(["For Power Users"])},for_developers:n=>{const{normalize:e}=n;return e(["For Developers"])},for_enterprise:n=>{const{normalize:e}=n;return e(["For Enterprises"])},prompt_serving:n=>{const{normalize:e}=n;return e(["Prompt Serving"])},prompt_tuning:n=>{const{normalize:e}=n;return e(["Prompt Tuning"])},model_serving:n=>{const{normalize:e}=n;return e(["Model Serving"])},model_tuning:n=>{const{normalize:e}=n;return e(["Model Tuning"])},embedding_serving:n=>{const{normalize:e}=n;return e(["Embedding Serving"])},embedding_tuning:n=>{const{normalize:e}=n;return e(["Embedding Tuning"])},prompt_tech_description:n=>{const{normalize:e}=n;return e([`At Jina AI, we recognize prompt engineering as vital for interacting with large language models (LLMs). As these models advance, the complexity of prompts escalates, encompassing intricate reasoning and logic. This advancement underscores the intertwined growth of LLMs and prompt sophistication.

We foresee a future where LLMs act as compilers, with prompts becoming the new programming language. This shift suggests that future technological proficiency may focus more on prompt mastery than traditional coding. Our commitment at Jina AI is to lead in this transformative area, making advanced AI accessible and practical for everyday use by mastering this emerging 'language'.`])},embedding_tech_description:n=>{const{normalize:e}=n;return e([`At Jina AI, we harness the power of embedding technology to revolutionize diverse AI applications. This technology serves as a unified method to efficiently represent and compress various data types, ensuring no loss of critical information. Our focus is on transforming complex datasets into a universally understandable embedding format, which is essential for precise and insightful AI analysis.

Embeddings are fundamental, especially in applications like precise image and voice recognition, where they help discern fine-grained details and nuances. In natural language processing, embeddings enhance understanding of context and sentiment, leading to more accurate conversational AI and language translation tools. They are also crucial in developing sophisticated recommendation systems that require a deep understanding of user preferences across different content forms, such as text, audio, and video.`])},prompt_serving_description:n=>{const{normalize:e}=n;return e(["Wrapping and serving prompts through an API, without hosting heavy models. The API calls a public large language model service and handles the orchestration of inputs and outputs in a chain of operations."])},prompt_tuning_description:n=>{const{normalize:e}=n;return e(["The process of crafting and refining the input prompts in order to guide its output towards specific, desired responses."])},embedding_serving_description:n=>{const{normalize:e}=n;return e(["Delivering embeddings through a robust, scalable microservice using cloud-native technologies."])},embedding_tuning_description:n=>{const{normalize:e}=n;return e(["Optimizing high-quality embeddings by integrating domain expertise for enhanced task-specific performance."])},model_serving_description:n=>{const{normalize:e}=n;return e(["The deployment of fine-tuned models in a production environment, usually requiring substantial resources such as GPU hosting. MLOps, emphasizing the serving of mid-size to large models in a scalable, efficient, and reliable manner."])},model_tuning_description:n=>{const{normalize:e}=n;return e(["Also known as fine-tuning, involves adjusting the parameters of a pre-trained model on a new, often task-specific dataset to improve its performance and adapt it to a specific application."])}},huggingface:{sentence_similarity:n=>{const{normalize:e}=n;return e(["Sentence embedding"])},updated_about:n=>{const{normalize:e}=n;return e(["Updated about"])}},impact_snapshots:{project1:n=>{const{normalize:e}=n;return e(["Enable high-accuracy search within 3D mesh data using point cloud information."])},project2:n=>{const{normalize:e}=n;return e(["Design a content-based search engine for short animation films."])},project3:n=>{const{normalize:e}=n;return e(["Enhance e-commerce conversion rates by fine-tuning embedding models."])},project4:n=>{const{normalize:e}=n;return e(["Execute prompt tuning to boost efficiency for a business consulting company."])},project5:n=>{const{normalize:e}=n;return e(["Pioneer game scene understanding and automatic annotation for a leading gaming enterprise."])},project6:n=>{const{normalize:e}=n;return e(["Implement real-time input expansion for a chatbot company, enhancing user experience."])},project7:n=>{const{normalize:e}=n;return e(["Revolutionize legal tech by enabling efficient search within lengthy legal documents."])},project8:n=>{const{normalize:e}=n;return e(["Support a high-throughput generative art service for large-scale operations."])},project9:n=>{const{normalize:e}=n;return e(["Carry out process mining and modeling using advanced language models."])},project10:n=>{const{normalize:e}=n;return e(["Leverage computer vision to improve digital accessibility of government websites."])},project11:n=>{const{normalize:e}=n;return e(["Fine-tune LLM for a consulting firm to optimize finance data analysis."])},project12:n=>{const{normalize:e}=n;return e(["Advance marketing strategies by fine-tuning text-to-image models for style transferring."])}},project_status:{observability:n=>{const{normalize:e}=n;return e(["Observability"])},graduated:n=>{const{normalize:e}=n;return e(["Graduated"])},incubating:n=>{const{normalize:e}=n;return e(["Incubating"])},sandbox:n=>{const{normalize:e}=n;return e(["Sandbox"])},archived:n=>{const{normalize:e}=n;return e(["Archived"])},kubernetes:n=>{const{normalize:e}=n;return e(["Kubernetes"])},cloud_native:n=>{const{normalize:e}=n;return e(["Cloud Native"])},prompt_tuning:n=>{const{normalize:e}=n;return e(["Prompt Tuning"])},model_serving:n=>{const{normalize:e}=n;return e(["Model Serving"])},model_tuning:n=>{const{normalize:e}=n;return e(["Model Tuning"])},embedding_serving:n=>{const{normalize:e}=n;return e(["Embedding Serving"])},embedding_tuning:n=>{const{normalize:e}=n;return e(["Embedding Tuning"])},prompt_serving:n=>{const{normalize:e}=n;return e(["Prompt Serving"])},core:n=>{const{normalize:e}=n;return e(["Core"])},small_size_model:n=>{const{normalize:e}=n;return e(["Small Size Model"])},large_size_model:n=>{const{normalize:e}=n;return e(["Large Size Model"])},mid_size_model:n=>{const{normalize:e}=n;return e(["Mid Size Model"])},orchestration:n=>{const{normalize:e}=n;return e(["Orchestration"])},linux_foundation:n=>{const{normalize:e}=n;return e(["Linux Foundation"])},vector_database:n=>{const{normalize:e}=n;return e(["Vector Database"])},data_structure:n=>{const{normalize:e}=n;return e(["Data Structure"])},vector_store:n=>{const{normalize:e}=n;return e(["Vector Store"])},llm1:n=>{const{normalize:e}=n;return e(["LLMOps"])},rag1:n=>{const{normalize:e}=n;return e(["RAG"])}},contact_us_page:{title:n=>{const{normalize:e}=n;return e(["Contact sales"])},description:n=>{const{normalize:e}=n;return e(["Grow your business with Jina AI."])},impact_snapshots:n=>{const{normalize:e}=n;return e(["Impact Snapshots"])},subtitle:n=>{const{normalize:e}=n;return e(["Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise."])},subtitle1:n=>{const{normalize:e}=n;return e(["Jina AI, a leader in multimodal AI, excels in embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. Leveraging cloud-native technologies like Kubernetes and serverless architectures, we deliver robust, scalable, and production-ready solutions. With expertise in large language models, text, image, video, audio understanding, neural search, and generative AI, we provide innovative, future-proof strategies to elevate your business."])},subtitle2:n=>{const{normalize:e}=n;return e(["Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise."])},trusted_by:n=>{const{normalize:e}=n;return e(["Trusted by"])},name:n=>{const{normalize:e}=n;return e(["Name"])},work_email:n=>{const{normalize:e}=n;return e(["Work email"])},country:n=>{const{normalize:e}=n;return e(["Country"])},company:n=>{const{normalize:e}=n;return e(["Company"])},company_size:n=>{const{normalize:e}=n;return e(["Company size"])},company_website:n=>{const{normalize:e}=n;return e(["Company website"])},company_website_placeholder:n=>{const{normalize:e}=n;return e(["URL for your company's homepage or LinkedIn profile"])},invalid_url:n=>{const{normalize:e}=n;return e(["URL is invalid"])},invalid_email:n=>{const{normalize:e}=n;return e(["Email is invalid"])},preferred_products:n=>{const{normalize:e}=n;return e(["Which products are you interested in?"])},department:n=>{const{normalize:e}=n;return e(["Department"])},role:n=>{const{normalize:e}=n;return e(["Job role"])},field_required:n=>{const{normalize:e}=n;return e(["Field is required"])},invalid_date_format:n=>{const{normalize:e}=n;return e(["Invalid date format. Please use DD-MM-YYYY format."])},invalid_number:n=>{const{normalize:e}=n;return e(["Invalid number. Please input again"])},anything_else:n=>{const{normalize:e}=n;return e(["Tell us more about your project"])},agreement:n=>{const{normalize:e}=n;return e(["By submitting, you confirm that you agree to the processing of your personal data by Jina AI as described in the"])},private_statement:n=>{const{normalize:e}=n;return e(["Privacy Statement"])},submit:n=>{const{normalize:e}=n;return e(["Submit"])},submit_success:n=>{const{normalize:e}=n;return e(["Thank you for your submission. We will get back to you shortly."])},submit_failed:n=>{const{normalize:e}=n;return e(["Submission failed. Please try again later."])},faq:n=>{const{normalize:e}=n;return e(["FAQ"])}},blog_tags:{featured:n=>{const{normalize:e}=n;return e(["Featured"])},"tech-blog":n=>{const{normalize:e}=n;return e(["Tech blog"])},all:n=>{const{normalize:e}=n;return e(["All"])},latest:n=>{const{normalize:e}=n;return e(["Latest"])},press:n=>{const{normalize:e}=n;return e(["Press release"])},events:n=>{const{normalize:e}=n;return e(["Event"])},insights:n=>{const{normalize:e}=n;return e(["Opinion"])},"knowledge-base":n=>{const{normalize:e}=n;return e(["Knowledge base"])},releases:n=>{const{normalize:e}=n;return e(["Software update"])}},newsroom_page:{academic:n=>{const{normalize:e}=n;return e(["Academic"])},academic_research:n=>{const{normalize:e}=n;return e(["Academic Publications"])},title:n=>{const{normalize:e}=n;return e(["Newsroom"])},top_stories:n=>{const{normalize:e}=n;return e(["Top stories"])},description:n=>{const{normalize:e}=n;return e(["Read the latest news and updates from Jina AI."])},description1:n=>{const{normalize:e}=n;return e(["Crafting AI innovations, one word at a time."])},tech_blog:n=>{const{normalize:e}=n;return e(["Tech blog"])},news_title:n=>{const{normalize:e}=n;return e(["Search All the Things: We're Running a MEME Contest for Jina 2.0"])},news_description:n=>{const{normalize:e}=n;return e(["For Jina 2.0, we listened to the community. Truly, deeply listened. \u201CWhat are your pain points?\u201D we asked, eagerly anticipating valuable feedback"])},engineering_group:n=>{const{normalize:e}=n;return e(["Engineering Group"])},engineering_group_date:n=>{const{normalize:e}=n;return e(["31 May, 2021"])},author:n=>{const{normalize:e}=n;return e(["Filter by author"])},product:n=>{const{normalize:e}=n;return e(["Filter by product"])},photos:n=>{const{normalize:e}=n;return e(["Photos"])},most_recent_articles:n=>{const{normalize:e}=n;return e(["Most recent articles"])},minutes_read:n=>{const{normalize:e}=n;return e(["minutes read"])},search:n=>{const{normalize:e}=n;return e(["Search by title"])}},news_page:{copy_link:n=>{const{normalize:e}=n;return e(["Copy the link to this section"])},news_not_found:n=>{const{normalize:e}=n;return e(["Article not found"])},redirect_to_news:n=>{const{normalize:e}=n;return e(["Redirecting to newsroom in 5 seconds..."])},back_to_newsroom:n=>{const{normalize:e}=n;return e(["Back to Newsroom"])},categories:n=>{const{normalize:e}=n;return e(["Categories"])},learn_more:n=>{const{normalize:e}=n;return e(["Learn more"])},in_this_article:n=>{const{normalize:e}=n;return e(["In this article"])}},github:{stars:n=>{const{normalize:e}=n;return e(["Stars"])}},share:{share_btn:n=>{const{normalize:e}=n;return e(["Share"])},"Hacker News":n=>{const{normalize:e}=n;return e(["Hacker News"])},LinkedIn:n=>{const{normalize:e}=n;return e(["LinkedIn"])},reddit:n=>{const{normalize:e}=n;return e(["Reddit"])},twitter:n=>{const{normalize:e}=n;return e(["X (Twitter)"])},facebook:n=>{const{normalize:e}=n;return e(["Facebook"])},rss:n=>{const{normalize:e}=n;return e(["RSS feed"])}},faq_button:n=>{const{normalize:e}=n;return e(["FAQ"])},faq:{question1:n=>{const{normalize:e}=n;return e(["What does Jina AI specialize in?"])},answer1:n=>{const{normalize:e}=n;return e(["Jina AI specializes in multimodal AI technologies, including embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. We leverage advanced tools like Kubernetes and serverless architectures to create robust, scalable, and production-ready solutions."])},question2:n=>{const{normalize:e}=n;return e(["What types of AI does Jina AI work with?"])},answer2:n=>{const{normalize:e}=n;return e(["Our expertise spans a broad spectrum, encompassing large language models, text, image, video, audio understanding, neural search, and generative art."])},question3:n=>{const{normalize:e}=n;return e(["Are your solutions scalable and production-ready?"])},answer3:n=>{const{normalize:e}=n;return e(["Yes, our solutions are designed to be scalable and ready for production. We build our solutions using cloud-native technologies that allow for efficient scaling and reliable performance in production environments."])},question4:n=>{const{normalize:e}=n;return e(["What industries can benefit from Jina AI's solutions?"])},answer4:n=>{const{normalize:e}=n;return e(["Our services are versatile and adaptable, making them suitable for a wide range of industries, including e-commerce, legal tech, digital marketing, gaming, healthcare, finance, and many more."])},question5:n=>{const{normalize:e}=n;return e(["How do we start a project with Jina AI?"])},answer5:n=>{const{normalize:e}=n;return e(["You can get in touch with our sales team through the contact form on this page. We would love to discuss your project requirements and how our solutions can help your business."])},question6:n=>{const{normalize:e}=n;return e(["What support do you provide after implementing a solution?"])},answer6:n=>{const{normalize:e}=n;return e(["We provide continuous support to ensure the smooth operation of our solutions. This includes troubleshooting, regular updates, and improvements based on your feedback and needs."])},question7:n=>{const{normalize:e}=n;return e(["What is the typical duration for a project?"])},answer7:n=>{const{normalize:e}=n;return e(["Project duration varies depending on the complexity and scope of the project. After understanding your requirements, we can provide a more accurate estimate."])},question8:n=>{const{normalize:e}=n;return e(["How does Jina AI protect my data?"])},answer8:n=>{const{normalize:e}=n;return e(["Data security is our top priority. We adhere to strict data protection policies and regulations to ensure your data is secure and confidential."])},question9:n=>{const{normalize:e}=n;return e(["What is the pricing structure for your services?"])},answer9:n=>{const{normalize:e}=n;return e(["Pricing depends on the project's complexity and requirements. We offer both project-based and retainer pricing models. Please contact our sales team for more information."])},question10:n=>{const{normalize:e}=n;return e(["What are the licensing terms for your solutions?"])},answer10:n=>{const{normalize:e}=n;return e(["We provide different licensing options based on the nature of the project and the client's needs. Detailed terms can be discussed with our sales team."])},question11:n=>{const{normalize:e}=n;return e(["What is your service area?"])},answer11:n=>{const{normalize:e}=n;return e(["We provide services globally, with our headquarters based in Berlin, Europe, and additional offices in Beijing and Shenzhen."])},question12:n=>{const{normalize:e}=n;return e(["Do you offer onsite support?"])},answer12:n=>{const{normalize:e}=n;return e(["Yes, we offer onsite support, especially for clients located near our offices in Berlin, Beijing, and Shenzhen. For other locations, we strive to provide the best possible remote support and can arrange for onsite support if necessary."])}},beta:n=>{const{normalize:e}=n;return e(["Beta"])},print:n=>{const{normalize:e}=n;return e(["Print"])},get_new_key:n=>{const{normalize:e}=n;return e(["Get your API key"])},subscribe_system:{used_product_required:n=>{const{normalize:e}=n;return e(["Select the model you are using or you are interested in"])},used_product:n=>{const{normalize:e}=n;return e(["Which model are you using?"])},domain_required:n=>{const{normalize:e}=n;return e(["Tell us your work domain helps us to provide better service"])},contactTitle_required:n=>{const{normalize:e}=n;return e(["Your job title is required"])},contactName_required:n=>{const{normalize:e}=n;return e(["How should we address you?"])},company_url_required:n=>{const{normalize:e}=n;return e(["Tell us your company's website helps us to provide better service"])},care_most_required:n=>{const{normalize:e}=n;return e(["When choosing a service, what do you care most about?"])},company_size_required:n=>{const{normalize:e}=n;return e(["Tell us your company's size helps us to provide better service"])},usage_type_required:n=>{const{normalize:e}=n;return e(["Tell us your usage type helps us to provide better service"])},get_new_key:n=>{const{normalize:e}=n;return e(["Get your API key"])},usage_type_options:{research:n=>{const{normalize:e}=n;return e(["Research"])},poc:n=>{const{normalize:e}=n;return e(["Proof of Concept"])},production:n=>{const{normalize:e}=n;return e(["Production"])},other:n=>{const{normalize:e}=n;return e(["Other"])}},care_most_options:{accuracy:n=>{const{normalize:e}=n;return e(["Accuracy"])},speed:n=>{const{normalize:e}=n;return e(["Speed"])},cost:n=>{const{normalize:e}=n;return e(["Cost"])},scalability:n=>{const{normalize:e}=n;return e(["Scalability"])},other:n=>{const{normalize:e}=n;return e(["Other"])}},contactTitle:n=>{const{normalize:e}=n;return e(["What is your job title?"])},full_survey:n=>{const{normalize:e}=n;return e(["Take the full survey and get faster response from our team"])},usage_type:n=>{const{normalize:e}=n;return e(["What type of usage best describes you?"])},care_most:n=>{const{normalize:e}=n;return e(["What do you care most about?"])},company_url:n=>{const{normalize:e}=n;return e(["What is your company's website?"])},company_size:n=>{const{normalize:e}=n;return e(["What is your company's size?"])},contactName:n=>{const{normalize:e}=n;return e(["Your name"])},email:n=>{const{normalize:e}=n;return e(["Email"])},email_contact:n=>{const{normalize:e}=n;return e(["Your contact Email"])},subscribe:n=>{const{normalize:e}=n;return e(["Subscribe"])},send:n=>{const{normalize:e}=n;return e(["Send"])},contact_us:n=>{const{normalize:e}=n;return e(["Contact us"])},sign_up:n=>{const{normalize:e}=n;return e(["Sign up"])},tell_domain:n=>{const{normalize:e}=n;return e(["What domain do you work in?"])},get_update_embeddings:n=>{const{normalize:e}=n;return e(["Get the latest updates for the embeddings"])},get_update_blog_posts:n=>{const{normalize:e}=n;return e(["Get the latest updates for the blog posts"])},email_required:n=>{const{normalize:e}=n;return e(["Email is required"])},email_invalid:n=>{const{normalize:e}=n;return e(["Email is invalid"])},fine_tuned_embedding:n=>{const{normalize:e}=n;return e(["Interested in fine-tuned embeddings tailored to your data and use case? Let's discuss!"])},fine_tuned_reranker:n=>{const{normalize:e}=n;return e(["Interested in fine-tuned rerankers tailored to your data and use case? Let's discuss!"])}},lab_dialog:{explain:n=>{const{normalize:e}=n;return e(["Discover hidden features on our website"])},GlobalQA:{title:n=>{const{normalize:e}=n;return e(["On-page RAG"])},description:n=>{const{normalize:e}=n;return e(["Press the '/' key on any page to open the question box. Type your query and hit 'Enter' to receive answers directly related to the page content. This feature is powered by PromptPerfect."])}},SceneXplainTooltip:{title:n=>{const{normalize:e}=n;return e(["Image Captioning"])},description:n=>{const{normalize:e}=n;return e(["Hover your cursor over any image on news pages or in our newsroom catalog to reveal the description of that image. Descriptions are precomputed by SceneXplain and embedded in the image's ALT attribute for accessibility."])}},Recommender:{title:n=>{const{normalize:e}=n;return e(["Related Article"])},description:n=>{const{normalize:e}=n;return e(["Open the recommendation box on any news page with 'Shift+2'. Select the reranker model to discover the top-5 articles related to that news page. Enjoy this real-time feature, powered by our Reranker API."])}}},recommender:{recommended_articles:n=>{const{normalize:e}=n;return e(["Top-5 similar articles"])},recommend:n=>{const{normalize:e}=n;return e(["Get top-5"])},out_of_quota:n=>{const{normalize:e}=n;return e(["This API key has run out of tokens. Please recharge your account or use a different API key."])},confirm_title:n=>{const{normalize:e}=n;return e(["Warning: High Token Usage"])},confirm_message:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Your API key has ",r(o("_leftTokens"))," tokens remaining. Sending the full text of ",r(o("_numArticles"))," articles to the Reranker API, utilizing the ",r(o("_selectedReranker"))," model to discover related articles for the current page, will significantly reduce the token count of your API key ",r(o("_APIKey")),". Do you want to proceed?"])}},finetuning:{api_key:n=>{const{normalize:e}=n;return e(["Enter your API key."])},new_key:n=>{const{normalize:e}=n;return e(["Get new key"])},continue:n=>{const{normalize:e}=n;return e(["Continue"])},preview:n=>{const{normalize:e}=n;return e(["Preview"])},use_url:n=>{const{normalize:e}=n;return e(["Use URL instead. Toggle it on means we will base on the page content of that URL to generate synthetic data for fine-tuning."])},job_acknowledged:n=>{const{normalize:e}=n;return e(["Your fine-tuning job has been queued. You will receive an email when the job starts. The full process often takes 20 minutes to complete."])},find_on_huggingface:n=>{const{normalize:e}=n;return e(["Find results on Hugging Face"])},not_enough_token:n=>{const{normalize:e}=n;return e(["Not enough tokens in this API key. Please top up your balance or use a different API key."])},wait_for_processing:n=>{const{normalize:e}=n;return e(["Please wait while we process your request..."])},back:n=>{const{normalize:e}=n;return e(["Back"])},failed_job:n=>{const{normalize:e}=n;return e(["The fine-tuning request failed. See the reason below."])},reset:n=>{const{normalize:e}=n;return e(["Start over"])},domain_hint:n=>{const{normalize:e}=n;return e(["Describe the domain you wish to fine-tune for."])},query_doc:n=>{const{normalize:e}=n;return e(["Query-document description"])},query_explain:n=>{const{normalize:e}=n;return e(["Describe what a query looks like."])},doc_explain:n=>{const{normalize:e}=n;return e(["Describe what a matched document should look like."])},url_explain:n=>{const{normalize:e}=n;return e(["Public URL of a webpage that contains the content you want to fine-tune on."])},query_doc_caption:n=>{const{normalize:e}=n;return e(["Describe what the query looks like and what the matched document looks like in your domain."])},url:n=>{const{normalize:e}=n;return e(["Or, webpage URL"])},url_caption:n=>{const{normalize:e}=n;return e(["Refer to the content from a URL for fine-tuning."])},general_instruction:n=>{const{normalize:e}=n;return e(["Or, general instruction"])},general_instruction_explain:n=>{const{normalize:e}=n;return e(['Describe your domain in free-form text. You can imagine it as a "prompt" like in ChatGPT.'])},general_instruction_caption:n=>{const{normalize:e}=n;return e(["Provide a detailed description of how the fine-tuned embeddings will be used."])},domain_explain:n=>{const{normalize:e}=n;return e(["Provide a detailed description of how the fine-tuned embeddings will be used. This is essential for generating high-quality synthetic data that will improve the performance of your embeddings."])},domain_explain2:n=>{const{normalize:e}=n;return e(["There are three ways to specify your requirement: a general instruction, a URL, or a query-document description. Choose one."])},select_base_model:n=>{const{normalize:e}=n;return e(["Choose a base embedding model for fine-tuning."])},base_model_selected:n=>{const{normalize:e}=n;return e(["Base model selected"])},select_base_model_explain:n=>{const{normalize:e}=n;return e(["Select a base model as the starting point for fine-tuning. Typically, base-en is a good choice, but for tasks in other languages, consider using a bilingual model."])},write_email_explain:n=>{const{normalize:e}=n;return e(["Fine-tuning takes time. We'll communicate via email about the start, progress, completion, and any issues of your fine-tuning job, along with details on the fine-tuned model and training dataset."])},cost_1m_token:n=>{const{normalize:e}=n;return e(["Each fine-tuning job consumes 1M tokens. Ensure you have sufficient tokens or top-up your balance. You can also generate a new API key. Every API key comes with 1M free tokens."])},placeholder:n=>{const{normalize:e}=n;return e(["Car insurance claims"])},which_domain:n=>{const{normalize:e}=n;return e(["Fine-tuning domain"])},start_tuning:n=>{const{normalize:e}=n;return e(["Start fine-tuning"])},click_start:n=>{const{normalize:e}=n;return e(["Agree to the terms and begin fine-tuning."])},how_it_works:n=>{const{normalize:e}=n;return e(["Learn about the fine-tuning process."])},email_not_match:n=>{const{normalize:e}=n;return e(["Email addresses do not match. Please verify."])},consent0:n=>{const{normalize:e}=n;return e(["I agree that synthetic data for model fine-tuning will be generated based on my instructions."])},consent1:n=>{const{normalize:e}=n;return e(["I acknowledge that the final model and synthetic data will be publicly accessible on Hugging Face."])},consent2:n=>{const{normalize:e}=n;return e(["I understand that this feature is in beta and Jina AI offers no warranties. The pricing and UX may change."])},confirm_title:n=>{const{normalize:e}=n;return e(["Confirm fine-tuning job"])},confirm_your_email:n=>{const{normalize:e}=n;return e(["Re-enter your email address to confirm the fine-tuning job. Updates and the download link will be sent to this email."])}},embedding:{turnstile_error:n=>{const{normalize:e}=n;return e(["We cannot generate an API key because we couldn't verify if you are human."])},turnstile_unsupported:n=>{const{normalize:e}=n;return e(["We cannot generate an API key because your browser isn't supported."])},mistake_contact:n=>{const{normalize:e}=n;return e(["If you believe this is an error, please contact us."])},auto_request:n=>{const{normalize:e}=n;return e(["Auto preview"])},auto_request_tooltip:n=>{const{normalize:e}=n;return e(["Automatically preview the API response when changing the model, using hundreds of tokens from your API key. Turn off to manually send a request by clicking 'Get response'."])},public_cloud_integration:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["With <b>",r(o("_numPartners")),"</b> cloud service providers"])},public_cloud_integration_desc:n=>{const{normalize:e}=n;return e(["Is your company using AWS or Azure? Then directly deploy our search foundation models on these platforms in your company, so your data stays secure and compliant."])},"3p_integration":n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["With <b>",r(o("_numPartners")),"</b> third-party services"])},"3p_integration_desc":n=>{const{normalize:e}=n;return e(["Integrate our search foundation with your existing services. Our partners have built connectors to our API, making it easy to use our models in your applications."])},input_type:n=>{const{normalize:e}=n;return e(["Embed as query or document"])},input_type_explain:n=>{const{normalize:e}=n;return e(["Some embedding models have dedicated embedding strategies for queries and documents. The same string can be embedded as a query or a document depending on its role in your application."])},return_format_title:n=>{const{normalize:e}=n;return e(["Returning data type"])},return_format_explain:n=>{const{normalize:e}=n;return e(["Besides the float, you can ask it to return as binary for faster vector retrieval, or as base64 encoding for faster transmission."])},example_inputs:n=>{const{normalize:e}=n;return e(["Example inputs"])},remaining_left:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["You have <b>",r(o("_leftTokens")),"</b> tokens left in the API key below."])},tax_may_apply:n=>{const{normalize:e}=n;return e(["Depending on your location, you may be charged in USD, EUR, or other currencies. Taxes may apply."])},auto_recharge:n=>{const{normalize:e}=n;return e(["Auto-recharge when tokens are low"])},auto_recharge_description:n=>{const{normalize:e}=n;return e(["Recommended for uninterrupted service in production. When your token balance is below the threshold you set, we will automatically recharge your credit card for the same amount as your last top-up. If you purchased multiple packs in the last top-up, we will recharge only one pack."])},auto_recharge_enable:n=>{const{normalize:e}=n;return e(["You have enabled auto-recharge on low tokens"])},auto_recharge_confirm_title:n=>{const{normalize:e}=n;return e(["Disable auto-recharge"])},auto_recharge_confirm_message:n=>{const{normalize:e}=n;return e(["Are you sure you want to disable auto-recharge? This will prevent automatic top-ups when your token balance is low."])},auto_recharge_enable_title:n=>{const{normalize:e}=n;return e(["Enable auto-recharge"])},auto_recharge_enable_message:n=>{const{normalize:e}=n;return e(["To enable auto-recharge, please purchase a pack with auto-recharge set to true."])},recharge_threshold:n=>{const{normalize:e}=n;return e(["Recharge threshold"])},includes:n=>{const{normalize:e}=n;return e(["Tokens valid for:"])},return_float:n=>{const{normalize:e}=n;return e(["Default (as float)"])},return_binary:n=>{const{normalize:e}=n;return e(["Binary (packed as int8)"])},return_ubinary:n=>{const{normalize:e}=n;return e(["Binary (packed as uint8)"])},return_base64:n=>{const{normalize:e}=n;return e(["Base64 (as string)"])},return_format:n=>{const{normalize:e}=n;return e(["Embeddings format"])},float_description:n=>{const{normalize:e}=n;return e(["The embeddings are returned as a list of floating-point numbers. Most common and easy to use."])},binary_description:n=>{const{normalize:e}=n;return e(["The embeddings are packed as int8. Much more efficient for storage, search and transmission."])},ubinary_description:n=>{const{normalize:e}=n;return e(["The embeddings are packed as uint8. Much more efficient for storage, search and transmission."])},base64_description:n=>{const{normalize:e}=n;return e(["The embeddings are returned as a base64-encoded string. More efficient for transmission."])},tuning:n=>{const{normalize:e}=n;return e(["Fine-Tune"])},onprem:n=>{const{normalize:e}=n;return e(["On-prem"])},running:n=>{const{normalize:e}=n;return e(["Active"])},sleeping:n=>{const{normalize:e}=n;return e(["Inactive"])},status_explain:n=>{const{normalize:e}=n;return e(["Our serverless architecture may offload certain models during periods of low usage. For active models, responses are immediate. Inactive models require a few seconds to load upon the initial request. After activation, subsequent requests are processed more swiftly."])},score:n=>{const{normalize:e}=n;return e(["Score"])},index_and_search1:n=>{const{normalize:e}=n;return e(["Index & search"])},index_and_search:n=>{const{normalize:e}=n;return e(["Index & search"])},none:n=>{const{normalize:e}=n;return e(["None"])},results_fed_to_reranker:n=>{const{normalize:e}=n;return e(["#docs fed to reranker"])},results_as_final_result:n=>{const{normalize:e}=n;return e(["#docs as result"])},please_select_model:n=>{const{normalize:e}=n;return e(["Please select an Embedding model or a Reranker model"])},embedding_none_description:n=>{const{normalize:e}=n;return e(["Do not use any embedding model"])},rank_none_description:n=>{const{normalize:e}=n;return e(["Do not use any reranker model"])},get_new_key_survey:n=>{const{normalize:e}=n;return e(["Fill in the survey, help us understand your usage, and get a new API key for free!"])},pricing:n=>{const{normalize:e}=n;return e(["API Pricing"])},right_api_key_to_charge:n=>{const{normalize:e}=n;return e(["Please input the right API key to top up"])},pricing_desc:n=>{const{normalize:e}=n;return e(["Our API pricing is structured around the number of tokens sent in the requests. For Reader API, it is the number of tokens in the responses. This pricing model is applicable to all products in Jina AI's search foundation: Embedding, Reranking, Reader, Auto Fine-Tuning APIs. With the same API key, you have access to all API services."])},select_embedding_model:n=>{const{normalize:e}=n;return e(["Select embeddings"])},select_rerank_model:n=>{const{normalize:e}=n;return e(["Select reranker"])},fill_example:n=>{const{normalize:e}=n;return e(["Fill in an example"])},"bge-small-en-v1_5_description":n=>{const{normalize:e}=n;return e(["A streamlined English model delivering efficient and high-quality embeddings."])},"bge-base-en-v1_5_description":n=>{const{normalize:e}=n;return e(["A robust English model balancing performance and efficiency for versatile use."])},"bge-large-en-v1_5_description":n=>{const{normalize:e}=n;return e(["A powerhouse English model offering top-tier embeddings with exceptional quality."])},"bge-small-zh-v1_5_description":n=>{const{normalize:e}=n;return e(["A compact Chinese model providing nimble and precise embeddings."])},"bge-base-zh-v1_5_description":n=>{const{normalize:e}=n;return e(["A well-rounded Chinese model balancing capability and efficiency."])},"bge-large-zh-v1_5_description":n=>{const{normalize:e}=n;return e(["A high-capacity Chinese model delivering superior and detailed embeddings."])},"bge-small-en_description":n=>{const{normalize:e}=n;return e(["An efficient English model for streamlined and accurate embeddings."])},"bge-base-en_description":n=>{const{normalize:e}=n;return e(["A balanced English model designed for solid and reliable performance."])},"bge-large-en_description":n=>{const{normalize:e}=n;return e(["A top-performing English model crafted for premium quality embeddings."])},"bge-small-zh_description":n=>{const{normalize:e}=n;return e(["An agile Chinese model for efficient and precise embeddings."])},"bge-base-zh_description":n=>{const{normalize:e}=n;return e(["A versatile Chinese model combining efficiency and robust performance."])},"bge-large-zh_description":n=>{const{normalize:e}=n;return e(["A high-performance Chinese model optimized for top-tier embeddings."])},"bge-m3_description":n=>{const{normalize:e}=n;return e(["A versatile multilingual model offering expansive capabilities and high-quality embeddings."])},multimodal:n=>{const{normalize:e}=n;return e(["Multimodal"])},multimodal_explain:n=>{const{normalize:e}=n;return e(["This model can encode both text and image inputs, making it ideal for multimodal search tasks."])},"jina-clip-v1_description":n=>{const{normalize:e}=n;return e(["Our latest multimodal embeddings for text and image retrieval."])},"jina-reranker-v1-base-en_description":n=>{const{normalize:e}=n;return e(["Our first reranker model maximizing search and RAG relevance"])},"jina-reranker-v1-turbo-en_description":n=>{const{normalize:e}=n;return e(["The best combination of fast inference speed and accurate relevance scores"])},"jina-reranker-v1-tiny-en_description":n=>{const{normalize:e}=n;return e(["The fastest reranker model, best suited for ranking a large number of documents reliably"])},"jina-reranker-v2-base-multilingual_description":n=>{const{normalize:e}=n;return e(["The latest and best reranker model with multilingual, function calling and code search support."])},"jina-colbert-v1-en_description":n=>{const{normalize:e}=n;return e(["Improved ColBERT with 8K-token length for embedding and reranking tasks"])},multi_embedding:n=>{const{normalize:e}=n;return e(["Multi-vector"])},multi_embedding_explain:n=>{const{normalize:e}=n;return e(["This model will return a bag of contextualized embeddings for a given input. Each token in the input is mapped to a vector in the output."])},read_api_docs:n=>{const{normalize:e}=n;return e(["Read the docs"])},input:n=>{const{normalize:e}=n;return e(["Request"])},output:n=>{const{normalize:e}=n;return e(["Response"])},usage_rerank:n=>{const{normalize:e}=n;return e(["Usage"])},input_length:n=>{const{normalize:e}=n;return e(["Input length"])},output_dimension:n=>{const{normalize:e}=n;return e(["Output dimensions"])},token_length_explain:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["The maximum length of the input token sequence is ",r(o("_tokenLength"))," for this model."])},output_dim_explain:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["The output dimension of an embedding vector from this model is ",r(o("_outputDim")),"."])},size_explain:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["The number of parameters in the model is ",r(o("_size")),", note that this is not the size of the model file."])},language_explain:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["This model best supports ",r(o("_language"))," language."])},opensource:n=>{const{normalize:e}=n;return e(["OSS"])},opensource_explain:n=>{const{normalize:e}=n;return e(["This model is open source and available on Hugging Face. Click this button to view the model on Hugging Face."])},wait_for_processing:n=>{const{normalize:e}=n;return e(["Your request is being processed."])},you_can_leave:n=>{const{normalize:e}=n;return e(["You can leave this page and we will send you the download link upon completion."])},click_upload_btn_above:n=>{const{normalize:e}=n;return e(["Click the upload button above to start."])},start_batch:n=>{const{normalize:e}=n;return e(["Start batch embedding"])},visualization_example_you_can:n=>{const{normalize:e}=n;return e(["Use our API below, you can do it too!"])},start_embedding:n=>{const{normalize:e}=n;return e(["Index"])},maximize_tooltip:n=>{const{normalize:e}=n;return e(["Maximize this panel"])},show_api_key:n=>{const{normalize:e}=n;return e(["Show API Key"])},batch_job:n=>{const{normalize:e}=n;return e(["Batch Job"])},bulk_embedding_failed:n=>{const{normalize:e}=n;return e(["Fail to create batch embedding job"])},search:n=>{const{normalize:e}=n;return e(["Search"])},visualize:n=>{const{normalize:e}=n;return e(["Visualize"])},bulk:n=>{const{normalize:e}=n;return e(["Batch embed"])},what_are_embedding:n=>{const{normalize:e}=n;return e(["What are Embeddings?"])},what_are_embedding_answer:n=>{const{normalize:e}=n;return e([`Imagine teaching a computer to grasp the nuanced meanings of words and phrases. Traditional methods, which relied on rigid, rule-based systems, fell short because language is too complex and fluid. Enter text embeddings: a powerful solution that translates text into a language of numbers\u2014specifically, into vectors in a high-dimensional space.

Consider the phrases "sunny weather" and "clear skies." To us, they paint a similar picture. Through the lens of embeddings, these phrases are transformed into numerical vectors that reside close to each other in this multi-dimensional space, capturing their semantic kinship. This closeness in the vector space is not just about words or phrases being similar; it's about understanding context, sentiment, and even subtle nuances in meaning.

Why is this breakthrough important? For starters, it bridges the gap between the richness of human language and the computational efficiency of algorithms. Algorithms excel at crunching numbers, not interpreting texts. By converting text into vectors, embeddings make it possible for these algorithms to 'understand' and process language in a way that was previously out of reach.

The practical applications are vast and varied. Whether it's recommending content that resonates with your interests, powering conversational AI that feels surprisingly human, or even detecting subtle patterns in large volumes of text, embeddings are the key. They enable machines to perform tasks like sentiment analysis, language translation, and much more, with an understanding of language that is increasingly nuanced and refined.`])},open_tensorboard:n=>{const{normalize:e}=n;return e(["Open visualizer"])},visualization_example:n=>{const{normalize:e}=n;return e(["Embedding all sentences from this section to a 3D vector space"])},visualize_done:n=>{const{normalize:e}=n;return e(["Visualization is done, you can now click the top button to open the visualizer."])},more_than_two2:n=>{const{normalize:e}=n;return e(["Please enter more than two documents, i.e. more than two lines."])},generating_visualization:n=>{const{normalize:e}=n;return e(["Generating visualization..."])},query:n=>{const{normalize:e}=n;return e(["Query"])},document:n=>{const{normalize:e}=n;return e(["Document"])},pairwise_test:n=>{const{normalize:e}=n;return e(["Pairwise"])},search_hint:n=>{const{normalize:e}=n;return e(["Type to search within the sentences listed below"])},original_documents:n=>{const{normalize:e}=n;return e(["Sentences to embed"])},total_documents:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Embedding progress: ",r(o("_Processed")),"/",r(o("_Count"))," sentences."])},embedding_done:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Successfully embedded ",r(o("_Count"))," sentences."])},please_fill_docs_first:n=>{const{normalize:e}=n;return e(["Please first enter some sentences below before search."])},original_documents_hint:n=>{const{normalize:e}=n;return e(["Enter your sentences here. Each new line will be considered a separate sentence/document."])},upload_file:n=>{const{normalize:e}=n;return e(["Click here to upload a file"])},max_file_size:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Maximum size allowed: ",r(o("_maxSize")),"."])},write_email_here:n=>{const{normalize:e}=n;return e(["Please enter the email where you want to receive the download link upon completion."])},upload:n=>{const{normalize:e}=n;return e(["Upload"])},download:n=>{const{normalize:e}=n;return e(["Download"])},batch_upload_hint:n=>{const{normalize:e}=n;return e(["We will use the API key and the model below to process the documents."])},autostart:n=>{const{normalize:e}=n;return e(["Embedding will automatically start after a brief delay"])},learn_more:n=>{const{normalize:e}=n;return e(["Learn more"])},why_do_you_need:n=>{const{normalize:e}=n;return e(["Choosing the Right Embeddings"])},why_do_you_need_before:n=>{const{normalize:e}=n;return e(["Our embedding models are designed to cover diverse search and GenAI applications."])},why_do_you_need_after:n=>{const{normalize:e}=n;return e(["Leveraging deep neural networks and LLMs, our embedding models represent multimodal data into a streamlined format, improving machine comprehension, efficient storage and enabling advanced AI applications. These embeddings play a crucial role in understanding the data, enhancing user engagement, overcoming language barriers, and optimizing development processes."])},why_need_1_title:n=>{const{normalize:e}=n;return e(["General-Purpose Embeddings"])},why_need_4_title:n=>{const{normalize:e}=n;return e(["Multimodal Embeddings"])},why_need_4_description:n=>{const{normalize:e}=n;return e(["Jina CLIP is our latest multimodal embedding model for image and text. A big improvement over OpenAI CLIP is that this single model can be used for text-text retrieval, as well as text-image, image-text, and image-image retrieval tasks! So one model, two modalities, four search directions!"])},why_need_1_description:n=>{const{normalize:e}=n;return e(["Our core embedding model, powered by JinaBERT, is built for a broad spectrum of applications. It excels in understanding detailed text, making it ideal for semantic search, content classification, and intricate language analysis. Its versatility is unmatched, supporting the creation of advanced sentiment analysis tools, text summarization, and personalized recommendation systems."])},why_need_2_title:n=>{const{normalize:e}=n;return e(["Bilingual Embeddings"])},why_need_2_description:n=>{const{normalize:e}=n;return e(["Our bilingual models facilitate communication across languages, enhancing multilingual platforms, global customer support, and cross-lingual content discovery. Designed to master German-English and Chinese-English translations, these models simplify interactions and foster understanding among diverse linguistic groups."])},why_need_3_title:n=>{const{normalize:e}=n;return e(["Code Embeddings"])},why_need_3_description:n=>{const{normalize:e}=n;return e(["Tailored for developers, our code embedding model optimizes coding tasks like summarization, code generation, and automatic reviews. It boosts productivity by offering deeper insights into code structures and suggesting improvements, making it essential for developing advanced IDE plugins, automatic documentation, and cutting-edge debugging tools."])},top_up_warning_message1:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["The current API key has ",r(o("_remainedTokens"))," tokens remaining and will be replaced by a new key with ",r(o("_freeTokens"))," tokens. You may continue to use or top up the old key if you have stored it securely. How do you want to proceed?"])},top_up_warning_title:n=>{const{normalize:e}=n;return e(["Replace Old API Key"])},top_up_button:n=>{const{normalize:e}=n;return e(["Top Up Old Key"])},top_up_button_explain:n=>{const{normalize:e}=n;return e(["Integrating this API key offers a more professional solution, eliminating the need for frequent key changes. Usage data is retained and accessible at any time."])},get_new_key_button_explain:n=>{const{normalize:e}=n;return e(["Opting for a new key will result in the loss of usage history associated with the old key."])},get_new_key_button:n=>{const{normalize:e}=n;return e(["Get New Key"])},cancel_button:n=>{const{normalize:e}=n;return e(["Cancel"])},"1M_free":n=>{const{normalize:e}=n;return e(["1M free tokens"])},free:n=>{const{normalize:e}=n;return e(["Free"])},Free1M:n=>{const{normalize:e}=n;return e(["1M tokens"])},"1M_free_description":n=>{const{normalize:e}=n;return e(["Enjoy free tokens in your new API key, no credit card needed."])},protectData1:n=>{const{normalize:e}=n;return e(["Request data and documents are not used for training models."])},protectData2:n=>{const{normalize:e}=n;return e(["Data encryption in transit (TLS 1.2+) and at rest (AES-GCM 256)."])},protectData3:n=>{const{normalize:e}=n;return e(["SOC 2 and GDPR compliant."])},multilingual:n=>{const{normalize:e}=n;return e(["Multilingual support"])},protect_data:n=>{const{normalize:e}=n;return e(["Protect Your Data"])},feature_multilingual:n=>{const{normalize:e}=n;return e(["Offering bilingual models for German-English, Chinese-English, Spanish-English among others, ideal for cross-lingual applications."])},add_pair:n=>{const{normalize:e}=n;return e(["New"])},poster:n=>{const{normalize:e}=n;return e(["The Evolution of Embeddings Poster"])},poster_description:n=>{const{normalize:e}=n;return e(["Discover the ideal poster for your space, featuring captivating infographics or breathtaking visuals tracing the evolution of text embedding models since 1950."])},buy_poster:n=>{const{normalize:e}=n;return e(["Buy a hard copy"])},learn_poster:n=>{const{normalize:e}=n;return e(["Learn how we made it"])},delete_pair:n=>{const{normalize:e}=n;return e(["Delete"])},length:n=>{const{normalize:e}=n;return e(["Token length"])},debugging:n=>{const{normalize:e}=n;return e(["Test"])},text1:n=>{const{normalize:e}=n;return e(["Left"])},text2:n=>{const{normalize:e}=n;return e(["Right"])},new:n=>{const{normalize:e}=n;return e(["New model"])},edit_text1_text:n=>{const{normalize:e}=n;return e(["Edit left text"])},edit_text2_text:n=>{const{normalize:e}=n;return e(["Edit right text"])},no_data1:n=>{const{normalize:e}=n;return e(["Add a pair of sentences to calculate the similarity"])},cosine_similarity:n=>{const{normalize:e}=n;return e(["Cosine similarity"])},"jina-embeddings-v2-base-es_description":n=>{const{normalize:e}=n;return e(["Spanish-English bilingual embeddings with SOTA performance"])},"jina-embeddings-v2-base-code_description":n=>{const{normalize:e}=n;return e(["Optimized for code and docstring search"])},"jina-embeddings-v2-small-en_description":n=>{const{normalize:e}=n;return e(["Optimized for low latency and memory footprint"])},"jina-embeddings-v2-base-en_description":n=>{const{normalize:e}=n;return e(["On par with OpenAI's text-embedding-ada002"])},"jina-embeddings-v2-base-zh_description":n=>{const{normalize:e}=n;return e(["Chinese-English bilingual embeddings with SOTA performance"])},"jina-embeddings-v2-base-de_description":n=>{const{normalize:e}=n;return e(["German-English bilingual embeddings with SOTA performance"])},learning1:n=>{const{normalize:e}=n;return e(["Learning about Embeddings"])},learning1_description:n=>{const{normalize:e}=n;return e(["Where to start with embeddings? We've got you covered. Learn about embeddings from the ground up with our comprehensive guide."])},feature_solid:n=>{const{normalize:e}=n;return e(["Best-in-class"])},feature_solid_description1:n=>{const{normalize:e}=n;return e(["Developed from our cutting-edge academic research and rigorously tested against the SOTA models to ensure unparalleled performance."])},feature_8k1:n=>{const{normalize:e}=n;return e(["8192 token-length"])},feature_8k_description1:n=>{const{normalize:e}=n;return e(["Pioneering the first open-source embedding model with an 8192-token length, enabling the representation of an entire chapter in a single vector."])},feature_top_perform1:n=>{const{normalize:e}=n;return e(["Seamless integration"])},feature_top_perform_description1:n=>{const{normalize:e}=n;return e(["Fully compatible with OpenAI's API. Effortlessly integrates with over 10 vector databases and RAG systems for a smooth user experience."])},feature_cheap:n=>{const{normalize:e}=n;return e(["50x cheaper"])},feature_cheap_v1:n=>{const{normalize:e}=n;return e(["5x more affordable"])},feature_cheap_v1_description1:n=>{const{normalize:e}=n;return e(["Start with free trials and enjoy a straightforward pricing structure. Get access to powerful embeddings for just 20% of OpenAI's cost."])},feature_on_premises:n=>{const{normalize:e}=n;return e(["Privacy first"])},feature_on_premises_description1:n=>{const{normalize:e}=n;return e(["Seamlessly deploy our embedding models directly within your Virtual Private Cloud (VPC). Currently supported on AWS Sagemaker, with forthcoming integrations for Microsoft Azure and Google Cloud Platform. For tailored Kubernetes deployments, reach out to our sales team for specialized assistance."])},feature_on_premises_description2:n=>{const{normalize:e}=n;return e(["Deploy Jina Embeddings models in AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description3:n=>{const{normalize:e}=n;return e(["Deploy Jina Embeddings models in AWS Sagemaker and Microsoft Azure, and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description4:n=>{const{normalize:e}=n;return e(["Deploy Jina Embedding and Reranker models on-premises using AWS SageMaker, Microsoft Azure, or Google Cloud Services, ensuring your data remains securely in your control."])},vector_database_integration1:n=>{const{normalize:e}=n;return e(["Integrations"])},integrate:n=>{const{normalize:e}=n;return e(["Integrate"])},vector_database_integration_description:n=>{const{normalize:e}=n;return e(["Seamlessly and easily integrate the Jina Embeddings API with any of these databases, frameworks and applications below. Our tutorials will show you how."])},vector_database_integration2:n=>{const{normalize:e}=n;return e(["Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start."])},vector_database_integration3:n=>{const{normalize:e}=n;return e(["Our Embedding & Reranker API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start."])},api_integration_short:n=>{const{normalize:e}=n;return e(["Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks."])},title:n=>{const{normalize:e}=n;return e(["Embedding API"])},description:n=>{const{normalize:e,linked:r,type:o}=n;return e([r("landing_page.embedding_desc1",void 0,o)])},key_enter_placeholder_to_topup:n=>{const{normalize:e}=n;return e(["Enter the API key you wish to recharge"])},key_to_top_up:n=>{const{normalize:e}=n;return e(["Already have a key? Put it here to recharge"])},key_enter_placeholder:n=>{const{normalize:e}=n;return e(["Please enter your API key"])},key_warn_v2:n=>{const{normalize:e}=n;return e(["Make sure to store your API key at a safe place!"])},last_7_days:n=>{const{normalize:e}=n;return e(["Usage in last 7 days"])},key_warn:n=>{const{normalize:e}=n;return e(["Make sure to store your API key at a safe place. Otherwise you will need to generate a new key"])},refresh_key_tooltip1:n=>{const{normalize:e}=n;return e(["Get a new API key for free"])},regenerate:n=>{const{normalize:e}=n;return e(["Regenerate"])},retry:n=>{const{normalize:e}=n;return e(["Retry"])},refresh:n=>{const{normalize:e}=n;return e(["Refresh"])},generate_api_key_error:n=>{const{normalize:e}=n;return e(["Fail to generate an API key"])},key:n=>{const{normalize:e}=n;return e(["API key"])},code:n=>{const{normalize:e}=n;return e(["code"])},manage_quota1:n=>{const{normalize:e}=n;return e(["API Key & Billing"])},manage_billing:n=>{const{normalize:e}=n;return e(["Manage invoice"])},manage_billing_tip:n=>{const{normalize:e}=n;return e(["Manage your billing information, get invoices, and set up auto-recharge."])},size:n=>{const{normalize:e}=n;return e(["Parameters"])},output_dim:n=>{const{normalize:e}=n;return e(["Dimensions"])},remaining:n=>{const{normalize:e}=n;return e(["Available tokens"])},usage:n=>{const{normalize:e}=n;return e(["Usage"])},api_integrations:n=>{const{normalize:e}=n;return e(["API Integrations"])},usage_history:n=>{const{normalize:e}=n;return e(["Usage in last 7 days"])},view_details:n=>{const{normalize:e}=n;return e(["View Details"])},input_api_key_error1:n=>{const{normalize:e}=n;return e(["Your API key is not valid!"])},usage_time:n=>{const{normalize:e}=n;return e(["Date time"])},usage_amount:n=>{const{normalize:e}=n;return e(["Tokens"])},usage_reason:n=>{const{normalize:e}=n;return e(["Description"])},usage_reason_trial:n=>{const{normalize:e}=n;return e(["Trial"])},usage_reason_consume:n=>{const{normalize:e}=n;return e(["Used"])},usage_reason_purchase:n=>{const{normalize:e}=n;return e(["Purchased"])},usage_history_explain:n=>{const{normalize:e}=n;return e(["Data is not in real-time and can be few minutes delayed."])},wait_stripe:n=>{const{normalize:e}=n;return e(["Opening Stripe payment, please wait"])},refresh_token_count1:n=>{const{normalize:e}=n;return e(["Refresh to get available tokens of current API key"])},what_is_a_token:n=>{const{normalize:e}=n;return e(['A token in text processing is a unit, often a word. For example, "Jina AI is great!" becomes five tokens, including the punctuation.'])},token_example:n=>{const{normalize:e}=n;return e([`A tweet is about 20 tokens, a news article is about 1000 tokens, and Charles Dickens' novel "A Tale of Two Cities" has over a million tokens.`])},buy_more_quota:n=>{const{normalize:e}=n;return e(["Top up this API key with more tokens"])},tokens:n=>{const{normalize:e}=n;return e(["Tokens"])},"500M tokens":n=>{const{normalize:e}=n;return e(["500M tokens"])},"1M tokens":n=>{const{normalize:e}=n;return e(["1 Million"])},"1M tokens_targetUser":n=>{const{normalize:e}=n;return e(["Toy Experiment"])},"1B tokens_targetUser":n=>{const{normalize:e}=n;return e(["Prototype Development"])},"11B tokens_targetUser":n=>{const{normalize:e}=n;return e(["Production Deployment"])},"1M tokens_intuition1":n=>{const{normalize:e}=n;return e(['Equivalent to reading the entire text of "The Hobbit" and "The Great Gatsby".'])},"500M tokens_intuition1":n=>{const{normalize:e}=n;return e(['Similar to watching every episode of "The Simpsons" from season 1 to season 30.'])},"1B tokens_intuition1":n=>{const{normalize:e}=n;return e(['About the same as reading the complete works of Shakespeare and the entire "Harry Potter" series.'])},"2_5B tokens_intuition1":n=>{const{normalize:e}=n;return e([`Comparable to transcribing every word spoken in the movie "The Lord of the Rings" trilogy 1,000 times.
`])},"5_5B tokens_intuition1":n=>{const{normalize:e}=n;return e(["Equivalent to reading the entire text of the Encyclopaedia Britannica."])},"11B tokens_intuition1":n=>{const{normalize:e}=n;return e(["Similar to reading all the English language articles on Wikipedia."])},"59B tokens_intuition1":n=>{const{normalize:e}=n;return e(["Equal to all tweets posted worldwide over a two-day period."])},"1B tokens":n=>{const{normalize:e}=n;return e(["1 Billion"])},"2_5B tokens":n=>{const{normalize:e}=n;return e(["2.5B tokens"])},"5_5B tokens":n=>{const{normalize:e}=n;return e(["5.5B tokens"])},"11B tokens":n=>{const{normalize:e}=n;return e(["11 Billion"])},"59B tokens":n=>{const{normalize:e}=n;return e(["59B tokens"])},per_k:n=>{const{normalize:e}=n;return e(["/ 1K tokens"])},per_m:n=>{const{normalize:e}=n;return e(["/ 1M tokens"])},faq:n=>{const{normalize:e,linked:r,type:o}=n;return e([r("contact_us_page.faq",void 0,o)])},file_type_not_supported:n=>{const{normalize:e}=n;return e(["File type not supported"])},file_size_exceed:n=>{const{normalize:e,interpolate:r,named:o}=n;return e(["Exceed max file size ",r(o("_size"))])},model_required:n=>{const{normalize:e}=n;return e(["Model is required"])},file_required:n=>{const{normalize:e}=n;return e(["File is required"])},faqs_v2:{title:n=>{const{normalize:e}=n;return e(["Embeddings-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["What is jina-clip-v1, can I use it for search text and image?"])},answer1:n=>{const{normalize:e}=n;return e(["Jina CLIP <code>jina-clip-v1</code> is the latest multimodal embedding model that supports text-text, text-image, image-image, and image-text retrieval tasks. Unlike OpenAI CLIP model that falls short on text-text search, Jina CLIP is trained to be your text retriever. You can read more about it from our tech report."])},question0:n=>{const{normalize:e}=n;return e(["How were the jina-embeddings-v2 models trained?"])},answer0:n=>{const{normalize:e}=n;return e(["For detailed information on our training processes, data sources, and evaluations, please refer to our technical report available on arXiv."])},question3:n=>{const{normalize:e}=n;return e(["Which languages do your models support?"])},answer3:n=>{const{normalize:e}=n;return e(["Our models support English, German, Spanish, Chinese, various programming languages and images. For more details, please refer to our publication on bilingual models."])},question4:n=>{const{normalize:e}=n;return e(["What is the maximum length for a single sentence input?"])},answer4:n=>{const{normalize:e}=n;return e(["Our models allow for an input length of up to 8192 tokens, which is significantly higher than most other models. A token can range from a single character, like 'a', to an entire word, such as 'apple'. The total number of characters that can be input depends on the length and complexity of the words used. This extended input capability enables our jina-embeddings-v2 models to perform more comprehensive text analysis and achieve higher accuracy in context understanding, especially for extensive textual data."])},question5:n=>{const{normalize:e}=n;return e(["What is the maximum number of sentences I can include in a single request?"])},question6:n=>{const{normalize:e}=n;return e(["How do I send images to the jina-clip-v1 model?"])},answer6:n=>{const{normalize:e}=n;return e(["You can use either <code>url</code> or <code>bytes</code> in the <code>input</code> field of the API request. For <code>url</code>, provide the URL of the image you want to process. For <code>bytes</code>, encode the image in base64 format and include it in the request. The model will return the embeddings of the image in the response."])},answer5:n=>{const{normalize:e}=n;return e(["A single API call can process up to 2048 sentences or texts, facilitating extensive text analysis in one request."])},question7:n=>{const{normalize:e}=n;return e(["How do Jina Embeddings models compare to OpenAI's text-embedding-ada-002 model?"])},answer7:n=>{const{normalize:e}=n;return e(["According to the MTEB Leaderboard, our Base model competes closely with OpenAI\u2019s text-embedding-ada-002, exhibiting comparable performance on average. Furthermore, our Base model excels in several tasks, including classification, pair-classification, re-ranking, and summarization, outperforming OpenAI\u2019s model."])},question8:n=>{const{normalize:e}=n;return e(["How seamless is the transition from OpenAI's text-embedding-ada-002 to your solution?"])},answer8:n=>{const{normalize:e}=n;return e(["The transition is streamlined, as our API endpoint, https://api.jina.ai/v1/embeddings, matches the input and output JSON schemas of OpenAI\u2019s text-embeddings-ada-002 model. This compatibility ensures users can easily replace the OpenAI model with ours when using OpenAI\u2019s endpoint."])},question9:n=>{const{normalize:e}=n;return e(["How tokens are calculated when using jina-clip-v1?"])},answer9:n=>{const{normalize:e}=n;return e([`The tokens are calculated based on the text length and image size. For text in the request, tokens are counted in the standard way. For image in the request, the following steps are conducted:
1.	Tile Size: Each image is divided into tiles of size 224x224 pixels.
	2.	Coverage: The number of tiles required to completely cover the input image is calculated. Even if the image dimensions are not perfectly divisible by 224, we will count partial tiles as full tiles.
	3.	Total Tiles: The total number of tiles covering the image determines the cost. For instance, if an image is 500x500 pixels, it would be covered by 3x3 tiles, resulting in 9 tiles.
	4.	Cost Calculation: Each tile contributes to the final cost of processing the image. The cost per tile is 1000 tokens.

Example:
For an image with dimensions 500x500 pixels:

	\u2022	The image is divided into 224x224 pixel tiles.
	\u2022	The total number of tiles required is 3 (horizontal) x 3 (vertical) = 9 tiles.
	\u2022	The cost will be 9*1000 = 9000 tokens`])},question17:n=>{const{normalize:e}=n;return e(["Do you provide models for embedding images or audio?"])},answer17:n=>{const{normalize:e}=n;return e(["Yes, jina-clip-v1 can embed both images and texts. Embedding models on more modalities will be announced soon!"])},question18:n=>{const{normalize:e}=n;return e(["Can Jina Embedding models be fine-tuned with private or company data?"])},answer18:n=>{const{normalize:e}=n;return e(["For inquiries about fine-tuning our models with specific data, please contact us to discuss your requirements. We are open to exploring how our models can be adapted to meet your needs."])},question19:n=>{const{normalize:e}=n;return e(["Can your endpoints be hosted privately on AWS, Azure, or GCP?"])},answer19:n=>{const{normalize:e}=n;return e(["Yes, our services are available on the AWS marketplace, and we are in the process of expanding to Azure and GCP marketplaces. If you have particular requirements, please contact us at sales AT jina.ai."])}},normalized:n=>{const{normalize:e}=n;return e(["L2 normalization"])},normalized_explain:n=>{const{normalize:e}=n;return e(["Scales the embedding so its Euclidean (L2) norm becomes 1, preserving direction. Useful when downstream involves dot-product, classification, visualization."])},compatible:n=>{const{normalize:e}=n;return e(["Compatible mode"])},compatible_explain:n=>{const{normalize:e}=n;return e(["Follows the same request format as our text embedding models. This allows you to switch between models without changing the request. Note, image input is not supported in this mode."])}},billing_general_faq:{title:n=>{const{normalize:e}=n;return e(["Billing-related common questions"])},question9:n=>{const{normalize:e}=n;return e(["Is billing based on the number of sentences or requests?"])},answer9:n=>{const{normalize:e}=n;return e(["Our pricing model is based on the total number of tokens processed, allowing users the flexibility to allocate these tokens across any number of sentences, offering a cost-effective solution for diverse text analysis requirements."])},question10:n=>{const{normalize:e}=n;return e(["Is there a free trial available for new users?"])},answer10:n=>{const{normalize:e}=n;return e(["We offer a welcoming free trial to new users, which includes one million tokens for use with any of our models, facilitated by an auto-generated API key. Once the free token limit is reached, users can easily purchase additional tokens for their API keys via the 'Buy tokens' tab."])},question13:n=>{const{normalize:e}=n;return e(["Are tokens charged for failed requests?"])},answer13:n=>{const{normalize:e}=n;return e(["No, tokens are not deducted for failed requests."])},question14:n=>{const{normalize:e}=n;return e(["What payment methods are accepted?"])},answer14:n=>{const{normalize:e}=n;return e(["Payments are processed through Stripe, supporting a variety of payment methods including credit cards, Google Pay, and PayPal for your convenience."])},question15:n=>{const{normalize:e}=n;return e(["Is invoicing available for token purchases?"])},answer15:n=>{const{normalize:e}=n;return e(["Yes, an invoice will be issued to the email address associated with your Stripe account upon the purchase of tokens."])}},api_general_faq:{title:n=>{const{normalize:e}=n;return e(["API-related common questions"])},question1:n=>{const{normalize:e}=n;return e(["Can I use the same API key for embedding, reranking, reader, fine-tuning APIs?"])},answer1:n=>{const{normalize:e}=n;return e(["Yes, the same API key is valid for all search foundation products from Jina AI. This includes the embedding, reranking, reader and fine-tuning APIs, with tokens shared between the all services."])},question5:n=>{const{normalize:e}=n;return e(["Do API keys expire?"])},answer5:n=>{const{normalize:e}=n;return e(["No, our API keys do not have an expiration date. However, if you suspect your key has been compromised and wish to retire it or transfer its tokens to a new key, please contact our support team for assistance."])},question3:n=>{const{normalize:e}=n;return e(["Can I monitor the token usage of my API key?"])},answer3:n=>{const{normalize:e}=n;return e(["Yes, token usage can be monitored in the 'Buy tokens' tab by entering your API key, allowing you to view the usage history and remaining tokens."])},question4:n=>{const{normalize:e}=n;return e(["What should I do if I forget my API key?"])},answer4:n=>{const{normalize:e}=n;return e(["If you have misplaced a topped-up key and wish to retrieve it, please contact support AT jina.ai with your registered email for assistance."])},question6:n=>{const{normalize:e}=n;return e(["Why is the first request for some models slow?"])},answer6:n=>{const{normalize:e}=n;return e(["This is because our serverless architecture offloads certain models during periods of low usage. The initial request activates or 'warms up' the model, which may take a few seconds. After this initial activation, subsequent requests process much more quickly."])},question12:n=>{const{normalize:e}=n;return e(["Is user input data used for training your models?"])},answer12:n=>{const{normalize:e}=n;return e(["We adhere to a strict privacy policy and do not use user input data for training our models."])}}};export{t as default};

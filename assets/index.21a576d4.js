var t={integrations:{embedding:e=>{const{normalize:n}=e;return n(["Embeddings"])},reranker:e=>{const{normalize:n}=e;return n(["Reranker"])},which_to_go:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Which one to integrate with ",r(o("_vendor")),"?"])}},searchbar:{results:e=>{const{normalize:n}=e;return n(["results"])},more_results:e=>{const{normalize:n,interpolate:r,named:o}=e;return n([r(o("_numMore"))," more results"])},placeholder:e=>{const{normalize:n}=e;return n(["Type your question about this page"])},proposing_solution:e=>{const{normalize:n}=e;return n(["Crafting answer from the page content..."])},ask_on_current_page:e=>{const{normalize:n}=e;return n(["Ask the current page about..."])},find_solution:e=>{const{normalize:n}=e;return n(["Generate a solution for..."])},hotkey:e=>{const{normalize:n}=e;return n(["Use / key for quick questions"])},hotkey1:e=>{const{normalize:n}=e;return n(["Use"])},hotkey2:e=>{const{normalize:n}=e;return n(["to toggle"])},hint:e=>{const{normalize:n}=e;return n(["Type your question here..."])},hotkey_long1:e=>{const{normalize:n}=e;return n(["At any time, press"])},hotkey_long3:e=>{const{normalize:n}=e;return n(["to open search bar"])},required:e=>{const{normalize:n}=e;return n(["Please describe your question with more details."])}},SEO_TAG_LINE:e=>{const{normalize:n}=e;return n(["Your Search Foundation, Supercharged."])},PRODUCT_DESCRIPTION:e=>{const{normalize:n}=e;return n(["Jina AI offers best-in-class embeddings, reranker and prompt optimizer, enabling advanced multimodal AI."])},notice:e=>{const{normalize:n}=e;return n(['\u{1F389} Our first book, "Neural Search \u2014 From Prototype to Production with Jina" is officially out today!'])},purchase_now:e=>{const{normalize:n}=e;return n(["Purchase now"])},copy:e=>{const{normalize:n}=e;return n(["Copy"])},copy_to_clipboard_success:e=>{const{normalize:n}=e;return n(["Copied to clipboard"])},powered_by:e=>{const{normalize:n}=e;return n(["Powered by"])},open_day:{title:e=>{const{normalize:n}=e;return n(["Open Day"])},description:e=>{const{normalize:n}=e;return n(["An exclusive opportunity to gain an insider's view of Jina AI."])},organization:e=>{const{normalize:n}=e;return n(["Organization"])},group_size:e=>{const{normalize:n}=e;return n(["Number of visitors"])},organization_website:e=>{const{normalize:n}=e;return n(["Organization website"])},organization_website_placeholder:e=>{const{normalize:n}=e;return n(["URL for your organization's homepage or LinkedIn profile"])},preferred_products:e=>{const{normalize:n}=e;return n(["Which products are you interested in?"])},preferred_date:e=>{const{normalize:n}=e;return n(["Preferred date"])},preferred_language:e=>{const{normalize:n}=e;return n(["Preferred language"])},subtitle:e=>{const{normalize:n}=e;return n(["A Glimpse into the Future of Multimodal AI"])},introduction:e=>{const{normalize:n}=e;return n(["Jina AI is delighted to open our doors to esteemed entities and organizations interested in the progress and future of Artificial Intelligence. We extend this exclusive opportunity for those in politics, NGOs, NPOs, and investment sectors to gain an insider's view of our operations and visions here at our Berlin headquarters."])},vision_title:e=>{const{normalize:n}=e;return n(["Our Vision for the Future"])},vision:e=>{const{normalize:n}=e;return n(["Join us for a comprehensive overview of the AI landscape as we see it. Our discussion will focus on the potential of Large Language Models, multimodal AI, and the impact of open-source technology in shaping the future of global innovation."])},experience:e=>{const{normalize:n}=e;return n(["We've arranged an immersive three-hour tour for our guests, available in German, English, French, Spanish, Chinese, and Russian. The tour covers an in-depth look into our advancements in multimodal AI, our perspective on the AI landscape, followed by a detailed examination of specific projects. We'll conclude with a group discussion to facilitate the exchange of ideas and insights. A lunch option is also available upon request."])},experience_title:e=>{const{normalize:n}=e;return n(["An Insider's Journey"])},impact_title:e=>{const{normalize:n}=e;return n(["Impact and Influence"])},impact:e=>{const{normalize:n}=e;return n(["Understand how our contributions to the open-source community and our work in multimodal AI technology are establishing Jina AI as an influential player in AI innovation. We aim to play a significant role in decision-making processes, ensuring that the advancement of AI technology benefits all."])},engage_title:e=>{const{normalize:n}=e;return n(["Engage with Us"])},engage:e=>{const{normalize:n}=e;return n(["We highly encourage an interactive dialogue throughout the day. The exchange of thoughts and perspectives is invaluable to us. Potential collaborations stemming from these discussions could significantly contribute to a more integrated and innovative future."])},tutor_title:e=>{const{normalize:n}=e;return n(["An Exclusive Deep Dive into"])},tutor_subtitle:e=>{const{normalize:n}=e;return n(["A meticulously curated three-hour tour, bringing you closer to the heart of Jina AI's groundbreaking work in multimodal AI technology."])},one_hour:e=>{const{normalize:n}=e;return n(["1 hour"])},motivation_to_attend_v2:e=>{const{normalize:n}=e;return n(["Why are you interested in our Open Day?"])},motivation_placeholder_v2:e=>{const{normalize:n}=e;return n(["Sharing your motivations will help us improve your experience."])},motivation_min_length_v1:e=>{const{normalize:n}=e;return n(["Please provide a more detailed motivation."])}},internship_faq:{question1:e=>{const{normalize:n}=e;return n(["Who can apply for the Jina AI internship program?"])},answer1:e=>{const{normalize:n}=e;return n(["Undergraduate, Masters, and Ph.D. students from all over the world, with interest in fields such as research, engineering, marketing, and sales, are encouraged to apply. We also welcome non-technical internships in marketing, sales, executive assistance, and more. We are seeking passionate individuals ready to pioneer multimodal AI with us."])},question2:e=>{const{normalize:n}=e;return n(["Where will the internship take place?"])},answer2:e=>{const{normalize:n}=e;return n(["Internships must be carried out onsite at one of our offices, which are located in Berlin, Beijing, and Shenzhen."])},question3:e=>{const{normalize:n}=e;return n(["Does Jina AI assist with visa processes?"])},answer3:e=>{const{normalize:n}=e;return n(["Yes, Jina AI offers reasonable assistance in the visa process for successful applicants."])},question4:e=>{const{normalize:n}=e;return n(["Does Jina AI provide any allowances or benefits for interns?"])},answer4:e=>{const{normalize:n}=e;return n(["Yes, Jina AI provides a reasonable amount of living cost coverage for interns during the internship period."])},question5:e=>{const{normalize:n}=e;return n(["Can I work on my Master's thesis during the internship at Jina AI?"])},answer5:e=>{const{normalize:n}=e;return n(["Yes, it is possible to work on your Master's thesis during your internship at Jina AI, typically applicable to students at German universities. However, you must have prior communication and agreement from your university's supervisor. Note that we do not help students find advisors."])},question6:e=>{const{normalize:n}=e;return n(["What does the application process involve?"])},answer6:e=>{const{normalize:n}=e;return n(["The application process includes submitting your application form, a resume, a cover letter expressing your interest and motivation, and any relevant professional links such as GitHub or LinkedIn. We evaluate candidates based on their performance during the interview and their performance in their university."])},question7:e=>{const{normalize:n}=e;return n(["Does Jina AI provide any letter of recommendation post-internship?"])},answer7:e=>{const{normalize:n}=e;return n(["Yes, successful interns may receive a letter of recommendation at the end of their internship, signed by our CEO."])},question8:e=>{const{normalize:n}=e;return n(["What is the duration of the internship?"])},answer8:e=>{const{normalize:n}=e;return n(["The duration of the internship varies based on the role and project. However, it typically ranges from three to six months."])},question9:e=>{const{normalize:n}=e;return n(["Can I apply if I don't have prior experience in AI?"])},answer9:e=>{const{normalize:n}=e;return n(["Yes, we welcome applications from all academic backgrounds. We value your passion and commitment to learn as much as prior experience."])},question10:e=>{const{normalize:n}=e;return n(["Is this a paid internship?"])},answer10:e=>{const{normalize:n}=e;return n(["Yes, our internship program offers competitive remuneration."])},question11:e=>{const{normalize:n}=e;return n(["What opportunities will I have as a Jina AI intern?"])},answer11:e=>{const{normalize:n}=e;return n(["As a Jina AI intern, you'll get hands-on experience working on challenging projects, learn from industry experts, be part of a vibrant community, and have the opportunity to make real contributions to our pioneering work in multimodal AI."])}},open_day_faq:{question1:e=>{const{normalize:n}=e;return n(["What languages do you offer for the tour?"])},answer1:e=>{const{normalize:n}=e;return n(["We offer tours in German, English, French, Spanish, Chinese, and Russian."])},question2:e=>{const{normalize:n}=e;return n(["What is the duration of the tour?"])},answer2:e=>{const{normalize:n}=e;return n(["The tour typically lasts for approximately three hours."])},question3:e=>{const{normalize:n}=e;return n(["Is lunch provided?"])},answer3:e=>{const{normalize:n}=e;return n(["Lunch is optional and can be arranged upon request."])},question4:e=>{const{normalize:n}=e;return n(["Can individuals register for the Open Day?"])},answer4:e=>{const{normalize:n}=e;return n(["Our Open Day is designed primarily for professional groups, such as politicians, NGOs, NPOs, and investors. However, we occasionally make exceptions based on the individual's profile."])},question5:e=>{const{normalize:n}=e;return n(["How many people can a group consist of for the Open Day?"])},answer5:e=>{const{normalize:n}=e;return n(["We can accommodate a variety of group sizes. Please indicate the size of your group in the registration form, and we will confirm the details with you."])},question6:e=>{const{normalize:n}=e;return n(["How can I specify areas of interest for the tour?"])},answer6:e=>{const{normalize:n}=e;return n(["There's a section in the registration form where you can specify your areas of interest or any special requests. We will do our best to tailor the tour according to your needs."])},question7:e=>{const{normalize:n}=e;return n(["Are tours available at your Beijing or Shenzhen offices?"])},answer7:e=>{const{normalize:n}=e;return n(["At this time, we only offer tours at our Berlin headquarter located in Kreuzberg. Our Beijing and Shenzhen offices are not currently open for tours."])}},header:{logos:e=>{const{normalize:n}=e;return n(["Download logo"])},open_in_full:e=>{const{normalize:n}=e;return n(["Show all enterprise products in a new window"])},products:e=>{const{normalize:n}=e;return n(["Products"])},news:e=>{const{normalize:n}=e;return n(["News"])},open_day:e=>{const{normalize:n}=e;return n(["Open day"])},for_power_users:e=>{const{normalize:n}=e;return n(["For Power Users"])},for_power_users_description:e=>{const{normalize:n}=e;return n(["Utilize our streamlined multimodal tools to enhance your productivity."])},power_users_others:e=>{const{normalize:n}=e;return n(["More power user tools"])},for_developers:e=>{const{normalize:n}=e;return n(["For Developers"])},for_developers_description:e=>{const{normalize:n}=e;return n(["Experience a comprehensive open-source multimodal AI stack designed for developers."])},developers_others:e=>{const{normalize:n}=e;return n(["More developer tools"])},for_enterprise:e=>{const{normalize:n}=e;return n(["For Enterprises"])},for_enterprise_description:e=>{const{normalize:n}=e;return n(["Discover scalable multimodal AI strategies tailored to meet business needs."])},enterprise_others:e=>{const{normalize:n}=e;return n(["More enterprise solutions"])},internship1:e=>{const{normalize:n}=e;return n(["Intern program"])},company:e=>{const{normalize:n}=e;return n(["Company"])},about_us:e=>{const{normalize:n}=e;return n(["About us"])},contact_us:e=>{const{normalize:n}=e;return n(["Contact sales"])},jobs:e=>{const{normalize:n}=e;return n(["Join us"])},join_discord:e=>{const{normalize:n}=e;return n(["Join our Discord community"])}},footer:{power_users:e=>{const{normalize:n}=e;return n(["Power Users"])},developers:e=>{const{normalize:n}=e;return n(["Developers"])},enterprise:e=>{const{normalize:n}=e;return n(["Enterprise"])},address_beijing:e=>{const{normalize:n}=e;return n(["Beijing, China"])},address_shenzhen:e=>{const{normalize:n}=e;return n(["Shenzhen, China"])},address_berlin:e=>{const{normalize:n}=e;return n(["Berlin, Germany (HQ)"])},offices:e=>{const{normalize:n}=e;return n(["Offices"])},docs:e=>{const{normalize:n}=e;return n(["Docs"])},company:e=>{const{normalize:n}=e;return n(["Company"])},all_rights_reserved:e=>{const{normalize:n}=e;return n(["All rights reserved."])},tc:e=>{const{normalize:n}=e;return n(["Terms & Conditions"])},tc1:e=>{const{normalize:n}=e;return n(["Terms"])},privacy:e=>{const{normalize:n}=e;return n(["Privacy"])},status:e=>{const{normalize:n}=e;return n(["Status"])},privacy_policy:e=>{const{normalize:n}=e;return n(["Privacy Policy"])},privacy_settings:e=>{const{normalize:n}=e;return n(["Privacy Settings"])}},prompt_perfect:{description:e=>{const{normalize:n}=e;return n(["Premier tool for prompt engineering"])},intro:e=>{const{normalize:n}=e;return n(["Premier tool for prompt engineering"])},intro1:e=>{const{normalize:n}=e;return n(["The premier tool for prompt engineering"])},original_title:e=>{const{normalize:n}=e;return n(["Original prompt"])},optimized_title:e=>{const{normalize:n}=e;return n(["Optimized prompt"])},original:e=>{const{normalize:n}=e;return n(["Your role is to be my brainstorming partner."])},optimized:e=>{const{normalize:n}=e;return n(["Your task is to be my brainstorming partner and provide creative ideas and suggestions for a given topic or problem. Your response should include original, unique, and relevant ideas that could help solve the problem or further explore the topic in an interesting way. Please note that your response should also take into account any specific requirements or constraints of the task."])},text_model:e=>{const{normalize:n}=e;return n(["Text models"])},image_model:e=>{const{normalize:n}=e;return n(["Image models"])}},jina_chat:{description:e=>{const{normalize:n}=e;return n(["More modality, longer memory, less cost"])},example_1:e=>{const{normalize:n}=e;return n(["Who are you?"])},example_2:e=>{const{normalize:n}=e;return n(["I'm a LLM chat service made by Jina AI"])}},scenex:{intro1:e=>{const{normalize:n}=e;return n(["Leading AI solution for image captions and video summaries"])},description:e=>{const{normalize:n}=e;return n(["Explore image storytelling beyond pixels"])},example1:e=>{const{normalize:n}=e;return n(["This video appears to be a nature footage featuring a charming white bunny and a butterfly in a grassy field. The bunny is seen interacting with the butterfly in different ways, showcasing their unique relationship. The natural surroundings provide a picturesque backdrop, enhancing the beauty of this simple yet captivating scene."])},caption_image_title:e=>{const{normalize:n}=e;return n(["Caption Image"])},caption_image_desc:e=>{const{normalize:n}=e;return n(["Generate a textual description of the image."])},json_image_title:e=>{const{normalize:n}=e;return n(["Extract JSON from Image"])},json_image_desc:e=>{const{normalize:n}=e;return n(["Generate a structured JSON format from the image using a predefined schema. This allows for specific data extraction from the image."])},visual_q_a_title:e=>{const{normalize:n}=e;return n(["Visual Q&A"])},visual_q_a_desc:e=>{const{normalize:n}=e;return n(["Answer a query based on the image's content."])},summarize_video_title:e=>{const{normalize:n}=e;return n(["Summarize Video"])},summarize_video_desc:e=>{const{normalize:n}=e;return n(["Generate a concise summary of the video, highlighting key events."])},generate_story_title:e=>{const{normalize:n}=e;return n(["Generate Story"])},generate_story_desc:e=>{const{normalize:n}=e;return n(["Craft a story inspired by the image, often featuring dialogues or monologues of its characters."])}},rationale:{description:e=>{const{normalize:n}=e;return n(["Ultimate AI decision-making tools"])},intro:e=>{const{normalize:n}=e;return n(["See two sides of the coin, make rational decisions"])},decision:e=>{const{normalize:n}=e;return n(["Decision"])}},best_banner:{description:e=>{const{normalize:n}=e;return n(["Blog to banner, without the prompts!"])},example_title:e=>{const{normalize:n}=e;return n(["Alice's Adventures in Wonderland - Chapter 1"])},example_description:e=>{const{normalize:n}=e;return n(["Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \u201Cand what is the use of a book,\u201D thought Alice \u201Cwithout pictures or conversations?\u201D So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her."])}},doc_array:{description:e=>{const{normalize:n}=e;return n(["The data structure for multimodal data"])}},jina:{description:e=>{const{normalize:n}=e;return n(["Build multimodal AI applications on the cloud"])}},finetuner:{description:e=>{const{normalize:n}=e;return n(["Fine-tune embeddings on domain specific data for better search quality"])},intro:e=>{const{normalize:n}=e;return n(["On-prem embedding-tuning for your company on your data"])}},hub:{description:e=>{const{normalize:n}=e;return n(["Share and discover building blocks for multimodal AI applications"])}},clip_as_service:{description:e=>{const{normalize:n}=e;return n(["Embed images and sentences into fixed-length vectors with CLIP"])}},dalle_flow:{description:e=>{const{normalize:n}=e;return n(["A human-in-the-Loop workflow for creating HD images from text"])}},disco_art:{description:e=>{const{normalize:n}=e;return n(["Create compelling Disco Diffusion artworks in one line of code"])}},think_gpt:{description:e=>{const{normalize:n}=e;return n(["Agent techniques to augment your LLM and push it beyond its limits"])}},jcloud:{description:e=>{const{normalize:n}=e;return n(["Deploy a local project as a cloud service. Radically easy, no nasty surprises."])}},"dev-gpt":{description:e=>{const{normalize:n}=e;return n(["Your virtual development team"])}},langchain_serve:{description:e=>{const{normalize:n}=e;return n(["Langchain apps on production with Jina & FastAPI"])}},vectordb:{description:e=>{const{normalize:n}=e;return n(["A Python vector database you just need - no more, no less"])}},open_gpt:{description:e=>{const{normalize:n}=e;return n(["An open-source cloud-native of large multimodal models serving framework"])}},jerboa:{description:e=>{const{normalize:n}=e;return n(["An experimental finetuner for open-source LLMs"])}},finetuner_plus:{description:e=>{const{normalize:n}=e;return n(["Empower your enterprise with on-premise finetuning solutions"])}},inference:{description:e=>{const{normalize:n}=e;return n(["State-of-the-art multimodal models available for inference"])}},cloud:{description:e=>{const{normalize:n}=e;return n(["Cloud hosting platform for multimodal AI applications"])}},semantic:{description:e=>{const{normalize:n}=e;return n(["Bridging the semantic gap in your existing search infrastructure"])}},searchscape:{description:e=>{const{normalize:n}=e;return n(["Navigate, interact, refine: reimagine product discovery"])}},reranker:{table:{title:e=>{const{normalize:n}=e;return n(["Below is the time cost of reranking one query and 100 documents in milliseconds:"])},number_token_document:e=>{const{normalize:n}=e;return n(["Number of tokens in each document"])},number_token_query:e=>{const{normalize:n}=e;return n(["Number of tokens in the query"])}},faq_v1:{title:e=>{const{normalize:n}=e;return n(["Reranker-related common questions"])},question1:e=>{const{normalize:n}=e;return n(["How much does the Reranker API cost?"])},answer1:e=>{const{normalize:n}=e;return n(["The pricing for the Reranker API is aligned with our Embedding API pricing structure. It begins with 1 million free tokens for each new API key. Beyond the free tokens, different packages are available for purchase. For more details, please visit our pricing section."])},question3:e=>{const{normalize:n}=e;return n(["What is the difference between the two rerankers?"])},answer3:e=>{const{normalize:n}=e;return n(["The primary difference lies in their architecture. For performance, we recommend jina-reranker-v1, which has been extensively tested and benchmarked against competitors. Jina-reranker-v1 utilizes a cross-encoder architecture, while Jina-colbert-v1 is based on the ColBERTv2 architecture but extends the context length of both the query and document to 8192, achieving even better performance than the original ColBERTv2 model."])},question4:e=>{const{normalize:n}=e;return n(["Is Jina Reranker open source?"])},answer4:e=>{const{normalize:n}=e;return n(["Yes, jina-colbert-v1 is open source and can be accessed via Huggingface. However, jina-reranker-v1 is not open source."])},question5:e=>{const{normalize:n}=e;return n(["Does the reranker support multiple languages?"])},answer5:e=>{const{normalize:n}=e;return n(["Currently, it supports English only. However, some users have reported that it also works well with Chinese. This may be partly because jina-reranker-v1-base-en shares some weights with our jina-embeddings-v2-base-zh embedding model."])},question6:e=>{const{normalize:n}=e;return n(["What is the maximum length for queries and documents?"])},answer6:e=>{const{normalize:n}=e;return n(["The maximum query token length is 512. There is no token limit for documents."])},question7:e=>{const{normalize:n}=e;return n(["What is the maximum number of documents I can rerank per query?"])},answer7:e=>{const{normalize:n}=e;return n(["You can rerank up to 2048 documents per query."])},question8:e=>{const{normalize:n}=e;return n(["What is the batch size and how many query-document tuples can I send in one request?"])},answer8:e=>{const{normalize:n}=e;return n(["There is no concept of batch size unlike our Embedding API. You can send only one query-document tuple per request, but the tuple can include up to 2048 candidate documents."])},question9:e=>{const{normalize:n}=e;return n(["What latency can I expect when reranking 100 documents?"])},answer9:e=>{const{normalize:n}=e;return n(["Latency varies from 100 milliseconds to 7 seconds, depending largely on the length of the documents and the query. For instance, reranking 100 documents of 256 tokens each with a 64-token query takes about 150 milliseconds. Increasing the document length to 4096 tokens raises the time to 3.5 seconds. If the query length is increased to 512 tokens, the time further increases to 7 seconds."])},question10:e=>{const{normalize:n}=e;return n(["Can I deploy Jina Reranker on AWS?"])},answer10:e=>{const{normalize:n}=e;return n(["Yes, Jina Reranker can be deployed on AWS. If you require on-premises deployment in an enterprise setting, you can easily do so via our AWS Marketplace offering."])},question11:e=>{const{normalize:n}=e;return n(["Do you offer a fine-tuned reranker on domain-specific data?"])},answer11:e=>{const{normalize:n}=e;return n(["If you are interested in a fine-tuned reranker tailored to specific domain data, please contact our sales team. Our team will respond to your inquiry promptly."])}},title:e=>{const{normalize:n}=e;return n(["Reranker API"])},read_more_about_benchmark:e=>{const{normalize:n}=e;return n(["Read more about the benchmark"])},read_more_about_turbo:e=>{const{normalize:n}=e;return n(["Read more about the turbo and tiny models"])},choose_turbo:e=>{const{normalize:n}=e;return n(["Get up to 5x speedup with reranker-turbo"])},choose_turbo_description:e=>{const{normalize:n}=e;return n(["We also offer two new open-source reranker models: jina-reranker-v1-turbo-en and jina-reranker-v1-tiny-en, the latter has only 30M parameters and four layers. These two new rerankers enjoy 5X faster inference speed than the base model at only a very small cost on the quality. They are perfect for applications that require real-time reranking. Read the benchmark below."])},benchmark_title:e=>{const{normalize:n}=e;return n(["Performance Benchmark"])},try_embedding:e=>{const{normalize:n}=e;return n(["Try embedding API for free"])},try_reranker:e=>{const{normalize:n}=e;return n(["Try reranker API for free"])},benchmark_description:e=>{const{normalize:n}=e;return n(["For comparison, we included three other leading rerankers by BGE (BAAI), BCE (Netease Youdao), and Cohere in the benchmark. As shown by the results below, Jina Reranker holds the highest average score in all relevant categories for reranking, making it a clear leader among its peers."])},benchmark:{title0:e=>{const{normalize:n}=e;return n(["LlamaIndex"])},title1:e=>{const{normalize:n}=e;return n(["BEIR"])},title2:e=>{const{normalize:n}=e;return n(["LoCo"])},title3:e=>{const{normalize:n}=e;return n(["MTEB"])},description0:e=>{const{normalize:n}=e;return n(["LlamaIndex assessed various combinations of embeddings and rerankers for RAG, conducting a replication study that measured the Mean Reciprocal Rank. The findings highlight the Jina Reranker's significant enhancement of search quality, a benefit that is independent of the specific embeddings used."])},description1:e=>{const{normalize:n}=e;return n(["BIER (Benchmarking IR) assesses a model's retrieval effectiveness, including relevance and NDCG. A higher BIER score correlates to more accurate matches and search result rankings."])},description2:e=>{const{normalize:n}=e;return n(["Through the LoCo benchmark, we measured a model's understanding of local coherence and context, together with query-specific ranking. A LoCo higher score reflects a better ability to identify and prioritize relevant information."])},description3:e=>{const{normalize:n}=e;return n(["The MTEB (Multilingual Text Embedding Benchmark), on the whole, tests a model\u2019s abilities in text embeddings, including clustering, classification, retrieval, and other metrics. However, for our comparison, we only used the MTEB\u2019s Reranking tasks."])}},vs_table:{title:e=>{const{normalize:n}=e;return n(["Comparison of Reranker, Vector Search, and BM25"])},subtitle:e=>{const{normalize:n}=e;return n(["The table below provides a comprehensive comparison of the Reranker, Vector/Embeddings Search, and BM25, highlighting their strengths and weaknesses across various categories."])},col0:e=>{const{normalize:n}=e;return n(["Reranker"])},col1:e=>{const{normalize:n}=e;return n(["Vector Search"])},col2:e=>{const{normalize:n}=e;return n(["BM25"])},header0:e=>{const{normalize:n}=e;return n(["Best For"])},header1:e=>{const{normalize:n}=e;return n(["Granularity"])},header2:e=>{const{normalize:n}=e;return n(["Query Time Complexity"])},header3:e=>{const{normalize:n}=e;return n(["Indexing Time Complexity"])},header4:e=>{const{normalize:n}=e;return n(["Training Time Complexity"])},header5:e=>{const{normalize:n}=e;return n(["Search Quality"])},header6:e=>{const{normalize:n}=e;return n(["Strengths"])},header7:e=>{const{normalize:n}=e;return n(["Weaknesses"])},col0_2:e=>{const{normalize:n}=e;return n(["Initial, rapid filtering"])},col0_1:e=>{const{normalize:n}=e;return n(["Enhanced search precision and relevance"])},col0_3:e=>{const{normalize:n}=e;return n(["General text retrieval across wide-ranging queries"])},col1_1:e=>{const{normalize:n}=e;return n(["Detailed: Sub-document and query segment"])},col1_2:e=>{const{normalize:n}=e;return n(["Broad: Entire documents"])},col1_3:e=>{const{normalize:n}=e;return n(["Intermediate: Various text segments"])},col2_1:e=>{const{normalize:n}=e;return n(["High"])},col2_2:e=>{const{normalize:n}=e;return n(["Medium"])},col2_3:e=>{const{normalize:n}=e;return n(["Low"])},col3_1:e=>{const{normalize:n}=e;return n(["Not required"])},col3_2:e=>{const{normalize:n}=e;return n(["High"])},col3_3:e=>{const{normalize:n}=e;return n(["Low, utilizes pre-built index"])},col4_1:e=>{const{normalize:n}=e;return n(["High"])},col4_2:e=>{const{normalize:n}=e;return n(["High"])},col4_3:e=>{const{normalize:n}=e;return n(["Not required"])},col5_1:e=>{const{normalize:n}=e;return n(["Superior for nuanced queries"])},col5_2:e=>{const{normalize:n}=e;return n(["Balanced between efficiency and accuracy"])},col5_3:e=>{const{normalize:n}=e;return n(["Consistent and reliable for a broad set of queries"])},col6_1:e=>{const{normalize:n}=e;return n(["Highly accurate with deep contextual understanding"])},col6_2:e=>{const{normalize:n}=e;return n(["Quick and efficient, with moderate accuracy"])},col6_3:e=>{const{normalize:n}=e;return n(["Highly scalable, with established efficacy"])},col7_1:e=>{const{normalize:n}=e;return n(["Resource-intensive with complex implementation"])},col7_2:e=>{const{normalize:n}=e;return n(["May not capture deep query context or nuances"])},col7_3:e=>{const{normalize:n}=e;return n(["May underperform for highly specific or contextual searches"])}},feature_solid_description:e=>{const{normalize:n}=e;return n(["Developed from our cutting-edge academic research and rigorously tested against the SOTA rerankers to ensure unparalleled performance."])},improve_performance:e=>{const{normalize:n}=e;return n(["+33% relevance over vector search"])},improve_performance_description:e=>{const{normalize:n}=e;return n(["Our evaluations show that search systems employing the Jina Reranker enjoy +8% in hit rate and +33% in mean reciprocal rank."])},description_rich:e=>{const{normalize:n}=e;return n(["Maximize the search relevancy and RAG accuracy with our cutting-edge reranker API. Start with 1M free tokens."])},reranker_description:e=>{const{normalize:n}=e;return n(["Try our cutting-edge reranker API to maximize your search relevancy and RAG accuracy. Starting for free!"])},description:e=>{const{normalize:n}=e;return n(["Maximize the search relevancy and RAG accuracy at ease"])},learning1:e=>{const{normalize:n}=e;return n(["Learning about Reranker"])},what_is:e=>{const{normalize:n}=e;return n(["What is a Reranker?"])},how_it_works:e=>{const{normalize:n}=e;return n(["Here's how it works:"])},how_it_works_v1:{title1:e=>{const{normalize:n}=e;return n(["Initial Retrieval"])},description1:e=>{const{normalize:n}=e;return n(["A search system uses embeddings/BM25 to find a broad set of potentially relevant documents based on the user's query."])},title2:e=>{const{normalize:n}=e;return n(["Reranking"])},description2:e=>{const{normalize:n}=e;return n(["The reranker then takes these results and analyzes them at a more granular level, considering the nuances of how the query terms interact with the document content."])},title3:e=>{const{normalize:n}=e;return n(["Improved Results"])},description3:e=>{const{normalize:n}=e;return n(["It reorders the search results, placing the ones it deems most relevant at the top, based on this deeper analysis."])}},what_is_answer_long:e=>{const{normalize:n}=e;return n([`The goal of a search system is to find the most relevant results quickly and efficiently. Traditionally, methods like BM25 or tf-idf have been used to rank search results based on keyword matching. Recent methods, such as embedding-based cosine similarity, have been implemented in many vector databases. These methods are straightforward but can sometimes miss the subtleties of language, and most importantly, the interaction between documents and a query's intent.

This is where the "reranker" shines. A reranker is an advanced AI model that takes the initial set of results from a search\u2014often provided by an embeddings/token-based search\u2014and reevaluates them to ensure they align more closely with the user's intent. It looks beyond the surface-level matching of terms to consider the deeper interaction between the search query and the content of the documents.`])},what_is_answer_long_ending:e=>{const{normalize:n}=e;return n(["The reranker can significantly improve the search quality because it operates at a sub-document and sub-query level, meaning it looks at the individual words and phrases, their meanings, and how they relate to each other within the query and the documents. This results in a more precise and contextually relevant set of search results."])},what_is_desc:e=>{const{normalize:n}=e;return n(["A reranker is an AI model that refines the search results from a vector search or a dense retrieval model. Read more."])},learning1_description:e=>{const{normalize:n}=e;return n(["What is a reranker? Why is vector search or cosine similarity not enough? Learn about rerankers from the ground up with our comprehensive guide."])},feature_on_premises_description2:e=>{const{normalize:n}=e;return n(["Deploy Jina Reranker on AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description3:e=>{const{normalize:n}=e;return n(["Deploy Jina Reranker on AWS Sagemaker and Microsoft Azure and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])}},reader:{is_free:e=>{const{normalize:n}=e;return n(["The best part? It's free!"])},is_free_description:e=>{const{normalize:n}=e;return n(["The Reader API is available at no cost and does not require an API key. Built on a scalable infrastructure, it offers high accessibility, concurrency, and reliability. We strive to be your preferred solution for all your LLM input requirements."])},reader_reads_images:e=>{const{normalize:n}=e;return n(["Reader also reads images!"])},reader_also_read_images:e=>{const{normalize:n}=e;return n(["Images on the webpage are automatically captioned using a vision language model in the reader and formatted as image alt tags in the output. This gives your downstream LLM just enough hints to incorporate those images into its reasoning and summarizing processes. This means you can ask questions about the images, select specific ones, or even forward their URLs to a more powerful VLM for deeper analysis!"])},description:e=>{const{normalize:n}=e;return n(["Read any URL into LLM-friendly text instantly, hassle-free."])},reader_description:e=>{const{normalize:n}=e;return n(["Convert any URL to an LLM-friendly input with a simple prefix https://r.jina.ai. Experience improved output for your agent and RAG systems at no cost."])},what_is:e=>{const{normalize:n}=e;return n(["What is a Reader?"])},what_is_desc:e=>{const{normalize:n}=e;return n(["A proxy that accesses any URL and transforms the main content into plain text optimized for LLMs."])},open:e=>{const{normalize:n}=e;return n(["Open in new tab"])},copy:e=>{const{normalize:n}=e;return n(["Copy"])},title:e=>{const{normalize:n}=e;return n(["Reader API"])},usage:e=>{const{normalize:n}=e;return n(["Usage"])},better_input:e=>{const{normalize:n}=e;return n(["Enhance input quality right from the start"])},fast_stream:e=>{const{normalize:n}=e;return n(["Immediate data streaming"])},free:e=>{const{normalize:n}=e;return n(["Free forever"])},fast:e=>{const{normalize:n}=e;return n(["Fast"])},faq_v1:{title:e=>{const{normalize:n}=e;return n(["Reader-related common questions"])},question1:e=>{const{normalize:n}=e;return n(["What are the costs associated with using the Reader API?"])},answer1:e=>{const{normalize:n}=e;return n(["The Reader API is free of charge and does not require an API key. Simply prepend 'https://r.jina.ai/' to your URL."])},question2:e=>{const{normalize:n}=e;return n(["How does the Reader API function?"])},answer2:e=>{const{normalize:n}=e;return n(["The Reader API uses a proxy to fetch any URL, rendering its content in a browser to extract high-quality main content."])},question3:e=>{const{normalize:n}=e;return n(["Is the Reader API open source?"])},answer3:e=>{const{normalize:n}=e;return n(["Yes, the Reader API is open source and available on the Jina AI GitHub repository."])},question4:e=>{const{normalize:n}=e;return n(["What is the typical latency for the Reader API?"])},answer4:e=>{const{normalize:n}=e;return n(["The Reader API generally processes URLs and returns content within 2 seconds, although complex or dynamic pages might require more time."])},question5:e=>{const{normalize:n}=e;return n(["Why should I use the Reader API instead of scraping the page myself?"])},answer5:e=>{const{normalize:n}=e;return n(["Scraping can be complicated and unreliable, particularly with complex or dynamic pages. The Reader API provides a streamlined, reliable output of clean, LLM-ready text."])},question6:e=>{const{normalize:n}=e;return n(["Does the Reader API support multiple languages?"])},answer6:e=>{const{normalize:n}=e;return n(["The Reader API returns content in the original language of the URL. It does not provide translation services."])},question7:e=>{const{normalize:n}=e;return n(["What should I do if a website blocks the Reader API?"])},answer7:e=>{const{normalize:n}=e;return n(["If you experience blocking issues, please contact our support team for assistance and resolution."])},question8:e=>{const{normalize:n}=e;return n(["Can the Reader API extract content from PDF files?"])},answer8:e=>{const{normalize:n}=e;return n(["While primarily designed for web pages, the Reader API can extract content from PDFs viewed in HTML format on websites like arXiv, but it is not optimized for general PDF extraction."])},question9:e=>{const{normalize:n}=e;return n(["Can the Reader API process media content from web pages?"])},answer9:e=>{const{normalize:n}=e;return n(["Currently, the Reader API does not process media content, but future enhancements will include image captioning and video summarization."])},question10:e=>{const{normalize:n}=e;return n(["Is it possible to use the Reader API on local HTML files?"])},answer10:e=>{const{normalize:n}=e;return n(["No, the Reader API can only process content from publicly accessible URLs."])},question11:e=>{const{normalize:n}=e;return n(["Does Reader API cache the content?"])},answer11:e=>{const{normalize:n}=e;return n(["If you request the same URL within 5 minutes, the Reader API will return the cached content."])},question12:e=>{const{normalize:n}=e;return n(["Can I use the Reader API to access content behind a login?"])},answer12:e=>{const{normalize:n}=e;return n(["Unfortunately not."])},question13:e=>{const{normalize:n}=e;return n(["Can I use the Reader API to access PDF on arXiv?"])},answer13:e=>{const{normalize:n}=e;return n(["Yes, but not directly access the PDF file but via the HTML version of the PDF file. For example, try https://r.jina.ai/https://arxiv.org/html/2310.19923v4"])},question14:e=>{const{normalize:n}=e;return n(["How does image caption work in Reader?"])},answer14:e=>{const{normalize:n}=e;return n(["Reader captions all images at the specified URL and adds `Image [idx]: [caption]` as an alt tag (if they initially lack one). This enables downstream LLMs to interact with the images in reasoning, summarizing etc."])},question15:e=>{const{normalize:n}=e;return n(["What is the scalability of the Reader? Can I use it in production?"])},answer15:e=>{const{normalize:n}=e;return n(["The Reader API is designed to be highly scalable. It is auto-scaled based on the real-time traffic and the maximum concurrency requests is now around 4000. We are maintaining it actively as one of the core products of Jina AI. So feel free to use it in production."])},question16:e=>{const{normalize:n}=e;return n(["What is the rate limit of the Reader API?"])},answer16:e=>{const{normalize:n}=e;return n(["The rate limit is 100 requests per minute per IP address. We are working on a more flexible rate limit policy. Stay tuned!"])}},demo:{raw_html:e=>{const{normalize:n}=e;return n(["Raw HTML"])},reader_output:e=>{const{normalize:n}=e;return n(["Reader Output"])},try_demo:e=>{const{normalize:n}=e;return n(["Demo"])},standard_usage:e=>{const{normalize:n}=e;return n(["Standard Usage"])},stream_mode:e=>{const{normalize:n}=e;return n(["Stream Mode"])},stream_mode_explain1:e=>{const{normalize:n}=e;return n(["Streaming mode is useful when you find that the standard mode provides an incomplete result. This is because streaming mode will wait a bit longer until the page is fully rendered. Use the accept-header to toggle the streaming mode:"])},stream_mode_explain:e=>{const{normalize:n}=e;return n(["Stream mode is useful when the target page is large to render. If you find standard mode gives you incomplete content, try stream mode."])},how_to_stream:e=>{const{normalize:n}=e;return n(["To process content as it becomes available, set the request header to stream mode. This minimizes the time until the first byte is received. Example in curl:"])},how_to_use:e=>{const{normalize:n}=e;return n(["Using the Reader API is straightforward. Simply prepend 'https://r.jina.ai/' to any URL in your code or tool where LLM access is needed."])},tagline:e=>{const{normalize:n}=e;return n(["Try the demo"])},your_url:e=>{const{normalize:n}=e;return n(["Enter your URL"])},reader_url:e=>{const{normalize:n}=e;return n(["Reader URL"])},fetch:e=>{const{normalize:n}=e;return n(["Fetch Content"])},ask_question:e=>{const{normalize:n}=e;return n(["Pose a Question"])},your_url_hint:e=>{const{normalize:n}=e;return n(["Click below to fetch the source code of the page directly"])},reader_url_hint:e=>{const{normalize:n}=e;return n(["Click below to obtain the content through our Reader API"])},ask_question_hint:e=>{const{normalize:n}=e;return n(["Input a question and combine it with the fetched content for LLM to generate an answer"])}},what_is_answer_long:e=>{const{normalize:n}=e;return n(["Feeding web information into LLMs is an important step of grounding, yet it can be challenging. The simplest method is to scrape the webpage and feed the raw HTML. However, scraping can be complex and often blocked, and raw HTML is cluttered with extraneous elements like markups and scripts. The Reader API addresses these issues by extracting the core content from a URL and converting it into clean, LLM-friendly text, ensuring high-quality input for your agent and RAG systems."])},free_description:e=>{const{normalize:n}=e;return n(["Reader API is free! It requires no credit card or API secret. It will not consume your token quota."])},fast_stream_description:e=>{const{normalize:n}=e;return n(["Need data quickly? Our Reader API can stream data to minimize latency."])},better_input_description:e=>{const{normalize:n}=e;return n(["Experiencing issues with your agent or RAG system output? It might be due to poor input quality."])}},landing_page:{get_started:e=>{const{normalize:n}=e;return n(["Get started"])},source_code:e=>{const{normalize:n}=e;return n(["Source code"])},reader:e=>{const{normalize:n}=e;return n(["Reader"])},podcast:e=>{const{normalize:n}=e;return n(["Podcast"])},get_api_now:e=>{const{normalize:n}=e;return n(["API"])},pricing:e=>{const{normalize:n}=e;return n(["Pricing"])},your_search_foundation1:e=>{const{normalize:n}=e;return n(["Your Search Foundation"])},supercharged1:e=>{const{normalize:n}=e;return n(["Supercharged."])},learn_more_embeddings:e=>{const{normalize:n}=e;return n(["Learn more about embeddings"])},learn_more_reranker:e=>{const{normalize:n}=e;return n(["Learn more about reranker"])},learn_more_reader:e=>{const{normalize:n}=e;return n(["Learn more about reader"])},try_it_for_free:e=>{const{normalize:n}=e;return n(["Try it for free, no credit card required"])},reranker:e=>{const{normalize:n}=e;return n(["Reranker"])},finding_faq:e=>{const{normalize:n}=e;return n(["Generating answer based on the FAQ knowledge below"])},embeddings:e=>{const{normalize:n}=e;return n(["Embeddings"])},new:e=>{const{normalize:n}=e;return n(["New"])},"on-premises":e=>{const{normalize:n}=e;return n(["On-premises"])},"on-prem-deploy":e=>{const{normalize:n}=e;return n(["On-premises deployment"])},also_available_on1:e=>{const{normalize:n}=e;return n(["Available on the marketplaces of your enterprise cloud"])},coming_soon:e=>{const{normalize:n}=e;return n(["Coming soon"])},try_our_saas:e=>{const{normalize:n}=e;return n(["Try our hosted solution, a drop-in replacement for OpenAI's embedding API."])},also_available_on:e=>{const{normalize:n}=e;return n(["Available on the marketplaces"])},our_publications:e=>{const{normalize:n}=e;return n(["Our Publications"])},embedding_desc1:e=>{const{normalize:n}=e;return n(["Start with 1M free tokens. Top-performing, 8192 context length bilingual embeddings for your search and RAG systems."])},enterprise_desc_v2:e=>{const{normalize:n}=e;return n(["Try our world-class embedding models to improve your search and RAG systems. Start with a free trial!"])},enterprise_desc_v3:e=>{const{normalize:n}=e;return n(["We develop cutting-edge search foundational models for high-quality enterprise search and RAG solutions. Start with a free trial!"])},researcher_desc:e=>{const{normalize:n}=e;return n(["To understand how our large language models were trained from scratch for embedding tasks, check out our latest research and publications. Meet our team at the EMNLP, ACL, SIGIR, NeurIPS, and ICML conferences."])},include_experiment:e=>{const{normalize:n}=e;return n(["Includes our experimental and archived projects in the solution."])},your_portal_to:e=>{const{normalize:n}=e;return n(["Your Portal to"])},copy:e=>{const{normalize:n}=e;return n(["Copy"])},require_full_question:e=>{const{normalize:n}=e;return n(["Please describe your problem with more details."])},proposing_solution:e=>{const{normalize:n}=e;return n(["Proposing a solution based on Jina AI products..."])},mentioned_products:e=>{const{normalize:n}=e;return n(["Mentioned products:"])},copied_to_clipboard:e=>{const{normalize:n}=e;return n(["Copied to clipboard"])},checkout_our_solution_for_you:e=>{const{normalize:n}=e;return n(["Find out our solution tailored for you"])},ask_how_your_question:e=>{const{normalize:n}=e;return n(["Please describe your problem"])},how_to:e=>{const{normalize:n}=e;return n(["How to"])},powered_by_promptperfect:e=>{const{normalize:n}=e;return n([`Powered by PromptPerfect's "Prompt optimization" and "Prompt as a service" feature`])},error:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["There was a problem with the fetch operation: ",r(o("message"))])},find_your_portal:e=>{const{normalize:n}=e;return n(["Find Your Portal"])},opensource:e=>{const{normalize:n}=e;return n(["Open Source"])},mmstack:e=>{const{normalize:n}=e;return n(["Multimodal Stack"])},mmstack_desc:e=>{const{normalize:n}=e;return n(["The developer-friendly, cloud-native, and production-ready stack for building multimodal AI applications."])},llm:e=>{const{normalize:n}=e;return n(["LLM embedding models"])},llm_desc:e=>{const{normalize:n}=e;return n(["We provide a collection of high-performance sentence embedding models, boasting between 35 million to 6 billion parameters. They're excellent for enhancing neural search, reranking, sentence similarity, recommendations, etc. Get ready to elevate your AI experience!"])},parameters:e=>{const{normalize:n}=e;return n(["Parameters"])},download_pdf:e=>{const{normalize:n}=e;return n(["Download PDF"])},multimodal_ai:e=>{const{normalize:n}=e;return n(["Multimodal AI"])},multimodal:e=>{const{normalize:n}=e;return n(["Multimodal"])},contact_sales:e=>{const{normalize:n}=e;return n(["Contact sales"])},join_community:e=>{const{normalize:n}=e;return n(["Join community"])},trusted_by:e=>{const{normalize:n}=e;return n(["TRUSTED BY"])},newsroom:e=>{const{normalize:n}=e;return n(["Newsroom"])},read_more:e=>{const{normalize:n}=e;return n(["Read more"])},for:e=>{const{normalize:n}=e;return n(["For"])},power_users:e=>{const{normalize:n}=e;return n(["Power Users"])},researchers:e=>{const{normalize:n}=e;return n(["Researchers"])},power_users_desc:e=>{const{normalize:n}=e;return n(["Experience powerful, user-friendly multimodal AI apps. No coding, no fuss, just results."])},developers:e=>{const{normalize:n}=e;return n(["Developers"])},developers_desc:e=>{const{normalize:n}=e;return n(["Unleash the full power of multimodal AI with cutting-edge cloud-native technologies and open-source infrastructure."])},sdk:e=>{const{normalize:n}=e;return n(["SDK"])},starter_kit:e=>{const{normalize:n}=e;return n(["Starter Kit"])},sdk_desc:e=>{const{normalize:n}=e;return n(["Want to build high-level AIGC applications using PromptPerfect, SceneXplain, BestBanner, JinaChat, Rationale APIs? We've got you covered! Try our easy-to-use SDK and get started in minutes."])},sdk_docs:e=>{const{normalize:n}=e;return n(["Read docs"])},sdk_example:e=>{const{normalize:n}=e;return n(["Example"])},build_python:e=>{const{normalize:n}=e;return n(["Build with Python"])},build_js:e=>{const{normalize:n}=e;return n(["Build with JavaScript"])},enterprise:e=>{const{normalize:n}=e;return n(["Enterprise"])},enterprise_desc:e=>{const{normalize:n}=e;return n(["Boost your business with scalable, secure, and bespoke multimodal AI solutions."])},embedding_paper_title:e=>{const{normalize:n}=e;return n(["Jina Embeddings: A Novel Set of High-P`erformance Sentence Embedding Models"])},embedding_paper_desc:e=>{const{normalize:n}=e;return n(["Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB)."])}},about_us_page:{title3:e=>{const{normalize:n}=e;return n(["Starts Here"])},download_logo:e=>{const{normalize:n}=e;return n(["Download logos"])},download_jina_logo:e=>{const{normalize:n}=e;return n(["Download the Jina AI Logo"])},download_jina_logo_desc:e=>{const{normalize:n}=e;return n(["Get the Jina AI logo in both light and dark modes, available in PNG and SVG formats. This logo is a registered trademark with the European Union Intellectual Property Office (EUIPO)."])},download_docarray_logo:e=>{const{normalize:n}=e;return n(["Download the DocArray Logo"])},download_docarray_logo_desc:e=>{const{normalize:n}=e;return n(["Access the DocArray logo, an open-source project initiated by Jina AI and contributed to the Linux Foundation in December 2022. Available in light and dark modes, in PNG and SVG formats."])},brochure_info:e=>{const{normalize:n}=e;return n(["Your Guide to Our Company Awaits"])},download_brochure1:e=>{const{normalize:n}=e;return n(["Download brochure"])},employees:e=>{const{normalize:n}=e;return n(["Employees"])},approach:e=>{const{normalize:n}=e;return n(["Our Approach"])},approach_content1:e=>{const{normalize:n}=e;return n(["In the rapidly evolving world of AI, strategies need to be both nimble and forward-thinking. While our core offering remains centered on enterprises, the AI landscape has shifted in ways that necessitate a rethinking of our approach to customer acquisition. Here's why introducing power users as the entry point of our funnel isn't just innovative, but crucial for our sustained growth in the enterprise sector."])},approach_content2:e=>{const{normalize:n}=e;return n(["At Jina AI, our strategy is to be proactive rather than reactive. The inclusion of power users as the funnel's entry point ensures we're not only capturing current market trends but are also strategically poised for future enterprise growth. Our commitment to enterprises remains unwavering; however, our approach to reaching them is innovative, robust, and, above all, forward-thinking."])},approach_new_paradigm:e=>{const{normalize:n}=e;return n(["Prompt-based Technology: A New Paradigm"])},approach_miss_mark:e=>{const{normalize:n}=e;return n(["Why Traditional MLOps Miss the Mark"])},approach_connect_dots:e=>{const{normalize:n}=e;return n(["Connecting the Dots: Power Users to Enterprises"])},approach_new_paradigm_description:e=>{const{normalize:n}=e;return n([`2023 heralded a significant change: the rise of prompt-based technology. By simplifying the AI development process, it has democratized access to AI tools. Now, those without extensive programming experience\u2014termed as 'power users'\u2014can engage in AI development without the steep learning curves associated with tools like Pytorch, Docker, or Kubernetes.

Drawing a parallel, this is akin to the evolution of personal computing. Initially, only tech experts operated computers. But with the advent of user-friendly interfaces, a broader audience could participate. Today, with prompt-based technology, we're witnessing a similar democratization in AI.`])},approach_miss_mark_description:e=>{const{normalize:n}=e;return n(["While the influx of power users is significant, traditional MLOps tools are ill-equipped to cater to their needs. These tools are reminiscent of using a tractor to navigate city streets\u2014they're heavy and often excessive. The new-gen developers demand agile, intuitive tools that complement their rapid development pace."])},approach_connect_dots_description:e=>{const{normalize:n}=e;return n(["So, why is a power user focus essential for our enterprise-centric model? Because it\u2019s about establishing early relationships. By catering to power users now, we're building bridges to the enterprises they'll influence in the future. It\u2019s a strategic play\u2014a long-term investment to ensure our enterprise offering remains top-of-mind when these power users ascend to decision-making roles within organizations."])},title:e=>{const{normalize:n}=e;return n(["About Jina AI"])},title0:e=>{const{normalize:n}=e;return n(["The Future"])},title1:e=>{const{normalize:n}=e;return n(["Starts"])},title2:e=>{const{normalize:n}=e;return n(["Here"])},description:e=>{const{normalize:n}=e;return n(["The future starts here."])},subtitle:e=>{const{normalize:n}=e;return n(["Revolutionizing content creation through AI-generated solutions to unlock infinite possibilities. Shaping the future of AI-generated content and enhancing human creativity."])},stats_v1:e=>{const{normalize:n}=e;return n(["Shaping the Future AI"])},founded:e=>{const{normalize:n}=e;return n(["Founded"])},founded_in:e=>{const{normalize:n}=e;return n(["Founded in"])},empower_developers:e=>{const{normalize:n}=e;return n(["Developers Empowered"])},technologies:e=>{const{normalize:n}=e;return n(["Technologies"])},users:e=>{const{normalize:n}=e;return n(["Users Registered"])},value:e=>{const{normalize:n}=e;return n(["Our Value"])},value_content1:e=>{const{normalize:n}=e;return n([`We believe that openness can accelerate innovation and foster collaboration. We're not just advocates\u2014we are active contributors, investing significantly in the open-source community.

From being an early donor to FastAPI, to actively supporting the Linux Foundation and the Python Software Foundation, we are passionate about giving back. But we don\u2019t stop there; we\u2019ve also open-sourced our models and projects, sharing our expertise with the world.`])},stats_1:e=>{const{normalize:n}=e;return n(["Founded in February 2020, Jina AI has swiftly emerged as a global pioneer in multimodal AI technology. Within an impressive timeframe of 20 months, we have successfully raised $37.5M, marking our strong position in the AI industry. Our ground-breaking technology, open-sourced on GitHub, has empowered over 40,000 developers around the globe to seamlessly build and deploy sophisticated multimodal applications."])},stats_2:e=>{const{normalize:n}=e;return n(["In 2023, we've made significant strides in advancing AI generation tools grounded on multimodal technology. This innovation has benefited over 250,000 users worldwide, catering to a plethora of unique business requirements. From facilitating business growth and enhancing operational efficiency to optimizing costs, Jina AI is dedicated to empowering businesses to excel in the multimodal era."])},stats_4:e=>{const{normalize:n}=e;return n(['Founded in 2020 in Berlin, Jina AI is a leading AI company. We provide the <span class="text-bold">search foundation</span>, a crucial component in multimodal AI, with the goal of enabling businesses and developers to harness the power of multimodal data for value creation and cost savings. Our commitment to open-source and open research has established us as a commercial open-source software company.'])},investors:e=>{const{normalize:n}=e;return n(["Our Investors"])},vision:e=>{const{normalize:n}=e;return n(["Our Mission"])},vision_content1:e=>{const{normalize:n}=e;return n(["Inspired by Yann LeCun's insight that '"])},vision_content3:e=>{const{normalize:n}=e;return n([`The future of AI is multimodal, and we are part of it. We realize that businesses face challenges in leveraging multimodal data. In response, we're committed to the <span class="text-bold">search foundation</span> to help businesses and developers better search, understand, and utilize multimodal data for all kinds of business potentials.`])},yannlecun_quote:e=>{const{normalize:n}=e;return n(["An artificial intelligence system trained on words and sentences alone will never approximate human understanding."])},our_answer:e=>{const{normalize:n}=e;return n(["Absolutely, Yann. We're on it, building bridges to a multimodal AI future!"])},mission:e=>{const{normalize:n}=e;return n(["Our Mission"])},mission_content1:e=>{const{normalize:n}=e;return n(["We  Our key technologies, including prompt-tuning, prompt-serving, model-tuning, and model-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains."])},mission_content2:e=>{const{normalize:n}=e;return n(["At the heart of Jina AI lies our mission to be the portal to multimodal AI for a diverse clientele, from power users and developers to enterprises. We deeply believe in the power of open-source and are dedicated to building advanced, accessible tools for the AI community. Our key technologies, including prompt-tuning, prompt-serving, embedding-tuning, and embedding-serving, embody our commitment to democratizing access to AI. Through our open-source initiative, we strive to foster innovation, collaboration, and transparency, ensuring scalable, efficient, and robust solutions. Jina AI is more than just a company; it's a community devoted to empowering businesses to meet the dynamic challenges of the digital age and thrive in their domains."])},mission_content3:e=>{const{normalize:n}=e;return n(["At Jina AI, our mission is to lead the advancement of multimodal AI through innovative embedding and prompt-based technologies, focusing specifically on areas like natural language processing, image and video analysis, and cross-modal data interaction. This specialization allows us to provide unique solutions that turn complex, multi-source data into actionable insights and groundbreaking applications."])},approach_content4:e=>{const{normalize:n}=e;return n(["At Jina AI, we are building the search foundation, which consists of embeddings, rerankers, prompt ops, and core infrastructure. These components work in concert to revolutionize how we search and understand data, thereby increasing relevance and reducing search time. This leads to improved user satisfaction and trust, higher engagement and conversion rates, a direct increase in sales volume, and the unlocking of new applications to foster business growth."])},team:e=>{const{normalize:n}=e;return n(["Inside the Portal of Jina AI"])},team_content1:e=>{const{normalize:n}=e;return n(["From diverse corners of the globe, we're building the future of AI. Our distinct perspectives enrich our work, sparking innovations. Within this portal, we embrace our individuality, and passionately pursue our dreams. Welcome to the portal of the AI future."])},team_join:e=>{const{normalize:n}=e;return n(["Join us"])},office:e=>{const{normalize:n}=e;return n(["Our Offices"])},berlin:e=>{const{normalize:n}=e;return n(["Berlin, Germany"])},berlin_address:e=>{const{normalize:n}=e;return n(["Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany"])},berlin_address2:e=>{const{normalize:n}=e;return n(["Gesch\xE4ftsanschrift: Leipziger str. 96, 10117 Berlin, Germany"])},bj:e=>{const{normalize:n}=e;return n(["Beijing, China"])},bj_address:e=>{const{normalize:n}=e;return n(["Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China"])},sz:e=>{const{normalize:n}=e;return n(["Shenzhen, China"])},sz_address:e=>{const{normalize:n}=e;return n(["402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China"])},awards:e=>{const{normalize:n}=e;return n(["Awards & Recognition"])},fastApiCaption:e=>{const{normalize:n}=e;return n(["Contributed over $20,000 since 2021."])},linuxFoundationCaption:e=>{const{normalize:n}=e;return n(["Makes an annual contribution of $10,000 starting from 2022."])},pythonSoftwareFoundationCaption:e=>{const{normalize:n}=e;return n(["Provided a one-time donation of $10,000 and sponsored multiple PyCon events including those in Germany, Italy, China, and the US."])},numfocusCaption:e=>{const{normalize:n}=e;return n(["Regularly donates each month starting from 2022."])},segmentFaultCaption:e=>{const{normalize:n}=e;return n(["Contributed a one-time donation of $6,000."])},otherProjectsCaption:e=>{const{normalize:n}=e;return n(["Donated over $3,000 via Github Sponsorship."])},understand_our_strength:e=>{const{normalize:n}=e;return n(["Understand Our Strength"])},understand_our_view2:e=>{const{normalize:n}=e;return n(["Understand the Search Foundation"])}},internship_page:{title:e=>{const{normalize:n}=e;return n(["Intern Program"])},description:e=>{const{normalize:n}=e;return n(["Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together."])},subtitle:e=>{const{normalize:n}=e;return n(["Our full-time internship programme provides hands-on work experience through well-designed internship projects in a wide range of scope."])},subtitle1:e=>{const{normalize:n}=e;return n(["Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together."])},alumni:e=>{const{normalize:n}=e;return n(["ALUMNI"])},about_internship_program:e=>{const{normalize:n}=e;return n(["About Internship Program"])},about_internship_program_desc1:e=>{const{normalize:n}=e;return n(["We are excited to offer this unique opportunity for talented individuals to join our dynamic team and contribute to groundbreaking projects in the field of Artificial Intelligence. This internship is designed to provide you with valuable hands-on experience, mentorship, and exposure to cutting-edge technologies that are shaping the future of AI."])},about_internship_program_desc2:e=>{const{normalize:n}=e;return n(["At Jina AI, we understand the significance of nurturing and harnessing young talent. We recognize that interns bring fresh perspectives, enthusiasm, and creativity to the table, invigorating our team with new ideas and approaches. By providing internships, we aim to foster the growth of future leaders in the AI industry while offering them real-world experience in a supportive and challenging environment."])},who_do_we_look_for:e=>{const{normalize:n}=e;return n(["Who do we look for?"])},who_do_we_look_for_desc:e=>{const{normalize:n}=e;return n(["We value diversity and encourage applicants from diverse profiles and backgrounds to join our Internship Program. The internship opportunities are offered in multiple departments, including Engineering, Design, Product Management, Sales and Account Management, Marketing and Community Management."])},enthusiastic:e=>{const{normalize:n}=e;return n(["ENTHUSIASTIC"])},self_motivated:e=>{const{normalize:n}=e;return n(["SELF-MOTIVATED"])},innovative:e=>{const{normalize:n}=e;return n(["INNOVATIVE"])},explore_stories_from_our_interns:e=>{const{normalize:n}=e;return n(["Explore stories from our interns"])},explore_stories_from_our_interns1:e=>{const{normalize:n}=e;return n(["Get inspired by our interns' journeys"])},software_engineer_intern:e=>{const{normalize:n}=e;return n(["Software Engineer Intern"])},recruiting_and_administrative_intern:e=>{const{normalize:n}=e;return n(["Recruiting and Administrative Intern"])},dev_rel_intern:e=>{const{normalize:n}=e;return n(["Developer Relations Intern"])},alumni_network:e=>{const{normalize:n}=e;return n(["Our thriving alumni network"])},intern_work1:e=>{const{normalize:n}=e;return n(["Fine-tuned LLM models for better embeddings"])},intern_work2:e=>{const{normalize:n}=e;return n(["Explored the potential of Retrieval Augmented Generation"])},intern_work3:e=>{const{normalize:n}=e;return n(["Published a paper on the topic of sentence embeddings"])},intern_work4:e=>{const{normalize:n}=e;return n(["Injecting continuous youthful vitality into the team"])},intern_work5:e=>{const{normalize:n}=e;return n(["Benchmarked quantization techniques to compress LLM"])},intern_work6:e=>{const{normalize:n}=e;return n(["Creating and promoting compelling campaign for PromptPerfect"])},application:e=>{const{normalize:n}=e;return n(["Application"])},submit_application:e=>{const{normalize:n}=e;return n(["Kickstart your adventure with Jina AI"])},application_desc:e=>{const{normalize:n}=e;return n(["Embark on a transformative journey with Jina AI. Our comprehensive internship program invites all passionate minds who aspire to shape the future of artificial intelligence. Join us to get real-world experience, work on challenging projects, and collaborate with some of the brightest minds in the AI industry."])},summer:e=>{const{normalize:n}=e;return n(["Summer"])},autumn:e=>{const{normalize:n}=e;return n(["Autumn"])},winter:e=>{const{normalize:n}=e;return n(["Winter"])},spring:e=>{const{normalize:n}=e;return n(["Spring"])},apply:e=>{const{normalize:n}=e;return n(["Apply now"])}},embeddings:{description:e=>{const{normalize:n}=e;return n(["Our world-class embeddings for your search and RAG systems"])}},spectrum:{click_to_learn_more:e=>{const{normalize:n}=e;return n(["Click to learn more"])},embeddings:e=>{const{normalize:n}=e;return n(["Embeddings"])},embeddings_desc:e=>{const{normalize:n}=e;return n(["Embeddings are the cornerstones of modern search system, representing multimodal data into vectors of numbers. This process enables a more nuanced and contextual understanding of content, far beyond simple keyword matching."])},rerankers:e=>{const{normalize:n}=e;return n(["Reranker"])},rerankers_desc:e=>{const{normalize:n}=e;return n(["Rerankers take the initial results from the embeddings and refine them, ensuring that the most relevant results are presented to the user. This is crucial for delivering high-quality search results that meet the user's intent."])},promptOps:e=>{const{normalize:n}=e;return n(["PromptOps"])},promptOps_desc:e=>{const{normalize:n}=e;return n(["Prompt Ops improve the input and output of the search system, including those used in queries expansion, LLM-input and results rewriting. This ensures that the the search understands better and results better."])},coreInfra:e=>{const{normalize:n}=e;return n(["Core Infra"])},coreInfra_desc:e=>{const{normalize:n}=e;return n(["Core infra provides a cloud-native layer for developing, deploying and orchestration search foundation models both in the public cloud and on-premises, enabling services to scale up and down effortlessly."])},prompt_tech:e=>{const{normalize:n}=e;return n(["Prompt & agent engineering"])},embedding_tech:e=>{const{normalize:n}=e;return n(["Embeddings"])},for_power_users:e=>{const{normalize:n}=e;return n(["For Power Users"])},for_developers:e=>{const{normalize:n}=e;return n(["For Developers"])},for_enterprise:e=>{const{normalize:n}=e;return n(["For Enterprises"])},prompt_serving:e=>{const{normalize:n}=e;return n(["Prompt Serving"])},prompt_tuning:e=>{const{normalize:n}=e;return n(["Prompt Tuning"])},model_serving:e=>{const{normalize:n}=e;return n(["Model Serving"])},model_tuning:e=>{const{normalize:n}=e;return n(["Model Tuning"])},embedding_serving:e=>{const{normalize:n}=e;return n(["Embedding Serving"])},embedding_tuning:e=>{const{normalize:n}=e;return n(["Embedding Tuning"])},prompt_tech_description:e=>{const{normalize:n}=e;return n([`At Jina AI, we recognize prompt engineering as vital for interacting with large language models (LLMs). As these models advance, the complexity of prompts escalates, encompassing intricate reasoning and logic. This advancement underscores the intertwined growth of LLMs and prompt sophistication.

We foresee a future where LLMs act as compilers, with prompts becoming the new programming language. This shift suggests that future technological proficiency may focus more on prompt mastery than traditional coding. Our commitment at Jina AI is to lead in this transformative area, making advanced AI accessible and practical for everyday use by mastering this emerging 'language'.`])},embedding_tech_description:e=>{const{normalize:n}=e;return n([`At Jina AI, we harness the power of embedding technology to revolutionize diverse AI applications. This technology serves as a unified method to efficiently represent and compress various data types, ensuring no loss of critical information. Our focus is on transforming complex datasets into a universally understandable embedding format, which is essential for precise and insightful AI analysis.

Embeddings are fundamental, especially in applications like precise image and voice recognition, where they help discern fine-grained details and nuances. In natural language processing, embeddings enhance understanding of context and sentiment, leading to more accurate conversational AI and language translation tools. They are also crucial in developing sophisticated recommendation systems that require a deep understanding of user preferences across different content forms, such as text, audio, and video.`])},prompt_serving_description:e=>{const{normalize:n}=e;return n(["Wrapping and serving prompts through an API, without hosting heavy models. The API calls a public large language model service and handles the orchestration of inputs and outputs in a chain of operations."])},prompt_tuning_description:e=>{const{normalize:n}=e;return n(["The process of crafting and refining the input prompts in order to guide its output towards specific, desired responses."])},embedding_serving_description:e=>{const{normalize:n}=e;return n(["Delivering embeddings through a robust, scalable microservice using cloud-native technologies."])},embedding_tuning_description:e=>{const{normalize:n}=e;return n(["Optimizing high-quality embeddings by integrating domain expertise for enhanced task-specific performance."])},model_serving_description:e=>{const{normalize:n}=e;return n(["The deployment of fine-tuned models in a production environment, usually requiring substantial resources such as GPU hosting. MLOps, emphasizing the serving of mid-size to large models in a scalable, efficient, and reliable manner."])},model_tuning_description:e=>{const{normalize:n}=e;return n(["Also known as fine-tuning, involves adjusting the parameters of a pre-trained model on a new, often task-specific dataset to improve its performance and adapt it to a specific application."])}},huggingface:{sentence_similarity:e=>{const{normalize:n}=e;return n(["Sentence embedding"])},updated_about:e=>{const{normalize:n}=e;return n(["Updated about"])}},impact_snapshots:{project1:e=>{const{normalize:n}=e;return n(["Enable high-accuracy search within 3D mesh data using point cloud information."])},project2:e=>{const{normalize:n}=e;return n(["Design a content-based search engine for short animation films."])},project3:e=>{const{normalize:n}=e;return n(["Enhance e-commerce conversion rates by fine-tuning embedding models."])},project4:e=>{const{normalize:n}=e;return n(["Execute prompt tuning to boost efficiency for a business consulting company."])},project5:e=>{const{normalize:n}=e;return n(["Pioneer game scene understanding and automatic annotation for a leading gaming enterprise."])},project6:e=>{const{normalize:n}=e;return n(["Implement real-time input expansion for a chatbot company, enhancing user experience."])},project7:e=>{const{normalize:n}=e;return n(["Revolutionize legal tech by enabling efficient search within lengthy legal documents."])},project8:e=>{const{normalize:n}=e;return n(["Support a high-throughput generative art service for large-scale operations."])},project9:e=>{const{normalize:n}=e;return n(["Carry out process mining and modeling using advanced language models."])},project10:e=>{const{normalize:n}=e;return n(["Leverage computer vision to improve digital accessibility of government websites."])},project11:e=>{const{normalize:n}=e;return n(["Fine-tune LLM for a consulting firm to optimize finance data analysis."])},project12:e=>{const{normalize:n}=e;return n(["Advance marketing strategies by fine-tuning text-to-image models for style transferring."])}},project_status:{observability:e=>{const{normalize:n}=e;return n(["Observability"])},graduated:e=>{const{normalize:n}=e;return n(["Graduated"])},incubating:e=>{const{normalize:n}=e;return n(["Incubating"])},sandbox:e=>{const{normalize:n}=e;return n(["Sandbox"])},archived:e=>{const{normalize:n}=e;return n(["Archived"])},kubernetes:e=>{const{normalize:n}=e;return n(["Kubernetes"])},cloud_native:e=>{const{normalize:n}=e;return n(["Cloud Native"])},prompt_tuning:e=>{const{normalize:n}=e;return n(["Prompt Tuning"])},model_serving:e=>{const{normalize:n}=e;return n(["Model Serving"])},model_tuning:e=>{const{normalize:n}=e;return n(["Model Tuning"])},embedding_serving:e=>{const{normalize:n}=e;return n(["Embedding Serving"])},embedding_tuning:e=>{const{normalize:n}=e;return n(["Embedding Tuning"])},prompt_serving:e=>{const{normalize:n}=e;return n(["Prompt Serving"])},core:e=>{const{normalize:n}=e;return n(["Core"])},small_size_model:e=>{const{normalize:n}=e;return n(["Small Size Model"])},large_size_model:e=>{const{normalize:n}=e;return n(["Large Size Model"])},mid_size_model:e=>{const{normalize:n}=e;return n(["Mid Size Model"])},orchestration:e=>{const{normalize:n}=e;return n(["Orchestration"])},linux_foundation:e=>{const{normalize:n}=e;return n(["Linux Foundation"])},vector_database:e=>{const{normalize:n}=e;return n(["Vector Database"])},data_structure:e=>{const{normalize:n}=e;return n(["Data Structure"])},vector_store:e=>{const{normalize:n}=e;return n(["Vector Store"])},llm1:e=>{const{normalize:n}=e;return n(["LLMOps"])},rag1:e=>{const{normalize:n}=e;return n(["RAG"])}},contact_us_page:{title:e=>{const{normalize:n}=e;return n(["Contact sales"])},description:e=>{const{normalize:n}=e;return n(["Grow your business with Jina AI."])},impact_snapshots:e=>{const{normalize:n}=e;return n(["Impact Snapshots"])},subtitle:e=>{const{normalize:n}=e;return n(["Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise."])},subtitle1:e=>{const{normalize:n}=e;return n(["Jina AI, a leader in multimodal AI, excels in embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. Leveraging cloud-native technologies like Kubernetes and serverless architectures, we deliver robust, scalable, and production-ready solutions. With expertise in large language models, text, image, video, audio understanding, neural search, and generative AI, we provide innovative, future-proof strategies to elevate your business."])},subtitle2:e=>{const{normalize:n}=e;return n(["Explore Jina AI, the forefront of multimodal AI. We excel in embedding and prompt technologies, utilizing cloud-native solutions like Kubernetes for robust, scalable systems. Specializing in large language models and media processing, we offer innovative, future-ready business strategies with our advanced AI expertise."])},trusted_by:e=>{const{normalize:n}=e;return n(["Trusted by"])},name:e=>{const{normalize:n}=e;return n(["Name"])},work_email:e=>{const{normalize:n}=e;return n(["Work email"])},country:e=>{const{normalize:n}=e;return n(["Country"])},company:e=>{const{normalize:n}=e;return n(["Company"])},company_size:e=>{const{normalize:n}=e;return n(["Company size"])},company_website:e=>{const{normalize:n}=e;return n(["Company website"])},company_website_placeholder:e=>{const{normalize:n}=e;return n(["URL for your company's homepage or LinkedIn profile"])},invalid_url:e=>{const{normalize:n}=e;return n(["URL is invalid"])},invalid_email:e=>{const{normalize:n}=e;return n(["Email is invalid"])},preferred_products:e=>{const{normalize:n}=e;return n(["Which products are you interested in?"])},department:e=>{const{normalize:n}=e;return n(["Department"])},role:e=>{const{normalize:n}=e;return n(["Job role"])},field_required:e=>{const{normalize:n}=e;return n(["Field is required"])},invalid_date_format:e=>{const{normalize:n}=e;return n(["Invalid date format. Please use DD-MM-YYYY format."])},invalid_number:e=>{const{normalize:n}=e;return n(["Invalid number. Please input again"])},anything_else:e=>{const{normalize:n}=e;return n(["Tell us more about your project"])},agreement:e=>{const{normalize:n}=e;return n(["By submitting, you confirm that you agree to the processing of your personal data by Jina AI as described in the"])},private_statement:e=>{const{normalize:n}=e;return n(["Privacy Statement"])},submit:e=>{const{normalize:n}=e;return n(["Submit"])},submit_success:e=>{const{normalize:n}=e;return n(["Thank you for your submission. We will get back to you shortly."])},submit_failed:e=>{const{normalize:n}=e;return n(["Submission failed. Please try again later."])},faq:e=>{const{normalize:n}=e;return n(["FAQ"])}},blog_tags:{featured:e=>{const{normalize:n}=e;return n(["Featured"])},"tech-blog":e=>{const{normalize:n}=e;return n(["Tech blog"])},all:e=>{const{normalize:n}=e;return n(["All"])},press:e=>{const{normalize:n}=e;return n(["Press release"])},events:e=>{const{normalize:n}=e;return n(["Events"])},insights:e=>{const{normalize:n}=e;return n(["Insights"])},"knowledge-base":e=>{const{normalize:n}=e;return n(["Knowledge base"])},releases:e=>{const{normalize:n}=e;return n(["Software updates"])}},newsroom_page:{title:e=>{const{normalize:n}=e;return n(["Newsroom"])},top_stories:e=>{const{normalize:n}=e;return n(["Top stories"])},description:e=>{const{normalize:n}=e;return n(["Read the latest news and updates from Jina AI."])},description1:e=>{const{normalize:n}=e;return n(["Crafting AI innovations, one word at a time."])},tech_blog:e=>{const{normalize:n}=e;return n(["Tech blog"])},news_title:e=>{const{normalize:n}=e;return n(["Search All the Things: We're Running a MEME Contest for Jina 2.0"])},news_description:e=>{const{normalize:n}=e;return n(["For Jina 2.0, we listened to the community. Truly, deeply listened. \u201CWhat are your pain points?\u201D we asked, eagerly anticipating valuable feedback"])},engineering_group:e=>{const{normalize:n}=e;return n(["Engineering Group"])},engineering_group_date:e=>{const{normalize:n}=e;return n(["31 May, 2021"])},author:e=>{const{normalize:n}=e;return n(["By author"])},product:e=>{const{normalize:n}=e;return n(["By product"])},photos:e=>{const{normalize:n}=e;return n(["Photos"])},most_recent_articles:e=>{const{normalize:n}=e;return n(["Most recent articles"])},minutes_read:e=>{const{normalize:n}=e;return n(["minutes read"])},search:e=>{const{normalize:n}=e;return n(["Search by title"])}},news_page:{copy_link:e=>{const{normalize:n}=e;return n(["Copy the link to this section"])},news_not_found:e=>{const{normalize:n}=e;return n(["Article not found"])},redirect_to_news:e=>{const{normalize:n}=e;return n(["Redirecting to newsroom in 5 seconds..."])},back_to_newsroom:e=>{const{normalize:n}=e;return n(["Back to Newsroom"])},categories:e=>{const{normalize:n}=e;return n(["Categories"])},learn_more:e=>{const{normalize:n}=e;return n(["Learn more"])},in_this_article:e=>{const{normalize:n}=e;return n(["In this article"])}},github:{stars:e=>{const{normalize:n}=e;return n(["Stars"])}},share:{share_btn:e=>{const{normalize:n}=e;return n(["Share"])},"Hacker News":e=>{const{normalize:n}=e;return n(["Hacker News"])},LinkedIn:e=>{const{normalize:n}=e;return n(["LinkedIn"])},reddit:e=>{const{normalize:n}=e;return n(["Reddit"])},twitter:e=>{const{normalize:n}=e;return n(["X (Twitter)"])},facebook:e=>{const{normalize:n}=e;return n(["Facebook"])},rss:e=>{const{normalize:n}=e;return n(["RSS feed"])}},faq:{question1:e=>{const{normalize:n}=e;return n(["What does Jina AI specialize in?"])},answer1:e=>{const{normalize:n}=e;return n(["Jina AI specializes in multimodal AI technologies, including embedding-tuning, embedding-serving, prompt-tuning, and prompt-serving. We leverage advanced tools like Kubernetes and serverless architectures to create robust, scalable, and production-ready solutions."])},question2:e=>{const{normalize:n}=e;return n(["What types of AI does Jina AI work with?"])},answer2:e=>{const{normalize:n}=e;return n(["Our expertise spans a broad spectrum, encompassing large language models, text, image, video, audio understanding, neural search, and generative art."])},question3:e=>{const{normalize:n}=e;return n(["Are your solutions scalable and production-ready?"])},answer3:e=>{const{normalize:n}=e;return n(["Yes, our solutions are designed to be scalable and ready for production. We build our solutions using cloud-native technologies that allow for efficient scaling and reliable performance in production environments."])},question4:e=>{const{normalize:n}=e;return n(["What industries can benefit from Jina AI's solutions?"])},answer4:e=>{const{normalize:n}=e;return n(["Our services are versatile and adaptable, making them suitable for a wide range of industries, including e-commerce, legal tech, digital marketing, gaming, healthcare, finance, and many more."])},question5:e=>{const{normalize:n}=e;return n(["How do we start a project with Jina AI?"])},answer5:e=>{const{normalize:n}=e;return n(["You can get in touch with our sales team through the contact form on this page. We would love to discuss your project requirements and how our solutions can help your business."])},question6:e=>{const{normalize:n}=e;return n(["What support do you provide after implementing a solution?"])},answer6:e=>{const{normalize:n}=e;return n(["We provide continuous support to ensure the smooth operation of our solutions. This includes troubleshooting, regular updates, and improvements based on your feedback and needs."])},question7:e=>{const{normalize:n}=e;return n(["What is the typical duration for a project?"])},answer7:e=>{const{normalize:n}=e;return n(["Project duration varies depending on the complexity and scope of the project. After understanding your requirements, we can provide a more accurate estimate."])},question8:e=>{const{normalize:n}=e;return n(["How does Jina AI protect my data?"])},answer8:e=>{const{normalize:n}=e;return n(["Data security is our top priority. We adhere to strict data protection policies and regulations to ensure your data is secure and confidential."])},question9:e=>{const{normalize:n}=e;return n(["What is the pricing structure for your services?"])},answer9:e=>{const{normalize:n}=e;return n(["Pricing depends on the project's complexity and requirements. We offer both project-based and retainer pricing models. Please contact our sales team for more information."])},question10:e=>{const{normalize:n}=e;return n(["What are the licensing terms for your solutions?"])},answer10:e=>{const{normalize:n}=e;return n(["We provide different licensing options based on the nature of the project and the client's needs. Detailed terms can be discussed with our sales team."])},question11:e=>{const{normalize:n}=e;return n(["What is your service area?"])},answer11:e=>{const{normalize:n}=e;return n(["We provide services globally, with our headquarters based in Berlin, Europe, and additional offices in Beijing and Shenzhen."])},question12:e=>{const{normalize:n}=e;return n(["Do you offer onsite support?"])},answer12:e=>{const{normalize:n}=e;return n(["Yes, we offer onsite support, especially for clients located near our offices in Berlin, Beijing, and Shenzhen. For other locations, we strive to provide the best possible remote support and can arrange for onsite support if necessary."])}},beta:e=>{const{normalize:n}=e;return n(["Beta"])},print:e=>{const{normalize:n}=e;return n(["Print"])},subscribe_system:{used_product_required:e=>{const{normalize:n}=e;return n(["Select the model you are using or you are interested in"])},used_product:e=>{const{normalize:n}=e;return n(["Which model are you using?"])},domain_required:e=>{const{normalize:n}=e;return n(["Tell us your work domain helps us to provide better service"])},contactTitle_required:e=>{const{normalize:n}=e;return n(["Your job title is required"])},contactName_required:e=>{const{normalize:n}=e;return n(["How should we address you?"])},company_url_required:e=>{const{normalize:n}=e;return n(["Tell us your company's website helps us to provide better service"])},care_most_required:e=>{const{normalize:n}=e;return n(["When choosing a service, what do you care most about?"])},company_size_required:e=>{const{normalize:n}=e;return n(["Tell us your company's size helps us to provide better service"])},usage_type_required:e=>{const{normalize:n}=e;return n(["Tell us your usage type helps us to provide better service"])},get_new_key:e=>{const{normalize:n}=e;return n(["Get a new API key"])},usage_type_options:{research:e=>{const{normalize:n}=e;return n(["Research"])},poc:e=>{const{normalize:n}=e;return n(["Proof of Concept"])},production:e=>{const{normalize:n}=e;return n(["Production"])},other:e=>{const{normalize:n}=e;return n(["Other"])}},care_most_options:{accuracy:e=>{const{normalize:n}=e;return n(["Accuracy"])},speed:e=>{const{normalize:n}=e;return n(["Speed"])},cost:e=>{const{normalize:n}=e;return n(["Cost"])},scalability:e=>{const{normalize:n}=e;return n(["Scalability"])},other:e=>{const{normalize:n}=e;return n(["Other"])}},contactTitle:e=>{const{normalize:n}=e;return n(["What is your job title?"])},full_survey:e=>{const{normalize:n}=e;return n(["Take the full survey and get faster response from our team"])},usage_type:e=>{const{normalize:n}=e;return n(["What type of usage best describes you?"])},care_most:e=>{const{normalize:n}=e;return n(["What do you care most about?"])},company_url:e=>{const{normalize:n}=e;return n(["What is your company's website?"])},company_size:e=>{const{normalize:n}=e;return n(["What is your company's size?"])},contactName:e=>{const{normalize:n}=e;return n(["Your name"])},email:e=>{const{normalize:n}=e;return n(["Email"])},email_contact:e=>{const{normalize:n}=e;return n(["Your contact Email"])},subscribe:e=>{const{normalize:n}=e;return n(["Subscribe"])},send:e=>{const{normalize:n}=e;return n(["Send"])},contact_us:e=>{const{normalize:n}=e;return n(["Contact us"])},sign_up:e=>{const{normalize:n}=e;return n(["Sign up"])},tell_domain:e=>{const{normalize:n}=e;return n(["What domain do you work in?"])},get_update_embeddings:e=>{const{normalize:n}=e;return n(["Get the latest updates for the embeddings"])},get_update_blog_posts:e=>{const{normalize:n}=e;return n(["Get the latest updates for the blog posts"])},email_required:e=>{const{normalize:n}=e;return n(["Email is required"])},email_invalid:e=>{const{normalize:n}=e;return n(["Email is invalid"])},fine_tuned_embedding:e=>{const{normalize:n}=e;return n(["Interested in fine-tuned embeddings tailored to your data and use case? Let's discuss!"])},fine_tuned_reranker:e=>{const{normalize:n}=e;return n(["Interested in fine-tuned rerankers tailored to your data and use case? Let's discuss!"])}},lab_dialog:{explain:e=>{const{normalize:n}=e;return n(["Discover hidden features on our website"])},GlobalQA:{title:e=>{const{normalize:n}=e;return n(["On-page RAG"])},description:e=>{const{normalize:n}=e;return n(["Press the '/' key on any page to open the question box. Type your query and hit 'Enter' to receive answers directly related to the page content. This feature is powered by PromptPerfect."])}},SceneXplainTooltip:{title:e=>{const{normalize:n}=e;return n(["Image Captioning"])},description:e=>{const{normalize:n}=e;return n(["Hover your cursor over any image on news pages or in our newsroom catalog to reveal the description of that image. Descriptions are precomputed by SceneXplain and embedded in the image's ALT attribute for accessibility."])}},Recommender:{title:e=>{const{normalize:n}=e;return n(["Related Article"])},description:e=>{const{normalize:n}=e;return n(["Open the recommendation box on any news page with 'Shift+2'. Select the reranker model to discover the top-5 articles related to that news page. Enjoy this real-time feature, powered by our Reranker API."])}}},recommender:{recommended_articles:e=>{const{normalize:n}=e;return n(["Top-5 most related articles"])},recommend:e=>{const{normalize:n}=e;return n(["Get top-5 related"])},out_of_quota:e=>{const{normalize:n}=e;return n(["This API key has run out of tokens. Please recharge your account or use a different API key."])},confirm_title:e=>{const{normalize:n}=e;return n(["Warning: High Token Usage"])},confirm_message:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Your API key has ",r(o("_leftTokens"))," tokens remaining. Sending the full text of ",r(o("_numArticles"))," articles to the Reranker API, utilizing the ",r(o("_selectedReranker"))," model to discover related articles for the current page, will significantly reduce the token count of your API key ",r(o("_APIKey")),". Do you want to proceed?"])}},finetuning:{continue:e=>{const{normalize:n}=e;return n(["Continue"])},preview:e=>{const{normalize:n}=e;return n(["Preview"])},use_url:e=>{const{normalize:n}=e;return n(["Use URL instead. Toggle it on means we will base on the page content of that URL to generate synthetic data for fine-tuning."])},job_acknowledged:e=>{const{normalize:n}=e;return n(["Your fine-tuning job has been acknowledged. You will receive an email shortly after the job started."])},find_on_huggingface:e=>{const{normalize:n}=e;return n(["Find results on Hugging Face"])},not_enough_token:e=>{const{normalize:n}=e;return n(["Not enough tokens in this API key. Please top up your balance or use a different API key."])},wait_for_processing:e=>{const{normalize:n}=e;return n(["Please wait while we process your request..."])},back:e=>{const{normalize:n}=e;return n(["Back"])},failed_job:e=>{const{normalize:n}=e;return n(["The fine-tuning request failed. See the reason below."])},reset:e=>{const{normalize:n}=e;return n(["Start over"])},domain_hint:e=>{const{normalize:n}=e;return n(["Describe the domain you wish to fine-tune for."])},domain_explain:e=>{const{normalize:n}=e;return n(["Provide a detailed description of how the fine-tuned embeddings will be used. This is essential for generating high-quality synthetic data that will improve the performance of your embeddings."])},select_base_model:e=>{const{normalize:n}=e;return n(["Choose a base embedding model for fine-tuning."])},base_model_selected:e=>{const{normalize:n}=e;return n(["Base model selected"])},select_base_model_explain:e=>{const{normalize:n}=e;return n(["Select a base model as the starting point for fine-tuning. Typically, base-en is a good choice, but for tasks in other languages, consider using a bilingual model."])},write_email_explain:e=>{const{normalize:n}=e;return n(["Fine-tuning takes time. We'll communicate via email about the start, progress, completion, and any issues of your fine-tuning job, along with details on the fine-tuned model and training dataset."])},cost_1m_token:e=>{const{normalize:n}=e;return n(["Each fine-tuning job consumes 1M tokens. Ensure you have sufficient tokens or top-up your balance. You can also generate a new API key. Every API key comes with 1M free tokens."])},placeholder:e=>{const{normalize:n}=e;return n(["Car insurance claims"])},which_domain:e=>{const{normalize:n}=e;return n(["Fine-tuning domain"])},start_tuning:e=>{const{normalize:n}=e;return n(["Start fine-tuning"])},click_start:e=>{const{normalize:n}=e;return n(["Agree to the terms and begin fine-tuning by clicking the button below."])},how_it_works:e=>{const{normalize:n}=e;return n(["Learn about the fine-tuning process."])},email_not_match:e=>{const{normalize:n}=e;return n(["Email addresses do not match. Please verify."])},consent0:e=>{const{normalize:n}=e;return n(["I agree that synthetic data for model fine-tuning will be generated based on my instructions."])},consent1:e=>{const{normalize:n}=e;return n(["I acknowledge that the final model and synthetic data will be publicly accessible on Hugging Face."])},consent2:e=>{const{normalize:n}=e;return n(["I understand that this feature is in beta and Jina AI offers no warranties."])},confirm_title:e=>{const{normalize:n}=e;return n(["Confirm fine-tuning job"])},confirm_your_email:e=>{const{normalize:n}=e;return n(["Re-enter your email address to confirm the fine-tuning job. Updates and the download link will be sent to this email."])}},embedding:{tuning:e=>{const{normalize:n}=e;return n(["Fine-Tune"])},onprem:e=>{const{normalize:n}=e;return n(["On-prem"])},running:e=>{const{normalize:n}=e;return n(["Active"])},sleeping:e=>{const{normalize:n}=e;return n(["Inactive"])},status_explain:e=>{const{normalize:n}=e;return n(["Our serverless architecture may offload certain models during periods of low usage. For active models, responses are immediate. Inactive models require a few seconds to load upon the initial request. After activation, subsequent requests are processed more swiftly."])},score:e=>{const{normalize:n}=e;return n(["Score"])},index_and_search1:e=>{const{normalize:n}=e;return n(["Index & search"])},index_and_search:e=>{const{normalize:n}=e;return n(["Index & search"])},none:e=>{const{normalize:n}=e;return n(["None"])},results_fed_to_reranker:e=>{const{normalize:n}=e;return n(["#docs fed to reranker"])},results_as_final_result:e=>{const{normalize:n}=e;return n(["#docs as result"])},please_select_model:e=>{const{normalize:n}=e;return n(["Please select an Embedding model or a Reranker model"])},embedding_none_description:e=>{const{normalize:n}=e;return n(["Do not use any embedding model"])},rank_none_description:e=>{const{normalize:n}=e;return n(["Do not use any reranker model"])},get_new_key_survey:e=>{const{normalize:n}=e;return n(["Fill in the survey, help us understand your usage, and get a new API key for free!"])},pricing:e=>{const{normalize:n}=e;return n(["API Pricing"])},right_api_key_to_charge:e=>{const{normalize:n}=e;return n(["Please input the right API key to top up"])},pricing_desc:e=>{const{normalize:n}=e;return n(["Our API pricing is structured around the quantity of tokens sent in the requests. This pricing model is applicable to both embedding and reranking APIs. With the same API key, you have access to both services."])},select_embedding_model:e=>{const{normalize:n}=e;return n(["Select embeddings"])},select_rerank_model:e=>{const{normalize:n}=e;return n(["Select reranker"])},fill_example:e=>{const{normalize:n}=e;return n(["Fill in the example"])},"jina-reranker-v1-base-en_description":e=>{const{normalize:n}=e;return n(["The leading reranker maximizing search and RAG relevance"])},"jina-reranker-v1-turbo-en_description":e=>{const{normalize:n}=e;return n(["The best combination of fast inference speed and accurate relevance scores"])},"jina-reranker-v1-tiny-en_description":e=>{const{normalize:n}=e;return n(["The fastest reranker model, best suited for ranking a large number of documents reliably"])},"jina-colbert-v1-en_description":e=>{const{normalize:n}=e;return n(["Improved ColBERT with 8K-token length for embedding and reranking tasks"])},multi_embedding:e=>{const{normalize:n}=e;return n(["Multi-vector"])},multi_embedding_explain:e=>{const{normalize:n}=e;return n(["This model will return a bag of contextualized embeddings for a given input. Each token in the input is mapped to a vector in the output."])},read_api_docs:e=>{const{normalize:n}=e;return n(["Read the docs"])},input:e=>{const{normalize:n}=e;return n(["Request"])},output:e=>{const{normalize:n}=e;return n(["Response"])},usage_rerank:e=>{const{normalize:n}=e;return n(["Usage"])},input_length:e=>{const{normalize:n}=e;return n(["Input length"])},output_dimension:e=>{const{normalize:n}=e;return n(["Output dimensions"])},token_length_explain:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["The maximum length of the input token sequence is ",r(o("_tokenLength"))," for this model."])},output_dim_explain:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["The output dimension of an embedding vector from this model is ",r(o("_outputDim")),"."])},size_explain:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["The number of parameters in the model is ",r(o("_size")),", note that this is not the size of the model file."])},language_explain:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["This model best supports ",r(o("_language"))," language."])},opensource:e=>{const{normalize:n}=e;return n(["OSS"])},opensource_explain:e=>{const{normalize:n}=e;return n(["This model is open source and available on Hugging Face. Click this button to view the model on Hugging Face."])},wait_for_processing:e=>{const{normalize:n}=e;return n(["Your request is being processed."])},you_can_leave:e=>{const{normalize:n}=e;return n(["You can leave this page and we will send you the download link upon completion."])},click_upload_btn_above:e=>{const{normalize:n}=e;return n(["Click the upload button above to start."])},start_batch:e=>{const{normalize:n}=e;return n(["Start batch embedding"])},visualization_example_you_can:e=>{const{normalize:n}=e;return n(["Use our API below, you can do it too!"])},start_embedding:e=>{const{normalize:n}=e;return n(["Index"])},maximize_tooltip:e=>{const{normalize:n}=e;return n(["Maximize this panel"])},show_api_key:e=>{const{normalize:n}=e;return n(["Show API Key"])},batch_job:e=>{const{normalize:n}=e;return n(["Batch Job"])},bulk_embedding_failed:e=>{const{normalize:n}=e;return n(["Fail to create batch embedding job"])},search:e=>{const{normalize:n}=e;return n(["Search"])},visualize:e=>{const{normalize:n}=e;return n(["Visualize"])},bulk:e=>{const{normalize:n}=e;return n(["Batch embed"])},what_are_embedding:e=>{const{normalize:n}=e;return n(["What are Embeddings?"])},what_are_embedding_answer:e=>{const{normalize:n}=e;return n([`At the heart of modern natural language processing (NLP) lies a transformative technique known as text embeddings. Imagine the challenge of teaching a computer to grasp the nuanced meanings of words and phrases. Traditional methods, which relied on rigid, rule-based systems, fell short because language is too complex and fluid. Enter text embeddings: a powerful solution that translates text into a language of numbers\u2014specifically, into vectors in a high-dimensional space.

Consider the phrases "sunny weather" and "clear skies." To us, they paint a similar picture. Through the lens of embeddings, these phrases are transformed into numerical vectors that reside close to each other in this multi-dimensional space, capturing their semantic kinship. This closeness in the vector space is not just about words or phrases being similar; it's about understanding context, sentiment, and even subtle nuances in meaning.

Why is this breakthrough important? For starters, it bridges the gap between the richness of human language and the computational efficiency of algorithms. Algorithms excel at crunching numbers, not interpreting texts. By converting text into vectors, embeddings make it possible for these algorithms to 'understand' and process language in a way that was previously out of reach.

The practical applications are vast and varied. Whether it's recommending content that resonates with your interests, powering conversational AI that feels surprisingly human, or even detecting subtle patterns in large volumes of text, embeddings are the key. They enable machines to perform tasks like sentiment analysis, language translation, and much more, with an understanding of language that is increasingly nuanced and refined.`])},open_tensorboard:e=>{const{normalize:n}=e;return n(["Open visualizer"])},visualization_example:e=>{const{normalize:n}=e;return n(["Embedding all sentences from this section to a 3D vector space"])},visualize_done:e=>{const{normalize:n}=e;return n(["Visualization is done, you can now click the top button to open the visualizer."])},more_than_two2:e=>{const{normalize:n}=e;return n(["Please enter more than two documents, i.e. more than two lines."])},generating_visualization:e=>{const{normalize:n}=e;return n(["Generating visualization..."])},query:e=>{const{normalize:n}=e;return n(["Query"])},document:e=>{const{normalize:n}=e;return n(["Document"])},pairwise_test:e=>{const{normalize:n}=e;return n(["Pairwise"])},search_hint:e=>{const{normalize:n}=e;return n(["Type to search within the documents listed below"])},original_documents:e=>{const{normalize:n}=e;return n(["Documents to embed"])},total_documents:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Embedding progress: ",r(o("_Processed")),"/",r(o("_Count"))," documents."])},embedding_done:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Successfully embedded ",r(o("_Count"))," documents."])},please_fill_docs_first:e=>{const{normalize:n}=e;return n(["Please first enter some documents below before search."])},original_documents_hint:e=>{const{normalize:n}=e;return n(["Enter your documents here. Each new line will be considered a separate document."])},upload_file:e=>{const{normalize:n}=e;return n(["Click here to upload a file"])},max_file_size:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Maximum size allowed: ",r(o("_maxSize")),"."])},write_email_here:e=>{const{normalize:n}=e;return n(["Please enter the email where you want to receive the download link upon completion."])},upload:e=>{const{normalize:n}=e;return n(["Upload"])},download:e=>{const{normalize:n}=e;return n(["Download"])},batch_upload_hint:e=>{const{normalize:n}=e;return n(["We will use the API key and the model below to process the documents."])},autostart:e=>{const{normalize:n}=e;return n(["Embedding will automatically start after a brief delay"])},learn_more:e=>{const{normalize:n}=e;return n(["Learn more"])},why_do_you_need:e=>{const{normalize:n}=e;return n(["Choosing the Right Embeddings"])},why_do_you_need_before:e=>{const{normalize:n}=e;return n(["Our embedding models are specifically designed to cater to diverse applications, combining language, code and multimodal representation to open up new possibilities in AI-driven solutions."])},why_do_you_need_after:e=>{const{normalize:n}=e;return n(["Leveraging deep neural networks and LLMs, our embedding models represent multimodal data into a streamlined format, improving machine comprehension, efficient storage and enabling advanced AI applications. These embeddings play a crucial role in understanding the data, enhancing user engagement, overcoming language barriers, and optimizing development processes."])},why_need_1_title:e=>{const{normalize:n}=e;return n(["General-Purpose Embeddings"])},why_need_1_description:e=>{const{normalize:n}=e;return n(["Our core embedding model, powered by JinaBERT, is built for a broad spectrum of applications. It excels in understanding detailed text, making it ideal for semantic search, content classification, and intricate language analysis. Its versatility is unmatched, supporting the creation of advanced sentiment analysis tools, text summarization, and personalized recommendation systems."])},why_need_2_title:e=>{const{normalize:n}=e;return n(["Bilingual Embeddings"])},why_need_2_description:e=>{const{normalize:n}=e;return n(["Our bilingual models facilitate communication across languages, enhancing multilingual platforms, global customer support, and cross-lingual content discovery. Designed to master German-English and Chinese-English translations, these models simplify interactions and foster understanding among diverse linguistic groups."])},why_need_3_title:e=>{const{normalize:n}=e;return n(["Code Embeddings"])},why_need_3_description:e=>{const{normalize:n}=e;return n(["Tailored for developers, our code embedding model optimizes coding tasks like summarization, code generation, and automatic reviews. It boosts productivity by offering deeper insights into code structures and suggesting improvements, making it essential for developing advanced IDE plugins, automatic documentation, and cutting-edge debugging tools."])},top_up_warning_message1:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["The current API key has ",r(o("_remainedTokens"))," tokens remaining and will be replaced by a new key with ",r(o("_freeTokens"))," tokens. You may continue to use or top up the old key if you have stored it securely. How do you want to proceed?"])},top_up_warning_title:e=>{const{normalize:n}=e;return n(["Replace Old API Key"])},top_up_button:e=>{const{normalize:n}=e;return n(["Top Up Old Key"])},top_up_button_explain:e=>{const{normalize:n}=e;return n(["Integrating this API key offers a more professional solution, eliminating the need for frequent key changes. Usage data is retained and accessible at any time."])},get_new_key_button_explain:e=>{const{normalize:n}=e;return n(["Opting for a new key will result in the loss of usage history associated with the old key."])},get_new_key_button:e=>{const{normalize:n}=e;return n(["Get New Key"])},cancel_button:e=>{const{normalize:n}=e;return n(["Cancel"])},"1M_free":e=>{const{normalize:n}=e;return n(["1M free tokens"])},"1M_free_description":e=>{const{normalize:n}=e;return n(["Receive 1 million free tokens with each new API key, no credit card needed. Suitable for both personal and commercial projects."])},protectData1:e=>{const{normalize:n}=e;return n(["Request data and documents are not used for training models."])},protectData2:e=>{const{normalize:n}=e;return n(["Data encryption in transit (TLS 1.2+) and at rest (AES-GCM 256)."])},protectData3:e=>{const{normalize:n}=e;return n(["SOC 2 and GDPR compliant."])},multilingual:e=>{const{normalize:n}=e;return n(["Multilingual support"])},protect_data:e=>{const{normalize:n}=e;return n(["Protect Your Data"])},feature_multilingual:e=>{const{normalize:n}=e;return n(["Offering bilingual models for German-English, Chinese-English, Spanish-English among others, ideal for cross-lingual applications."])},add_pair:e=>{const{normalize:n}=e;return n(["New"])},poster:e=>{const{normalize:n}=e;return n(["The Evolution of Embeddings Poster"])},poster_description:e=>{const{normalize:n}=e;return n(["Discover the ideal poster for your space, featuring captivating infographics or breathtaking visuals tracing the evolution of text embedding models since 1950."])},buy_poster:e=>{const{normalize:n}=e;return n(["Buy a hard copy"])},learn_poster:e=>{const{normalize:n}=e;return n(["Learn how we made it"])},delete_pair:e=>{const{normalize:n}=e;return n(["Delete"])},length:e=>{const{normalize:n}=e;return n(["Token length"])},debugging:e=>{const{normalize:n}=e;return n(["Test"])},text1:e=>{const{normalize:n}=e;return n(["Left"])},text2:e=>{const{normalize:n}=e;return n(["Right"])},new:e=>{const{normalize:n}=e;return n(["New model"])},edit_text1_text:e=>{const{normalize:n}=e;return n(["Edit left text"])},edit_text2_text:e=>{const{normalize:n}=e;return n(["Edit right text"])},no_data1:e=>{const{normalize:n}=e;return n(["Add a pair of sentences to calculate the similarity"])},cosine_similarity:e=>{const{normalize:n}=e;return n(["Cosine similarity"])},"jina-embeddings-v2-base-es_description":e=>{const{normalize:n}=e;return n(["Spanish-English bilingual embeddings with SOTA performance"])},"jina-embeddings-v2-base-code_description":e=>{const{normalize:n}=e;return n(["Optimized for code and docstring search"])},"jina-embeddings-v2-small-en_description":e=>{const{normalize:n}=e;return n(["Optimized for low latency and memory footprint"])},"jina-embeddings-v2-base-en_description":e=>{const{normalize:n}=e;return n(["On par with OpenAI's text-embedding-ada002"])},"jina-embeddings-v2-base-zh_description":e=>{const{normalize:n}=e;return n(["Chinese-English bilingual embeddings with SOTA performance"])},"jina-embeddings-v2-base-de_description":e=>{const{normalize:n}=e;return n(["German-English bilingual embeddings with SOTA performance"])},learning1:e=>{const{normalize:n}=e;return n(["Learning about Embeddings"])},learning1_description:e=>{const{normalize:n}=e;return n(["Where to start with embeddings? We've got you covered. Learn about embeddings from the ground up with our comprehensive guide."])},feature_solid:e=>{const{normalize:n}=e;return n(["Best-in-class"])},feature_solid_description1:e=>{const{normalize:n}=e;return n(["Developed from our cutting-edge academic research and rigorously tested against the SOTA models to ensure unparalleled performance."])},feature_8k1:e=>{const{normalize:n}=e;return n(["8192 token-length"])},feature_8k_description1:e=>{const{normalize:n}=e;return n(["Pioneering the first open-source embedding model with an 8192-token length, enabling the representation of an entire chapter in a single vector."])},feature_top_perform1:e=>{const{normalize:n}=e;return n(["Seamless integration"])},feature_top_perform_description1:e=>{const{normalize:n}=e;return n(["Fully compatible with OpenAI's API. Effortlessly integrates with over 10 vector databases and RAG systems for a smooth user experience."])},feature_cheap:e=>{const{normalize:n}=e;return n(["50x cheaper"])},feature_cheap_v1:e=>{const{normalize:n}=e;return n(["5x more affordable"])},feature_cheap_v1_description1:e=>{const{normalize:n}=e;return n(["Start with free trials and enjoy a straightforward pricing structure. Get access to powerful embeddings for just 20% of OpenAI's cost."])},feature_on_premises:e=>{const{normalize:n}=e;return n(["Privacy first"])},feature_on_premises_description1:e=>{const{normalize:n}=e;return n(["Seamlessly deploy our embedding models directly within your Virtual Private Cloud (VPC). Currently supported on AWS Sagemaker, with forthcoming integrations for Microsoft Azure and Google Cloud Platform. For tailored Kubernetes deployments, reach out to our sales team for specialized assistance."])},feature_on_premises_description2:e=>{const{normalize:n}=e;return n(["Deploy Jina Embeddings models in AWS Sagemaker, and soon in Microsoft Azure and Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description3:e=>{const{normalize:n}=e;return n(["Deploy Jina Embeddings models in AWS Sagemaker and Microsoft Azure, and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers."])},feature_on_premises_description4:e=>{const{normalize:n}=e;return n(["Deploy Jina Embedding and Reranker models on-premises using AWS SageMaker, Microsoft Azure, or Google Cloud Services, ensuring your data remains securely in your control."])},vector_database_integration1:e=>{const{normalize:n}=e;return n(["Integrations"])},integrate:e=>{const{normalize:n}=e;return n(["Integrate"])},vector_database_integration_description:e=>{const{normalize:n}=e;return n(["Seamlessly and easily integrate the Jina Embeddings API with any of these databases, frameworks and applications below. Our tutorials will show you how."])},vector_database_integration2:e=>{const{normalize:n}=e;return n(["Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start."])},vector_database_integration3:e=>{const{normalize:n}=e;return n(["Our Embedding & Reranker API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start."])},api_integration_short:e=>{const{normalize:n}=e;return n(["Our Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks."])},title:e=>{const{normalize:n}=e;return n(["Embedding API"])},description:e=>{const{normalize:n,linked:r,type:o}=e;return n([r("landing_page.embedding_desc1",void 0,o)])},key_enter_placeholder_to_topup:e=>{const{normalize:n}=e;return n(["Enter the API key you wish to recharge"])},key_to_top_up:e=>{const{normalize:n}=e;return n(["API key for recharge"])},key_enter_placeholder:e=>{const{normalize:n}=e;return n(["Please enter your API key"])},key_warn_v2:e=>{const{normalize:n}=e;return n(["Each new key has some free tokens for you to try out. You can top up your key at any time. Make sure to store your API key at a safe place!"])},key_warn:e=>{const{normalize:n}=e;return n(["Make sure to store your API key at a safe place. Otherwise you will need to generate a new key"])},refresh_key_tooltip1:e=>{const{normalize:n}=e;return n(["Get a new API key for free"])},regenerate:e=>{const{normalize:n}=e;return n(["Regenerate"])},retry:e=>{const{normalize:n}=e;return n(["Retry"])},refresh:e=>{const{normalize:n}=e;return n(["Refresh"])},generate_api_key_error:e=>{const{normalize:n}=e;return n(["Fail to generate an API key"])},key:e=>{const{normalize:n}=e;return n(["API key"])},code:e=>{const{normalize:n}=e;return n(["code"])},manage_quota1:e=>{const{normalize:n}=e;return n(["Buy tokens"])},size:e=>{const{normalize:n}=e;return n(["Parameters"])},output_dim:e=>{const{normalize:n}=e;return n(["Dimensions"])},remaining:e=>{const{normalize:n}=e;return n(["Available tokens"])},usage:e=>{const{normalize:n}=e;return n(["Usage"])},api_integrations:e=>{const{normalize:n}=e;return n(["API Integrations"])},usage_history:e=>{const{normalize:n}=e;return n(["Usage history"])},view_details:e=>{const{normalize:n}=e;return n(["View Details"])},input_api_key_error1:e=>{const{normalize:n}=e;return n(["Your API key doesn't exist"])},usage_time:e=>{const{normalize:n}=e;return n(["Time"])},usage_amount:e=>{const{normalize:n}=e;return n(["Amount"])},usage_reason:e=>{const{normalize:n}=e;return n(["Reason"])},usage_reason_trial:e=>{const{normalize:n}=e;return n(["Trial"])},usage_reason_consume:e=>{const{normalize:n}=e;return n(["Consume"])},usage_reason_purchase:e=>{const{normalize:n}=e;return n(["Purchase"])},wait_stripe:e=>{const{normalize:n}=e;return n(["Opening Stripe payment, please wait"])},refresh_token_count1:e=>{const{normalize:n}=e;return n(["Refresh to get available tokens of current API key"])},what_is_a_token:e=>{const{normalize:n}=e;return n(['A token in text processing is a unit, often a word. For example, "Jina AI is great!" becomes five tokens, including the punctuation.'])},token_example:e=>{const{normalize:n}=e;return n([`A tweet is about 20 tokens, a news article is about 1000 tokens, and Charles Dickens' novel "A Tale of Two Cities" has over a million tokens.`])},buy_more_quota:e=>{const{normalize:n}=e;return n(["Top up this API key by selecting the tokens you need"])},tokens:e=>{const{normalize:n}=e;return n(["Tokens"])},"500M tokens":e=>{const{normalize:n}=e;return n(["500M tokens"])},"500M tokens_intuition1":e=>{const{normalize:n}=e;return n(["10 years of the NY Times every day, or reading In Search of Lost Time 300 times."])},"1B tokens":e=>{const{normalize:n}=e;return n(["1B tokens"])},"1B tokens_intuition1":e=>{const{normalize:n}=e;return n(["Roughly 10,000 average novels, or rereading all the Harry Potter novels 1000 times."])},"2_5B tokens":e=>{const{normalize:n}=e;return n(["2.5B tokens"])},"2_5B tokens_intuition1":e=>{const{normalize:n}=e;return n(["All the words spoken in one day by 150,000 people, or 100 times the size of the US Code of Federal Regulations."])},"5_5B tokens":e=>{const{normalize:n}=e;return n(["5.5B tokens"])},"5_5B tokens_intuition1":e=>{const{normalize:n}=e;return n(["All the words ever published in the NY Times."])},"11B tokens":e=>{const{normalize:n}=e;return n(["11B tokens"])},"11B tokens_intuition1":e=>{const{normalize:n}=e;return n(["A bit more than the entire English Wikipedia."])},"59B tokens":e=>{const{normalize:n}=e;return n(["59B tokens"])},"59B tokens_intuition1":e=>{const{normalize:n}=e;return n(["All the tweets in the world for two days."])},per_k:e=>{const{normalize:n}=e;return n(["/ 1K tokens"])},per_m:e=>{const{normalize:n}=e;return n(["/ 1M tokens"])},faq:e=>{const{normalize:n,linked:r,type:o}=e;return n([r("contact_us_page.faq",void 0,o)])},file_type_not_supported:e=>{const{normalize:n}=e;return n(["File type not supported"])},file_size_exceed:e=>{const{normalize:n,interpolate:r,named:o}=e;return n(["Exceed max file size ",r(o("_size"))])},model_required:e=>{const{normalize:n}=e;return n(["Model is required"])},file_required:e=>{const{normalize:n}=e;return n(["File is required"])},faqs_v2:{title:e=>{const{normalize:n}=e;return n(["Embeddings-related common questions"])},question1:e=>{const{normalize:n}=e;return n(["How many API requests can I make per second?"])},answer1:e=>{const{normalize:n}=e;return n(["Each user is allowed to make up to 100 requests per second, equating to 204,800 input sentences per second."])},question2:e=>{const{normalize:n}=e;return n(["Are the models behind the API open source?"])},answer2:e=>{const{normalize:n}=e;return n(["Yes, all of our models are open source and accessible on Hugging Face."])},question0:e=>{const{normalize:n}=e;return n(["How were the jina-embeddings-v2 models trained?"])},answer0:e=>{const{normalize:n}=e;return n(["For detailed information on our training processes, data sources, and evaluations, please refer to our technical report available on arXiv."])},question3:e=>{const{normalize:n}=e;return n(["Which languages do your models support?"])},answer3:e=>{const{normalize:n}=e;return n(["Our models support English, German, Spanish, Chinese, and various programming languages. For more details, please refer to our publication on bilingual models."])},question4:e=>{const{normalize:n}=e;return n(["What is the maximum length for a single sentence input?"])},answer4:e=>{const{normalize:n}=e;return n(["Our models allow for an input length of up to 8192 tokens, which is significantly higher than most other models. A token can range from a single character, like 'a', to an entire word, such as 'apple'. The total number of characters that can be input depends on the length and complexity of the words used. This extended input capability enables our jina-embeddings-v2 models to perform more comprehensive text analysis and achieve higher accuracy in context understanding, especially for extensive textual data."])},question5:e=>{const{normalize:n}=e;return n(["What is the maximum number of sentences I can include in a single request?"])},answer5:e=>{const{normalize:n}=e;return n(["A single API call can process up to 2048 sentences or texts, facilitating extensive text analysis in one request."])},question7:e=>{const{normalize:n}=e;return n(["How do Jina Embeddings models compare to OpenAI's text-embedding-ada-002 model?"])},answer7:e=>{const{normalize:n}=e;return n(["According to the MTEB Leaderboard, our Base model competes closely with OpenAI\u2019s text-embedding-ada-002, exhibiting comparable performance on average. Furthermore, our Base model excels in several tasks, including classification, pair-classification, re-ranking, and summarization, outperforming OpenAI\u2019s model."])},question8:e=>{const{normalize:n}=e;return n(["How seamless is the transition from OpenAI's text-embedding-ada-002 to your solution?"])},answer8:e=>{const{normalize:n}=e;return n(["The transition is streamlined, as our API endpoint, https://api.jina.ai/v1/embeddings, matches the input and output JSON schemas of OpenAI\u2019s text-embeddings-ada-002 model. This compatibility ensures users can easily replace the OpenAI model with ours when using OpenAI\u2019s endpoint."])},question17:e=>{const{normalize:n}=e;return n(["Do you provide models for embedding images or audio?"])},answer17:e=>{const{normalize:n}=e;return n(["We are currently developing multimodal embeddings that will jointly process text, images, and audio. Updates will be announced soon!"])},question18:e=>{const{normalize:n}=e;return n(["Can Jina Embedding models be fine-tuned with private or company data?"])},answer18:e=>{const{normalize:n}=e;return n(["For inquiries about fine-tuning our models with specific data, please contact us to discuss your requirements. We are open to exploring how our models can be adapted to meet your needs."])},question19:e=>{const{normalize:n}=e;return n(["Can your endpoints be hosted privately on AWS, Azure, or GCP?"])},answer19:e=>{const{normalize:n}=e;return n(["Yes, our services are available on the AWS marketplace, and we are in the process of expanding to Azure and GCP marketplaces. If you have particular requirements, please contact us at sales AT jina.ai."])}}},billing_general_faq:{title:e=>{const{normalize:n}=e;return n(["Billing-related common questions"])},question9:e=>{const{normalize:n}=e;return n(["Is billing based on the number of sentences or requests?"])},answer9:e=>{const{normalize:n}=e;return n(["Our pricing model is based on the total number of tokens processed, allowing users the flexibility to allocate these tokens across any number of sentences, offering a cost-effective solution for diverse text analysis requirements."])},question10:e=>{const{normalize:n}=e;return n(["Is there a free trial available for new users?"])},answer10:e=>{const{normalize:n}=e;return n(["We offer a welcoming free trial to new users, which includes one million tokens for use with any of our models, facilitated by an auto-generated API key. Once the free token limit is reached, users can easily purchase additional tokens for their API keys via the 'Buy tokens' tab."])},question13:e=>{const{normalize:n}=e;return n(["Are tokens charged for failed requests?"])},answer13:e=>{const{normalize:n}=e;return n(["No, tokens are not deducted for failed requests."])},question14:e=>{const{normalize:n}=e;return n(["What payment methods are accepted?"])},answer14:e=>{const{normalize:n}=e;return n(["Payments are processed through Stripe, supporting a variety of payment methods including credit cards, Google Pay, and PayPal for your convenience."])},question15:e=>{const{normalize:n}=e;return n(["Is invoicing available for token purchases?"])},answer15:e=>{const{normalize:n}=e;return n(["Yes, an invoice will be issued to the email address associated with your Stripe account upon the purchase of tokens."])}},api_general_faq:{title:e=>{const{normalize:n}=e;return n(["API-related common questions"])},question1:e=>{const{normalize:n}=e;return n(["Can I use the same API key for both the embedding and reranking APIs?"])},answer1:e=>{const{normalize:n}=e;return n(["Yes, the same API key is usable for both the embedding and reranking APIs, with tokens shared between the two services."])},question5:e=>{const{normalize:n}=e;return n(["Do API keys expire?"])},answer5:e=>{const{normalize:n}=e;return n(["No, our API keys do not have an expiration date. However, if you suspect your key has been compromised and wish to retire it or transfer its tokens to a new key, please contact our support team for assistance."])},question3:e=>{const{normalize:n}=e;return n(["Can I monitor the token usage of my API key?"])},answer3:e=>{const{normalize:n}=e;return n(["Yes, token usage can be monitored in the 'Buy tokens' tab by entering your API key, allowing you to view the usage history and remaining tokens."])},question4:e=>{const{normalize:n}=e;return n(["What should I do if I forget my API key?"])},answer4:e=>{const{normalize:n}=e;return n(["If you have misplaced a topped-up key and wish to retrieve it, please contact support AT jina.ai with your registered email for assistance."])},question6:e=>{const{normalize:n}=e;return n(["Why is the first request for some models slow?"])},answer6:e=>{const{normalize:n}=e;return n(["This is because our serverless architecture offloads certain models during periods of low usage. The initial request activates or 'warms up' the model, which may take a few seconds. After this initial activation, subsequent requests process much more quickly."])},question12:e=>{const{normalize:n}=e;return n(["Is user input data used for training your models?"])},answer12:e=>{const{normalize:n}=e;return n(["We adhere to a strict privacy policy and do not use user input data for training our models."])}}};export{t as default};
